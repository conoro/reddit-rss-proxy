<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-07T13:49:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1o00bnb</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-10-07T00:31:27+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://b.thumbs.redditmedia.com/vo1uUejG2VE5tG6ipAmjCB3f3SG-JplGsGuMrSb94Ug.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI, here are the local/edge highlights from today's edition:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ModernVBERT - 250M beats 2.5B models&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;7x faster CPU inference&lt;/li&gt; &lt;li&gt;Bidirectional attention beats causal by +10.6 nDCG@5&lt;/li&gt; &lt;li&gt;Runs on devices that can't load traditional models&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2510.01149"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/ModernVBERT"&gt;HuggingFace&lt;/a&gt; | &lt;a href="https://colab.research.google.com/drive/1bT5LWeO1gPL83GKUZsFeFEleHmEDEQRy"&gt;Colab&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r15po9xz3ltf1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=729ce13d7c40e57130be324b03b66d1a978b31d7"&gt;https://preview.redd.it/r15po9xz3ltf1.png?width=1170&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=729ce13d7c40e57130be324b03b66d1a978b31d7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-VL - GPT-5 performance at 3B active params&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Matches GPT-5-Mini and Claude4-Sonnet&lt;/li&gt; &lt;li&gt;Handles STEM, VQA, OCR, video, agents&lt;/li&gt; &lt;li&gt;FP8 quantized version available&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;DocPruner - Cut storage by 60%&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;lt;1% performance drop&lt;/li&gt; &lt;li&gt;Adaptive pruning per document&lt;/li&gt; &lt;li&gt;Makes multi-vector retrieval affordable&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2509.23883"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/05ix4vj34ltf1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d993b6e742f4fd49b72dfdb3c37e3c0fe19a21ba"&gt;The illustration of comparison between OCR-based (a) &amp;amp; LVLM-based (b) paradigms for VDR, and DocPruner (c), a novel framework to adaptively prune the patch-level embeddings for diverse document types.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fathom-DeepResearch - 4B SOTA web investigation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Two specialized 4B models&lt;/li&gt; &lt;li&gt;DuetQA dataset + RAPO optimization&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2509.24107"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/FractalAIResearchLabs/Fathom-DeepResearch"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4.5 codes for 30+ hours straight&lt;/li&gt; &lt;li&gt;Ovi generates synchronized audio-video&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o00bnb/video/qfohebyw4ltf1/player"&gt;https://reddit.com/link/1o00bnb/video/qfohebyw4ltf1/player&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CU-1 achieves 67.5% GUI click accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1o00bnb/video/8syoo09y4ltf1/player"&gt;https://reddit.com/link/1o00bnb/video/8syoo09y4ltf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full newsletter(demos,papers,more): &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-27-small-models"&gt;https://thelivingedge.substack.com/p/multimodal-monday-27-small-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o00bnb/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o08bcz</id>
    <title>NVIDIA 5060Ti or AMD Radeon RX 9070 XT for running local LLMs?</title>
    <updated>2025-10-07T07:38:11+00:00</updated>
    <author>
      <name>/u/Solid-Language-7106</name>
      <uri>https://old.reddit.com/user/Solid-Language-7106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning to set up a local machine for running LLMs and I'm debating between two GPUs: the &lt;strong&gt;NVIDIA RTX 5060 Ti&lt;/strong&gt; and the &lt;strong&gt;AMD Radeon RX 9070 XT&lt;/strong&gt;. My budget is tight, so the &lt;strong&gt;RX 9070 XT would be the highest I can go&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid-Language-7106"&gt; /u/Solid-Language-7106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08bcz/nvidia_5060ti_or_amd_radeon_rx_9070_xt_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08bcz/nvidia_5060ti_or_amd_radeon_rx_9070_xt_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o08bcz/nvidia_5060ti_or_amd_radeon_rx_9070_xt_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T07:38:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0dsd8</id>
    <title>Help setting up a RAG Pipeline.</title>
    <updated>2025-10-07T12:46:25+00:00</updated>
    <author>
      <name>/u/vaibhavyagnik</name>
      <uri>https://old.reddit.com/user/vaibhavyagnik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;I am an Instrumentation Engineer and i have to deal with a lot a documents in the form of PDF, Word and large excel documents. I want to create a locally hosted LLM which can answer questions based on the documents I feed it. I have watched a lot of videos on how to do it. So far I have infered that the process is called RAG - Retrieval Augmented Generation. Basically documents are parsed, chunked and stored in vector database and LLM answers looking at the database. For parsing and chunking I have identified docling which I have installed on a server running Ubuntu 24.04 LTS with dual xeon CPUs and 178 GB of RAM, No GPU unfortunately. For webui, I have installed docling-serve. For LLM, I have gone with openweb-ui and I have tried phi3 and mistral 7b.&lt;/p&gt; &lt;p&gt;I have tried to run docling so that it writes to the same db as openwebui but so far the answers have been very very wrong. I even tried to upload documents directly to the model. The answers are better but that not what I want to achieve.&lt;/p&gt; &lt;p&gt;Do you guys have any insights on what can I do to&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Feed documents and keep increasing the knowledge of LLM&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Verify that knowledge is indeed getting updated&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Improve answering accuracy of LLM&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vaibhavyagnik"&gt; /u/vaibhavyagnik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dsd8/help_setting_up_a_rag_pipeline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dsd8/help_setting_up_a_rag_pipeline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dsd8/help_setting_up_a_rag_pipeline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T12:46:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzimvg</id>
    <title>October 2025 model selections, what do you use?</title>
    <updated>2025-10-06T13:10:24+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"&gt; &lt;img alt="October 2025 model selections, what do you use?" src="https://preview.redd.it/syzg3f8oqhtf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e686e4b8db47ba995e1c43c3c24fb0dd3547175e" title="October 2025 model selections, what do you use?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/syzg3f8oqhtf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzimvg/october_2025_model_selections_what_do_you_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T13:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzrhkg</id>
    <title>Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!</title>
    <updated>2025-10-06T18:42:07+00:00</updated>
    <author>
      <name>/u/cogwheel0</name>
      <uri>https://old.reddit.com/user/cogwheel0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"&gt; &lt;img alt="Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!" src="https://external-preview.redd.it/NDR0cGwzZ3NkanRmMe3itAdHO7JxtY5YivFkYCiYZ8sXROLwyG4vlc6wIxOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bc502b079d4c56d8fea5995ce2aefb9a0d85fd7" title="Conduit 2.0 - OpenWebUI Mobile Client: Completely Redesigned, Faster, and Smoother Than Ever!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;A few months back, &lt;a href="https://www.reddit.com/r/selfhosted/comments/1mo9w3t/built_a_native_openwebui_client_for_ios_android/"&gt;I shared my native mobile client for OpenWebUI&lt;/a&gt;. I'm thrilled to drop version 2.0 today, which is basically a full rebuild from the ground up. I've ditched the old limitations for a snappier, more customizable experience that feels right at home on iOS and Android.&lt;/p&gt; &lt;p&gt;If you're running OpenWebUI on your server, this update brings it to life in ways the PWA just can't match. Built with Flutter for cross-platform magic, it's open-source (as always) and pairs perfectly with your self-hosted setup.&lt;/p&gt; &lt;p&gt;Here's what's new in 2.0:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance Overhaul&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switched to Riverpod 3 for state management, go_router for navigation, and Hive for local storage.&lt;/li&gt; &lt;li&gt;New efficient Markdown parser means smoother scrolling and rendering—chats load instantly, even with long threads. (Pro tip: Data migrates automatically on update. If something glitches, just clear app data and log back in.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fresh Design &amp;amp; Personalization&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total UI redesign: Modern, clean interfaces that are easier on the eyes and fingers.&lt;/li&gt; &lt;li&gt;Ditch the purple-only theme, pick from new accent colors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upgraded Chat Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Share handling:&lt;/strong&gt; Share text/image/files from anywhere to start a chat. Android users also get an OS-wide 'Ask Conduit' context menu option when selecting text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two input modes:&lt;/strong&gt; Minimal for quick chats, or extended with one-tap access to tools, image generation, and web search.&lt;/li&gt; &lt;li&gt;Slash commands! Type &amp;quot;/&amp;quot; in the input to pull up workspace prompts.&lt;/li&gt; &lt;li&gt;Follow-up suggestions to keep conversations flowing.&lt;/li&gt; &lt;li&gt;Mermaid diagrams now render beautifully in.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;AI Enhancements&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-to-Speech (TTS) for reading responses aloud. (Live calling is being worked on for the next release!)&lt;/li&gt; &lt;li&gt;Realtime status updates for image gen, web searches, and tools, matching OpenWebUI's polished UX.&lt;/li&gt; &lt;li&gt;Sources and citations for web searches and RAG based responses.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Grab it now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;iOS&lt;/strong&gt;: &lt;a href="https://apps.apple.com/us/app/conduit-openwebui-client/id6749840287"&gt;App Store Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Android&lt;/strong&gt;: &lt;a href="https://play.google.com/store/apps/details?id=app.cogwheel.conduit"&gt;Google Play Link&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source &amp;amp; Builds&lt;/strong&gt;: &lt;a href="https://github.com/cogwheel0/conduit"&gt;GitHub Repo&lt;/a&gt; (FOSS forever—stars and PRs welcome!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huge thanks to the community for the feedback on 1.x. What do you think? Any must-have features for 2.1? Post below, or open an issue on GitHub if you're running into setup quirks. Happy self-hosting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cogwheel0"&gt; /u/cogwheel0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s0i7luesdjtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzrhkg/conduit_20_openwebui_mobile_client_completely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:42:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0bo9q</id>
    <title>Help Needed: Local MP3 Translation Workflow (to English) Using Open-Source LLMs</title>
    <updated>2025-10-07T11:06:19+00:00</updated>
    <author>
      <name>/u/Snoo-6077</name>
      <uri>https://old.reddit.com/user/Snoo-6077</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need help setting up a local translation workflow (to English) for MP3 audio using only open-source LLMs. I’ve tried this repo: &lt;a href="https://github.com/kyutai-labs/delayed-streams-modeling"&gt;https://github.com/kyutai-labs/delayed-streams-modeling&lt;/a&gt; — it can convert speach-to-text with timestamps, but it doesn’t seem to support using timestamps for text-to-audio alignment. Any advice or examples on how to build a working pipeline for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo-6077"&gt; /u/Snoo-6077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0bo9q/help_needed_local_mp3_translation_workflow_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0bo9q/help_needed_local_mp3_translation_workflow_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0bo9q/help_needed_local_mp3_translation_workflow_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T11:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0ensk</id>
    <title>llm-registry - Track model capabilities, costs, and features across 15+ providers (OpenAI, Anthropic, Google, etc.)</title>
    <updated>2025-10-07T13:23:26+00:00</updated>
    <author>
      <name>/u/yamanahlawat</name>
      <uri>https://old.reddit.com/user/yamanahlawat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I built &lt;strong&gt;LLM Registry&lt;/strong&gt; - a Python tool to manage LLM model metadata across multiple providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; Check a model's capabilities before making API calls, compare costs across providers, and maintain custom configurations. Tracks costs, features (streaming, tools, vision, JSON mode), API parameters, and context limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it exists:&lt;/strong&gt; No unified way to query model capabilities programmatically. You either hardcode this or check docs constantly. Messy when building multi-provider tools, comparing costs, or managing custom models.&lt;/p&gt; &lt;p&gt;Includes 70+ verified models (OpenAI, Anthropic, Google, Cohere, Mistral, Meta, xAI, Amazon, Microsoft, DeepSeek, Ollama, etc.). Add your own too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built with:&lt;/strong&gt; Python 3.13+, Pydantic (data validation), Typer + Rich (CLI)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick example:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```python from llm_registry import CapabilityRegistry&lt;/p&gt; &lt;p&gt;registry = CapabilityRegistry() model = registry.get_model(&amp;quot;gpt-5&amp;quot;) print(f&amp;quot;Cost: ${model.token_costs.input_cost}/M tokens&amp;quot;) ```&lt;/p&gt; &lt;p&gt;CLI: &lt;code&gt;bash pip install llm-registry llmr list --provider openai llmr get gpt-5 --json &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt; - GitHub: &lt;a href="https://github.com/yamanahlawat/llm-registry"&gt;https://github.com/yamanahlawat/llm-registry&lt;/a&gt; - PyPI: &lt;a href="https://pypi.org/project/llm-registry/"&gt;https://pypi.org/project/llm-registry/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback or contributions! Let me know if you find this useful or have ideas for improvements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yamanahlawat"&gt; /u/yamanahlawat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ensk/llmregistry_track_model_capabilities_costs_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ensk/llmregistry_track_model_capabilities_costs_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0ensk/llmregistry_track_model_capabilities_costs_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T13:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzskwx</id>
    <title>Kiln RAG Builder: Now with Local &amp; Open Models</title>
    <updated>2025-10-06T19:23:09+00:00</updated>
    <author>
      <name>/u/davernow</name>
      <uri>https://old.reddit.com/user/davernow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"&gt; &lt;img alt="Kiln RAG Builder: Now with Local &amp;amp; Open Models" src="https://external-preview.redd.it/NzFjNWRjcHdranRmMTouR_gGQN0P-1NcenpN-hyrI6W-iM6M3YFQQOwxr0u6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=353cf1fe2f3a8b489a6a9078f45dea8e0c4b7989" title="Kiln RAG Builder: Now with Local &amp;amp; Open Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone - two weeks ago we launched our new RAG-builder &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnso4p/new_rag_builder_create_a_sota_rag_system_in_under/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;on here&lt;/a&gt; and &lt;a href="https://github.com/kiln-ai/kiln"&gt;Github&lt;/a&gt;. It allows you to build a RAG in under 5 minutes with a simple drag and drop interface. Unsurprisingly, LocalLLaMA requested local + open model support! Well we've added a bunch of open-weight/local models in our new release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Extraction models&lt;/strong&gt; (vision models which convert documents into text for RAG indexing): Qwen 2.5VL 3B/7B/32B/72B, Qwen 3VL and GLM 4.5V Vision&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding models&lt;/strong&gt;: Qwen 3 embedding 0.6B/4B/8B, Embed Gemma 300M, Nomic Embed 1.5, ModernBert, M2 Bert, E5, BAAI/bge, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can run fully local with a config like Qwen 2.5VL + Qwen 3 Embedding. We added an &amp;quot;All Local&amp;quot; RAG template, so you can get started with local RAG with 1-click.&lt;/p&gt; &lt;p&gt;Note: we’re waiting on Llama.cpp support for Qwen 3 VL (so it’s open, but not yet local). We’ll add it as soon as it’s available, for now you can use it via the cloud.&lt;/p&gt; &lt;p&gt;Progress on other asks from the community in the last thread:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic chunking&lt;/strong&gt;: We have this working. It's still in a branch while we test it, but if anyone wants early access let us know on &lt;a href="https://getkiln.ai/discord"&gt;Discord&lt;/a&gt;. It should be in our next release.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph RAG (specifically Graphiti)&lt;/strong&gt;: We’re looking into this, but it’s a bigger project. It will take a while as we figure out the best design.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some links to the repo and guides:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/Kiln-AI/Kiln"&gt;Kiln AI on Github: &amp;gt;4k stars&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.kiln.tech/docs/documents-and-search-rag"&gt;Documents &amp;amp; Search (RAG) Docs/Guide&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://getkiln.ai/discord"&gt;Kiln Discord&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://kiln.tech"&gt;Homepage&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to answer questions if anyone wants details or has ideas! Let me know if you want support for any specific local vision models or local embedding models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/davernow"&gt; /u/davernow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lioqj7pwkjtf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzskwx/kiln_rag_builder_now_with_local_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T19:23:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o03s6a</id>
    <title>SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size</title>
    <updated>2025-10-07T03:14:57+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large language models (LLMs) face significant computational and memory challenges, making extremely low-bit quantization crucial for their efficient deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size, a novel framework that enables extremely low-bit quantization of LLMs while preserving their linguistic reasoning capabilities. A distinctive feature of SDQ-LLM is the continuous adjustability of the Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal trade-off between model size and accuracy. SDQ-LLM uses upsampling combined with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding high-precision parameters into 1-bit or 1.58-bit representations, replacing the multiplication operations within linear layers with addition. This approach significantly enhances inference efficiency under extremely low-bit quantization. To further reduce the loss of quantization precision, we incorporate Hadamard-based weight smoothing prior to quantization, improving the stability and robustness of the weight representations. Furthermore, to fully leverage the continuity of the OSR and reduce precision loss, recognizing the correlation between quantization sensitivity and weight variance, we propose a fine-grained, layer- and linear-wise OSR allocation strategy, MultiOSR. This strategy distributes OSR both across layers and within each layer, based on weight variance and parameter scale. Finally, extensive experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a more efficient and high-precision performance even under highly aggressive low-OSR settings. Our code is available at &lt;a href="https://github.com/Dreamlittlecat/LLM-Quant-Factory"&gt;https://github.com/Dreamlittlecat/LLM-Quant-Factory&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Dreamlittlecat/LLM-Quant-Factory"&gt;https://github.com/Dreamlittlecat/LLM-Quant-Factory&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.03275"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o03s6a/sdqllm_sigmadelta_quantization_for_1bit_llms_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o03s6a/sdqllm_sigmadelta_quantization_for_1bit_llms_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T03:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1o09ldg</id>
    <title>Code2Video — generate educational videos via executable code</title>
    <updated>2025-10-07T09:01:16+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o09ldg/code2video_generate_educational_videos_via/"&gt; &lt;img alt="Code2Video — generate educational videos via executable code" src="https://external-preview.redd.it/UOWl8T4J9irFfHRdqJtGJYpkTYi2lEDU3BuG-j3RMng.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b2858b89f6b366fbb8e1feb65f0bff1a3739bb8" title="Code2Video — generate educational videos via executable code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/yez52b86nntf1.gif"&gt;https://i.redd.it/yez52b86nntf1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/showlab/code2video"&gt;GitHub&lt;/a&gt;&lt;br /&gt; Agentic, &lt;em&gt;code-centric&lt;/em&gt; pipeline that turns a knowledge point into a clear Manim video—prioritizing structure, reproducibility, and teaching quality. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tri-agent flow:&lt;/strong&gt; Planner → Coder → Critic; uses executable Manim to control timing/layout. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quick try:&lt;/strong&gt; &lt;code&gt;pip install -r requirements.txt&lt;/code&gt;, add LLM/VLM keys; authors note best results with &lt;strong&gt;Claude-4-Opus&lt;/strong&gt; (coding) + &lt;strong&gt;Gemini 2.5&lt;/strong&gt; (layout).&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o09ldg/code2video_generate_educational_videos_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o09ldg/code2video_generate_educational_videos_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o09ldg/code2video_generate_educational_videos_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T09:01:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzozpg</id>
    <title>Granite4 Small-h 32b-A9b (Q4_K_M) at FULL 1M context window is using only 73GB of VRAM - Life is good!</title>
    <updated>2025-10-06T17:09:38+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This model seems to fit nicely on a single H100 or RTX Pro 6000. it’s great for high context RAG. This is the perfect model for my use case of models that call multiple tools in the same prompt while RAGing a bunch of knowledge bases. Might be our new daily driver for RAG use cases. If they add reasoning and vision then this is probably going to be everybody’s workhorse model. Great job big blue!! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;KV cache set to Q8_0&lt;/li&gt; &lt;li&gt;Output tokens set to 131,072&lt;/li&gt; &lt;li&gt;Num_ctx set to 1000000 (I know it’s supposed to be 1048576 but Ollama errors out at that value for some reason) &lt;/li&gt; &lt;li&gt;Unsloth recommended settings for everything else. &lt;/li&gt; &lt;li&gt;Seems to support and perform “native” tool calling as well as GPT-OSS. &lt;/li&gt; &lt;li&gt;70.88 response tokens/s &lt;/li&gt; &lt;li&gt;Open WebUI as my front end client and Ollama 0.12.4 rc6 for inference &lt;/li&gt; &lt;li&gt;FRIGGIN’ 1 Million context window locally is crazy to me!! &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzozpg/granite4_smallh_32ba9b_q4_k_m_at_full_1m_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T17:09:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1o065ig</id>
    <title>AudioBook Maker with Ebook Editor Using Chatterbox TTS</title>
    <updated>2025-10-07T05:23:16+00:00</updated>
    <author>
      <name>/u/Devajyoti1231</name>
      <uri>https://old.reddit.com/user/Devajyoti1231</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Desktop application to create Full Audiobooks from ebook(epub/text) , chapterwise audio for the ebook etc using chatterbox tts and Easy Ebook Editor to Edit ebooks, export chapters from it, import chapters, create new ebook, edit metadata etc&lt;/p&gt; &lt;p&gt;Other options are-&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Direct Local TTS&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Remote API Support with tts-webui (&lt;/strong&gt;&lt;a href="https://github.com/rsxdalv/TTS-WebUI"&gt;&lt;strong&gt;https://github.com/rsxdalv/TTS-WebUI&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multiple Input Formats&lt;/strong&gt; - TXT, PDF, EPUB support&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Voice Management&lt;/strong&gt; - Easy voice reference handling&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Advanced Settings&lt;/strong&gt; - Full control over TTS parameters&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Preset System&lt;/strong&gt; - Save and load your favorite settings&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Audio Player&lt;/strong&gt; - Preview generated audio instantly&lt;/p&gt; &lt;p&gt;Github link - &lt;a href="https://github.com/D3voz/audiobook-maker-pro"&gt;https://github.com/D3voz/audiobook-maker-pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full 33 min long one chapter sample from final empire - &lt;a href="https://screenapp.io/app/#/shared/JQh3r66YZw"&gt;https://screenapp.io/app/#/shared/JQh3r66YZw&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Performance Comparison (NVIDIA 4060 Ti):&lt;/h1&gt; &lt;p&gt;-Local Mode Speed: ~37 iterations/sec&lt;/p&gt; &lt;p&gt;-API Mode Speed(using tts-webui) : ~80+ iterations/sec (over 2x faster)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Devajyoti1231"&gt; /u/Devajyoti1231 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065ig/audiobook_maker_with_ebook_editor_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065ig/audiobook_maker_with_ebook_editor_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o065ig/audiobook_maker_with_ebook_editor_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T05:23:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzx061</id>
    <title>GLM 4.6 is the top new open weight model on Design Arena</title>
    <updated>2025-10-06T22:06:33+00:00</updated>
    <author>
      <name>/u/No-Tackle-5388</name>
      <uri>https://old.reddit.com/user/No-Tackle-5388</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"&gt; &lt;img alt="GLM 4.6 is the top new open weight model on Design Arena" src="https://b.thumbs.redditmedia.com/omSAbCz7n8MkX7LnPoLxwq4PBhxTLhqcszulgIHDZZI.jpg" title="GLM 4.6 is the top new open weight model on Design Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 4.6 is outperforming the new Kimi models and both DeepSeek 3.2 and 3.2-exp in the seven day overall category on design arena. It's also beating every non-Anthropic SOTA model.&lt;/p&gt; &lt;p&gt;I saw a post a few days ago showing it also took the top position on lmarena (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm%5C_46%5C_new%5C_best%5C_open%5C_weight%5C_overall%5C_on%5C_lmarena/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nxbbxe/glm\_46\_new\_best\_open\_weight\_overall\_on\_lmarena/&lt;/a&gt;) and it looks like it's doing the same for visual reasoning. This model is insane&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hzp0gpp8ektf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a7c84277e40c130e803a7bb5c6c7d8a2674f6a1"&gt;https://preview.redd.it/hzp0gpp8ektf1.png?width=1883&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a7c84277e40c130e803a7bb5c6c7d8a2674f6a1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Tackle-5388"&gt; /u/No-Tackle-5388 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzx061/glm_46_is_the_top_new_open_weight_model_on_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T22:06:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzzurf</id>
    <title>Granite 4 (gguf) is useless if you try to use the full 128k context.</title>
    <updated>2025-10-07T00:09:43+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; After some research, no model is actually able to use that context size, all model maker are liar. I'm learning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; its useless with long context from my test with multiple model, and configuration. Both MLX and GUFF&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I had a special task, required 156k token, decided to try it.&lt;/p&gt; &lt;p&gt;I have a game guide i made with AI, i know its full of error(i'm slowly correcting them as i spot them), so i gave the guide, with the full wiki of said game, and ask the model to find mistake.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;The website contain wrong information. Find them by comparing the information to the official wiki. Report all of them. &amp;lt;website&amp;gt; ... &amp;lt;/website&amp;gt; &amp;lt;game wiki&amp;gt; ... &amp;lt;/game wiki&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With LmStudio, All runtime updated. M2 Max 64GB.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I tried Granite 4.0 H Small 8Bit MLX at first (had to trim some data, MLX only support about 131k context for some reason?).&lt;/p&gt; &lt;p&gt;The response was a barely coherent new guide covering one of the subject of the game.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-h-small-mlx (23.24 tok/sec, 781 tokens, 607.44s to first token, Stop reason: User Stopped) Introduction In this guide, we'll discuss the various methods for generating income in the game RimWorld. By understanding these strategies and optimizing your colony's operations, you'll be able to build a thriving settlement capable of surviving any challenge thrown its way. Farming One of the primary methods for generating income in RimWorld is through farming. Planting crops and harvesting them can provide a steady stream of resources, which in turn can be sold or processed into various products. &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;I never had any luck with any MLX, so i gave the GUFF a try.&lt;/p&gt; &lt;p&gt;Now with : Granite 4.0 H Small GGUF Q4_K_M(Q8 with that much context wont fit my 64gb) (let me push to 164K context keeping all my data)&lt;/p&gt; &lt;p&gt;After about 5 minuet of prompt processing, it crashed :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;lmstudio/granite-4.0-h-small@q4_k_m Processing Prompt... 85% This message contains no content. The AI has nothing to say. &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Lets try tiny, its not like it need knowledge, i give it all it need.&lt;/p&gt; &lt;p&gt;Again, ignore, instruction, write a small guide on part of the content.&lt;/p&gt; &lt;p&gt;ibm/granite-4-h-tiny Q8&lt;/p&gt; &lt;pre&gt;&lt;code&gt;23.88 tok/sec•822 tokens•221.59s to first token•Stop reason: EOS Token Found Ideology Min-Maxing Here are some tips and suggestions for optimizing your ideoligion choices for the best results: Fluid vs Static Ideology: If you don't need specific memes immediately, choose fluid to adapt to various scenarios in a single game. Choose static if you have a clear idea of what you want from the start. &lt;/code&gt;&lt;/pre&gt; &lt;hr /&gt; &lt;p&gt;Since not all Guff are created equal, lets try the latest sensation everybody toot : unsloth, ill even follow their guide : &lt;a href="https://docs.unsloth.ai/new/ibm-granite-4.0"&gt;https://docs.unsloth.ai/new/ibm-granite-4.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ill even trim my context to 114k , unsloth guide say its max 131k, even though Ibm said no limit, i guess GUFF have limitation?&lt;/p&gt; &lt;p&gt;unsloth/granite-4.0-h-tiny-GGUF(26.68 tok/sec•794 tokens•152.78s to first token•Stop reason: EOS Token Found&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-h-tiny Food crops: Corn: Product = Silver 163.5 per hour work; Profit/day plant growth = Silver 1.16. Haygrass: Product = Silver 73.0 per hour work; Profit/day plant growth = Silver 0.83. Rice: Product = Silver 44.6 per hour work; Profit/day plant growth = Silver 1.19. Potatoes: Product = Silver 81.77 per hour work; Profit/day plant growth = Silver 1.12. Strawberries: Product = Silver 101.35 per hour work; Profit/day plant growth = Silver 1.02. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Again, its just write a small guide on part of the data...&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Lets try the unsloth small version with recommended setting, we never know, i might have screw up setting.&lt;/p&gt; &lt;p&gt;unsloth : Granite 4.0 H Small GGUF Q4_K_M&lt;/p&gt; &lt;pre&gt;&lt;code&gt;granite-4.0-h-small Processing Prompt... 81% This message contains no content. The AI has nothing to say. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Crash while processing the prompt, while under the 131k limit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzzurf/granite_4_gguf_is_useless_if_you_try_to_use_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzzurf/granite_4_gguf_is_useless_if_you_try_to_use_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzzurf/granite_4_gguf_is_useless_if_you_try_to_use_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:09:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nz722n</id>
    <title>Biggest Provider for the community for at moment thanks to them</title>
    <updated>2025-10-06T02:17:03+00:00</updated>
    <author>
      <name>/u/dead-supernova</name>
      <uri>https://old.reddit.com/user/dead-supernova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt; &lt;img alt="Biggest Provider for the community for at moment thanks to them" src="https://preview.redd.it/6kl3hy76ietf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86e1c6a42810d36cbc6b71792855914f69ca24a1" title="Biggest Provider for the community for at moment thanks to them" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dead-supernova"&gt; /u/dead-supernova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6kl3hy76ietf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nz722n/biggest_provider_for_the_community_for_at_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T02:17:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzru92</id>
    <title>AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker</title>
    <updated>2025-10-06T18:55:22+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"&gt; &lt;img alt="AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker" src="https://external-preview.redd.it/cRxMiHUULYNg9ilSv0igBEg5GDo6YuyoZ5csopbw4k0.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd1ec0577ebabb74b53154e89231b7a124d894a1" title="AMD stock skyrockets 30% as OpenAI looks to take stake in AI chipmaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/06/openai-amd-chip-deal-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzru92/amd_stock_skyrockets_30_as_openai_looks_to_take/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T18:55:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o065kb</id>
    <title>Can you recommend a course for my youngster?</title>
    <updated>2025-10-07T05:23:21+00:00</updated>
    <author>
      <name>/u/pleok</name>
      <uri>https://old.reddit.com/user/pleok</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 13-year-old whose school rules do not allow kids to pass off AI work as their own, which I generally support. Whether my kids starts using AI now or later, I know it's going to be ubiquitous tech throughout my kid's formative years, so I am thinking of a positive way my family can dispell some of the mystique, learn about it, and take advantage of the tech while keeping our eyes out for potential dangers. I feel my kid should know a little about what an LLm is comprised of and how it works. To that end, I am looking for an online course on how to build and train your own LLM from scratch, would be appropriate for tech savvy kids, requires little to no programming skills (or just basic programming skills that can be learned along the way), and whose goals would be to teach the &amp;quot;basics&amp;quot; of how an LLM works by having the student follow along and build/train their own with ollama or whatever. While I am not a complete novice when it comes to LLMs, I have never built/trained my own models. For my kid's setup, we could use a Lenovo gaming laptop i9, 32 gb ram, Nvidia geforce rtx4070, 8 gb vram. Not good for big models but maybe enough for the basics (?). I suppose we could just buy the compute power, but I think having a local model residing on our own machine would be cooler and provide some good learning opportunities. Heck, I might even join my kid in the course. Any suggestions for an online course (free or paid)? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pleok"&gt; /u/pleok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065kb/can_you_recommend_a_course_for_my_youngster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o065kb/can_you_recommend_a_course_for_my_youngster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o065kb/can_you_recommend_a_course_for_my_youngster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T05:23:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzl8y5</id>
    <title>How Transformers avoids becoming a black box, even at 1M+ LOC</title>
    <updated>2025-10-06T14:53:04+00:00</updated>
    <author>
      <name>/u/El_Olbap</name>
      <uri>https://old.reddit.com/user/El_Olbap</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I'm Pablo from Hugging Face Open-Source team. We just wrote a software-engineering focused deep dive on how we keep the `transformers` library hackable/maintainable while it keeps growing and growing. If you're running models locally, fine-tuning on your own hardware, or just want to understand the code you're using, I recommend the read!&lt;/p&gt; &lt;p&gt;Light spoilers about what's in it:&lt;/p&gt; &lt;p&gt;- ****One Model, One File:**** You can still read a `modeling_*.py` top-to-bottom and see exactly what's happening.&lt;/p&gt; &lt;p&gt;- ****Modular Transformers:**** This is our trick to fight code bloat. Contributors can reuse code via a small `modular_*.py` file, but we auto-generate the full, readable modeling file so you never lose the &amp;quot;one file&amp;quot; experience. It cut our maintenance work by ~15x.&lt;/p&gt; &lt;p&gt;- ****Config-Driven Performance:**** Features like FlashAttention(and ofc 2,3..), tensor parallelism (`tp_plan`), and per-layer attention schedules are enabled in the config, not by changing the model code. A `Linear` layer is always just a `Linear` layer, you don't have to change it depending on how it's sliced.&lt;/p&gt; &lt;p&gt;- ****Tools for Local Use:**** This philosophy lets us build helpful tools. The post covers an attention visualizer, a model tracer for debugging ports, and faster CUDA warmups, and we also go over `transformers serve` usage. &lt;/p&gt; &lt;p&gt;Hope you enjoy the read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/El_Olbap"&gt; /u/El_Olbap &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/transformers-community/Transformers-tenets"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzl8y5/how_transformers_avoids_becoming_a_black_box_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T14:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0a437</id>
    <title>Human or LLM? - Guess the human-written sentence</title>
    <updated>2025-10-07T09:34:53+00:00</updated>
    <author>
      <name>/u/n00bi3s</name>
      <uri>https://old.reddit.com/user/n00bi3s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How many times can you find the human written texts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n00bi3s"&gt; /u/n00bi3s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai-or-human.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0a437/human_or_llm_guess_the_humanwritten_sentence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0a437/human_or_llm_guess_the_humanwritten_sentence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T09:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzn1mk</id>
    <title>Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU</title>
    <updated>2025-10-06T15:58:14+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt; &lt;img alt="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" src="https://external-preview.redd.it/_S9eclPc4WRscWHOsVO80UpXnpu4dfbG_wCYpnVLuPA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d53f3fcd941a63a8ecb5a50a6c26e1cf55db3e1a" title="Running GPT-OSS (OpenAI) Exclusively on AMD Ryzen™ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a fast runtime for running &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama&lt;/strong&gt;, but deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;✨ &lt;strong&gt;From Idle Silicon to Instant Power — FastFlowLM (FLM) Makes Ryzen™ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10× more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (14 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo → Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt; &lt;em&gt;→ Quick start guide, NPU vs CPU vs GPU, etc.&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ksYyiUQvYfo?si=zfBjb7U86P947OYW"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzn1mk/running_gptoss_openai_exclusively_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T15:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o08igx</id>
    <title>Improved "time to first token" in LM Studio</title>
    <updated>2025-10-07T07:51:05+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"&gt; &lt;img alt="Improved &amp;quot;time to first token&amp;quot; in LM Studio" src="https://preview.redd.it/m2ttxrud9ntf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca5e7ac12bb3b1413f0d820c13cc0f0b9bd9d1b5" title="Improved &amp;quot;time to first token&amp;quot; in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benching some of my models on my M4 Max 128GB a few days ago, see the attached image.&lt;/p&gt; &lt;p&gt;Today I noticed an update of the MLX runtime in LM Studio:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;MLX version info: - mlx-engine==6a8485b - mlx==0.29.1 - mlx-lm==0.28.1 - mlx-vlm==0.3.3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With this, &amp;quot;time to first token&amp;quot; has been improved dramatically. As an example:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next:80b&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;4 bit MLX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// 80k context window + 36k token prompt length Time to first token: 47 ➔ 46 seconds :| // 120k context window + 97k token prompt length Time to first token: 406 ➔ 178 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next:80b&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;6 bit MLX&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;// 80k context window + 36k token prompt length Time to first token: 140 ➔ 48 seconds // 120k context window + 97k token prompt length Time to first token: 436 ➔ 190 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can anyone confirm?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m2ttxrud9ntf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o08igx/improved_time_to_first_token_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T07:51:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o00ban</id>
    <title>Open Source Alternative to Perplexity</title>
    <updated>2025-10-07T00:31:00+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here’s a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Podcasts support with local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mergeable MindMaps.&lt;/li&gt; &lt;li&gt;Note Management&lt;/li&gt; &lt;li&gt;Multi Collaborative Notebooks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o00ban/open_source_alternative_to_perplexity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T00:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1o04s7y</id>
    <title>2 things we never forget, our first GPU and when your first GPU dies</title>
    <updated>2025-10-07T04:07:24+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just had a 3090 die, maybe I will resurrect it, maybe not. It comes with the territory of buying used GPUs from miners.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o04s7y/2_things_we_never_forget_our_first_gpu_and_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T04:07:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nzwnbj</id>
    <title>The qwen3-next pr in llamacpp has been validated with a small test model</title>
    <updated>2025-10-06T21:52:11+00:00</updated>
    <author>
      <name>/u/Betadoggo_</name>
      <uri>https://old.reddit.com/user/Betadoggo_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt; &lt;img alt="The qwen3-next pr in llamacpp has been validated with a small test model" src="https://b.thumbs.redditmedia.com/WI7I1cXVA_soqNsjPi7jb3WgjXU9vLhmlQDKP6zYAUY.jpg" title="The qwen3-next pr in llamacpp has been validated with a small test model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to comment: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382"&gt;https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3373977382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been stalking this pr since it was opened and figured I'd share this update since I know a lot of others were interested in this model. Pwilkin has done some crazy work getting this together so quickly.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Betadoggo_"&gt; /u/Betadoggo_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nzwnbj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nzwnbj/the_qwen3next_pr_in_llamacpp_has_been_validated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-06T21:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o0dy0y</id>
    <title>More love for GLM4.6 (evaluation vs. Claude 4.5 for NLP tasks)</title>
    <updated>2025-10-07T12:53:15+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been putting GLM4.6 and Claude 4.5 head to head relentlessly since both were released, and really can't overstate how impressive GLM4.6 is. I'm using both over OpenRouter. &lt;/p&gt; &lt;p&gt;My use case: critically evaluating published AI literature, working on my own architecture ideas, summarizing large articles, picking through sprawling conversations for the salient ideas.&lt;/p&gt; &lt;p&gt;What's really impressive to me is how good GLM4.6 is at following my instructions to the letter, understanding nuanced ways that I want it to analyze data, and avoiding putting its own spin on things. It's also absolutely fantastic at &amp;quot;thinking in character&amp;quot; (I use persona prompts to process information in parallel from different perspectives - ie. one run to critique literature and probe quality of experimental set-ups, another run to evaluate whether are creative implications that I'm missing, etc.) - this is a model that loves a great system prompt. The ability to shape the way GLM4.6 reasons is really impressive. The draw back in terms of persona prompting is that while GLM4.6 is great at functionally behaving according to the prompt, its tonal style usually drifts. I think this is more a factor of how MoE models process RP-adjacent prompting (I find that dense models are massively better at this) than it is a GLM4.6 problem specifically. GLM4.6 holds on to technical details of what I'm either reading or writing *spectacularly* well. It seems even more clear-headed than Claude when it comes to working on implementation ideas, or paying attention to implementation that I'm reading about. &lt;/p&gt; &lt;p&gt;Claude Sonnet 4.5 is impressive in terms of its ability to follow a huge list of complicated topics across many turns. Of every LLM I have tried, this truly keeps its head together longer than any I've tried. I have pushed the context window ridiculously far and have only seen one or two minor factual errors. Exact instruction following (ie. system instructions about cognitive processing requirements) gets dulled over time, for sure. And while 4.5 seems far better at persona prompting than 4 did, there's an underlying Claude-ness that just can't be denied. Even without the obnoxious LCR stuff going on in the Anthropic UI (not to mention their shady data mining reversal), Claude can't help but lapse into Professor Dad mode. (Just like Gemini can't really avoid being a former high school valedictorian who got into an Ivy on a lacrosse scholarship while still suffering from imposter syndrome) &lt;/p&gt; &lt;p&gt;GLM4.6 doesn't stay coherent quite as long - and there are some weird glitches: lapses into Chinese, confusing its reasoning layer for its response layer, and becoming repetitive in long responses (ie. saying the same thing twice). Still, it remains coherent FAR longer than Gemini 2.5 Pro. &lt;/p&gt; &lt;p&gt;What I find really interesting about GLM4.6 is that it seems to have no overtly detectable ideological bias - it's really open, and depending on how you prompt it, can truly look at things from multiple perspectives. DeepSeek and Kimi K2 both have slants (which I happen to dig!) - this might be the most flexible model I have tried, period. &lt;/p&gt; &lt;p&gt;If the lapse-into-chinese and repetitive loops could be stamped out a bit, this would be the no-brainer LLM to build with for what I do. (As always, with the caveat that I'm praying daily for a dense Gemma 3 or Gemma 4 model in the 50B+ range)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o0dy0y/more_love_for_glm46_evaluation_vs_claude_45_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-07T12:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
