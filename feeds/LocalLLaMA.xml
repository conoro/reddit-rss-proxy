<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-28T16:26:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qp3piq</id>
    <title>When you know you nailed it! Or not. GLM-4.7-NVFP4 (B300 - Blackwell Ultra)</title>
    <updated>2026-01-28T06:27:24+00:00</updated>
    <author>
      <name>/u/Lorelabbestia</name>
      <uri>https://old.reddit.com/user/Lorelabbestia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp3piq/when_you_know_you_nailed_it_or_not_glm47nvfp4/"&gt; &lt;img alt="When you know you nailed it! Or not. GLM-4.7-NVFP4 (B300 - Blackwell Ultra)" src="https://b.thumbs.redditmedia.com/whhmXC3xgqp-kaf0fOpdyFPOwbGy8V8MEwBiFJeO4CQ.jpg" title="When you know you nailed it! Or not. GLM-4.7-NVFP4 (B300 - Blackwell Ultra)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/u8wp6rwx11gg1.png?width=1234&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a1704120504f79731501b6efc23bf0ae80b36db"&gt;https://preview.redd.it/u8wp6rwx11gg1.png?width=1234&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a1704120504f79731501b6efc23bf0ae80b36db&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quite new to Hyperparameter Tuning, I found &lt;a href="https://docs.sglang.io/advanced_features/hyperparameter_tuning.html"&gt;this guide&lt;/a&gt; on sglang and started playing with it. I have a multi-agent system using GLM-4.7, which runs 24/7 full throttle and I'm assessing if it makes sense to rent a GPU to do so. Any suggestion would be welcome!&lt;/p&gt; &lt;p&gt;I tried Cerebras and it is crazy fast, but it costs a lot of money. &lt;/p&gt; &lt;p&gt;I'm currently on a GLM Max Plan and it's crazy slow, but the value is unbeatable.&lt;/p&gt; &lt;p&gt;I was able to crank up the GPU, memory usage, parallelism and token usage on SGLang, but still it seems to me that the overall throughput and also prompt processing are quite low (or at least below my expectations), I assume due to low memory to actually parallelize.&lt;/p&gt; &lt;p&gt;My workflow is basically a bunch of agents at about max. 20K in and max 5K out, so I was testing out the worst case scenario and I was able to fit in 16 concurrent requests (representing each agent), but gen throughput was only at about ~210 tok/s.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yc2wgjiu61gg1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f66580edf68f5385449622a8323895d9b13e729"&gt;https://preview.redd.it/yc2wgjiu61gg1.png?width=1878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3f66580edf68f5385449622a8323895d9b13e729&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I guess the issue here is the fact that the amount of parallellism achievable was quite low due to memory limitation of a single B300 on such a large model (even at NVFP4). There was only space to fit 339,524 tk BF16 KV Cache. &lt;/p&gt; &lt;p&gt;I saw that BF16 is faster due to SGLang lacking native FP4 cache without decompression, but I think it would've been better to run at lower quant cache to allow higher parallellism on more memory left, but I still have to try it out.&lt;/p&gt; &lt;p&gt;Next time I'll try with 2xB300 for comparison.&lt;/p&gt; &lt;p&gt;Just for quick reference, this is how much tokens I spend daily on GLM-4.7 Max Plan:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zpdq3rn591gg1.png?width=3168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b174538855a88dd537a1c30251f8f111b277d4b8"&gt;https://preview.redd.it/zpdq3rn591gg1.png?width=3168&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b174538855a88dd537a1c30251f8f111b277d4b8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When I'm all in I use about 600M daily (that's not throughput though), for about 80$/3 months = 0,86$ a day. So it's still much better for me to have multiple of these subscriptions. If you worry about keeping data private that's another concern, in my use case I don't have anything concerning privacy, so for me cheaper is better.&lt;/p&gt; &lt;p&gt;Configs used:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;docker run --rm -d \ --name sglang-glm47-nvfp4 \ --gpus '&amp;quot;device=0&amp;quot;' \ --ipc=host \ --shm-size 64g \ -v &amp;quot;/models:/models&amp;quot; \ -p 30000:30000 \ --ulimit memlock=-1 \ --ulimit stack=67108864 \ nvcr.io/nvidia/sglang:25.12-py3 \ python3 -m sglang.launch_server \ --model Salyut1/GLM-4.7-NVFP4 \ --host 0.0.0.0 \ --port 30000 \ --tp 1 \ --trust-remote-code \ --quantization modelopt_fp4 \ --attention-backend triton \ --mem-fraction-static 0.95 \ --max-running-requests 256 \ --schedule-conservativeness 0.3 \ --disable-radix-cache \ --chunked-prefill-size 24576 \ --max-prefill-tokens 24576 \ --schedule-policy fcfs \ --enable-torch-compile \ --enable-piecewise-cuda-graph \ --piecewise-cuda-graph-max-tokens 1300 \ --enable-mixed-chunk &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lorelabbestia"&gt; /u/Lorelabbestia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp3piq/when_you_know_you_nailed_it_or_not_glm47nvfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp3piq/when_you_know_you_nailed_it_or_not_glm47nvfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp3piq/when_you_know_you_nailed_it_or_not_glm47nvfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:27:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpf3np</id>
    <title>nosy: CLI to summarize various types of content</title>
    <updated>2026-01-28T15:53:52+00:00</updated>
    <author>
      <name>/u/aqny</name>
      <uri>https://old.reddit.com/user/aqny</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpf3np/nosy_cli_to_summarize_various_types_of_content/"&gt; &lt;img alt="nosy: CLI to summarize various types of content" src="https://external-preview.redd.it/rA9Q1S4OYwKSmxu6XBJuFnEJwIjr2YX_5W2NY7LTjLs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b6fc5caa22233d6c07e63a3744922b704ad8577" title="nosy: CLI to summarize various types of content" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m the author of &lt;strong&gt;nosy&lt;/strong&gt;. I’m posting for feedback/discussion, not as a link drop.&lt;/p&gt; &lt;p&gt;I often want a repeatable way to turn “a URL or file” into clean text and then a summary, regardless of format. So I built a small CLI that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Accepts &lt;strong&gt;URLs or local files&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Fetches via &lt;strong&gt;HTTP GET&lt;/strong&gt; or &lt;strong&gt;headless browser&lt;/strong&gt; (for pages that need JS)&lt;/li&gt; &lt;li&gt;Auto-selects a text extractor by &lt;strong&gt;MIME type / extension&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Extracts from &lt;strong&gt;HTML, PDF, Office docs (via pandoc), audio/video (via Whisper transcription)&lt;/strong&gt;, etc.&lt;/li&gt; &lt;li&gt;Summarizes with &lt;strong&gt;multiple LLM providers&lt;/strong&gt; (OpenAI / Anthropic / Gemini / …)&lt;/li&gt; &lt;li&gt;Lets you customize tone/structure via &lt;strong&gt;Handlebars templates&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Has shell &lt;strong&gt;tab completion&lt;/strong&gt; (zsh/bash/fish)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aqny"&gt; /u/aqny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ynqa/nosy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpf3np/nosy_cli_to_summarize_various_types_of_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpf3np/nosy_cli_to_summarize_various_types_of_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp5apn</id>
    <title>Testing GLM-4.7 Flash: Multi-GPU Vulkan vs ROCm in llama-bench | (2x 7900 XTX)</title>
    <updated>2026-01-28T07:59:08+00:00</updated>
    <author>
      <name>/u/SemaMod</name>
      <uri>https://old.reddit.com/user/SemaMod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp5apn/testing_glm47_flash_multigpu_vulkan_vs_rocm_in/"&gt; &lt;img alt="Testing GLM-4.7 Flash: Multi-GPU Vulkan vs ROCm in llama-bench | (2x 7900 XTX)" src="https://a.thumbs.redditmedia.com/I3E7eArXJ7Eng4huN47Lbx-C6gf4gehqqViYHL_-jM4.jpg" title="Testing GLM-4.7 Flash: Multi-GPU Vulkan vs ROCm in llama-bench | (2x 7900 XTX)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After hearing so much about Vulkan perf I decided to build llama.cpp and test it out. I also saw the latest mesa-amdgpu-vulkan-drivers (v26) were supposed to give a big perf boost for gaming specifically, but the update seems to have made Vulkan stretch its lead even further.&lt;/p&gt; &lt;h1&gt;Building Llama.cpp:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \ cmake -S . -B build -DGGML_HIP=ON -DGGML_VULKAN=ON -DGGML_HIP_ROCWMMA_FATTN=ON -DGPU_TARGETS=gfx1100 -DCMAKE_BUILD_TYPE=Release \ &amp;amp;&amp;amp; cmake --build build --config Release -- -j 16 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Benchmarks ran&lt;/h1&gt; &lt;h1&gt;Vulkan&lt;/h1&gt; &lt;p&gt;&lt;code&gt;llama-bench -m ~/.cache/llama.cpp/unsloth_GLM-4.7-Flash-GGUF_GLM-4.7-Flash-Q8_0.gguf -dev Vulkan0/Vulkan1 -fa 0/1 -mg 1&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;ROCm&lt;/h1&gt; &lt;p&gt;&lt;code&gt;llama-bench -m ~/.cache/llama.cpp/unsloth_GLM-4.7-Flash-GGUF_GLM-4.7-Flash-Q8_0.gguf -dev ROCm0/ROCm1 -fa 0/1 -mg 1&lt;/code&gt;&lt;/p&gt; &lt;h1&gt;Vulkan before and after update&lt;/h1&gt; &lt;p&gt;llama.cpp build: f2571df8b (7850)&lt;/p&gt; &lt;h1&gt;Before:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;dev&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Vulkan0/Vulkan1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1852.25 ± 25.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Vulkan0/Vulkan1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;78.28 ± 0.23&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;After:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;threads&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;dev&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Vulkan0/Vulkan1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;2209.46 ± 30.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Vulkan0/Vulkan1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;81.12 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Without FA:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;threads&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;dev&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Vulkan0/Vulkan1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;2551.11 ± 44.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;Vulkan0/Vulkan1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;81.36 ± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Rocm testing for posterity&lt;/h1&gt; &lt;h1&gt;FA On:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;fa&lt;/th&gt; &lt;th align="left"&gt;dev&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;ROCm0/ROCm1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1424.35 ± 20.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;ROCm0/ROCm1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;64.46 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;FA Off:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;main_gpu&lt;/th&gt; &lt;th align="left"&gt;dev&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;ROCm0/ROCm1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;1411.89 ± 19.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;29.65 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;ROCm0/ROCm1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;60.08 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;build: f2571df8b (7850)&lt;/p&gt; &lt;h1&gt;Conclusions&lt;/h1&gt; &lt;p&gt;ROCm still has a ways to go. I'm using the latest &lt;code&gt;TheRock&lt;/code&gt; release (7.11) and was expecting it to come out way ahead, especially across 2 GPU's. Apparently not.&lt;/p&gt; &lt;h1&gt;EDIT&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jpac1ciz62gg1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=799398556ed4dc58f07b8a512807a8d38e4fbc8f"&gt;ROCm is better than Vulkan after 10k tokens&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After some further testing, it looks like ROCm with FA wins over Vulkan after 10k tokens &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SemaMod"&gt; /u/SemaMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp5apn/testing_glm47_flash_multigpu_vulkan_vs_rocm_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp5apn/testing_glm47_flash_multigpu_vulkan_vs_rocm_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp5apn/testing_glm47_flash_multigpu_vulkan_vs_rocm_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T07:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpbcez</id>
    <title>Orchestrating multiple coding agents - what's your setup?</title>
    <updated>2026-01-28T13:28:20+00:00</updated>
    <author>
      <name>/u/seetherealitynow</name>
      <uri>https://old.reddit.com/user/seetherealitynow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on parallelising my AI dev workflow. I'm currently running multiple Claude Code instances, but the coordination is manual and messy. &lt;/p&gt; &lt;p&gt;I'm Interested in how others approach this: &lt;/p&gt; &lt;p&gt;- Containerization/isolation for each agent? &lt;/p&gt; &lt;p&gt;- How do you handle shared context vs isolated workspaces? &lt;/p&gt; &lt;p&gt;- Any orchestration layer you've built or use? &lt;/p&gt; &lt;p&gt;My dream is to treat AI agents like processes - spawn them, give them tasks, monitor status, and handle their &amp;quot;interrupts&amp;quot; (questions/permissions). &lt;/p&gt; &lt;p&gt;Anyone building in this space, or have a setup that works? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seetherealitynow"&gt; /u/seetherealitynow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpbcez/orchestrating_multiple_coding_agents_whats_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpbcez/orchestrating_multiple_coding_agents_whats_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpbcez/orchestrating_multiple_coding_agents_whats_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T13:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpe2zk</id>
    <title>Does anyone want to combine forces to run local SOTA models?</title>
    <updated>2026-01-28T15:16:08+00:00</updated>
    <author>
      <name>/u/ahgroseclose</name>
      <uri>https://old.reddit.com/user/ahgroseclose</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all, &lt;/p&gt; &lt;p&gt;I am long time enterprise IT/datacenter guy, and I have free reign of a datacenter facility here in Ohio. (and free power too, shhh) Ive been doing alot of genAI and MLOps/InferenceOps stuff over the last few years, and over this time I have accumulated a hodge podge of GPUs and high end servers - but I was stuck with 70b models, oss120b, and awq quants etc - because all my GPUs were disparate and in different systems. I want to change that in 2026&lt;/p&gt; &lt;p&gt;I have the capital to buy 2, maybe 3 blackwells right now - but as many of you know 4-6 is the sweet spot for these larger local sota models. &lt;/p&gt; &lt;p&gt;I bought two 4u AI geared Gigabyte servers that can fit 4-5 blackwells in each, and I was wondering if anyone wants to go in on blackwells if we can split the use of their time or collaborate. I have a couple of projects that would really benefit from cheap local inference, so we could even combine forces on my current ventures if its something that is of interest to you.&lt;/p&gt; &lt;p&gt;Maybe you already have some blackwells? Id be glad to host them. I'm hoping to get the b6000 server edition maxq, but workstation would suffice. &lt;/p&gt; &lt;p&gt;I know it would require alot of trust etc, but we can connect and share LinkedIn and have a call or even meet in person. Just throwing this out there in case anyone might be interested!&lt;/p&gt; &lt;p&gt;DM or comment, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ahgroseclose"&gt; /u/ahgroseclose &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpe2zk/does_anyone_want_to_combine_forces_to_run_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpe2zk/does_anyone_want_to_combine_forces_to_run_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpe2zk/does_anyone_want_to_combine_forces_to_run_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:16:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoiep6</id>
    <title>The z-image base is here!</title>
    <updated>2026-01-27T16:21:29+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image"&gt;https://huggingface.co/Tongyi-MAI/Z-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp98mj</id>
    <title>Sharing my set of distilled small language models (3B) + training data in more than 50 low-resource languages</title>
    <updated>2026-01-28T11:49:39+00:00</updated>
    <author>
      <name>/u/Peter-Devine</name>
      <uri>https://old.reddit.com/user/Peter-Devine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp98mj/sharing_my_set_of_distilled_small_language_models/"&gt; &lt;img alt="Sharing my set of distilled small language models (3B) + training data in more than 50 low-resource languages" src="https://preview.redd.it/8hzyyzyqu2gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9df42f096128883200b2f6dfbfae24a14c026b68" title="Sharing my set of distilled small language models (3B) + training data in more than 50 low-resource languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Peter Devine here. You might remember me from such projects as &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1i0vrm5/here_is_our_new_reranker_model_which_we_trained/"&gt;lb-reranker&lt;/a&gt; and &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1cbrgsa/multilingual_llama_3_8b_instruct_from_lightblue/"&gt;Suzume&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I’m sharing Kakugo: a pipeline, set of datasets, and collection of 54 models (3B parameters) I designed to perform general tasks in low-resource languages.&lt;/p&gt; &lt;p&gt;The pipeline only needs the user to specify a language name to create a language model for that language. The pipeline starts with GPT OSS 120B prompted to create instruction/conversation data in the user's target language in 4 ways, and this data is then used to finetune IBM’s Granite 4 Micro (3B), which was the best open source small language model I could find across a wide range of low-resource languages.&lt;/p&gt; &lt;p&gt;The pipeline is completely local and can be run on any rig which can inference GPT OSS 120B and train a 3B model (I used 8x3090). This means greater data sovereignty from data creation to final model production. This is &lt;em&gt;local&lt;/em&gt;llama after all!&lt;/p&gt; &lt;p&gt;The languages I have covered (so far) are:&lt;br /&gt; Amharic, Aranese, Assamese, Asturian, Bashkir, Bengali, Cebuano, Central Kurdish, Chuvash, Eastern Yiddish, Egyptian Arabic, Faroese, Galician, Guarani, Haitian Creole, Hausa, Igbo, Irish, Javanese, Kinyarwanda, Kyrgyz, Lao, Lhasa Tibetan, Luxembourgish, Maltese, Maori, Mizo, Mongolian, Najdi Arabic, Northern Kurdish, Nyanja, Papiamento, Plateau Malagasy, Rundi, Samoan, Scottish Gaelic, Shona, Sindhi (Arabic script), Sinhala, South Azerbaijani, Southern Pashto, Southern Sotho, Sundanese, Swahili, Tajik, Tatar, Telugu, Tigrinya, Turkmen, Uyghur, Welsh, Xhosa, Yoruba, and Zulu&lt;/p&gt; &lt;p&gt;Many base small language models are quite poor at interacting in low resource languages, so my aim with this project was to address that gap to allow communities of low resource languages (e.g. Scottish Gaelic) to use small language model too.&lt;/p&gt; &lt;p&gt;In the future, I would like to try improving the teacher and student models, as well as tinker with the data generation methods to make them better. But these models are hopefully a good first step towards more parity between high and low resource languages in small language models.&lt;/p&gt; &lt;p&gt;I hope you have fun playing with these models, and if you have any feedback on the data or the models in a given language, I would love to hear it!&lt;/p&gt; &lt;p&gt;Also, if there are any other languages that you would like me to develop a model for using this pipeline and add to the collection, just let me know and I will see what I can do.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.14051"&gt;[Paper]&lt;/a&gt; - &lt;a href="https://arxiv.org/abs/2601.14051"&gt;https://arxiv.org/abs/2601.14051&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/collections/ptrdvn/kakugo-models"&gt;[Models]&lt;/a&gt; - &lt;a href="https://hf.co/collections/ptrdvn/kakugo-models"&gt;https://hf.co/collections/ptrdvn/kakugo-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/collections/ptrdvn/kakugo-datasets"&gt;[Datasets]&lt;/a&gt; - &lt;a href="https://hf.co/collections/ptrdvn/kakugo-datasets"&gt;https://hf.co/collections/ptrdvn/kakugo-datasets&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Peter-Devine/kakugo"&gt;[Code]&lt;/a&gt; - &lt;a href="https://github.com/Peter-Devine/kakugo"&gt;https://github.com/Peter-Devine/kakugo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peter-Devine"&gt; /u/Peter-Devine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8hzyyzyqu2gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp98mj/sharing_my_set_of_distilled_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp98mj/sharing_my_set_of_distilled_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T11:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpce5w</id>
    <title>Self hosted Kimi K2.5 (no content) issue</title>
    <updated>2026-01-28T14:11:25+00:00</updated>
    <author>
      <name>/u/pratiknarola</name>
      <uri>https://old.reddit.com/user/pratiknarola</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys,&lt;br /&gt; I am hosting Kimi K2.5 on 8x H200 node.&lt;br /&gt; I am able to get really nice speed and output. but I am having following issues :&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Observing &lt;code&gt;(no content)&lt;/code&gt; in response from the model randomly in between text, reasoning and in tool calls when running the model with vllm. Observed similar behavior while using sglang too where tool calls itself was not working&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;type&amp;quot;:&amp;quot;text&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;(no content)&amp;quot;} {&amp;quot;role&amp;quot;:&amp;quot;assistant&amp;quot;,&amp;quot;content&amp;quot;:[{&amp;quot;type&amp;quot;:&amp;quot;text&amp;quot;,&amp;quot;text&amp;quot;:&amp;quot;(no content)&amp;quot;},{&amp;quot;type&amp;quot;:&amp;quot;tool_use&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;functions.Read:2&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Read&amp;quot;,&amp;quot;input&amp;quot;:{&amp;quot;file_path&amp;quot;:&amp;quot;/Users/pratik.narola/workspace/opencode/README.md&amp;quot;}},{&amp;quot;type&amp;quot;:&amp;quot;tool_use&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;functions.Bash:3&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Bash&amp;quot;,&amp;quot;input&amp;quot;:{&amp;quot;command&amp;quot;:&amp;quot;ls -la /Users/pratik.narola/workspace/opencode/packages&amp;quot;,&amp;quot;description&amp;quot;:&amp;quot;List packages directory structure&amp;quot;}},{&amp;quot;type&amp;quot;:&amp;quot;tool_use&amp;quot;,&amp;quot;id&amp;quot;:&amp;quot;functions.Read:4&amp;quot;,&amp;quot;name&amp;quot;:&amp;quot;Read&amp;quot;,&amp;quot;input&amp;quot;:{&amp;quot;file_path&amp;quot;:&amp;quot;/Users/pratik.narola/workspace/opencode/package.json&amp;quot;},&amp;quot;cache_control&amp;quot;:{&amp;quot;type&amp;quot;:&amp;quot;ephemeral&amp;quot;}}]}, &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;With opencode, Its breaking completely. tool call parser and might be chat template thats breaking. markers like &amp;lt;|tool_calls_section_begin|&amp;gt; &amp;lt;|tool_calls_section_end|&amp;gt; are leaking into the final content output.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am using vllm with&lt;/p&gt; &lt;pre&gt;&lt;code&gt; --tool-call-parser kimi_k2 \ --reasoning-parser kimi_k2 \ &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Please let me know if you have experienced anything like this or has any suggestions or ideas for me to try out.&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pratiknarola"&gt; /u/pratiknarola &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpce5w/self_hosted_kimi_k25_no_content_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpce5w/self_hosted_kimi_k25_no_content_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpce5w/self_hosted_kimi_k25_no_content_issue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp7so2</id>
    <title>bailingmoe - Ling(17B) models' speed is better now</title>
    <updated>2026-01-28T10:29:52+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Useful for some people who uses these models.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o7kkf0/poor_gpu_club_8gb_vram_moe_models_ts_with_llamacpp/"&gt;After 3 months&lt;/a&gt;, I ran llama-bench again for some models &amp;amp; found that Ling models' speed is better than what was 3 months ago. From 30-100+% performance. Big deal IMO with 8GB VRAM + 32GB RAM.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ling-mini-2.0-Q6_K_L - 52 t/s Then&lt;/li&gt; &lt;li&gt;Ling-mini-2.0-Q6_K_L - &lt;strong&gt;97&lt;/strong&gt; t/s &lt;strong&gt;Now&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ling-mini-2.0-IQ4_XS - 75 t/s Then&lt;/li&gt; &lt;li&gt;Ling-mini-2.0-IQ4_XS - &lt;strong&gt;160+&lt;/strong&gt; t/s &lt;strong&gt;Now&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ling-mini-2.0-IQ4_XS - &lt;strong&gt;83&lt;/strong&gt; t/s &lt;strong&gt;Now&lt;/strong&gt; with 32K context&lt;/li&gt; &lt;li&gt;Ling-Coder-lite.i1-IQ4_XS - 69 t/s Then&lt;/li&gt; &lt;li&gt;Ling-Coder-lite.i1-IQ4_XS - &lt;strong&gt;90&lt;/strong&gt; t/s &lt;strong&gt;Now&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Size of IQ4_XS quants of these 2 models are 8.2 GB &amp;amp; 8.5 GB so it won't fit my 8GB VRAM. ~7.5 GB model files could give more better t/s(possibly 200+) without system RAM.&lt;/p&gt; &lt;p&gt;12 or 16 or more GB VRAM users could see massive speed improvements for these models. Also they have other models such Ring(17B), Ling-flash(100B), Ring-flash(100B), Ming .... hopefully they too have similar performance increase now.&lt;/p&gt; &lt;p&gt;Noticed one other thing.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ling-mini-2.0-IQ4_XS - &lt;strong&gt;70&lt;/strong&gt; t/s &lt;strong&gt;CPU-only&lt;/strong&gt; performance(just with 32GB RAM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Used llama-cli &amp;amp; chat for some time with this model &amp;amp; it gave me solid &lt;strong&gt;50+&lt;/strong&gt; t/s just with &lt;strong&gt;CPU-only&lt;/strong&gt; performance.&lt;/p&gt; &lt;p&gt;Grateful to inclusionAI for their 16-17B MOE models which performs better on my 8GB VRAM + RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp7so2/bailingmoe_ling17b_models_speed_is_better_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp7so2/bailingmoe_ling17b_models_speed_is_better_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp7so2/bailingmoe_ling17b_models_speed_is_better_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoml1n</id>
    <title>[LEAKED] Kimi K2.5’s full system prompt + tools (released &lt;24h ago)</title>
    <updated>2026-01-27T18:44:50+00:00</updated>
    <author>
      <name>/u/Pretty_Mountain2714</name>
      <uri>https://old.reddit.com/user/Pretty_Mountain2714</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was messing around with Moonshot's new Kimi K2.5 and pulled the whole system prompt + tools. (~5k tk) &lt;/p&gt; &lt;p&gt;Got hyped I grabbed this so fast cause usually someone posts this stuff way before I get to it&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/dnnyngyen/kimi-k2.5-prompts-tools"&gt; https://github.com/dnnyngyen/kimi-k2.5-prompts-tools &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contents:&lt;br /&gt; -full system prompt&lt;br /&gt; -all tool schemas + instructions&lt;br /&gt; -memory CRUD protocols&lt;br /&gt; -context engineering + assembling user profile&lt;br /&gt; -basic guardrails/rules&lt;br /&gt; -external datasources (finance, arxiv, etc)&lt;/p&gt; &lt;p&gt;After running a couple attempts/verification across 2 different accounts: &lt;a href="https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b"&gt; https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to be able to contribute sum to this community&lt;/p&gt; &lt;p&gt;[EDIT 1]: independent verification of the same prompt posted in CN earlier today: &lt;a href="https://linux.do/t/topic/1523104"&gt;https://linux.do/t/topic/1523104 &lt;/a&gt;&lt;br /&gt; [EDIT 2]: another independent verification just posted:&lt;br /&gt; &lt;a href="https://linux.do/t/topic/1518643"&gt;https://linux.do/t/topic/1518643&lt;/a&gt;&lt;br /&gt; [EDIT 3]: independent verification just posted on &lt;a href="/u/Spiritual_Spell_9469"&gt;u/Spiritual_Spell_9469&lt;/a&gt;'s thread on&lt;a href="https://www.reddit.com/r/ClaudeAIJailbreak/comments/1qoeos7/kimi_k25_jailbroken/"&gt; jailbreaking Kimi K2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pretty_Mountain2714"&gt; /u/Pretty_Mountain2714 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T18:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qorbdk</id>
    <title>Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp; GPU inference. Surprising results.</title>
    <updated>2026-01-27T21:31:27+00:00</updated>
    <author>
      <name>/u/Icy-Measurement8245</name>
      <uri>https://old.reddit.com/user/Icy-Measurement8245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/"&gt; &lt;img alt="Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp;amp; GPU inference. Surprising results." src="https://b.thumbs.redditmedia.com/eWRvxwIb2VPDDptePsLNamF8MJiBRPWZaLITl4Q7jPs.jpg" title="Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp;amp; GPU inference. Surprising results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Me and my team have been building AI workstations for enterprise use and wanted to share some real benchmark data on a dual RTX PRO 6000 Blackwell Max-Q setup (192GB VRAM total) with over 1.15TB of DDR5 RAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Can a $30K-$50K workstation serve a team of 4-50 people or run multiple agents? Tested MiniMax M2.1 native fp8 (GPU+CPU via KTransformers) vs int4 quantized (GPU-only via SGLang). &lt;strong&gt;Key finding: int4 on GPU only is 2-4x faster on prefill but maxes out at ~3 concurrent requests due to KV-cache constraints. Native fp8 scales much better to 10+ users on large contexts but remains slower E2E.&lt;/strong&gt; Full configs and data below. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2x NVIDIA RTX PRO 6000 Max-Q (192GB VRAM total))&lt;/li&gt; &lt;li&gt;AMD EPYC9645 96-core/192-thread &lt;/li&gt; &lt;li&gt;12x DDR5 ECC RDIMM 96GB 5600 Mt/s (1152GB total)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model tested so far:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native fp8 version: MiniMax-M2.1 (&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Quantized version: MiniMax-M2.1-BF16-INT4-AWQ (&lt;a href="https://huggingface.co/mratsim/MiniMax-M2.1-BF16-INT4-AWQ"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to compare two approaches: fp8 precision with CPU offloading vs quantized weights fitting entirely in VRAM.&lt;/p&gt; &lt;h1&gt;Why I’m sharing this&lt;/h1&gt; &lt;p&gt;Most workstation benchmarks show single-user performance with limited context sizes. Given the investment here, I wanted to test if one plug-and-play workstation could actually serve an entire team or multiple simultaneous agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I want to know how many people or agents can use this setup before it degrades too much.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Key metrics: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prefill speed per user (tokens/s/user): Request processing speed&lt;/li&gt; &lt;li&gt;TTFT (Time To First Tokens) (s/request): Time until first output generated&lt;/li&gt; &lt;li&gt;Decode speed per user (tokens/s/request): Generation speed&lt;/li&gt; &lt;li&gt;E2E request time (s/request): Total time from request to completion&lt;/li&gt; &lt;li&gt;Queue time (s/request): Time waiting before processing starts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The priority use case is a coding agent as we would like to run a vibecoding platform 100% locally, hence the choice of MiniMax-M2.1 (more in follow-up posts).&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;p&gt;There are two types of tests for now:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Simple chat&lt;/strong&gt; (~140 tokens input, 300 tokens max output)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large context&lt;/strong&gt; (~64K tokens input, 300 tokens max output)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Key details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Used sglang’s per request metrics logs, in order to properly measure TTFT, prefill and decode speed. &lt;/li&gt; &lt;li&gt;Measured queueing time separately, as it is a good indicator to see if the server starts to be overloaded.&lt;/li&gt; &lt;li&gt;No prefix caching &lt;/li&gt; &lt;li&gt;Tested with 1, 2, 4, 6, 8 and 10 simultaneous users (threads calling the API over and over again)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results: short context (~140 tokens input)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;[see graphs attached]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway:&lt;/strong&gt; The quantized model is running on GPU alone far better than the fp8 model running on CPU and GPU, which was expected.&lt;/p&gt; &lt;p&gt;However the use of the fp8 model is still usable, for up to 2 or 4 simultaneous users (less than 30s processing time). And while the prefill speed with the fp8 model is very low (260 to 110 tokens/s) on short context, it’s important to note the speed increase over larger contexts.&lt;/p&gt; &lt;p&gt;Over a certain input size threshold (about 4k tokens) KTransformer processes the prefill layer-wise, which adds a constant overhead but greatly increases the computation speed by doing all the computation on the GPU, loading and processing one layer at a time, leading to the following results on large contexts.&lt;/p&gt; &lt;h1&gt;Results: Large context (64K tokens)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;[see graphs attached]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Processing 64K tokens with one user takes ~15s for MiniMax-M2.1-INT4 on GPU-only and double that with MiniMax-M2.1 with GPU and CPU offloading.&lt;/p&gt; &lt;p&gt;But here's the thing: INT4 has way less KV-cache available since the model must fit entirely in VRAM. It maxes out at 3 parallel requests. Beyond that, processing speed per request stays flat - requests just pile up in the queue. Queue time explodes and becomes the dominant factor in TTFT and E2E processing.&lt;/p&gt; &lt;p&gt;The results on large contexts are more favorable to the GPU+CPU setup. It's not significantly slower, and the massive KV-cache means real-world usage would see a lot of cache-hit in real usage, furthermore improving processing speed. However, the decode rate remains low (8 to 3 tokens/s for 4 to 10 simultaneous users), so for long generation tasks it may be of limited use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key message. Do not underestimate queue time, it becomes an essential element of bottleneck. Moreover, recompute of prefill can be costly and grow over time.&lt;/strong&gt; &lt;/p&gt; &lt;h1&gt;SGLang and KTransformers were used for GPU and CPU offloading with MiniMax-M2.1&lt;/h1&gt; &lt;p&gt;At first, I started experimenting with llama.cpp, which worked okay with CPU offloading but didn’t scale well with several simultaneous users. In addition, no optimisation is done for long inputs. I then switched to KTransformers, which supports layer-wise prefill with CPU offloading, which works great for long inputs. It’s based on SGLang and also runs great for simultaneous users. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;KTransformers configuration, highly biased toward kv-cache size:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kt run --enable-shared-experts-fusion \ --cpu-threads 96 \ --chunked-prefill-size 60000 \ --model-path /fast-data/ktransformer/MinimaxM2.1/ \ --max-total-tokens 600000 \ --gpu-experts 20 \ -p 8000 MiniMax-M2.1 \ --mem-fraction-static 0.85 \ --max-running-requests 12 \ --max-prefill-tokens 80000 \ --export-metrics-to-file \ --enable-metrics \ --export-metrics-to-file-dir ./metrics/ \ --enable-request-time-stats-logging \ --enable-cache-report &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SGLang config:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 -m sglang.launch_server \ --host 127.0.0.1 \ --port &amp;quot;8000&amp;quot; \ --sleep-on-idle \ --disable-custom-all-reduce \ --max-running-requests 16 \ --cuda-graph-max-bs 16 \ --attention-backend flashinfer \ --served-model-name &amp;quot;MiniMax-M2.1&amp;quot; \ --model-path &amp;quot;mratsim/MiniMax-M2.1-BF16-INT4-AWQ&amp;quot; \ --tool-call-parser minimax-m2 \ --reasoning-parser minimax \ --trust-remote-code \ --export-metrics-to-file \ --enable-metrics \ --export-metrics-to-file-dir ./metrics/ \ --enable-request-time-stats-logging \ --enable-cache-report \ --tp 2 \ --mem-fraction-static 0.93 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's next&lt;/h1&gt; &lt;p&gt;I want to extend the tests to larger workloads and context. My next test is to run coding agents using Claude Code in parallel on real coding tasks in “Ralph” mode. I will continue comparing MiniMax-M2.1 and MiniMax-M2.1-INT4. I am also in the process of testing other models: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-235B-A22B&lt;/li&gt; &lt;li&gt;GPT-OSS 120B &lt;/li&gt; &lt;li&gt;DeepSeek V3.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to run specific tests if there's interest. Also curious if anyone else has multi-user scaling data on similar hardware. &lt;/p&gt; &lt;p&gt;&lt;em&gt;We're a small team deploying local AI agents and setting up private infrastructures. If you have questions about the setup or want us to test something specific, drop a comment.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Measurement8245"&gt; /u/Icy-Measurement8245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qorbdk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T21:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpcdjg</id>
    <title>Should data centers be required to include emergency shutdown mechanisms as we have with nuclear power plants?</title>
    <updated>2026-01-28T14:10:43+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpcdjg/should_data_centers_be_required_to_include/"&gt; &lt;img alt="Should data centers be required to include emergency shutdown mechanisms as we have with nuclear power plants?" src="https://external-preview.redd.it/dnQ2eHBuOWNsM2dnMbsCN6d3L6_HXnnRFCSSTAWqyWY23lOHKBvxCRMllsYF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c21745e2d5cd89c46aea64639f6fc8d263fdf360" title="Should data centers be required to include emergency shutdown mechanisms as we have with nuclear power plants?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xvo60r3cl3gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpcdjg/should_data_centers_be_required_to_include/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpcdjg/should_data_centers_be_required_to_include/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:10:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpfse6</id>
    <title>Run Kimi K2.5 Locally</title>
    <updated>2026-01-28T16:17:45+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt; &lt;img alt="Run Kimi K2.5 Locally" src="https://preview.redd.it/rxqfj5os74gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2606f30079a77f14bb28c31413be651c092abaa9" title="Run Kimi K2.5 Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-K2.5 achieves SOTA performance in vision, coding, agentic and chat tasks. &lt;/p&gt; &lt;p&gt;The 1T parameter hybrid reasoning model requires 600GB of disk space, while the quantized &lt;strong&gt;Unsloth Dynamic 1.8-bit&lt;/strong&gt; version reduces this to &lt;strong&gt;240GB (-60% size).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Kimi-K2.5-GGUF"&gt;&lt;strong&gt;Kimi-K2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide:&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/kimi-k2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/kimi-k2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rxqfj5os74gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T16:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpdn1t</id>
    <title>FASHN VTON v1.5: Apache-2.0 virtual try-on model, runs on consumer GPUs (~8GB VRAM), ~1B params</title>
    <updated>2026-01-28T14:59:46+00:00</updated>
    <author>
      <name>/u/JYP_Scouter</name>
      <uri>https://old.reddit.com/user/JYP_Scouter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpdn1t/fashn_vton_v15_apache20_virtual_tryon_model_runs/"&gt; &lt;img alt="FASHN VTON v1.5: Apache-2.0 virtual try-on model, runs on consumer GPUs (~8GB VRAM), ~1B params" src="https://external-preview.redd.it/dzB2eTU3dTB0M2dnMQAcCp4XjN-WlzBmxKbhH4j6EHXSl-nuFCETM3brGZzN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa21a6beabb7206bc0fa0f9980387312dbb7c8ce" title="FASHN VTON v1.5: Apache-2.0 virtual try-on model, runs on consumer GPUs (~8GB VRAM), ~1B params" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just open-sourced FASHN VTON v1.5, a virtual try-on model that generates photorealistic images of people wearing garments. We've been running this as a production API for the past year, and now we're releasing the weights and inference code under Apache-2.0.&lt;/p&gt; &lt;h1&gt;Why we're releasing this&lt;/h1&gt; &lt;p&gt;Most open-source VTON models are either research prototypes that require significant engineering to deploy, or they're locked behind restrictive licenses. As state-of-the-art capabilities consolidate into massive generalist models, we think there's value in releasing focused, efficient models that researchers and developers can actually own, study, and extend commercially.&lt;/p&gt; &lt;p&gt;We also want to demonstrate that competitive results in this domain don't require massive compute budgets. Total training cost was in the $5-10k range on rented A100s.&lt;/p&gt; &lt;p&gt;This follows our &lt;a href="https://www.reddit.com/r/MachineLearning/comments/1qax221/p_opensourcing_a_human_parsing_model_trained_on/"&gt;human parser release&lt;/a&gt; from a couple weeks ago.&lt;/p&gt; &lt;h1&gt;Specs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 972M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Custom MMDiT&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM:&lt;/strong&gt; ~8GB minimum&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Runs on consumer GPUs (RTX 30xx/40xx)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; ~5 seconds on H100&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; Apache-2.0 (fully permissive, commercial use allowed)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Technical highlights&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Pixel-space operation:&lt;/strong&gt; Unlike most diffusion models that work in VAE latent space, we operate directly on RGB pixels. This avoids lossy VAE encoding/decoding that can blur fine garment details like textures, patterns, and text.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Maskless inference:&lt;/strong&gt; No segmentation mask required on the target person. The model learns where clothing boundaries should be rather than being told.&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/fashn-AI/fashn-vton-1.5"&gt;fashn-AI/fashn-vton-1.5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/fashn-ai/fashn-vton-1.5"&gt;fashn-ai/fashn-vton-1.5&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Project page:&lt;/strong&gt; &lt;a href="https://fashn.ai/research/vton-1-5"&gt;fashn.ai/research/vton-1-5&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick example&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from fashn_vton import TryOnPipeline from PIL import Image pipeline = TryOnPipeline(weights_dir=&amp;quot;./weights&amp;quot;) person = Image.open(&amp;quot;person.jpg&amp;quot;).convert(&amp;quot;RGB&amp;quot;) garment = Image.open(&amp;quot;garment.jpg&amp;quot;).convert(&amp;quot;RGB&amp;quot;) result = pipeline( person_image=person, garment_image=garment, category=&amp;quot;tops&amp;quot;, ) result.images[0].save(&amp;quot;output.png&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Coming soon&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Space:&lt;/strong&gt; Online demo&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Technical paper:&lt;/strong&gt; Architecture decisions, training methodology, and design rationale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about running this locally or the implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JYP_Scouter"&gt; /u/JYP_Scouter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x689lnt0t3gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpdn1t/fashn_vton_v15_apache20_virtual_tryon_model_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpdn1t/fashn_vton_v15_apache20_virtual_tryon_model_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qouf0x</id>
    <title>Arcee AI releases Trinity Large : OpenWeight 400B-A13B</title>
    <updated>2026-01-27T23:28:47+00:00</updated>
    <author>
      <name>/u/abkibaarnsit</name>
      <uri>https://old.reddit.com/user/abkibaarnsit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"&gt; &lt;img alt="Arcee AI releases Trinity Large : OpenWeight 400B-A13B" src="https://external-preview.redd.it/kpKiKke1xSzMMszPwBcvRHFEWu1KcRJDoXwrfinT_zM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=409d9b6883693b08a64db17e15a4f65b6ef32a87" title="Arcee AI releases Trinity Large : OpenWeight 400B-A13B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abkibaarnsit"&gt; /u/abkibaarnsit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arcee.ai/blog/trinity-large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qou799</id>
    <title>Stanford Proves Parallel Coding Agents are a Scam</title>
    <updated>2026-01-27T23:20:18+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"&gt; &lt;img alt="Stanford Proves Parallel Coding Agents are a Scam" src="https://a.thumbs.redditmedia.com/bSPz9zEUo0BXRhW42ijPdUhwVHlYft5MVuPWRmgiAK0.jpg" title="Stanford Proves Parallel Coding Agents are a Scam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/coxs8w3z3zfg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0875df6bf260ca3af0f9fe7eef7bbd3697a0c73"&gt;https://preview.redd.it/coxs8w3z3zfg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0875df6bf260ca3af0f9fe7eef7bbd3697a0c73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;A fascinating new &lt;a href="https://cooperbench.com/static/pdfs/main.pdf"&gt;preprint&lt;/a&gt; from Stanford and SAP drops a truth bomb that completely upends the &amp;quot;parallel coordinated coding&amp;quot; &amp;quot;productivity boost&amp;quot; assumption for AI coding agents.&lt;/p&gt; &lt;p&gt;Their &amp;quot;CooperBench&amp;quot; reveals what they call the &amp;quot;curse of coordination.&amp;quot; When you add a second coding agent, performance doesn't just fail to improve - it plummets. On average, two agents working together have a 30% lower success rate. For top models like GPT-5 and Claude 4.5 Sonnet, the success rate is a staggering 50% lower than just using one agent to do the whole job.&lt;/p&gt; &lt;p&gt;Why? The agents are terrible teammates. They fail to model what their partner is doing (42% of failures), don't follow through on commitments (32%), and have communication breakdowns (26%). They hallucinate shared states and silently overwrite each other's work.&lt;/p&gt; &lt;p&gt;This brings me to the elephant in the room. Platforms like Cursor, Antigravity, and others are increasingly marketing &amp;quot;parallel agent&amp;quot; features as a productivity revolution. But if foundational research shows this approach is fundamentally broken and makes you less productive, what are they actually selling? It feels like they're monetizing a feature they might know is a scam, &amp;quot;persuading&amp;quot; users into thinking they're getting a 10x team when they're really getting a mess of conflicting code.&lt;/p&gt; &lt;p&gt;As the Stanford authors put it, it's &amp;quot;hard to imagine how an agent incapable of coordination would contribute to such a future however strong the individual capabilities.&amp;quot; Food for thought next time you see a &amp;quot;parallel-agent&amp;quot; feature advertised.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qos25i</id>
    <title>Kimi K2 Artificial Analysis Score</title>
    <updated>2026-01-27T21:58:50+00:00</updated>
    <author>
      <name>/u/Virenz</name>
      <uri>https://old.reddit.com/user/Virenz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"&gt; &lt;img alt="Kimi K2 Artificial Analysis Score" src="https://preview.redd.it/0xqbgnt0syfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e64564291959092586bed5a52ce15a55c2fad64c" title="Kimi K2 Artificial Analysis Score" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/i/status/2016250137115557953"&gt;https://x.com/i/status/2016250137115557953&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virenz"&gt; /u/Virenz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0xqbgnt0syfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T21:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp4ftj</id>
    <title>I made a Coding Eval, and ran it against 49 different coding agent/model combinations, including Kimi K2.5.</title>
    <updated>2026-01-28T07:08:34+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may remember me from my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o77ag4/a_guide_to_the_best_agentic_tools_and_the_best/"&gt;A guide to the best agentic tools and the best way to use them on the cheap, locally or free&lt;/a&gt; post from 3 months ago. Where I submitted a big wall of text at 4 am in stream of consciousness format. For some reason, I still get random replies on it, about not putting enough effort in to format it. Well I'm back, and this time I've written my own benchmarking tool for evaluating coding agent/model ability, and ran it against (as of writing) 49 different coding agent and model combinations. Are you guys entertained now?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Coding Eval - SanityHarness&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is my purpose-made coding eval, that I wanted to be agent-agnostic as possible to use (I've run a lot of other coding evals and some of them are a pain in the butt to get working with many agents). I carefully curated and put together tasks across 6 different languages, specifically focusing on problems for measuring model understanding and agent capability rather than training data regurgitation. If you're interested in the implementation or want to run it yourself, check it out on &lt;a href="https://github.com/lemon07r/SanityHarness"&gt;GitHub | lemon07r/SanityHarness&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Coding Agent Leaderboard - SanityBoard&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Now, for the part you’re probably most interested in, and where I invested too many hours: &lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt; (source available on GH &lt;a href="https://github.com/lemon07r/SanityBoard"&gt;here&lt;/a&gt;). There are currently 49 entries, and &lt;strong&gt;many&lt;/strong&gt; more still being added. I tried to provide as much relevant data as possible, and present it in an easy to digest format with sort/filter controls and report pages with the full run data. This includes run dates, agent version numbers, etc, things that I feel are important but often left out in some leaderboards.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join the Discord Server! Also consider giving my GH repos a star&lt;/strong&gt; ☆&lt;/p&gt; &lt;p&gt;Consider leaving a star in my github repos, as I did put in a lot of work in these projects, and will continue doing so. If any of you would like to see a specific agent or model tested (or retested), need any help running the eval, or have any other questions about the eval or leaderboard consider joining my &lt;a href="https://discord.gg/rXNQXCTWDt"&gt;Discord&lt;/a&gt; server (I am looking for more peeps to discuss ai and coding related topics with!)&lt;/p&gt; &lt;h1&gt;Some Extra Stuff, and Future Plans&lt;/h1&gt; &lt;p&gt;This post started out as another big block of text, but I've decided to spare you guys and re-wrote most of it to separate all the extra stuff as optional reading below. This includes some usage cost analysis' and some pretty cool stuff I have planned for the future.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MCP Server Evals&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For one, you might have noticed an &amp;quot;MCP&amp;quot; column on my leaderboard. That's right, I will eventually do runs with MCP tools enabled, but before this I have something even cooler planned. I'm going to be testing different MCP tools to see which ones make any difference (if it all), and which MCP tools are the best in their respective categories (web search, code indexing + semantic retrieval, etc), then afterwards, the best MCP combinations. I will be testing all of these in my evals; the goal is to figure what MCP tools and tool combination is best, and to see which ones might even negatively impact coding ability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent Skills&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also going to do evals against different skills files to see if they actually help and which ones are best (these are obviously very project/task dependant but I hope we can still figure out some good blanket-use ones).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;More Agents and Models to Test&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There will be more coding agents tested. And models. Oh-My-Opencode is on my radar, I want to try testing a few different configurations to see if it's actually any better than vanilla opencode, or if it's all smoke and mirrors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Usage, Cost and why Some Agents Were Left Off&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;AI credit plans suck. The coding agents that only support these monetization models are horrible. They wont support BYOK for a reason; they know their monetization models are downright horrendous and predatory. I was able to confirm this while monitoring the usage of some of my runs. Some agents that didn't make the cut because of this include Warp, Letta Code and Codebuff. Seriously, just support BYOK. Or at least have a decent value plan or free usage.&lt;/p&gt; &lt;p&gt;Here is a good example of how horrible some of these guys can be. Codebuff. 100 credits = $1. When I ran my tests against codebuff, my eval got through ONLY 9 of my 26 tasks, burning through $7.5 worth of credits. They even advertise how they use 30% less tokens than claude code or something like that. So you're telling me with codebuff you get to spend more money to use less tokens? I cannot explain how terrible this is. Maybe you'll have an idea of how bad it is, when you see below how much usage other plans or providers will give you (yes even AMP free, gives you more usage daily than you get from two months of free Codebuff credits).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;AMP Smart Mode (mixed) - $6.53&lt;/li&gt; &lt;li&gt;AMP Rush Mode (mixed) - $3.8~&lt;/li&gt; &lt;li&gt;Copilot CLI GPT 5.2 High - 26 Premium Requests (basically $0.86 on pro plan)&lt;/li&gt; &lt;li&gt;Copilot CLI Opus - 78 Premium Requests (expensive, no reasoning or gimped somehow, use something else)&lt;/li&gt; &lt;li&gt;Codex GPT 5.2-Codex xhigh - 65% of daily, 20% of weekly (business seat)&lt;/li&gt; &lt;li&gt;Codex GPT 5.2 xhigh - 100% of daily, 30% of weekly (business seat)&lt;/li&gt; &lt;li&gt;Factory Gemini 3 Flash High - 1m tokens (these are all &amp;quot;Factory&amp;quot; tokens, 1m = $1)&lt;/li&gt; &lt;li&gt;Factory GLM 4.7 High - 0.7m tokens&lt;/li&gt; &lt;li&gt;Factory K2.5 - 0.8m tokens&lt;/li&gt; &lt;li&gt;Factory Gemini 3 Pro High - 2m tokens&lt;/li&gt; &lt;li&gt;Factory GPT 5.2 Codex xhigh - 2m tokens&lt;/li&gt; &lt;li&gt;Factory GPT 5.1 Codex Max xhigh - 2m tokens&lt;/li&gt; &lt;li&gt;Factory GPT 5.2 xhigh - 2.4m tokens&lt;/li&gt; &lt;li&gt;Factory Opus 4.5 High - 3m tokens&lt;/li&gt; &lt;li&gt;Kim For Coding Plan (K2.5) - Around 120-130 Req each run on OpenCode, Claude Code and Kimi CLI (with 2k weekly limit on $19 plan, this is essentially $0.30 a run).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;API Credits, Keys, And Integrity&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm accepting API credits/keys for testing more models and agents otherwise I will be limited to what I have access to currently (DM me). If you are an official provider for your model/agent, or have your own coding agent, feel free to reach out to me to get your stuff on my leaderboard.&lt;/p&gt; &lt;p&gt;Full disclosure, I do not do any manipulation of any kind and try to keep things completely fair, bias free, etc. Droid did provide me extra usage to run my evals, and Minimax has provided me a Coding Max Plan, but as you can see from my leaderboard that will not save some of them from having poor results.&lt;/p&gt; &lt;p&gt;I keep all my runs and can provide the entirety of them on request if anyone wants to see them for improving their model, agent or to see how valid my runs are (I do thoroughly check each of them for issues and have done complete reruns of every model and agent when I found any issues that needed fixing).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future Updated Model and Agent Guide&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I am going to make a revised and updated guide soon. This will cover the best coding models and agents, covering various different grounds, like best open weight models, best open source agents, best free tier setups (including both open and closed options), and best value/bang for your buck setups. I will provide some actual analysis on my coding eval results and other data, including some behind the scenes stuff and experience, or other knowledge I've gathered from talking to experienced people in the field. There are a lot of insights and things to be gathered from outside evals and leaderboards, these results don't tell the full story.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp4ftj/i_made_a_coding_eval_and_ran_it_against_49/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T07:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp814d</id>
    <title>Sam Altman Says OpenAI Is Slashing Its Hiring Pace as Financial Crunch Tightens</title>
    <updated>2026-01-28T10:43:40+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp814d/sam_altman_says_openai_is_slashing_its_hiring/"&gt; &lt;img alt="Sam Altman Says OpenAI Is Slashing Its Hiring Pace as Financial Crunch Tightens" src="https://external-preview.redd.it/js5rUnxnssRKiIPCHK8jH1gE4HhRk8kF6Ui2iEIglL4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=00c0125d100a3eaf17a548840fc4935c5b5cac3c" title="Sam Altman Says OpenAI Is Slashing Its Hiring Pace as Financial Crunch Tightens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a livestreamed town hall, Sam Altman admitted OpenAI is 'dramatically slowing down' hiring as the company faces increasing financial pressure. This follows reports of an internal 'Code Red' memo urging staff to fix ChatGPT as competitors gain ground. With analysts warning of an 'Enron-like' cash crunch within 18 months and the company resorting to ads for revenue, the era of unlimited AI spending appears to be hitting a wall.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://futurism.com/artificial-intelligence/sam-altman-openai-slashing-hiring"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp814d/sam_altman_says_openai_is_slashing_its_hiring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp814d/sam_altman_says_openai_is_slashing_its_hiring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoty38</id>
    <title>Kimi K2.5 costs almost 10% of what Opus costs at a similar performance</title>
    <updated>2026-01-27T23:10:16+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"&gt; &lt;img alt="Kimi K2.5 costs almost 10% of what Opus costs at a similar performance" src="https://preview.redd.it/xz7okply3zfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74631e6bf621619eca977768f8c7287dc6164e45" title="Kimi K2.5 costs almost 10% of what Opus costs at a similar performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying out Kimi k2.5 and this is the first time that I feel an open model is truly competitive with SOTA closed models.&lt;/p&gt; &lt;p&gt;Compared to GLM, Kimi is a bit better, specially when it comes to non-website tasks.&lt;/p&gt; &lt;p&gt;Have you tried it? What's your take?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xz7okply3zfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpde5g</id>
    <title>Backup those models, because of calls for regulations</title>
    <updated>2026-01-28T14:50:06+00:00</updated>
    <author>
      <name>/u/ProfessionalSpend589</name>
      <uri>https://old.reddit.com/user/ProfessionalSpend589</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.euronews.com/next/2026/01/28/humanity-needs-to-wake-up-to-ai-threats-anthropic-ceo-says"&gt;‘Humanity needs to wake up’ to AI threats, Anthropic CEO says&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;gt; Dario Amodei, the CEO of Anthropic, says that humanity needs to regulate the use of AI,…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalSpend589"&gt; /u/ProfessionalSpend589 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpde5g/backup_those_models_because_of_calls_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpde5g/backup_those_models_because_of_calls_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpde5g/backup_those_models_because_of_calls_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T14:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp6rm5</id>
    <title>API pricing is in freefall. What's the actual case for running local now beyond privacy?</title>
    <updated>2026-01-28T09:27:55+00:00</updated>
    <author>
      <name>/u/Distinct-Expression2</name>
      <uri>https://old.reddit.com/user/Distinct-Expression2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;K2.5 just dropped at roughly 10% of Opus pricing with competitive benchmarks. Deepseek is practically free. Gemini has a massive free tier. Every month the API cost floor drops another 50%.&lt;/p&gt; &lt;p&gt;Meanwhile running a 70B locally still means either a k+ GPU or dealing with quantization tradeoffs and 15 tok/s on consumer hardware.&lt;/p&gt; &lt;p&gt;I've been running local for about a year now and I'm genuinely starting to question the math. The three arguments I keep hearing:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt; — legit, no argument. If you're processing sensitive data, local is the only option.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No rate limits&lt;/strong&gt; — fair, but most providers have pretty generous limits now unless you're doing something unusual.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;It's free after hardware costs&amp;quot;&lt;/strong&gt; — this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The argument I never hear but actually find compelling: &lt;strong&gt;latency control and customization&lt;/strong&gt;. If you need a fine-tuned model for a specific domain with predictable latency, local still wins. But that's a pretty niche use case.&lt;/p&gt; &lt;p&gt;What's keeping you all running local at this point? Genuinely curious if I'm missing something or if the calculus has actually shifted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Expression2"&gt; /u/Distinct-Expression2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T09:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp87tk</id>
    <title>Kimi K2.5 is the best open model for coding</title>
    <updated>2026-01-28T10:54:13+00:00</updated>
    <author>
      <name>/u/npc_gooner</name>
      <uri>https://old.reddit.com/user/npc_gooner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt; &lt;img alt="Kimi K2.5 is the best open model for coding" src="https://preview.redd.it/unxlhercm2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23f59fb1153598db9b9c8a3b5ce9067435dfba28" title="Kimi K2.5 is the best open model for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;they really cooked&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npc_gooner"&gt; /u/npc_gooner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/unxlhercm2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; 👋&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AM–11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;⚠️ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don’t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
