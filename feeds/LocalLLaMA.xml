<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-21T09:23:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nltfwx</id>
    <title>AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right.</title>
    <updated>2025-09-20T09:12:06+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"&gt; &lt;img alt="AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right." src="https://external-preview.redd.it/MHdnZnppa2VkYXFmMfboDEJV_8E07yibCTC4f2dErk0sK7LfErgP63h2qGj9.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=322dc704f600fd94bc23fadf748b6bdace23594e" title="AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kdoptgkedaqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm31u5</id>
    <title>What's the next model you are really excited to see?</title>
    <updated>2025-09-20T16:43:21+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have had so many new models in the last few months I have lost track on what is to come. What's the next model you are really excited to see coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm31u5/whats_the_next_model_you_are_really_excited_to_see/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm31u5/whats_the_next_model_you_are_really_excited_to_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm31u5/whats_the_next_model_you_are_really_excited_to_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T16:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmijrp</id>
    <title>TTD-DR, a framework that uses a Deep Research agent to draft and revise its own drafts using high-quality retrieved information. This approach achieves new state-of-the-art results in writing long-form research reports and completing complex reasoning tasks.</title>
    <updated>2025-09-21T04:23:26+00:00</updated>
    <author>
      <name>/u/AdFluffy920</name>
      <uri>https://old.reddit.com/user/AdFluffy920</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmijrp/ttddr_a_framework_that_uses_a_deep_research_agent/"&gt; &lt;img alt="TTD-DR, a framework that uses a Deep Research agent to draft and revise its own drafts using high-quality retrieved information. This approach achieves new state-of-the-art results in writing long-form research reports and completing complex reasoning tasks." src="https://b.thumbs.redditmedia.com/jgYAYA9qAXadrG6tUUcUVRSRNrvE8pMWcHpVmqX46vo.jpg" title="TTD-DR, a framework that uses a Deep Research agent to draft and revise its own drafts using high-quality retrieved information. This approach achieves new state-of-the-art results in writing long-form research reports and completing complex reasoning tasks." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.google/blog/deep-researcher-with-test-time-diffusion/"&gt;https://research.google/blog/deep-researcher-with-test-time-diffusion/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2507.16075"&gt;https://arxiv.org/abs/2507.16075&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdFluffy920"&gt; /u/AdFluffy920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nmijrp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmijrp/ttddr_a_framework_that_uses_a_deep_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmijrp/ttddr_a_framework_that_uses_a_deep_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:23:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlkwr3</id>
    <title>OpenWebUI is the most bloated piece of s**t on earth, not only that but it's not even truly open source anymore, now it just pretends it is because you can't remove their branding from a single part of their UI. Suggestions for new front end?</title>
    <updated>2025-09-20T01:08:06+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly, I'm better off straight up using SillyTavern, I can even have some fun with a cute anime girl as my assistant helping me code or goof off instead of whatever dumb stuff they're pulling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T01:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlufzx</id>
    <title>llama.ui: new updates!</title>
    <updated>2025-09-20T10:14:45+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt; &lt;img alt="llama.ui: new updates!" src="https://preview.redd.it/mjwmirusoaqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b3160b178b7387b2186e3b81e5c0b04c1d83fe5" title="llama.ui: new updates!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to announce an update to &lt;strong&gt;llama.ui&lt;/strong&gt;, a privacy focused web interface for interacting with Large Language Models! We bring some awesome new features and performance improvements: - Configuration Presets: Save and load your favorite configurations for different models and use cases. - Text-to-Speech: Listen to the AI's responses! Supports multiple voices and languages. - Database Export/Import: Backup your chat history or transfer to a new device! - Conversation Branching: Experiment with different paths in your conversations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mjwmirusoaqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T10:14:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmn66e</id>
    <title>Running LLMs locally with iGPU or CPU not dGPU (keep off plz lol)? Post t/s</title>
    <updated>2025-09-21T09:05:37+00:00</updated>
    <author>
      <name>/u/General-Cookie6794</name>
      <uri>https://old.reddit.com/user/General-Cookie6794</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thread may help a middle to low rage laptop buyer make a decision. Any hardware is welcomed weather new or old, snapdragon elite, Intel, AMD. Not for Dedicated GPU users. &lt;/p&gt; &lt;p&gt;Post your hardware(laptop type ram size and speed if possible, CPU type), AI model and if using lmstudio or ollama we want to see token generation in t/s. Prefil tokens is optional. Some clips maybe useful.&lt;/p&gt; &lt;p&gt;Let's go &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/General-Cookie6794"&gt; /u/General-Cookie6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmn66e/running_llms_locally_with_igpu_or_cpu_not_dgpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmn66e/running_llms_locally_with_igpu_or_cpu_not_dgpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmn66e/running_llms_locally_with_igpu_or_cpu_not_dgpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T09:05:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmf0hw</id>
    <title>New E-commerce encoders in town: RexBERT</title>
    <updated>2025-09-21T01:16:54+00:00</updated>
    <author>
      <name>/u/Minute_Smile5698</name>
      <uri>https://old.reddit.com/user/Minute_Smile5698</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF blog published: &lt;a href="https://huggingface.co/blog/thebajajra/rexbert-encoders"&gt;https://huggingface.co/blog/thebajajra/rexbert-encoders&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Outperforms ModernBERT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minute_Smile5698"&gt; /u/Minute_Smile5698 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4b0q</id>
    <title>Efficient 4B parameter gpt OSS distillation without the over-censorship</title>
    <updated>2025-09-20T17:32:58+00:00</updated>
    <author>
      <name>/u/ApprehensiveTart3158</name>
      <uri>https://old.reddit.com/user/ApprehensiveTart3158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've personally loved using gpt oss, but it wasn't very fast locally and was totally over censored. &lt;/p&gt; &lt;p&gt;So I've thought about it and made a fine tune of qwen3 4B thinking on GPT OSS outputs, with MOST of the &amp;quot;I can't comply with that&amp;quot; removed from the fine tuning dataset. &lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://huggingface.co/Pinkstack/DistilGPT-OSS-qwen3-4B"&gt;https://huggingface.co/Pinkstack/DistilGPT-OSS-qwen3-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yes, it is small and no it cannot be properly used for speculative decoding but it is pretty cool to play around with and it is very fast. &lt;/p&gt; &lt;p&gt;From my personal testing (note, not benchmarked yet as that does take quite a bit of compute that I don't have right now): Reasoning efforts (low, high, medium) all works as intended and absolutely do change how long the model thinks which is huge. It thinks almost exactly like gpt oss and yes it does think about &amp;quot;policies&amp;quot; but from what I've seen with high reasoning it may start thinking about rejecting then convince itself to answer.. Lol(for example if you ask it to let's say swear at you, it would most of the time comply), unless what you asked is really unsafe it would probably comply, and it feels exactly like gpt oss, same style of code, almost identical output styles just not as much general knowledge as it is just 4b parameters!!&lt;/p&gt; &lt;p&gt;If you have questions or want to share something please comment and let me know, would live to hear what you think! :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveTart3158"&gt; /u/ApprehensiveTart3158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T17:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmm22k</id>
    <title>Is it possible to run AI coding tools off strong server CPUs?</title>
    <updated>2025-09-21T07:56:19+00:00</updated>
    <author>
      <name>/u/inevitabledeath3</name>
      <uri>https://old.reddit.com/user/inevitabledeath3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have at my university some servers with dual Xeon Gold 6326 CPUs and 1 TB of RAM.&lt;/p&gt; &lt;p&gt;Is it practical in any way to run an automated coding tool off of something like this? It's for my PhD project on using LLMs in cybersecurity education. I am trying to get a system that can generate things like insecure software and malware for students to analyze.&lt;/p&gt; &lt;p&gt;If I can use SGLang or VLLM with prompt caching is this practical? Likely I can setup the system to generate in parallel as there will be dozens of VMs being generated in the same run. From what I understand having parallel requests increases aggregate throughput. Waiting a few hours for a response is not a big issue, though I know AI coding tools have annoying timeout limitations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inevitabledeath3"&gt; /u/inevitabledeath3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm22k/is_it_possible_to_run_ai_coding_tools_off_strong/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm22k/is_it_possible_to_run_ai_coding_tools_off_strong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm22k/is_it_possible_to_run_ai_coding_tools_off_strong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T07:56:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmm9hy</id>
    <title>Best way to benchmark offline LLMs?</title>
    <updated>2025-09-21T08:08:56+00:00</updated>
    <author>
      <name>/u/YT_Brian</name>
      <uri>https://old.reddit.com/user/YT_Brian</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering if anyone had a favorite way to test your PC for benchmarking, specific LLM you use just for that or prompt, that type of thing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YT_Brian"&gt; /u/YT_Brian &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm9hy/best_way_to_benchmark_offline_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm9hy/best_way_to_benchmark_offline_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmm9hy/best_way_to_benchmark_offline_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T08:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmlluu</id>
    <title>Mini-PC Dilemma: 96GB vs 128GB. How Much RAM is it worth buying?</title>
    <updated>2025-09-21T07:27:46+00:00</updated>
    <author>
      <name>/u/Dull-Breadfruit-3241</name>
      <uri>https://old.reddit.com/user/Dull-Breadfruit-3241</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I'm planning to pick up one of the new mini-PCs powered by the AMD Ryzen AI Max+ 395 CPU,specifically the Bosgame M5. The 96GB RAM model looks more cost-effective, but I'm weighing whether it's worth spending ~15% more for the 128GB version.&lt;/p&gt; &lt;p&gt;From what I understand, the 96GB config allows up to 64GB to be allocated to the integrated GPU, while the 128GB model can push that up to 96GB. That extra memory could make a difference on whether be able to run larger LLMs.&lt;/p&gt; &lt;p&gt;So hereâ€™s my question: will larger models that fit thanks to the extra memory actually run at decent speeds? Will I miss out on larger better models that would still run at decent speed on this machine by choosing the model that can allocate only 64GB of RAM to the GPU?&lt;/p&gt; &lt;p&gt;My goal is to experiment with LLMs and other AI projects locally, and Iâ€™d love to hear from anyone whoâ€™s tested similar setups or has insight into how well these systems scale with RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dull-Breadfruit-3241"&gt; /u/Dull-Breadfruit-3241 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmlluu/minipc_dilemma_96gb_vs_128gb_how_much_ram_is_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T07:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm9uye</id>
    <title>My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)</title>
    <updated>2025-09-20T21:17:42+00:00</updated>
    <author>
      <name>/u/picturpoet</name>
      <uri>https://old.reddit.com/user/picturpoet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"&gt; &lt;img alt="My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)" src="https://preview.redd.it/0dhjzmgzydqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79b9409fbebd0a546d3ef854d3b29ce2460c94e" title="My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Mac Studio M4 Max base model just came through and I was so excited to run something locally having always depended on cloud based models.&lt;/p&gt; &lt;p&gt;I don't know what use cases I will build yet but just so exciting that there's a new fun model available to try the moment I began.&lt;/p&gt; &lt;p&gt;Any ideas of what I should do next on my Local Llama roadmap and how I can get to being an intermediate localllm user from my current noob status is fully appreciated. ðŸ˜„&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/picturpoet"&gt; /u/picturpoet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0dhjzmgzydqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T21:17:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nly3w1</id>
    <title>Qwen 3 VL next week</title>
    <updated>2025-09-20T13:24:32+00:00</updated>
    <author>
      <name>/u/Long_Bluejay_5368</name>
      <uri>https://old.reddit.com/user/Long_Bluejay_5368</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt; &lt;img alt="Qwen 3 VL next week" src="https://a.thumbs.redditmedia.com/fLB-QxQX_aAn0F5HXNaiy2dlb5JbWlBjS-VuT3q3TC0.jpg" title="Qwen 3 VL next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3"&gt;https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;what do you think about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_Bluejay_5368"&gt; /u/Long_Bluejay_5368 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeq9w</id>
    <title>Llama.cpp support for Ling Mini 2.0 is probably coming next week</title>
    <updated>2025-09-21T01:02:01+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"&gt; &lt;img alt="Llama.cpp support for Ling Mini 2.0 is probably coming next week" src="https://external-preview.redd.it/2pTMNMbI2akSWow2DVQcK_a-oWX8FigInIZ74WH_NyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2f02e74afa0abefa42b4330fc05577e733ff328" title="Llama.cpp support for Ling Mini 2.0 is probably coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama.cpp support for Ling Mini 2.0 is coming in the following days, it seems thereâ€™s already a PR waiting to be merged and some GGUFs already out.&lt;/p&gt; &lt;p&gt;An interesting thing about this model is that it has 16B total parameters, but only 1.4B are activated per input token, and it outperforms Ernie 4.5 21B A3B, which is a tad bigger and uses more active parameters. Quite a nice addition for the GPU-poor folks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16036"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:02:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm0mzw</id>
    <title>Whisper Large v3 running in real-time on a M2 Macbook Pro</title>
    <updated>2025-09-20T15:08:21+00:00</updated>
    <author>
      <name>/u/rruk01</name>
      <uri>https://old.reddit.com/user/rruk01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt; &lt;img alt="Whisper Large v3 running in real-time on a M2 Macbook Pro" src="https://external-preview.redd.it/NnkxeHk1bTIxY3FmMbdFe5hFZkGFnrWFqBq5GQzhAAe-tezJH5BHnp8SS6Dh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4c3798d766be6b03e6447b1663fb9590cdfcffe" title="Whisper Large v3 running in real-time on a M2 Macbook Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on using the Whisper models on device for 2-3 years now and wanted to share my progress. &lt;/p&gt; &lt;p&gt;I've figured out several optimisations which combined together means I can run the Whisper Large v3 (not turbo) model on a macbook with about 350-600ms latency for live (hypothesis/cyan) requests and 900-1200ms for completed (white) requests. It can also run on an iPhone 14 Pro with about 650-850ms latency for live requests and 1900ms for completed requests. The optimisations work for all the Whisper models and would probably work for the NVIDIA Parakeet / Canary models too. &lt;/p&gt; &lt;p&gt;The optimisations include speeding up the encoder on Apple Neural Engine so it runs at &lt;strong&gt;150ms&lt;/strong&gt; per run, this is compared to a naive 'ANE-optimised' encoder which runs at about &lt;strong&gt;500ms&lt;/strong&gt;. This does not require significant quantisation. The model running in the demo is quantised at Q8, but mainly so it takes up less hard-disk space, FP16 runs at similar speed. I've also optimised hypothesis requests so the output is much more stable. &lt;/p&gt; &lt;p&gt;If there's interest I'd be happy to write up a blog post on these optimisations, I'm also considering making an open source SDK so people can run this themselves, again if there's interest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rruk01"&gt; /u/rruk01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibrz4m21cqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T15:08:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmiqjh</id>
    <title>OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext.</title>
    <updated>2025-09-21T04:34:19+00:00</updated>
    <author>
      <name>/u/AdFluffy920</name>
      <uri>https://old.reddit.com/user/AdFluffy920</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"&gt; &lt;img alt="OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext." src="https://b.thumbs.redditmedia.com/d6NOhInKANeH8JCDvtmFBeHE3iU2bdeRVBiujDCsTrI.jpg" title="OPEN WEIGHTS: Isaac 0.1. Perceptive-language model. 2B params. Matches or beats models significantly larger on core perception as claimed by Perceptron AI. Links to download in bodytext." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://www.perceptron.inc/blog/introducing-isaac-0-1"&gt;https://www.perceptron.inc/blog/introducing-isaac-0-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo: &lt;a href="https://www.perceptron.inc/demo"&gt;https://www.perceptron.inc/demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Download weights: &lt;a href="https://huggingface.co/PerceptronAI/Isaac-0.1"&gt;https://huggingface.co/PerceptronAI/Isaac-0.1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdFluffy920"&gt; /u/AdFluffy920 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nmiqjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmiqjh/open_weights_isaac_01_perceptivelanguage_model_2b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:34:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlu3cd</id>
    <title>The iPhone 17 Pro can run LLMs fast!</title>
    <updated>2025-09-20T09:53:52+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt; &lt;img alt="The iPhone 17 Pro can run LLMs fast!" src="https://a.thumbs.redditmedia.com/lazvh4ZugenSKXRU1IYFLEO6hichFPkV7Tw3LqJ6h_8.jpg" title="The iPhone 17 Pro can run LLMs fast!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new A19 Pro finally integrates neural accelerators into the GPU cores themselves, essentially Appleâ€™s version of Nvidiaâ€™s Tensor cores which are used for accelerating matrix multiplication that is prevalent in the transformers models we love so much. So I thought it would be interesting to test out running our smallest finetuned models on it!&lt;/p&gt; &lt;p&gt;Boy does the GPU fly compared to running the model only on CPU. The token generation is only about double but the prompt processing is over 10x faster! Itâ€™s so much faster that itâ€™s actually usable even on longer context as the prompt processing doesnâ€™t quickly become too long and the token generation speed is still high.&lt;/p&gt; &lt;p&gt;I tested using the Pocket Pal app on IOS which runs regular llamacpp with MLX Metal optimizations as far as I know. Shown are the comparison of the model running on GPU fully offloaded with Metal API and flash attention enabled vs running on CPU only. &lt;/p&gt; &lt;p&gt;Judging by the token generation speed, the A19 Pro must have about 70-80GB/s of memory bandwidth to the GPU and the CPU can access only about half of that bandwidth. &lt;/p&gt; &lt;p&gt;Anyhow the new GPU with the integrated tensor cores now look very interesting for running LLMs. Perhaps when new Mac Studios with updated M chips comes out with a big version of this new GPU architecture, I might even be able to use them to serve models for our low cost API. ðŸ¤”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nlu3cd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm6v83</id>
    <title>Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b</title>
    <updated>2025-09-20T19:14:37+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt; &lt;img alt="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" src="https://b.thumbs.redditmedia.com/Ol0Pxbro6vazPUaSzrEtZA_JvTzjW_-by2F4t8qWnuU.jpg" title="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this test on my M4 Max MacBook Pro 128 GB laptop. The interesting find is how prompt processing speed stays relatively flat as context grows. This is completely different behavior from Qwen3 Coder.&lt;/p&gt; &lt;p&gt;GPT 120b starts out faster but then becomes slower as context fills. However only the 4 bit quant of Qwen Next manages to overtake it when looking at total elapsed time. And that first happens at 80k context length. For most cases the GPT model stays the fastest then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nm6v83"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T19:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlyy6n</id>
    <title>Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping</title>
    <updated>2025-09-20T14:00:49+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt; &lt;img alt="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" src="https://external-preview.redd.it/942g63AteF3sF5KI6YzwLlHNUjooze5_uZcUA7PiVqQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=259bd6663f4a689dc50651317dca845a29e37f3f" title="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-arc-pro-b60-24gb-professional-gpu-listed-at-599-in-stock-and-shipping"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T14:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nme5xy</id>
    <title>4x MI50 32GB reach 22 t/s with Qwen3 235B-A22B and 36 t/s with Qwen2.5 72B in vllm</title>
    <updated>2025-09-21T00:33:31+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;It is exciting to see AMD is finally fixing their software stack. I recently updated my MI50 GPU drivers and ROCm stack to 6.4.3. AMD officially deprecated support for MI50 (gfx906). But ROCm 6.4.3 works with one simple fix. You need to copy tensile library of MI50 from a package and paste it in rocm folder (details: &lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;For performance tests, I used vllm backend - &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt; . Thank you &lt;a href="/u/NaLanZeYu"&gt;u/NaLanZeYu&lt;/a&gt; for supporting gfx906 in a separate vllm fork!&lt;/p&gt; &lt;p&gt;In my venv, I installed pytorch 2.8. I kept the original triton 3.3 but I earlier checked and triton 3.5 was also working with MI50. For single GPU, there were no package issues. For multi-GPU, there was an issue - rccl was compiled without gfx906 support. What I did was I compiled rccl with gfx906 support.&lt;/p&gt; &lt;p&gt;Downloaded rccl 2.22.3 (for ROCm 6.4.3) from &lt;a href="https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3"&gt;https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;extracted the zip file.&lt;/p&gt; &lt;p&gt;installed in ubuntu terminal:&lt;/p&gt; &lt;p&gt;```sudo ./install.sh --amdgpu_targets gfx906 -i -j 32 -p -r```&lt;/p&gt; &lt;p&gt;in vllmenv installation folder find &lt;a href="http://lbrccl.so"&gt;lbrccl.so&lt;/a&gt; and rename or delete it so that pytorch cannot use it. e.g. _librccl.so&lt;/p&gt; &lt;p&gt;in vllmenv, import the new rccl library location:&lt;/p&gt; &lt;p&gt;VLLM_NCCL_SO_PATH=/opt/rocm/lib&lt;/p&gt; &lt;p&gt;(or LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH)&lt;/p&gt; &lt;p&gt;now, vllm supports multi-GPU properly for MI50 with ROCm 6.4.3.&lt;/p&gt; &lt;p&gt;Some metrics:&lt;/p&gt; &lt;p&gt;single MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama-3.1-8B-AWQ-4bit - TG 93t/s; PP 945t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;four MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5 72B gptq int4 (TP 4) - TG 36/s; PP 500t/s&lt;/li&gt; &lt;li&gt;Qwen3-235B-A22B-AWQ (TP 4) - TG 22t/s; PP 290t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of them are connected to my MB with PCIE4.0 16x speed. CPU: AMD EPYC 7532 with 8x32GB DDR4 3200Mhz ECC RAM.&lt;/p&gt; &lt;p&gt;Overall, there is a great performance uplift (up to 25%) when we use ROCm 6.4.3 with gfx906.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T00:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmj3cr</id>
    <title>Lucy-Edit : 1st Open-sourced model for Video editing</title>
    <updated>2025-09-21T04:54:54+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lucy-Edit-Dev, based on Wan2.2 5B is the first open-sourced AI model with video editing capabilities, calling itself the nano banana for video editing. It can change clothes, characters, backgrounds, object, etc.&lt;/p&gt; &lt;p&gt;Model weights : &lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;https://huggingface.co/decart-ai/Lucy-Edit-Dev&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmj3cr/lucyedit_1st_opensourced_model_for_video_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:54:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeu5s</id>
    <title>Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted</title>
    <updated>2025-09-21T01:07:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt; &lt;img alt="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" src="https://external-preview.redd.it/VoGpbOIxrqAHEzxUbIOFVzMNSL9glnfyk27odhpB_Jk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d87cfdb2cbad767672c45769d597618162abb80" title="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/41025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:07:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmkswn</id>
    <title>Just dropped: Qwen3-4B Function calling on just 6GB VRAM</title>
    <updated>2025-09-21T06:37:33+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to bring this to you if you are looking for a superior model for toolcalling to use with ollama for local Codex style personal coding assistant on terminal:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex"&gt;https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;âœ… &lt;strong&gt;Fine-tuned on 60K function calling examples&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;âœ… &lt;strong&gt;4B parameters&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;âœ… &lt;strong&gt;GGUF format&lt;/strong&gt; (optimized for CPU/GPU inference)&lt;/li&gt; &lt;li&gt;âœ… &lt;strong&gt;3.99GB download&lt;/strong&gt; (fits on any modern system)&lt;/li&gt; &lt;li&gt;âœ… &lt;strong&gt;Production-ready&lt;/strong&gt; with 0.518 training loss&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this works with&lt;br /&gt; &lt;a href="https://github.com/ymichael/open-codex/"&gt;https://github.com/ymichael/open-codex/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/8ankur8/anything-codex"&gt;https://github.com/8ankur8/anything-codex&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/dnakov/anon-codex"&gt;https://github.com/dnakov/anon-codex&lt;/a&gt;&lt;br /&gt; preferable: &lt;a href="https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code"&gt;https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T06:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmg185</id>
    <title>Qwen3Omni</title>
    <updated>2025-09-21T02:08:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt; &lt;img alt="Qwen3Omni" src="https://preview.redd.it/wcxu5ypyefqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b0e169e57d635253c780f31d6542861df594c98" title="Qwen3Omni" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcxu5ypyefqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T02:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmii5y</id>
    <title>Magistral 1.2 is incredible. Wife prefers it over Gemini 2.5 Pro.</title>
    <updated>2025-09-21T04:21:00+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL:DR - AMAZING general use model. Y'all gotta try it. &lt;/p&gt; &lt;p&gt;Just wanna let y'all know that Magistral is worth trying. Currently running the UD Q3KXL quant from Unsloth on Ollama with Openwebui. &lt;/p&gt; &lt;p&gt;The model is incredible. It doesn't overthink and waste tokens unnecessarily in the reasoning chain. &lt;/p&gt; &lt;p&gt;The responses are focused, concise and to the point. No fluff, just tells you what you need to know. &lt;/p&gt; &lt;p&gt;The censorship is VERY minimal. My wife has been asking it medical-adjacent questions and it always gives you a solid answer. I am an ICU nurse by trade and am studying for advanced practice and can vouch for the advice magistral is giving is legit. &lt;/p&gt; &lt;p&gt;Before this, wife has been using Gemini 2.5 pro and hates the censorship and the way it talks to you like a child (let's break this down, etc). &lt;/p&gt; &lt;p&gt;The general knowledge in Magistral is already really good. Seems to know obscure stuff quite well. &lt;/p&gt; &lt;p&gt;Now, once you hook it up to a web search tool call is where this model I feel like can hit as hard as proprietary LLMs. The model really does wake up even more when hooked up to the web. &lt;/p&gt; &lt;p&gt;Model even supports image input. I have not tried that specifically but I loved image processing from Mistral 3.2 2506 so I expect no issues there.&lt;/p&gt; &lt;p&gt;Currently using with Openwebui with the recommended parameters. If you do use it with OWUI, be sure to set up the reasoning tokens in the model settings so thinking is kept separate from the model response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building ðŸ”¨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio ðŸ‘¾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
