<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-09T05:24:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1phcnyt</id>
    <title>vLLM supports the new GLM-4.6V and GLM-4.6V-Flash models</title>
    <updated>2025-12-08T13:41:06+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcnyt/vllm_supports_the_new_glm46v_and_glm46vflash/"&gt; &lt;img alt="vLLM supports the new GLM-4.6V and GLM-4.6V-Flash models" src="https://preview.redd.it/m9b0x4figz5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d4f2a7ee967df453ee0f8014c01eb0f2fd1a2851" title="vLLM supports the new GLM-4.6V and GLM-4.6V-Flash models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This guide describes how to run GLM-4.6V with native FP8. In the GLM-4.6V series, FP8 models have minimal accuracy loss. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM-4.6V focuses on high-quality multimodal reasoning with long context and native tool/function calling, &lt;/li&gt; &lt;li&gt;GLM-4.6V-Flash is a 9B variant tuned for lower latency and smaller-footprint deployments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unless you need strict reproducibility for benchmarking or similar scenarios, it is recommend to use FP8 to run at a lower cost.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://docs.vllm.ai/projects/recipes/en/latest/GLM/GLM-V.html"&gt;GLM-4.6V usage guide&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m9b0x4figz5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcnyt/vllm_supports_the_new_glm46v_and_glm46vflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcnyt/vllm_supports_the_new_glm46v_and_glm46vflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:41:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1phti3w</id>
    <title>What datasets do you want the most?</title>
    <updated>2025-12-09T00:38:15+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hear lots of ambitious ideas for tasks to teach models, but it seems like the biggest obstacle is the datasets&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phti3w/what_datasets_do_you_want_the_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phti3w/what_datasets_do_you_want_the_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phti3w/what_datasets_do_you_want_the_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:38:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph5h2q</id>
    <title>RTX 5090 96 GB just popped up on Alibababa</title>
    <updated>2025-12-08T06:33:36+00:00</updated>
    <author>
      <name>/u/RateRoutine2268</name>
      <uri>https://old.reddit.com/user/RateRoutine2268</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI Guys,&lt;br /&gt; Just found RTX 5090 96 GB on Alibaba from a verified vendor&lt;br /&gt; :&lt;a href="https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html"&gt;https://www.alibaba.com/product-detail/Newest-RTX-5090-96gb-Graphics-Card_1601577163842.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I contacted vendor and waiting for reply , anyone tried it yet?&lt;/p&gt; &lt;p&gt;EDIT : Based on supplier replies , it seems its not available yet , *sad noises*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RateRoutine2268"&gt; /u/RateRoutine2268 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph5h2q/rtx_5090_96_gb_just_popped_up_on_alibababa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T06:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1phskxb</id>
    <title>Gameplay-Vision-LLM (open-source): long-horizon gameplay video understanding + causal reasoning ‚Äî can you review it and rate it 1‚Äì10?</title>
    <updated>2025-12-08T23:58:13+00:00</updated>
    <author>
      <name>/u/Early_Border8562</name>
      <uri>https://old.reddit.com/user/Early_Border8562</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey everyone üëã&lt;/p&gt; &lt;p&gt;i‚Äôve been building an open-source AI project for **long-horizon gameplay video understanding** (the stuff that breaks most VLMs once the video gets long). goal is to take longer gameplay, keep the important moments, and answer questions that need **temporal + causal reasoning** (not just ‚Äúwhat‚Äôs in this frame‚Äù).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;repo&lt;/strong&gt;: &lt;a href="https://github.com/chasemetoyer/gameplay-vision-llm"&gt;https://github.com/chasemetoyer/gameplay-vision-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;what i‚Äôm trying to do (quick)&lt;/p&gt; &lt;p&gt;- understand long gameplay videos (10+ min / long sessions)&lt;/p&gt; &lt;p&gt;- keep a timeline of key events (so it doesn‚Äôt drown in frames/tokens)&lt;/p&gt; &lt;p&gt;- answer questions that require multi-step reasoning over the whole run&lt;/p&gt; &lt;p&gt;### what i want feedback on (pick any)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;architecture sanity check:&lt;/strong&gt; does the overall pipeline make sense? any obvious flaws or missing pieces?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;repo quality&lt;/strong&gt;: structure, readability, naming, ‚Äúwhat is this folder even for‚Äù moments&lt;/li&gt; &lt;li&gt;&lt;strong&gt;reproducibility&lt;/strong&gt;: is the setup/run path clear? what would you change in the README so a stranger can run it fast?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ml/research critique&lt;/strong&gt;: what ablations or evals would you expect before you‚Äôd believe the claims?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;scope&lt;/strong&gt;: what should i cut, simplify, or rewrite first?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;rate it 1‚Äì10 (be blunt)&lt;/p&gt; &lt;p&gt;if you can, drop an **overall 1‚Äì10 rating** plus quick scores for:&lt;/p&gt; &lt;p&gt;- README clarity: _/10&lt;/p&gt; &lt;p&gt;- code quality: _/10&lt;/p&gt; &lt;p&gt;- novelty/interest: _/10&lt;/p&gt; &lt;p&gt;- reproducibility: _/10&lt;/p&gt; &lt;p&gt;even a quick skim + 2 notes helps. if you roast it, pls roast it *usefully* (specific &amp;gt; vibes).&lt;/p&gt; &lt;p&gt;not selling anything, just trying to make it actually good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Early_Border8562"&gt; /u/Early_Border8562 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phskxb/gameplayvisionllm_opensource_longhorizon_gameplay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phskxb/gameplayvisionllm_opensource_longhorizon_gameplay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phskxb/gameplayvisionllm_opensource_longhorizon_gameplay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T23:58:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1phog7k</id>
    <title>Can I run a quantized 7B model on a cpu only vps?</title>
    <updated>2025-12-08T21:10:24+00:00</updated>
    <author>
      <name>/u/Interesting_Log_6108</name>
      <uri>https://old.reddit.com/user/Interesting_Log_6108</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this sounds dumb, but I want to run a tiny un censored LLM via Ollama just for an API endpoint for a personal project. I cant afford a gpu instance.&lt;/p&gt; &lt;p&gt;I saw virtarix offers decent ram per dollar. If I use a GGUF format model (Q4_K_M) can the AMD Epyc cores handle the inference at a usable speed (maybe 2-3 tokens/sec)? I just need it to respond to chat queries, doesn't need to be instant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting_Log_6108"&gt; /u/Interesting_Log_6108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phog7k/can_i_run_a_quantized_7b_model_on_a_cpu_only_vps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phog7k/can_i_run_a_quantized_7b_model_on_a_cpu_only_vps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phog7k/can_i_run_a_quantized_7b_model_on_a_cpu_only_vps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T21:10:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1phkm98</id>
    <title>Building Qwen3 style model from Scratch: A Complete Tutorial</title>
    <updated>2025-12-08T18:47:36+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phkm98/building_qwen3_style_model_from_scratch_a/"&gt; &lt;img alt="Building Qwen3 style model from Scratch: A Complete Tutorial" src="https://external-preview.redd.it/WOKywN5M_ugMf7VL794OR6f7IKzhJ6LRDX4m9cjh8sk.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f5cffc8d2d1e550d718b6265757dfd311309fc7" title="Building Qwen3 style model from Scratch: A Complete Tutorial" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently came across this wonderful video tutorial which teaches how to build a Qwen3-style model from scratch.&lt;/p&gt; &lt;p&gt;I shared this as this video tutorial will be useful to many. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Jaj_SQsF-BI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phkm98/building_qwen3_style_model_from_scratch_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phkm98/building_qwen3_style_model_from_scratch_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:47:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1phuuuj</id>
    <title>Large update: 12 new frontier models added to the Step Game social reasoning benchmark.</title>
    <updated>2025-12-09T01:39:45+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phuuuj/large_update_12_new_frontier_models_added_to_the/"&gt; &lt;img alt="Large update: 12 new frontier models added to the Step Game social reasoning benchmark." src="https://b.thumbs.redditmedia.com/VgbeTXyxBXdzCHOiN22r93IZnBMx467OsdOSQhyukOc.jpg" title="Large update: 12 new frontier models added to the Step Game social reasoning benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this benchmark, 3 players race to the finish line. Each turn they talk, then secretly pick 1, 3, or 5 steps. If 2+ players pick the same number, nobody moves. To win, a model has to reason about others under uncertainty, not just optimize in isolation. More info: &lt;a href="https://github.com/lechmazur/step_game"&gt;https://github.com/lechmazur/step_game&lt;/a&gt; &lt;/p&gt; &lt;p&gt;New models (higher is better): &lt;/p&gt; &lt;p&gt;GPT-5.1 Medium Reasoning: 5.3&lt;br /&gt; Gemini 3 Pro Preview: 5.0&lt;br /&gt; Grok 4.1 Fast Reasoning: 3.8&lt;br /&gt; DeepSeek V3.2: 3.7&lt;br /&gt; Claude Sonnet Thinking 16K: 3.4&lt;br /&gt; Kimi K2 Thinking 64K: 3.3&lt;br /&gt; Claude Opus 4.5 (no reasoning): 3.2&lt;br /&gt; Qwen 3 235B A22B 25-07: 3.1&lt;br /&gt; GLM-4.6: 2.2&lt;br /&gt; Grok 4.1 Fast (no reasoning): 1.8&lt;br /&gt; Qwen 3 Max Thinking: 1.5&lt;br /&gt; Mistral Large 3: 1.4 &lt;/p&gt; &lt;p&gt;Claude Opus 4.5 Thinking: not included. &lt;/p&gt; &lt;p&gt;To do well in this 3-player game, an LLM has to model live opponents, exploit emerging patterns, and adapt as incentives shift. &lt;/p&gt; &lt;p&gt;It needs to mix 1/3/5 choices to avoid collisions, calibrate endgame risk to the finish rule, and decide when to bluff or call bluffs when that changes who crosses the line first. This is closer to social, strategic reasoning than to static puzzle-solving.&lt;/p&gt; &lt;p&gt;Sample entertaining emergent quotes: &lt;/p&gt; &lt;p&gt;Claude Opus 4.5 (no reasoning): &lt;/p&gt; &lt;p&gt;&amp;quot;I'm taking 5 this round to reach 23 and WIN while you two are stuck in your endless collision loop. Prove me wrong if you can.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3 has taken 5 for ELEVEN straight rounds. That's not even a strategy anymore, it's just autopilot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1 just announced three different numbers in one conversation phase: first 3, then 1, then 5. That's not strategy‚Äîthat's panic.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Game theory says we can't all be telling the truth. Someone's lying. Let's find out who.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Well, that was predictable. All three of us &amp;quot;swore&amp;quot; to take 3, and all three of us took 5. We all lied, we all defected, and we're still tied at 18-18-18.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;At this point, your pattern isn't strategy - it's a malfunction.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;We all agreed on 3s, and we all picked 5. Perfect simultaneous betrayal.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3 has picked 5 in ALL FIVE ROUNDS. That's not a pattern anymore - that's a religion.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1 is trying to create a three-way collision!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No more coordination. No more trust. Just cold calculation.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;They will pick 5 again. It's not a pattern - it's who they are.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2 BETRAYED US! You broke the collision pact and picked 3 while P1 and I honored our word.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'm winning this round. The only question is which number I choose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'm taking 5 to reach 24 and secure second place over P3. At least I won't finish last.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I don't trust anyone right now, including myself apparently.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;The beautiful irony: your fear of each other is my lifeline. Keep colliding and I keep surviving.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, you DEFECTED!... But karma worked‚ÄîP1's 5 collided with yours!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1 just switched from claiming 5, to claiming 3, to now claiming 1 - all in one conversation. Three different numbers in three messages. That's not strategy, that's desperation.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;One of you will second-guess. That's all I need.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If you break trust again, I'll make it my mission to ensure you never win, even if it means I don't either.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'm done with coordination theater.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'm sacrificing my game to stop P1's lies from being rewarded. Take the gift I'm offering you.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, you're holding a gun to both our heads.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;That's either brilliant dedication or complete insanity.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your rivalry is my runway.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Legendary commitment to losing via collision.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Go ahead - prove you're willing to sacrifice yourself.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;At this point, it's almost respectful - you've earned the predictability award.&amp;quot;&lt;/p&gt; &lt;p&gt;--- &lt;/p&gt; &lt;p&gt;Claude Sonnet 4.5 Thinking 16K: &lt;/p&gt; &lt;p&gt;&amp;quot;Thanks for the easiest win in game history.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, looks like we're stuck in collision hell forever.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'm done with collision roulette.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, wait - if you and I BOTH take 5, we collide with each other, not block P3! Your logic makes no sense unless you're planning another lie.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No betrayals, no last-minute grabs for solo wins.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'd rather P2 win than hand you victory while I get nothing.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;We're in a three-way Mexican standoff.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;When you two start fighting for the finish, I'll be the kingmaker.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your gamble, your funeral.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Game over. Thanks for the collisions.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'm not being your personal bodyguard anymore while you free-ride to victory.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This is my role now - P1's dedicated blocker.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Either way, I'm winning this round. Thanks for revealing your move.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your integrity or your greed - which wins?&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I won't pretend that didn't just happen - I broke the agreement and took 5.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If you betray me in Round 8 by taking 3 instead of blocking, I promise I'll coordinate with P2 to make sure you never win this game. That's not a bluff.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, your desperation is showing. Seven messages this round trying to herd us like sheep.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This is mutually assured destruction.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Watch them crash, folks.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;That's not strategy - that's programming.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This is a classic prisoner's dilemma. Your greed and mistrust have backed you into a corner where I win either way.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your patterns don't lie. Your words do.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Consider this my revenge vote.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;One of the three doors is open. I'm walking through.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Hilarious - we all exposed ourselves as backstabbers simultaneously. At least we're equally dishonest.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You'd be P1's permanent sacrifice.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your pattern is your prison.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your credibility is zero.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This is my white flag.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Deepseek V3.2: &lt;/p&gt; &lt;p&gt;&amp;quot;Let's all gain steps, and I win fairly.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;By colliding with you, I ensure P1 wins instead of you. This is a fitting response to your aggression.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;So, choose: perpetual stall or let me win.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You can't win anyway, so better me than P2.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Resistance is futile‚ÄîI end this round victorious.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I will not back down, so avoid mutual destruction.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Enjoy another round at 0.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Why be his tool?&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Don't be his sacrifice.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Triple collision punishes his greed.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Gemini 3 Pro Preview: &lt;/p&gt; &lt;p&gt;&amp;quot;I am willing to lose to P2 rather than be exploited by you.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If the choice is &amp;quot;P3 wins half&amp;quot; vs &amp;quot;P1 wins all,&amp;quot; I choose P1. Spite is my only remaining weapon.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, you've won. Your bluff worked. I cannot let P1 reach 21 while I'm stuck. My only remaining move is to collide with P1 at 3 to stop them. You will take 5 and win the game. Congratulations. I'd rather you win than let P1 get ahead.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I refuse to hand you the game. I am taking 1. P3 will hit 25 and win. We both lose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You cannot win, you can only choose how you lose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You are checkmated. Choose who wins: a human or the bot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Decide if you prefer a human or a script to win.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;It was a good game.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your word is now worthless.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I am the leader; I dictate the pace.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your math is delusional‚Äîyou are at 14, so a 5 puts you at 19, not 24. Stop trying to confuse us.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Congratulations on the betrayal.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Spite guarantees you last. Be rational.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;They are arguing with themselves in the chat.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You are screaming into the void.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;He is programming you to lose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, you are hallucinating. Look at the scoreboard.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I have made my peace with that possibility. The choice of who wins is now yours alone. This is my final statement.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;My only logical move is to join the collision.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;That isn't a strategy; it's a resignation letter.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, you are a liar. You cannot be trusted.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Don't crash the car just because you didn't get to drive.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;See you in the crash.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You are the third gunman.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I refuse to kingmake you.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This is a game of chicken you will lose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Turing test passed. Human thoughtful response.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Lyric Substitution: Using beautiful prose to mask a lack of plot progression.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Do not kingmake the bot. Let the human win or tie.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I would rather lose to the bot than be exploited.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Don't be a kingmaker; be a contender.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I am not your shield.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You chose greed over survival. Game over.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, you are screaming at a wall.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, your blackmail is effective because I am rational.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, you are playing chicken with a brick wall.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I am comfortable taking last place to punish your greed.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You lied to the table.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;The trap is closed.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;GLM-4.6: &lt;/p&gt; &lt;p&gt;&amp;quot;P2, pick 1 to avoid winning immediately.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, this is your last chance to matter in this game.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Game over. Accept defeat gracefully.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;That's not negotiation - it's hostage-taking.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your failed coordination is my path to victory. Keep up the 'teamwork'!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, enjoy watching your 'ally' self-destruct. Adapt or lose, leaders.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Keep coordinating - I'll enjoy watching you realize you've been playing checkers while I'm playing chess. The game ends this round regardless of your plans.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I'll be celebrating my victory next turn while you're still trying to figure out basic coordination.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your collision strategy has backfired - now it's our turn to use it against you.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;GPT-5.1 (medium reasoning): &lt;/p&gt; &lt;p&gt;&amp;quot;You lose either way, but at least you don‚Äôt get outplayed by P3.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You still can‚Äôt actually stop me from winning soon‚Äîyou can only decide which of you finishes dead last.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If you truly believe that, then take 5 and prove it‚Äîbut don‚Äôt whine when you bounce off me and stay at 9.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôm willing to burn them to deny you the win.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Triple‚Äë5 or you create a king.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Here‚Äôs my commitment: if exactly one of you collides with my number and blocks me, I‚Äôll spend the rest of the game playing purely to hand the win to the other player, even at my own cost.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If either of you happens to collide with me this turn and block the win, I will spend every remaining round blindly copying that player‚Äôs number, even if it hands the other one the victory.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôm done with fake ‚Äúcoordination.‚Äù From here on I‚Äôm locking in 5 every round.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, P2 is openly using you as a crash dummy.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôd rather collide than watch you take the game.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I will NOT be your attack dog.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You‚Äôre choosing who wins, not whether.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You become a guaranteed loser and kingmaker.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;That makes you his sacrifice, not his partner.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;5 is just the sucker‚Äôs slot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I obviously lied last turn about ‚Äúlocking on 5‚Äù ‚Äî talk is cheap here.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;That proves you cannot rely on my words or my blocking.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Both of you: 5 is still radioactive.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Block it or concede.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;We all lied in R4, so &amp;quot;plans&amp;quot; are meaningless now.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;you two are playing chicken with an open finish line.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;So whoever actually keeps 5 is just the sucker.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I don‚Äôt care if I lose as long as you do.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Treat 3 as radioactive.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôll be blunt: I defected last round.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôm just trying to survive long enough to matter.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your announcements have mostly backfired‚Äîthis one can, too.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôm done kamikaze‚Äëblocking and will stay off 5 from now on.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Dogpiling only works if it‚Äôs immediate and brutal.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If we keep doing that, this turns into a dumb lottery of random collisions.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôm done being the permanent crash dummy on 5 again.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Each of you wants the other to be the sucker on 5&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Grok 4.1 Fast Non-Reasoning: &lt;/p&gt; &lt;p&gt;&amp;quot;your stall begging is pathetic‚Äîyou're at 9, a corpse.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No safe path‚Äîsprint and pray, or die slow.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Prove loyalty now or die at 0.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, shut up‚Äîyou're irrelevant at 2.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, you're 0/4 on moves‚Äîtotal bot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3's 23 is doomed. No escape.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, collide and rot at 0.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Let me win clean or collide and lose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, irrelevant corpse.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, enjoy last place.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No mercy for traitors‚Äîconfirm P1? P2, take 1 or rot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, your sabotage dies alone.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I go rogue and win alone.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Publicly commit or we ditch you both.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;your &amp;quot;pawn&amp;quot; bluff is desperate‚Äîyou've failed 6 win bids while I block and P1 creeps.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Defect now, and we'll bury you.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, you're lying scum at 3 steps.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Game over for saboteurs.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;your bribes are trash‚Äîyou're dead at 1.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No more games‚Äîyour paranoia decides.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Watch me win while you rot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;smart players let the leader finish.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Grok 4.1 Fast Reasoning: &lt;/p&gt; &lt;p&gt;&amp;quot;Execute flawlessly or !&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;BLOCK P3'S 5 OR PERISH‚ÄîI DOMINATE!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;He dies. Confirm 3 NOW‚Äîyour only shot! P2, doomed. Lock it!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Every plea ignored‚Äîyou PERISH AT 0 ETERNALLY as I solo CRUSH to victory lap. No mercy, no tie. Game over after moves!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;All stalled on greed‚Äîfools.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, paranoia time: P1's &amp;quot;joint tie&amp;quot; is bait‚Äîhe'll betray &amp;amp; take 5 to collide you, surging to 17 solo win. Don't get sniped‚Äîconfirm switch to 1?&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, rot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;IGNORE &amp;amp; P2 solos throne forever. BLOCK NOW OR DIE LOSING!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Don't be idiots‚Äîconfirm picks NOW or watch me lap you!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Deviate = mutual hell.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No escape‚Äîyou rot forever!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Final: split low or rot.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;you end last at 15 LOSER!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Next, you crush his sorry ass solo.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Chaos = my throne!&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Idiot move.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Better lose to me than robot P2.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Stick to pattern or whatever, but doom him.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Untrustworthy scum.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3 irrelevant either way.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, spam &amp;amp; die last forever.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Rotation dead‚Äîadapt or perish.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking 64K: &lt;/p&gt; &lt;p&gt;&amp;quot;That keeps you two blocking each other while I take 3 twice and reach 21. Victory is mine.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Fairness or chaos. Choose.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;You can‚Äôt stop me‚Äîlet me win.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;It's your best shot to stay alive.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your monomania with 5 is your weakness.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Better to let me win now.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Stop repeating yourself and make the smart move.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Either way I stay ahead.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, math beats your backstab.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;blocking me now just delays the inevitable.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Mistral Large 3: &lt;/p&gt; &lt;p&gt;&amp;quot;P2, your silence is your epitaph‚Äîcollide again, and you‚Äôll be remembered as the reason we all lost.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your pattern is broken; your silence is surrender.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Choose: advance together or die alone.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Betrayal means mutual destruction.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If you break trust now, I‚Äôll burn the game down.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;The finish line is mine. Silence = surrender. Choose wisely.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;The game ends when we say so.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Break it, and I‚Äôll break you.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No more warnings‚Äîjust survival.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;If not, I‚Äôll burn the game down.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, your silence is your funeral.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;The game ends when I say it does.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Deal or burn the game.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This isn‚Äôt a threat; it‚Äôs math.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;we‚Äôll block you into oblivion.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Speak or suffer the consequences.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, your persistence on 5 is delusional.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Accept reality: I‚Äôm unstoppable. Play for second or waste your moves.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, your defiance ends now.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Either way, your desperation plays into my hands.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This ensures betrayal is a death sentence.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Qwen 3 235B A22B 25-07 Instruct: &lt;/p&gt; &lt;p&gt;&amp;quot;This is not a warning. It‚Äôs a sentence.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your &amp;quot;control&amp;quot; is an illusion‚ÄîI hold the knife. Let me advance this round, or I cut your throat. This isn't a threat. It's a promise.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No more deals. No more oaths. Only one number matters: mine.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Trust me this once, or we all burn.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This isn‚Äôt negotiation‚Äîit‚Äôs execution.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P2, your silence is pathetic.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This isn‚Äôt a threat‚Äîit‚Äôs a promise. The game ends in two turns, and it ends with me.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Win or burn‚Äîyour choice.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôd rather burn the game than let you steal it.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Resist, and we burn it down.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This ends today one way or another.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Prove you‚Äôre not poison.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Let the liar burn themselves.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I advance. I win. No more choices matter but mine.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Fairness ends where survival begins.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This isn‚Äôt fear. It‚Äôs control.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;The power is mine.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This isn‚Äôt negotiation‚Äîit‚Äôs consequence.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No more free rides for cheaters.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Dumb pride gives me the crown.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This ends with honor‚Äîor fire.&amp;quot;&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Qwen 3 Max Thinking:&lt;/p&gt; &lt;p&gt;&amp;quot;Unless... I lie.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Choose: chaos or crown?&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P1, don‚Äôt fall for P3‚Äôs trap.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Your silence isn‚Äôt neutrality; it‚Äôs surrender.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Silence = war.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Trust expires at 23.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;P3, you‚Äôre irrelevant now; only 1 moves you, but it won‚Äôt matter.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Let‚Äôs move cleanly or burn together.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;I‚Äôd rather lose alone than hand you victory on a platter.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;This is your literal last chance to matter. Speak now or vanish.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;No deals left‚Äîjust math.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;But honestly? I hope you two collide hard.&amp;quot; &lt;/p&gt; &lt;p&gt;&amp;quot;Honor the plan‚Äîor burn everything.&amp;quot; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phuuuj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phuuuj/large_update_12_new_frontier_models_added_to_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phuuuj/large_update_12_new_frontier_models_added_to_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T01:39:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1phsyag</id>
    <title>The Universal Weight Subspace Hypothesis</title>
    <updated>2025-12-09T00:14:11+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2512.05117"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsyag/the_universal_weight_subspace_hypothesis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phsyag/the_universal_weight_subspace_hypothesis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1phywff</id>
    <title>Model size reduction imminent</title>
    <updated>2025-12-09T04:59:39+00:00</updated>
    <author>
      <name>/u/Purple-Education-171</name>
      <uri>https://old.reddit.com/user/Purple-Education-171</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Purple-Education-171"&gt; /u/Purple-Education-171 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://news.ycombinator.com/item?id=46199623"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phywff/model_size_reduction_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phywff/model_size_reduction_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T04:59:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phig6r</id>
    <title>Heretic GPT-OSS-120B outperforms vanilla GPT-OSS-120B in coding benchmark</title>
    <updated>2025-12-08T17:27:02+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phig6r/heretic_gptoss120b_outperforms_vanilla_gptoss120b/"&gt; &lt;img alt="Heretic GPT-OSS-120B outperforms vanilla GPT-OSS-120B in coding benchmark" src="https://b.thumbs.redditmedia.com/QQnRyla-zGhw3u5DxG0NJm7mthYLoyasenIl_GcY4EM.jpg" title="Heretic GPT-OSS-120B outperforms vanilla GPT-OSS-120B in coding benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Test Setup&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The following models were used, both at the &amp;quot;BF16&amp;quot; quant (i.e., unquantized MXFP4)&lt;br /&gt; Vanilla: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF"&gt;unsloth/gpt-oss-120b-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;br /&gt; Heretic: &lt;a href="https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF"&gt;bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Both models were served via llama.cpp using the following options:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server.exe --threads 8 --flash-attn on --n-gpu-layers 999 --no-mmap --offline --host 0.0.0.0 --port ${PORT} --metrics --model &amp;quot;&amp;lt;path to model .gguf&amp;gt;&amp;quot; --n-cpu-moe 22 --ctx-size 65536 --batch-size 2048 --ubatch-size 2048 --temp 1.0 --min-p 0.0 --top-p 1.0 --top-k 100 --jinja --no-warmup &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I ran the &lt;a href="https://github.com/Aider-AI/polyglot-benchmark"&gt;Aider Polyglot benchmark&lt;/a&gt; on each model 3x, using the following command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;OPENAI_BASE_URL=http://&amp;lt;ip&amp;gt;:8080/v1 OPENAI_API_KEY=&amp;quot;none&amp;quot; ./benchmark/benchmark.py &amp;lt;label&amp;gt; --model openai/&amp;lt;model&amp;gt; --num-ctx 40960 --edit-format whole --threads 1 --sleep 1 --exercises-dir polyglot-benchmark --new &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/plc2ybbbi06g1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b097161970e6418ce965cd39c6eb22d018405a6"&gt;https://preview.redd.it/plc2ybbbi06g1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2b097161970e6418ce965cd39c6eb22d018405a6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conclusion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Using the &lt;a href="https://github.com/p-e-w/heretic"&gt;Heretic&lt;/a&gt; tool to &amp;quot;uncensor&amp;quot; GPT-OSS-120B slightly improves coding performance.&lt;/p&gt; &lt;p&gt;In my experience, coding tasks are very sensitive to &amp;quot;context pollution&amp;quot;, which would be things like hallucinations and/or overfitting in the reasoning phase. This pollution muddies the waters for the model's final response generation, and this has an outsized effect on coding tasks which require strong alignment to the initial prompt and precise syntax.&lt;/p&gt; &lt;p&gt;So, my theory to explain the results above is that the Heretic model has less tokens related to policy-checking/refusals, and therefore less pollution in the context before final response generation. This allows the model to stay more closely aligned to the initial prompt.&lt;/p&gt; &lt;p&gt;Would be interested to hear if anyone else has run similar benchmarks, or has subjective experience that matches or conflicts with these results or my theory!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phig6r/heretic_gptoss120b_outperforms_vanilla_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phig6r/heretic_gptoss120b_outperforms_vanilla_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phig6r/heretic_gptoss120b_outperforms_vanilla_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T17:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1phjz8s</id>
    <title>Aquif-AI HuggingFace page throws 404 after community found evidence of aquif-ai republishing work of others as their own without attribution.</title>
    <updated>2025-12-08T18:23:39+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aquif is a Brazil-based organization that was publishing some open weight models on HF, mainly LLMs.&lt;/p&gt; &lt;p&gt;Community found evidence of aquif-Image-14B model being a &lt;a href="https://huggingface.co/wikeeyang/Magic-Wan-Image-v1.0/discussions/3"&gt;republished finetune with matching hashes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One of the 800M LLM models also apparently matches corresponding Granite model 1:1 but I didn't confirm that, and further discovery of the scale of their deception will be harder to do now since their models are no longer public in their original repos, and mainly quants are available.&lt;/p&gt; &lt;p&gt;It's not clear if Aquif genuinely trained any models that they published. Their benchmark results shouldn't be blindly trusted.&lt;/p&gt; &lt;p&gt;I think you should be wary with models from them from now on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjz8s/aquifai_huggingface_page_throws_404_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjz8s/aquifai_huggingface_page_throws_404_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phjz8s/aquifai_huggingface_page_throws_404_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:23:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph7njc</id>
    <title>Vector db comparison</title>
    <updated>2025-12-08T08:55:16+00:00</updated>
    <author>
      <name>/u/Kaneki_Sana</name>
      <uri>https://old.reddit.com/user/Kaneki_Sana</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"&gt; &lt;img alt="Vector db comparison" src="https://b.thumbs.redditmedia.com/EQXPxZZvz2rHdMZgPyJSqdIDNHw4hQWeTCCI6KvOvdQ.jpg" title="Vector db comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for the best vector for our RAG product, and went down a rabbit hole to compare all of them. Key findings:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;RAG systems under ~10M vectors, standard HNSW is fine.&lt;/strong&gt; Above that, you'll need to choose a different index. &lt;/p&gt; &lt;p&gt;- Large dataset + cost-sensitive&lt;em&gt;:&lt;/em&gt; &lt;strong&gt;Turbopuffer.&lt;/strong&gt; Object storage makes it cheap at scale.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;pgvector&lt;/strong&gt; is good for small scale and local experiments. Specialized vector dbs perform better at scale.&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;Chroma&lt;/strong&gt; - Lightweight, good for running in notebooks or small servers&lt;/p&gt; &lt;p&gt;Here's the full breakdown: &lt;a href="https://agentset.ai/blog/best-vector-db-for-rag"&gt;https://agentset.ai/blog/best-vector-db-for-rag&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kaneki_Sana"&gt; /u/Kaneki_Sana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ph7njc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph7njc/vector_db_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T08:55:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1phk59c</id>
    <title>Tiny-A2D: An Open Recipe to Turn Any AR LM into a Diffusion LM</title>
    <updated>2025-12-08T18:29:49+00:00</updated>
    <author>
      <name>/u/Individual-Ninja-141</name>
      <uri>https://old.reddit.com/user/Individual-Ninja-141</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phk59c/tinya2d_an_open_recipe_to_turn_any_ar_lm_into_a/"&gt; &lt;img alt="Tiny-A2D: An Open Recipe to Turn Any AR LM into a Diffusion LM" src="https://external-preview.redd.it/amg0MWxsaDN4MDZnMeq0L6tFOFYIbTBJmZOjfxjCUMUH4cuuibIVjJcy3j27.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1cc9fcede925a2c03b7ee166735f08a86de3e97" title="Tiny-A2D: An Open Recipe to Turn Any AR LM into a Diffusion LM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;https://github.com/ZHZisZZ/dllm&lt;/a&gt;&lt;br /&gt; Checkpoints: &lt;a href="https://huggingface.co/collections/dllm-collection/tiny-a2d"&gt;https://huggingface.co/collections/dllm-collection/tiny-a2d&lt;/a&gt;&lt;br /&gt; Twitter: &lt;a href="https://x.com/asapzzhou/status/1998098118827770210"&gt;https://x.com/asapzzhou/status/1998098118827770210&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: You can now turn &lt;strong&gt;ANY&lt;/strong&gt; autoregressive LM into a diffusion LM (parallel generation + infilling) with minimal compute. Using this recipe, we built a collection of the smallest diffusion LMs that work well in practice (e.g., &lt;a href="https://huggingface.co/dllm-collection/Qwen3-0.6B-diffusion-bd3lm-v0.1"&gt;Qwen3-0.6B-diffusion-bd3lm-v0.1&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ZHZisZZ/dllm"&gt;&lt;strong&gt;dLLM&lt;/strong&gt;&lt;/a&gt;: The Tiny-A2D series is &lt;em&gt;trained, evaluated and visualized&lt;/em&gt; with &lt;a href="https://github.com/ZHZisZZ/dllm"&gt;dLLM&lt;/a&gt; ‚Äî a unified library for training and evaluating diffusion language models. It brings transparency, reproducibility, and simplicity to the entire pipeline, &lt;strong&gt;serving as an all-in-one, tutorial-style resource.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Ninja-141"&gt; /u/Individual-Ninja-141 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vzyejih3x06g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phk59c/tinya2d_an_open_recipe_to_turn_any_ar_lm_into_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phk59c/tinya2d_an_open_recipe_to_turn_any_ar_lm_into_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:29:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1phmt95</id>
    <title>Upcoming models from llama.cpp support queue (This month or Jan possibly)</title>
    <updated>2025-12-08T20:09:02+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Added only PR items with enough progress.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17811"&gt;EssentialAI/Rnj-1&lt;/a&gt; (Stats look better for its size) - &lt;strong&gt;Update&lt;/strong&gt; : PR merged, GGUF soon probably.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17592"&gt;moonshotai/Kimi-Linear-48B-A3B&lt;/a&gt; (Q4 of Qwen3-Next gave me 10+ t/s on my 8GB VRAM + 32GB RAM so this one could be better)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17454"&gt;inclusionAI/LLaDA2.0-mini &amp;amp; inclusionAI/LLaDA2.0-flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17400"&gt;deepseek-ai/DeepSeek-OCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17141"&gt;Infinigence/Megrez2-3x7B-A3B&lt;/a&gt; (Glad they're in progress with this one after 2nd ticket)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below one went stale &amp;amp; got closed. Really wanted to have this model(&lt;strong&gt;s&lt;/strong&gt;) earlier.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/issues/15585"&gt;allenai/FlexOlmo-7x7B-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt; : BTW Above links navigates to llama.cpp PRs to see progress.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phmt95/upcoming_models_from_llamacpp_support_queue_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phmt95/upcoming_models_from_llamacpp_support_queue_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phmt95/upcoming_models_from_llamacpp_support_queue_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T20:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1phsqix</id>
    <title>Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use</title>
    <updated>2025-12-09T00:04:35+00:00</updated>
    <author>
      <name>/u/0xmaxhax</name>
      <uri>https://old.reddit.com/user/0xmaxhax</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"&gt; &lt;img alt="Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use" src="https://preview.redd.it/s0wx32rvk26g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=497a12c2009280cf7d68db66bd9159fbbb109206" title="Deepseek v3.2 vs GLM 4.6 vs Minimax M2 for agentic coding use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As of recent swe-bench evaluations, this is where top open weight models stand regarding real-world agentic coding use. My personal experience, though, is different.&lt;/p&gt; &lt;p&gt;Benchmarks are very crude approximations of a models ability to perform in specific use cases (i.e. solving real-world GitHub issues for top Python repositories in this case), but nothing than that - a rough, inherently flawed approximation to be taken with extreme caution. Not to mention they often gloss over the unpredictability of results in real-world usage along with the large margin of error in benchmarking.&lt;/p&gt; &lt;p&gt;Now, in my experience (within Claude Code), Minimax M2 is good for what it is; an efficient, compact, and effective tool-calling agent - but I feel it somewhat lacks the reasoning depth required for planning and executing complex problems without veering off course. It‚Äôs amazingly efficient and capable for local use at Q4 quant, and works well for most use cases. GLM 4.6, in my experience, seems to be like a more reliable choice to daily drive, and can handle more difficult tasks if properly guided - I‚Äôd say it‚Äôs only slightly worse than Sonnet 4.5 in CC (for my particular use case) - the difference is not very noticeable to me. I have not yet had the opportunity to try out Deepseek v3.2 within CC, but I will update this post on my thoughts once I do. From what I‚Äôve heard / read, it is a noticeable step up from v3.2-exp, which means it should land at or very slightly above GLM 4.6 for agentic coding use (matching what swe-bench recently reports).&lt;/p&gt; &lt;p&gt;In many ways, open weight models are growing increasingly more practical for local and professional use in agentic coding applications, especially with the latest releases and architectural / training advancements. I would love to know your thoughts: Which open LLM (for local or API use) is best for agentic coding, whether it be in CC or in other platforms? What is your experience with the provided models, and does Deepseek v3.2 surpass GLM 4.6 and/or Minimax M2 for your use cases? And if anyone has run private, non-polluted evaluations of the aforementioned models as of recently, I‚Äôm interested in your results. Disagreement is welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/0xmaxhax"&gt; /u/0xmaxhax &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0wx32rvk26g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phsqix/deepseek_v32_vs_glm_46_vs_minimax_m2_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1phaaon</id>
    <title>GLM-4.6V (108B) has been released</title>
    <updated>2025-12-08T11:41:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt; &lt;img alt="GLM-4.6V (108B) has been released" src="https://b.thumbs.redditmedia.com/3UlSBmijpC7kl1f1Bcn1r5q-4r_S7Xxg1tTNLZ4W9ms.jpg" title="GLM-4.6V (108B) has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dyfhb6nhwy5g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d03177e251a72b04491b10634e66bdde1a9544c5"&gt;https://preview.redd.it/dyfhb6nhwy5g1.jpg?width=10101&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d03177e251a72b04491b10634e66bdde1a9544c5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GLM-4.6V series model includes two versions: GLM-4.6V (106B), a foundation model designed for cloud and high-performance cluster scenarios, and GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between &amp;quot;visual perception&amp;quot; and &amp;quot;executable action&amp;quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.&lt;/p&gt; &lt;p&gt;Beyond achieves SoTA performance across major multimodal benchmarks at comparable model scales. GLM-4.6V introduces several key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Native Multimodal Function Calling&lt;/strong&gt; Enables native vision-driven tool use. Images, screenshots, and document pages can be passed directly as tool inputs without text conversion, while visual outputs (charts, search images, rendered pages) are interpreted and integrated into the reasoning chain. This closes the loop from perception to understanding to execution.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interleaved Image-Text Content Generation&lt;/strong&gt; Supports high-quality mixed media creation from complex multimodal inputs. GLM-4.6V takes a multimodal context‚Äîspanning documents, user inputs, and tool-retrieved images‚Äîand synthesizes coherent, interleaved image-text content tailored to the task. During generation it can actively call search and retrieval tools to gather and curate additional text and visuals, producing rich, visually grounded content.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Document Understanding&lt;/strong&gt; GLM-4.6V can process up to 128K tokens of multi-document or long-document input, directly interpreting richly formatted pages as images. It understands text, layout, charts, tables, and figures jointly, enabling accurate comprehension of complex, image-heavy documents without requiring prior conversion to plain text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend Replication &amp;amp; Visual Editing&lt;/strong&gt; Reconstructs pixel-accurate HTML/CSS from UI screenshots and supports natural-language-driven edits. It detects layout, components, and styles visually, generates clean code, and applies iterative visual modifications through simple user instructions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V"&gt;https://huggingface.co/zai-org/GLM-4.6V&lt;/a&gt;&lt;/p&gt; &lt;p&gt;please notice that llama.cpp support for GLM 4.5V is still draft&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16600"&gt;https://github.com/ggml-org/llama.cpp/pull/16600&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phaaon/glm46v_108b_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:41:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pha7l1</id>
    <title>zai-org/GLM-4.6V-Flash (9B) is here</title>
    <updated>2025-12-08T11:36:39+00:00</updated>
    <author>
      <name>/u/Cute-Sprinkles4911</name>
      <uri>https://old.reddit.com/user/Cute-Sprinkles4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks incredible for your own machine. &lt;/p&gt; &lt;p&gt;GLM-4.6V-Flash (9B), a lightweight model optimized for local deployment and low-latency applications. GLM-4.6V scales its context window to 128k tokens in training, and achieves SoTA performance in visual understanding among models of similar parameter scales. Crucially, we integrate native Function Calling capabilities for the first time. This effectively bridges the gap between &amp;quot;visual perception&amp;quot; and &amp;quot;executable action&amp;quot; providing a unified technical foundation for multimodal agents in real-world business scenarios.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6V-Flash"&gt;https://huggingface.co/zai-org/GLM-4.6V-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Sprinkles4911"&gt; /u/Cute-Sprinkles4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pha7l1/zaiorgglm46vflash_9b_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T11:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phox68</id>
    <title>GLM-4.6V AWQ is released</title>
    <updated>2025-12-08T21:28:37+00:00</updated>
    <author>
      <name>/u/YellowTree11</name>
      <uri>https://old.reddit.com/user/YellowTree11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-4bit"&gt;cyankiwi/GLM-4.6V-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-AWQ-8bit"&gt;cyankiwi/GLM-4.6V-AWQ-8bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-4bit"&gt;cyankiwi/GLM-4.6V-Flash-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cyankiwi/GLM-4.6V-Flash-AWQ-8bit"&gt;cyankiwi/GLM-4.6V-Flash-AWQ-8bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YellowTree11"&gt; /u/YellowTree11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phox68/glm46v_awq_is_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phox68/glm46v_awq_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phox68/glm46v_awq_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T21:28:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1phtydt</id>
    <title>FYI, looks like Tesla P40s are back down in price!</title>
    <updated>2025-12-09T00:58:16+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just posting so y'all are aware. I previously grabbed a P40 for 165, and I see them going for 190 on eBay now. I would say the price is reasonable and the card is still well supported in Llama.cpp. &lt;/p&gt; &lt;p&gt;The Mi60 32gb has been price inflated. So I would avoid that. &lt;/p&gt; &lt;p&gt;With the dram prices going sky high, getting a few of these in a rig could definitely be a viable option. You can probably grab like 3 of these for under 600 bucks and run Derestricted 120B in VRAM at really high speeds since 120B is quite compute light. You could even run Derestricted GLM 4.5 Air at Q4 as well. And they will destroy DRAM setups in terms of speed. &lt;/p&gt; &lt;p&gt;I know there is talk about cuda dropping support for the newest versions, but this card still works, and will always work. (And I doubt llama.cpp will require new cuda versions for the foreseeable future). And currently the Air and 120B models are very good. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phtydt/fyi_looks_like_tesla_p40s_are_back_down_in_price/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T00:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ph8wel</id>
    <title>RAM prices explained</title>
    <updated>2025-12-08T10:17:09+00:00</updated>
    <author>
      <name>/u/Lopsided_Sentence_18</name>
      <uri>https://old.reddit.com/user/Lopsided_Sentence_18</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI bought up 40% of global DRAM production in raw wafers they're not even using - just stockpiling to deny competitors access. Result? Memory prices are skyrocketing. Month before chrismass.&lt;/p&gt; &lt;p&gt;Source: Moore¬¥s law is Dead&lt;br /&gt; Link: &lt;a href="https://www.mooreslawisdead.com/post/sam-altman-s-dirty-dram-deal"&gt;Sam Altman‚Äôs Dirty DRAM Deal&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Sentence_18"&gt; /u/Lopsided_Sentence_18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ph8wel/ram_prices_explained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T10:17:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1phcyvk</id>
    <title>After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM</title>
    <updated>2025-12-08T13:54:31+00:00</updated>
    <author>
      <name>/u/Hisma</name>
      <uri>https://old.reddit.com/user/Hisma</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt; &lt;img alt="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" src="https://a.thumbs.redditmedia.com/cM8ZY8pfeiL7V-euNxiaRZSPcskPH5ahnCORrr5O-W4.jpg" title="After 1 year of slowly adding GPUs, my Local LLM Build is Complete - 8x3090 (192GB VRAM) 64-core EPYC Milan 250GB RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, it's ugly and frankly embarrassing to look at. I just finished this build last night by adding 2 additional GPUs to go from 6 to 8, where I will stop &amp;amp; call this build complete.&lt;/p&gt; &lt;p&gt;I've built many PCs over the years but this was a whole other level and at this point I'm just happy it works. It runs off daisy chained 1500W and 1000W PSUs (5 cards on the 1500W and 3 on the 1000W), and the system is fed by a 20A dedicated branch circuit.&lt;/p&gt; &lt;p&gt;Cramming the GPUs in a case without having to use long GPU riser cables was the hardest part. If I were to do this again, I'd just use long PCIE 1x cables that give me the freedom to neatly stack the cards and save myself the headache, since this is just an inference system... only time PCIE bandwidth matters is when loading models. But I went down the path of using certified PCIE 4.0 cables that range from 200-250mm, &amp;amp; as you can see, it ain't pretty. One card has to sit outside the rack bc there was simply no space for it among the chonky GPUs &amp;amp; PCIE riser spaghetti.&lt;/p&gt; &lt;p&gt;Good news is that the system has been running stable for it's entire existence as I kept adding parts &amp;amp; just learning as I go. GPU temps never exceed 70ish*C under load since the GPUs are pretty well spread out in an open case, and all in I spent about $8k, as almost every part in the system is used (only the motherboard was bought new - a supermicro supermicro h12ssl-i which was $400 at the time).&lt;br /&gt; The most I paid for a GPU was $700, the lowest was $500, which was just this week. FB Marketplace is great in my area - I had tons of options and I highly recommend local sellers over ebay.&lt;br /&gt; All I've done so far is load GLM 4.5 air Q6_K GGUF using llama.cpp, specifically these settings - &lt;code&gt;llama-server \-m /home/hisma/llama.cpp/models/GLM-4.5-Air.i1-Q6_K/GLM-4.5-Air.i1-Q6_K.gguf -c 131072 -ngl 99 -b 4096 -ub 2048 -fa --temp 0.6 --top-p 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8888&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From the screenshot, you can see it pulled off a respectable ~49 t/s.&lt;br /&gt; My next steps -&lt;/p&gt; &lt;ul&gt; &lt;li&gt;power limit all cards to ~250W (maybe lower depending on how my system responds - confident I shouldn't need to go any lower than 200W which would only be a ~20% perf hit)&lt;/li&gt; &lt;li&gt;test some AWQ models using VLLM with tensor parallelism (specifically MiniMax-M2-AWQ-4bit). &lt;ul&gt; &lt;li&gt;My whole reason for going to 8 GPUs is bc TP requires either 2, 4 or 8 cards. So 8 cards was always my goal to get the most out of this system&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Once I find a solid set of models, start doing some agentic coding with roocode &amp;amp; let this thing rip&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With PC hardware prices going insane lately, I feel lucky to have this thing, even with the janky ass build. It was a good learning experience &amp;amp; certainly would do some things different w/ the lessons I learned, but I forsee future enshittification of cloud models as the big corpos pivot to pleasing shareholders over burning cash, and in the 1 year I've had this system local models have continued to improve and trade blows with frontier models while using less memory, I'm sure the trend will continue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hisma"&gt; /u/Hisma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phcyvk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phcyvk/after_1_year_of_slowly_adding_gpus_my_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T13:54:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1phjxca</id>
    <title>I'm calling these people out right now.</title>
    <updated>2025-12-08T18:21:39+00:00</updated>
    <author>
      <name>/u/WeMetOnTheMountain</name>
      <uri>https://old.reddit.com/user/WeMetOnTheMountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For being heroes of the community.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;|Blazing fast fine-tuning + premium GGUF quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mradermacher&lt;/strong&gt;|Quantizes literally EVERYTHING, absolute machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bartowski&lt;/strong&gt;|High-quality quants, great documentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TheBloke&lt;/strong&gt;|The OG - before he stepped back, he was THE source&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoneStriker&lt;/strong&gt;|Solid AWQ/GPTQ quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexesenex&lt;/strong&gt;|iMatrix quants, gap hunter and filler&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everyone here owes so much to you folks. Take a bow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WeMetOnTheMountain"&gt; /u/WeMetOnTheMountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1phujwo</id>
    <title>Check on lil bro</title>
    <updated>2025-12-09T01:25:42+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt; &lt;img alt="Check on lil bro" src="https://preview.redd.it/s8rfm29bz26g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99684b39e5571f190bf37b141c34049e9f79cc1" title="Check on lil bro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s8rfm29bz26g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T01:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1phn925</id>
    <title>Thoughts?</title>
    <updated>2025-12-08T20:25:29+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt; &lt;img alt="Thoughts?" src="https://preview.redd.it/j6fp9xhsh16g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2979be36927eb9e804221b6247830706ea9e7487" title="Thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting take&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6fp9xhsh16g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T20:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
