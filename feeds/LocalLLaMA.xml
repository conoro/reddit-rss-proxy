<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-27T21:37:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qoktn4</id>
    <title>Just a question</title>
    <updated>2026-01-27T17:44:49+00:00</updated>
    <author>
      <name>/u/Temporary-Cookie838</name>
      <uri>https://old.reddit.com/user/Temporary-Cookie838</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today is 2026. I'm just wondering, is there any open source model out there that is as good or better than Claude 3.5 at least out there? I'd love to run a capable coding assistant locally if possible. I'm a web dev btw. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Cookie838"&gt; /u/Temporary-Cookie838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoktn4/just_a_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoktn4/just_a_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoktn4/just_a_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T17:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo75sj</id>
    <title>Mixture of Lookup Experts are God Tier for the average guy (RAM+Disc Hybrid Inference)</title>
    <updated>2026-01-27T07:24:00+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently Deepseek's Engram piqued interest into using disc offloading for inference. However, a DeepseekV3 model with half engram weights doesn't change the fact that you need to read 20B worth of expert weights from disc every token. Active parameters, and the resulting read bandwidth latency are exactly the same. &lt;/p&gt; &lt;p&gt;There is another type of MoE which can essentially the reduce read bandwidth latency of the experts to 0. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2503.15798"&gt;https://arxiv.org/abs/2503.15798&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mixture of Lookup Experts are MoEs with precomputed experts as lookup-tables. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For inference you create a &lt;strong&gt;giant&lt;/strong&gt; dictionary of all your possible computation results beforehand for your experts.&lt;/p&gt; &lt;p&gt;Normally, you need to read the experts sitting in ram for computing with cpu offload. Reading 10GB of 8 active experts with 50GB/s would 1/5th of a second, with further delays expected. However, with this method, you just want the output, which will be KB sized per expert. You can see the bottleneck of expert offloading is completely eliminated, but we still retain the performance value. &lt;/p&gt; &lt;p&gt;Please let me know your thoughts. When I first read the paper, I was confused by the fact that they activated all experts. But it's not important, you can do training at top-k 8. There are some improvements in another paper, because this one doesn't train experts with positional information. It trains experts with raw token embeddings rather than intermediate states. I want to talk about it because re-parameterizing experts is the best optimization trick I've read to-date. I don't want the idea to die. It's perfect for us, given RAM is more expensive. Maybe Arcee or upcoming labs can give the idea a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo75sj/mixture_of_lookup_experts_are_god_tier_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo75sj/mixture_of_lookup_experts_are_god_tier_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo75sj/mixture_of_lookup_experts_are_god_tier_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T07:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qnwa33</id>
    <title>GLM 4.7 Flash: Huge performance improvement with -kvu</title>
    <updated>2026-01-26T23:07:49+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; Try passing -kvu to llama.cpp when running GLM 4.7 Flash. &lt;/p&gt; &lt;p&gt;On RTX 6000, my tokens per second on a 8K token output rose from 17.7t/s to 100t/s&lt;/p&gt; &lt;p&gt;Also, check out the one shot zelda game it made, pretty good for a 30B:&lt;br /&gt; &lt;a href="https://talented-fox-j27z.pagedrop.io"&gt;https://talented-fox-j27z.pagedrop.io&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwa33/glm_47_flash_huge_performance_improvement_with_kvu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwa33/glm_47_flash_huge_performance_improvement_with_kvu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qnwa33/glm_47_flash_huge_performance_improvement_with_kvu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-26T23:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qooruc</id>
    <title>Prompt -&gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant</title>
    <updated>2026-01-27T20:00:56+00:00</updated>
    <author>
      <name>/u/thecoder12322</name>
      <uri>https://old.reddit.com/user/thecoder12322</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qooruc/prompt_offline_voice_ai_app_in_11_mins_we_forked/"&gt; &lt;img alt="Prompt -&amp;gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant" src="https://external-preview.redd.it/bXQ3OGcwdWszeWZnMar3PqhOAeGgunpwEz0ru_QCwuvxRMLdSfMMDfA6VBMp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27d6f4c3fbcbdec4ca735f7c21d1ae9b9fa06535" title="Prompt -&amp;gt; Offline Voice AI App in ~11 mins. We forked Expo to bundle native on-device AI runtimes ‚Äî Replit agent builds a fully offline voice assistant" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtu.be/rDVUaI8P4L0"&gt;https://youtu.be/rDVUaI8P4L0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I recorded a demo showing a &lt;a href="/u/Replit"&gt;u/Replit&lt;/a&gt; agent generating an &lt;a href="/u/Expo"&gt;u/Expo&lt;/a&gt; app that runs a full on-device voice pipeline (speech-to-text ‚Üí LLM ‚Üí text-to-speech). The goal is to collapse the ‚Äúnative setup + bindings + glue‚Äù problem that slows down on-device AI experimentation.&lt;/p&gt; &lt;p&gt;What we built:&lt;/p&gt; &lt;p&gt;- RunAnywhere: open-source SDK for running LLM / STT / TTS locally on iOS/Android&lt;/p&gt; &lt;p&gt;- A fork of Expo with our native runtimes baked in&lt;/p&gt; &lt;p&gt;- A custom client (‚ÄúRunAnywhere AI Studio‚Äù) so you can scan a QR and run the app on your phone like the normal Expo workflow&lt;/p&gt; &lt;p&gt;In the demo, the agent builds a ‚ÄúPunny Voice Assistant‚Äù that runs locally and responds via TTS.&lt;/p&gt; &lt;p&gt;This is early and I want real feedback from devs:&lt;/p&gt; &lt;p&gt;- What‚Äôs the first offline-first AI app you‚Äôd actually build?&lt;/p&gt; &lt;p&gt;- What would make this production-usable?&lt;/p&gt; &lt;p&gt;Links in the comment&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecoder12322"&gt; /u/thecoder12322 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/h4frf0uk3yfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qooruc/prompt_offline_voice_ai_app_in_11_mins_we_forked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qooruc/prompt_offline_voice_ai_app_in_11_mins_we_forked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T20:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qochwc</id>
    <title>GLM OCR Support Merged in Transformers GitHub.</title>
    <updated>2026-01-27T12:28:27+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qochwc/glm_ocr_support_merged_in_transformers_github/"&gt; &lt;img alt="GLM OCR Support Merged in Transformers GitHub." src="https://external-preview.redd.it/hXuIHPHgqmecwOLhbqT1msTfBCBkBrhYVRX6INqoNqE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6516d0dd8125be6933dd5889d21f2505fbc1fdd7" title="GLM OCR Support Merged in Transformers GitHub." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/commit/4854dbf9da4086731256496cf4a8e4ea45d4d54e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qochwc/glm_ocr_support_merged_in_transformers_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qochwc/glm_ocr_support_merged_in_transformers_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T12:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qogkgr</id>
    <title>I built a local-first AI tool: generate ST character cards via local-first LLM endpoints or openai API + optional image backends ‚Äî feedback wanted</title>
    <updated>2026-01-27T15:14:58+00:00</updated>
    <author>
      <name>/u/JaxxonAI</name>
      <uri>https://old.reddit.com/user/JaxxonAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open-source, local-first Character Card Generator for SillyTavern character cards (JSON + PNG cards). It‚Äôs a Vue/Node web app that talks to your local LLM endpoint (KoboldCPP or OpenAI-compatible), and optionally your local image backend (ComfyUI / SDAPI).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates ST fields with structured output (supports ‚Äúfill missing fields‚Äù + regenerate selected fields)&lt;/li&gt; &lt;li&gt;Field detail presets: Short / Detailed / Verbose + per-field overrides&lt;/li&gt; &lt;li&gt;Timeouts + max token controls for long generations&lt;/li&gt; &lt;li&gt;Multi-repo library (CardGen + external folders like SillyTavern) with copy/move + search/sort&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love your feedback on the app. &lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/ewizza/ST-CardGen"&gt;https://github.com/ewizza/ST-CardGen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Background thread in &lt;a href="/r/SillyTavernAI"&gt;r/SillyTavernAI&lt;/a&gt;: &lt;a href="https://www.reddit.com/r/SillyTavernAI/comments/1qhe1a4/new_character_generator_with_llm_and_image_api/"&gt;https://www.reddit.com/r/SillyTavernAI/comments/1qhe1a4/new_character_generator_with_llm_and_image_api/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JaxxonAI"&gt; /u/JaxxonAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qogkgr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qogkgr/i_built_a_localfirst_ai_tool_generate_st/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qogkgr/i_built_a_localfirst_ai_tool_generate_st/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T15:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo8r8s</id>
    <title>Kimi K2.5 Launches, Unsloth quantisations coming soon</title>
    <updated>2026-01-27T09:00:51+00:00</updated>
    <author>
      <name>/u/Plastic-Accident862</name>
      <uri>https://old.reddit.com/user/Plastic-Accident862</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://platform.moonshot.ai/docs/guide/kimi-k2-5-quickstart"&gt;https://platform.moonshot.ai/docs/guide/kimi-k2-5-quickstart&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plastic-Accident862"&gt; /u/Plastic-Accident862 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo8r8s/kimi_k25_launches_unsloth_quantisations_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo8r8s/kimi_k25_launches_unsloth_quantisations_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo8r8s/kimi_k25_launches_unsloth_quantisations_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T09:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoge2t</id>
    <title>Kimi K2.5 Architecture Dive: 1T Params, 384 Experts, Native INT4 (and it beats GPT-5 on reasoning)</title>
    <updated>2026-01-27T15:08:29+00:00</updated>
    <author>
      <name>/u/comebackch</name>
      <uri>https://old.reddit.com/user/comebackch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The specs on the new Moonshot AI model (Kimi K2.5) are actually wild, and I feel like the architectural shift is being overlooked because of the &amp;quot;Agent&amp;quot; hype.&lt;/p&gt; &lt;p&gt;I dug into the technical report/release notes, and this isn't just a Llama clone. It looks like a very aggressive optimization of the MoE (Mixture-of-Experts) architecture specifically for consumer hardware efficiency relative to performance.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture Breakdown:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Parameters:&lt;/strong&gt; 1 Trillion.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Active Parameters:&lt;/strong&gt; Only 32B per token.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert Granularity:&lt;/strong&gt; 384 specialized experts (vs 256 in DeepSeek V3).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Routing:&lt;/strong&gt; Selects top-8 experts + 1 &amp;quot;shared&amp;quot; expert for common grammar/logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native QAT:&lt;/strong&gt; It was trained with Quantization-Aware Training for INT4 from day one. This explains how they fit it on 4x H100s instead of a massive cluster.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why the &amp;quot;Shared Expert&amp;quot; matters:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;They seem to have solved the &amp;quot;interference&amp;quot; problem where learning code degrades creative writing. By isolating micro-domains (like &amp;quot;Rust syntax&amp;quot; or &amp;quot;Classical Poetry&amp;quot;) into specific experts and keeping a shared expert for the basics, the model maintains coherence better than dense models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The &amp;quot;Thinking&amp;quot; Mode:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It's using a System 2 approach similar to recent reasoning models, generating internal &amp;quot;thought tokens&amp;quot; to decompose problems before answering.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (If you trust them):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Humanity's Last Exam:&lt;/strong&gt; 50.2% (vs GPT-5 at 41.7%).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiveCodeBench:&lt;/strong&gt; 83.1% (Approaching GPT-5, crushing Claude 3.5 Sonnet).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone pulled the weights yet to verify the VRAM requirements for local inference? The 32B active param count suggests it might be runnable on dual 3090s/4090s with heavy quantization, but the full MOE routing usually requires keeping more in VRAM.&lt;/p&gt; &lt;p&gt;Thoughts on this &amp;quot;Hyper-MoE&amp;quot; trend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/comebackch"&gt; /u/comebackch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoge2t/kimi_k25_architecture_dive_1t_params_384_experts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoge2t/kimi_k25_architecture_dive_1t_params_384_experts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoge2t/kimi_k25_architecture_dive_1t_params_384_experts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T15:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qomra4</id>
    <title>Some initial benchmarks of Kimi-K2.5 on 4xB200</title>
    <updated>2026-01-27T18:50:51+00:00</updated>
    <author>
      <name>/u/benno_1237</name>
      <uri>https://old.reddit.com/user/benno_1237</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qomra4/some_initial_benchmarks_of_kimik25_on_4xb200/"&gt; &lt;img alt="Some initial benchmarks of Kimi-K2.5 on 4xB200" src="https://preview.redd.it/zyvu6wcjsxfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d94c746acf2b874a21b6387a83bb8da32100ae9" title="Some initial benchmarks of Kimi-K2.5 on 4xB200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just had some fun and ran a (very crude) benchmark script. Sadly, one GPU is busy so I can only run on 4 instead of 8 (thus limiting me to ~30k context without optimizations).&lt;/p&gt; &lt;p&gt;Command used (with random-input-len changing between sample points):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve \ --backend openai \ --base-url http://localhost:8000 \ --model /models/huggingface/moonshotai/Kimi-K2.5 \ --dataset-name random \ --random-input-len 24000 \ --random-output-len 512 \ --request-rate 2 \ --num-prompts 20 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;One full data point:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 20 Failed requests: 0 Request rate configured (RPS): 2.00 Benchmark duration (s): 61.48 Total input tokens: 480000 Total generated tokens: 10240 Request throughput (req/s): 0.33 Output token throughput (tok/s): 166.55 Peak output token throughput (tok/s): 420.00 Peak concurrent requests: 20.00 Total token throughput (tok/s): 7973.52 ---------------Time to First Token---------------- Mean TTFT (ms): 22088.76 Median TTFT (ms): 22193.34 P99 TTFT (ms): 42553.83 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 34.37 Median TPOT (ms): 37.72 P99 TPOT (ms): 39.72 ---------------Inter-token Latency---------------- Mean ITL (ms): 34.37 Median ITL (ms): 17.37 P99 ITL (ms): 613.91 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, first token latency is terrible. This is probably due to an unoptimized tokenizer and inefficient chunk prefilling. I wanted to see the model perform with default vllm settings though.&lt;/p&gt; &lt;p&gt;Coding looks okay-ish at the moment but the context is limiting (this is a me problem, not the model).&lt;/p&gt; &lt;p&gt;Let me know if you want to see some benchmarks/have me try some settings.&lt;/p&gt; &lt;p&gt;Edit: &lt;/p&gt; &lt;p&gt;Maybe also interesting to know: first start took about 1.5h (with already downloaded safetensors). This is by far the longest time I ever had to wait for anything to start. Consecutive starts are much faster though&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benno_1237"&gt; /u/benno_1237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zyvu6wcjsxfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qomra4/some_initial_benchmarks_of_kimik25_on_4xb200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qomra4/some_initial_benchmarks_of_kimik25_on_4xb200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T18:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoocgn</id>
    <title>allenai released new open coding models</title>
    <updated>2026-01-27T19:45:55+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"&gt; &lt;img alt="allenai released new open coding models" src="https://b.thumbs.redditmedia.com/40jQPf7fLYGIV-9GrNxTp-Fu-rEYdD8Bc_t8B0upc1U.jpg" title="allenai released new open coding models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/open-coding-agents"&gt;https://huggingface.co/collections/allenai/open-coding-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3wanlr674yfg1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c31d64089433fd350f3aaa72d94242e9326b7ab"&gt;https://preview.redd.it/3wanlr674yfg1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c31d64089433fd350f3aaa72d94242e9326b7ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://allenai.org/papers/opencodingagents"&gt;https://allenai.org/papers/opencodingagents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T19:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qofdc3</id>
    <title>tencent/Youtu-VL-4B-Instruct ¬∑ Hugging Face</title>
    <updated>2026-01-27T14:29:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qofdc3/tencentyoutuvl4binstruct_hugging_face/"&gt; &lt;img alt="tencent/Youtu-VL-4B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/ymzcxya4MbijdDp2b6xf6VUOGUPz9k8M8eOkIWZO9fk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bc2cda3e10198c3bb8f2e9efad0fe60e87895b4" title="tencent/Youtu-VL-4B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Youtu-VL&lt;/strong&gt; is a lightweight yet robust Vision-Language Model (VLM) built on the Youtu-LLM with 4B parameters. It pioneers Vision-Language Unified Autoregressive Supervision (VLUAS), which markedly strengthens visual perception and multimodal understanding. This enables a standard VLM to perform vision-centric tasks without task-specific additions. Across benchmarks, Youtu-VL stands out for its versatility, achieving competitive results on both vision-centric and general multimodal tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF"&gt;https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qofdc3/tencentyoutuvl4binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qofdc3/tencentyoutuvl4binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T14:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo349m</id>
    <title>deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face</title>
    <updated>2026-01-27T03:56:49+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face" src="https://external-preview.redd.it/c9LaruBvjfhr_AFkVVpu9jJ8NabAKdroEOMl2Akgn-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a35ff741fcd21d9b3346fefa618503befa19d18" title="deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T03:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo3ri5</id>
    <title>Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement</title>
    <updated>2026-01-27T04:26:25+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt; &lt;img alt="Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement" src="https://preview.redd.it/0qp4pz0fbtfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12b737e67556ca654785997ea815b78511476ed2" title="Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing Jan-v3-4B-base-instruct, a 4B-parameter model trained with &lt;strong&gt;continual pre-training&lt;/strong&gt; and &lt;strong&gt;RL&lt;/strong&gt;, to improve capabilities across common tasks while preserving other general capabilities.&lt;/p&gt; &lt;p&gt;What it‚Äôs for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A good starting point for further fine-tuning&lt;/li&gt; &lt;li&gt;Improved math and coding performance for lightweight assistance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to run it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jan Desktop&lt;/p&gt; &lt;p&gt;Download Jan Desktop: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt; and then download Jan v3 via Jan Hub. &lt;/p&gt; &lt;p&gt;Model links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v3-4B: &lt;a href="https://huggingface.co/Menlo/Jan-v3-4B-base-instruct"&gt;https://huggingface.co/janhq/Jan-v3-4B-base-instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan-v3-4B-GGUF: &lt;a href="https://huggingface.co/Menlo/Jan-v3-4B-base-instruct-gguf"&gt;https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recommended parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;temperature: 0.7&lt;/li&gt; &lt;li&gt;top_p: 0.8&lt;/li&gt; &lt;li&gt;top_k: 20&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What‚Äôs coming next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Jan-Code&lt;/strong&gt; (finetuned of Jan-v3-4B-base-instruct)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jan-v3-Seach-4B&lt;/strong&gt; (renewal of Jan-nano on Jan-v3-4B-base-instruct)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A 30B Jan-v3 family of models&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0qp4pz0fbtfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T04:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qol3s5</id>
    <title>[Preliminary] New subquadratic attention: ~20k tok/s prefill / ~100 tok/s decode @ 1M context (single GPU)</title>
    <updated>2026-01-27T17:54:19+00:00</updated>
    <author>
      <name>/u/Sad-Size2723</name>
      <uri>https://old.reddit.com/user/Sad-Size2723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;Wanted to share some preliminary feasibility results from my work on a new attention mechanism (with custom kernels) on NVIDIA Nemotron Nano v3 30B. I am now able to run 1M context on a single GPU with this setup, and the early throughput numbers look promising. &lt;/p&gt; &lt;p&gt;TL;DR: 30B model + 1M context on a single GPU, with a jump-search-style attention mechanism. (Manuscript link: &lt;a href="https://arxiv.org/abs/2601.18401"&gt;https://arxiv.org/abs/2601.18401&lt;/a&gt;) &lt;/p&gt; &lt;p&gt;Numbers (single batch/sequence; single GPU: NVIDIA B200, similar results on RTX PRO 6000 Blackwell):&lt;br /&gt; - &lt;strong&gt;~20,000 tok/s&lt;/strong&gt; prefill&lt;br /&gt; - &lt;strong&gt;~100 tok/s&lt;/strong&gt; decode at &lt;strong&gt;1M&lt;/strong&gt; context&lt;br /&gt; - &lt;strong&gt;66 GB&lt;/strong&gt; GPU memory (6GB KV cache + 60GB FP16 model)&lt;br /&gt; - perfect NIAH (needle in a haystack) at 256K context (limited training so far) &lt;/p&gt; &lt;p&gt;I have completed an initial feasibility study, and I'm continuing to train the model toward real production use. The plan is to fully open-source the model for local inference, with a target of running a fully filled 1M context for a 30B model locally on ~24GB GPU memory. I'm cleaning up the codebase and plan to release the kernel implementations soon. For the model itself, I'll share it once we feel good about long-context performance/quality. &lt;/p&gt; &lt;p&gt;(Just to be clear: these are early numbers, and quality/evals are still in progress.) &lt;/p&gt; &lt;p&gt;1) What‚Äôs the main idea &lt;/p&gt; &lt;p&gt;You can think about the transformer attention mechanism as a search algorithm to find the relevant information to predict the next token. Standard attention is basically O(L) brute-force search. We‚Äôre doing an O(L^0.5) jump-search-style approach instead. For example, if you 10x the context length, a sqrt(L) search budget only grows by ~3.2x. &lt;/p&gt; &lt;p&gt;That subquadratic scaling really matters for long context, since the cost still grows with L. The main innovation is keeping that scaling while still making sure every token is reachable (i.e., not a fixed sliding window; think ‚Äò&lt;strong&gt;global random access&lt;/strong&gt;‚Äô). Most likely in long context inference, a large fraction of long-context computation is wasted by brute-force scanning, and that if we are smart about it, we can compute it much more efficiently. &lt;/p&gt; &lt;p&gt;2) What's the goal &lt;/p&gt; &lt;p&gt;Targeting high-quality and fast (~100 tok/s) open-source local models at long context: &lt;/p&gt; &lt;p&gt;- 1M context on a 24GB GPU: ~6GB KV cache + ~15GB 4-bit quantized model&lt;br /&gt; - 10M context on a 96GB GPU: ~60GB KV cache + ~30GB 8-bit quantized model &lt;/p&gt; &lt;p&gt;Our initial feasibility results suggest we‚Äôre already in the right ballpark on inference speed. The main work now is scaling training and doing broader quality evals on real long-context tasks. I‚Äôm sure we‚Äôll hit obstacles as we scale up, but overall we feel this direction is achievable. &lt;/p&gt; &lt;p&gt;3) Questions/feedback &lt;/p&gt; &lt;p&gt;I‚Äôm a big fan of running models locally (work + teaching + personal projects). Before COVID I bought 4√ó 1070 Ti GPUs for some non-LLM stuff, and these days I mostly use an A6000 at home. I‚Äôm excited about this because it could make really long-context workflows practical without needing a cluster.&lt;/p&gt; &lt;p&gt;Would love feedback / sanity checks on a few things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What would you actually use 1M‚Äì10M context for locally? (offline search over docs, codebase-scale assistants, long-form editing, ‚Äúpersonal knowledge base‚Äù, etc.)&lt;/li&gt; &lt;li&gt;What evals would you trust most for long-context quality (beyond simple needle-in-a-haystack)?&lt;/li&gt; &lt;li&gt;What baselines should I compare against to make the speed/quality tradeoffs clear&lt;/li&gt; &lt;li&gt;What would make an open-source release most useful to you (kernels only vs full inference stack vs training code/configs)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I kept this post high-level, but happy to go deeper if there‚Äôs interest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Size2723"&gt; /u/Sad-Size2723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T17:54:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoi1jc</id>
    <title>SERA 8B/32B</title>
    <updated>2026-01-27T16:08:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"&gt; &lt;img alt="SERA 8B/32B" src="https://a.thumbs.redditmedia.com/aiyHVH_H-noFKcxtqbQ2Vb5ZhuqTajFG0oLHTJx9LW8.jpg" title="SERA 8B/32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/of9u5blh1xfg1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf11d0dc7016f0fadeee4eea761c68d7fed48098"&gt;https://preview.redd.it/of9u5blh1xfg1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf11d0dc7016f0fadeee4eea761c68d7fed48098&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allenai/SERA-32B"&gt;https://huggingface.co/allenai/SERA-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allenai/SERA-32B-GA"&gt;https://huggingface.co/allenai/SERA-32B-GA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allenai/SERA-8B-GA"&gt;https://huggingface.co/allenai/SERA-8B-GA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ykqidl1c1xfg1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b78c42146c0984889cd81cb6391cf3a03f061a5a"&gt;https://preview.redd.it/ykqidl1c1xfg1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b78c42146c0984889cd81cb6391cf3a03f061a5a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo7wur</id>
    <title>OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances</title>
    <updated>2026-01-27T08:09:28+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"&gt; &lt;img alt="OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances" src="https://external-preview.redd.it/v44P77PnYI5tRIZpnmaNJbBahgYNI024hkyMrYI6J24.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=147c3b3cde07c4eba8b8831f89c4a1dbc6ec6942" title="OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new financial analysis predicts OpenAI could burn through its cash reserves by mid-2027. The report warns that Sam Altman‚Äôs '$100 billion Stargate' strategy is hitting a wall: training costs are exploding, but revenue isn't keeping up. With Chinese competitors like DeepSeek now offering GPT-5 level performance for 95% less cost, OpenAI‚Äôs 'moat' is evaporating faster than expected. If AGI doesn't arrive to save the economics, the model is unsustainable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T08:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qocvd4</id>
    <title>built an AI agent with shell access. found out the hard way why that's a bad idea.</title>
    <updated>2026-01-27T12:46:00+00:00</updated>
    <author>
      <name>/u/YogurtIll4336</name>
      <uri>https://old.reddit.com/user/YogurtIll4336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;was building a tool to let claude/gpt4 navigate my codebase. gave it bash access, seemed fine.&lt;/p&gt; &lt;p&gt;then i tried asking it to &amp;quot;check imports and make ascii art from my env file&amp;quot;&lt;/p&gt; &lt;p&gt;it did both. printed my api keys as art.&lt;/p&gt; &lt;p&gt;went down a rabbit hole reading about this. turns out prompt injection is way worse than i thought:&lt;/p&gt; &lt;p&gt;anthropic has a whole page on it but it's pretty surface level&lt;/p&gt; &lt;p&gt;found this practical writeup from some YC startup that actually tested bypasses: &lt;a href="https://www.codeant.ai/blogs/agentic-rag-shell-sandboxing"&gt;https://www.codeant.ai/blogs/agentic-rag-shell-sandboxing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;simon willison has been screaming about this for months (&lt;a href="https://simonwillison.net/series/prompt-injection/"&gt;https://simonwillison.net/series/prompt-injection/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;apparently docker shared kernel isn't enough. gvisor adds overhead. firecracker seems like overkill but it's what aws lambda uses so... maybe not? stuck between &amp;quot;ship it and hope&amp;quot; vs &amp;quot;burn 2 weeks adding proper isolation&amp;quot;&lt;/p&gt; &lt;p&gt;has anyone actually solved this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YogurtIll4336"&gt; /u/YogurtIll4336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T12:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qob8de</id>
    <title>Honest question: what do you all do for a living to afford these beasts?</title>
    <updated>2026-01-27T11:23:48+00:00</updated>
    <author>
      <name>/u/ready_to_fuck_yeahh</name>
      <uri>https://old.reddit.com/user/ready_to_fuck_yeahh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically I am from India, a medium high end job here pays Rs. 1 lakh($ 1100) per month and there are deductions on top of it.&lt;/p&gt; &lt;p&gt;An RTX Pro 6000 starts from 8 lakh and goes upto 10 lakh($ 10989), 5090 costs 3.5 lakhs($ 3800), threadripper costs 7-8 lakhs($ 8800), ram prices have soared and corsair vengeance costs 52,000 ($ 571) for 32GB, motherboard, cabinet, and other accessories makes it look like a dream to own in a lifetime. And people here are using multi gpu setup, recently saw 4xrtx 6000 pro setup here.&lt;/p&gt; &lt;p&gt;Been seeing a lot of beautiful multi-GPU setups here and I'm genuinely curious about the community makeup.&lt;/p&gt; &lt;p&gt;Are most of you:&lt;/p&gt; &lt;p&gt;Software engineers / AI researchers (expensing to employer or side business)?&lt;/p&gt; &lt;p&gt;Serious hobbyists with high-paying day jobs?&lt;/p&gt; &lt;p&gt;Consultants/freelancers writing off hardware?&lt;/p&gt; &lt;p&gt;Something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ready_to_fuck_yeahh"&gt; /u/ready_to_fuck_yeahh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T11:23:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoml1n</id>
    <title>[LEAKED] Kimi K2.5‚Äôs full system prompt + tools (released &lt;24h ago)</title>
    <updated>2026-01-27T18:44:50+00:00</updated>
    <author>
      <name>/u/Pretty_Mountain2714</name>
      <uri>https://old.reddit.com/user/Pretty_Mountain2714</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My first post on LLAMA‚Ä¶&lt;/p&gt; &lt;p&gt;Was messing around with Moonshot's new Kimi K2.5 and I think I pulled the whole system prompt + tools lol. (~5k tk)&lt;/p&gt; &lt;p&gt;Got hyped I grabbed this so fast cause usually someone posts this stuff way before I get to it&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/dnnyngyen/kimi-k2.5-prompts-tools"&gt;https://github.com/dnnyngyen/kimi-k2.5-prompts-tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contents:&lt;/p&gt; &lt;p&gt;- full system prompt&lt;br /&gt; - all tool schemas + instructions&lt;br /&gt; - memory CRUD protocols&lt;br /&gt; - context engineering + assembling user profile&lt;br /&gt; - basic guardrails/rules&lt;br /&gt; - external datasource integrations (finance, arxiv, etc)&lt;/p&gt; &lt;p&gt;My og chat: &lt;a href="https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b"&gt;https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b&lt;/a&gt; (never had a model fold this easily lmao)&lt;/p&gt; &lt;p&gt;Sharing it here first &amp;lt;3&lt;br /&gt; Happy to be able to contribute sum to this community&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pretty_Mountain2714"&gt; /u/Pretty_Mountain2714 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T18:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoikji</id>
    <title>Drummer's Rocinante X 12B v1 - It's back and it's stronger than ever! A funtastic creative Claude-like RP model at home!</title>
    <updated>2026-01-27T16:27:12+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoikji/drummers_rocinante_x_12b_v1_its_back_and_its/"&gt; &lt;img alt="Drummer's Rocinante X 12B v1 - It's back and it's stronger than ever! A funtastic creative Claude-like RP model at home!" src="https://external-preview.redd.it/vMt_thwlfOcsDVODCeD_E13hrou5XcQcrAzk35oTLtU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad8c88d8902c9e03aa9ee1c38b6f2620bca2b91" title="Drummer's Rocinante X 12B v1 - It's back and it's stronger than ever! A funtastic creative Claude-like RP model at home!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rocinante-X-12B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoikji/drummers_rocinante_x_12b_v1_its_back_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoikji/drummers_rocinante_x_12b_v1_its_back_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo595n</id>
    <title>Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence</title>
    <updated>2026-01-27T05:39:09+00:00</updated>
    <author>
      <name>/u/Kimi_Moonshot</name>
      <uri>https://old.reddit.com/user/Kimi_Moonshot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîπ&lt;strong&gt;Global SOTA on Agentic Benchmarks&lt;/strong&gt;: HLE full set (50.2%), BrowseComp (74.9%) &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Open-source SOTA on Vision and Coding&lt;/strong&gt;: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%) &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Code with Taste&lt;/strong&gt;: turn chats, images &amp;amp; videos into aesthetic websites with expressive motion. &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Agent Swarm (Beta)&lt;/strong&gt;: self-directed agents working in parallel, at scale. Up to &lt;strong&gt;100&lt;/strong&gt; sub-agents, &lt;strong&gt;1,500&lt;/strong&gt; tool calls, &lt;strong&gt;4.5√ó&lt;/strong&gt; faster compared with single-agent setup. &lt;/p&gt; &lt;p&gt;ü•ù&lt;strong&gt;K2.5&lt;/strong&gt; is now live on &lt;a href="https://t.co/YutVbwktG0"&gt;http://kimi.com&lt;/a&gt; in &lt;strong&gt;chat mod&lt;/strong&gt;e and &lt;strong&gt;agent mode&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;ü•ù&lt;strong&gt;K2.5 Agent Swarm&lt;/strong&gt; in beta for high-tier users. &lt;/p&gt; &lt;p&gt;ü•ùFor production-grade coding, you can pair K2.5 with &lt;strong&gt;Kim&lt;/strong&gt;i Code: &lt;a href="https://t.co/A5WQozJF3s"&gt;https://kimi.com/code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóAPI: &lt;a href="https://t.co/EOZkbOwCN4"&gt;https://platform.moonshot.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóTech blog: &lt;a href="https://www.kimi.com/blog/kimi-k2-5.html"&gt;https://www.kimi.com/blog/kimi-k2-5.html&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üîóWeights &amp;amp; code: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2.5"&gt;https://huggingface.co/moonshotai/Kimi-K2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412"&gt;https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kimi_Moonshot"&gt; /u/Kimi_Moonshot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T05:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qonyhc</id>
    <title>Benchmark of Qwen3-32B reveals 12x capacity gain at INT4 with only 1.9% accuracy drop</title>
    <updated>2026-01-27T19:32:13+00:00</updated>
    <author>
      <name>/u/AIMultiple</name>
      <uri>https://old.reddit.com/user/AIMultiple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We ran 12,000+ MMLU-Pro questions and 2,000 inference runs to settle the quantization debate. INT4 serves 12x more users than BF16 while keeping 98% accuracy.&lt;/p&gt; &lt;p&gt;Benchmarked Qwen3-32B across BF16/FP8/INT8/INT4 on a single H100. The memory savings translate directly to concurrent user capacity. Went from 4 users (BF16) to 47 users (INT4) at 4k context. Full methodology and raw numbers here: &lt;a href="https://research.aimultiple.com/llm-quantization/"&gt;https://research.aimultiple.com/llm-quantization/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIMultiple"&gt; /u/AIMultiple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qonyhc/benchmark_of_qwen332b_reveals_12x_capacity_gain/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qonyhc/benchmark_of_qwen332b_reveals_12x_capacity_gain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qonyhc/benchmark_of_qwen332b_reveals_12x_capacity_gain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T19:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoa8rp</id>
    <title>The Qwen Devs Are Teasing Something</title>
    <updated>2026-01-27T10:28:56+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"&gt; &lt;img alt="The Qwen Devs Are Teasing Something" src="https://preview.redd.it/umvks92vcvfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=297a92382cbb71a347dd9192a2d8ae1054cf9fb2" title="The Qwen Devs Are Teasing Something" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm going to assume a new VL model&lt;/p&gt; &lt;p&gt;Edit: It's likely to be Z-Image&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/umvks92vcvfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T10:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoiep6</id>
    <title>The z-image base is here!</title>
    <updated>2026-01-27T16:21:29+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image"&gt;https://huggingface.co/Tongyi-MAI/Z-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
