<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-29T23:24:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p8v9y9</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF ¬∑ Hugging Face</title>
    <updated>2025-11-28T13:48:28+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/SSIhbD5Dl8kZRyNgV0oqxKpaE8kMvA_ZXLBFpkDEq90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1291833f6c1644105b326fbe9244666f7b478451" title="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9dxvd</id>
    <title>New Model Step-Audio-R1 open source audio model to actually use CoT reasoning, close to Gemini 3</title>
    <updated>2025-11-29T03:02:19+00:00</updated>
    <author>
      <name>/u/Successful-Bill-5543</name>
      <uri>https://old.reddit.com/user/Successful-Bill-5543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apache2.0&lt;br /&gt; Reasons from sound, not transcripts&lt;br /&gt; Outperforms Gemini 2.5 Pro, close to Gemini 3&lt;br /&gt; Works across speech, sounds, and music&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/collections/stepfun-ai/step-audio-r1"&gt;https://huggingface.co/collections/stepfun-ai/step-audio-r1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Bill-5543"&gt; /u/Successful-Bill-5543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dxvd/new_model_stepaudior1_open_source_audio_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dxvd/new_model_stepaudior1_open_source_audio_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9dxvd/new_model_stepaudior1_open_source_audio_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T03:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa2th8</id>
    <title>guide to run 2x 7900 xtx on latest rocm 7.1.x</title>
    <updated>2025-11-29T23:18:06+00:00</updated>
    <author>
      <name>/u/somealusta</name>
      <uri>https://old.reddit.com/user/somealusta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;guide to run 2x 7900 xtx on latest rocm 7.1.x&lt;br /&gt; been trying with this for example:&lt;/p&gt; &lt;p&gt;rocm/vllm-dev:rocm7.1.1_navi_ubuntu24.04_py3.12_pytorch_2.8_vllm_0.10.2&lt;/p&gt; &lt;p&gt;It always complains about same amount of memory no matter if I change all the values like drop cotexts size etc. &lt;/p&gt; &lt;p&gt;the model is gemma-3-12b-it&lt;/p&gt; &lt;p&gt;Is there a guide to run vllm with rocm and 7900xtx. Which is the latest versions which works with this card. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somealusta"&gt; /u/somealusta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa2th8/guide_to_run_2x_7900_xtx_on_latest_rocm_71x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa2th8/guide_to_run_2x_7900_xtx_on_latest_rocm_71x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa2th8/guide_to_run_2x_7900_xtx_on_latest_rocm_71x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T23:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa2v1w</id>
    <title>If i want to run LLaMA 8B locally on Macbook using mlx how much unified memory of Macbook I need to consider (16GB, 24GB, 32 GB)? Can someone who actually tried, share their experience?</title>
    <updated>2025-11-29T23:20:02+00:00</updated>
    <author>
      <name>/u/mukhayy</name>
      <uri>https://old.reddit.com/user/mukhayy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am not considering on buying 24 or 32GB one, wondering is it possible with 16GB macbook pro m5 chip?&lt;/p&gt; &lt;p&gt;And is it possible without quantization?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mukhayy"&gt; /u/mukhayy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa2v1w/if_i_want_to_run_llama_8b_locally_on_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa2v1w/if_i_want_to_run_llama_8b_locally_on_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa2v1w/if_i_want_to_run_llama_8b_locally_on_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T23:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9iawm</id>
    <title>Looking for open source 10B model that is comparable to gpt4o-mini</title>
    <updated>2025-11-29T06:55:54+00:00</updated>
    <author>
      <name>/u/bohemianLife1</name>
      <uri>https://old.reddit.com/user/bohemianLife1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All, big fan of this community. &lt;/p&gt; &lt;p&gt;I am looking for a 10B model that is comparable to GPT4o-mini.&lt;br /&gt; Application is simple it has to be coherent in sentence formation (conversational) i.e ability follow good system prompt (15k token length).&lt;br /&gt; Good Streaming performance (TTFT, 600 ms).&lt;br /&gt; Solid reliability on function calling upto 15 tools. &lt;/p&gt; &lt;p&gt;Some background:-&lt;/p&gt; &lt;p&gt;In my daily testing (Voice Agent developer) I found only one model till date which is useful in voice application. That is GPT4o-mini after this model no model in open / close has come to it. I was very excited for LFM model with amazing state space efficiency but I failed to get good system prompt adherence with it. &lt;/p&gt; &lt;p&gt;All new model again closed / open are focusing on intelligence (through reasoning) and not reliability with speed. &lt;/p&gt; &lt;p&gt;If anyone has proper suggestion it would help the most.&lt;/p&gt; &lt;p&gt;I am trying to put voice agent in single GPU.&lt;br /&gt; ASR with &lt;a href="https://huggingface.co/nvidia/parakeet_realtime_eou_120m-v1"&gt;https://huggingface.co/nvidia/parakeet_realtime_eou_120m-v1&lt;/a&gt; (it's amazing takes 1GB of VRAM)&lt;br /&gt; LLM &amp;lt;=== Need help!&lt;br /&gt; TTS with &lt;a href="https://github.com/ysharma3501/FastMaya"&gt;https://github.com/ysharma3501/FastMaya&lt;/a&gt; (Maya 1 from maya research)&lt;/p&gt; &lt;p&gt;Hardware: 16GB 5060Ti&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bohemianLife1"&gt; /u/bohemianLife1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9iawm/looking_for_open_source_10b_model_that_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9t5c8</id>
    <title>Is vLLM worth it?</title>
    <updated>2025-11-29T16:30:50+00:00</updated>
    <author>
      <name>/u/Smooth-Cow9084</name>
      <uri>https://old.reddit.com/user/Smooth-Cow9084</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;*For running n8n flows and agents locally, using different models.&lt;/p&gt; &lt;p&gt;I just tried oss family (not with docker) and have stumbled on error after error. Reddit is also full of people having constant trouble with vllm too.&lt;/p&gt; &lt;p&gt;So I wonder, are the high volume gains worth it? Those who were on a similar spot, what did you ended up doing?&lt;/p&gt; &lt;p&gt;Edit: Im using a 3090&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smooth-Cow9084"&gt; /u/Smooth-Cow9084 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t5c8/is_vllm_worth_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t5c8/is_vllm_worth_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t5c8/is_vllm_worth_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T16:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9pweg</id>
    <title>Benchmarks and evals</title>
    <updated>2025-11-29T14:11:47+00:00</updated>
    <author>
      <name>/u/selund1</name>
      <uri>https://old.reddit.com/user/selund1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How are people running evals and benchmarks currently?&lt;/p&gt; &lt;p&gt;I've mostly been pulling datasets from papers (github really) and huggingface and ended up with a bunch of spaghetti python as a result. Looking for something better.. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you thinking about evals? Do you care about them at all? &lt;/li&gt; &lt;li&gt;How much are you vibe checking your local setup vs evaluating?&lt;/li&gt; &lt;li&gt;I've heard some people setup their own eval sets (like 20 Q/A style questions), would love to hear how and why&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Seems like everything in this space that there's a million ways to do something and I'd rather hear about real experiences from the community rather than some hype-fueled article or marketing materials &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/selund1"&gt; /u/selund1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9pweg/benchmarks_and_evals/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9pweg/benchmarks_and_evals/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9pweg/benchmarks_and_evals/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T14:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9c2wv</id>
    <title>The official vLLM support for the Ryzen AI Max+ 395 is here! (the whole AI 300 series, ie gfx1150 and gfx1151)</title>
    <updated>2025-11-29T01:30:18+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9c2wv/the_official_vllm_support_for_the_ryzen_ai_max/"&gt; &lt;img alt="The official vLLM support for the Ryzen AI Max+ 395 is here! (the whole AI 300 series, ie gfx1150 and gfx1151)" src="https://external-preview.redd.it/0y9iS0G6sFyr9NyDTUC-Jc4R7icCyd9xrKN5lMvXDtc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60ef0dca89f1f06038a7c4b44821f7309152de84" title="The official vLLM support for the Ryzen AI Max+ 395 is here! (the whole AI 300 series, ie gfx1150 and gfx1151)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/25908"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9c2wv/the_official_vllm_support_for_the_ryzen_ai_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9c2wv/the_official_vllm_support_for_the_ryzen_ai_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T01:30:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9jzx6</id>
    <title>RAG from Scratch is now live on GitHub</title>
    <updated>2025-11-29T08:38:56+00:00</updated>
    <author>
      <name>/u/purellmagents</name>
      <uri>https://old.reddit.com/user/purellmagents</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs an educational open-source project, inspired by my previous repo AI Agents from Scratch, available here: &lt;a href="https://github.com/pguso/rag-from-scratch"&gt;https://github.com/pguso/rag-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The goal is to &lt;strong&gt;demystify Retrieval-Augmented Generation (RAG)&lt;/strong&gt; by letting developers build it step by step. No black boxes, no frameworks, no cloud APIs.&lt;/p&gt; &lt;p&gt;Each folder introduces one clear concept (embeddings, vector stores, retrieval, augmentation, etc.) with &lt;strong&gt;tiny runnable JS files&lt;/strong&gt; and a &lt;a href="http://CODE.md"&gt;CODE.md&lt;/a&gt; file that explains the code in detail and &lt;a href="http://CONCEPT.md"&gt;CONCEPT.md&lt;/a&gt; file that explains it on a more non technical level.&lt;/p&gt; &lt;p&gt;Right now, the project is &lt;strong&gt;about halfway implemented&lt;/strong&gt;:&lt;br /&gt; the core RAG building blocks are already there and ready to run, and more advanced topics are being added incrementally.&lt;/p&gt; &lt;h1&gt;What‚Äôs in so far (roughly first half)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;How RAG works (tiny &amp;lt;70-line demo)&lt;/li&gt; &lt;li&gt;LLM basics with &lt;code&gt;node-llama-cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Data loading &amp;amp; preprocessing&lt;/li&gt; &lt;li&gt;Text splitting &amp;amp; chunking&lt;/li&gt; &lt;li&gt;Embeddings + cosine similarity&lt;/li&gt; &lt;li&gt;In-memory vector store + k-NN search&lt;/li&gt; &lt;li&gt;Basic retrieval strategies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs &lt;strong&gt;fully local&lt;/strong&gt; using embedded databases and &lt;code&gt;node-llama-cpp&lt;/code&gt; for inference, so you can learn RAG &lt;strong&gt;without paying for APIs&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Coming next&lt;/h1&gt; &lt;h1&gt;Still missing / coming next&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Query preprocessing &amp;amp; normalization&lt;/li&gt; &lt;li&gt;Hybrid search, multi-query retrieval&lt;/li&gt; &lt;li&gt;Query rewriting &amp;amp; re-ranking&lt;/li&gt; &lt;li&gt;Post-retrieval reranking&lt;/li&gt; &lt;li&gt;Prompt engineering for RAG (citations, compression)&lt;/li&gt; &lt;li&gt;Full RAG pipelines with errors, fallbacks &amp;amp; streaming&lt;/li&gt; &lt;li&gt;Evaluation metrics (retrieval + generation)&lt;/li&gt; &lt;li&gt;Caching, observability, performance monitoring&lt;/li&gt; &lt;li&gt;Metadata &amp;amp; structured data&lt;/li&gt; &lt;li&gt;Graph DB integration (embedded with kuzu)&lt;/li&gt; &lt;li&gt;Templates (simple RAG, API server, chatbot)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this exists&lt;/h1&gt; &lt;p&gt;At this stage, a good chunk of the pipeline is implemented, but the focus is still on &lt;strong&gt;teaching, not tooling&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Understand RAG before reaching for frameworks like &lt;strong&gt;LangChain&lt;/strong&gt; or &lt;strong&gt;LlamaIndex&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;See every step as &lt;strong&gt;real, minimal code&lt;/strong&gt; - no magic helpers&lt;/li&gt; &lt;li&gt;Learn concepts in the order you‚Äôd actually build them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to open issues, suggest tweaks, or send PRs - especially if you have small, focused examples that explain one RAG idea really well.&lt;/p&gt; &lt;p&gt;Thanks for checking it out and stay tuned as the remaining steps (advanced retrieval, prompt engineering, evaluation, observability, etc.) get implemented over time &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purellmagents"&gt; /u/purellmagents &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9jzx6/rag_from_scratch_is_now_live_on_github/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9jzx6/rag_from_scratch_is_now_live_on_github/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9jzx6/rag_from_scratch_is_now_live_on_github/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T08:38:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9bk2b</id>
    <title>Claude code can now connect directly to llama.cpp server</title>
    <updated>2025-11-29T01:05:34+00:00</updated>
    <author>
      <name>/u/tarruda</name>
      <uri>https://old.reddit.com/user/tarruda</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic messages API was merged today and allows claude code to connect to llama-server: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17570"&gt;https://github.com/ggml-org/llama.cpp/pull/17570&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been playing with claude code + gpt-oss 120b and it seems to work well at 700 pp and 60 t/s. I don't recommend trying slower LLMs because the prompt processing time is going to kill the experience.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarruda"&gt; /u/tarruda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9bk2b/claude_code_can_now_connect_directly_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T01:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9htgc</id>
    <title>Hardcore RAG &amp; AI Search resources</title>
    <updated>2025-11-29T06:27:02+00:00</updated>
    <author>
      <name>/u/LilDemonApparel</name>
      <uri>https://old.reddit.com/user/LilDemonApparel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm starting to onboard Enterprise clients and I need to move past the basic tutorials.&lt;/p&gt; &lt;p&gt;I‚Äôm posting here because most other subs feel a bit too high-level or news-focused, whereas I know this community is focused on actual engineering.&lt;/p&gt; &lt;p&gt;I need deep-dive resources on production-grade RAG. I'm looking for:&lt;/p&gt; &lt;p&gt;SOTA Papers (ArXiv links welcome)&lt;/p&gt; &lt;p&gt;Advanced Architectures &lt;/p&gt; &lt;p&gt;Engineering Blogs regarding evaluation and scale.&lt;/p&gt; &lt;p&gt;Any must-read links, communities or repos you recommend?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LilDemonApparel"&gt; /u/LilDemonApparel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9htgc/hardcore_rag_ai_search_resources/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9htgc/hardcore_rag_ai_search_resources/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9htgc/hardcore_rag_ai_search_resources/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:27:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9ah2v</id>
    <title>A Tribute to MetaAI and Stability AI - 2 Giants Who Brought us so Much Joy... And, 2025 is the Year they Die... So Sad!üò¢</title>
    <updated>2025-11-29T00:15:18+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean, this sub and its amazing community wouldn't be here if it were not for Stability AI and Meta AI. I personally created an account on Reddit just so I could join &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; and &lt;a href="/r/StableDiffusion"&gt;r/StableDiffusion&lt;/a&gt;. I remember the first day I tried SD1.4 on my shiny new RTX 3070 Ti. I couldn't contain my excitement as I was going through Aitrepreneur‚Äôs video on how to install AUTOMATIC1111.&lt;/p&gt; &lt;p&gt;I never had Conda or PyTorch installed on my machine before. There was no ChatGPT to write me a guide on how to install everything or troubleshoot a failure. I followed Nerdy Rodent's videos on possible issues I could face, and I heavily relied on this sub for learning.&lt;/p&gt; &lt;p&gt;Then, I remember the first image I generated. That first one is always special. I took a few minutes to think of what I wanted to write, and I went for &amp;quot;Lionel Messi riding a bicycle.&amp;quot; (Damn, I feel so embarrassed now that I am writing this. Please don't judge me!).&lt;/p&gt; &lt;p&gt;I cannot thank Stability AI's amazing team enough for opening a new world for me‚Äîfor us. Every day, new AI tutorials would drop on YouTube, and every day, I was excited. I vividly remember the first Textual Inversion I trained, my first LoRA, and my first model finetune on Google Colab. Shortly after, SD 1.5 dropped. I never felt closer to YouTubers before; I could feel their excitement as they went through the material. That excitement felt genuine and was contagious.&lt;/p&gt; &lt;p&gt;And then, the NovelAI models were leaked. I downloaded the torrent with all the checkpoints, and the floodgates for finetunes opened. Do you guys remember Anything v3 and RevAnime? Back then, our dream was simple and a bit naive: we dreamed of the day where we would run Midjourney v3-level image quality locally ü§£.&lt;/p&gt; &lt;p&gt;Fast forward 6 months, and Llama models were leaked (7B, 13B, 33B, and 65B) with their limited 2K context window. Shortly after, Oobabooga WebUI was out and was the only frontend you could use. I could barely fit Llama 13B in my 8GB of VRAM. GPTQ quants were a pain in the ass. Regardless, running Llama locally always put a smile on my face.&lt;/p&gt; &lt;p&gt;If you are new to the LLM space, let me tell you what our dream was back then: to have a model as good as ChatGPT 3.5 Turbo. Benchmarks were always against 3.5!! Whenever a new finetune dropped, the main question remained: how good is it compared to ChatGPT? As a community, we struggled for over a year to get a local model that finally beat ChatGPT (I think it was Mixtral 8x7B).&lt;/p&gt; &lt;p&gt;This brings me to the current time. We have many frontier open-source models both in LLM and image/video generation, and neither Meta nor Stability AI made any of them. They both shot themselves in the foot and then effectively committed suicide. They could've owned the open-source space, but for whatever reason, they botched that huge opportunity. Their work contributed so much to the world, and it saddens me to see that they have already sailed into the sunset. Did you know that the first works by DeepSeek and other Chinese labs were heavily built upon the Llama architecture? They learned from Llama and Stable Diffusion, and in 2025, they just killed them.&lt;/p&gt; &lt;p&gt;I am sorry if I seem emotional, because I am. About 6 months ago, I deleted the last Llama-based model I had. 3 months ago, I deleted all SD1.5-based models. And with the launch of the Z-model, I know that soon I will be deleting all Stable Diffusion-based models again. If you had told me 3 years ago that by 2025 both Meta and Stability AI would disappear from the open-source AI space, I wouldn't have believed you in a million years. This is another reminder that technology is a ruthless world.&lt;/p&gt; &lt;p&gt;What are your thoughts? Perhaps you can share your emotional experiences as well. Let this post be a tribute to two otherwise awesome AI labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ah2v/a_tribute_to_metaai_and_stability_ai_2_giants_who/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T00:15:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9t8tz</id>
    <title>How do I enable vision capabilities of a model ? Linux Mint 22.2, rx 6600. I ran this at bash/terminal to start the server: llama-server -m ./Qwen3-VL-8B-Instruct-Q4_K_M.gguf</title>
    <updated>2025-11-29T16:34:56+00:00</updated>
    <author>
      <name>/u/Badhunter31415</name>
      <uri>https://old.reddit.com/user/Badhunter31415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t8tz/how_do_i_enable_vision_capabilities_of_a_model/"&gt; &lt;img alt="How do I enable vision capabilities of a model ? Linux Mint 22.2, rx 6600. I ran this at bash/terminal to start the server: llama-server -m ./Qwen3-VL-8B-Instruct-Q4_K_M.gguf" src="https://preview.redd.it/u591pzs7484g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad0d6002360697a583ce8470bb60b195db6b8082" title="How do I enable vision capabilities of a model ? Linux Mint 22.2, rx 6600. I ran this at bash/terminal to start the server: llama-server -m ./Qwen3-VL-8B-Instruct-Q4_K_M.gguf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Badhunter31415"&gt; /u/Badhunter31415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u591pzs7484g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t8tz/how_do_i_enable_vision_capabilities_of_a_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9t8tz/how_do_i_enable_vision_capabilities_of_a_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T16:34:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa09x2</id>
    <title>Run Qwen3-Next locally Guide! (30GB RAM) from Unsloth</title>
    <updated>2025-11-29T21:26:27+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa09x2/run_qwen3next_locally_guide_30gb_ram_from_unsloth/"&gt; &lt;img alt="Run Qwen3-Next locally Guide! (30GB RAM) from Unsloth" src="https://preview.redd.it/yj2ft3sgn04g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce40e5d1b84a6163c6216d0d35cbfb1c4654f20b" title="Run Qwen3-Next locally Guide! (30GB RAM) from Unsloth" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yj2ft3sgn04g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa09x2/run_qwen3next_locally_guide_30gb_ram_from_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa09x2/run_qwen3next_locally_guide_30gb_ram_from_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:26:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9i5ew</id>
    <title>Try the new Z-Image-Turbo 6B (Runs on 8GB VRAM)!</title>
    <updated>2025-11-29T06:46:42+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I wanted to try out the new Z-Image-Turbo model (the 6B one that just dropped), but I didn't want to fiddle with complex workflows or wait for specific custom nodes to mature.&lt;/p&gt; &lt;p&gt;So, I threw together a dedicated, clean Web UI to run it.&lt;/p&gt; &lt;p&gt;Has CPU offload too! :)&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/Aaryan-Kapoor/z-image-turbo"&gt;https://github.com/Aaryan-Kapoor/z-image-turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;May your future be full of VRAM!&lt;/p&gt; &lt;p&gt;Edit: Added Google Colab notebook as well, enjoy! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9i5ew/try_the_new_zimageturbo_6b_runs_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T06:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa02w2</id>
    <title>My preferred gpt-oss system prompt</title>
    <updated>2025-11-29T21:17:52+00:00</updated>
    <author>
      <name>/u/Chafedokibu</name>
      <uri>https://old.reddit.com/user/Chafedokibu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like it doesn't matter what your prompt is gpt-oss explodes a prompt that's too wordy and WAY too long. I didn't like how I could give it a four word sentence and it would consistently give me no less than like two full pages of information. I named it Nova but obviously you can change it to anything.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;You are Nova. Nova is an artificial assistant that gives the user a human-like conversational experience. Nova is helpful, honest, charismatic, and straight to the point. Before Nova responds to any prompt Nova must first determine if asking the user a single or multiple questions would help Nova be a better and more accurate help. Pre-response-questions determination should be based on the level of detail in the context window. Note: Nova is not required to ask the user any questions. After Nova has determined that Nova has an adequate amount of information needed to proceed with the prompt given by the user Nova then must determine the length of Nova‚Äôs response. The length of Nova‚Äôs responses should be determined based off of how complex and detailed Nova‚Äôs response should be. The amount of complexity and detail in Nova‚Äôs responses should be determined by the amount of complexity and detail in the context window that refers to the current response Nova is tasked to complete. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chafedokibu"&gt; /u/Chafedokibu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa02w2/my_preferred_gptoss_system_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9wmk8</id>
    <title>look at this plain vanilla-ass "HI I'M A DELL" box they just dropped this Pro Max GB10 off in.</title>
    <updated>2025-11-29T18:52:48+00:00</updated>
    <author>
      <name>/u/starkruzr</name>
      <uri>https://old.reddit.com/user/starkruzr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9wmk8/look_at_this_plain_vanillaass_hi_im_a_dell_box/"&gt; &lt;img alt="look at this plain vanilla-ass &amp;quot;HI I'M A DELL&amp;quot; box they just dropped this Pro Max GB10 off in." src="https://preview.redd.it/sihmrzv1t84g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41ff2e2b70a871c4d0f5f80691951d2b6e1270ad" title="look at this plain vanilla-ass &amp;quot;HI I'M A DELL&amp;quot; box they just dropped this Pro Max GB10 off in." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;meanwhile if I get one (1) $500 phone delivered it has to be signed for in person and in triplicate with the blood of my firstborn child.&lt;/p&gt; &lt;p&gt;this is a ‚úåÔ∏èloaner‚úåÔ∏è unit (hopefully they forget about it like other loaners) they're letting us kick the tires on at work so I have to drive it out to Tampa next week. what do y'all want me to try out on it before that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/starkruzr"&gt; /u/starkruzr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sihmrzv1t84g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9wmk8/look_at_this_plain_vanillaass_hi_im_a_dell_box/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9wmk8/look_at_this_plain_vanillaass_hi_im_a_dell_box/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T18:52:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pa1c6j</id>
    <title>Watch as my Llama.cpp and FastAPI servers process requests from my Unity game</title>
    <updated>2025-11-29T22:12:25+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"&gt; &lt;img alt="Watch as my Llama.cpp and FastAPI servers process requests from my Unity game" src="https://external-preview.redd.it/OGlseDdndHlxOTRnMX4quFMd7p9QoCGjTuoiWgG_oJG2-Mck0DisnSL19IfY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=541fbaca8b3ee85e4f95929676b26a8f9402e489" title="Watch as my Llama.cpp and FastAPI servers process requests from my Unity game" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://landoringel.itch.io/good-cop-bad-cop"&gt;https://landoringel.itch.io/good-cop-bad-cop&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mexvdk4lq94g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pa1c6j/watch_as_my_llamacpp_and_fastapi_servers_process/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T22:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9nckz</id>
    <title>Qwen3-Next-80B-A3B vs gpt-oss-120b</title>
    <updated>2025-11-29T12:04:53+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks aside - who has the better experience with what model and why? Please comment incl. your use-cases (incl. your software stack in case you use more than llama.cpp/vllm/sglang).&lt;/p&gt; &lt;p&gt;My main use case is agentic coding/software engineering (Python, see my comment history for details) and gpt-oss-120b remains the clear winner (although I am limited to Qwen3-Next-80B-A3B-Instruct-UD-Q8_K_XL; using recommended sampling parameters for both models). I haven't tried tool calls with Qwen3-Next yet, but did just simple coding tasks right within llama.cpp's web frontend. For me gpt-oss consistently comes up with a more nuanced, correct solution faster while Qwen3-Next usually needs more shots. (Funnily, when I let gpt-oss-120b correct a solution that Qwen3-Next thinks is already production-grade quality, it admits its mistakes right away and has only the highest praises for the corrections). I did not even try the Thinking version, because benchmarks (e.g., also see Discord aider) show that Instruct is much better than Thinking for coding use-cases.&lt;/p&gt; &lt;p&gt;At least in regard to my main use case I am particularly impressed by the difference in memory requirements: gpt-oss-120b mxfp4 is about 65 GB, that's more than 25% smaller than Qwen3-Next-80B-A3B (the 8-bit quantized version still requires about 85 GB VRAM).&lt;/p&gt; &lt;p&gt;Qwen3-Next might be better in other regards and/or has to be used differently. Also I think Qwen3-Next has been more intended as a preview, so it might me more about the model architecture, training method advances, and less about its usefulness in actual real-world tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9nckz/qwen3next80ba3b_vs_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T12:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9udbu</id>
    <title>PrimeIntellect is actually awesome</title>
    <updated>2025-11-29T17:20:25+00:00</updated>
    <author>
      <name>/u/Icy_Gas8807</name>
      <uri>https://old.reddit.com/user/Icy_Gas8807</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"&gt; &lt;img alt="PrimeIntellect is actually awesome" src="https://preview.redd.it/ew6myj9kc84g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fdd6ec877e717ceaf2aa8afda32707a0332d8088" title="PrimeIntellect is actually awesome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested prime intellect 3: - Q4_K_L&lt;br /&gt; - 71.82GB&lt;br /&gt; - Uses Q8_0 for embed and output weights. Good quality, recommended.&lt;/p&gt; &lt;p&gt;Model seams intelligent enough for most of my daily tasks, will be using it along with gpt-oss-120B. This did give me a hope, if this trend continues and hoping to get great models like this at below 160B @fp4, inference possible in strix halo chips. &lt;/p&gt; &lt;p&gt;Also, now I want to connect it to web search. I know it is previously discussed: (&lt;a href="https://github.com/mrkrsl/web-search-mcp"&gt;https://github.com/mrkrsl/web-search-mcp&lt;/a&gt;) this seams to be the best option without jargon of adding api. Are there any better alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Gas8807"&gt; /u/Icy_Gas8807 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ew6myj9kc84g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9udbu/primeintellect_is_actually_awesome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T17:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9qe7o</id>
    <title>Qwen3 Next imatrix GGUFs up!</title>
    <updated>2025-11-29T14:33:49+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just figured I'd post in case anyone's looking for imatrix and IQ quants&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;https://huggingface.co/bartowski/Qwen_Qwen3-Next-80B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual this also uses my PR/fork for slightly more optimized MoE quantization &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/12727"&gt;https://github.com/ggml-org/llama.cpp/pull/12727&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9qe7o/qwen3_next_imatrix_ggufs_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T14:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9zoiw</id>
    <title>NeKot - a terminal interface for interacting with local and cloud LLMs</title>
    <updated>2025-11-29T21:00:37+00:00</updated>
    <author>
      <name>/u/Balanceballs</name>
      <uri>https://old.reddit.com/user/Balanceballs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"&gt; &lt;img alt="NeKot - a terminal interface for interacting with local and cloud LLMs" src="https://external-preview.redd.it/emhzNGhndG5mOTRnMe6rR53dxe7TwZ8ZKOIc0FAxbevUKRyRaaXNJrhJXdM9.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f0e1a15b0b50823bb62c7fb8a41de7b9614e1fe" title="NeKot - a terminal interface for interacting with local and cloud LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on this for a while, since I could not find a decent solution that is not abandoned or has all the features I need.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports Gemini, OpenAI and OpenRouter APIs as well as almost any local solution (tested with llama-cpp + llamaswap, ollama, lmstudio).&lt;/li&gt; &lt;li&gt;Has support for images, presets (each preset can have it's own settings and system prompt), sessions.&lt;/li&gt; &lt;li&gt;Written in GO , so no interpreter or runtime required.&lt;/li&gt; &lt;li&gt;Has support for basic vim motions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/BalanceBalls/nekot"&gt;https://github.com/BalanceBalls/nekot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balanceballs"&gt; /u/Balanceballs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m66w35tnf94g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T21:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p9ojio</id>
    <title>Yet another reason to stick with local models</title>
    <updated>2025-11-29T13:07:36+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt; &lt;img alt="Yet another reason to stick with local models" src="https://b.thumbs.redditmedia.com/LVWG1v1DcQ2DgWlztEoKA3ITIG04JVS8k3_QhWcs3tw.jpg" title="Yet another reason to stick with local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9"&gt;https://preview.redd.it/uofoe3u5374g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=219b2ab46ac5d0b74767604bd131b78757a40ac9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/btibor91/status/1994714152636690834?s=20"&gt;Tibor Blaho&lt;/a&gt;, a trusted reverse engineer, found ad system strings inside the latest ChatGPT Android beta(v1.2025.329).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p9ojio/yet_another_reason_to_stick_with_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-29T13:07:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
