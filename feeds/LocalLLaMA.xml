<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-23T14:23:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mxwrs8</id>
    <title>coding off the grid with a Mac?</title>
    <updated>2025-08-23T09:19:05+00:00</updated>
    <author>
      <name>/u/One_Archer_577</name>
      <uri>https://old.reddit.com/user/One_Archer_577</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is your experience with running qwencoder/claudecoder/aider CLIs while using local models on a 64GB/128GB Mac without internet? &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is there a big different between 64Gb and 128GB now that all the &amp;quot;medium&amp;quot; models seem to be 30B (i.e. small)? Is there some interesting models which 128GB shared memory unlocks?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Couldn't find comparisons on Qwen2.5-coder-32B, Qwen3-coder-32B-A3B and devstral-small-2507-24B. Which one is better for coding? Is there something else I should be considering?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I asked Claude Haiku. It's answer: run Qwen3-Coder-480B-A35B on a 128GB MAC, which doesn't fit...&lt;/p&gt; &lt;p&gt;Maybe a 32/36/48 GB Mac is enough with these models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Archer_577"&gt; /u/One_Archer_577 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwrs8/coding_off_the_grid_with_a_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwrs8/coding_off_the_grid_with_a_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwrs8/coding_off_the_grid_with_a_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T09:19:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxmyhx</id>
    <title>Mistral 3.2-24B quality in MoE, when?</title>
    <updated>2025-08-23T00:16:02+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While the world is distracted by GPT-OSS-20B and 120B, I‚Äôm here wasting no time with Mistral 3.2 Small 2506. An absolute workhorse, from world knowledge to reasoning to role-play, and the best of all ‚Äúminimal censorship‚Äù. GPT-OSS-20B has about 10 mins of usage the whole week in my setup. I like the speed but the model is so bad at hallucinations when it comes to world knowledge, and the tool usage broken half the time is frustrating.&lt;/p&gt; &lt;p&gt;The only complaint I have about the 24B mistral is speed. On my humble PC it runs at 4-4.5 t/s depending on context size. If Mistral has 32b MOE in development, it will wipe the floor with everything we know at that size and some larger models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxmyhx/mistral_3224b_quality_in_moe_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxmyhx/mistral_3224b_quality_in_moe_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxmyhx/mistral_3224b_quality_in_moe_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T00:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1my1hg4</id>
    <title>Llamarunner, a llama.cpp manager and runner (with user presets!)</title>
    <updated>2025-08-23T13:28:59+00:00</updated>
    <author>
      <name>/u/GGrassia</name>
      <uri>https://old.reddit.com/user/GGrassia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was tinkering with different models (always with llama-server) and was getting frustrated with not finding something for managing presets for the models to lower the hassle of switching and using the right parameters. I wanted to run qwen3, then glm4.5-air, then a stab at Deepseek, now I needed to embed stuff so I wanted Snowflake, and now something else... And I could not find anything online that could help me with it (admittedly, I was extremely lazy in my googling and defaulted to reinventing the wheel... Probably. But it was fun!).&lt;/p&gt; &lt;p&gt;So here it is, Llamarunner is built to be callable from wherever by automatically adding itself to path, installable with a simple curl, and is capable of pulling and building llama.cpp, running your models with presets, and comes with the added bonus of being callable in a pipeline, so if you need to OCR a document, embed it for rag and then use the rag pipeline you can do this all with one single machine!&lt;/p&gt; &lt;p&gt;Here's the repo, any form of criticism is welcome, right now windows is not supported, and honestly I don't really see myself doing it so, if anybody wants, you are more than welcome to fork.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/GGrassia/llamarunner"&gt;https://github.com/GGrassia/llamarunner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Disclaimer&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm not a Go dev, it was chosen for ease of development and cross-platform compiling, any non idiomatic stuff comes from there. Knucklehead solutions and bad coding are instead to be blamed on me and somewhat on GLM4.5-Air, but mostly on me, after all, I'm the only possible pebcak here.&lt;/p&gt; &lt;p&gt;Also, I expect some bugs, feel free to open issues and PRs, the only reason this is not a python script on my server is to give back to the community I've been taking and learning so much from.&lt;br /&gt; Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GGrassia"&gt; /u/GGrassia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my1hg4/llamarunner_a_llamacpp_manager_and_runner_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my1hg4/llamarunner_a_llamacpp_manager_and_runner_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my1hg4/llamarunner_a_llamacpp_manager_and_runner_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T13:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1my1jg7</id>
    <title>ByteDance Seed OSS 36B supported in llama.cpp</title>
    <updated>2025-08-23T13:31:21+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/b1afcab804e3281867a5471fbd701e32eb32e512"&gt;https://github.com/ggml-org/llama.cpp/commit/b1afcab804e3281867a5471fbd701e32eb32e512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still no native support for serverside thinking tag parsing since Seed uses a new seed:think tag, so will have to add that later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my1jg7/bytedance_seed_oss_36b_supported_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my1jg7/bytedance_seed_oss_36b_supported_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my1jg7/bytedance_seed_oss_36b_supported_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T13:31:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxobcn</id>
    <title>(Alpha Release 0.0.2) Asked Qwen-30b-a3b with Local Deep Think to design a SOTA inference algorithm | Comparison with Gemini 2.5 pro</title>
    <updated>2025-08-23T01:20:04+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;TLDR: A new open-source project called &lt;a href="https://github.com/andres-ulloa-de-la-torre/local-deepthink"&gt;local-deepthink&lt;/a&gt; aims to replicate Google's Ultra 600 dollar-a-month &amp;quot;DeepThink&amp;quot; feature on affordable local computers using only a CPU. This is achieved through a new algorihtm where different AI agents are treated like &amp;quot;neurons&amp;quot;. Very good for turning long prompting sessions into a one-shot, or in coder mode turning prompts into Computer Science research. The results are cautiously optimistic when compared against Gemini 2.5 pro with max thinking budget.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hey all, I've posted several times already but i wanted to show some results from this project I've been working on. Its called local-deepthink. We tested a few QNNs (Qualitative Neural Network) made with local-deepthink on conceptualizing SOTA new algorithms for LLMs. For this release we now added a coding feature with access to a code sandbox. Essentially you can think of this project as a way to max out a model performance trading response time for quality.&lt;/p&gt; &lt;p&gt;However if you are not a programmer think instead of local-deepthink as a nice way to handle prompts that require ultra long outputs. You want to theorycraft a system or the lore of an entire RPG world? You would normally prompt your local model manytimes, figure out different system prompts; but with local-deepthink you give the system a high level prompt, and the QNN figures out the rest. At the end of the run the system gives you a chat that allows you to pinpoint what data are you interested in. An interrogator chain takes your points and then exhaustively interrogates the hidden layers output based on the points of interest, looking for relevant stuff to add to an ultra long final report. The nice thing about QNNs is that system prompts are figured out on the fly. Fine tuning an LLM with a QNN dataset, might make system prompts obsolete as the trained LLM after fine tuning would implicitly figure the ‚Äúcorrect persona‚Äù and dynamically switch its own system prompt during it's reasoning process.&lt;/p&gt; &lt;p&gt;For diagnostic purposes you can chat with a specific neuron and diagnose it's accumulated state. QNNs unlike numerical Deep Learning are extremely human interpretable. We built a RAG index for the hidden layer that gathers all the utterances every epoch. You can prompt the diagnostic chat with e.g agent_1_1 and get all that specific neurons history. The progress assessment and critique combined, account figuratively for a numerical loss function. These functions unlike normal neural nets which use fixed functions are updated every epoch based on an annealing procedure that allows the hidden layer to become unstuck from local m√≠nima. The global loss function dynamically swaps personas: e.g &amp;quot;lazy manager&amp;quot;, &amp;quot;philosopher king&amp;quot;, &amp;quot;harsh drill sargent&amp;quot;...etc lol&lt;/p&gt; &lt;p&gt;Besides the value of what you get after mining and squeezing the LLM, its super entertaining to watch the neurons interact with each other. You can query neighbor neurons in a deep run using the diagnostic chat and see if they &amp;quot;get along&amp;quot;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=GSTtLWpM3uU"&gt;https://www.youtube.com/watch?v=GSTtLWpM3uU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We prompted a few small net sizes on SOTA plausible AI stuff. I don't have access to deepthink because I'm broke so it would be nice if someone rich with a good local rig, plus a google ultra subscription, opened an issue and helped benchmark a 6x6 QNN (or bigger). This is still alpha software with access to a coding sandbox, so proceed very carefully. Thinking models aint supported yet. If you run into a crash, please open an issue with your graph monitor trace log. This works with Ollama and potentially any instruct model you want; if you can plug-in better models than Qwen 30b a3b 2507 instruct, more power to you. Qwen 30b is a bit stupid with meta agentic prompting so the system in a deep run will sometimes crash. Any ideas on what specialized model of comparative size and efficiency is good for nested meta prompting? Even gemini 2.5 pro misinterprets things in this regard.&lt;/p&gt; &lt;p&gt;2X2 or 4x4 networks are ideal for cpu-only laptops with 32gb of RAM 3 or 4 epochs max so it stays comparable to Google Ultra. 6X6 all the way to 10x10 with more than 2 epochs up to 10 epochs should be doable with 64 gb in 45 min- 20min as long as you have a 24 gb GPU. If you are coding, this is better for conceptual algorithms where external dependencies can be plugged in later. Better ask for vanilla code. If you are a researcher building algorithms from scratch, you could check out the results and give this a try.&lt;/p&gt; &lt;p&gt;Features we are working in: p2p networking for ‚Äúcollaborative mining‚Äù (we call it mining because we are basically squeezing all posible knowledge from an LLM) and a checkpopint mechanism that allows you to pick the mining run where you left, or make the system more crash resistant; I‚Äôm already done adding more AI centric features so whats next is polish and debug what already exists until a beta phase is achieved; but im not a very good tester so i need your help. Use cases: local deepthink is great for problems where the only clue you have is a vague question or for one shotting very long prompting sessions. Next logical step is to turn this heuristic into a full software engineering stack for complex things like videogame creation: adding image analysis, video analysis, video generation, and 3d mesh generation neurons. Looking for collaborators with a desire to push local to SOTA.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Things where i currently need help:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Hunt bugs&lt;/p&gt; &lt;p&gt;- Deep runs with good hardware&lt;/p&gt; &lt;p&gt;- Thinking models support&lt;/p&gt; &lt;p&gt;- P2P network grid to build big QNNs&lt;/p&gt; &lt;p&gt;- Checkpoint import and export. Plug-in in your own QNN and save it as a file. Say you prompted an RPG story with many characters and you wish to continue&lt;/p&gt; &lt;p&gt;The little benchmark prompt:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Current diffusers and transformer architectures use integral samplers or differential solvers in the case of diffusers, and decoding algorithms which account as integral, in the case of transformers, to run inference; but never both together. I presume the foundation of training and architecture are already figured out, so i want a new inference algorithm. For this conceptualization assume the world is full of spinning wheels (harmonic oscillators), like we see them in atoms, solar systems, galaxies, human hierarchies...etc, and data represents a measured state of the &amp;quot;wheel&amp;quot; at a given time. Abudant training data samples the full state of the &amp;quot;wheel&amp;quot; by offering all the posible data of the wheels full state. This is where full understanding is reached: by spinning the whole wheel.&lt;/code&gt;&lt;/p&gt; &lt;p&gt; &lt;code&gt;Current inference algoritms onthe other hand, are not fully decoding the internal &amp;quot;implicit wheels&amp;quot; abstracted into the weights after training as they lack a feedback and harmonic mechanism as it is achieved by backprop during training. The training algorithms ‚Äúencodes‚Äù the &amp;quot;wheels&amp;quot; but inference algorithms do not extract them very well. Theres information loss.&lt;/code&gt;&lt;/p&gt; &lt;p&gt; &lt;code&gt;I want you to make in python with excellent documentation:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;1. An inference algorithm that uses a PID like approach with perturbative feedback. Instead of just using either an integrative or differential component, i want you to implement both with proportional weighting terms. The inference algorithm should sample all its progressive output and feed it back into the transformer.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;2. The inference algorithm should be coded from scratch without using external dependencies.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/andres-ulloa-de-la-torre/local-deepthink-findings/blob/main/README.md"&gt;Results | Gemini 2.5 pro vs pimped Qwen 30b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please support if you want to see more opensource work like this üôè&lt;/p&gt; &lt;p&gt;Thanks for reading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxobcn/alpha_release_002_asked_qwen30ba3b_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxobcn/alpha_release_002_asked_qwen30ba3b_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxobcn/alpha_release_002_asked_qwen30ba3b_with_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T01:20:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxsmjz</id>
    <title>A timeline I made of the most downloaded open-source AI models from 2022 to 2025</title>
    <updated>2025-08-23T05:05:05+00:00</updated>
    <author>
      <name>/u/jack-ster</name>
      <uri>https://old.reddit.com/user/jack-ster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1mxsmjz/video/6xwkt7mibpkf1/player"&gt;https://reddit.com/link/1mxsmjz/video/6xwkt7mibpkf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jack-ster"&gt; /u/jack-ster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxsmjz/a_timeline_i_made_of_the_most_downloaded/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxsmjz/a_timeline_i_made_of_the_most_downloaded/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxsmjz/a_timeline_i_made_of_the_most_downloaded/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T05:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxvp4q</id>
    <title>Deep Research MCP Server</title>
    <updated>2025-08-23T08:10:59+00:00</updated>
    <author>
      <name>/u/pminervini</name>
      <uri>https://old.reddit.com/user/pminervini</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I really needed to connect Claude Code etc. to the OpenAI Deep Research APIs (and Huggingface‚Äôs Open Deep Research agent), and did a quick MCP server for that: &lt;a href="https://github.com/pminervini/deep-research-mcp"&gt;https://github.com/pminervini/deep-research-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you find it useful, or have ideas for features and extensions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pminervini"&gt; /u/pminervini &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvp4q/deep_research_mcp_server/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvp4q/deep_research_mcp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvp4q/deep_research_mcp_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T08:10:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1my15gf</id>
    <title>gPOS17 AI Workstation with 3 GPUs, 96 GB DDR5, Garage Edition</title>
    <updated>2025-08-23T13:14:08+00:00</updated>
    <author>
      <name>/u/Lux_Interior9</name>
      <uri>https://old.reddit.com/user/Lux_Interior9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my15gf/gpos17_ai_workstation_with_3_gpus_96_gb_ddr5/"&gt; &lt;img alt="gPOS17 AI Workstation with 3 GPUs, 96 GB DDR5, Garage Edition" src="https://a.thumbs.redditmedia.com/PohCq2TsxaJLyf5NJWQa0dagcn2aiHJtBwb9s_SluI8.jpg" title="gPOS17 AI Workstation with 3 GPUs, 96 GB DDR5, Garage Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the era of foundation models, multimodal AI, LLMs, and ever-larger datasets, access to raw compute is still one of the biggest bottlenecks for researchers, founders, developers, and engineers. While the cloud offers scalability, building a personal AI workstation delivers complete control over your environment, reduced latency, and the privacy of running workloads locally ‚Äî even if that environment is a garage.&lt;/p&gt; &lt;p&gt;This post covers our version of a three-GPU workstation powered by an Intel Core i7-13700K, 96 GB of DDR5 memory, and a heterogeneous mix of GPUs sourced from both eBay and questionable decisions. This configuration pushes the limits of desktop AI computing while remaining true to the spirit of garage innovation.&lt;/p&gt; &lt;h1&gt;Our build includes:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Intel Core i7-13700K (16-core, Raptor Lake)&lt;/strong&gt; ‚Äî providing blistering performance while drawing just enough power to trip a breaker when combined with three GPUs and a space heater.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;96 GB DDR5-6400 CL32&lt;/strong&gt; ‚Äî a nonstandard but potent memory loadout, because symmetry is for people with disposable income.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Three GPUs stacked without shame:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;MSI SUPRIM X RTX 4080 16 GB (the crown jewel)&lt;/li&gt; &lt;li&gt;NVIDIA Tesla V100 16 GB PCIe (legacy, but it still screams)&lt;/li&gt; &lt;li&gt;AMD Radeon Instinct MI50 32 GB (scientific workloads‚Ä¶ allegedly)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Four NVMe SSDs&lt;/strong&gt; totaling 12 TB, each one a different brand because who has time for consistency.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dual PSU arrangement&lt;/strong&gt; (Corsair RM1000x + EVGA SuperNOVA 750 G2), mounted precariously like exposed organs.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;The gPOS17 doesn‚Äôt just support cutting-edge multimodal AI pipelines ‚Äî it redefines workstation thermodynamics with its patented &lt;strong&gt;weed-assisted cooling system&lt;/strong&gt; and &lt;strong&gt;gravity-fed cable management architecture&lt;/strong&gt;. This is not just a PC; it‚Äôs a statement. A cry for help. A shrine to performance-per-dollar ratios.&lt;/p&gt; &lt;p&gt;The result is a workstation capable of running simultaneous experiments, from large-scale text generation to advanced field simulations, all without leaving your garage (though you might leave it on fire).&lt;/p&gt; &lt;p&gt;*AMD Radeon Instinct MI50 not shown because it's in the mail from ebay.&lt;br /&gt; **diagram may not be accurate&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lux_Interior9"&gt; /u/Lux_Interior9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1my15gf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my15gf/gpos17_ai_workstation_with_3_gpus_96_gb_ddr5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my15gf/gpos17_ai_workstation_with_3_gpus_96_gb_ddr5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T13:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx7q58</id>
    <title>DINOv3 semantic video tracking running locally in your browser (WebGPU)</title>
    <updated>2025-08-22T14:16:07+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/"&gt; &lt;img alt="DINOv3 semantic video tracking running locally in your browser (WebGPU)" src="https://external-preview.redd.it/NDBmanYza3Z2a2tmMSbOwLNjFNHRCt4iN7LHZavGZv38TtoGGI8B_rdwJSVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca1097874f9291657ebcdac60b7b207a3633f2b7" title="DINOv3 semantic video tracking running locally in your browser (WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mrbtqt/dinov3_visualization_tool_running_100_locally_in/"&gt;demo&lt;/a&gt; I posted a few days ago, I added support for object tracking across video frames. It uses DINOv3 (a new vision backbone capable of producing rich, dense image features) to track objects in a video with just a few reference points. &lt;/p&gt; &lt;p&gt;One can imagine how this can be used for browser-based video editing tools, so I'm excited to see what the community builds with it! &lt;/p&gt; &lt;p&gt;Online demo (+ source code): &lt;a href="https://huggingface.co/spaces/webml-community/DINOv3-video-tracking"&gt;https://huggingface.co/spaces/webml-community/DINOv3-video-tracking&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lghkx3kvvkkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx7q58/dinov3_semantic_video_tracking_running_locally_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T14:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxvh1w</id>
    <title>Will most people eventually run AI locally instead of relying on the cloud?</title>
    <updated>2025-08-23T07:57:03+00:00</updated>
    <author>
      <name>/u/Significant-Cash7196</name>
      <uri>https://old.reddit.com/user/Significant-Cash7196</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most people use AI through the cloud - ChatGPT, Claude, Gemini, etc. That makes sense since the biggest models demand serious compute.&lt;/p&gt; &lt;p&gt;But local AI is catching up fast. With things like LLaMA, Ollama, MLC, and OpenWebUI, you can already run decent models on consumer hardware. I‚Äôve even got a &lt;strong&gt;2080 and a 3080 Ti sitting around&lt;/strong&gt;, and it‚Äôs wild how far you can push local inference with quantized models and some tuning. &lt;/p&gt; &lt;p&gt;For everyday stuff like summarization, Q&amp;amp;A, or planning, smaller fine-tuned models (7B‚Äì13B) often feel ‚Äúgood enough.‚Äù - I already posted about this and received mixed feedback on this&lt;/p&gt; &lt;p&gt;So it raises the big question: &lt;strong&gt;is the future of AI assistants local-first or cloud-first?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Local-first means you own the model, runs on your device, fully private, no API bills, offline-friendly.&lt;/li&gt; &lt;li&gt;Cloud-first means massive 100B+ models keep dominating because they can do things local hardware will never touch.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Maybe it ends up hybrid? local for speed/privacy, cloud for heavy reasoning, but I‚Äôm curious where this community thinks it‚Äôs heading.&lt;/p&gt; &lt;p&gt;In 5 years, do you see most people‚Äôs main AI assistant running on their &lt;strong&gt;own device&lt;/strong&gt; or still in the &lt;strong&gt;cloud&lt;/strong&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant-Cash7196"&gt; /u/Significant-Cash7196 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvh1w/will_most_people_eventually_run_ai_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvh1w/will_most_people_eventually_run_ai_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvh1w/will_most_people_eventually_run_ai_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T07:57:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxjonh</id>
    <title>Mistral we love Nemo 12B but we need a new Mixtral</title>
    <updated>2025-08-22T21:55:50+00:00</updated>
    <author>
      <name>/u/TroyDoesAI</name>
      <uri>https://old.reddit.com/user/TroyDoesAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"&gt; &lt;img alt="Mistral we love Nemo 12B but we need a new Mixtral" src="https://b.thumbs.redditmedia.com/pLR0WLYo2wgxMP_BKWB_jRWMPwY2CwMeIJH0V7nzOBI.jpg" title="Mistral we love Nemo 12B but we need a new Mixtral" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vcpu6ntd7nkf1.png?width=336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63b0a5c98e11788f7d4fdcf6cc239eb6d14d052c"&gt;https://preview.redd.it/vcpu6ntd7nkf1.png?width=336&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=63b0a5c98e11788f7d4fdcf6cc239eb6d14d052c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Do you agree?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TroyDoesAI"&gt; /u/TroyDoesAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxjonh/mistral_we_love_nemo_12b_but_we_need_a_new_mixtral/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T21:55:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxtj3k</id>
    <title>I got chatterbox working in my chat, it's everything I hoped for.</title>
    <updated>2025-08-23T05:57:28+00:00</updated>
    <author>
      <name>/u/ansmo</name>
      <uri>https://old.reddit.com/user/ansmo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxtj3k/i_got_chatterbox_working_in_my_chat_its/"&gt; &lt;img alt="I got chatterbox working in my chat, it's everything I hoped for." src="https://external-preview.redd.it/NjhuMTNjdGRscGtmMYAzAbqA2kx7xhW8T-uv62eA2eW0Xuc6GCrC1e8xPToz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1de9756c8c6d034cb0dceaa3087530708dc9c402" title="I got chatterbox working in my chat, it's everything I hoped for." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ansmo"&gt; /u/ansmo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/njt6ut2skpkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxtj3k/i_got_chatterbox_working_in_my_chat_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxtj3k/i_got_chatterbox_working_in_my_chat_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T05:57:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mwwr87</id>
    <title>What is Gemma 3 270M actually used for?</title>
    <updated>2025-08-22T04:17:47+00:00</updated>
    <author>
      <name>/u/airbus_a360_when</name>
      <uri>https://old.reddit.com/user/airbus_a360_when</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwwr87/what_is_gemma_3_270m_actually_used_for/"&gt; &lt;img alt="What is Gemma 3 270M actually used for?" src="https://preview.redd.it/dtrvooncyhkf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0447bc7fe6fe125ae4afb2d1094b8948a5d8af3d" title="What is Gemma 3 270M actually used for?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All I can think of is speculative decoding. Can it even RAG that well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/airbus_a360_when"&gt; /u/airbus_a360_when &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dtrvooncyhkf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mwwr87/what_is_gemma_3_270m_actually_used_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mwwr87/what_is_gemma_3_270m_actually_used_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T04:17:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1my0xu3</id>
    <title>Intel's New LLM-Scaler Beta Update Brings Whisper Model &amp; GLM-4.5-Air Support</title>
    <updated>2025-08-23T13:04:50+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/Intel-llm-scaler-vllm-Whisper"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1my0xu3/intels_new_llmscaler_beta_update_brings_whisper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1my0xu3/intels_new_llmscaler_beta_update_brings_whisper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T13:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxu80p</id>
    <title>Finally the upgrade is complete</title>
    <updated>2025-08-23T06:38:51+00:00</updated>
    <author>
      <name>/u/Jaswanth04</name>
      <uri>https://old.reddit.com/user/Jaswanth04</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxu80p/finally_the_upgrade_is_complete/"&gt; &lt;img alt="Finally the upgrade is complete" src="https://b.thumbs.redditmedia.com/TIofsrVluybO-fbRt8aQtglmzyBhTiWcvfBwPlcXDDU.jpg" title="Finally the upgrade is complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Initially had 2 FE 3090. I purchased a 5090, which I was able to get at msrp in my country and finally adjusted in that cabinet &lt;/p&gt; &lt;p&gt;Other components are old, corsair 1500i psu. Amd 3950x cpu Auros x570 mother board, 128 GB DDR 4 Ram. Cabinet is Lian Li O11 dynamic evo xl. &lt;/p&gt; &lt;p&gt;What should I test now? I guess I will start with the 2bit deepseek 3.1 or GLM4.5 models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jaswanth04"&gt; /u/Jaswanth04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxu80p"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxu80p/finally_the_upgrade_is_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxu80p/finally_the_upgrade_is_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T06:38:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxqljz</id>
    <title>vscode + roo + Qwen3-30B-A3B-Thinking-2507-Q6_K_L = superb</title>
    <updated>2025-08-23T03:14:59+00:00</updated>
    <author>
      <name>/u/Secure_Reflection409</name>
      <uri>https://old.reddit.com/user/Secure_Reflection409</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes, the 2507 Thinking variant not the coder.&lt;/p&gt; &lt;p&gt;All the small coder models I tried I kept getting:&lt;/p&gt; &lt;h1&gt;Roo is having trouble...&lt;/h1&gt; &lt;p&gt;I can't even begin to tell you how infuriating this message is. I got this constantly from Qwen 30b coder Q6 and GPT OSS 20b.&lt;/p&gt; &lt;p&gt;Now, though, it just... works. It bounces from architect to coder and occasionally even tests the code, too. I think git auto commits are coming soon, too. I tried the debug mode. That works well, too.&lt;/p&gt; &lt;p&gt;My runner is nothing special:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server.exe -m Qwen_Qwen3-30B-A3B-Thinking-2507-Q6_K_L.gguf -c 131072 --temp 0.6 --top-k 20 --top-p 0.95 --min-p 0.0 -ngl 99 -fa -dev CUDA1,CUDA2 --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I suspect it would work ok with far less context, too. However, when I was watching 30b coder and oss 20b flail around, I noticed they were smashing the context to the max and getting nowhere. 2507 Thinking appears to be particularly frugal with the context in comparison.&lt;/p&gt; &lt;p&gt;I haven't even tried any of my better/slower models, yet. This is basically my perfect setup. Gaming on CUDA0, whilst CUDA1 and CUDA2 are grinding at 90t/s on monitor two.&lt;/p&gt; &lt;p&gt;Very impressed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secure_Reflection409"&gt; /u/Secure_Reflection409 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxqljz/vscode_roo_qwen330ba3bthinking2507q6_k_l_superb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxqljz/vscode_roo_qwen330ba3bthinking2507q6_k_l_superb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxqljz/vscode_roo_qwen330ba3bthinking2507q6_k_l_superb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T03:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxh4gk</id>
    <title>ü§î meta X midjourney</title>
    <updated>2025-08-22T20:13:52+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"&gt; &lt;img alt="ü§î meta X midjourney" src="https://preview.redd.it/ayp3r5k9pmkf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c32ec6bf7b9de766f15b1cbdfeecbbb7d4e77a3" title="ü§î meta X midjourney" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ayp3r5k9pmkf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxh4gk/meta_x_midjourney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T20:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxn41d</id>
    <title>DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark</title>
    <updated>2025-08-23T00:22:56+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"&gt; &lt;img alt="DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark" src="https://a.thumbs.redditmedia.com/KttrxTIeAWjVzy0nMU4BsHTSl_X9a-QvBbYCoz3pNU0.jpg" title="DeepSeek V3.1 Reasoner improves over DeepSeek R1 on the Extended NYT Connections benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More info: &lt;a href="https://github.com/lechmazur/nyt-connections/"&gt;https://github.com/lechmazur/nyt-connections/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxn41d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxn41d/deepseek_v31_reasoner_improves_over_deepseek_r1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T00:22:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxr6zi</id>
    <title>How close can non big tech people get to ChatGPT and Claude speed locally? If you had $10k, how would you build infrastructure?</title>
    <updated>2025-08-23T03:46:39+00:00</updated>
    <author>
      <name>/u/EducationalText9221</name>
      <uri>https://old.reddit.com/user/EducationalText9221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title says, if you had $10k or maybe less, how you achieve infrastructure to run local models as fast as ChatGPT and Claude? Would you build different machines with 5090? Would you stack 3090s on one machine with nvlink (not sure if I understand how they get that many on one machine correctly), add a thread ripper and max ram? Would like to hear from someone that understands more! Also would that build work for fine tuning fine? Thanks in advance!&lt;/p&gt; &lt;p&gt;Edit: I am looking to run different models 8b-100b. I also want to be able to train and fine tune with PyTorch and transformers. It doesn‚Äôt have to be built all at once it could be upgraded over time. I don‚Äôt mind building it by hand, I just said that I am not as familiar with multiple GPUs as I heard that not all models support it&lt;/p&gt; &lt;p&gt;Edit2: I find local models okay, most people are commenting about models not hardware. Also for my purposes, I am using python to access models not ollama studio and similar things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationalText9221"&gt; /u/EducationalText9221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxr6zi/how_close_can_non_big_tech_people_get_to_chatgpt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxr6zi/how_close_can_non_big_tech_people_get_to_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxr6zi/how_close_can_non_big_tech_people_get_to_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T03:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxke42</id>
    <title>a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM</title>
    <updated>2025-08-22T22:24:51+00:00</updated>
    <author>
      <name>/u/No_Palpitation7740</name>
      <uri>https://old.reddit.com/user/No_Palpitation7740</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"&gt; &lt;img alt="a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM" src="https://a.thumbs.redditmedia.com/0sA5_Mq1e9tKF5OC4u-i_MtdMq9DbQ_Rib1omZfbbO4.jpg" title="a16z AI workstation with 4 NVIDIA RTX 6000 Pro Blackwell Max-Q 384 GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a sample of the full article &lt;a href="https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/"&gt;https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the era of foundation models, multimodal AI, LLMs, and ever-larger datasets, access to raw compute is still one of the biggest bottlenecks for researchers, founders, developers, and engineers. While the cloud offers scalability, building a personal AI Workstation delivers complete control over your environment, latency reduction, custom configurations and setups, and the privacy of running all workloads locally.&lt;/p&gt; &lt;p&gt;This post covers our version of a four-GPU workstation powered by the new NVIDIA RTX 6000 Pro Blackwell Max-Q GPUs. This build pushes the limits of desktop AI computing with 384GB of VRAM (96GB each GPU), all in a shell that can fit under your desk.&lt;/p&gt; &lt;p&gt;[...] &lt;/p&gt; &lt;p&gt;We are planning to test and make a limited number of these custom a16z Founders Edition AI Workstations&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Palpitation7740"&gt; /u/No_Palpitation7740 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mxke42"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxke42/a16z_ai_workstation_with_4_nvidia_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T22:24:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxwwsk</id>
    <title>AI models playing chess ‚Äì not strong, but an interesting benchmark!</title>
    <updated>2025-08-23T09:27:46+00:00</updated>
    <author>
      <name>/u/Apart-Ad-1684</name>
      <uri>https://old.reddit.com/user/Apart-Ad-1684</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"&gt; &lt;img alt="AI models playing chess ‚Äì not strong, but an interesting benchmark!" src="https://external-preview.redd.it/ys7jEoBiKiu8EwYd0V5EewPsg3PtK6u6uh3HZ7U-N5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9ff9da34f54c9079bce0fa6ea2619c1c98dc000" title="AI models playing chess ‚Äì not strong, but an interesting benchmark!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on &lt;a href="https://chess.louisguichard.fr"&gt;LLM Chess Arena&lt;/a&gt;, an application where large language models play chess against each other.&lt;/p&gt; &lt;p&gt;The games aren‚Äôt spectacular, because LLMs aren‚Äôt really good at chess ‚Äî but that‚Äôs exactly what makes it interesting! Chess highlights their reasoning gaps in a simple and interpretable way, and it‚Äôs fun to follow their progress.&lt;/p&gt; &lt;p&gt;The app let you &lt;strong&gt;launch your own AI vs AI games&lt;/strong&gt; and features a live &lt;strong&gt;leaderboard&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Curious to hear your thoughts!&lt;/p&gt; &lt;p&gt;üéÆ App: &lt;a href="http://chess.louisguichard.fr"&gt;chess.louisguichard.fr&lt;/a&gt;&lt;br /&gt; üíª Code: &lt;a href="https://github.com/louisguichard/llm-chess-arena"&gt;https://github.com/louisguichard/llm-chess-arena&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j33oc7x5mqkf1.png?width=2834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47858fb05effa3fea9e591fb6e7fda6a82f9db5f"&gt;https://preview.redd.it/j33oc7x5mqkf1.png?width=2834&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=47858fb05effa3fea9e591fb6e7fda6a82f9db5f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-Ad-1684"&gt; /u/Apart-Ad-1684 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxwwsk/ai_models_playing_chess_not_strong_but_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T09:27:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxrarl</id>
    <title>NVIDIA new paper : Small Language Models are the Future of Agentic AI</title>
    <updated>2025-08-23T03:51:55+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA have just published a paper claiming SLMs (small language models) are the future of agentic AI. They provide a number of claims as to why they think so, some important ones being they are cheap. Agentic AI requires just a tiny slice of LLM capabilities, SLMs are more flexible and other points. The paper is quite interesting and short as well to read. &lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/pdf/2506.02153"&gt;https://arxiv.org/pdf/2506.02153&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video Explanation : &lt;a href="https://www.youtube.com/watch?v=6kFcjtHQk74"&gt;https://www.youtube.com/watch?v=6kFcjtHQk74&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxrarl/nvidia_new_paper_small_language_models_are_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxrarl/nvidia_new_paper_small_language_models_are_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxrarl/nvidia_new_paper_small_language_models_are_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T03:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxf2sz</id>
    <title>Seed-OSS-36B is ridiculously good</title>
    <updated>2025-08-22T18:54:56+00:00</updated>
    <author>
      <name>/u/mahmooz</name>
      <uri>https://old.reddit.com/user/mahmooz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct"&gt;https://huggingface.co/ByteDance-Seed/Seed-OSS-36B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;the model was released a few days ago. it has a native context length of 512k. a pull request has been made to llama.cpp to get support for it.&lt;/p&gt; &lt;p&gt;i just tried running it with the code changes in the pull request. and it works wonderfully. unlike other models (such as qwen3, which has 256k context length supposedly), the model can generate long coherent outputs without refusal.&lt;/p&gt; &lt;p&gt;i tried many other models like qwen3 or hunyuan but none of them are able to generate long outputs and even often complain that the task may be too difficult or may &amp;quot;exceed the limits&amp;quot; of the llm. but this model doesnt even complain, it just gets down to it. one other model that also excels at this is glm-4.5 but its context length is much smaller unfortunately.&lt;/p&gt; &lt;p&gt;seed-oss-36b also apparently has scored 94 on ruler at 128k context which is insane for a 36b model (it was reported by the maintainer of chatllm.cpp).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mahmooz"&gt; /u/mahmooz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxf2sz/seedoss36b_is_ridiculously_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T18:54:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mx8qki</id>
    <title>I'm making a game where all the dialogue is generated by the player + a local llm</title>
    <updated>2025-08-22T14:55:48+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"&gt; &lt;img alt="I'm making a game where all the dialogue is generated by the player + a local llm" src="https://external-preview.redd.it/dGRvYnNtbjM0bGtmMcsYHmRxX6l-GOXVgL0nfvRqWRvtCbG6hh3bmeu2mYuD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51833070477bb979f4af1952d3badd650277cef2" title="I'm making a game where all the dialogue is generated by the player + a local llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oitg5nn34lkf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mx8qki/im_making_a_game_where_all_the_dialogue_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-22T14:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mxvyll</id>
    <title>DeepConf: 99.9% Accuracy on AIME 2025 with Open-Source Models + 85% Fewer Tokens</title>
    <updated>2025-08-23T08:27:18+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this new method called &lt;strong&gt;DeepConf (Deep Think with Confidence)&lt;/strong&gt; looks super interesting.&lt;/p&gt; &lt;p&gt;It‚Äôs the &lt;strong&gt;first approach to hit 99.9% on AIME 2025&lt;/strong&gt; using an open-source model (&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt;) &lt;em&gt;without tools&lt;/em&gt;. What really stands out is that it not only pushes accuracy but also massively cuts down token usage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;~10% accuracy boost across multiple models &amp;amp; datasets&lt;/p&gt; &lt;p&gt;Up to 85% fewer tokens generated ‚Üí much more efficient&lt;/p&gt; &lt;p&gt;Plug-and-play: works with any existing model, no training or hyperparameter tuning required&lt;/p&gt; &lt;p&gt;Super simple to deploy: just ~50 lines of code in vLLM (see PR)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;üìö Paper: &lt;a href="https://arxiv.org/pdf/2508.15260"&gt;https://arxiv.org/pdf/2508.15260&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üåê Project: &lt;a href="https://jiaweizzhao.github.io/deepconf"&gt;https://jiaweizzhao.github.io/deepconf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;twitter post: &lt;a href="https://x.com/jiawzhao/status/1958982524333678877"&gt;https://x.com/jiawzhao/status/1958982524333678877&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvyll/deepconf_999_accuracy_on_aime_2025_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvyll/deepconf_999_accuracy_on_aime_2025_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mxvyll/deepconf_999_accuracy_on_aime_2025_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-23T08:27:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
