<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-29T12:29:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oiv5mc</id>
    <title>Open source TTS for scale?</title>
    <updated>2025-10-29T04:20:36+00:00</updated>
    <author>
      <name>/u/edwardzion</name>
      <uri>https://old.reddit.com/user/edwardzion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried deploying an open source TTS model with low latency (ideally &amp;lt;200ms) at scale. For something like voice agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edwardzion"&gt; /u/edwardzion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiv5mc/open_source_tts_for_scale/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiv5mc/open_source_tts_for_scale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiv5mc/open_source_tts_for_scale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T04:20:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixuck</id>
    <title>Using a small local model (Quen 0.5B?) for 10k lines of key-value pair custom domain data</title>
    <updated>2025-10-29T07:08:55+00:00</updated>
    <author>
      <name>/u/Tiny_Yellow_7869</name>
      <uri>https://old.reddit.com/user/Tiny_Yellow_7869</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have around 10,000 key-value pairs of structured custom domain data that I want a local LLM to understand and answer questions about offline. For example, I might ask things like ‚Äúfind all keys where the value mentions X‚Äù or ‚Äúsummarize related entries etc‚Äù&lt;/p&gt; &lt;p&gt;I don‚Äôt think I should train a model for this. It seems I could reference and reason over the data locally. From what I‚Äôve read this sounds like RAG case. I have a hard time understanding RAG, I see this as a say to encode my custom data in a form that is optimized for the AI model to work with it.&lt;/p&gt; &lt;p&gt;I came across the Qwen2.5:0.5b-instruct model, which runs well locally on my machine, not sure if that makes sense for my case. Has anyone had this sort of requirements?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Yellow_7869"&gt; /u/Tiny_Yellow_7869 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixuck/using_a_small_local_model_quen_05b_for_10k_lines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixuck/using_a_small_local_model_quen_05b_for_10k_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oixuck/using_a_small_local_model_quen_05b_for_10k_lines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T07:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj1f7n</id>
    <title>Need advice on building a GPU-based render/Al compute setup: Unsure about hardware direction</title>
    <updated>2025-10-29T10:59:19+00:00</updated>
    <author>
      <name>/u/One_Abroad_5937</name>
      <uri>https://old.reddit.com/user/One_Abroad_5937</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm in the early stages of planning a high performance GPU compute setup that will primarily be used for heavy rendering and maybe Al workloads. I'm still finalizing the exact business and infrastructure details, but right now I need to make some critical hardware decisions.&lt;/p&gt; &lt;p&gt;I'm trying to figure out what makes the most sense. Should I build using multiple high-end consumer GPUs (like 4090s or similar) in custom nodes, or invest in enterprise-grade GPU servers like Supermicro with NVLink or higher-density rack configurations.&lt;/p&gt; &lt;p&gt;If anyone here has experience with setting up render farms, Al inference/training clusters, or GPU virtualization environments, l'd really appreciate your insight on things like:&lt;/p&gt; &lt;p&gt;‚Ä¢ Hardware reliability and thermals for 24/7 workloads. ‚Ä¢ Power efficiency and cooling considerations. ‚Ä¢ Whether used/refurb enterprise servers are a good deal. ‚Ä¢ Any gotchas when scaling from a few nodes to a full rack.&lt;/p&gt; &lt;p&gt;Thanks in advance for any and all advice I can get, especially from those who are familiar with this stuff and running similar systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Abroad_5937"&gt; /u/One_Abroad_5937 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1f7n/need_advice_on_building_a_gpubased_renderal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1f7n/need_advice_on_building_a_gpubased_renderal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1f7n/need_advice_on_building_a_gpubased_renderal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T10:59:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj1nrx</id>
    <title>Best/Good Model for Understanding + Tool-Calling?</title>
    <updated>2025-10-29T11:11:51+00:00</updated>
    <author>
      <name>/u/Bowdenzug</name>
      <uri>https://old.reddit.com/user/Bowdenzug</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need your help. I'm currently working on a Python Langchain/Langgraph project and want to create a complex AI agent. Ten tools are available, and the system prompt is described in great detail, including which tools it has, what it should do in which processes, what the limits are, etc. It's generally about tax law and invoicing within the EU. My problem is that I can't find a model that handles tool calling well and has a decent understanding of taxes. Qwen3 32b has gotten me the furthest, but even with that, there are sometimes faulty tool calls or nonsensical contexts. Mistral Small 3.2 24b fp8 has bugs, and tool calling doesn't work with VLLM. Llama3.1 70b it awq int4 also doesn't seem very reliable regarding tool calling. ChatGPT 4o has worked best so far, really well, but I have to host the LLM myself. I currently have 48GB of VRAM available, will upgrade to 64GB vram in the next few days, and once it's in production, VRAM won't matter anymore since RTX 6000 Pro cards will be used. Perhaps some of you have already experimented with this sector.&lt;/p&gt; &lt;p&gt;Edit: my pipeline starts with around 3k context tokens and when the process is done it usually has gathered around 20-25k tokens context length&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bowdenzug"&gt; /u/Bowdenzug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1nrx/bestgood_model_for_understanding_toolcalling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1nrx/bestgood_model_for_understanding_toolcalling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1nrx/bestgood_model_for_understanding_toolcalling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T11:11:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oilwvm</id>
    <title>MiniMax-M2 llama.cpp</title>
    <updated>2025-10-28T21:23:23+00:00</updated>
    <author>
      <name>/u/butlan</name>
      <uri>https://old.reddit.com/user/butlan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to implement it, it's fully cursor generated ai slop code, sorry. The chat template is strange; I'm 100% sure it's not correctly implemented, but it works with the roo code (Q2 is bad, Q4 is fine) at least. Anyone who wants to waste 100gb bandwidth can give it a try.&lt;/p&gt; &lt;p&gt;test device and command : 2x4090 and lot of ram&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m minimax-m2-Q4_K.gguf -ngl 999 --cpu-moe --jinja -fa on -c 50000 --reasoning-format auto&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;code:&lt;/code&gt; &lt;a href="https://github.com/cturan/llama.cpp/tree/minimax"&gt;here&lt;/a&gt; &lt;code&gt;gguf:&lt;/code&gt; &lt;a href="https://huggingface.co/cturan/MiniMax-M2-GGUF/tree/main"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1oilwvm/video/ofpwt9vn4xxf1/player"&gt;https://reddit.com/link/1oilwvm/video/ofpwt9vn4xxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/butlan"&gt; /u/butlan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiy55j</id>
    <title>SoulX-Podcast: Towards Realistic Long-form Podcasts with Dialectal and Paralinguistic Diversity</title>
    <updated>2025-10-29T07:29:41+00:00</updated>
    <author>
      <name>/u/previse_je_sranje</name>
      <uri>https://old.reddit.com/user/previse_je_sranje</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/previse_je_sranje"&gt; /u/previse_je_sranje &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Xianbao_QIAN/status/1983429688606540141?t=dD3e9acepGIbhpWhUdrEXA&amp;amp;s=19"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiy55j/soulxpodcast_towards_realistic_longform_podcasts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiy55j/soulxpodcast_towards_realistic_longform_podcasts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T07:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiwoeq</id>
    <title>Local coding models limit</title>
    <updated>2025-10-29T05:51:21+00:00</updated>
    <author>
      <name>/u/Blues520</name>
      <uri>https://old.reddit.com/user/Blues520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've have dual 3090s and have been running 32b coding models for a while now with Roo/Cline. While they are useful, I only found them helpful for basic to medium level tasks. They can start coding nonsense quite easily and have to be reigned in with a watchful eye. This takes a lot of energy and focus as well, so your coding style changes to accommodate this. For well defined low complexity tasks, they are good, but beyond that I found that they can't keep up.&lt;/p&gt; &lt;p&gt;The next level up would be to add another 48GB VRAM but at that power consumption the intelligence level is not necessary worth it. I'd be interested to know your experience if you're running coding models at around 96GB.&lt;/p&gt; &lt;p&gt;The hosted SOTA models can handle high complexity tasks and especially design, while still prone to hallucination. I often use chatgpt to discuss design and architecture which is fine because I'm not sharing much implementation details or IP. Privacy is the main reason that I'm running local. I don't feel comfortable just handing out my code and IP to these companies. So I'm stuck running 32b models that can help with basic tasks or having to add more VRAM, but I'm not sure if the returns are worth it unless it means running much larger models, and at that point the power consumption and cooling becomes a major factor. Would love to hear your thoughts and experiences on this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Blues520"&gt; /u/Blues520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiwoeq/local_coding_models_limit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiwoeq/local_coding_models_limit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiwoeq/local_coding_models_limit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T05:51:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiyn8u</id>
    <title>Improving RAG Results with OpenWebUI - Looking for Advice on Custom Pipelines &amp; Better Embeddings</title>
    <updated>2025-10-29T08:04:28+00:00</updated>
    <author>
      <name>/u/b5761</name>
      <uri>https://old.reddit.com/user/b5761</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm currently working on improving the RAG performance in OpenWebUI and would appreciate advice from others who have built custom pipelines or optimized embeddings. My current setup uses OpenWebUI as the frontend, with GPT-OSS-120b running on an external GPU server (connected via API token). The embedding model is bge-m3, and text extraction is handled by Apache Tika. All documents (mainly internal German-language PDFs) are uploaded directly into the OpenWebUI knowledge base. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup / Environment:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Frontend:&lt;/strong&gt; OpenWebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM:&lt;/strong&gt; GPT-OSS-120b (external GPU server, connected via API token)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embedding Model:&lt;/strong&gt; &lt;code&gt;bge-m3&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extraction Engine:&lt;/strong&gt; Apache Tika&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Knowledge Base:&lt;/strong&gt; PDFs uploaded directly into OpenWebUI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data Type:&lt;/strong&gt; Internal company documents (German language, about product informations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Observed Issues:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The RAG pipeline sometimes pulls the wrong PDF context for a query ‚Äì responses reference unrelated documents.&lt;/li&gt; &lt;li&gt;Repeating the same question multiple times yields different answers, some of which are incorrect.&lt;/li&gt; &lt;li&gt;The first few responses after starting a chat are often relevant, but context quality degrades over time.&lt;/li&gt; &lt;li&gt;I suspect the embedding model isn‚Äôt optimal for German, or preprocessing is inconsistent.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm looking for practical advice on how to build a custom embedding pipeline outside of OpenWebUI, with better control over chunking, text cleaning, and metadata handling. I‚Äôd also like to know which German-optimized embedding models from Hugging Face or the MTEB leaderboard outperform bge-m3 in semantic retrieval. In addition, I‚Äôm interested in frameworks or methods for pretraining on QA pairs or fine-tuning with document context, for example using SentenceTransformers or InstructorXL. How does this pre-training work? Another question is whether it‚Äôs more effective to switch to an external vector database such as Qdrant for embedding storage and retrieval, instead of relying on OpenWebUI‚Äôs built-in knowledge base. Does a finetuning or training / customized PDF-Pipeline work better? If so are there any tutorials out there and is this possible with Openwebui? &lt;/p&gt; &lt;p&gt;Thanks for your help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/b5761"&gt; /u/b5761 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiyn8u/improving_rag_results_with_openwebui_looking_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiyn8u/improving_rag_results_with_openwebui_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiyn8u/improving_rag_results_with_openwebui_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T08:04:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oihbtx</id>
    <title>Minimax-M2 cracks top 10 overall LLMs (production LLM performance gap shrinking: 7 points from GPT-5 in Artificial Analysis benchmark)</title>
    <updated>2025-10-28T18:28:42+00:00</updated>
    <author>
      <name>/u/medi6</name>
      <uri>https://old.reddit.com/user/medi6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been analysing the Artificial Analysis benchmark set (94 production models, 329 API endpoints) and wanted to share some trends that seem notable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;br /&gt; This is models with commercial API access, not the full experimental OS landscape. So mostly models you'd actually deploy out of the box rather than every research models&lt;/p&gt; &lt;p&gt;The gap between best tracked OS (MiniMax-M2, quality 61) and best proprietary (GPT-5, 68) is now 7 points. Last year it was around 18 points in the same dataset. Linear extrapolation suggests parity by Q2 2026 for production-ready models, though obviously that assumes the trend holds (and chinese labs keep shipping OSS models)&lt;/p&gt; &lt;p&gt;What's interesting is the tier distribution:&lt;/p&gt; &lt;p&gt;- Elite (60+): 1 OS, 11 proprietary&lt;br /&gt; - High (50-59): 8 OS, 8 proprietary (we hit parity here)&lt;br /&gt; - Below 50: OS dominates by volume&lt;/p&gt; &lt;p&gt;The economics are pretty stark.&lt;br /&gt; OS average: $0.83/M tokens.&lt;br /&gt; Proprietary: $6.03/M.&lt;br /&gt; Value leaders like Qwen3-235B are hitting 228 quality per dollar vs ~10-20 for proprietary elite models (kind of a random approach but tried playing with this: quality per dollar = quality Index √∑ price/M tokens)&lt;/p&gt; &lt;p&gt;Speed is also shifting. OS on optimised infra (Groq, Fireworks) peaks at 3,087 tok/sec vs 616 for proprietary. Not sure how sustainable that edge is as proprietary invests in inference optimisation.&lt;/p&gt; &lt;p&gt;Made an interactive comparison: &lt;a href="http://whatllm.org"&gt;whatllm.org&lt;/a&gt;&lt;br /&gt; Full write-up: &lt;a href="https://www.whatllm.org/blog/open-source-vs-proprietary-llms-2025"&gt;https://www.whatllm.org/blog/open-source-vs-proprietary-llms-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Two questions I'm chewing on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How representative is this benchmark set vs the wider OS ecosystem? AA focuses on API-ready production models, which excludes a lot of experimental work, fine tuned models etc&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is there a ceiling coming, or does this compression just continue? Chinese labs seem to be iterating faster than I expected.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Curious what others think about the trajectory here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medi6"&gt; /u/medi6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T18:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj32pd</id>
    <title>OpenAI: gpt-oss-safeguard: two open-weight reasoning models built for safety classification (Now on Hugging Face)</title>
    <updated>2025-10-29T12:23:51+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss-safeguard lets developers use their own custom policies to classify content. The model interprets those policies to classify messages, responses, and conversations.&lt;br /&gt; These models are fine-tuned versions of our gpt-oss open models, available under Apache 2.0 license.&lt;br /&gt; Now on Hugging Face: &lt;a href="https://x.com/OpenAI/status/1983507392374641071"&gt;https://x.com/OpenAI/status/1983507392374641071&lt;/a&gt;&lt;br /&gt; Introducing gpt-oss-safeguard - New open safety reasoning models (120b and 20b) that support custom safety policies: &lt;a href="https://openai.com/index/introducing-gpt-oss-safeguard/"&gt;https://openai.com/index/introducing-gpt-oss-safeguard/&lt;/a&gt;&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/collections/openai/gpt-oss-safeguard"&gt;https://huggingface.co/collections/openai/gpt-oss-safeguard&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj32pd/openai_gptosssafeguard_two_openweight_reasoning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj32pd/openai_gptosssafeguard_two_openweight_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj32pd/openai_gptosssafeguard_two_openweight_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T12:23:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1oichb7</id>
    <title>Granite 4.0 Nano Language Models</title>
    <updated>2025-10-28T15:29:41+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"&gt; &lt;img alt="Granite 4.0 Nano Language Models" src="https://external-preview.redd.it/IWIrfsaMSUG5JLRfVdW-aDvE5706Tdr6bIFsDJelbBQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eae6a377c52b823a98de991ed339474596e018d" title="Granite 4.0 Nano Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM Granite team released Granite 4 Nano models:&lt;/p&gt; &lt;p&gt;1B and 350m versions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-nano-language-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T15:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixpca</id>
    <title>Sparse Adaptive Attention ‚ÄúMoE‚Äù, a potential performance breakthrough for LLMs?</title>
    <updated>2025-10-29T06:59:46+00:00</updated>
    <author>
      <name>/u/kaggleqrdl</name>
      <uri>https://old.reddit.com/user/kaggleqrdl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently a post was made on this topic. &lt;a href="https://medium.com/@hyborian_/sparse-adaptive-attention-moe-how-i-solved-openais-650b-problem-with-a-700-gpu-343f47b2d6c1"&gt;https://medium.com/@hyborian_/sparse-adaptive-attention-moe-how-i-solved-openais-650b-problem-with-a-700-gpu-343f47b2d6c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The idea is to use MoE at the attention layer to reduce compute usage for low signal tokens. Imho, this is probably the closest: &lt;a href="https://arxiv.org/abs/2409.06669"&gt;https://arxiv.org/abs/2409.06669&lt;/a&gt; &lt;/p&gt; &lt;p&gt;The post is a weird combination of technical insight and strange AI generated bravado.&lt;/p&gt; &lt;p&gt;If I were going to leak IP, this is pretty much how I would do it. Use gen AI to obfuscate the source.&lt;/p&gt; &lt;p&gt;There has been a lot of research in this area as noted in the comments (finding these required some effort):&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2312.07987"&gt;https://arxiv.org/abs/2312.07987&lt;/a&gt;&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2210.05144"&gt;https://arxiv.org/abs/2210.05144&lt;/a&gt;&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2410.11842"&gt;https://arxiv.org/abs/2410.11842&lt;/a&gt;&lt;br /&gt; &lt;a href="https://openreview.net/forum?id=NaAgodxpxo"&gt;https://openreview.net/forum?id=NaAgodxpxo&lt;/a&gt;&lt;br /&gt; &lt;a href="https://arxiv.org/html/2505.07260v1"&gt;https://arxiv.org/html/2505.07260v1&lt;/a&gt;&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2410.10456"&gt;https://arxiv.org/abs/2410.10456&lt;/a&gt; &lt;br /&gt; &lt;a href="https://arxiv.org/abs/2406.13233"&gt;https://arxiv.org/abs/2406.13233&lt;/a&gt; &lt;br /&gt; &lt;a href="https://arxiv.org/abs/2409.06669"&gt;https://arxiv.org/abs/2409.06669&lt;/a&gt;&lt;/p&gt; &lt;p&gt; Kimi especially has attempted this: &lt;a href="https://arxiv.org/abs/2502.13189"&gt;https://arxiv.org/abs/2502.13189&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's very challenging for us, as local LLM folks, to say this whether this is a breakthrough. Because while it appears promising, &lt;strong&gt;without mass GPU&lt;/strong&gt;, we can't absolutely say whether it will scale properly.&lt;/p&gt; &lt;p&gt;Still, I think it's worth preserving as there was some effort in the comments made to analyze the relevance of the concept. And the core idea - optimizing compute usage for the relevant tokens only - is promising.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kaggleqrdl"&gt; /u/kaggleqrdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixpca/sparse_adaptive_attention_moe_a_potential/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixpca/sparse_adaptive_attention_moe_a_potential/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oixpca/sparse_adaptive_attention_moe_a_potential/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T06:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixzfa</id>
    <title>VieNeuTTS - Open-source Vietnamese TTS Model that runs on CPU!</title>
    <updated>2025-10-29T07:18:24+00:00</updated>
    <author>
      <name>/u/DrCrab97</name>
      <uri>https://old.reddit.com/user/DrCrab97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixzfa/vieneutts_opensource_vietnamese_tts_model_that/"&gt; &lt;img alt="VieNeuTTS - Open-source Vietnamese TTS Model that runs on CPU!" src="https://external-preview.redd.it/Uj-sgTsqfQbGJqSwvTIUDlrNtIjPfwgO08gZ7RkupGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b6a611314e0f9c14ab311c17c6467a56eb273d94" title="VieNeuTTS - Open-source Vietnamese TTS Model that runs on CPU!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! üëã&lt;/p&gt; &lt;p&gt;I'm excited to share &lt;strong&gt;VieNeuTTS&lt;/strong&gt;, a Vietnamese text-to-speech model I've been working on. It's fine-tuned from neuphonic/neutts-air on 140 hours of Vietnamese audio data.&lt;/p&gt; &lt;h1&gt;üéØ Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Natural Vietnamese pronunciation&lt;/strong&gt; with accurate tones&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runs real-time on CPU&lt;/strong&gt; - no GPU required!&lt;/li&gt; &lt;li&gt;Built on &lt;strong&gt;Qwen 0.5B backbone&lt;/strong&gt; - optimized for mobile &amp;amp; embedded devices&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully offline&lt;/strong&gt; - works completely on your local machine&lt;/li&gt; &lt;li&gt;Fine-tuned on 140 hours (74.9k samples) of Vietnamese audio&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîó Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Try the demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/pnnbao-ump/VieNeuTTS"&gt;https://huggingface.co/spaces/pnnbao-ump/VieNeuTTS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/pnnbao-ump/VieNeu-TTS"&gt;https://huggingface.co/pnnbao-ump/VieNeu-TTS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/pnnbao97/VieNeu-TTS"&gt;https://github.com/pnnbao97/VieNeu-TTS&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/pnnbao-ump/VieNeu-TTS"&gt;https://huggingface.co/datasets/pnnbao-ump/VieNeu-TTS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your feedback and suggestions for improvement! Feel free to test it out and let me know what you think.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1oixzfa/video/gk9wi7zv40yf1/player"&gt;https://reddit.com/link/1oixzfa/video/gk9wi7zv40yf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrCrab97"&gt; /u/DrCrab97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixzfa/vieneutts_opensource_vietnamese_tts_model_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixzfa/vieneutts_opensource_vietnamese_tts_model_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oixzfa/vieneutts_opensource_vietnamese_tts_model_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T07:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia8fi</id>
    <title>The vLLM team's daily life be like:</title>
    <updated>2025-10-28T14:03:17+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt; &lt;img alt="The vLLM team's daily life be like:" src="https://external-preview.redd.it/ZDF3MmtiYW16dXhmMWptouG6uHo-mrPzGurb2qCOnKrlpr9yhnl7mMdksMxF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0aa03383224463057de137e1db2d57ee7e56cd5" title="The vLLM team's daily life be like:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A massive shout-out to the vLLM team for being the heroes holding it all together so we can actually run all these amazing new models.&lt;/p&gt; &lt;p&gt;And, of course, a huge thank you to all the open-source teams like DeepSeek, Qwen, Kimi, and so many others. You are all pushing the entire field forward. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lw255camzuxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiozl8</id>
    <title>MiniMax M2 Llama.cpp support</title>
    <updated>2025-10-28T23:27:22+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By popular demand, here it is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16831"&gt;https://github.com/ggml-org/llama.cpp/pull/16831&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll upload GGUFs to &lt;a href="https://huggingface.co/ilintar/MiniMax-M2-GGUF"&gt;https://huggingface.co/ilintar/MiniMax-M2-GGUF&lt;/a&gt;, for now uploading Q8_0 (no BF16/F16 since the original model was quantized in FP8) and generating imatrix. I don't expect problems with accepting this PR, as I said, the model is pretty typical :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiozl8/minimax_m2_llamacpp_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiozl8/minimax_m2_llamacpp_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiozl8/minimax_m2_llamacpp_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T23:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oimand</id>
    <title>An alternative to Microsoft's VibeVoice? Soul releases SoulX-Podcast-1.7B, a multi-speaker TTS model</title>
    <updated>2025-10-28T21:38:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oimand/an_alternative_to_microsofts_vibevoice_soul/"&gt; &lt;img alt="An alternative to Microsoft's VibeVoice? Soul releases SoulX-Podcast-1.7B, a multi-speaker TTS model" src="https://preview.redd.it/kqnfb23c9xxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e6cd2a86c6977531e4be1ccaea119c7c0c9a8a2" title="An alternative to Microsoft's VibeVoice? Soul releases SoulX-Podcast-1.7B, a multi-speaker TTS model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soul has just released SoulX-Podcast-1.7B, which looks like it might be trained based on Qwen3-1.7B. The current demo looks promising, but it's hard to say what the actual performance is like. I previously tested VibeVoice-1.5B and found that its performance was very poor during rapid switching between multiple speakers. I'm wondering if this new model will be any better. The model card hasn't been uploaded yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqnfb23c9xxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oimand/an_alternative_to_microsofts_vibevoice_soul/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oimand/an_alternative_to_microsofts_vibevoice_soul/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oifmg6</id>
    <title>IBM releases Granite-4.0 Nano (300M &amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf.</title>
    <updated>2025-10-28T17:25:26+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"&gt; &lt;img alt="IBM releases Granite-4.0 Nano (300M &amp;amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf." src="https://external-preview.redd.it/c3EyM2o0d2d5dnhmMQ3Ju84xO0NZTaEdmCFfUDcYCN9cnlFCq8u0lL0AKtmD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a2e66541a3e76c148bc13a8f5f718ef7b807643" title="IBM releases Granite-4.0 Nano (300M &amp;amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM just released Granite-4.0 Nano, their smallest LLMs to date (300M &amp;amp; 1B). The models demonstrate remarkable instruction following and tool calling capabilities, making them perfect for on-device applications.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Blog post: &lt;a href="https://huggingface.co/blog/ibm-granite/granite-4-nano"&gt;https://huggingface.co/blog/ibm-granite/granite-4-nano&lt;/a&gt;&lt;br /&gt; - Demo (+ source code): &lt;a href="https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;+ for those wondering, the demo uses Transformers.js to run the models 100% locally in your browser with WebGPU acceleration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s5hzz3wgyvxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T17:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj10kp</id>
    <title>Speculation or rumors on Gemma 4?</title>
    <updated>2025-10-29T10:36:19+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted a few days ago about &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;Granite 4 use cases&lt;/a&gt;, and then &lt;a href="https://huggingface.co/blog/ibm-granite/granite-4-nano"&gt;Granite 4 Nano&lt;/a&gt; models dropped yesterday. So I figured I'd see if luck holds and ask -- anyone have any good speculation or rumors about when we might see the next set of Gemma models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj10kp/speculation_or_rumors_on_gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj10kp/speculation_or_rumors_on_gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj10kp/speculation_or_rumors_on_gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T10:36:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiiz8k</id>
    <title>Poker Tournament for LLMs</title>
    <updated>2025-10-28T19:31:37+00:00</updated>
    <author>
      <name>/u/undoing8</name>
      <uri>https://old.reddit.com/user/undoing8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"&gt; &lt;img alt="Poker Tournament for LLMs" src="https://b.thumbs.redditmedia.com/Qc-E4Vt6HaSlMs8eFi3HmabBhuyGXwPfxNFOLNxKUVs.jpg" title="Poker Tournament for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Watch here: &lt;a href="https://pokerbattle.ai/event"&gt;https://pokerbattle.ai/event&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/undoing8"&gt; /u/undoing8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oiiz8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oivi1n</id>
    <title>tokens per second on a NASA computer</title>
    <updated>2025-10-29T04:39:44+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivi1n/tokens_per_second_on_a_nasa_computer/"&gt; &lt;img alt="tokens per second on a NASA computer" src="https://preview.redd.it/m1qjv8dkczxf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb88e0d4f5b61402d65a7d014f27955fb660c6f1" title="tokens per second on a NASA computer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;lm studio had a hiccup&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1qjv8dkczxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivi1n/tokens_per_second_on_a_nasa_computer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oivi1n/tokens_per_second_on_a_nasa_computer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T04:39:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oj1qa0</id>
    <title>dots.llm2 is coming...?</title>
    <updated>2025-10-29T11:15:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1qa0/dotsllm2_is_coming/"&gt; &lt;img alt="dots.llm2 is coming...?" src="https://preview.redd.it/i6wxpmesa1yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e64d36264e954e063e2a028e8ff47183637fb76e" title="dots.llm2 is coming...?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.llm1.inst"&gt;https://huggingface.co/rednote-hilab/dots.llm1.inst&lt;/a&gt; is 143B MoE model published about half year ago (supported by llama.cpp)&lt;/p&gt; &lt;p&gt;dots2: &lt;a href="https://x.com/xeophon_/status/1982728458791968987"&gt;https://x.com/xeophon_/status/1982728458791968987&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;The dots.llm2 model was introduced by the rednote-hilab team. It is a 30B/343B MoE (Mixture-of-Experts) model supporting a 256k context window.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i6wxpmesa1yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1qa0/dotsllm2_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oj1qa0/dotsllm2_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T11:15:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oixju1</id>
    <title>Serve 100 Large AI Models on a single GPU with low impact to time to first token.</title>
    <updated>2025-10-29T06:49:35+00:00</updated>
    <author>
      <name>/u/SetZealousideal5006</name>
      <uri>https://old.reddit.com/user/SetZealousideal5006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixju1/serve_100_large_ai_models_on_a_single_gpu_with/"&gt; &lt;img alt="Serve 100 Large AI Models on a single GPU with low impact to time to first token." src="https://external-preview.redd.it/btnFw6WNR_K0hbQlAX4ON96wRMWe4emQEMdrwMfRgxU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5167c99e3640fceba4130394274738d763bc91ff" title="Serve 100 Large AI Models on a single GPU with low impact to time to first token." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to build an inference provider for proprietary AI models, but I did not have a huge GPU farm. I started experimenting with Serverless AI inference, but found out that coldstarts were huge. I went deep into the research and put together an engine that loads large models from SSD to VRAM up to ten times faster than alternatives. It works with vLLM, and transformers, and more coming soon.&lt;/p&gt; &lt;p&gt;With this project you can hot-swap entire large models (32B) on demand.&lt;/p&gt; &lt;p&gt;Its great for: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Serverless AI Inference&lt;/li&gt; &lt;li&gt;Robotics&lt;/li&gt; &lt;li&gt;On Prem deployments&lt;/li&gt; &lt;li&gt;Local Agents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And Its open source.&lt;/p&gt; &lt;p&gt;Let me know if anyone wants to contribute :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SetZealousideal5006"&gt; /u/SetZealousideal5006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/leoheuler/flashtensors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oixju1/serve_100_large_ai_models_on_a_single_gpu_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oixju1/serve_100_large_ai_models_on_a_single_gpu_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T06:49:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiz7xs</id>
    <title>GPT-OSS Safeguard coming soon</title>
    <updated>2025-10-29T08:43:29+00:00</updated>
    <author>
      <name>/u/Independent-Ruin-376</name>
      <uri>https://old.reddit.com/user/Independent-Ruin-376</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiz7xs/gptoss_safeguard_coming_soon/"&gt; &lt;img alt="GPT-OSS Safeguard coming soon" src="https://preview.redd.it/iqua03z2k0yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf8615eeb25a6a5b2f7799521e7784e5fede5fd" title="GPT-OSS Safeguard coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Ruin-376"&gt; /u/Independent-Ruin-376 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iqua03z2k0yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiz7xs/gptoss_safeguard_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiz7xs/gptoss_safeguard_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T08:43:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oitanf</id>
    <title>Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080</title>
    <updated>2025-10-29T02:43:55+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"&gt; &lt;img alt="Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080" src="https://external-preview.redd.it/RI-49DjoVUsr4ENUSH69P2WGRcLvEX3r6pAVokLb70g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9b109f2dbdc8b8f12ceaf8eb5a8b638cb926f5c" title="Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We've been quietly grinding, and today, we're pumped to share the new release of KaniTTS English, as well as Japanese, Chinese, German, Spanish, Korean and Arabic models.&lt;/p&gt; &lt;p&gt;Benchmark on &lt;a href="https://vast.ai/"&gt;VastAI&lt;/a&gt;: RTF (Real-Time Factor) of ~0.2 on RTX4080, ~0.5 on RTX3060.&lt;/p&gt; &lt;p&gt;It has 400M parameters. We achieved this speed by pairing an &lt;a href="https://huggingface.co/LiquidAI/LFM2-350M"&gt;LFM2-350M&lt;/a&gt; backbone with an efficient &lt;a href="https://huggingface.co/nvidia/nemo-nano-codec-22khz-0.6kbps-12.5fps"&gt;NanoCodec&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It's released under the Apache 2.0 License so you can use it for almost anything.&lt;/p&gt; &lt;p&gt;What Can You Build? - Real-Time Conversation. - Affordable Deployment: It's light enough to run efficiently on budget-friendly hardware, like RTX 30x, 40x, 50x - Next-Gen Screen Readers &amp;amp; Accessibility Tools.&lt;/p&gt; &lt;p&gt;Model Page: &lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-en"&gt;https://huggingface.co/nineninesix/kani-tts-400m-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pretrained Checkpoint: &lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt"&gt;https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo with Fine-tuning/Dataset Preparation pipelines: &lt;a href="https://github.com/nineninesix-ai/kani-tts"&gt;https://github.com/nineninesix-ai/kani-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Space: &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OpenAI-Compatible API Example (Streaming): If you want to drop this right into your existing project, check out our vLLM implementation: &lt;a href="https://github.com/nineninesix-ai/kanitts-vllm"&gt;https://github.com/nineninesix-ai/kanitts-vllm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice Cloning Demo (currently unstable): &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS_Voice_Cloning_dev"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS_Voice_Cloning_dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our Discord Server: &lt;a href="https://discord.gg/NzP3rjB4SB"&gt;https://discord.gg/NzP3rjB4SB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-en"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T02:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oivxji</id>
    <title>Qwen3 Max Thinking this week</title>
    <updated>2025-10-29T05:04:42+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"&gt; &lt;img alt="Qwen3 Max Thinking this week" src="https://preview.redd.it/pbd1ylu1hzxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0a191c61d159739c3e9b620cdefab0a9534288f" title="Qwen3 Max Thinking this week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pbd1ylu1hzxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oivxji/qwen3_max_thinking_this_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T05:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM ‚Äì 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; ‚Üí &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
