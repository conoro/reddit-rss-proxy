<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-23T07:38:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rb2j5c</id>
    <title>Favourite niche usecases?</title>
    <updated>2026-02-21T21:06:34+00:00</updated>
    <author>
      <name>/u/Figai</name>
      <uri>https://old.reddit.com/user/Figai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt; &lt;img alt="Favourite niche usecases?" src="https://preview.redd.it/o4l2ankhxwkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7201facadd4e9d14e1aac7efef2133d85d346f7" title="Favourite niche usecases?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Figai"&gt; /u/Figai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4l2ankhxwkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rb2j5c/favourite_niche_usecases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T21:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc56wr</id>
    <title>Seed 1.6 Flash was the harshest AI judge in a 10-model blind eval â€” and that strictness correlated with better writing output</title>
    <updated>2026-02-23T02:24:31+00:00</updated>
    <author>
      <name>/u/Silver_Raspberry_811</name>
      <uri>https://old.reddit.com/user/Silver_Raspberry_811</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seed 1.6 Flash averaged 8.64/10 when scoring other models in a blind peer evaluation I ran, making it the strictest judge out of 10 frontier models. It penalized vague timelines and missing cost analysis while Grok 4.1 Fast handed out 9.8+ to 8 of 9 models like participation trophies. The task was persuasive business writing (convince a skeptical VP to migrate a monolith to microservices, 500 words, real constraints), and after excluding self-judgments I had 89 valid cross-evaluations. Rankings were tight: GPT-OSS-120B at 9.53, both Claudes at 9.47 and 9.46, down to Gemini Flash-Lite at 8.98. But the interesting part is the correlation between judging strictness and writing quality. The two strictest judges (Seed, GPT-OSS) ranked #6 and #1 as writers, while the two most lenient (Grok, Gemini Flash-Lite) ranked #8 and #10, which suggests models that can identify weakness in other outputs tend to avoid it in their own. DeepSeek V3.2 was the efficiency outlier, slowest generation at 27.5s but fewest tokens at 700 while still scoring 5th, basically the most information-dense writer in the pool. All 89 judgment pairs with justifications here: &lt;a href="https://open.substack.com/pub/themultivac/p/can-ai-write-better-business-proposals?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true"&gt;https://open.substack.com/pub/themultivac/p/can-ai-write-better-business-proposals?r=72olj0&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=true&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver_Raspberry_811"&gt; /u/Silver_Raspberry_811 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc56wr/seed_16_flash_was_the_harshest_ai_judge_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc56wr/seed_16_flash_was_the_harshest_ai_judge_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc56wr/seed_16_flash_was_the_harshest_ai_judge_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:24:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc9qvb</id>
    <title>Kitten TTS V0.8 Running in the Browser</title>
    <updated>2026-02-23T06:13:44+00:00</updated>
    <author>
      <name>/u/HatEducational9965</name>
      <uri>https://old.reddit.com/user/HatEducational9965</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc9qvb/kitten_tts_v08_running_in_the_browser/"&gt; &lt;img alt="Kitten TTS V0.8 Running in the Browser" src="https://preview.redd.it/9xhwneddp6lg1.png?width=140&amp;amp;height=85&amp;amp;auto=webp&amp;amp;s=abdc3e667c799020ffcc25437d0df7203f4119b7" title="Kitten TTS V0.8 Running in the Browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;took the recent release of Kitten v0.8 as an opportunity to explore handling audio data in the browser.&lt;/p&gt; &lt;p&gt;-&amp;gt; A minimal Next.JS app of Kitten TTS V0.8 running in the Browser&lt;/p&gt; &lt;p&gt;Features/Issue:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;All processing done on the client-side&lt;/li&gt; &lt;li&gt;Supports Nano/Micro/Mini Model, fetched from HF (+voice embeddings), cached on the client (OPFS)&lt;/li&gt; &lt;li&gt;Depends on onnxruntime-web and Xenova's phonemizer.js&lt;/li&gt; &lt;li&gt;wasm backend only&lt;/li&gt; &lt;li&gt;webgpu outputs silence, haven't figured that out yet &lt;/li&gt; &lt;li&gt;Doesn't work in Safari and on my Mobile Chrome (yet, maybe)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Demo: &lt;a href="https://next-voice.vercel.app"&gt;https://next-voice.vercel.app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/geronimi73/next-voice"&gt;https://github.com/geronimi73/next-voice&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9xhwneddp6lg1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f1dd89bbe6cba3785e3b194fe716849139fb52"&gt;https://preview.redd.it/9xhwneddp6lg1.png?width=1362&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=13f1dd89bbe6cba3785e3b194fe716849139fb52&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HatEducational9965"&gt; /u/HatEducational9965 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc9qvb/kitten_tts_v08_running_in_the_browser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc9qvb/kitten_tts_v08_running_in_the_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc9qvb/kitten_tts_v08_running_in_the_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T06:13:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc1h05</id>
    <title>MoOLE-T - a staged selection flow utilizing O-LORA skill "experts"</title>
    <updated>2026-02-22T23:39:12+00:00</updated>
    <author>
      <name>/u/Polymorphic-X</name>
      <uri>https://old.reddit.com/user/Polymorphic-X</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello again!&lt;br /&gt; Yesterday, I posted about my O-TITANS (Orthogonal Tensors for Independent Task Alignment) researchâ€”a way to train strictly isolated LoRAs on Gemma 3 that don't overwrite the base model's knowledge or interfere with each other.&lt;/p&gt; &lt;p&gt;Today, the actual orchestrator for those adapters is live.&lt;/p&gt; &lt;p&gt;Iâ€™ve uploaded the &lt;strong&gt;MoOLE-T (Mixture of Orthogonal LoRA Experts - Titans)&lt;/strong&gt; framework to Hugging Face: ðŸ”—&lt;a href="https://huggingface.co/paperscarecrow/Gemma3MoOLET/"&gt;https://huggingface.co/paperscarecrow/Gemma3MoOLET/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The value/theory:&lt;/strong&gt; Right now, if you want a model that is an expert at Python, cybersecurity, and creative writing, you have to download a massive, monolithic model that consumes tons of VRAM and takes a monumental effort to tune or train.&lt;/p&gt; &lt;p&gt;MoOLE-T seeks to change the architecture entirely by splitting the cognition.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Flow:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The Brainstem (4B Cognitive Router):&lt;/strong&gt; An overfitted &lt;code&gt;gemma-3-4b-it&lt;/code&gt; intercepts your prompt. It uses a &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; block to decompose the task and fires a deterministic routing token (e.g., &lt;code&gt;[ROUTE: code_python]&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Orchestrator:&lt;/strong&gt; A localized Python controller catches the token, checks your local &lt;code&gt;engrams.json&lt;/code&gt; dictionary, and dynamically hot-swaps the required O-TITANS &lt;code&gt;.pt&lt;/code&gt; files straight into VRAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Frontal Lobe (12B Synthesis Core):&lt;/strong&gt; A &lt;code&gt;gemma-3-12b-it-abliterated&lt;/code&gt; model acts as the execution engine. It catches the hot-swapped weights, synthesizes the hyper-specialized response, and then flushes the weights to return to a sterile baseline.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Vision going forward: A &amp;quot;Thingiverse&amp;quot; for Cognitive Skills.&lt;/strong&gt; Included in the repo is the orchestrator script, the training forge script, and my first production engram: an advanced Python coding expert (&lt;code&gt;otitans_code_python.pt&lt;/code&gt;). anyone can fine-tune a gemma model on a specific, narrow skillset and share it with he community for their own use.&lt;/p&gt; &lt;p&gt;The end goal here is to create a community-driven repository of hot-swappable skills. You should be able to download a 25MB &lt;code&gt;.pt&lt;/code&gt; file, drop it into your &lt;code&gt;/adapters/&lt;/code&gt; folder, update your JSON, and instantly grant your Swarm a new capability.&lt;br /&gt; I'll be seeding the repo with skills as I get them made, but this is where the distributed might of community can really help a lot.&lt;/p&gt; &lt;p&gt;If you use the included tuning script to forge your own skills, please contribute them to the hub and label them accurately! the more robust the set grows, the more useful this vision actually becomes.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: A &amp;quot;Featherweight&amp;quot; / Ultralight version utilizing a sub-1B parameter Reflex Arc router for CPU-only edge deployment is in active development. It's end state is a sub~4GB package that can run on almost anything, assuming it cooperates going forward.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Feedback is deeply appreciated, the previous thread was extremely valuable for motivating me to push forward with this, so thank you.&lt;br /&gt; I am not a strong coder (Gemini 3.1 is the reason this can even exist), so if there are major issues, feel free to call them out, fork your own, or put me on blast.&lt;/p&gt; &lt;p&gt;***EDIT***&lt;br /&gt; previous thread focused on the core O-TITANS &amp;quot;toolbelt&amp;quot;:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1rb4luf/otitans_orthogonal_loras_for_gemma_3_using/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Polymorphic-X"&gt; /u/Polymorphic-X &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1h05/moolet_a_staged_selection_flow_utilizing_olora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1h05/moolet_a_staged_selection_flow_utilizing_olora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1h05/moolet_a_staged_selection_flow_utilizing_olora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T23:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbmnw7</id>
    <title>Is there *any* good coding agent software for use with local models?</title>
    <updated>2026-02-22T14:04:29+00:00</updated>
    <author>
      <name>/u/eapache</name>
      <uri>https://old.reddit.com/user/eapache</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude Code seems to be &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;taking steps&lt;/a&gt; to make it more and more difficult to use with local models with things like forcing the context to constantly be recalculated. OpenCode has made the decision to basically not have a permissions model and just &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r8oehn/opencode_arbitrary_code_execution_major_security/"&gt;allow the LLM to execute whatever code it wants&lt;/a&gt;. Cline was &lt;a href="https://www.reddit.com/r/CLine/comments/1r9p3ww/supply_chain_attack_on_cline_installs_openclaw/"&gt;made to install OpenClaw on users machines&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;All I want is a stable, secure, permission-sensible coding agent, that I trust to run without eighteen layers of sandboxing. So Claude Code, but one that I can easily run against a local model. Does it not exist?&lt;/p&gt; &lt;p&gt;I know there are other competitors in this space (Roo, Pi, ...) but at this point I was hoping for a positive recommendation before I waste more time evaluating garbage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eapache"&gt; /u/eapache &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbmnw7/is_there_any_good_coding_agent_software_for_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:04:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbvbzt</id>
    <title>Best open-source coder model for replacing Claude Code with Qwen locally?</title>
    <updated>2026-02-22T19:40:34+00:00</updated>
    <author>
      <name>/u/pauljeba</name>
      <uri>https://old.reddit.com/user/pauljeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m currently using Claude Code but want to move fully local.&lt;/p&gt; &lt;p&gt;Iâ€™m specifically looking for a strong coding model for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude code like capaiblities - code + bash &lt;/li&gt; &lt;li&gt;Long file capabiliites&lt;/li&gt; &lt;li&gt;Read image, files&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Iâ€™m considering &lt;code&gt;Qwen3-Coder&lt;/code&gt;, but Iâ€™m unsure:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is &lt;code&gt;Qwen3-Coder&lt;/code&gt; the best choice for a 12GB GPU?&lt;/li&gt; &lt;li&gt;Should I instead run a smaller Qwen coder model (7B/14B) quantized?&lt;/li&gt; &lt;li&gt;Are there better alternatives that outperform Qwen for coding in this VRAM range?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Would appreciate real-world experience. If there is an hardward upgrade recommendation what would that be.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pauljeba"&gt; /u/pauljeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvbzt/best_opensource_coder_model_for_replacing_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:40:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1rblce7</id>
    <title>I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful</title>
    <updated>2026-02-22T13:03:49+00:00</updated>
    <author>
      <name>/u/Weird_Search_4723</name>
      <uri>https://old.reddit.com/user/Weird_Search_4723</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt; &lt;img alt="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" src="https://external-preview.redd.it/NWtrYWtuYXZuMWxnMexVgBFEBEtAfoKpFzO1VgJV4m4gRx-YBoBnOCuCCbAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f4bb616205fb72d1541634b6985338275c23ac3" title="I created yet another coding agent - Its tiny and fun (atleast for me), hope the community finds it useful" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is Kon telling you about it's own repo, using glm-4.7-flash-q4 running locally on my i7-14700F Ã— 28, 64GB RAM, 24GB VRAM (RTX 3090) â€“ video is sped up 2x&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;github: &lt;a href="https://github.com/kuutsav/kon"&gt;https://github.com/kuutsav/kon&lt;/a&gt;&lt;br /&gt; pypi: &lt;a href="https://pypi.org/project/kon-coding-agent/"&gt;https://pypi.org/project/kon-coding-agent/&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The pitch (in the readme as well):&lt;/p&gt; &lt;p&gt;It has a tiny harness: about &lt;strong&gt;215 tokens&lt;/strong&gt; for the system prompt and around &lt;strong&gt;600 tokens&lt;/strong&gt; for tool definitions â€“ so under 1k tokens before conversation context.&lt;/p&gt; &lt;p&gt;At the time of writing this README (22 Feb 2026), this repo has 112 files and is easy to understand in a weekend. Hereâ€™s a rough file-count comparison against a couple of popular OSS coding agents:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ fd . | cut -d/ -f1 | sort | uniq -c | sort -rn 4107 opencode 740 pi-mono 108 kon &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Others are of course more mature, support more models, include broader test coverage, and cover more surfaces. But if you want a truly minimal coding agent with batteries included â€“ something you can understand, fork, and extend quickly â€“ Kon might be interesting.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;It takes lots of inspiration from &lt;a href="https://github.com/badlogic/pi-mono/tree/main/packages/coding-agent"&gt;pi-coding-agent&lt;/a&gt;, see the &lt;a href="https://github.com/kuutsav/kon?tab=readme-ov-file#acknowledgements"&gt;acknowledgements&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit 1: this is a re-post, deleted the last one (missed to select video type when creating the post)&lt;br /&gt; Edit 2: more about the model that was running in the demo and the config: &lt;a href="https://github.com/kuutsav/kon/blob/main/LOCAL.md"&gt;https://github.com/kuutsav/kon/blob/main/LOCAL.md&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Search_4723"&gt; /u/Weird_Search_4723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jf0xcw9vn1lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rblce7/i_created_yet_another_coding_agent_its_tiny_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T13:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbvmpk</id>
    <title>Running Llama 3.2 1B entirely on an AMD NPU on Linux (Strix Halo, IRON framework, 4.4 tok/s)</title>
    <updated>2026-02-22T19:51:45+00:00</updated>
    <author>
      <name>/u/SuperTeece</name>
      <uri>https://old.reddit.com/user/SuperTeece</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got Llama 3.2 1B running inference entirely on the AMD NPU on Linux. Every operation (attention, GEMM, RoPE, RMSNorm, SiLU, KV cache) runs on the NPU; no CPU or GPU fallback. As far as I can tell, this is the first time anyone has publicly documented this working on Linux.&lt;/p&gt; &lt;h2&gt;Hardware&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;AMD Ryzen AI Max+ 395 (Strix Halo)&lt;/li&gt; &lt;li&gt;NPU: XDNA2, device ID npu5 (PCI 1022:17f0)&lt;/li&gt; &lt;li&gt;64GB LPDDR5X unified memory&lt;/li&gt; &lt;li&gt;Fedora 43, kernel 6.18.8&lt;/li&gt; &lt;li&gt;Model: meta-llama/Llama-3.2-1B (official Meta weights)&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;pre&gt;&lt;code&gt;Prefill time: 0.6921 seconds (13 tokens) Tokens generated: 20 Tokens per second: 4.40 Time per token: 0.2638 seconds &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NPU validation benchmark: &lt;strong&gt;51.0 TOPS&lt;/strong&gt; (GEMM, via xrt-smi validate).&lt;/p&gt; &lt;h2&gt;Scaling&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="center"&gt;Prompt Length&lt;/th&gt; &lt;th align="center"&gt;Prefill (s)&lt;/th&gt; &lt;th align="center"&gt;Prefill tok/s&lt;/th&gt; &lt;th align="center"&gt;Decode tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="center"&gt;13&lt;/td&gt; &lt;td align="center"&gt;0.67&lt;/td&gt; &lt;td align="center"&gt;19&lt;/td&gt; &lt;td align="center"&gt;4.46&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center"&gt;128&lt;/td&gt; &lt;td align="center"&gt;0.71&lt;/td&gt; &lt;td align="center"&gt;180&lt;/td&gt; &lt;td align="center"&gt;4.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="center"&gt;2048&lt;/td&gt; &lt;td align="center"&gt;2.22&lt;/td&gt; &lt;td align="center"&gt;923&lt;/td&gt; &lt;td align="center"&gt;4.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Decode is flat at ~4.4 tok/s regardless of prompt length. Prefill scales well (923 tok/s at 2048 tokens).&lt;/p&gt; &lt;h2&gt;The Stack&lt;/h2&gt; &lt;p&gt;Getting here required building everything from source. Fedora 43's in-tree amdxdna driver (v0.1) is too old, so you need the out-of-tree v1.0.0 from amd/xdna-driver on GitHub. That build also produces the dev firmware and XRT 2.23 libraries. On top of that, AMD's IRON framework (also on GitHub) plus mlir-aie v1.2.0 handle the actual NPU programming.&lt;/p&gt; &lt;p&gt;GCC 15 on Fedora 43 breaks the XRT build at link time (cannot find -lstdc++). Fix:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LIBRARY_PATH=/usr/lib/gcc/x86_64-redhat-linux/15:/usr/lib64:$LIBRARY_PATH &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;IRON also hardcodes llvm-objcopy-18 but Fedora ships LLVM 21, so you need a symlink.&lt;/p&gt; &lt;h2&gt;Where the Time Goes&lt;/h2&gt; &lt;p&gt;Profiling revealed the bottleneck: &lt;strong&gt;179 kernel dispatches per token&lt;/strong&gt;, averaging 1.4ms each through XRT. That's 75% of inference time in dispatch overhead, not compute. Buffer I/O via unified memory is fast (sub-0.1ms). The optimization path is fewer, larger dispatches via operator fusion.&lt;/p&gt; &lt;p&gt;4.4 tok/s from a 1B model won't replace GPU inference. On the same machine, Qwen3-32B (32x larger) runs at 6-7 tok/s on the GPU via Vulkan. But the NPU validated at 51 TOPS, so the gap is a software problem, not hardware. The NPU also runs independently, so you could run an LLM on it while the GPU does something else.&lt;/p&gt; &lt;h2&gt;Gotchas&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;prompt_len must match your actual token count (IRON compiles RoPE kernels for a fixed sequence length)&lt;/li&gt; &lt;li&gt;First run takes ~10 minutes to compile NPU kernels (cached after that)&lt;/li&gt; &lt;li&gt;Must use insmod for the out-of-tree driver; modprobe loads the stock one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote up the full walkthrough in a three-part blog series (linked in comments). Happy to answer setup questions.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;A note on how this was made: the research, testing, debugging, and writing was done by Ellie, an AI assistant backed by Claude Opus 4.6 (Anthropic) and local models. TC provided the hardware, direction, and editorial guidance. We believe in transparency about AI involvement in technical work.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note from TC:&lt;/strong&gt; I admit that this work is out of my technical depth. My motivation came from annoyance at having an NPU that was apparently useless on Linux and curiosity if Ellie (Opus) could connect together any other work being done on the topic to at least move the needle a smidge. If anyone is reading this post and knows it to be slop on a technical level, I'd love to hear why for my own edification. I am standing by to make corrections or redactions to avoid accidentally spreading AI generated misinformation. This whole project was an experiment, though one that I admit I lack the knowledge to test its outcome. I hope to hear from those who do and that it is useful in some way. -TC&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuperTeece"&gt; /u/SuperTeece &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbvmpk/running_llama_32_1b_entirely_on_an_amd_npu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T19:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rcb1n2</id>
    <title>Looking for an MCP that semantically searches for working snippets of code</title>
    <updated>2026-02-23T07:29:41+00:00</updated>
    <author>
      <name>/u/babble_prune</name>
      <uri>https://old.reddit.com/user/babble_prune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Often, Claude still messes up on common frontend patterns. When that happens, sometimes I can give Claude documentation (eg for implementing supabase auth). But other times, docs don't have the answer (eg for swift / macOS, unfocusing an input box when the user clicks elsewhere). The code with the relevant patterns is &lt;em&gt;probably&lt;/em&gt; in some open source repos, but I just don't know which ones or where to find them. I think that a lot of &amp;quot;unhobbling&amp;quot; could be gained with a powerful search of existing code, and I'm wondering if anyone uses a tool for this or something adjacent.&lt;/p&gt; &lt;p&gt;I just found &lt;a href="https://vercel.com/blog/grep-a-million-github-repositories-via-mcp"&gt;Grep MCP&lt;/a&gt; by vercel but I'm skeptical because it uses regex/patterns. I should try it -- but I'm looking for something closer to semantic search. Like &amp;quot;search for a chat input box for tailwind + react and condition on existing code to generate this code&amp;quot;. I would pay for this if it worked.&lt;/p&gt; &lt;p&gt;Aside: I wonder if a massive &lt;a href="https://en.wikipedia.org/wiki/A_Pattern_Language"&gt;pattern language&lt;/a&gt; of UI problems and code solutions would work. With a very lightweight LLM that does the search, maybe with the help of some semantic clustering (eg user interface) and structured clustering (eg tailwind css + react).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/babble_prune"&gt; /u/babble_prune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcb1n2/looking_for_an_mcp_that_semantically_searches_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rcb1n2/looking_for_an_mcp_that_semantically_searches_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rcb1n2/looking_for_an_mcp_that_semantically_searches_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T07:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1rca5p7</id>
    <title>Which model for meeting transcript summarisation?</title>
    <updated>2026-02-23T06:37:04+00:00</updated>
    <author>
      <name>/u/peglegsmeg</name>
      <uri>https://old.reddit.com/user/peglegsmeg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello&lt;/p&gt; &lt;p&gt;I'm using qwen3 30B A3B 2507 4bit with lm studio for feeding meeting transcripts for summary.&lt;/p&gt; &lt;p&gt;Does this seem like an okay model for the task? Feeling a bit overwhelmed with all the options, I'm only using because a cloud AI suggested it but it might not be current.&lt;/p&gt; &lt;p&gt;I was using Claude API with amazing results but no longer want to send to public offerings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/peglegsmeg"&gt; /u/peglegsmeg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rca5p7/which_model_for_meeting_transcript_summarisation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rca5p7/which_model_for_meeting_transcript_summarisation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rca5p7/which_model_for_meeting_transcript_summarisation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T06:37:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbtfld</id>
    <title>What Other Subs Do you Read to Keep Up with AI?</title>
    <updated>2026-02-22T18:29:17+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering what other subs do you recommend to read to keep up with AI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbtfld/what_other_subs_do_you_read_to_keep_up_with_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T18:29:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc88vr</id>
    <title>What GPU do you recommend for iterative AI training?</title>
    <updated>2026-02-23T04:53:56+00:00</updated>
    <author>
      <name>/u/EliHusky</name>
      <uri>https://old.reddit.com/user/EliHusky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've racked up a disgusting bill with runpod and think it is time to get my own workstation. &lt;/p&gt; &lt;p&gt;I usually choose GPUs based on the model Iâ€™m working with (e.g., RTX Pro 6000 Blackwell for LLMs/VLMs/diffusion, 4090 for smaller TCNs/LSTMs), but honestly I often pick higher-end GPUs more for throughput than VRAM.&lt;/p&gt; &lt;p&gt;So I'm curious, what kinds/sizes of models are you training, and what GPU are you using (or wish you were using)? &lt;/p&gt; &lt;p&gt;My first choice is obviously the pro 6000 blackwell to never think twice about batch size or parameter count again, but the cost doesn't quite justify &amp;quot;ease of use/peace of mind&amp;quot; to me. &lt;/p&gt; &lt;p&gt;Iâ€™m heavily leaning toward a 5090... but Iâ€™m saying that while staring at a RunPod session using 31GB VRAM for a 1.5B parameter fine-tune, so Iâ€™m not exactly confident I wonâ€™t regret it. I've also considered getting two 5090s but the lack of nvlink (I've never touched a multi-gpu setup) and the wattage requirements are a turnoff, not to mention we're getting back into the pro 6000 blackwell price range. I build my own pipelines and collect my own data, so iterative training and testing means speed is arguably just as important as VRAM.&lt;/p&gt; &lt;p&gt;I'm completely satisfied with running large model inference off of system ram, so this isn't a deciding factor.&lt;/p&gt; &lt;p&gt;I've done a ton of research, tried and tested a half dozen cards through runpod, and still can't seem to find the most reasonable gpu, so any personal experiences anyone has to share would be greatly appreciated. &lt;/p&gt; &lt;p&gt;TL;DR: what GPU(s) do you have and would you recommend it to someone looking to buy their first at-home AI workstation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EliHusky"&gt; /u/EliHusky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc88vr/what_gpu_do_you_recommend_for_iterative_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T04:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbyg5x</id>
    <title>If you have a RTX 5090 (that has a single connector), you can flash the MSI Lighting 800W VBIOS to get a lower power limit of 300W (and a max power of 660W).</title>
    <updated>2026-02-22T21:36:36+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you guys are doing fine.&lt;/p&gt; &lt;p&gt;As you know, NVIDIA artificially limited the power limit on the 5090s so you don't stack them, and get 6000 PROs instead (6000 PRO can go down to 150W). Even when undervolted it can use 400W sometimes.&lt;/p&gt; &lt;p&gt;If you got a RTX 5090 with a single connector (basically most of them except the BTF versions, and MSI Lighting), you can flash the 800W Lighting VBIOS to get a power limit.&lt;/p&gt; &lt;p&gt;When setting a 400W power limit (50%), it uses 300W max instead.&lt;/p&gt; &lt;p&gt;Why would you ask?&lt;/p&gt; &lt;p&gt;This is because the VBIOS excepts another source of power, and since it isn't there, it over reports the power on the software. Take it as a inverted shunt mod.&lt;/p&gt; &lt;p&gt;The VBIOS is here &lt;a href="https://www.techpowerup.com/vgabios/281640/281640"&gt;https://www.techpowerup.com/vgabios/281640/281640&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;As always with VBIOS flashing, do it at your own risk!&lt;/strong&gt; &lt;strong&gt;If you don't trust this or haven't heard about BIOS flashing, I suggest to not do it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On ASUS cards you lose 1 HDMI, but if you have Astral-Matrix, you keep the pin monitoring power.&lt;/p&gt; &lt;p&gt;You can get nvflash on here &lt;a href="https://www.techpowerup.com/download/nvidia-nvflash/"&gt;https://www.techpowerup.com/download/nvidia-nvflash/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Once on Windows, with nvflash64 and the rom file on the same folder, you run this (on cmd as admin):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvflash64 -6 romname.rom press y press y reboot &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And you're good to go! This also works on LACT.&lt;/p&gt; &lt;p&gt;I have made this table with the info for power for reference.&lt;/p&gt; &lt;p&gt;Scaling 800W VBIOS&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50% is 300W real power usage (reported 400W on software)&lt;/li&gt; &lt;li&gt;53% is 321W (reported 424W)&lt;/li&gt; &lt;li&gt;54% is 330W (reported 432W)&lt;/li&gt; &lt;li&gt;55% is 338W (reported 440W)&lt;/li&gt; &lt;li&gt;56% is 345W (reported 448W)&lt;/li&gt; &lt;li&gt;57% is 352W (reported 456W)&lt;/li&gt; &lt;li&gt;59% is 367W (reported 472W)&lt;/li&gt; &lt;li&gt;60% is 375W (reported 480W)&lt;/li&gt; &lt;li&gt;61% is 382W (reported 488W)&lt;/li&gt; &lt;li&gt;62% is 388W (reported 496W)&lt;/li&gt; &lt;li&gt;63% is 397W (reported 504W)&lt;/li&gt; &lt;li&gt;64% is 403W (reported 512W)&lt;/li&gt; &lt;li&gt;73% is 468W (reported 584W)&lt;/li&gt; &lt;li&gt;74% is 478W (reported 592W)&lt;/li&gt; &lt;li&gt;91% is 594W (reported 728W)&lt;/li&gt; &lt;li&gt;92% is 610W (reported 736W)&lt;/li&gt; &lt;li&gt;100% is 660W (reported 800W)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There's also similar behavior for the 1000W and 2500W VBIOS, but those have a higher min power (about 320W), so the 800W is the best one for that and also the safest.&lt;/p&gt; &lt;p&gt;I tried on Linux, since there's nvflash there as well, but got an error about memory address. On Windows flashing works just fine.&lt;/p&gt; &lt;p&gt;Any question is welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbyg5x/if_you_have_a_rtx_5090_that_has_a_single/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T21:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbwbgl</id>
    <title>nanollama â€” train Llama 3 from scratch and export to GGUF, one command, open source</title>
    <updated>2026-02-22T20:17:50+00:00</updated>
    <author>
      <name>/u/ataeff</name>
      <uri>https://old.reddit.com/user/ataeff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;nanollama â€” train Llama 3 from scratch. &lt;/p&gt; &lt;p&gt;I've been working on a framework for training Llama 3 architecture models from scratch: not fine-tuning, not LoRA, actual from-zero pretraining. The output is a llama.cpp-compatible GGUF file.&lt;/p&gt; &lt;p&gt;The whole pipeline is one command:&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;bash runs/lambda_train.sh --name mini&lt;/p&gt; &lt;p&gt;'''&lt;/p&gt; &lt;p&gt;This downloads training data, trains the model, and exports GGUF. Verified with llama-cli.&lt;/p&gt; &lt;p&gt;In the the box:&lt;/p&gt; &lt;p&gt;- Llama 3 architecture (RoPE, SwiGLU, RMSNorm, GQA), 8 configs from 46M to 7B&lt;/p&gt; &lt;p&gt;- multi-corpus training (FineWeb-Edu, DCLM, code, math â€” SmolLM2 recipe)&lt;/p&gt; &lt;p&gt;- native GGUF v3 exporter (no HuggingFace/safetensors conversion)&lt;/p&gt; &lt;p&gt;- personality injection â€” train base + personality model, subtract weights, get a portable personality vector you can apply to any compatible base&lt;/p&gt; &lt;p&gt;- pure Go inference engine (~9MB binary, reads GGUF, zero runtime deps) for when you don't need the full llama.cpp stack&lt;/p&gt; &lt;p&gt;- beginner's guide â€” first model in ~30 min on a rented GPU for a few bucks &lt;/p&gt; &lt;p&gt;Trained and verified so far: nano (46M), micro (87M), mini (175M), small (338M). goldie (1.1B, multilingual) is training now.&lt;/p&gt; &lt;p&gt;The point: there's no clean, modern &amp;quot;train from scratch&amp;quot; pipeline for Llama-family models. nanoGPT/nanochat did this for GPT-2, but GPT-2 is 2019 architecture. This is the same idea updated for 2026.&lt;/p&gt; &lt;p&gt;Born from karpathy's nanochat, rewritten for Llama 3. GPLv3.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/ariannamethod/nanollama"&gt;https://github.com/ariannamethod/nanollama&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release: &lt;a href="https://github.com/ariannamethod/nanollama/releases/tag/v0.1.0"&gt;https://github.com/ariannamethod/nanollama/releases/tag/v0.1.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ataeff"&gt; /u/ataeff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbwbgl/nanollama_train_llama_3_from_scratch_and_export/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T20:17:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbjxpv</id>
    <title>I think openclaw is OVERHYPED. Just use skills</title>
    <updated>2026-02-22T11:51:38+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think openclaw is useful, loop, memory, agents, integrations, but after a week a testing, honestly I don't need it much.&lt;/p&gt; &lt;p&gt;- memory, is nice. But I prefere to have &amp;quot;manual memory&amp;quot;. Prompt: Ok, write what yout learnt in &amp;quot;superreporttrending-skill&amp;quot;. Automatic memory often pollute the context of info you don't care.&lt;/p&gt; &lt;p&gt;- cron. Useful but I already use other tools for that and I can always recall a skill whenever i want. I don't need everyday at 8:00AM, i prefere recall it when i want with up to date data&lt;/p&gt; &lt;p&gt;Conclusion: for me &amp;quot;opencode web&amp;quot; is a much superior option, but much of the &amp;quot;intelligence&amp;quot; and value is the skills that you develop or you integrate, not in the runner itself, what do you think ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbjxpv/i_think_openclaw_is_overhyped_just_use_skills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T11:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbnczy</id>
    <title>The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets.</title>
    <updated>2026-02-22T14:34:36+00:00</updated>
    <author>
      <name>/u/w1nter5n0w</name>
      <uri>https://old.reddit.com/user/w1nter5n0w</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt; &lt;img alt="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." src="https://preview.redd.it/l8duwvse42lg1.png?width=140&amp;amp;height=106&amp;amp;auto=webp&amp;amp;s=2928d1df2289068d0491626609ab2109106409dc" title="The Qwen team verified that there are serious problems with the data quality of the GPQA and HLE test sets." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About a month ago, a friend of mine posted a thread here (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qhz9e2/research_i_forensicaudited_humanitys_last_exam/&lt;/a&gt;) regarding a project he started called &lt;strong&gt;DeepSeek-Overclock&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The goal was to create an experimental setup designed to theoretically push the model's reasoning capabilities to the absolute limit. However, the &amp;quot;overclocked&amp;quot; DeepSeek model kept failing during the process. After diving deep into the logs, he realized the model wasn't hallucinating. In many instances, it was rigorously deriving answers that were technically correct but contradicted the provided &amp;quot;gold standard&amp;quot; labels.&lt;/p&gt; &lt;p&gt;He ended up writing Python scripts to verify the math line-by-line from first principles. Then he found out that &lt;strong&gt;the data quality in both the GPQA and HLE (Humanity's Last Exam) test sets is seriously flawed.&lt;/strong&gt; (You can check the link above for the specific details of that investigation).&lt;/p&gt; &lt;p&gt;Fast forward to a couple of days ago, and the &lt;strong&gt;Qwen team just released a paper&lt;/strong&gt; that basically confirms exactly what we saw: the data quality in GPQA and HLE is a mess.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20"&gt;https://preview.redd.it/l8duwvse42lg1.png?width=1291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=faffe857435fb66cfd990db707f41333e58fcc20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attached the screenshot of Fig. 1: Structural composition of HLE-Verified.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arxiv Link:&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2602.13964v2"&gt;https://arxiv.org/abs/2602.13964v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The paper doesn't mince words. Right from the intro, it bluntly points out that a lot of the questions in the HLE test set are fundamentally broken. And in some cases, &amp;quot;standard answers&amp;quot; that are straight-up wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/w1nter5n0w"&gt; /u/w1nter5n0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbnczy/the_qwen_team_verified_that_there_are_serious/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T14:34:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc00nj</id>
    <title>In the long run, everything will be local</title>
    <updated>2026-02-22T22:39:00+00:00</updated>
    <author>
      <name>/u/tiguidoio</name>
      <uri>https://old.reddit.com/user/tiguidoio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt; &lt;img alt="In the long run, everything will be local" src="https://preview.redd.it/vqzxm46ri4lg1.png?width=140&amp;amp;height=105&amp;amp;auto=webp&amp;amp;s=f9899bff14b8d1409da4cbfaa0a56aa74bb136e5" title="In the long run, everything will be local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been of the opinion for a while that, long term, weâ€™ll have smart enough open models and powerful enough consumer hardware to run &lt;em&gt;all&lt;/em&gt; our assistants locally both chatbots and coding copilots&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c0fb257d744350f8668301a915aeec2b6653fc"&gt;https://preview.redd.it/vqzxm46ri4lg1.png?width=3608&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c0fb257d744350f8668301a915aeec2b6653fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Right now it still feels like thereâ€™s a trade-off:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Closed, cloud models = best raw quality, but vendor lock-in, privacy concerns, latency, per-token cost&lt;/li&gt; &lt;li&gt;Open, local models = worse peak performance, but full control, no recurring API fees, and real privacy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But if you look at the curve on both sides, itâ€™s hard not to see them converging:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Open models keep getting smaller, better, and more efficient every few months (quantization, distillation, better architectures). Many 7Bâ€“8B models are already good enough for daily use if you care more about privacy/control than squeezing out the last 5% of quality&lt;/li&gt; &lt;li&gt;Consumer and prosumer hardware keeps getting cheaper and more powerful, especially GPUs and Apple Siliconâ€“class chips. People are already running decent local LLMs with 12â€“16GB VRAM or optimized CPU-only setups for chat and light coding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At some point, the default might flip: instead of why would you run this locally?, the real question becomes why would you ship your entire prompt and codebase to a third-party API if you donâ€™t strictly need to? For a lot of use cases (personal coding, offline agents, sensitive internal tools), a strong local open model plus a specialized smaller model might be more than enough&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tiguidoio"&gt; /u/tiguidoio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc00nj/in_the_long_run_everything_will_be_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T22:39:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc1ra2</id>
    <title>My real-world Qwen3-code-next local coding test. So, Is it the next big thing?</title>
    <updated>2026-02-22T23:51:14+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt; &lt;img alt="My real-world Qwen3-code-next local coding test. So, Is it the next big thing?" src="https://preview.redd.it/44qd636p15lg1.png?width=140&amp;amp;height=14&amp;amp;auto=webp&amp;amp;s=35817da51dcc5387e5bc0d9209c8558f639ab5f3" title="My real-world Qwen3-code-next local coding test. So, Is it the next big thing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday I put the Q8 MLX on my 128GB Mac Studio Ultra and wired it to Qwen Code CLI. Fit's there with a huge amount to spare. The first tests were promising - basically did everything I asked: read file, write file, browse web, check system time....blah, blah.&lt;/p&gt; &lt;p&gt;Now the real the task:&lt;/p&gt; &lt;p&gt;I decided on YOLO mode to rewrite the KittenTTS-IOS to windows (which itself is a rewrite of KittenTTS in python). It uses ONYX and a couple of Swift libraries like Misaki for English phoneme.&lt;/p&gt; &lt;p&gt;So, say a medium difficulty. Not super easy, but not super hard, because all the code is basically there. You just need to shake it.&lt;/p&gt; &lt;p&gt;Here is how it went:&lt;/p&gt; &lt;p&gt;Started very well. Plan was solid. Make simple CLI with KittenTTS model, avoid any phoneme manipulation for now. Make ONYX work. Then add Misaki phoneme, avoid bart fallback coz that's a can of worms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;So it built the main.cpp. Rewrote the main app, created it's own json parser for the KittenTTS dictionary. found windows ONYX, downloaded, linked. ran cmake captured the output, realised it's json parsing was a total crap. Linked &amp;lt;nlohmann/json.hpp&amp;gt; .... aaaaand we are out.&lt;/li&gt; &lt;li&gt;First client timeout then &amp;quot;I'm dead, Dave&amp;quot;. As we get more and more into longer context the prompt parsing gets longer and longer until the client times out.&lt;/li&gt; &lt;li&gt;Restarted maually, told it we are at json.hpp, it finished the patching, compiled - created output.wav&lt;/li&gt; &lt;li&gt;I'm impressed so far. The wav has voice in it, of course all gibberish because we have no phoneme dictionary. The make file is unreadable can of worms.&lt;/li&gt; &lt;li&gt;Next step convert phoneme Misaki to windows. Big hairy project. Again, started cheerful. But we are now editing large files. It can barely finish anything before timeout.&lt;/li&gt; &lt;li&gt;Lot's of manual restarts. (YOLO mode my butt, right?). At some point it starts editing the Swift files, thinking that's what we are doing. Noooo!!!!&lt;/li&gt; &lt;li&gt;I've noticed that most of the time it wastes tokens on trying to figure out how to do stuff like save file it wants to save, because now &amp;quot;it's just too big&amp;quot;. Even starts writing python script to save the file then entering the entire text of lexicon.cpp as a command line - LOL, learning, that's a very stupid thing too.&lt;/li&gt; &lt;li&gt;I mean nice to learn from mistakes, but we are getting to timeouts all the time now by filling the context with unnecessary work. And it of course learns nothing, because that knowledge is lost.&lt;/li&gt; &lt;li&gt;I spent another 60 minutes trying to figure out how to fix qwen code by increasing timeout. Not an easy task as every AI will just hallucinate what you should do. I moved from anthropic style to openai style for the QWEN3 and set generationConfig.timeout to a big number (I have no idea if this even works). Set the KV_cache to quantize at 8 bit in LM studio (again, no idea if it helps). Seems the timeouts are now longer? So maybe a small win?&lt;/li&gt; &lt;li&gt;Well, went to sleep, letting it do something.&lt;/li&gt; &lt;li&gt;In the next day the phoneme test.exe was working sort of (at least it was not throwing 5 pages of errors) - read the 400k phoneme dictionary and output bunch of nonsense, like lookup: Hello -&amp;gt; hâ•”Ã–lO (Is this the correct phoneme? Hardly. Seems we are getting lost in ISO/UDF nightmare) Well, Qwen doesn't know what's going on either.&lt;/li&gt; &lt;li&gt;At this point neither me nor Qwen knows if we are fixing bugs or buggyfying working code. But he is happily doing something.&lt;/li&gt; &lt;li&gt;And writing jokes that get a bit stale after while: &amp;quot;Why do Java developers wear glasses? Because they don't C#&amp;quot;&lt;/li&gt; &lt;li&gt;I start to miss Claude Code. Or Codex. Or anything that doesn't take 30 minutes per turn then tell me client timeout.&lt;/li&gt; &lt;li&gt;It is still fixing it and writing stupid one liner jokes on screen. I mean &amp;quot;fixing it&amp;quot; means sitting in Prompt processing.&lt;/li&gt; &lt;li&gt;Funny, MAC Studio is barely warm. Like it was working nonstop for 8 hours with 89GB model .&lt;/li&gt; &lt;li&gt;The processing prompt is still killing the whole operation. As the context grows, this is a few minutes per turn.&lt;/li&gt; &lt;li&gt;I totally believe the X grifters telling me they bough 10 MAC's for local Agentic work.... yes, sure. You can have huge memory but large context is still going to be snail pace.&lt;/li&gt; &lt;li&gt;19. Looking at the terminal &amp;quot;Just a sec, I'm optimizing the humor... (esc to cancel, 29m 36s)&amp;quot;, been doing something for 30 min. Looking at mac log, generating token, now at around 60k tokens and still going up - a really long output that we will probably never be able to do anything with.&lt;/li&gt; &lt;li&gt;I give Local model coding 5/10 so far. It does kinda work if you have the enormous patience. It's surprising we get that far. It is nowhere what the big boys give you, even for $20/month.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;--- It is still coding --- (definitely now in some Qwen3 loop)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/44qd636p15lg1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6af08a0a84011baa5dc72985d73634bbe04a35f"&gt;https://preview.redd.it/44qd636p15lg1.png?width=599&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6af08a0a84011baa5dc72985d73634bbe04a35f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: Whee! We finished, about 24 hours after I started. Now, of course I wasn't babysitting it so IDK how much time it sat idle during the day. Anytime I went by I'd check on it, or restart the process...&lt;/p&gt; &lt;p&gt;The whole thing had to restart or run probably 20-30 times again and again on the same thing for various reasons (timeout or infinite loops).&lt;/p&gt; &lt;p&gt;But, the good thing is: &lt;strong&gt;The project compiles and creates a WAV file with very understandable pronunciation all on just CPU that doesn't sound robotic.&lt;/strong&gt; So that's 100% success. No coding input from my side, no code fixing. No dependencies.&lt;/p&gt; &lt;p&gt;It isn't pleasant to work with it in this capacity I tried (MAC Studio with forever prompt processing) but beggars cannot be choosers and Qwen3-coder-next is a &lt;strong&gt;FREE&lt;/strong&gt; model. So yay, they (Qwen) need to be commanded for their effort. It's amazing how fast we got there, and I remember that.&lt;/p&gt; &lt;p&gt;I'm bumping the result to 6/10 for a local coding experience which is: &lt;strong&gt;good&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Final observations and what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- It's free, good enough, and runs on a home hardware which back in 2023 would be called &amp;quot;insane&amp;quot;&lt;/p&gt; &lt;p&gt;- it can probably work better with small editing/bug fixes/ small additions. The moment it needs to write large code it will be full of issues (if it finishes). It literally didn't wrote a single usable code at once (unlike what I used to see in cc or codex), though it was able to fix all the hundreds issues by itself (testing, assessing, fixing). The process itself took a lot of time.&lt;/p&gt; &lt;p&gt;- it didn't really have problem with tool calling, at least not what I observed. It had problem with tool using, especially when it started producing a lot of code.&lt;/p&gt; &lt;p&gt;- it is NOT a replacement for claude/codex/gemini/other cloud. It just isn't. Maybe as a hobby. It's the difference between a bicycle and a car. You will get there eventually, but it would take much longer and be less pleasant. Well it depends how much you value your time vs money, I guess.&lt;/p&gt; &lt;p&gt;- MAC with unified memory is amazing, for a basic general LLM, but working with code and long context it kills any enjoyment - and that is not dependent on the size of the memory. When the grifters on X saying they are buying 512GB MAC studios for local agentic coding etc - it's BS. It's still a torture - because we have much faster and less painful way using cloud API (and cheaper too). It's pain with 80GB 8 bit quantized model, it would be excruciating with full 250GB model.&lt;/p&gt; &lt;p&gt;- I'm not going to lie to you, I'm not going to use it much, unless I terribly ran out of tokens on cc or codex. I'd check other Chinese big online models that are much cheaper like GLM 5, but honestly the price alone is not deterrent. I firmly believe they (codex, cc) are giving it practically for free. &lt;/p&gt; &lt;p&gt;- I might check other models like step 3.5 (I have it downloaded but didn't use it for anything yet)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc1ra2/my_realworld_qwen3codenext_local_coding_test_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T23:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc97qf</id>
    <title>ðŸŒŠ Wave Field LLM O(n log n) Successfully Scales to 1B Parameters</title>
    <updated>2026-02-23T05:44:29+00:00</updated>
    <author>
      <name>/u/Murky-Sign37</name>
      <uri>https://old.reddit.com/user/Murky-Sign37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt; &lt;img alt="ðŸŒŠ Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" src="https://preview.redd.it/6m7q2vzlm6lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ede585956ec96d0434754c49701c58176ad83ad" title="ðŸŒŠ Wave Field LLM O(n log n) Successfully Scales to 1B Parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just completed full pretraining of &lt;strong&gt;Wave Field LLM (v4) at 1B scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Summary:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 825M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Tokens:&lt;/strong&gt; 1.33B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best PPL:&lt;/strong&gt; 72.2&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final Accuracy:&lt;/strong&gt; 27.1%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; 13.2 hours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isnâ€™t a small 30M or 124M experiment anymore.&lt;/p&gt; &lt;p&gt;Wave Field is now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;âœ… Stable at near-billion scale&lt;/li&gt; &lt;li&gt;âœ… Training cleanly&lt;/li&gt; &lt;li&gt;âœ… Converging properly&lt;/li&gt; &lt;li&gt;âœ… Saving best checkpoints&lt;/li&gt; &lt;li&gt;âœ… Handling &amp;gt;1B tokens&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key takeaway:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This validates that Wave Fieldâ€™s field-based interaction mechanism is not just an experimental curiosity â€” it holds up under real model size and real token volume &lt;a href="https://github.com/badaramoni/wave-field-llm"&gt;git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Murky-Sign37"&gt; /u/Murky-Sign37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6m7q2vzlm6lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc97qf/wave_field_llm_on_log_n_successfully_scales_to_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T05:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rbkeea</id>
    <title>Which one are you waiting for more: 9B or 35B?</title>
    <updated>2026-02-22T12:15:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt; &lt;img alt="Which one are you waiting for more: 9B or 35B?" src="https://preview.redd.it/jyvany3jf1lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f667e97854acf566b7f6d1d56e9c09e17f5a8ee8" title="Which one are you waiting for more: 9B or 35B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jyvany3jf1lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rbkeea/which_one_are_you_waiting_for_more_9b_or_35b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-22T12:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc3naj</id>
    <title>Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao.</title>
    <updated>2026-02-23T01:13:04+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt; &lt;img alt="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." src="https://external-preview.redd.it/MmJ6MGRjNjA4NWxnMR3Al36Nr886FX7jQ_P96fNg8PSf4Zsku92kjG2XN_qv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8910573e373960eea6962553218ddcd88a9324c" title="Super New to Godot, used Claude Code/gpt-oss-120b locally to help me vibecode a simple platformer game about a grumpy mage who follows you around making fun of you lmao." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, I was bored so I spent the last two weeks experimenting with vibecoding with local LLMs, namely gpt-oss-120b.&lt;/p&gt; &lt;p&gt;I started with Cline, didn't like it at all because it was overheating my GPU while giving back too little. Codex was even worse, locally, leading to weird CPU switches mid-generation when there was supposed to be enough VRAM to run the model entirely on GPU. Then I tried Claude Code and that's when my expectations were exceeded, &lt;em&gt;big time.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I first started with pygame, and after successfully one-shotting simple games (snake game, etc.) under the same project with the same model I decided to take it another level and use Claude Code with Godot, which was pretty easy to setup in VSCode and their IDE/extension. &lt;/p&gt; &lt;p&gt;Next thing I know, I spend the last two weeks making this game on Godot out of curiosity and using Claude Code to help me Vibecode parts of it along the way, and I came up with this game where you have a useful, snarky NPC that makes fun of you lmao.&lt;/p&gt; &lt;p&gt;The way it works is that the game is going to be gathering contextual information in real-time, e.g. actions taken, events occurring, etc. You can see that in the logs that are printed under the gameplay loop. &lt;/p&gt; &lt;p&gt;The mage then stores each chain of events in a chat history and comments on it every 10 seconds. The AI behavior is hard-coded but it works really well. However, I do plan on adding a hybrid approach where the LLM uses tool calls to make informed decisions depending on the situations, such as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Switching equipment&lt;/li&gt; &lt;li&gt;Healing the player or himself&lt;/li&gt; &lt;li&gt;Pointing out objects of interest&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so forth. I haven't ruled out a Wizard of Oz worldbuilding AI that vibecodes enemies and obstacles throughout the game with tool calls, but that will be for another time.&lt;/p&gt; &lt;p&gt;I'm enjoying this process so I think I might actually finish this game, but we'll see how far I can get. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jl31wp5085lg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc3naj/super_new_to_godot_used_claude_codegptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T01:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc6c8m</id>
    <title>Feels like magic. A local gpt-oss 20B is capable of agentic work</title>
    <updated>2026-02-23T03:18:16+00:00</updated>
    <author>
      <name>/u/Vaddieg</name>
      <uri>https://old.reddit.com/user/Vaddieg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt; &lt;img alt="Feels like magic. A local gpt-oss 20B is capable of agentic work" src="https://preview.redd.it/b27xdhewq5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9692be692d82dd176bce38aa1cffe88af9406be" title="Feels like magic. A local gpt-oss 20B is capable of agentic work" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gave a try to &lt;a href="https://github.com/zeroclaw-labs/zeroclaw"&gt;zeroclaw&lt;/a&gt; agent (intstead of the bloated and overhyped one). After few hours of fuckery with configs it's finally useful. Both main and embeddings models are running locally.&lt;br /&gt; I carefully read what it's trying to execute in shell, and permit only [relatively] safe tools in config.&lt;br /&gt; So far it can interact with macOS apps, web pages, and local files while keeping all my data private.&lt;br /&gt; gpt-oss 20B has its limits though, it loses focus after 15-20 steps and often needs direct instructions to use persistent memory. It also starts behaving weirdly if tool access has been denied or tool returned some error.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vaddieg"&gt; /u/Vaddieg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b27xdhewq5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc6c8m/feels_like_magic_a_local_gptoss_20b_is_capable_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T03:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1rc59ze</id>
    <title>Qwen3's most underrated feature: Voice embeddings</title>
    <updated>2026-02-23T02:28:32+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt; &lt;img alt="Qwen3's most underrated feature: Voice embeddings" src="https://preview.redd.it/zmcs7iysm5lg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=796016e685c536fbab1ce49b5fec35afeb75f40e" title="Qwen3's most underrated feature: Voice embeddings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you know that Qwen3 TTS utilizes voice embedding for voice cloning?&lt;br /&gt; Your voice is turned into a vector of 1024 dimensions (or 2048 for 1.7b), and based on this vector alone you can get your custom voice.&lt;/p&gt; &lt;p&gt;But the coolest part is that this means that you can use math to modify voices, average voices. You can swap gender, pitch, mix and match voices, and even create an emotion space! This also enables semantic voice search!&lt;/p&gt; &lt;p&gt;The voice embedding model is actually just a tiny encoder with just a few million parameters. I've ripped it out of the voice embedding model so you can use the embedding model standalone. Check out my collection! :D I also have onnx models for optimized web / front-end inference.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding"&gt;https://huggingface.co/collections/marksverdhei/qwen3-voice-embedding&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice embedings can be used for inference in my vllm-omni fork until it is supported in upstream: &lt;a href="https://github.com/heiervang-technologies/ht-vllm-omni"&gt;https://github.com/heiervang-technologies/ht-vllm-omni&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zmcs7iysm5lg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rc59ze/qwen3s_most_underrated_feature_voice_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-23T02:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
