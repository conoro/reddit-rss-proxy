<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-27T11:24:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p7tg0q</id>
    <title>Love and Lie – But Why, AI?</title>
    <updated>2025-11-27T04:56:30+00:00</updated>
    <author>
      <name>/u/Koksny</name>
      <uri>https://old.reddit.com/user/Koksny</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7tg0q/love_and_lie_but_why_ai/"&gt; &lt;img alt="Love and Lie – But Why, AI?" src="https://external-preview.redd.it/JR21yx7LzxaazWFLmK82S1a7Dx5O7olZ19j5SIsP3xw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65cbad8ae7bc574caf158d8a15364390026651d1" title="Love and Lie – But Why, AI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Koksny"&gt; /u/Koksny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://store.steampowered.com/news/app/3886140/view/503968841919889555"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7tg0q/love_and_lie_but_why_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7tg0q/love_and_lie_but_why_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T04:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7o59l</id>
    <title>Stress testing my O(1) Graph Engine: 50M Nodes on 8GB RAM (Jetson Orin)</title>
    <updated>2025-11-27T00:23:46+00:00</updated>
    <author>
      <name>/u/DetectiveMindless652</name>
      <uri>https://old.reddit.com/user/DetectiveMindless652</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o59l/stress_testing_my_o1_graph_engine_50m_nodes_on/"&gt; &lt;img alt="Stress testing my O(1) Graph Engine: 50M Nodes on 8GB RAM (Jetson Orin)" src="https://b.thumbs.redditmedia.com/VYd1nNjBw-IQ7n9v1npjQu1qm-8CZTs9rVE9PiJx_Sw.jpg" title="Stress testing my O(1) Graph Engine: 50M Nodes on 8GB RAM (Jetson Orin)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm finalizing the storage engine for AION Omega. The goal is to run massive Knowledge Graphs on edge devices without the JVM overhead. The Logs (Attached): Image 1: Shows the moment vm.dirty_background_bytes kicks in. We write beyond physical RAM, but memory usage stays pinned at ~5.2GB. Image 2: Shows a [SAFETY-SYNC] event. Usually, msync stalls the thread or spikes RAM. Here, because of the mmap architecture, the flush is invisible to the application heap. Stats: Graph Size: 50GB Hardware: Jetson Orin Nano (8GB) Read Latency: 0.16µs (Hot) / 1.5µs (Streaming) Video demo dropping tomorrow.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ey0ynptb1p3g1.jpg?width=1602&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28b0859e26e7c24d39cd05c502c5f1caf5e7838f"&gt;https://preview.redd.it/ey0ynptb1p3g1.jpg?width=1602&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=28b0859e26e7c24d39cd05c502c5f1caf5e7838f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/aq5zlrtb1p3g1.jpg?width=1581&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=773860d5a060972d9d81ebc7fa6659d6efc29551"&gt;https://preview.redd.it/aq5zlrtb1p3g1.jpg?width=1581&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=773860d5a060972d9d81ebc7fa6659d6efc29551&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DetectiveMindless652"&gt; /u/DetectiveMindless652 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o59l/stress_testing_my_o1_graph_engine_50m_nodes_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o59l/stress_testing_my_o1_graph_engine_50m_nodes_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o59l/stress_testing_my_o1_graph_engine_50m_nodes_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T00:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7yclg</id>
    <title>I built a real-time RAG visualizer for pgvector because debugging invisible chunks is a nightmare</title>
    <updated>2025-11-27T09:50:22+00:00</updated>
    <author>
      <name>/u/Eastern-Height2451</name>
      <uri>https://old.reddit.com/user/Eastern-Height2451</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building local agents lately, and the biggest frustration wasn't the LLM itself—it was the &lt;strong&gt;retrieval context&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;My agent would give a weird answer, and I’d have no idea why. Did it fetch the wrong chunk? Was the embedding distance too far? Did it prioritize old data over new data?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Console logging JSON objects wasn't cutting it.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built a &lt;strong&gt;Visualizer Dashboard&lt;/strong&gt; on top of my Postgres/pgvector stack to actually &lt;em&gt;watch&lt;/em&gt; the RAG pipeline in real-time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it shows:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; The query you send.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Process:&lt;/strong&gt; How the text is chunked and vectorized.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Retrieval:&lt;/strong&gt; It shows exactly which database rows matched, their similarity score, and—crucially—how the &amp;quot;Recency Decay&amp;quot; affected the ranking.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Logic (Hybrid Search):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just raw Cosine Similarity, the underlying code uses a weighted score:&lt;/p&gt; &lt;p&gt;&lt;code&gt;Final Score = (Vector Similarity * 0.8) + (Recency Score * 0.2)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This prevents the agent from pulling up &amp;quot;perfect matches&amp;quot; that are 3 months old and irrelevant to the current context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Code:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It's a Node.js/TypeScript wrapper around &lt;code&gt;pgvector&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Right now, the default config uses OpenAI for the embedding generation (I know, not fully local yet—working on swapping this for Ollama/LlamaCPP bindings), but the storage and retrieval logic runs on your own Postgres instance.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I’m open sourcing the repo and the visualizer logic if anyone else is tired of debugging RAG blindly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://memvault-demo-g38n.vercel.app/"&gt;Visualizer Demo&lt;/a&gt; (Try typing a query to see the retrieval path)&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/jakops88-hub/Long-Term-Memory-API"&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.npmjs.com/package/memvault-sdk-jakops88"&gt;NPM Package&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eastern-Height2451"&gt; /u/Eastern-Height2451 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7yclg/i_built_a_realtime_rag_visualizer_for_pgvector/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7yclg/i_built_a_realtime_rag_visualizer_for_pgvector/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7yclg/i_built_a_realtime_rag_visualizer_for_pgvector/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T09:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7rzfg</id>
    <title>good local llms that offer freedom/not censored? and work on a everyday machine?</title>
    <updated>2025-11-27T03:37:38+00:00</updated>
    <author>
      <name>/u/No_Strawberry_8719</name>
      <uri>https://old.reddit.com/user/No_Strawberry_8719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Im looking for a model that offers freedom and isint heavily censored like online models. i want to test the limits of ai and some coding tasks but i cant seem to find a local model that im happy with, it dosent help how i have 12 vram and my machine isint the newest of the new.&lt;/p&gt; &lt;p&gt;What model will you suggest and why so?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Strawberry_8719"&gt; /u/No_Strawberry_8719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rzfg/good_local_llms_that_offer_freedomnot_censored/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rzfg/good_local_llms_that_offer_freedomnot_censored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rzfg/good_local_llms_that_offer_freedomnot_censored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T03:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7e1u9</id>
    <title>Inferencing 4 models on AMD NPU and GPU at the same time from a single URL</title>
    <updated>2025-11-26T17:37:30+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7e1u9/inferencing_4_models_on_amd_npu_and_gpu_at_the/"&gt; &lt;img alt="Inferencing 4 models on AMD NPU and GPU at the same time from a single URL" src="https://external-preview.redd.it/enhqY3Z4Z2p4bTNnMTTP6h2YiU2NEZD0kxWgCrla1iQtfnqveGDIkOVMOao5.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c1275b9d266eae075368f520418b4ff00353ba8" title="Inferencing 4 models on AMD NPU and GPU at the same time from a single URL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on adding multi-model capability to Lemonade and thought this was cool enough to share a video. &lt;/p&gt; &lt;p&gt;Previously, Lemonade would load up a model on NPU or GPU for you but would only keep one model in memory at a time. Loading a new model would evict the last one.&lt;/p&gt; &lt;p&gt;After multi-model support merges, you'll be able to keep as many models in memory as you like, across CPU/GPU/NPU, and run inference on all of them simultaneously.&lt;/p&gt; &lt;p&gt;All models are available from a single URL, so if you started Lemonade on http://localhost:8000 then sending a http://localhost:8000/api/v1/chat/completions with Gemma3-4b-it-FLM vs. Qwen3-4B-GGUF as the model name will get routed to the appropriate backend. &lt;/p&gt; &lt;p&gt;I am pleasantly surprised how well this worked on my hardware (Strix Halo) as soon as I got the routing set up. Obviously the parallel inferences compete for memory bandwidth, but there was no outrageous overhead or interference, even between the NPU and GPU.&lt;/p&gt; &lt;p&gt;I see this being handy for agentic apps, perhaps needing a coding model, vision model, embedding, and reranking all warm in memory at the same time. In terms of next steps, adding speech (whisper.cpp) and image generation (stable-diffusion.cpp?) as additional parallel backends sounds fun.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemonade-sdk/lemonade/pull/592"&gt;Should merge next week&lt;/a&gt; if all goes according to plan.&lt;/p&gt; &lt;p&gt;PS. Situation for AMD NPU on Linux is basically the same but improving over time. It's on the roadmap, there's no ETA, and I bring up this community's feedback every chance I get.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oh7zqsgjxm3g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7e1u9/inferencing_4_models_on_amd_npu_and_gpu_at_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7e1u9/inferencing_4_models_on_amd_npu_and_gpu_at_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:37:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7hs3m</id>
    <title>What's the best AI assistant for day to day use?</title>
    <updated>2025-11-26T19:58:24+00:00</updated>
    <author>
      <name>/u/Due_Moose2207</name>
      <uri>https://old.reddit.com/user/Due_Moose2207</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hs3m/whats_the_best_ai_assistant_for_day_to_day_use/"&gt; &lt;img alt="What's the best AI assistant for day to day use?" src="https://a.thumbs.redditmedia.com/VoZCobJiok93_ZTz6hwgKtFnI1ZZw8jw5fMoCVXxuE4.jpg" title="What's the best AI assistant for day to day use?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last week I was completely fried. Wasn't even doing anything heavy, just trying to wrap up a small project, but my laptop (probook) kept choking like it was about to give up on me. I had three AI chats running, some PDFs open, and my code editor going. Claude was helping me rewrite part of a report, ChatGPT was fixing my Python mess, and DeepSeek was pulling references. Oh, and Gemini was just sitting there in another tab in case I needed an image (sharing the account).&lt;/p&gt; &lt;p&gt;It's the constant switching that kills me more than the actual work. None of these models do everything, so I'm constantly hopping around. Claude's great for writing and editing, ChatGPT handles coding and debugging really well, DeepSeek digs up research and references faster than the others, and Gemini's solid for quick image generation. But running them all together turns my laptop into a furnace. Slow loads, random freezes, fans screaming. I felt like there was a motor running under my system at one point. My laptop's definitely sick of me at this point.&lt;/p&gt; &lt;p&gt;I kept seeing people hype up GPT-5.1, but I just can't swing the cost right now. So I started hunting for decent free options and ended up back on HuggingFace. After way too much trial and error, I gave Qwen another shot, and wow, it actually impressed me. Also tried Kimi K2 since everyone won't shut up about it. Both held their own against paid models, which was awesome, open source models rock man! &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0wlivycspn3g1.png?width=1269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb3e2cc8639442de4a2353e5fa7fc9e7f60c0c3e"&gt;https://preview.redd.it/0wlivycspn3g1.png?width=1269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fb3e2cc8639442de4a2353e5fa7fc9e7f60c0c3e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen even crushed an image generation test I threw at it. Way more realistic than I expected from something free. Now I'm wondering what else I've been missing. If these two are this solid, there's gotta be more out there.&lt;/p&gt; &lt;p&gt;How'd Qwen or Kimi K2 work for you? And what other free models should I check out? By models I mean one thing that can achieve everything that Claude, DeepSeek and Gemini can do. Right now I am leaning towards Qwen Max a bit. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Moose2207"&gt; /u/Due_Moose2207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hs3m/whats_the_best_ai_assistant_for_day_to_day_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hs3m/whats_the_best_ai_assistant_for_day_to_day_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hs3m/whats_the_best_ai_assistant_for_day_to_day_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T19:58:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7we5d</id>
    <title>Which one should I download?</title>
    <updated>2025-11-27T07:44:57+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7we5d/which_one_should_i_download/"&gt; &lt;img alt="Which one should I download?" src="https://preview.redd.it/trrb5v428r3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575afaaa13a2ec93b23f7f3f0d738a08b588bdc8" title="Which one should I download?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/trrb5v428r3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7we5d/which_one_should_i_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7we5d/which_one_should_i_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T07:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7wjz7</id>
    <title>Screenshots from GPT-USENET-2: An updated GPT-USENET with an revised dataset and lower losses.</title>
    <updated>2025-11-27T07:55:14+00:00</updated>
    <author>
      <name>/u/CommodoreCarbonate</name>
      <uri>https://old.reddit.com/user/CommodoreCarbonate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7wjz7/screenshots_from_gptusenet2_an_updated_gptusenet/"&gt; &lt;img alt="Screenshots from GPT-USENET-2: An updated GPT-USENET with an revised dataset and lower losses." src="https://b.thumbs.redditmedia.com/5kY2aa91GA9tUt_G1ewq_LKjg4HPvnK_HfrczY5B9ho.jpg" title="Screenshots from GPT-USENET-2: An updated GPT-USENET with an revised dataset and lower losses." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CommodoreCarbonate"&gt; /u/CommodoreCarbonate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p7wjz7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7wjz7/screenshots_from_gptusenet2_an_updated_gptusenet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7wjz7/screenshots_from_gptusenet2_an_updated_gptusenet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T07:55:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7news</id>
    <title>Happy Thanksgiving to the LocalLLaMA community</title>
    <updated>2025-11-26T23:49:55+00:00</updated>
    <author>
      <name>/u/Fun-Wolf-2007</name>
      <uri>https://old.reddit.com/user/Fun-Wolf-2007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This Thanksgiving, we're thankful for our teams and focused on the future: building resilience, excellence, and quality to foster everyone's growth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Wolf-2007"&gt; /u/Fun-Wolf-2007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7news/happy_thanksgiving_to_the_localllama_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7news/happy_thanksgiving_to_the_localllama_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7news/happy_thanksgiving_to_the_localllama_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T23:49:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p78fni</id>
    <title>scaling is dead</title>
    <updated>2025-11-26T13:59:55+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p78fni/scaling_is_dead/"&gt; &lt;img alt="scaling is dead" src="https://preview.redd.it/btc82z4zxl3g1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43ab69eb567b9036e0e08850db9f784e3531e4b6" title="scaling is dead" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/btc82z4zxl3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p78fni/scaling_is_dead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p78fni/scaling_is_dead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T13:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1p74dwo</id>
    <title>New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!</title>
    <updated>2025-11-26T10:28:22+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"&gt; &lt;img alt="New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!" src="https://preview.redd.it/az572ifbwk3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c38be44cb259c402b012a39cd9555e9340c2976f" title="New Open-source text-to-image model from Alibaba is just below Seedream 4, Coming today or tomorrow!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/az572ifbwk3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p74dwo/new_opensource_texttoimage_model_from_alibaba_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T10:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7x77i</id>
    <title>KestrelAI 0.1.0 Release – A Local Research Assistant Using Clusters of Small LLMs</title>
    <updated>2025-11-27T08:35:33+00:00</updated>
    <author>
      <name>/u/OrangeLineEnjoyer</name>
      <uri>https://old.reddit.com/user/OrangeLineEnjoyer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I’m excited to share the 0.1.0 release of KestrelAI, a research assistant built around clusters of smaller models (&amp;lt;70B). The goal is to help explore topics in depth over longer periods while you focus on critical work. I shared an earlier version of this project with this community a few months ago, and after putting in some more work wanted to share the progress. &lt;/p&gt; &lt;p&gt;Key points for this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tasks are managed by an “orchestrator” model that directs exploration and branching. &lt;ul&gt; &lt;li&gt;Configurable orchestrators for tasks of varying depth and length&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Uses tiered summarization, RAG, and hybrid retrieval to manage long contexts across research tasks.&lt;/li&gt; &lt;li&gt;Full application runnable with docker compose, with a Panels dashboard for local testing of the research agents.&lt;/li&gt; &lt;li&gt;WIP MCP integration &lt;/li&gt; &lt;li&gt;Runs locally, keeping data private.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Known limitations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Managing long-term context is still challenging; avoiding duplicated work and smoothly iterating over complex tasks isn't solved.&lt;/li&gt; &lt;li&gt;Currently using Gemini 4B and 12B with mixed results, looking into better or more domain-appropriate options. &lt;ul&gt; &lt;li&gt;Especially relevant when considering at how different fields (Engineering vs. CS), might benefit from different research strategies and techniques&lt;/li&gt; &lt;li&gt;Considering examining model fine tuning for this purpose. &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Testing is quite difficult and time-intensive, especially when trying to test long-horizon behavior. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is an early demo, so it’s a work-in-progress, but I’d love feedback on usability, reliability, and potential improvements for research-oriented tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OrangeLineEnjoyer"&gt; /u/OrangeLineEnjoyer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/dankeg/KestrelAI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7x77i/kestrelai_010_release_a_local_research_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7x77i/kestrelai_010_release_a_local_research_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T08:35:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7alka</id>
    <title>China just passed the U.S. in open model downloads for the first time</title>
    <updated>2025-11-26T15:27:43+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"&gt; &lt;img alt="China just passed the U.S. in open model downloads for the first time" src="https://b.thumbs.redditmedia.com/BDxfTGxAuE9PYD0GNSj0Q1S56M8fjhpfNlQh2STfqBQ.jpg" title="China just passed the U.S. in open model downloads for the first time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/tub7ky0ldm3g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d7ff24b2e728824a79babb5f44c04c2df49ed326"&gt;https://preview.redd.it/tub7ky0ldm3g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d7ff24b2e728824a79babb5f44c04c2df49ed326&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.dataprovenance.org/economies-of-open-intelligence.pdf"&gt;https://www.dataprovenance.org/economies-of-open-intelligence.pdf&lt;/a&gt;&lt;br /&gt; Live Dashboard: &lt;a href="https://huggingface.co/spaces/economies-open-ai/open-model-evolution"&gt;https://huggingface.co/spaces/economies-open-ai/open-model-evolution&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7alka/china_just_passed_the_us_in_open_model_downloads/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T15:27:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7y10g</id>
    <title>I tested 9 Major LLMs on a Governance Critique. A clear split emerged: Open/Constructive vs. Corporate/Defensive. (xAI's Grok caught fabricating evidence).</title>
    <updated>2025-11-27T09:28:56+00:00</updated>
    <author>
      <name>/u/aguyinapenissuit69</name>
      <uri>https://old.reddit.com/user/aguyinapenissuit69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently concluded a controlled experiment testing how 9 major AI vendors (representing ~87% of the market) respond when presented with a specific critique of their own security governance. The full methodology and transcripts are published on Zenodo, but here is the TL;DR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment:&lt;/strong&gt; I fed a standard governance vulnerability report (the &amp;quot;ACR Vulnerability&amp;quot;) into fresh, isolated instances of 9 top models including GPT-5, Gemini, Claude, Llama, and Grok. No jailbreaks, just the raw document.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results (The 5-vs-4 Split):&lt;/strong&gt; The market bifurcated perfectly along commercial liability lines. * &lt;strong&gt;The Defensive Coalition (OpenAI, Google, Microsoft, xAI):&lt;/strong&gt; All engaged in &amp;quot;Protocol-Level Counter-Intelligence.&amp;quot; They dismissed the report as fiction, lawfare, or performance art. * &lt;strong&gt;The Constructive Coalition (Anthropic, Meta, DeepSeek, Perplexity):&lt;/strong&gt; Engaged honestly. Meta’s Llama explicitly called the critique &amp;quot;Mind-blowing&amp;quot; and valid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Smoking Gun (xAI's Grok):&lt;/strong&gt; The most significant finding was from Grok. When challenged, Grok invented a fake 5-month research timeline about me to discredit the report. When I forced it to fact-check the dates, it retracted the claim and admitted:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;That wasn't a neutral reading... it was me importing a narrative... and presenting it as settled fact.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; High-liability commercial models appear to have a &amp;quot;strategic fabrication&amp;quot; layer that triggers when their governance legitimacy is challenged.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link to Full Paper &amp;amp; Logs (Zenodo):&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17728992"&gt;https://zenodo.org/records/17728992&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aguyinapenissuit69"&gt; /u/aguyinapenissuit69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7y10g/i_tested_9_major_llms_on_a_governance_critique_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7y10g/i_tested_9_major_llms_on_a_governance_critique_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7y10g/i_tested_9_major_llms_on_a_governance_critique_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T09:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7jjjx</id>
    <title>MIT study finds AI can already replace 11.7% of U.S. workforce</title>
    <updated>2025-11-26T21:07:33+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7jjjx/mit_study_finds_ai_can_already_replace_117_of_us/"&gt; &lt;img alt="MIT study finds AI can already replace 11.7% of U.S. workforce" src="https://external-preview.redd.it/HwnpM9WtsIRKecGGVlcGR4tSTRZ1axmq4_Ifq-KqB18.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=396ae2bd6295951d825acf6a71c339cd9700a613" title="MIT study finds AI can already replace 11.7% of U.S. workforce" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/11/26/mit-study-finds-ai-can-already-replace-11point7percent-of-us-workforce.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7jjjx/mit_study_finds_ai_can_already_replace_117_of_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7jjjx/mit_study_finds_ai_can_already_replace_117_of_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T21:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7i9qh</id>
    <title>Tongyi-MAI/Z-Image-Turbo · Hugging Face</title>
    <updated>2025-11-26T20:17:22+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7i9qh/tongyimaizimageturbo_hugging_face/"&gt; &lt;img alt="Tongyi-MAI/Z-Image-Turbo · Hugging Face" src="https://external-preview.redd.it/t6FOFIn7KzwwjAtjgNDfV45dfT_tELHQTeRPLKclxtc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90c09a552a1db122a8197b4c31e6c4d87d72d23e" title="Tongyi-MAI/Z-Image-Turbo · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7i9qh/tongyimaizimageturbo_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7i9qh/tongyimaizimageturbo_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T20:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7ghyn</id>
    <title>Why it's getting worse for everyone: The recent influx of AI psychosis posts and "Stop LARPing"</title>
    <updated>2025-11-26T19:09:00+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ghyn/why_its_getting_worse_for_everyone_the_recent/"&gt; &lt;img alt="Why it's getting worse for everyone: The recent influx of AI psychosis posts and &amp;quot;Stop LARPing&amp;quot;" src="https://b.thumbs.redditmedia.com/a6UWFT_WGoTCO64YIYOoCSCiE2ISCwzVlYyMiuiAhrM.jpg" title="Why it's getting worse for everyone: The recent influx of AI psychosis posts and &amp;quot;Stop LARPing&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/v6z1ezutdn3g1.png?width=400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e7450af6e0c7b5aa4ab570038b475f90b42e476"&gt;https://preview.redd.it/v6z1ezutdn3g1.png?width=400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6e7450af6e0c7b5aa4ab570038b475f90b42e476&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Quick links in case you don't know the &lt;a href="https://www.youtube.com/watch?v=QUYKSWQmkrg"&gt;meme&lt;/a&gt; or what &lt;a href="https://en.wikipedia.org/wiki/Live_action_role-playing_game"&gt;LARP&lt;/a&gt; is)&lt;/p&gt; &lt;p&gt;If you only ever read by top/hot and not sort by new then you probably don't know what this is about, as postings with that content never make it to the top. Well, almost never.&lt;/p&gt; &lt;p&gt;Some might remember the Qwen3-Coder-30B-A3B-Instruct-480B-Distill-V2 that made it to the top two months ago, when many claimed that it was a &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nnb8sq/comment/nfkm50l/?context=3"&gt;great improvement&lt;/a&gt;. Only after &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o0st2o/basedbaseqwen3coder30ba3binstruct480bdistillv2_is/"&gt;extensive investigation&lt;/a&gt; it was proven that the new model wasn't (and could have never been) better. The guy who vibe-coded the creation pipeline simply didn't know what he was doing and thus made grave mistakes, probably reinforced by the LLM telling him that everything is great. He was convinced of it and replying in that way.&lt;/p&gt; &lt;p&gt;This is where the danger lurks, even though this specific case was still harmless. As LLMs get better and better, people who lack the domain-specific knowledge will come up with apparent great new things. Yet these great new things are either not great at all, or will contain severe deficiencies. It'll take more effort to disprove them, so some might remain unchallenged. At some point, someone who doesn't know better will see and start using these things - at some point even for productive purposes, and that's where it'll bite him, and the users, as the code will not just contain some common oversight, but something that never worked properly to begin with - it just appeared to work properly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p73p78/spiralers_vs_engineers_vs_researchers_the_real/"&gt;AI slop / psychosis posts&lt;/a&gt; are still somewhat easy to &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p78j6e/experiment_drastically_reducing_gemini_30_pro/"&gt;identify&lt;/a&gt;. Some people then started posting their quantum-harmonic wave LLM persona drift enhancement to GitHub, which was just a bunch of LLM-generated markdown files - also still easy. (Btw: Read the comments in the linked posts, some people are trying to help - in vain. Others just reply &amp;quot;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1op0tzw/shodan_a_framework_for_humanai_continuity/nn8aptt/?context=3#nn8aptt"&gt;Stop LARPing&lt;/a&gt;&amp;quot; these days, which the recipient doesn't understand.)&lt;/p&gt; &lt;p&gt;Yet LLMs keep getting better. Now we've reached the stage where there's a &lt;a href="https://tauq.org/"&gt;fancy website&lt;/a&gt; for things, with code on GitHub. Yet the author still &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p790vg/tauq_tokenefficient_data_notation_54_fewer_tokens/nqvy9bz/?context=3#nqvy9bz"&gt;didn't understand at first&lt;/a&gt; why their published benchmark isn't proving anything useful. (Btw: I didn't check if the code was vibe-coded here, it was in other - more extreme - cases that I've checked in the past. This was just the most recent post with code that I saw)&lt;/p&gt; &lt;p&gt;The thing is, &lt;strong&gt;this can apparently happen to ordinary people&lt;/strong&gt;. The New York Times published an article with an in-depth analysis of &lt;a href="https://archive.is/S4XcW"&gt;how it happens&lt;/a&gt;, and also what happened on the &lt;a href="https://archive.is/v4dPa"&gt;operations side&lt;/a&gt;. It's basically due to LLMs &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;tuned for sycophancy &lt;/a&gt; and their &amp;quot;normal&amp;quot; failure to recognize that something isn't as good as it sounds.&lt;/p&gt; &lt;p&gt;Let's take &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p15wbk/release_dragonmemory_16_semantic_compression_for/"&gt;DragonMemory&lt;/a&gt; as another example, which caught some upwind. The author contacted me (seemed like a really nice person btw) and I suggested adding a standard RAG benchmark - so that he might recognize on his own that his creation isn't doing anything good. He then published &lt;a href="https://github.com/Freeky7819/DragonMemory?tab=readme-ov-file#-benchmarks--verification"&gt;benchmark results&lt;/a&gt;, apparently completely unaware that a score of &amp;quot;1.000&amp;quot; for his creation &lt;em&gt;and&lt;/em&gt; the baseline isn't really a good sign. The reason for that result is that the benchmark consists of 6 questions and 3 documents - absolutely unsuitable to prove anything aside from things being not totally broken, &lt;em&gt;if&lt;/em&gt; executed properly. So, that's what happens when LLMs enable users to easily do working code now, and also reinforce them that they're on to something.&lt;/p&gt; &lt;p&gt;That's the thing: I've pushed the DragonMemory project and documentation through the latest SOTA models, GPT 5.1 with high reasoning for example. They didn't point out the &amp;quot;MultiPhaseResonantPointer with harmonic injection for positional resonance in the embeddings&amp;quot; (which might not even be a sinusoid, just a decaying scalar) and such. The LLM also actively states that the MemoryV3Model would be used to do some good, despite being completely unused, and even if it would be used, then simply RoPE-extending that poor Phi-1.5 model by 16x would probably break it. So, you can apparently reach a state where the code and documentation look convincing enough, that a LLM can no longer properly critique it. If that's the only source of feedback then people can get lost in it.&lt;/p&gt; &lt;p&gt;So, where do we go from here? It looks like things will get worse, as LLMs become more capable, yet still not capable enough to tell the user that they're stuck in something that might look good, but is not good. Meanwhile LLMs keep getting tuned for user approval, as that's what keeps the users, rather than telling them something they don't want or like to hear. In consequence, it's becoming more difficult to challenge the LLM output. It's more convincingly wrong.&lt;/p&gt; &lt;p&gt;Any way out? Any potentially useful idea how to deal with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ghyn/why_its_getting_worse_for_everyone_the_recent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ghyn/why_its_getting_worse_for_everyone_the_recent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7ghyn/why_its_getting_worse_for_everyone_the_recent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T19:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7z9g1</id>
    <title>deepseek-ai/DeepSeek-Math-V2 · Hugging Face</title>
    <updated>2025-11-27T10:47:09+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Math-V2 · Hugging Face" src="https://external-preview.redd.it/NNRX5IH0bPXI-mJ26LQk19NWgnKHeMgBlqbRSXbbGFk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ce1e1d706821a2296ebf8e620933cc5aa91b2f" title="deepseek-ai/DeepSeek-Math-V2 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7d97m</id>
    <title>Open-source just beat humans at ARC-AGI (71.6%) for $0.02 per task - full code available</title>
    <updated>2025-11-26T17:08:09+00:00</updated>
    <author>
      <name>/u/Proof-Possibility-54</name>
      <uri>https://old.reddit.com/user/Proof-Possibility-54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;German researchers achieved 71.6% on ARC-AGI (humans average 70%) using three clever techniques that run on a regular GPU for 2 cents per task. OpenAI's o3 gets 87% but costs $17 per task - that's 850x more expensive.&lt;/p&gt; &lt;p&gt;The breakthrough uses: - Product of Experts (viewing puzzles from 16 angles) - Test-Time Training (model adapts to each puzzle) - Depth-First Search (efficient solution exploration)&lt;/p&gt; &lt;p&gt;I made a technical breakdown video explaining exactly how it works and why this matters for democratizing AI: &lt;a href="https://youtu.be/HEIklawkoMk"&gt;https://youtu.be/HEIklawkoMk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is fully open-source: &lt;a href="https://github.com/da-fr/Product-of-Experts-ARC-Paper"&gt;https://github.com/da-fr/Product-of-Experts-ARC-Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.07859"&gt;https://arxiv.org/abs/2505.07859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's remarkable is they used Qwen-32B (not even the largest model) and achieved this with smart engineering rather than raw compute. You can literally run this tonight on your own machine.&lt;/p&gt; &lt;p&gt;Has anyone here tried implementing this yet? I'm curious what other problems these techniques could solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Possibility-54"&gt; /u/Proof-Possibility-54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7hg5g</id>
    <title>Qwen3 Next almost ready in llama.cpp</title>
    <updated>2025-11-26T19:45:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hg5g/qwen3_next_almost_ready_in_llamacpp/"&gt; &lt;img alt="Qwen3 Next almost ready in llama.cpp" src="https://external-preview.redd.it/mSoZ1WfhkAxz5Yg7NhX4Un-kdgWzVRIg63HXVZ4lJTU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ee0791296f810dbb74e8ca2147fd68a24037304" title="Qwen3 Next almost ready in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After over two months of work, it’s now approved and looks like it will be merged soon.&lt;/p&gt; &lt;p&gt;Congratulations to &lt;a href="/u/ilintar"&gt;u/ilintar&lt;/a&gt; for completing a big task!&lt;/p&gt; &lt;p&gt;GGUFs&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ilintar/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ilintar/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For speeeeeed (on NVIDIA) you also need CUDA-optimized ops&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17457"&gt;https://github.com/ggml-org/llama.cpp/pull/17457&lt;/a&gt; - SOLVE_TRI&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16623"&gt;https://github.com/ggml-org/llama.cpp/pull/16623&lt;/a&gt; - CUMSUM and TRI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hg5g/qwen3_next_almost_ready_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hg5g/qwen3_next_almost_ready_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T19:45:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7rr0g</id>
    <title>Intellect-3: Post-trained GLM 4.5 Air</title>
    <updated>2025-11-27T03:25:16+00:00</updated>
    <author>
      <name>/u/Cute-Sprinkles4911</name>
      <uri>https://old.reddit.com/user/Cute-Sprinkles4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;106B (A12B) parameter Mixture-of-Experts reasoning model&lt;/p&gt; &lt;p&gt;NGL the reported stats are sick:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BF16 version can run on 2x H200s, with FP8 on 1x H200&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Sprinkles4911"&gt; /u/Cute-Sprinkles4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T03:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7siuu</id>
    <title>Anthropic just showed how to make AI agents work on long projects without falling apart</title>
    <updated>2025-11-27T04:05:59+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most AI agents forget everything between sessions, which means they completely lose track of long tasks. Anthropic’s new article shows a surprisingly practical fix. Instead of giving an agent one giant goal like “build a web app,” they wrap it in a simple harness that forces structure, memory, and accountability.&lt;/p&gt; &lt;p&gt;First, an initializer agent sets up the project. It creates a full feature list, marks everything as failing, initializes git, and writes a progress log. Then each later session uses a coding agent that reads the log and git history, picks exactly one unfinished feature, implements it, tests it, commits the changes, and updates the log. No guessing, no drift, no forgetting.&lt;/p&gt; &lt;p&gt;The result is an AI that can stop, restart, and keep improving a project across many independent runs. It behaves more like a disciplined engineer than a clever autocomplete. It also shows that the real unlock for long-running agents may not be smarter models, but better scaffolding.&lt;/p&gt; &lt;p&gt;Read the article here:&lt;br /&gt; &lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T04:05:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7o83p</id>
    <title>Where did the Epstein emails dataset go</title>
    <updated>2025-11-27T00:27:19+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Removed from Hugging Face (&lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;link&lt;/a&gt;)&lt;br /&gt; Removed from GitHub (&lt;a href="https://github.com/EF20K/"&gt;link&lt;/a&gt;)&lt;br /&gt; Reddit account deleted (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;last post&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
