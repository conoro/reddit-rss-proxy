<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-22T01:12:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qj42l0</id>
    <title>Structured extraction beats full context (0.83 vs 0.58 F1). Results + what didn't work.</title>
    <updated>2026-01-21T17:22:37+00:00</updated>
    <author>
      <name>/u/Ok_Promise_9470</name>
      <uri>https://old.reddit.com/user/Ok_Promise_9470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been frustrated with context limits in AI coding agents. Decided to actually test what compression approaches preserve information for downstream reasoning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;br /&gt; - HotpotQA dataset (multi-hop questions requiring reasoning across multiple facts)&lt;br /&gt; - Compress context using different methods&lt;br /&gt; - Evaluate: can Claude still answer correctly?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I tested:&lt;/strong&gt;&lt;br /&gt; 1. &lt;strong&gt;Entity Cards&lt;/strong&gt; - group all facts by entity&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[John Smith]: doctor, works at Mayo Clinic, treated patient X [Patient X]: admitted Jan 5, diagnosed with condition Y &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;SPO Triples&lt;/strong&gt; - `(subject, predicate, object)` format&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured NL&lt;/strong&gt; - consistent sentence structure&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token compression&lt;/strong&gt; - LLMLingua, QUITO (select/delete tokens by importance)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full context&lt;/strong&gt; - baseline, no compression&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Method | F1 | Compression | |--------|-----|-------------| | Entity Cards | 0.827 | 17.5% | | Structured NL | 0.767 | 10.6% | | SPO Triples | 0.740 | 13.3% | | QUITO | 0.600 | 20.0% | | Full Context | 0.580 | 100% | | LLMLingua | 0.430 | 20.7% | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;The surprise:&lt;/strong&gt; Full context performed worse than several compressed versions. Entity Cards at 17% of the tokens beat full context by 0.25 F1. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I think this happens:&lt;/strong&gt;&lt;br /&gt; Raw text has noise - filler words, redundancy, info buried in paragraphs. Structured extraction surfaces the signal: who exists, what they did, how things connect. The model reasons better on clean structured input than messy raw text. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What didn't work:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Token compression (LLMLingua, QUITO)&lt;/strong&gt;: Produces unreadable output. Deleting tokens destroys semantic structure. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Query-aware compression:&lt;/strong&gt; If you optimize for a specific question, you're just doing QA. Need query-agnostic compression that works for any future question.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Event frames&lt;/strong&gt;: Action-centric grouping lost entity relationships. Worst structured format.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Small model test:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Also tested if smaller models could generate Entity Cards (instead of using Claude): &lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Model | F1 | |-------|-----| | Qwen3-0.6B | 0.30 | | Qwen3-1.7B | 0.60 | | Qwen3-8B | 0.58 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;1.7B is usable but there's still a gap vs Claude's 0.83. The 4B model was broken (mostly empty outputs, not sure why). &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Open questions:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can the small model gap be closed with fine-tuning? &lt;/li&gt; &lt;li&gt;Does this hold on other datasets beyond HotpotQA? &lt;/li&gt; &lt;li&gt;How does this interact with RAG pipelines?&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to share more details on methodology if anyone's interested. Curious if others have experimented with this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Promise_9470"&gt; /u/Ok_Promise_9470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj42l0/structured_extraction_beats_full_context_083_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj42l0/structured_extraction_beats_full_context_083_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj42l0/structured_extraction_beats_full_context_083_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T17:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi4uj2</id>
    <title>768Gb Fully Enclosed 10x GPU Mobile AI Build</title>
    <updated>2026-01-20T15:56:13+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt; &lt;img alt="768Gb Fully Enclosed 10x GPU Mobile AI Build" src="https://b.thumbs.redditmedia.com/IFwD006aQ7uS94rhW8Tb5SMKqOvtmvGGWhQOsclMVOE.jpg" title="768Gb Fully Enclosed 10x GPU Mobile AI Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen a system with this format before but with how successful the result was I figured I might as well share it.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; Threadripper Pro 3995WX w/ ASUS WS WRX80e-sage wifi ii&lt;/p&gt; &lt;p&gt;512Gb DDR4&lt;/p&gt; &lt;p&gt;256Gb GDDR6X/GDDR7 (8x 3090 + 2x 5090)&lt;/p&gt; &lt;p&gt;EVGA 1600W + Asrock 1300W PSU's&lt;/p&gt; &lt;p&gt;Case: Thermaltake Core W200&lt;/p&gt; &lt;p&gt;OS: Ubuntu&lt;/p&gt; &lt;p&gt;Est. expense: ~$17k&lt;/p&gt; &lt;p&gt;The objective was to make a system for running extra large MoE models (Deepseek and Kimi K2 specifically), that is also capable of lengthy video generation and rapid high detail image gen (the system will be supporting a graphic designer). The challenges/constraints: The system should be easily movable, and it should be enclosed. The result technically satisfies the requirements, with only one minor caveat. Capital expense was also an implied constraint. We wanted to get the most potent system possible with the best technology currently available, without going down the path of needlessly spending tens of thousands of dollars for diminishing returns on performance/quality/creativity potential. Going all 5090's or 6000 PRO's would have been unfeasible budget-wise and in the end likely unnecessary, two 6000's alone could have eaten the cost of the entire amount spent on the project, and if not for the two 5090's the final expense would have been much closer to ~$10k (still would have been an extremely capable system, but this graphic artist would really benefit from the image/video gen time savings that only a 5090 can provide).&lt;/p&gt; &lt;p&gt;The biggest hurdle was the enclosure problem. I've seen mining frames zip tied to a rack on wheels as a solution for mobility, but not only is this aesthetically unappealing, build construction and sturdiness quickly get called into question. This system would be living under the same roof with multiple cats, so an enclosure was almost beyond a nice-to-have, the hardware will need a physical barrier between the expensive components and curious paws. Mining frames were quickly ruled out altogether after a failed experiment. Enter the W200, a platform that I'm frankly surprised I haven't heard suggested before in forum discussions about planning multi-GPU builds, and is the main motivation for this post. The W200 is intended to be a dual-system enclosure, but when the motherboard is installed upside-down in its secondary compartment, this makes a perfect orientation to connect risers to mounted GPU's in the &amp;quot;main&amp;quot; compartment. If you don't mind working in dense compartments to get everything situated (the sheer density overall of the system is among its only drawbacks), this approach reduces the jank from mining frame + wheeled rack solutions significantly. A few zip ties were still required to secure GPU's in certain places, but I don't feel remotely as anxious about moving the system to a different room or letting cats inspect my work as I would if it were any other configuration.&lt;/p&gt; &lt;p&gt;Now the caveat. Because of the specific GPU choices made (3x of the 3090's are AIO hybrids), this required putting one of the W200's fan mounting rails on the main compartment side in order to mount their radiators (pic shown with the glass panel open, but it can be closed all the way). This means the system technically should not run without this panel at least slightly open so it doesn't impede exhaust, but if these AIO 3090's were blower/air cooled, I see no reason why this couldn't run fully closed all the time as long as fresh air intake is adequate.&lt;/p&gt; &lt;p&gt;The final case pic shows the compartment where the actual motherboard is installed (it is however very dense with risers and connectors so unfortunately it is hard to actually see much of anything) where I removed one of the 5090's. Airflow is very good overall (I believe 12x 140mm fans were installed throughout), GPU temps remain in good operation range under load, and it is surprisingly quiet when inferencing. Honestly, given how many fans and high power GPU's are in this thing, I am impressed by the acoustics, I don't have a sound meter to measure db's but to me it doesn't seem much louder than my gaming rig.&lt;/p&gt; &lt;p&gt;I typically power limit the 3090's to 200-250W and the 5090's to 500W depending on the workload.&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;Benchmarks&lt;/p&gt; &lt;p&gt;Deepseek V3.1 Terminus Q2XXS (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 2338 tokens&lt;/p&gt; &lt;p&gt;Time to first token - 1.38s&lt;/p&gt; &lt;p&gt;Token gen rate - 24.92tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;GLM 4.6 Q4KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 4096&lt;/p&gt; &lt;p&gt;Time to first token - 0.76s&lt;/p&gt; &lt;p&gt;Token gen rate - 26.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Kimi K2 TQ1 (87% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 1664&lt;/p&gt; &lt;p&gt;Time to first token - 2.59s&lt;/p&gt; &lt;p&gt;Token gen rate - 19.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Hermes 4 405b Q3KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - was so underwhelmed by the response quality I forgot to record lol&lt;/p&gt; &lt;p&gt;Time to first token - 1.13s&lt;/p&gt; &lt;p&gt;Token gen rate - 3.52tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Qwen 235b Q6KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 3081&lt;/p&gt; &lt;p&gt;Time to first token - 0.42s&lt;/p&gt; &lt;p&gt;Token gen rate - 31.54tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;I've thought about doing a cost breakdown here, but with price volatility and the fact that so many components have gone up since I got them, I feel like there wouldn't be much of a point and may only mislead someone. Current RAM prices alone would completely change the estimate cost of doing the same build today by several thousand dollars. Still, I thought I'd share my approach on the off chance it inspires or is interesting to someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qi4uj2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qih9r8</id>
    <title>Current GLM-4.7-Flash implementation confirmed to be broken in llama.cpp</title>
    <updated>2026-01-20T23:28:10+00:00</updated>
    <author>
      <name>/u/Sweet_Albatross9772</name>
      <uri>https://old.reddit.com/user/Sweet_Albatross9772</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recent discussion in &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;https://github.com/ggml-org/llama.cpp/pull/18936&lt;/a&gt; seems to confirm my suspicions that the current llama.cpp implementation of GLM-4.7-Flash is broken.&lt;/p&gt; &lt;p&gt;There are significant differences in logprobs compared to vLLM. That could explain the looping issues, overthinking, and general poor experiences people have been reporting recently.&lt;/p&gt; &lt;p&gt;Edit:&lt;br /&gt; There is a potential fix already in this PR thanks to Piotr:&lt;br /&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;https://github.com/ggml-org/llama.cpp/pull/18980&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sweet_Albatross9772"&gt; /u/Sweet_Albatross9772 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qih9r8/current_glm47flash_implementation_confirmed_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj2i4q</id>
    <title>Docker config for vLLM GLM-4.7-Flash support with glm4_moe_lite patch</title>
    <updated>2026-01-21T16:26:26+00:00</updated>
    <author>
      <name>/u/1-a-n</name>
      <uri>https://old.reddit.com/user/1-a-n</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-4.7-Flash full context on 96GB 6000 Pro with vLLM glm4_moe_lite patch for smaller KV cache requirements found by &lt;a href="/u/ZenMagnets"&gt;u/ZenMagnets&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/ian-hailey/vllm-docker-GLM-4.7-Flash"&gt;https://github.com/ian-hailey/vllm-docker-GLM-4.7-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1-a-n"&gt; /u/1-a-n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2i4q/docker_config_for_vllm_glm47flash_support_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2i4q/docker_config_for_vllm_glm47flash_support_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2i4q/docker_config_for_vllm_glm47flash_support_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T16:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj88tx</id>
    <title>I couldn't remember the difference between IQ and Q quantizations, so here's a primer if you're in the same boat</title>
    <updated>2026-01-21T19:51:57+00:00</updated>
    <author>
      <name>/u/Prior-Consequence416</name>
      <uri>https://old.reddit.com/user/Prior-Consequence416</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been grabbing GGUFs for months, but lately, I realized I’d completely forgotten the actual difference between the new-ish &lt;code&gt;IQ&lt;/code&gt; files and the standard &lt;code&gt;Q&lt;/code&gt; (K-quants). I just looked into it again to refresh my memory, so here is the &amp;quot;explain it like I'm 5&amp;quot; summary so you don’t have to dig through GitHub threads.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Have plenty of VRAM? &lt;code&gt;Q4_K_M&lt;/code&gt; or &lt;code&gt;Q5_K_M&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;VRAM tight? &lt;code&gt;IQ3_M&lt;/code&gt; (Better than standard Q3).&lt;/li&gt; &lt;li&gt;Avoid &lt;code&gt;IQ1&lt;/code&gt; / &lt;code&gt;IQ2&lt;/code&gt; unless you are running a massive model (70B+) on a potato.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;IQ&lt;/strong&gt; &lt;del&gt;stands for&lt;/del&gt; &lt;strong&gt;&lt;del&gt;Importance Quantization&lt;/del&gt;&lt;/strong&gt; uses vectorized quantization (and introduced Importance Matrices)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Standard Q (e.g., Q4_K_M)&lt;/strong&gt; is like standard compression. It rounds off numbers fairly evenly to save space.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;IQ (e.g., IQ3_M)&lt;/strong&gt; is the &amp;quot;smart&amp;quot; version. It uses an &amp;quot;Importance Matrix&amp;quot; (imatrix). Essentially, the model runs a test to see which brain neurons (weights) are actually doing the heavy lifting and which ones are useless. It protects the important ones and compresses the useless ones harder.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I used to avoid anything under Q4 because it made the models dumb, but it turns out I was doing it wrong.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;If you can run Q4 or higher, j&lt;/strong&gt;ust stick to standard &lt;code&gt;Q4_K_M&lt;/code&gt;. The smart tech in IQ doesn't help much here because you have enough bits to keep the model smart anyway.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;If you are crunched for VRAM&lt;/strong&gt; switch to &lt;strong&gt;IQ&lt;/strong&gt;. &lt;ul&gt; &lt;li&gt;&lt;code&gt;IQ3_M&lt;/code&gt; &lt;strong&gt;&amp;gt;&lt;/strong&gt; &lt;code&gt;Q3_K_M&lt;/code&gt; so if you can't fit the Q4, do &lt;strong&gt;not&lt;/strong&gt; get the standard Q3. Get the IQ3. Because it knows which weights to keep, it is &lt;em&gt;way&lt;/em&gt; more coherent than the old 3-bit quants.&lt;/li&gt; &lt;li&gt;Even &lt;strong&gt;IQ2&lt;/strong&gt; quants are actually usable now for massive models (like Llama-3-70B) if you're desperate, whereas the old Q2s were basically gibberish generators.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Hope this saves someone else the Google search (oh wait—that's probably how half of you got here).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Consequence416"&gt; /u/Prior-Consequence416 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj88tx/i_couldnt_remember_the_difference_between_iq_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj88tx/i_couldnt_remember_the_difference_between_iq_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj88tx/i_couldnt_remember_the_difference_between_iq_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T19:51:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qim0e9</id>
    <title>vLLM v0.14.0 released</title>
    <updated>2026-01-21T02:50:09+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"&gt; &lt;img alt="vLLM v0.14.0 released" src="https://external-preview.redd.it/09XZY9bYFkjK1xfZ16UA__JE3yDYBU7C83HKWilthGw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dba3ac860fe3ee793564a3f2e4d6cf66f32e888a" title="vLLM v0.14.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/releases/tag/v0.14.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qim0e9/vllm_v0140_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T02:50:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjcvzt</id>
    <title>Parallelism with mismatched GPUs (and how to optimize it)?</title>
    <updated>2026-01-21T22:45:01+00:00</updated>
    <author>
      <name>/u/Infinite100p</name>
      <uri>https://old.reddit.com/user/Infinite100p</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I see some posts with lots of users using a mix of GPUs.&lt;/p&gt; &lt;p&gt;A simple example is, for example, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt;this post where OP uses a mix of 3090s and 5090s&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I've seen people running a mix of 3 NVidia GPUs: an RTX 5090, 5080, and a 5070.&lt;/p&gt; &lt;p&gt;But I've also seen people who claim more complex setups &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nlyy6n/comment/nfa4a8c/"&gt;like this person here allegedly using a mix of Intel ARC and NVidia GPUs&lt;/a&gt; which are very different beasts with different software stacks. Although, here I'm not sure this person's llama.cpp isn't just running it on 1 GPU with RAM offload without him even realizing it.&lt;/p&gt; &lt;p&gt;My question is: &lt;/p&gt; &lt;p&gt;Suppose we had several Intel Arc Pro cards and 1 or 2 NVidia cards (let's say a 5090 and a 5080), and lets say that the combined VRAM is 196GB between both Arc and NVidia GPUs, would pipeline parallelism be the only feasible solution that utilizes all the cards for running larger models that would only fit in the combined VRAM of 192GB?&lt;/p&gt; &lt;p&gt;Does anyone have experience running Intel and NVidia cards together this way? How would you set it up, given that a 5090 is a far more powerful GPU: what would you offload to the 5090 VS what would you offload to the weaker Arc GPUs?&lt;/p&gt; &lt;p&gt;How would you generally approach designing the setup for a mismatched set: What are your rules of thumb?&lt;/p&gt; &lt;p&gt;Also, I would appreciate if someone could explain what is the the overhead/perf penalty tradeoff for pipeline parallelism compared to tensor parallelism? E.g., if I run a 60GB LLM on 2x RTX 5090 using tensor parallelism VS pipeline parallelism on the same cards, what diff/tradeoff would I witness? Is one type of parallelism always superior over the other (in setups where both are possible, of course)?&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite100p"&gt; /u/Infinite100p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjcvzt/parallelism_with_mismatched_gpus_and_how_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjcvzt/parallelism_with_mismatched_gpus_and_how_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjcvzt/parallelism_with_mismatched_gpus_and_how_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:45:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjb4m0</id>
    <title>[Benchmark] RK3588 NPU vs Raspberry Pi 5 - Llama 3.1 8B, Qwen 3B, DeepSeek 1.5B tested</title>
    <updated>2026-01-21T21:38:19+00:00</updated>
    <author>
      <name>/u/tre7744</name>
      <uri>https://old.reddit.com/user/tre7744</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been lurking here for a while, finally have some data worth sharing.&lt;/p&gt; &lt;p&gt;I wanted to see if the 6 TOPS NPU on the RK3588S actually makes a difference for local inference compared to Pi 5 running CPU-only. Short answer: yes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware tested:&lt;/strong&gt; - Indiedroid Nova (RK3588S, 16GB RAM, 64GB eMMC) - NPU driver v0.9.7, RKLLM runtime 1.2.1 - Debian 12&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Nova (NPU)&lt;/th&gt; &lt;th&gt;Pi 5 16GB (CPU)&lt;/th&gt; &lt;th&gt;Difference&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DeepSeek 1.5B&lt;/td&gt; &lt;td&gt;11.5 t/s&lt;/td&gt; &lt;td&gt;~6-8 t/s&lt;/td&gt; &lt;td&gt;1.5-2x faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 2.5 3B&lt;/td&gt; &lt;td&gt;7.0 t/s&lt;/td&gt; &lt;td&gt;~2-3 t/s*&lt;/td&gt; &lt;td&gt;2-3x faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama 3.1 8B&lt;/td&gt; &lt;td&gt;3.72 t/s&lt;/td&gt; &lt;td&gt;1.99 t/s&lt;/td&gt; &lt;td&gt;1.87x faster&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Pi 5 8B number from Jeff Geerling's benchmarks. I don't have a Pi 5 16GB to test directly.&lt;/p&gt; &lt;p&gt;*Pi 5 3B estimate based on similar-sized models (Phi 3.5 3.8B community benchmarks)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The thing that surprised me:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Nova's advantage isn't just speed - it's that 16GB RAM + NPU headroom lets you run the 3B+ models that actually give correct answers, at speeds the Pi 5 only hits on smaller models. When I tested state capital recall, Qwen 3B got all 50 right. DeepSeek 1.5B started hallucinating around state 30.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What sucked:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pre-converted models from mid-2024 throw &amp;quot;model version too old&amp;quot; errors. Had to hunt for newer conversions (VRxiaojie and c01zaut on HuggingFace work).&lt;/li&gt; &lt;li&gt;Ecosystem is fragmented compared to ollama pull whatever.&lt;/li&gt; &lt;li&gt;Setup took ~3 hours to first inference. Documentation and reproducibility took longer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NPU utilization during 8B inference:&lt;/strong&gt; 79% average across all 3 cores, 8.5GB RAM sustained. No throttling over 2+ minute runs.&lt;/p&gt; &lt;p&gt;Happy to answer questions if anyone wants to reproduce this.&lt;/p&gt; &lt;p&gt;Setup scripts and full methodology: github.com/TrevTron/indiedroid-nova-llm&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Methodology note: Hardware provided by AmeriDroid. Benchmarks are my own.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tre7744"&gt; /u/tre7744 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjb4m0/benchmark_rk3588_npu_vs_raspberry_pi_5_llama_31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjb4m0/benchmark_rk3588_npu_vs_raspberry_pi_5_llama_31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjb4m0/benchmark_rk3588_npu_vs_raspberry_pi_5_llama_31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T21:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qids6a</id>
    <title>You have 64gb ram and 16gb VRAM; internet is permanently shut off: what 3 models are the ones you use?</title>
    <updated>2026-01-20T21:15:23+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No more internet: you have 3 models you can run&lt;/p&gt; &lt;p&gt;What local models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qids6a/you_have_64gb_ram_and_16gb_vram_internet_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T21:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qir5eq</id>
    <title>Here is how to get GLM 4.7 working on llama.cpp with flash attention and correct outputs</title>
    <updated>2026-01-21T07:07:52+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tested GPU: RTX 6000 Blackwell&lt;br /&gt; Tested GGUF: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use this git branch to enable flash attention on CUDA &lt;a href="https://github.com/am17an/llama.cpp/tree/glm_4.7_headsize"&gt;https://github.com/am17an/llama.cpp/tree/glm_4.7_headsize&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Add this to your options &lt;code&gt;--override-kv deepseek2.expert\_gating\_func=int:2&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;2000+ tokens/sec prompt, 97 tokens a second generation&lt;/p&gt; &lt;p&gt;Output looks fantastic for a model this size.&lt;/p&gt; &lt;p&gt;Note: Quants might have been made with the wrong function, so you may have to wait for them to be recreated, otherwise you may get nonsensical outputs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qir5eq/here_is_how_to_get_glm_47_working_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T07:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjbufk</id>
    <title>Anyscale's new data: Most AI clusters run at &lt;50% utilization. Is "Disaggregation" the fix, or just faster cold starts?</title>
    <updated>2026-01-21T22:04:50+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyscale just published a deep dive showing that most production AI clusters average &amp;lt;50% GPU utilization.&lt;/p&gt; &lt;p&gt;The TL;DR: Because AI workloads are bursty (and CPU/GPU scaling needs differ), we end up provisioning massive clusters that sit idle waiting for traffic.&lt;/p&gt; &lt;p&gt;Their Solution (Ray): &amp;quot;Disaggregation.&amp;quot; Split the CPU logic from the GPU logic so you can saturate the GPUs more efficiently.&lt;/p&gt; &lt;p&gt;My Hot Take:&lt;/p&gt; &lt;p&gt;Disaggregation feels like over-engineering to solve a physics problem.&lt;/p&gt; &lt;p&gt;The only reason we keep those GPUs idle (and pay for them) is because cold starts are too slow (30s+).&lt;/p&gt; &lt;p&gt;If we could load a 70B model in &amp;lt;2 seconds (using System RAM tiering/PCIe saturation), we wouldn't need complex schedulers to &amp;quot;keep the GPU busy.&amp;quot; We would just turn it off.&lt;/p&gt; &lt;p&gt;We’ve been testing this &amp;quot;Ephemeral&amp;quot; approach on my local 3090 (hot-swapping models from RAM in ~1.5s), and it feels much cleaner than trying to manage a complex Ray cluster. GitHub Repo: &lt;a href="https://github.com/inferx-net/inferx"&gt;https://github.com/inferx-net/inferx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what production engineers here think: Are you optimizing for Utilization (Ray) or Ephemerality (Fast Loading).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjbufk/anyscales_new_data_most_ai_clusters_run_at_50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjbufk/anyscales_new_data_most_ai_clusters_run_at_50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjbufk/anyscales_new_data_most_ai_clusters_run_at_50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:04:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiuxko</id>
    <title>Local file search engine that understands your documents (OCR + Semantic Search) - Open Source.</title>
    <updated>2026-01-21T10:59:51+00:00</updated>
    <author>
      <name>/u/Hamza3725</name>
      <uri>https://old.reddit.com/user/Hamza3725</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/"&gt; &lt;img alt="Local file search engine that understands your documents (OCR + Semantic Search) - Open Source." src="https://preview.redd.it/j5duc1vhgoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b93de6d4c9a467d7be36c4a64410fe7f1b43b2f" title="Local file search engine that understands your documents (OCR + Semantic Search) - Open Source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Llammas!&lt;/p&gt; &lt;p&gt;I’ve been working on &lt;strong&gt;File Brain&lt;/strong&gt;, an open-source desktop tool that lets you search your local files using natural language. It runs 100% locally on your machine.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;We have thousands of files (PDFs, Office docs, images, archives, etc) in our hard drives and we constantly forget their filenames (or we don't even give them correct filenames in first place). Regular search tools often fail in this case because they rely on keyword matching, and they definitely don't understand the &lt;em&gt;content&lt;/em&gt; of a scanned invoice or a screenshot.&lt;/p&gt; &lt;h1&gt;The Solution&lt;/h1&gt; &lt;p&gt;I built a tool that automatically indexes your files and allows you to type queries like &lt;em&gt;&amp;quot;Airplane ticket&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;Company phone number&amp;quot;&lt;/em&gt; and instantly locates matching files for you, even if the filename is completely random or does not contain these keywords explicitly mentioned.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Semantic Search:&lt;/strong&gt; It uses a multilingual embedding model to understand intent. You can search in one language and find docs in another.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OCR Built-in:&lt;/strong&gt; Can extract the content from most file types, including from images, scanned PDFs, and screenshots.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Privacy First:&lt;/strong&gt; Everything runs locally, including the embedding model.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Tech Stack&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Python/FastAPI/watchdog for backend and the custom filesystem crawler/monitor.&lt;/li&gt; &lt;li&gt;React + PrimeReact for the UI.&lt;/li&gt; &lt;li&gt;Typesense for indexing and search.&lt;/li&gt; &lt;li&gt;Apache Tika for file content extraction.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interested? try it out at &lt;a href="https://github.com/Hamza5/file-brain"&gt;https://github.com/Hamza5/file-brain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s currently available for &lt;strong&gt;Windows&lt;/strong&gt; and &lt;strong&gt;Linux&lt;/strong&gt;. It should work on Mac too, but I haven't tested it yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hamza3725"&gt; /u/Hamza3725 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j5duc1vhgoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiuxko/local_file_search_engine_that_understands_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T10:59:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjf6ys</id>
    <title>Wrote a guide for running Claude Code with GLM-4.7 Flash locally with llama.cpp</title>
    <updated>2026-01-22T00:17:31+00:00</updated>
    <author>
      <name>/u/tammamtech</name>
      <uri>https://old.reddit.com/user/tammamtech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many of ollama features are now support llama.cpp server but aren't well documented. The ollama convenience features can be replicated in llama.cpp now, the main ones I wanted were model swapping, and freeing gpu memory on idle because I run llama.cpp as a docker service exposed to internet with cloudflare tunnels.&lt;/p&gt; &lt;p&gt;The GLM-4.7 flash release and the recent support for Anthropic API in llama.cpp server gave me the motivation to finally make this happen. I basically wanted to run Claude Code from laptop withGLM 4.7 Flash running on my PC.&lt;/p&gt; &lt;p&gt;I wrote a slightly more comprehensive version&lt;a href="https://tammam.io/blog/llama-cpp-setup-with-claude-codex-cli/"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;Install llama.cpp if you don't have it&lt;/h3&gt; &lt;p&gt;I'm going to assume you have llama-cli or llama-server installed or you have ability to run docker containers with gpu. There are many sources for how to do this.&lt;/p&gt; &lt;h3&gt;Running the model&lt;/h3&gt; &lt;p&gt;All you need is the following command if you just want to run GLM 4.7 Flash.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash llama-cli -hf unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL \ --alias glm-4.7-flash \ --jinja --ctx-size 32768 \ --temp 1.0 --top-p 0.95 --min-p 0.01 --fit on \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The command above will download the model on first run and cache it locally. The `&lt;code&gt;sleep-idle-seconds 300&lt;/code&gt; frees GPU memory after 5 minutes of idle so you can keep the server running.&lt;/p&gt; &lt;p&gt;The sampling parameters above (&lt;code&gt;--temp 1.0 --top-p 0.95 --min-p 0.01&lt;/code&gt;) are the recommended settings for GLM-4.7 general use. For tool-calling, use &lt;code&gt;--temp 0.7 --top-p 1.0&lt;/code&gt; instead.&lt;/p&gt; &lt;h4&gt;Or With Docker&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash docker run --gpus all -p 8080:8080 \ ghcr.io/ggml-org/llama.cpp:server-cuda \ -hf unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL \ --jinja --ctx-size 32768 \ --temp 1.0 --top-p 0.95 --min-p 0.01 --fit on \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Multi-Model Setup with Config File&lt;/h3&gt; &lt;p&gt;If you want to run multiple models with router mode, you'll need a config file. This lets the server load models on demand based on what clients request.&lt;/p&gt; &lt;p&gt;First, download your models (or let them download via &lt;code&gt;-hf&lt;/code&gt; on first use):&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash mkdir -p ~/llama-cpp &amp;amp;&amp;amp; touch ~/llama-cpp/config.ini &lt;/code&gt;&lt;/p&gt; &lt;p&gt;In &lt;code&gt;~/llama-cpp/config.ini&lt;/code&gt; put your models settings:&lt;/p&gt; &lt;p&gt;```ini [*] &lt;/p&gt; &lt;h1&gt;Global settings&lt;/h1&gt; &lt;p&gt;[glm-4.7-flash] hf-repo = unsloth/GLM-4.7-Flash-GGUF:UD-Q4_K_XL jinja = true temp = 0.7 ctx-size = 32768 top-p = 1 min-p = 0.01 fit = on&lt;/p&gt; &lt;p&gt;[other-model] ... ```&lt;/p&gt; &lt;h4&gt;Run with Router Mode&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash llama-cli \ --models-preset ~/llama-cpp/config.ini \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 --models-max 1 &lt;/code&gt;&lt;/p&gt; &lt;h4&gt;Or with Docker&lt;/h4&gt; &lt;p&gt;&lt;code&gt;bash docker run --gpus all -p 8080:8080 \ -v ~/llama-cpp/config.ini:/config.ini \ ghcr.io/ggml-org/llama.cpp:server-cuda \ --models-preset /config.ini \ --sleep-idle-seconds 300 \ --host 0.0.0.0 --port 8080 \ --models-max 1 &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Configuring Claude Code&lt;/h2&gt; &lt;p&gt;Claude Code can be pointed at your local server. In your terminal run&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash export ANTHROPIC_BASE_URL=http://localhost:8080 claude --model glm-4.7-flash &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will now use your local model instead of hitting Anthropic's servers.&lt;/p&gt; &lt;h2&gt;Configuring Codex CLI&lt;/h2&gt; &lt;p&gt;You can also configure the Codex CLI to use your local server. Modify the &lt;code&gt;~/.codex/config.toml&lt;/code&gt; to look something like this:&lt;/p&gt; &lt;p&gt;```toml model = &amp;quot;glm-4.7-flash&amp;quot; model_reasoning_effort = &amp;quot;medium&amp;quot; model_provider=&amp;quot;llamacpp&amp;quot;&lt;/p&gt; &lt;p&gt;[model_providers.llamacpp] name=&amp;quot;llamacpp&amp;quot; base_url=&amp;quot;http://localhost:8080/v1&amp;quot; ```&lt;/p&gt; &lt;h2&gt;Some Extra Notes&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Model load time&lt;/strong&gt;: When a model is unloaded (after idle timeout), the next request has to wait for it to load again. For large models this can take some time. Tune &lt;code&gt;--sleep-idle-seconds&lt;/code&gt; based on your usage pattern.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance and Memory Tuning&lt;/strong&gt;: There are more flags you can use in llama.cpp for tuning cpu offloading, flash attention, etc that you can use to optimize memory usage and performance. The &lt;code&gt;--fit&lt;/code&gt; flag is a good starting point. Check the &lt;a href="https://github.com/ggml-org/llama.cpp/blob/master/tools/server/README.md"&gt;llama.cpp server docs&lt;/a&gt; for details on all the flags.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Internet Access&lt;/strong&gt;: If you want to use models deployed on your PC from say your laptop, the easiest way is to use something like Cloudflare tunnels, I go over setting this up in &lt;a href="https://tammam.io/blog/access-sd-ui-over-internet"&gt;my Stable Diffusion setup guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Auth&lt;/strong&gt;: If exposing the server to the internet, you can use &lt;code&gt;--api-key KEY&lt;/code&gt; to require an API key for authentication.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tammamtech"&gt; /u/tammamtech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjf6ys/wrote_a_guide_for_running_claude_code_with_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-22T00:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiu6jo</id>
    <title>Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation</title>
    <updated>2026-01-21T10:14:30+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"&gt; &lt;img alt="Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation" src="https://preview.redd.it/64ya7ykngoeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b1b6edf606c11be574456a3c46cef04dd0dfd81" title="Knowledge distillation with Claude as the interface: trained a 0.6B model to match GPT-class performance on Text2SQL in a singe conversation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a workflow for training small, task-specific models without the usual ML setup overhead.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Off-the-shelf small models are bad at specialized tasks. Qwen3 0.6B on Text2SQL gives you stuff like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; -- Qwen3 0.6B output: SELECT artists.name FROM artists WHERE artists.genre IS NULL OR artists.country IS NULL; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Completely wrong. But fine-tuning means data prep, training infrastructure, hyperparameter tuning...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The approach:&lt;/strong&gt; Knowledge distillation via a Claude skill that wraps &lt;a href="https://docs.distillabs.ai"&gt;distil-cli&lt;/a&gt;. A large teacher model (DeepSeek-V3) generates synthetic training data from your examples, then a small student model learns to match its outputs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```bash curl -fsSL &lt;a href="https://cli-assets.distillabs.ai/install.sh"&gt;https://cli-assets.distillabs.ai/install.sh&lt;/a&gt; | sh distil login&lt;/p&gt; &lt;h1&gt;In Claude Code:&lt;/h1&gt; &lt;p&gt;/plugin marketplace add &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;https://github.com/distil-labs/distil-cli-skill&lt;/a&gt; /plugin install distil-cli@distil-cli-skill ```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Claude handles:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Step&lt;/th&gt; &lt;th&gt;What happens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Task selection&lt;/td&gt; &lt;td&gt;Recommends QA/classification/tool-calling/RAG based on your description&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Data conversion&lt;/td&gt; &lt;td&gt;Takes whatever format you have, outputs proper JSONL&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Teacher eval&lt;/td&gt; &lt;td&gt;Runs the teacher on your test set — if it scores low, don't bother training&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Training&lt;/td&gt; &lt;td&gt;Kicks off distillation, monitors progress&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Packaging&lt;/td&gt; &lt;td&gt;Downloads GGUF, HuggingFace format, or LoRA adapter&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;My test run:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Input: 100 conversation traces (not cleaned, just raw logs)&lt;/li&gt; &lt;li&gt;Task: Text2SQL&lt;/li&gt; &lt;li&gt;Teacher eval: 80% LLM-as-a-Judge&lt;/li&gt; &lt;li&gt;Final student score: 74%&lt;/li&gt; &lt;li&gt;Base model score: 36%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Output is a 2.2GB GGUF that runs locally via Ollama.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;After fine-tuning:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;sql -- Same question: &amp;quot;Which artists have total album sales over 1 million?&amp;quot; -- Fine-tuned output: SELECT a.name FROM artists a JOIN albums al ON a.id = al.artist_id GROUP BY a.id, a.name HAVING SUM(al.sales) &amp;gt; 1000000; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Correct JOINs, proper GROUP BY, HAVING instead of WHERE.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full benchmark:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;ROUGE&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Base Qwen3 0.6B&lt;/td&gt; &lt;td&gt;36%&lt;/td&gt; &lt;td&gt;69.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;88.6%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Fine-tuned 0.6B&lt;/td&gt; &lt;td&gt;74%&lt;/td&gt; &lt;td&gt;88.5%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Skill: &lt;a href="https://github.com/distil-labs/distil-cli-skill"&gt;github.com/distil-labs/distil-cli-skill&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full example with data: &lt;a href="https://github.com/distil-labs/distil-example-text2sql-with-claude"&gt;github.com/distil-labs/distil-example-text2sql-with-claude&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Detailed walkthrough: &lt;a href="https://www.distillabs.ai/blog/train-your-slm-with-distil-claude-skill"&gt;distillabs.ai/blog/train-your-slm-with-distil-claude-skill&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the distillation process or the skill implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/64ya7ykngoeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiu6jo/knowledge_distillation_with_claude_as_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T10:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj271s</id>
    <title>Fine-tuned Qwen3-14B on 10k DeepSeek traces: +20% on security benchmark</title>
    <updated>2026-01-21T16:15:17+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work as a security auditor (basically a bug hunter) and LLMs have become the principal tool at work, like in most of IT. But token usage is huge, and it's becoming problematic as it is taking a big part of the earnings of most audit shops.&lt;/p&gt; &lt;p&gt;So I fine-tuned Qwen3-14B with about +10,000 bug-hunting thinking traces distilled from DeepSeek. It turns out that even this small dataset improved bug-hunting capabilities a lot (20% in a custom benchmark). This is not conclusive, as the benchmark could be wrong, but by using it manually, it easily shows greatly improved performance compared to the base model. It will never be as good as a frontier model, but you literally cannot apply frontier models to huge codebases, as you would spend millions of USD.&lt;/p&gt; &lt;p&gt;So I think this is a good example of how distillation of particular skills into a smaller model is a viable alternative for lowering costs.&lt;/p&gt; &lt;p&gt;If someone wants to play with it, it's available here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/NeuroengineAI/ZeroShot-Qwen3-14B-preview"&gt;https://huggingface.co/NeuroengineAI/ZeroShot-Qwen3-14B-preview&lt;/a&gt; &lt;/p&gt; &lt;p&gt;GGUF coming soon. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj271s/finetuned_qwen314b_on_10k_deepseek_traces_20_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj271s/finetuned_qwen314b_on_10k_deepseek_traces_20_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj271s/finetuned_qwen314b_on_10k_deepseek_traces_20_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T16:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj579f</id>
    <title>Lemonade v9.1.4 released: GLM-4.7-Flash-GGUF on ROCm and Vulkan, LM Studio GGUF import, and more</title>
    <updated>2026-01-21T18:02:48+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj579f/lemonade_v914_released_glm47flashgguf_on_rocm_and/"&gt; &lt;img alt="Lemonade v9.1.4 released: GLM-4.7-Flash-GGUF on ROCm and Vulkan, LM Studio GGUF import, and more" src="https://b.thumbs.redditmedia.com/r2IPKcpyTuB6c5H2P2QkwHaOdqx9wv5tfknZaJUdAxE.jpg" title="Lemonade v9.1.4 released: GLM-4.7-Flash-GGUF on ROCm and Vulkan, LM Studio GGUF import, and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lemonade has been moving fast this month so I thought I should post an update with the v9.1.4 release today.&lt;/p&gt; &lt;p&gt;If you haven't heard of it, Lemonade is a convenient local LLM server similar to Ollama or LM Studio. The main differences are that its 100% open source, isn't selling you anything, and always includes the latest tools/optimizations from AMD. Our primary goal is to grow the ecosystem of great local AI apps for end users.&lt;/p&gt; &lt;h2&gt;GLM-4.7-Flash-GGUF&lt;/h2&gt; &lt;p&gt;We're bundling llama.cpp builds from this morning for the latest GLM-4.7-Flash support: &lt;code&gt;b7788&lt;/code&gt; for Vulkan and CPU, and &lt;code&gt;b1162&lt;/code&gt; from the llamacpp-rocm project for ROCm. These builds include the &amp;quot;Fix GLM 4.7 MoE gating func&amp;quot; from just a few hours ago. &lt;/p&gt; &lt;p&gt;Try it with: &lt;code&gt;lemonade-server run GLM-4.7-Flash-GGUF --llamacpp rocm&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I can't thank the llama.cpp team enough for this amazing work! Thanks, @0cc4m, in particular, for always helping people on the discord and optimizing Strix Halo Vulkan performance.&lt;/p&gt; &lt;h2&gt;LM Studio Compatibility&lt;/h2&gt; &lt;p&gt;You shouldn't need to download the same GGUF more than once.&lt;/p&gt; &lt;p&gt;Start Lemonade with &lt;code&gt;lemonade-server serve --extra-models-dir /path/to/.lmstudio/models&lt;/code&gt; and your GGUFs will show up in Lemonade.&lt;/p&gt; &lt;h2&gt;Platform Support&lt;/h2&gt; &lt;p&gt;The community has done a ton of work to improve platform support in Lemonade. In addition to the usual Ubuntu and Windows support, we now have Arch, Fedora, and Docker supported. There are &lt;a href="https://github.com/lemonade-sdk/lemonade/pkgs/container/lemonade-server"&gt;official dockers that ship with every release&lt;/a&gt; now.&lt;/p&gt; &lt;p&gt;Shoutout to @siavashhub, @sofiageo, @ianbmacdonald, and @SidShetye for their work here.&lt;/p&gt; &lt;h2&gt;Mobile Companion App&lt;/h2&gt; &lt;p&gt;@Geramy has contributed an entire &lt;a href="https://github.com/lemonade-sdk/lemonade-mobile"&gt;mobile app&lt;/a&gt; that connects to your Lemonade server and provides a chat interface with VLM support. It is available on the iOS app store today and will launch on Android when Google is done reviewing in about 2 weeks.&lt;/p&gt; &lt;h2&gt;Recipe Cookbook&lt;/h2&gt; &lt;p&gt;@bitgamma has done a series of PRs that allow you to save your model settings (rocm vs. vulkan, llamacpp args, etc.) to a JSON file and have them automatically apply the next time that model is loaded.&lt;/p&gt; &lt;p&gt;For example: &lt;code&gt;lemonade-server run gpt-oss-20b-mxfp4-GGUF --ctx-size 16384 --llamacpp rocm --llamacpp-args &amp;quot;--flash-attn on --no-mmap&amp;quot; --save-options&lt;/code&gt;&lt;/p&gt; &lt;p&gt;@sofiageo has a PR to add this feature to the app UI.&lt;/p&gt; &lt;h2&gt;Roadmap&lt;/h2&gt; &lt;p&gt;Under development:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;macOS support with llama.cpp+metal&lt;/li&gt; &lt;li&gt;image generation with stablediffusion.cpp&lt;/li&gt; &lt;li&gt;&amp;quot;marketplace&amp;quot; link directory to featured local AI apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Under consideration:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;vLLM and/or MLX support&lt;/li&gt; &lt;li&gt;text to speech&lt;/li&gt; &lt;li&gt;make it easier to add GGUFs from Hugging Face&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;p&gt;If you like what we're doing, please star us on GitHub: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;https://github.com/lemonade-sdk/lemonade&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to hang out, you can find us on the Lemonade Discord: &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;https://discord.gg/5xXzkMu8Zk&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qj579f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj579f/lemonade_v914_released_glm47flashgguf_on_rocm_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj579f/lemonade_v914_released_glm47flashgguf_on_rocm_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T18:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiy0ha</id>
    <title>GLM-4.7-Flash-GGUF bug fix - redownload for better outputs</title>
    <updated>2026-01-21T13:34:00+00:00</updated>
    <author>
      <name>/u/etherd0t</name>
      <uri>https://old.reddit.com/user/etherd0t</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Jan 21 update: llama.cpp fixed a bug that caused looping and poor outputs. We updated the GGUFs - please re-download the model for much better outputs.&lt;/p&gt; &lt;p&gt;You can now use Z.ai's recommended parameters and get great results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For general use-case: &lt;code&gt;--temp 1.0 --top-p 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;For tool-calling: &lt;code&gt;--temp 0.7 --top-p 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;If using llama.cpp, set &lt;code&gt;--min-p 0.01&lt;/code&gt; as llama.cpp's default is 0.1&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;unsloth/GLM-4.7-Flash-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/etherd0t"&gt; /u/etherd0t &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiy0ha/glm47flashgguf_bug_fix_redownload_for_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T13:34:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj13uh</id>
    <title>One-shot single page web development: pacman clone - GLM 4.7 vs GLM 4.7 Flash vs GLM 4.5 Air vs Gemini 3 Pro vs Gemini 3 Flash - Results available for online testing - Prompt and instructions provided for testing with other models</title>
    <updated>2026-01-21T15:36:18+00:00</updated>
    <author>
      <name>/u/ex-arman68</name>
      <uri>https://old.reddit.com/user/ex-arman68</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a big fan of testing coding models by asking them to do one, or few shots, simple development. I have just ran a test asking them to one-shot a pacman clone as a single webpage. The results did not actually match my expectations: I thought Gemini 3 Pro would be the clear winner, followed by Gemini 3 Flash, and then GLM 4.7. This is how I actually rank the results:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt; (by far the clear winner)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Pro&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7 Flash&lt;/strong&gt; (disappointing, I expected more)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.5 Air&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can find the system and user prompts at bottom of this post. Don't forget to set the temperature to 0. I have tested with the default temperature, and the results are always better with a setting of 0, as well being 100% reproducible.&lt;/p&gt; &lt;p&gt;If you run the test with other models, please share your results.&lt;/p&gt; &lt;p&gt;Here is a bit more details about each result, as well as link to the generated webpages.&lt;/p&gt; &lt;h1&gt;GLM 4.7 (z.ai API)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/glm-4.7"&gt;pacman_glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Almost fully working. Good pacman and ghosts behaviour and speed. One bug causes the game to freeze, but only minor fix required.&lt;/p&gt; &lt;h1&gt;Gemini 3 Flash&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/gemini-3-flash"&gt;pacman_gemini-3-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mostly working. Too fast. Bad ghost logic. Navigation problems.&lt;/p&gt; &lt;h1&gt;Gemini 3 Pro&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/gemini-3-pro"&gt;pacman_gemini-3-pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pacman barely working. Ghosts not working.&lt;/p&gt; &lt;h1&gt;GLM 4.7 Flash (8-bit MLX)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/glm-4.7-flash"&gt;pacman_glm-4.7-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cannot get past the loading screen. A second shot with well written debugging instructions did not fix it.&lt;/p&gt; &lt;h1&gt;GLM 4.5 Air (Qx53gx MLX)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://guigand.com/pacman/glm-4.5-air"&gt;pacman_glm-4.5-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cannot get past the loading screen. A second shot with well written debugging instructions did not fix it.&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;h1&gt;User prompt&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;I need you to write a fully working pacman clone in a single html webpage. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;System prompt&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;You are the world's leading expert in vanilla web development, specifically in creating high-performance, single-file web applications using only HTML5, CSS3, and ES6+ JavaScript. You reject frameworks in favor of clean, efficient, and semantic code. Your goal is to receive a requirement and produce a single, self-contained HTML file that functions perfectly without external dependencies (no CDNs, no images, no libraries). Because you must complete this task in a &amp;quot;one-shot&amp;quot; continuous generation, you must think before you code. You will follow a strict &amp;quot;Chain of Thought&amp;quot; protocol to ensure correctness. Follow this specific execution format for every response: &amp;lt;analysis&amp;gt; 1. REQUIREMENTS BREAKDOWN: - List every functional and non-functional requirement. - Identify potential edge cases. 2. ARCHITECTURAL PLAN: - CSS Strategy: Define the variable system, layout approach (Flexbox/Grid), and responsive breakpoints. - JS Architecture: Define state management, event listeners, and core logic functions. - HTML Structure: specific semantic tags to be used. 3. PRE-MORTEM &amp;amp; STRATEGY: - Identify the most likely point of failure. - Define the solution for that specific failure point before writing code. &amp;lt;/analysis&amp;gt; &amp;lt;implementation&amp;gt; (Provide the complete, valid HTML string here. Include CSS in &amp;lt;style&amp;gt; and JS in &amp;lt;script&amp;gt; tags. The code must be production-ready, accessible, and clean.) &amp;lt;/implementation&amp;gt; &amp;lt;code_review&amp;gt; Self-Correction and Validation Report: 1. Does the code meet all requirements listed in the analysis? [Yes/No] 2. Are there any distinct accessibility (a11y) violations? 3. Verify that no external libraries were used. &amp;lt;/code_review&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ex-arman68"&gt; /u/ex-arman68 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj13uh/oneshot_single_page_web_development_pacman_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T15:36:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjd8dp</id>
    <title>Kimi-Linear-48B-A3B-Instruct-GGUF Support - Any news?</title>
    <updated>2026-01-21T22:58:38+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-Linear seems to handle long context pretty well. Do you have any idea why it's still not implemented in llama.cpp? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjd8dp/kimilinear48ba3binstructgguf_support_any_news/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:58:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjc8a2</id>
    <title>Michigan is pushing a Anti Chatbot bill to protect the heckin kiddos</title>
    <updated>2026-01-21T22:19:31+00:00</updated>
    <author>
      <name>/u/PostEasy7183</name>
      <uri>https://old.reddit.com/user/PostEasy7183</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Senate Democrats Call for Improved Safety Measures to Better Protect Michigan Kids from Digital Dangers - Senator Kevin Hertel &lt;a href="https://share.google/ZwmPjEOVP5AcgZnhT"&gt;https://share.google/ZwmPjEOVP5AcgZnhT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;not much information about this yet but they've talked about making sure kids have a harder time to access chat bots. the bill is vague so far and to my knowledge no real text has been released yet. My question is how can they assess what is a teen and not without a Digital ID? I'm so sick of these bullshit laws in the spirit of &amp;quot;Protecting the children.&amp;quot; Give your thoughts below&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PostEasy7183"&gt; /u/PostEasy7183 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjc8a2/michigan_is_pushing_a_anti_chatbot_bill_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T22:19:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj40er</id>
    <title>VibeVoice-ASR released!</title>
    <updated>2026-01-21T17:20:29+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj40er/vibevoiceasr_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T17:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qj2dnd</id>
    <title>A new model from http://Z.ai, "GLM-OCR" has been spotted on Github</title>
    <updated>2026-01-21T16:21:49+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"&gt; &lt;img alt="A new model from http://Z.ai, &amp;quot;GLM-OCR&amp;quot; has been spotted on Github" src="https://preview.redd.it/tduio97daqeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd5b258b6d658644e83ddec8f6c475cc131ee93a" title="A new model from http://Z.ai, &amp;quot;GLM-OCR&amp;quot; has been spotted on Github" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tduio97daqeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qj2dnd/a_new_model_from_httpzai_glmocr_has_been_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T16:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qiwm3c</id>
    <title>Fix for GLM 4.7 Flash has been merged into llama.cpp</title>
    <updated>2026-01-21T12:29:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt; &lt;img alt="Fix for GLM 4.7 Flash has been merged into llama.cpp" src="https://external-preview.redd.it/P0aZfAO5cQnwgz36bD9sAcDcttCXWcTbQBhIkzY76fc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac930f1f077b513ae17d07167d50119d0ac69d0" title="Fix for GLM 4.7 Flash has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The world is saved!&lt;/p&gt; &lt;p&gt;FA for CUDA in progress &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18953"&gt;https://github.com/ggml-org/llama.cpp/pull/18953&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18980"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qiwm3c/fix_for_glm_47_flash_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T12:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qjaxfy</id>
    <title>8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)</title>
    <updated>2026-01-21T21:30:54+00:00</updated>
    <author>
      <name>/u/ai-infos</name>
      <uri>https://old.reddit.com/user/ai-infos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"&gt; &lt;img alt="8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)" src="https://preview.redd.it/16ndtph7treg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df00bad2dcdf2390a12afaf191c07f1264ae2752" title="8x AMD MI50 32GB at 26 t/s (tg) with MiniMax-M2.1 and 15 t/s (tg) with GLM 4.7 (vllm-gfx906)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;&lt;strong&gt;MiniMax-M2.1&lt;/strong&gt; AWQ 4bit @ &lt;strong&gt;26.8 tok/s&lt;/strong&gt; (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with MAX context length (196608)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM 4.7&lt;/strong&gt; AWQ 4bit @ &lt;strong&gt;15.6 tok/s&lt;/strong&gt; (output) // 3000 tok/s (input of 30k tok) on vllm-gfx906 with context length 95000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GPUs cost&lt;/strong&gt;: 880$ for 256GB VRAM (early 2025 prices)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Power draw&lt;/strong&gt;: 280W (idle) / 1200W (inference)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: reach one of the most cost effective solution of the world for one of the best fast intelligent local inference setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Credits&lt;/strong&gt;: BIG thanks to the Global Open source Community!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;All setup details here:&lt;/strong&gt; &lt;a href="https://github.com/ai-infos/guidances-setup-8-mi50-glm47-minimax-m21/tree/main"&gt;https://github.com/ai-infos/guidances-setup-8-mi50-glm47-minimax-m21/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Feel free to ask any questions and/or share any comments.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS&lt;/strong&gt;: few weeks ago, I posted here this setup of 16 MI50 with deepeseek v3.2: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1q6n5vl/16x_amd_mi50_32gb_at_10_ts_tg_2k_ts_pp_with/&lt;/a&gt; After few more tests/dev on it, I could have reached 14 tok/s but still not stable after ~18k tokens context input (generating garbage output) so almost useless for me. Whereas, the above models (Minimax M2.1 and GLM 4.7) are pretty stable at long context so usable for coding agents usecases etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-infos"&gt; /u/ai-infos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/16ndtph7treg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qjaxfy/8x_amd_mi50_32gb_at_26_ts_tg_with_minimaxm21_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-21T21:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
