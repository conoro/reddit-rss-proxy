<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-29T23:06:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pyikha</id>
    <title>EditMGT — fast, localized image editing with Masked Generative Transformers</title>
    <updated>2025-12-29T10:05:26+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt; &lt;img alt="EditMGT — fast, localized image editing with Masked Generative Transformers" src="https://a.thumbs.redditmedia.com/ipfAbO3mXGnc9dJmlMhzMZSQdDCR5P6AtDUpbQk9sR0.jpg" title="EditMGT — fast, localized image editing with Masked Generative Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o8ngfvhy94ag1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af9bacb07a0773d78259c8a08f832a922d503104"&gt;https://preview.redd.it/o8ngfvhy94ag1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af9bacb07a0773d78259c8a08f832a922d503104&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First &lt;strong&gt;MGT-based&lt;/strong&gt; editing framework that confines changes to target regions, mitigating diffusion “edit leakage.” &lt;strong&gt;&amp;lt;1B params&lt;/strong&gt;, reported &lt;strong&gt;~6× faster&lt;/strong&gt; edits (paper notes ~&lt;strong&gt;2s&lt;/strong&gt; per edit). &lt;/p&gt; &lt;ul&gt; &lt;li&gt;How it works: &lt;strong&gt;multi-layer attention consolidation&lt;/strong&gt; + &lt;strong&gt;region-hold sampling&lt;/strong&gt; for precise localization/preservation. &lt;a href="https://arxiv.org/html/2512.11715v1?utm_source=chatgpt.com"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Data: &lt;strong&gt;CrispEdit-2M&lt;/strong&gt; (~2M hi-res, 7 categories) released for training/eval. &lt;a href="https://huggingface.co/datasets/WeiChow/CrispEdit-2M?utm_source=chatgpt.com"&gt;Hugging Face+1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Links: &lt;strong&gt;GitHub repo&lt;/strong&gt; &lt;a href="https://github.com/weichow23/EditMGT"&gt;https://github.com/weichow23/EditMGT&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pz0phb</id>
    <title>Working examples of AMD MI50 on Proxmox 9.1 in a LXC passthrough</title>
    <updated>2025-12-29T22:46:02+00:00</updated>
    <author>
      <name>/u/bkvargyas</name>
      <uri>https://old.reddit.com/user/bkvargyas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working for 3 days trying to get two Instinct MI50 cards in a server to work on Proxmox 9.1 with Kernel 6.17.&lt;/p&gt; &lt;p&gt;Proxmox includes amdgpu drivers (I think they are rocm 6.1). I can set up the LXC, do the hardware passthrough of the cards to the LXC, get a docker container of ollama and openwebui spun up in the LXC, but ollama refuses to see the MI50 card and use the CPU.&lt;/p&gt; &lt;p&gt;rocminfo, rocm-smi and radiontop all work within the LXC. I'm using the following docker-compse for ollama, with no results. I have even went down the path of trying GPU passthrough to a VM with vendor-reset, and no luck. The LXC method has worked for be for NVIDIA, figured AMD would work as well. Also tried compiling &amp;quot;The Rock 7.10&amp;quot;, and the build fails the compile, so unable to install any newer drivers then what Proxmox has. What am I missing?&lt;/p&gt; &lt;p&gt;version: &amp;quot;3.8&amp;quot;&lt;/p&gt; &lt;p&gt;services:&lt;/p&gt; &lt;p&gt;ollama:&lt;/p&gt; &lt;p&gt;image: ollama/ollama:rocm&lt;/p&gt; &lt;p&gt;container_name: ollama&lt;/p&gt; &lt;p&gt;ports:&lt;/p&gt; &lt;p&gt;- 11434:11434&lt;/p&gt; &lt;p&gt;volumes:&lt;/p&gt; &lt;p&gt;- ollama_data:/root/.ollama&lt;/p&gt; &lt;p&gt;devices:&lt;/p&gt; &lt;p&gt;- /dev/kfd:/dev/kfd&lt;/p&gt; &lt;p&gt;- /dev/dri/renderD128:/dev/dri/renderD128&lt;/p&gt; &lt;p&gt;group_add:&lt;/p&gt; &lt;p&gt;- &amp;quot;44&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;128&amp;quot;&lt;/p&gt; &lt;p&gt;environment:&lt;/p&gt; &lt;p&gt;- HSA_OVERRIDE_GFX_VERSION=gfx906 # Adjust based on your GPU&lt;/p&gt; &lt;p&gt;- ROCR_VISIBLE_DEVICES=0 # GPU device ID (0 for first GPU)&lt;/p&gt; &lt;p&gt;- GPU_DEVICE_ORDINAL=0&lt;/p&gt; &lt;p&gt;- HIP_VISIBLE_DEVICES=0&lt;/p&gt; &lt;p&gt;- OLLAMA_DEBUG=1&lt;/p&gt; &lt;p&gt;- OLLAMA_NUM_GPU=1&lt;/p&gt; &lt;p&gt;- OLLAMA_GPU_OVERHEAD=0&lt;/p&gt; &lt;p&gt;- OLLAMA_MAX_LOADED_MODELS=1&lt;/p&gt; &lt;p&gt;restart: unless-stopped&lt;/p&gt; &lt;p&gt;networks:&lt;/p&gt; &lt;p&gt;- ollama_network&lt;/p&gt; &lt;p&gt;# Optional: Ollama Web UI (Open WebUI)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bkvargyas"&gt; /u/bkvargyas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz0phb/working_examples_of_amd_mi50_on_proxmox_91_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pz0phb/working_examples_of_amd_mi50_on_proxmox_91_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pz0phb/working_examples_of_amd_mi50_on_proxmox_91_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T22:46:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyiqly</id>
    <title>do MoEoE models stand a chance?</title>
    <updated>2025-12-29T10:15:49+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive heard about plans for DeepSeek to make their new models surpass 1 trillion parameter territory, and with them doing that, im sure other labs will too (especially labs like InclusionAI, where &amp;quot;scaling is all you need&amp;quot;)&lt;/p&gt; &lt;p&gt;so that begs the question, *would* and MoEoE model work? as in mixture of experts models that manage even more experts instead of parameters? imagine a 2-3 trillion model only having to decide on 128 experts instead of 2048 to keep low activated params? &lt;/p&gt; &lt;p&gt;i dont know enough about LLMs to answer this question, so id like to ask all of you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyk3jg</id>
    <title>Help me build a (reasonable) 4GPU low-cost LLM machine, is ASUS WS X299 PRO/SE still good?</title>
    <updated>2025-12-29T11:35:12+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I kind of exhausted what could be done with my fast. but VRAM poor, 4090 OC edition, so I was dreaming of designing an openframe 4 GPU machine that can drive with acceptable speed 4 GPUs.&lt;/p&gt; &lt;p&gt;My preliminary research found rather acceptable priced WS X299 PRO/SE workstation motherboards that paired with an 48-Lane CPU may just do the trick, also the 64GB DDR4 for it is really price acceptable. &lt;/p&gt; &lt;p&gt;So are there any better mobo/CPU combo under lesr than 1000EUR capable of driving 4 GPUS (proven solutions are getting a super thanks) , please share your experiences and thoughts, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyuvnd</id>
    <title>Built a Python library that translates embeddings from MiniLM to OpenAI — and it actually works!</title>
    <updated>2025-12-29T19:01:42+00:00</updated>
    <author>
      <name>/u/Interesting-Town-433</name>
      <uri>https://old.reddit.com/user/Interesting-Town-433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;I built a Python library called&lt;/em&gt; &lt;strong&gt;&lt;em&gt;EmbeddingAdapters&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;that&lt;/em&gt; &lt;strong&gt;&lt;em&gt;provides multiple pre-trained adapters for translating embeddings from one model space into another&lt;/em&gt;&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/PotentiallyARobot/EmbeddingAdapters/"&gt;https://github.com/PotentiallyARobot/EmbeddingAdapters/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;```&lt;br /&gt; &lt;code&gt;pip install embedding-adapters&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;embedding-adapters embed --source sentence-transformers/all-MiniLM-L6-v2 --target openai/text-embedding-3-small --flavor large --text &amp;quot;Where can I get a hamburger near me?&amp;quot;&lt;/code&gt;&lt;br /&gt; ```&lt;/p&gt; &lt;p&gt;&lt;em&gt;This works because&lt;/em&gt; &lt;strong&gt;&lt;em&gt;each adapter is trained on a restrictive domain&lt;/em&gt;&lt;/strong&gt; allowing the adapter to specialize in interpreting the semantic signals of smaller models into higher dimensional spaces without losing fidelity. &lt;strong&gt;&lt;em&gt;A quality endpoint then lets you determine how well the adapter will perform&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;on a given input.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;This has been super useful to me, and I'm quickly iterating on it.&lt;/p&gt; &lt;p&gt;Uses for &lt;strong&gt;&lt;em&gt;EmbeddingAdapters&lt;/em&gt;&lt;/strong&gt; so far:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You want to &lt;strong&gt;use an existing vector index built with one embedding model and query it with another&lt;/strong&gt; - if it's expensive or problematic to re-embed your entire corpus, this is the package for you.&lt;/li&gt; &lt;li&gt;You can also &lt;strong&gt;operate mixed vector indexes&lt;/strong&gt; and map to the embedding space that works best for different questions.&lt;/li&gt; &lt;li&gt;You can &lt;strong&gt;save cost on questions that are easily adapted&lt;/strong&gt;, &amp;quot;What's the nearest restaurant that has a Hamburger?&amp;quot; no need to pay for an expensive cloud provider, or wait to perform an unnecessary network hop, embed locally on the device with an embedding adapter and return results instantly.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It also lets you experiment with provider embeddings you may not have access to. By using the adapters on some queries and examples, you can compare how different embedding models behave relative to one another and get an early signal on what might work for your data before committing to a provider.&lt;/p&gt; &lt;p&gt;This makes it practical to:&lt;br /&gt; - &lt;strong&gt;sample providers you don't have direct access to&lt;/strong&gt;&lt;br /&gt; - &lt;strong&gt;migrate or experiment with embedding models gradually&lt;/strong&gt; instead of re-embedding everything at once,&lt;br /&gt; - &lt;strong&gt;&lt;em&gt;evaluate multiple providers side by side&lt;/em&gt;&lt;/strong&gt; in a consistent retrieval setup,&lt;br /&gt; - &lt;strong&gt;&lt;em&gt;handle provider outages or rate limits&lt;/em&gt;&lt;/strong&gt; without breaking retrieval,&lt;br /&gt; - &lt;strong&gt;&lt;em&gt;run RAG in air-gapped or restricted environments&lt;/em&gt;&lt;/strong&gt; with no outbound embedding calls,&lt;br /&gt; - &lt;strong&gt;&lt;em&gt;keep a stable “canonical” embedding space&lt;/em&gt;&lt;/strong&gt; while changing what runs at the edge.&lt;/p&gt; &lt;p&gt;The adapters aren't perfect clones of the provider spaces but they are pretty close, for in domain queries the minilm to openai adapter recovered 98% of the openai embedding and dramatically outperforms minilm -&amp;gt; minilm RAG setups&lt;/p&gt; &lt;p&gt;It's still early in this project. I’m actively expanding the set of supported adapter pairs, adding domain-specialized adapters, expanding the training sets, stream lining the models and improving evaluation and quality tooling.&lt;/p&gt; &lt;p&gt;I’d love feedback from anyone who might be interested in using this:&lt;br /&gt; &lt;em&gt;- What data would you like to see these adapters trained on?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;- What domains would be most helpful to target?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;- Which model pairs would you like me to add next?&lt;/em&gt;&lt;br /&gt; &lt;em&gt;- How could I make this more useful for you to use?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So far the library supports:&lt;br /&gt; &lt;em&gt;minilm &amp;lt;-&amp;gt; openai&lt;/em&gt; &lt;br /&gt; &lt;em&gt;openai &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;br /&gt; &lt;em&gt;e5 &amp;lt;-&amp;gt; minilm&lt;/em&gt;&lt;br /&gt; &lt;em&gt;e5 &amp;lt;-&amp;gt; openai&lt;/em&gt;&lt;br /&gt; &lt;em&gt;e5 &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;br /&gt; &lt;em&gt;minilm &amp;lt;-&amp;gt; gemini&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions and if anyone has any ideas please let me know.&lt;br /&gt; I could use any support you can give, especially if anyone wants to chip in to help cover the training cost.&lt;/p&gt; &lt;p&gt;Please upvote if you can, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Town-433"&gt; /u/Interesting-Town-433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyuvnd/built_a_python_library_that_translates_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyuvnd/built_a_python_library_that_translates_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyuvnd/built_a_python_library_that_translates_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T19:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylu7n</id>
    <title>Fine-tuning a Small LM for browser control with GRPO and OpenEnv</title>
    <updated>2025-12-29T13:07:04+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylu7n/finetuning_a_small_lm_for_browser_control_with/"&gt; &lt;img alt="Fine-tuning a Small LM for browser control with GRPO and OpenEnv" src="https://external-preview.redd.it/uTkHDDGDMOR6Vy-vy4ASQJNPUnxM33lqhu9afcVyTD8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ac3ac0b4424152125ee4380c95bfa3bfc0481bd" title="Fine-tuning a Small LM for browser control with GRPO and OpenEnv" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today I want to share with you the write-up of a live 60-minute session I hosted on the&lt;a href="https://discord.gg/tVvcxtkkhv"&gt; Liquid AI Discord Community&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The topic? How to teach Language Models to navigate websites and complete tasks using Reinforcement Learning.&lt;/p&gt; &lt;p&gt;We’re talking about building browser agents that can click buttons, fill forms, and even book flights, all by learning from trial and error instead of perfect demonstrations.&lt;/p&gt; &lt;p&gt;You’ll see how to build the complete training pipeline with GRPO, BrowserGym, and LFM2-350M, starting with a simple “click-test” task and scaling up from there.&lt;/p&gt; &lt;p&gt;Let me know if you have questions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://paulabartabajo.substack.com/p/fine-tuning-lfm2-350m-for-browser"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylu7n/finetuning_a_small_lm_for_browser_control_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylu7n/finetuning_a_small_lm_for_browser_control_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:07:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyqgi5</id>
    <title>Llama 3.2 3B fMRI (updated findings)</title>
    <updated>2025-12-29T16:19:13+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m building a local interpretability tool that lets me visualize hidden-state activity and &lt;strong&gt;intervene on individual hidden dimensions during inference&lt;/strong&gt; (via forward hooks). While scanning attn_out, I identified a persistent hidden dimension (dim 3039) that appeared repeatedly across prompts. I'll spare you all the Gradio screenshots, there are quite a few.&lt;/p&gt; &lt;p&gt;Initial probing suggested a loose “expressive vs constrained” effect, but that interpretation didn’t hold up under tighter controls. I then ran more systematic tests across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;multiple prompt types (social, procedural, factual, preference-based)&lt;/li&gt; &lt;li&gt;early / mid / late layers&lt;/li&gt; &lt;li&gt;both positive and negative intervention&lt;/li&gt; &lt;li&gt;long generations (1024 tokens)&lt;/li&gt; &lt;li&gt;repeated runs when results were ambiguous&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Across all of these conditions, &lt;strong&gt;the only stable, cross-prompt effect&lt;/strong&gt; was a change in the model’s &lt;em&gt;degree of commitment&lt;/em&gt; to its current generative trajectory.&lt;/p&gt; &lt;p&gt;Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Increasing intervention magnitude (regardless of sign) caused the model to respond &lt;strong&gt;more confidently and decisively&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;This did &lt;strong&gt;not&lt;/strong&gt; correlate with improved factual accuracy&lt;/li&gt; &lt;li&gt;In some cases (especially early-layer intervention), higher intervention increased &lt;strong&gt;confident hallucination&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Constrained procedural prompts (e.g. PB&amp;amp;J instructions) showed minimal variation, while open-ended prompts (e.g. greetings, blog-style responses) showed much larger stylistic and tonal shifts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The effect appears to modulate &lt;strong&gt;how strongly the model commits to whatever path it has already sampled&lt;/strong&gt;, rather than influencing &lt;em&gt;which&lt;/em&gt; path is chosen. This shows up as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;reduced hedging&lt;/li&gt; &lt;li&gt;increased assertiveness&lt;/li&gt; &lt;li&gt;stronger persistence of narrative frame&lt;/li&gt; &lt;li&gt;less self-correction once a trajectory is underway&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Importantly, this dimension does &lt;strong&gt;not&lt;/strong&gt; behave like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a semantic feature&lt;/li&gt; &lt;li&gt;an emotion representation&lt;/li&gt; &lt;li&gt;a creativity or verbosity knob&lt;/li&gt; &lt;li&gt;a factual reasoning mechanism&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A more accurate framing is that it functions as a &lt;strong&gt;global commitment / epistemic certainty gain&lt;/strong&gt;, influencing how readily the model doubles down on its internal state.&lt;/p&gt; &lt;p&gt;This also explains earlier inconsistencies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;early-layer interventions affect task framing (sometimes badly)&lt;/li&gt; &lt;li&gt;later-layer interventions affect delivery and tone&lt;/li&gt; &lt;li&gt;highly constrained tasks limit the observable effect&lt;/li&gt; &lt;li&gt;magnitude matters more than direction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At this stage, the claim is intentionally narrow.&lt;/p&gt; &lt;p&gt;Edit: adjusted punctuation.&lt;/p&gt; &lt;p&gt;Next steps (not yet done) include residual-stream analysis to see whether this feature accumulates across layers, and ablation tests to check whether removing it increases hedging and self-revision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyqgi5/llama_32_3b_fmri_updated_findings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyqgi5/llama_32_3b_fmri_updated_findings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyqgi5/llama_32_3b_fmri_updated_findings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T16:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyvqvn</id>
    <title>Whats about new Local LM apps and research platforms</title>
    <updated>2025-12-29T19:33:44+00:00</updated>
    <author>
      <name>/u/Safe-Clothes5925</name>
      <uri>https://old.reddit.com/user/Safe-Clothes5925</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys as you know, there are many ordinary applications aimed at end users, such as LM Studio, Sanctum, Anything, OpenUI, Kotaemon Biniou, etc.&lt;/p&gt; &lt;p&gt;But I'm looking for something a bit more complex and functional, like &amp;quot;transformerLAB&amp;quot;Kiln&amp;quot; or similar applications.&lt;/p&gt; &lt;p&gt;CLI or UI doesn't matter.&lt;/p&gt; &lt;p&gt;What new applications and repositories are you using these days? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Safe-Clothes5925"&gt; /u/Safe-Clothes5925 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyvqvn/whats_about_new_local_lm_apps_and_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyvqvn/whats_about_new_local_lm_apps_and_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyvqvn/whats_about_new_local_lm_apps_and_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T19:33:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjohv</id>
    <title>LM Studio alternative for images / Videos / Audio ?</title>
    <updated>2025-12-29T11:11:00+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With LM Studio (and others alike) it is super easy to run LLMs locally. Ist there anything as easy to create pictures, videos and audios locally using open models?&lt;/p&gt; &lt;p&gt;I tried ComfyUI but didn't find it as easy. With LM Studio I can search for models, see if they will run fast/good with my specs (M3 Pro, 36GB Unified) before downloading them, and in general it is super straight forward.&lt;/p&gt; &lt;p&gt;Two extra questions:&lt;br /&gt; 1. Which models would you recommend for this specs?&lt;br /&gt; 2. For LLMs in Mac, the mlx format makes a huge difference. Is there anything similar for image/video/audio models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pysmcf</id>
    <title>What's the best LLM for 96gb VRAM with vision</title>
    <updated>2025-12-29T17:39:34+00:00</updated>
    <author>
      <name>/u/LiteratureAcademic34</name>
      <uri>https://old.reddit.com/user/LiteratureAcademic34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've mostly been into the stable diffusion space, but I've been enjoying playing around with LLMs more often. I have access to an RTX Pro 6000 Blackwell and a Macbook Pro M4 Pro 24gb. I'm currently downloading Minimax m2.1 at IQ3_XXS for my 6000 Pro, but I want other options with vision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiteratureAcademic34"&gt; /u/LiteratureAcademic34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pysmcf/whats_the_best_llm_for_96gb_vram_with_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pysmcf/whats_the_best_llm_for_96gb_vram_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pysmcf/whats_the_best_llm_for_96gb_vram_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pydegt</id>
    <title>Benchmarking local llms for speed with CUDA and vulkan, found an unexpected speedup for select models</title>
    <updated>2025-12-29T05:09:41+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benchmarking my local llm collection to get an idea of tokens rates. I thought it might be interesting to compare CUDA vs Vulkan on my 3080 10GB. As expected, in almost all cases CUDA was the better option as far as token rate However, I found one suprise that affects a small number of models.&lt;/p&gt; &lt;p&gt;Disclaimer: take the following results with a pinch of salt. I'm not a statistician nor mathmetician. I have been programming for some decades but this test code is mostly deslopped jive code. YMMV.&lt;/p&gt; &lt;p&gt;The main findings is that when running certain models partially offloaded to GPU, some models perform much better on Vulkan than CUDA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM4 9B Q6 had a 2.2x speedup on PP, and 1.7x speedup on TG&lt;/li&gt; &lt;li&gt;Qwen3 8B Q6 had a 1.5x speedup on PP, and 1.1x speedup on PP (meh)&lt;/li&gt; &lt;li&gt;and Ministral3 14B 2512 Q4 had a 4.4x speedup on PP, and a 1.6x speedup on TG&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;edit: should add my setup: using latest llama.cpp build. Most ggufs are Unsloth UD. I primarily target models that can produce at least 20t/s. Ryzen 5 something or other, 32GB cheapest DDR4 RAM.&lt;/h2&gt; &lt;p&gt;The following tables only show models that are partially offloaded onto GPU:&lt;/p&gt; &lt;h3&gt;Token generation (tg) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;13.2&lt;/td&gt; &lt;td&gt;-12.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;25.4&lt;/td&gt; &lt;td&gt;44.0&lt;/td&gt; &lt;td&gt;+18.6&lt;/td&gt; &lt;td&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;40.4&lt;/td&gt; &lt;td&gt;21.6&lt;/td&gt; &lt;td&gt;-18.9&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;36.1&lt;/td&gt; &lt;td&gt;57.1&lt;/td&gt; &lt;td&gt;+21.0&lt;/td&gt; &lt;td&gt;1.58x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;23.1&lt;/td&gt; &lt;td&gt;15.9&lt;/td&gt; &lt;td&gt;-7.1&lt;/td&gt; &lt;td&gt;0.69x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;23.7&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;+2.1&lt;/td&gt; &lt;td&gt;1.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;104.3&lt;/td&gt; &lt;td&gt;61.4&lt;/td&gt; &lt;td&gt;-42.9&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;30.4&lt;/td&gt; &lt;td&gt;22.4&lt;/td&gt; &lt;td&gt;-8.0&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;16.4&lt;/td&gt; &lt;td&gt;12.9&lt;/td&gt; &lt;td&gt;-3.5&lt;/td&gt; &lt;td&gt;0.79x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;30.6&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-14.3&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;46.1&lt;/td&gt; &lt;td&gt;23.4&lt;/td&gt; &lt;td&gt;-22.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Prompt processing (pp) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;24.5&lt;/td&gt; &lt;td&gt;13.3&lt;/td&gt; &lt;td&gt;-11.2&lt;/td&gt; &lt;td&gt;0.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;34.0&lt;/td&gt; &lt;td&gt;75.6&lt;/td&gt; &lt;td&gt;+41.6&lt;/td&gt; &lt;td&gt;2.22x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;20.2&lt;/td&gt; &lt;td&gt;-16.8&lt;/td&gt; &lt;td&gt;0.55x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;58.1&lt;/td&gt; &lt;td&gt;255.4&lt;/td&gt; &lt;td&gt;+197.2&lt;/td&gt; &lt;td&gt;4.39x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;21.4&lt;/td&gt; &lt;td&gt;14.0&lt;/td&gt; &lt;td&gt;-7.3&lt;/td&gt; &lt;td&gt;0.66x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;30.3&lt;/td&gt; &lt;td&gt;46.0&lt;/td&gt; &lt;td&gt;+15.8&lt;/td&gt; &lt;td&gt;1.52x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;88.4&lt;/td&gt; &lt;td&gt;55.6&lt;/td&gt; &lt;td&gt;-32.8&lt;/td&gt; &lt;td&gt;0.63x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;28.2&lt;/td&gt; &lt;td&gt;20.9&lt;/td&gt; &lt;td&gt;-7.4&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;72.3&lt;/td&gt; &lt;td&gt;42.5&lt;/td&gt; &lt;td&gt;-29.8&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;29.1&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-12.8&lt;/td&gt; &lt;td&gt;0.56x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;221.9&lt;/td&gt; &lt;td&gt;112.1&lt;/td&gt; &lt;td&gt;-109.8&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T05:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyn4ny</id>
    <title>Kimi k2 thinking vs glm 4.7</title>
    <updated>2025-12-29T14:05:53+00:00</updated>
    <author>
      <name>/u/Worried_Goat_8604</name>
      <uri>https://old.reddit.com/user/Worried_Goat_8604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys for agentic coding using opencode , which ai model is better? - Kimi k2 thinking or glm 4.7? Its mainly python coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worried_Goat_8604"&gt; /u/Worried_Goat_8604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyn4ny/kimi_k2_thinking_vs_glm_47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyn4ny/kimi_k2_thinking_vs_glm_47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyn4ny/kimi_k2_thinking_vs_glm_47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T14:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pywgsb</id>
    <title>Best LLM Related Open Source Tools - 2025?</title>
    <updated>2025-12-29T20:00:49+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think 2025 is good year &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;LLM wise&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Now please share the tools you're using with LLMs. I know that half of us here involves with coding by using tools such as Cline, RooCode, KiloCode, QwenCode, MistralVibe, etc.,&lt;/p&gt; &lt;p&gt;Similarly some of us here involves with writing by using Finetuned Writing models. Of course we need tools for writing too. I came across Mikupad, Writingway2, Arrows(p-e-w), WritingTools(theJayTea)&lt;/p&gt; &lt;p&gt;Coding &amp;amp; Writing are just 2 categories I mentioned. Also I mentioned only few tools here(from my bookmarks) &amp;amp; Of course there are so many more other tools exist online which everyone yet to catch. I'm sure around 50 tools available for each category, lets bring those here.&lt;/p&gt; &lt;p&gt;So what other tools are you using? (Please mention category or concise use case)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Just mentioning some categories below to get quick &amp;amp; more replies&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Prompt, RAG, Brainstorm, AudioBook Maker, Ebook Maker, Second brain, Benchmarks, AI Assistant, Agents, Notebook, NoCode, Wiki, Storytelling/Worldbuilding, Image processing, Game creation,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Mentioned tools are from github only, I can share link if you need. The reason I didn't include links in this thread because sometime reddit filters remove threads automatically if multiple links present.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pywgsb/best_llm_related_open_source_tools_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T20:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pykkqx</id>
    <title>Looking back at end of 2024 vs now</title>
    <updated>2025-12-29T12:02:18+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been rebuilding a few agent systems recently, and I kept having this vague feeling that everything already feels outdated, even compared to the middle of this year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;br /&gt; GPT-4o → o3 → GPT-5.2&lt;br /&gt; Claude 3.5 → Claude 3.7 → Claude 4.5&lt;br /&gt; Gemini 1.5 → Gemini 2.5 → Gemini 3&lt;br /&gt; DeepSeek v2 → DeepSeek R1 → DeepSeek v3&lt;br /&gt; ...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent logic&lt;/strong&gt;&lt;br /&gt; single prompt loop → planner / executor split → long-running agent with state&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG / retrieval&lt;/strong&gt;&lt;br /&gt; top-k doc chunks → hybrid retrieve + rerank → implicit context reads&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;br /&gt; chat history only → session + long-term memory → stateful memory across runs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tool use&lt;/strong&gt;&lt;br /&gt; function calling JSON → structured tool execution → permissioned tool calls&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workflows&lt;/strong&gt;&lt;br /&gt; python scripts / cron → visual workflows (agent steps) → resumable execution engine&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observability&lt;/strong&gt;&lt;br /&gt; prompt logs → agent + tool traces → evals tied to deploys&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Protocols / integration&lt;/strong&gt;&lt;br /&gt; custom tool schema per app → MCP-style shared interface → standardized interface + security boundaries&lt;/p&gt; &lt;p&gt;Curious if others rebuilding systems recently feel the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T12:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyxwsh</id>
    <title>AI-Doomsday-Toolbox Distributed inference + workflows</title>
    <updated>2025-12-29T20:56:23+00:00</updated>
    <author>
      <name>/u/ManuXD32</name>
      <uri>https://old.reddit.com/user/ManuXD32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI Doomsday Toolbox v0.513 Update!&lt;/p&gt; &lt;p&gt;It took some major work but we now have&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed LLM Inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Run large models across multiple phones! Master-worker setup via llama.cpp Manually add workers + set RAM/layer proportions per device&lt;/p&gt; &lt;ul&gt; &lt;li&gt;New Workflows + templates for them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Transcribe + Summarize: Audio/video → Whisper transcription → LLM summary (with template saving!)&lt;/p&gt; &lt;p&gt;Txt2Img + Upscale: Generate + auto-upscale in one workflow Share audio/video directly to transcription workflow&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better Storage Management&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Models/ZIMs now used in-place (no copying!) - requires All Files Access permission Don't move files after importing or reimport them&lt;/p&gt; &lt;ul&gt; &lt;li&gt;UI Improvements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Manual input for all sliders (threads, context, temperature)&lt;/p&gt; &lt;p&gt;Redesigned image gallery with generation badges&lt;/p&gt; &lt;p&gt;Recordings linked in notes for easy playback&lt;/p&gt; &lt;p&gt;Separated RPC worker logs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bug Fixes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fixed ghost notifications after force-close&lt;/p&gt; &lt;p&gt;⚠️ Breaking change: Uninstall previous version first (database schema changed)&lt;/p&gt; &lt;p&gt;Repo &lt;a href="https://github.com/ManuXD32/AI-Doomsday-Toolbox"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback is appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManuXD32"&gt; /u/ManuXD32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyxwsh/aidoomsdaytoolbox_distributed_inference_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyxwsh/aidoomsdaytoolbox_distributed_inference_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyxwsh/aidoomsdaytoolbox_distributed_inference_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T20:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylstj</id>
    <title>Single RTX PRO 6000 - Minimax M2.1 (IQ2_M) speed</title>
    <updated>2025-12-29T13:05:12+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylstj/single_rtx_pro_6000_minimax_m21_iq2_m_speed/"&gt; &lt;img alt="Single RTX PRO 6000 - Minimax M2.1 (IQ2_M) speed" src="https://preview.redd.it/n0sxcy3q55ag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a383a6b2fe22a5ff97c3cee75cb4cdbaf506167c" title="Single RTX PRO 6000 - Minimax M2.1 (IQ2_M) speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&amp;quot;What's the speed?&amp;quot;. It depends.&lt;/h1&gt; &lt;p&gt;I run the model using &lt;code&gt;llama-server -m ~/models/unsloth/MiniMax-M2.1-GGUF/UD-IQ2_M/MiniMax-M2.1-UD-IQ2_M-00001-of-00002.gguf --jinja -ngl 99 -t 80 -c 160000 -fa 1 -ctv q8_0 -ctk q8_0 --host 0.0.0.0 --port 8080 -cram -1 --log-file ~/m2.1.log&lt;/code&gt;&lt;/p&gt; &lt;p&gt;KV quantized to Q8&lt;/p&gt; &lt;p&gt;160k max context&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total samples:&lt;/strong&gt; 107&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Date generated:&lt;/strong&gt; 2025-12-29 13:27&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Key Statistics&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Min&lt;/th&gt; &lt;th&gt;Max&lt;/th&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;th&gt;Median&lt;/th&gt; &lt;th&gt;Std Dev&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;prompt_eval_speed&lt;/td&gt; &lt;td&gt;23.09&lt;/td&gt; &lt;td&gt;1695.32&lt;/td&gt; &lt;td&gt;668.78&lt;/td&gt; &lt;td&gt;577.88&lt;/td&gt; &lt;td&gt;317.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;eval_speed&lt;/td&gt; &lt;td&gt;30.02&lt;/td&gt; &lt;td&gt;91.17&lt;/td&gt; &lt;td&gt;47.97&lt;/td&gt; &lt;td&gt;46.36&lt;/td&gt; &lt;td&gt;14.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Key Insights&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Highest prompt eval speed:&lt;/strong&gt; 1695.32 tokens/sec (n_tokens=15276)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lowest prompt eval speed:&lt;/strong&gt; 23.09 tokens/sec (n_tokens=67201)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Highest eval speed:&lt;/strong&gt; 91.17 tokens/sec (n_tokens=15276)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lowest eval speed:&lt;/strong&gt; 30.02 tokens/sec (n_tokens=92160)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;So bottom line, bigger context = lower speed (both PP &amp;amp; TG)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n0sxcy3q55ag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylstj/single_rtx_pro_6000_minimax_m21_iq2_m_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylstj/single_rtx_pro_6000_minimax_m21_iq2_m_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyao6g</id>
    <title>Meta released RPG, a research plan generation dataset on Hugging Face</title>
    <updated>2025-12-29T02:58:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt; &lt;img alt="Meta released RPG, a research plan generation dataset on Hugging Face" src="https://external-preview.redd.it/_Vt3gVwDJIJ3tdTBBf0E6Y1zVMQL8lOjQzN3Hnt2brY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=062e0684599eddb333c0833911a29ff674bc632c" title="Meta released RPG, a research plan generation dataset on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training AI co-scientists&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/facebook/research-plan-gen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T02:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyyp59</id>
    <title>What is the best way to allocated $15k right now for local LLMs?</title>
    <updated>2025-12-29T21:26:38+00:00</updated>
    <author>
      <name>/u/LargelyInnocuous</name>
      <uri>https://old.reddit.com/user/LargelyInnocuous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best bang for $15k right now? Would like to be able to run DeepSeek, Kimi K2 and GLM 4.5+. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LargelyInnocuous"&gt; /u/LargelyInnocuous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyyp59/what_is_the_best_way_to_allocated_15k_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T21:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyntxz</id>
    <title>BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language</title>
    <updated>2025-12-29T14:36:30+00:00</updated>
    <author>
      <name>/u/AgencyInside407</name>
      <uri>https://old.reddit.com/user/AgencyInside407</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"&gt; &lt;img alt="BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language" src="https://external-preview.redd.it/dWN4dmJ1ZmttNWFnMY4GET1d7wPCVUTfL2kwUQvVU9zlAAxjJ2PRs52epB4h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=604938b875bebc02ca972dc626bb9367cd1bc819" title="BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody! I hope all is well. I just wanted to share a project that I have been working on for the last several months called BULaMU-Dream. It is the first text to image model in the world that has been trained from scratch to respond to prompts in an African Language (Luganda). The details of how I trained it are &lt;a href="https://zenodo.org/records/18086776"&gt;here&lt;/a&gt; and a demo can be found &lt;a href="https://x.com/mwebazarick/status/2005643851655168146?s=12"&gt;here&lt;/a&gt;. I am open to any feedback that you are willing to share because I am going to continue working on improving BULaMU-Dream. I really believe that tiny conditional diffusion models like this can broaden access to multimodal AI tools by allowing people train and use these models on relatively inexpensive setups, like the M4 Mac Mini. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencyInside407"&gt; /u/AgencyInside407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ty3cvnfkm5ag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T14:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjjbw</id>
    <title>Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together</title>
    <updated>2025-12-29T11:02:29+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt; &lt;img alt="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" src="https://a.thumbs.redditmedia.com/bB4zUj7vleOqJdnXmwlZ9s4tewjkzGrf2kPk8Dbebv4.jpg" title="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HyperCLOVA X SEED 32B Think: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HyperCLOVA X SEED 8B Omni: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed"&gt;https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Artificial Analysis on 𝕏: &lt;a href="https://x.com/ArtificialAnlys/status/2005429176615174207"&gt;https://x.com/ArtificialAnlys/status/2005429176615174207&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyjjbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrjke</id>
    <title>Benchmarks for Quantized Models? (for users locally running Q8/Q6/Q2 precision)</title>
    <updated>2025-12-29T17:00:09+00:00</updated>
    <author>
      <name>/u/No-Grapefruit-1358</name>
      <uri>https://old.reddit.com/user/No-Grapefruit-1358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Many of us use the quantized Q8/Q6/Q2 model instead of fp16 for obvious reasons. Is there a collection of benchmarks which show SWE, HLE etc on Q8/Q6/Q2 quantized models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Grapefruit-1358"&gt; /u/No-Grapefruit-1358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrn9v</id>
    <title>I Finished a Fully Local Agentic RAG Tutorial</title>
    <updated>2025-12-29T17:03:44+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I’ve just finished a &lt;strong&gt;complete Agentic RAG tutorial + repository&lt;/strong&gt; that shows how to build a fully local, end-to-end system.&lt;/p&gt; &lt;p&gt;No APIs, no cloud, no hidden costs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;💡 What’s inside&lt;/h3&gt; &lt;p&gt;The tutorial covers the full pipeline, including the parts most examples skip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF → Markdown ingestion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hierarchical chunking (parent / child)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hybrid retrieval (dense + sparse)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Vector store with &lt;strong&gt;Qdrant&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Query rewriting + &lt;strong&gt;human-in-the-loop&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Context summarization&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent map-reduce&lt;/strong&gt; with &lt;strong&gt;LangGraph&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Local inference with &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Simple &lt;strong&gt;Gradio&lt;/strong&gt; UI&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;🎯 Who it’s for&lt;/h3&gt; &lt;p&gt;If you want to &lt;strong&gt;understand Agentic RAG by building it&lt;/strong&gt;, not just reading theory, this might help.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;🔗 Repo&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyg4yt</id>
    <title>Tencent just released WeDLM 8B Instruct on Hugging Face</title>
    <updated>2025-12-29T07:38:43+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt; &lt;img alt="Tencent just released WeDLM 8B Instruct on Hugging Face" src="https://b.thumbs.redditmedia.com/C56gntSOSvM_cfj95m0peqGLfh8p1Tnt02oONhKPwFM.jpg" title="Tencent just released WeDLM 8B Instruct on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/tencent/WeDLM-8B-Instruct"&gt;https://huggingface.co/tencent/WeDLM-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A diffusion language model that runs 3-6× faster than vLLM-optimized Qwen3-8B on math reasoning tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyg4yt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
