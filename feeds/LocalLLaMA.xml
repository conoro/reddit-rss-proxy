<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-19T10:07:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nkwl09</id>
    <title>Streaming TTS on google colab?</title>
    <updated>2025-09-19T06:57:20+00:00</updated>
    <author>
      <name>/u/Kiyumaa</name>
      <uri>https://old.reddit.com/user/Kiyumaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for a TTS that can work with a streaming text from a LLM, and also able to run on colab. I been looking for one but only saw stuff that only work on a laptop/pc and not colab, so i don't know if it even possible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kiyumaa"&gt; /u/Kiyumaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkwl09/streaming_tts_on_google_colab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkwl09/streaming_tts_on_google_colab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkwl09/streaming_tts_on_google_colab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T06:57:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkcbwp</id>
    <title>GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with "/think" command anywhere in the system prompt.</title>
    <updated>2025-09-18T16:03:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"&gt; &lt;img alt="GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." src="https://b.thumbs.redditmedia.com/-CO-B8aIjArUbhh-kSj2q3zmOt03NEOLl8XW32plQVQ.jpg" title="GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nkcbwp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nky2vh</id>
    <title>I made project called Local Agent personal artificial intelligence also known as LAPAI, i need some advice or what do you think about my project, because i still new on this thing, AI offline for support dev integrate AI to their project entirely offline</title>
    <updated>2025-09-19T08:33:57+00:00</updated>
    <author>
      <name>/u/Ambitious_Cry3080</name>
      <uri>https://old.reddit.com/user/Ambitious_Cry3080</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here i made AI engine that improve and enhance tiny model like 8B have ability to have memory and stuff like that, and work entirely offline the reason for this it's for support dev who want to integrate AI to their project without data go to cloud, entirely offline, but i still need some advice, because i am still new on this thing, and i just made it, detail on my GitHub: &lt;a href="https://github.com/NaosaikaDevelopment/LAPAI_Project_Experimental"&gt;Local Agent Personal Artificial Intelligence&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thank you for your time to see this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ambitious_Cry3080"&gt; /u/Ambitious_Cry3080 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nky2vh/i_made_project_called_local_agent_personal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nky2vh/i_made_project_called_local_agent_personal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nky2vh/i_made_project_called_local_agent_personal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T08:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkyqpy</id>
    <title>What are your most-wanted datasets?</title>
    <updated>2025-09-19T09:17:12+00:00</updated>
    <author>
      <name>/u/superbardibros</name>
      <uri>https://old.reddit.com/user/superbardibros</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have received a grant and would like to spend a portion of the funds on curating and releasing free and open source datasets on huggingface, what would you say are the modalities / types of datasets you would like to have readily available?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superbardibros"&gt; /u/superbardibros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkyqpy/what_are_your_mostwanted_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkyqpy/what_are_your_mostwanted_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkyqpy/what_are_your_mostwanted_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T09:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkud68</id>
    <title>NVIDIA + Intel collab means better models for us locally</title>
    <updated>2025-09-19T04:46:35+00:00</updated>
    <author>
      <name>/u/ChipCrafty4327</name>
      <uri>https://old.reddit.com/user/ChipCrafty4327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think this personal computing announcement directly implies they’re building unified memory similar to Apple devices&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/artificial-intelligence/intel-and-nvidia-to-jointly-develop-ai-infrastructure-and-personal-computing-products"&gt;https://newsroom.intel.com/artificial-intelligence/intel-and-nvidia-to-jointly-develop-ai-infrastructure-and-personal-computing-products&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChipCrafty4327"&gt; /u/ChipCrafty4327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkud68/nvidia_intel_collab_means_better_models_for_us/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkud68/nvidia_intel_collab_means_better_models_for_us/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkud68/nvidia_intel_collab_means_better_models_for_us/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T04:46:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkdnf2</id>
    <title>A dialogue where god tries (and fails) to prove to satan that humans can reason</title>
    <updated>2025-09-18T16:52:06+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"&gt; &lt;img alt="A dialogue where god tries (and fails) to prove to satan that humans can reason" src="https://preview.redd.it/fqm6nmw8dypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d353499d3cd5e9c56a4af4ceac147c01524c7fe1" title="A dialogue where god tries (and fails) to prove to satan that humans can reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.astralcodexten.com/p/what-is-man-that-thou-art-mindful"&gt;Full article here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fqm6nmw8dypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkxjch</id>
    <title>Use VLLM to guard your house</title>
    <updated>2025-09-19T07:58:35+00:00</updated>
    <author>
      <name>/u/LJ-Hao</name>
      <uri>https://old.reddit.com/user/LJ-Hao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkxjch/use_vllm_to_guard_your_house/"&gt; &lt;img alt="Use VLLM to guard your house" src="https://a.thumbs.redditmedia.com/3YihVtk6HvkVFBr6NtmujY-qpaXdUcJnJvwgyxQ1Eu4.jpg" title="Use VLLM to guard your house" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1bchncgwu2qf1.png?width=1804&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52f8336409d1f2f044b5a69c350025c287ba3a5"&gt;https://preview.redd.it/1bchncgwu2qf1.png?width=1804&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d52f8336409d1f2f044b5a69c350025c287ba3a5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello everyone, I've recently been using an Nvidia GPU to run Ollama and have built a project that leverages VLLM for real-time monitoring of my home.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LJ-Hao"&gt; /u/LJ-Hao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkxjch/use_vllm_to_guard_your_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkxjch/use_vllm_to_guard_your_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkxjch/use_vllm_to_guard_your_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T07:58:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkh88k</id>
    <title>Can you guess what model you're talking to in 5 prompts?</title>
    <updated>2025-09-18T19:05:38+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"&gt; &lt;img alt="Can you guess what model you're talking to in 5 prompts?" src="https://external-preview.redd.it/NTh4aG80cW8xenBmMfQ6ULqGkcZNtZeiwHOodBaY1uWCovO-Ocod72xeRKh_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb846ec47ccac331a0bed323c431ac8221cc38d8" title="Can you guess what model you're talking to in 5 prompts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a &lt;a href="https://whichllama.com"&gt;web version&lt;/a&gt; of the WhichLlama? bot in our Discord server (you should &lt;a href="https://discord.gg/bNQP7DcQ"&gt;join&lt;/a&gt;!) to share here. I think my own &amp;quot;LLM palate&amp;quot; isn't refined enough to tell models apart (drawing an analogy to coffee and wine tasting).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y7dajeso1zpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:05:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkzehc</id>
    <title>Today I'm competing against Apple on Product Hunt. I need some help, guys.</title>
    <updated>2025-09-19T09:58:39+00:00</updated>
    <author>
      <name>/u/Distinct_Criticism36</name>
      <uri>https://old.reddit.com/user/Distinct_Criticism36</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkzehc/today_im_competing_against_apple_on_product_hunt/"&gt; &lt;img alt="Today I'm competing against Apple on Product Hunt. I need some help, guys." src="https://preview.redd.it/p1rc3d7xg3qf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d1ac35831d083b3f0318b51bd18a0057e39179ab" title="Today I'm competing against Apple on Product Hunt. I need some help, guys." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two years ago I left my job( tesla ) to build SuperU - an AI voice agent platform. For this, i burned through savings, lived on ramen, coded in my apartment, the whole thing.&lt;/p&gt; &lt;p&gt;I launched on Product Hunt today and just found out Google and Apple are launching their products the same day. My heart literally sank when I saw the lineup.&lt;/p&gt; &lt;p&gt;Here's the thing - I know SuperU is good. Really good. We've got enterprise customers using our AI agents for thousands of calls, 200ms response time, works in 100+ languages. I built something that actually solves problems.&lt;/p&gt; &lt;p&gt;But I'm just one guy who's now competing against teams of hundreds with big dollar marketing budgets.&lt;/p&gt; &lt;p&gt;I've been grinding on this for 730 days straight. Learned to code APIs, figured out voice synthesis, built integrations with CRMs, handled customer support calls at 2am. Everything.&lt;/p&gt; &lt;p&gt;The worst part? I almost gave up three months ago when my savings hit zero. Had to move back with my parents. My college friends are buying houses while I'm explaining why I'm 30 and sleeping in my childhood bedroom.&lt;/p&gt; &lt;p&gt;But then this fintech company signed up and ran 10,000 calls through SuperU in their first month. Their CEO said it saved them more money than their entire software budget. That's when I knew this thing could actually work.&lt;/p&gt; &lt;p&gt;Today feels like everything I've worked for comes down to one day. If I can get some visibility on Product Hunt, maybe more companies will discover what I've built. If not... honestly, I don't know what happens next.&lt;/p&gt; &lt;p&gt;I'm not asking for charity or sympathy votes. But if you've got 30 seconds to check out SuperU on Product Hunt and think it's genuinely useful, a vote would mean everything to me.&lt;/p&gt; &lt;p&gt;Thanks for reading. Back to refreshing analytics and pretending I'm not terrified.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct_Criticism36"&gt; /u/Distinct_Criticism36 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p1rc3d7xg3qf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkzehc/today_im_competing_against_apple_on_product_hunt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkzehc/today_im_competing_against_apple_on_product_hunt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T09:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk706v</id>
    <title>Qwen Next is my new go to model</title>
    <updated>2025-09-18T12:32:30+00:00</updated>
    <author>
      <name>/u/Miserable-Dare5090</name>
      <uri>https://old.reddit.com/user/Miserable-Dare5090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is blazing fast, made 25 back to back tool calls with no errors, both as mxfp4 and qx86hi quants. I had been unable to test until now, and previously OSS-120B had become my main model due to speed/tool calling efficiency. Qwen delivered! &lt;/p&gt; &lt;p&gt;Have not tested coding, or RP (I am not interested in RP, my use is as a true assistant, running tasks). what are the issues that people have found? i prefer it to Qwen 235 which I can run at 6 bits atm. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable-Dare5090"&gt; /u/Miserable-Dare5090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T12:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkpohe</id>
    <title>I can can get GPUs as a tax write off. Thinking of doubling down on my LLM/ML learning adventure by buying one or two RTX 6000 pros.</title>
    <updated>2025-09-19T00:56:21+00:00</updated>
    <author>
      <name>/u/Tired__Dev</name>
      <uri>https://old.reddit.com/user/Tired__Dev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was having a lot of fun a few months back learning graph/vector based RAG. Then work unloaded a ridiculous level of work. I started by trying to use my ASUS M16 with a 4090 for local 3b models. It didn't work as I hoped. Now I'll probably sell the thing to build a local desktop rig that I can remotely use across the world (original reason I got the M16). &lt;/p&gt; &lt;p&gt;Reason I want it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Over the last two years I've taken it upon myself to start future proofing my career. I've learn IoT, game development, and now mostly LLMs. I want to also learn how to do things like object detection. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's a tax write off.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If I'm jobless I don't have to pay cloud costs and I have something I can liquidate if need be. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It would expand what I could do startup wise. &lt;strong&gt;(Most important reason)&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So my question is, what's the limit of one or two RTX 6000 Pro Blackwells? Would I be able to essentially do any RAG, Object detection, or ML like start up? What type of accuracy could I hope to accomplish with a good RAG pipeline and the open source models that'd be able to run on one or two of these GPUs? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tired__Dev"&gt; /u/Tired__Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkpohe/i_can_can_get_gpus_as_a_tax_write_off_thinking_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkpohe/i_can_can_get_gpus_as_a_tax_write_off_thinking_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkpohe/i_can_can_get_gpus_as_a_tax_write_off_thinking_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T00:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nknpgd</id>
    <title>System prompt to make a model help users guess its name?</title>
    <updated>2025-09-18T23:24:54+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nknpgd/system_prompt_to_make_a_model_help_users_guess/"&gt; &lt;img alt="System prompt to make a model help users guess its name?" src="https://preview.redd.it/shq50qtyb0qf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dcdc6bd12cac219bcc5a1cf9ba84a06b87fe4e3d" title="System prompt to make a model help users guess its name?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on this bot (you can find it in the &lt;a href="/r/LocalLLaMa"&gt;/r/LocalLLaMa&lt;/a&gt; Discord server) that plays a game asking users to guess which model it is. My system prompt asks the model to switch to riddles if the user directly asks for its identity, because that’s how some users may choose to play the game. But what I’m finding is that the riddles are often useless because the model doesn’t know its own identity (or it is intentionally lying).&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: I know asking directly for identity is a bad strategy, I just want to make it less bad for users who try it!&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Case in point, Mistral designing an elaborate riddle about itself being made by Google: &lt;a href="https://whichllama.com/?share=SMJXbCovucr8AVqy"&gt;https://whichllama.com/?share=SMJXbCovucr8AVqy&lt;/a&gt; (why?!)&lt;/p&gt; &lt;p&gt;Now, I can plug the true model name into the system prompt myself, but that is either ignored by the model or used in a way that makes it too easy to guess. Any tips on how I can design the system prompt to balance between being too easy and difficult?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/shq50qtyb0qf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nknpgd/system_prompt_to_make_a_model_help_users_guess/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nknpgd/system_prompt_to_make_a_model_help_users_guess/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T23:24:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkw8v6</id>
    <title>Favorite agentic coding llm up to 144GB of vram?</title>
    <updated>2025-09-19T06:35:44+00:00</updated>
    <author>
      <name>/u/Grouchy_Ad_4750</name>
      <uri>https://old.reddit.com/user/Grouchy_Ad_4750</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;br /&gt; in past weeks I've been evaluating agentic coding setups on server with 6x 24 GB gpus (5x 3090 + 1x 4090). &lt;/p&gt; &lt;p&gt;I'd like to have setup that will allow me to have inline completion (can be separate model) and agentic coder (crush, opencode, codex, ...). &lt;/p&gt; &lt;p&gt;Inline completion isn't really issue I use &lt;a href="https://github.com/milanglacier/minuet-ai.nvim"&gt;https://github.com/milanglacier/minuet-ai.nvim&lt;/a&gt; and it just queries openai chat endpoint so if it works it works (almost any model will work with it).&lt;/p&gt; &lt;p&gt;Main issue is agentic coding. So far only setup that worked for me reliably is gpt-oss-120b with llama.cpp on 4x 3090 + codex. I've also tried gpt-oss-120b on vllm but there are tool calling issues when streaming (which is shame since it allows for multiple requests at once).&lt;/p&gt; &lt;p&gt;I've also tried to evaluate (test cases and results here &lt;a href="https://github.com/hnatekmarorg/llm-eval/tree/main/output"&gt;https://github.com/hnatekmarorg/llm-eval/tree/main/output&lt;/a&gt; ) multiple models which are recommended here:&lt;/p&gt; &lt;p&gt;- qwen3-30b-* seems to exhibit tool calling issues both on vllm and llama.cpp but maybe I haven't found good client for it. Qwen3-30b-coder (in my tests its called qwen3-coder-plus since it worked with qwen client) seems ok but dumber (which is expected for 30b vs 60b model) than gpt-oss but it does create pretty frontend&lt;/p&gt; &lt;p&gt;- gpt-oss-120b seems good enough but if there is something better I can run I am all ears&lt;/p&gt; &lt;p&gt;- nemotron 49b is lot slower then gpt-oss-120b (expected since it isn't MoE) and for my use case doesn't seem better&lt;/p&gt; &lt;p&gt;- glm-4.5-air seems to be strong contender but I haven't had luck with any of the clients I could test&lt;/p&gt; &lt;p&gt;Rest aren't that interesting I've also tried lower quants of qwen3-235b (I believe it was Q3) and it didn't seem worth it based on speed and quality of response.&lt;/p&gt; &lt;p&gt;So if you have recommendations on how to improve my setup (gpt-oss-120b for agentic + some smaller faster model for inline completions) let me know.&lt;/p&gt; &lt;p&gt;Also I should mention that I haven't really had time to test these thing comprehensively so if I missed something obvious I apologize in advance &lt;/p&gt; &lt;p&gt;Also if that inline completion model could fit into 8GB of VRAM I can run it on my notebook... (maybe something like smaller qwen2.5-coder with limited context wouldn't be a worst idea in the world)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grouchy_Ad_4750"&gt; /u/Grouchy_Ad_4750 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkw8v6/favorite_agentic_coding_llm_up_to_144gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkw8v6/favorite_agentic_coding_llm_up_to_144gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkw8v6/favorite_agentic_coding_llm_up_to_144gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T06:35:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkycpq</id>
    <title>GPU power limiting measurements update</title>
    <updated>2025-09-19T08:51:47+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkycpq/gpu_power_limiting_measurements_update/"&gt; &lt;img alt="GPU power limiting measurements update" src="https://a.thumbs.redditmedia.com/h8FrrL9owCPm8g5aKNTjB9Zx8uCHz93-d210Nk-CGe8.jpg" title="GPU power limiting measurements update" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is an update to this thread: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1n89wi8/power_limit_your_gpus_to_reduce_electricity_costs/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In that thread I was recommended to use a special tool from Nvidia to log the actual energy usage: &lt;a href="https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html"&gt;https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I've run the test again and got some interesting results, for example the GPU consumes less power than the power limit set, the higher the limit the bigger the difference with the actual power draw. The VRAM clock does not change with the different power limits and always stays almost at its maximum value of 14001 MHz, but the GPU clock varies. And the most interesting chart is &amp;quot;minutes elapsed vs energy consumed&amp;quot; chart: the &lt;code&gt;llama-bench&lt;/code&gt; takes the same time to complete the task (process/generate 1024 tokens for 5 times), and the GPU just wastes more energy with the higher power limits. It appeared that I was wrong with the conclusion that 360W is the best power limit for PRO 6000: the actual best spot seems to be around 310W (the actual power draw should be around 290W).&lt;/p&gt; &lt;p&gt;Also people recommend to downvolt the GPU instead of power limiting it, for example see these threads:&lt;/p&gt; &lt;p&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1nhcf8t/successfully_tuning_5090s_for_low_heat_high_speed/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I did not run the proper tests yet but from the quick testing it seems that raising the power limit plus limiting the GPU clock MHz indeed works better than simply lowering the power limit. I will run a similar test with DCGM but limiting the clock instead of the power, and will report back later.&lt;/p&gt; &lt;p&gt;Here is the testing script I've made (slightly modified and not rechecked prior to posting to Reddit so I might have fucked it up, check the code before running it), has to be run as root.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/bash gpuname=' PRO 6000 '; # search the GPU id by this string startpower=150; # Watt endpower=600; # Watt increment=30; # Watt llama_bench='/path/to/bin/llama-bench'; model='/path/to/Qwen_Qwen3-32B-Q8_0.gguf'; n_prompt=1024; n_gen=1024; repetitions=5; filenamesuffix=$(date +%Y%m%d); check() { if [ &amp;quot;$?&amp;quot; -ne &amp;quot;0&amp;quot; ]; then echo 'something is wrong, exit'; exit 1; fi; } type nvidia-smi &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; if [ &amp;quot;$?&amp;quot; -ne &amp;quot;0&amp;quot; ]; then echo 'install nvidia-smi'; exit 1; fi; type dcgmi &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; if [ &amp;quot;$?&amp;quot; -ne &amp;quot;0&amp;quot; ]; then echo 'install datacenter-gpu-manager'; exit 1; fi; type awk &amp;gt;/dev/null 2&amp;gt;&amp;amp;1; if [ &amp;quot;$?&amp;quot; -ne &amp;quot;0&amp;quot; ]; then echo 'install gawk or mawk'; exit 1; fi; test -f &amp;quot;$llama_bench&amp;quot;; if [ &amp;quot;$?&amp;quot; -ne &amp;quot;0&amp;quot; ]; then echo 'error: llama-bench not found' &amp;amp;&amp;amp; exit 1; fi; test -f &amp;quot;$model&amp;quot;; if [ &amp;quot;$?&amp;quot; -ne &amp;quot;0&amp;quot; ]; then echo 'error: LLM model not found'; exit 1; fi; GPUnv=$(nvidia-smi --list-gpus | grep &amp;quot;$gpuname&amp;quot; | cut -d\ -f2 | sed 's/://'); # I hope these IDs won't be different but anything could happen LOL GPUdc=$(dcgmi discovery -l | grep &amp;quot;$gpuname&amp;quot; | head -n 1 | awk '{print $2}'); if [ &amp;quot;x$GPUnv&amp;quot; = &amp;quot;x&amp;quot; ] || [ &amp;quot;x$GPUdc&amp;quot; = &amp;quot;x&amp;quot; ]; then echo 'error getting GPU ID, check \$gpuname'; exit 1; fi; echo &amp;quot;###### nvidia-smi GPU id = $GPUnv; DCGM GPU id = $GPUdc&amp;quot;; iterations=$(expr $(expr $endpower - $startpower) / $increment); if [ &amp;quot;x$iterations&amp;quot; = &amp;quot;x&amp;quot; ]; then echo 'error calculating iterations, exit'; exit 1; fi; echo &amp;quot;###### resetting GPU clocks to default&amp;quot;; nvidia-smi -i $GPUnv --reset-gpu-clocks; check; nvidia-smi -i $GPUnv --reset-memory-clocks; check; echo &amp;quot;###### recording current power limit value&amp;quot;; oldlimit=$(nvidia-smi -i $GPUnv -q | grep 'Requested Power Limit' | head -n 1 | awk '{print $5}'); if [ &amp;quot;x$oldlimit&amp;quot; = &amp;quot;x&amp;quot; ]; then echo 'error saving old power limit'; exit 1; fi; echo &amp;quot;###### = $oldlimit W&amp;quot;; echo &amp;quot;###### creating DCGM group&amp;quot;; oldgroup=$(dcgmi group -l | grep -B1 powertest | head -n 1 | awk '{print $6}'); if [ &amp;quot;x$oldgroup&amp;quot; = &amp;quot;x&amp;quot; ]; then true; else dcgmi --delete $oldgroup; fi; dcgmi group -c powertest; check; group=$(dcgmi group -l | grep -B1 powertest | head -n 1 | awk '{print $6}'); dcgmi group -g $group -a $GPUdc; check; dcgmi stats -g $group -e -u 500 -m 43200; check; # enable stats monitoring, update interval 500 ms, keep stats for 12 hours for i in $(seq 0 $iterations); do echo &amp;quot;###### iteration $i&amp;quot;; powerlimit=$(expr $startpower + $(expr $i \* $increment)); echo &amp;quot;###### cooling GPU for 1 min...&amp;quot;; sleep 60; echo &amp;quot;###### flushing RAM for cold start&amp;quot;; echo 3 &amp;gt; /proc/sys/vm/drop_caches; echo 1 &amp;gt; /proc/sys/vm/compact_memory; echo &amp;quot;######################## setting power limit = $powerlimit ########################&amp;quot;; nvidia-smi --id=$GPUnv --power-limit=$powerlimit 2&amp;gt;&amp;amp;1 | grep -v 'persistence mode is disabled'; check; echo &amp;quot;###### start collecting stats&amp;quot;; dcgmi stats -g $group -s $powerlimit; check; echo &amp;quot;###### running llama-bench&amp;quot;; CUDA_VISIBLE_DEVICES=$GPUnv $llama_bench -fa 1 --n-prompt $n_prompt --n-gen $n_gen --repetitions $repetitions -m $model -o csv | tee &amp;quot;${filenamesuffix}_${powerlimit}_llamabench.txt&amp;quot;; echo &amp;quot;###### stop collecting stats&amp;quot;; dcgmi stats -g $group -x $powerlimit; check; echo &amp;quot;###### saving log: ${filenamesuffix}_${powerlimit}.log&amp;quot;; dcgmi stats -g $group -j $powerlimit -v &amp;gt; &amp;quot;${filenamesuffix}_${powerlimit}.log&amp;quot;; echo;echo;echo; done echo &amp;quot;###### test done, resetting power limit and removing DCGM stats&amp;quot;; nvidia-smi -i $GPUnv --power-limit=$oldlimit; dcgmi stats -g $group --jremoveall; dcgmi stats -g $group -d; dcgmi group -d $group; echo &amp;quot;###### finish, check ${filenamesuffix}_${powerlimit}*&amp;quot;; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nkycpq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkycpq/gpu_power_limiting_measurements_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkycpq/gpu_power_limiting_measurements_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T08:51:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkkghp</id>
    <title>Decart-AI releases “Open Source Nano Banana for Video”</title>
    <updated>2025-09-18T21:09:20+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt; &lt;img alt="Decart-AI releases “Open Source Nano Banana for Video”" src="https://preview.redd.it/eisyod0snzpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b42197a4268b3f80790f0506f12d7d6cfc5a4bb" title="Decart-AI releases “Open Source Nano Banana for Video”" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building “Open Source Nano Banana for Video” - here is open source demo v0.1&lt;/p&gt; &lt;p&gt;We are open sourcing Lucy Edit, the first foundation model for text-guided video editing!&lt;/p&gt; &lt;p&gt;Lucy Edit lets you prompt to try on uniforms or costumes - with motion, face, and identity staying perfectly preserved&lt;/p&gt; &lt;p&gt;Get the model on @huggingface 🤗, API on @FAL, and nodes on @ComfyUI 🧵&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/decartai/status/1968769793567207528?s=46"&gt;https://x.com/decartai/status/1968769793567207528?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;https://huggingface.co/decart-ai/Lucy-Edit-Dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lucy Edit Node on ComfyUI: &lt;a href="https://github.com/decartAI/lucy-edit-comfyui"&gt;https://github.com/decartAI/lucy-edit-comfyui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eisyod0snzpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T21:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkmc7z</id>
    <title>Moondream 3 (Preview) -- hybrid reasoning vision language model</title>
    <updated>2025-09-18T22:24:59+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkmc7z/moondream_3_preview_hybrid_reasoning_vision/"&gt; &lt;img alt="Moondream 3 (Preview) -- hybrid reasoning vision language model" src="https://external-preview.redd.it/4djziNvQ2zvOfJv3_xVajpCMtf-Z4Exi5Qyi8qcyMmc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=690d6b125016267d773d6fd42ebb4a21aff8aca7" title="Moondream 3 (Preview) -- hybrid reasoning vision language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/moondream/moondream3-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkmc7z/moondream_3_preview_hybrid_reasoning_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkmc7z/moondream_3_preview_hybrid_reasoning_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T22:24:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkfvrl</id>
    <title>Local LLM Coding Stack (24GB minimum, ideal 36GB)</title>
    <updated>2025-09-18T18:14:56+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"&gt; &lt;img alt="Local LLM Coding Stack (24GB minimum, ideal 36GB)" src="https://preview.redd.it/ia5muohupypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1148b1bf986af2a5825964a50e7d6bdf8dc5dc16" title="Local LLM Coding Stack (24GB minimum, ideal 36GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps this could be useful to someone trying to get his/her own local AI coding stack. I do scientific coding stuff, not web or application development related stuff, so the needs might be different. &lt;/p&gt; &lt;p&gt;Deployed on a 48gb Mac, but this should work on 32GB, and maybe even 24GB setups:&lt;/p&gt; &lt;p&gt;General Tasks, used 90% of the time: Cline on top of Qwen3Coder-30b-a3b. Served by LM Studio in MLX format for maximum speed. This is the backbone of everything else...&lt;/p&gt; &lt;p&gt;Difficult single script tasks, 5% of the time: QwenCode on top of GPT-OSS 20b (Reasoning effort: High). Served by LM Studio. This cannot be served at the same time of Qwen3Coder due to lack of RAM. The problem cracker. GPT-OSS can be swept with other reasoning models with tool use capabilities (Magistral, DeepSeek, ERNIE-thinking, EXAONE, etc... lot of options here)&lt;/p&gt; &lt;p&gt;Experimental, hand-made prototyping: Continue doing auto-complete work on top of Qwen2.5-Coder 7b. Served by Ollama to be always available together with the model served by LM Studio. When you need to be in the loop of creativity this is the one.&lt;/p&gt; &lt;p&gt;IDE for data exploration: Spyder&lt;/p&gt; &lt;p&gt;Long Live to Local LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ia5muohupypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk7jbi</id>
    <title>NVIDIA invests 5 billions $ into Intel</title>
    <updated>2025-09-18T12:56:04+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"&gt; &lt;img alt="NVIDIA invests 5 billions $ into Intel" src="https://external-preview.redd.it/n5kP5NRletQy7r254iQxj6sHk25NybFeHBeqLvZxjz8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ddf7ad07021e40c3004f38a19f49697e1cd4cc6" title="NVIDIA invests 5 billions $ into Intel" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bizarre news, so NVIDIA is like 99% of the market now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/09/18/intel-nvidia-investment.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T12:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkjpu3</id>
    <title>Model: Qwen3 Next Pull Request llama.cpp</title>
    <updated>2025-09-18T20:40:40+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt; &lt;img alt="Model: Qwen3 Next Pull Request llama.cpp" src="https://a.thumbs.redditmedia.com/_7Yokv2mOxidJMZ0PGNDJ03EhjWToQTSVgNpJKxH3R0.jpg" title="Model: Qwen3 Next Pull Request llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yy1fvsujizpf1.png?width=424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabb8a9874c6f2d8ac968673d1a4d84bf1f4eec0"&gt;https://preview.redd.it/yy1fvsujizpf1.png?width=424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabb8a9874c6f2d8ac968673d1a4d84bf1f4eec0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're fighting with you guys! Maximum support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T20:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkbrk1</id>
    <title>Local Suno just dropped</title>
    <updated>2025-09-18T15:42:25+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors"&gt;https://huggingface.co/fredconex/SongBloom-Safetensors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/fredconex/ComfyUI-SongBloom"&gt;https://github.com/fredconex/ComfyUI-SongBloom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;br /&gt; &lt;a href="https://files.catbox.moe/i0iple.flac"&gt;https://files.catbox.moe/i0iple.flac&lt;/a&gt;&lt;br /&gt; &lt;a href="https://files.catbox.moe/96i90x.flac"&gt;https://files.catbox.moe/96i90x.flac&lt;/a&gt;&lt;br /&gt; &lt;a href="https://files.catbox.moe/zot9nu.flac"&gt;https://files.catbox.moe/zot9nu.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a DPO trained one that just came out &lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors/blob/main/songbloom_full_150s_dpo.safetensors"&gt;https://huggingface.co/fredconex/SongBloom-Safetensors/blob/main/songbloom_full_150s_dpo.safetensors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using the DPO one this was feeding it the start of Metallica fade to black and some claude generated lyrics&lt;br /&gt; &lt;a href="https://files.catbox.moe/sopv2f.flac"&gt;https://files.catbox.moe/sopv2f.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was higher cfg / lower temp / another seed: &lt;a href="https://files.catbox.moe/olajtj.flac"&gt;https://files.catbox.moe/olajtj.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Crazy leap for local&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T15:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkr6op</id>
    <title>Qwen3-Next experience so far</title>
    <updated>2025-09-19T02:07:13+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using this model as my primary model and its safe to say , the benchmarks don't lie.&lt;/p&gt; &lt;p&gt;This model is amazing, i have been using a mix of GLM-4.5-Air, Gpt-oss-120b, llama 4 scout and llama 3.3 in comparison to it.&lt;/p&gt; &lt;p&gt;And its safe to say it beat them by a good margin , i used both the thinking and instruct versions for multiple use cases mostly coding, summarizing &amp;amp; writing , RAG and tool use .&lt;/p&gt; &lt;p&gt;I am curious about your experiences aswell. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkr6op/qwen3next_experience_so_far/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkr6op/qwen3next_experience_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkr6op/qwen3next_experience_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T02:07:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkwx12</id>
    <title>Everyone’s trying vectors and graphs for AI memory. We went back to SQL.</title>
    <updated>2025-09-19T07:17:52+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When we first started building with LLMs, the gap was obvious: they could reason well in the moment, but forgot everything as soon as the conversation moved on.&lt;/p&gt; &lt;p&gt;You could tell an agent, &lt;em&gt;“I don’t like coffee,”&lt;/em&gt; and three steps later it would suggest espresso again. It wasn’t broken logic, it was missing memory.&lt;/p&gt; &lt;p&gt;Over the past few years, people have tried a bunch of ways to fix it:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt stuffing / fine-tuning&lt;/strong&gt; – Keep prepending history. Works for short chats, but tokens and cost explode fast.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vector databases (RAG)&lt;/strong&gt; – Store embeddings in Pinecone/Weaviate. Recall is semantic, but retrieval is noisy and loses structure.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graph databases&lt;/strong&gt; – Build entity-relationship graphs. Great for reasoning, but hard to scale and maintain.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid systems&lt;/strong&gt; – Mix vectors, graphs, key-value, and relational DBs. Flexible but complex.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And then there’s the twist:&lt;br /&gt; &lt;strong&gt;Relational databases! Yes,&lt;/strong&gt; the tech that’s been running banks and social media for decades is looking like one of the most practical ways to give AI persistent memory.&lt;/p&gt; &lt;p&gt;Instead of exotic stores, you can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keep short-term vs long-term memory in SQL tables&lt;/li&gt; &lt;li&gt;Store entities, rules, and preferences as structured records&lt;/li&gt; &lt;li&gt;Promote important facts into permanent memory&lt;/li&gt; &lt;li&gt;Use joins and indexes for retrieval&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is the approach we’ve been working on at &lt;strong&gt;Gibson&lt;/strong&gt;. We built an open-source project called &lt;a href="https://github.com/gibsonai/memori"&gt;Memori&lt;/a&gt; , a &lt;strong&gt;multi-agent memory engine&lt;/strong&gt; that gives your AI agents human-like memory.&lt;/p&gt; &lt;p&gt;It’s kind of ironic, after all the hype around vectors and graphs, one of the best answers to AI memory might be the tech we’ve trusted for 50+ years.&lt;/p&gt; &lt;p&gt;I would love to know your thoughts about our approach!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkwx12/everyones_trying_vectors_and_graphs_for_ai_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkwx12/everyones_trying_vectors_and_graphs_for_ai_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkwx12/everyones_trying_vectors_and_graphs_for_ai_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T07:17:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nktfxl</id>
    <title>New Wan MoE video model</title>
    <updated>2025-09-19T03:57:51+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nktfxl/new_wan_moe_video_model/"&gt; &lt;img alt="New Wan MoE video model" src="https://external-preview.redd.it/TgMeHU4GJUa5aR0M3117isJqdoSEY-Q0uxO6S138yuw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6de68752b1ead1487008f27659ea654e42269c7e" title="New Wan MoE video model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wan AI just dropped this new MoE video diffusion model: Wan2.2-Animate-14B&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Wan-AI/Wan2.2-Animate-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nktfxl/new_wan_moe_video_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nktfxl/new_wan_moe_video_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T03:57:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkieo3</id>
    <title>PSA it costs authors $12,690 to make a Nature article Open Access</title>
    <updated>2025-09-18T19:50:42+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"&gt; &lt;img alt="PSA it costs authors $12,690 to make a Nature article Open Access" src="https://preview.redd.it/xkcal9zq9zpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07dcfaf4df77e0f86644296480de4064b0f6ca22" title="PSA it costs authors $12,690 to make a Nature article Open Access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And the DeepSeek folks paid up so we can read their work without hitting a paywall. Massive respect for absorbing the costs so the public benefits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xkcal9zq9zpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkvgn0</id>
    <title>Wow, Moondream 3 preview is goated</title>
    <updated>2025-09-19T05:48:41+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkvgn0/wow_moondream_3_preview_is_goated/"&gt; &lt;img alt="Wow, Moondream 3 preview is goated" src="https://preview.redd.it/nwfm02if82qf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebab95faf4918729235e9d66f345bf7bf80fbb91" title="Wow, Moondream 3 preview is goated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If the &amp;quot;preview&amp;quot; is this great, how great will the full model be?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nwfm02if82qf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkvgn0/wow_moondream_3_preview_is_goated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkvgn0/wow_moondream_3_preview_is_goated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T05:48:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building 🔨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio 👾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
