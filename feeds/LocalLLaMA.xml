<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-13T17:55:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qby242</id>
    <title>What would you do with ONE Dell Pro Max GB10 (DGX Spark class box) in a Copilot heavy org?</title>
    <updated>2026-01-13T17:54:08+00:00</updated>
    <author>
      <name>/u/nofuture09</name>
      <uri>https://old.reddit.com/user/nofuture09</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi&lt;/p&gt; &lt;p&gt;My company bought me a Dell Pro Max with GB10 (DGX Spark class). We are a few hundred employees and we already use Microsoft Copilot a lot because we have M365 Enterprise. So I am trying to be realistic about what a single on prem GPU box is actually good for, instead of building a toy.&lt;/p&gt; &lt;p&gt;What I want is simple: a useful, safe, low maintenance setup that complements Copilot.&lt;/p&gt; &lt;p&gt;My main questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Can I run a clean ‚Äúinternal ChatGPT‚Äù style UI (Open WebUI or similar) on this and point it at a local inference server?&lt;/li&gt; &lt;li&gt;What inference stack would you pick on this kind of system (Ollama vs vLLM vs SGLang vs TensorRT-LLM), if the goal is stability and easy ops?&lt;/li&gt; &lt;li&gt;&lt;p&gt;Which models are a good starting point for general chat on a single box? I was thinking Mistral or Qwen3, but I am open.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If I want the assistant to be current, is the right pattern ‚Äútool use with web search‚Äù (grounding) instead of trying to fine tune?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If I do that, what is the safest architecture so it can browse the web but still keep internal PDF Q&amp;amp;A fully local?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If the PDFs are internal/confidential, what are the non negotiable controls you would put in place? (network isolation, auth/SSO, RBAC, audit logs, encryption at rest, backups/retention)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any gotchas with web UIs like Open WebUI in enterprise settings? I saw there was at least one security issue around connecting to external model servers, so I want to avoid obvious footguns.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Constraints / goal - Only one box for now, not a cluster. - I want high ROI and low drama. Something I can demo internally and maybe expand later.&lt;/p&gt; &lt;p&gt;If you had this hardware what are the top 2 to 3 use cases you would prioritize and what would you avoid?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nofuture09"&gt; /u/nofuture09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qby242/what_would_you_do_with_one_dell_pro_max_gb10_dgx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qby242/what_would_you_do_with_one_dell_pro_max_gb10_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qby242/what_would_you_do_with_one_dell_pro_max_gb10_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T17:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbsukp</id>
    <title>Extracting technical docs with mixed content - what's working for you?</title>
    <updated>2026-01-13T14:32:20+00:00</updated>
    <author>
      <name>/u/missing-in-idleness</name>
      <uri>https://old.reddit.com/user/missing-in-idleness</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably asked multiple times with somewhat similar cases, but I have a little bit of complicated scenario here:&lt;/p&gt; &lt;p&gt;I have a couple hundred technical training documents, mostly pdf but also presentations or word etc.&lt;/p&gt; &lt;p&gt;Only text based ones are easy to convert into markdown but the ones in hybrid format like text+screenshots+arrows pointing at things+tables and such are a pain in my butt to extract. When I use text extract only I lose all of this information, when I use OCR like docling, markitdown etc. it captures the tables, formulas but screenshots are still missing.&lt;/p&gt; &lt;p&gt;I set some hand crafted benchmark to test some approaches and compare (think of table names, codes, etc) in terms of recall, precision like.&lt;/p&gt; &lt;p&gt;I am stuck between paddlepaddle, deepseek and maybe some api call to big models (ikr). What is the current sota for keeping the most of semantic relations while keeping the precision and recall to ground truth document these days? Any tips and tricks worked for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/missing-in-idleness"&gt; /u/missing-in-idleness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbsukp/extracting_technical_docs_with_mixed_content/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbsukp/extracting_technical_docs_with_mixed_content/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbsukp/extracting_technical_docs_with_mixed_content/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T14:32:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb1w7a</id>
    <title>How do people even afford these expensive graphic cards...?...</title>
    <updated>2026-01-12T17:53:33+00:00</updated>
    <author>
      <name>/u/boisheep</name>
      <uri>https://old.reddit.com/user/boisheep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought some used computer with a RTX 3090 so I could learn ML/LLM and I am already running slow, when running pytorch processes from scratch, it's good, but anything Diffusion/LLM explodes my rig.&lt;/p&gt; &lt;p&gt;Then I'd ponder about these larger cards, and they are like 10k.&lt;/p&gt; &lt;p&gt;Benefit of a larger card is that diffusion models just do not seem to go well with dual, they can split processes of each step but there is no true speed gain on the processing itself; as for Llama it can be done in dual with llama.ccp for example.&lt;/p&gt; &lt;p&gt;Another used 3090 would be 700 + new power supply, and I don't even know if I need another motherboard with these lanes be running at 8x; but then I get no benefit for diffusion processes that need to load in a single card (esp if using comfy).&lt;/p&gt; &lt;p&gt;My current objective is to make a game engine, and that means I've been coding internals; and I am frustrated that it seems I am making the RPG engine with most graphic cards requirement ever when it's just for visual novel; characters have their own coding, actual code, beyond text prompts; and the more characters in a location, the more inferences because they also need to use reasoning, and very complex reasoning; I've been optimizing hard, 70B quantized bare minimum, and my 3090 is catching smoke.&lt;/p&gt; &lt;p&gt;It's impressive how much better memory and awareness they gain by having an inner monologe and fake simulated feelings; but boy it is slow, and while at 1 to 1 with inner monologe off it seems usable, it gets slow and I have no parallelism. Meanwhile I read people here talking about GPUs that cost as much as a summer cottage.&lt;/p&gt; &lt;p&gt;Is there a hidden stash of cards or secret or people really put 10k into a freaking graphics card?... how does that make financial sense?...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/boisheep"&gt; /u/boisheep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb1w7a/how_do_people_even_afford_these_expensive_graphic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T17:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbtwp8</id>
    <title>Text summaries</title>
    <updated>2026-01-13T15:14:08+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What LLMs are good for text summaries at the moment?&lt;/p&gt; &lt;p&gt;Are there any good frameworks or github repos in this area?&lt;/p&gt; &lt;p&gt;Are there good techniques beyond hierarchical summary-of-summary or grounded-summarisation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbtwp8/text_summaries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbtwp8/text_summaries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbtwp8/text_summaries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T15:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb47fn</id>
    <title>Unsloth's GGUFs for GLM 4.7 REAP are up.</title>
    <updated>2026-01-12T19:15:08+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb47fn/unsloths_ggufs_for_glm_47_reap_are_up/"&gt; &lt;img alt="Unsloth's GGUFs for GLM 4.7 REAP are up." src="https://external-preview.redd.it/_K5KJ1U4NAv0qO7ekkQCUeqpCvaUcBCkDVL7JrcRGaU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce183f1754e4ead5ff49938f48c097acb4a0cf1d" title="Unsloth's GGUFs for GLM 4.7 REAP are up." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-REAP-218B-A32B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb47fn/unsloths_ggufs_for_glm_47_reap_are_up/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb47fn/unsloths_ggufs_for_glm_47_reap_are_up/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T19:15:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbuuxp</id>
    <title>Open-source JAX library for coupled oscillator networks and dynamical systems analysis</title>
    <updated>2026-01-13T15:50:13+00:00</updated>
    <author>
      <name>/u/samim23</name>
      <uri>https://old.reddit.com/user/samim23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just released OscNet - a framework for building neural networks based on oscillatory dynamics.&lt;/p&gt; &lt;p&gt;Focus is on classical dynamical systems (Kuramoto, FitzHugh-Nagumo, Stuart-Landau) as computational primitives, with tools for:&lt;/p&gt; &lt;p&gt;- Stability and bifurcation analysis&lt;br /&gt; - Floquet multipliers&lt;br /&gt; - Edge-of-chaos detection&lt;br /&gt; - Various coupling topologies (hierarchical, power-law)&lt;/p&gt; &lt;p&gt;Built on JAX/Equinox for differentiable training. &lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://samim.io/p/2026-01-07-oscnet/"&gt;https://samim.io/p/2026-01-07-oscnet/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/samim23/oscnet"&gt;https://github.com/samim23/oscnet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samim23"&gt; /u/samim23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuuxp/opensource_jax_library_for_coupled_oscillator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuuxp/opensource_jax_library_for_coupled_oscillator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbuuxp/opensource_jax_library_for_coupled_oscillator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T15:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbnebk</id>
    <title>500Mb Named Entity Recognition (NER) model to identify and classify entities in any text locally. Easily fine-tune on any language locally (see example for Spanish).</title>
    <updated>2026-01-13T09:56:10+00:00</updated>
    <author>
      <name>/u/Ok_Hold_5385</name>
      <uri>https://old.reddit.com/user/Ok_Hold_5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tanaos/tanaos-NER-v1"&gt;https://huggingface.co/tanaos/tanaos-NER-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A small (500Mb, 0.1B params) but efficient Named Entity Recognition (NER) model which &lt;strong&gt;identifies and classifies entities in text into predefined categories&lt;/strong&gt; (person, location, date, organization...) locally.&lt;/p&gt; &lt;h1&gt;Use-case&lt;/h1&gt; &lt;p&gt;You have unstructured text and you want to extract specific chunks of information from it, such as names, dates, products, organizations and so on, for further processing.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;quot;John landed in Barcelona at 15:45.&amp;quot; &amp;gt;&amp;gt;&amp;gt; [{'entity_group': 'PERSON', 'word': 'John', 'start': 0, 'end': 4}, {'entity_group': 'LOCATION', 'word': 'Barcelona', 'start': 15, 'end': 24}, {'entity_group': 'TIME', 'word': '15:45.', 'start': 28, 'end': 34}] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Fine-tune on custom domain or language without labeled data (no GPU needed)&lt;/h1&gt; &lt;p&gt;Do you want to tailor the model to your specific domain (medical, legal, engineering etc.) or to a different language? Use the &lt;a href="https://github.com/tanaos/artifex"&gt;Artifex library&lt;/a&gt; to fine-tune the model on CPU by generating synthetic training data on-the-fly.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from artifex import Artifex ner = Artifex().named_entity_recognition ner.train( domain=&amp;quot;documentos medico&amp;quot;, named_entities={ &amp;quot;PERSONA&amp;quot;: &amp;quot;Personas individuales, personajes ficticios&amp;quot;, &amp;quot;ORGANIZACION&amp;quot;: &amp;quot;Empresas, instituciones, agencias&amp;quot;, &amp;quot;UBICACION&amp;quot;: &amp;quot;√Åreas geogr√°ficas&amp;quot;, &amp;quot;FECHA&amp;quot;: &amp;quot;Fechas absolutas o relativas, incluidos a√±os, meses y/o d√≠as&amp;quot;, &amp;quot;HORA&amp;quot;: &amp;quot;Hora espec√≠fica del d√≠a&amp;quot;, &amp;quot;NUMERO&amp;quot;: &amp;quot;Mediciones o expresiones num√©ricas&amp;quot;, &amp;quot;OBRA_DE_ARTE&amp;quot;: &amp;quot;T√≠tulos de obras creativas&amp;quot;, &amp;quot;LENGUAJE&amp;quot;: &amp;quot;Lenguajes naturales o de programaci√≥n&amp;quot;, &amp;quot;GRUPO_NORP&amp;quot;: &amp;quot;Grupos nacionales, religiosos o pol√≠ticos&amp;quot;, &amp;quot;DIRECCION&amp;quot;: &amp;quot;Direcciones completas&amp;quot;, &amp;quot;NUMERO_DE_TELEFONO&amp;quot;: &amp;quot;N√∫meros de tel√©fono&amp;quot; }, language=&amp;quot;espa√±ol&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Don't want to self-host the model?&lt;/h1&gt; &lt;p&gt;If you don't want to self-host this model, and you'd rather use an API, you can use this model via the Small-Language-Model API. Try it for free directly on your browser: &lt;a href="https://slm.tanaos.com/docs"&gt;https://slm.tanaos.com/docs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Hold_5385"&gt; /u/Ok_Hold_5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbnebk/500mb_named_entity_recognition_ner_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbnebk/500mb_named_entity_recognition_ner_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbnebk/500mb_named_entity_recognition_ner_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T09:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qaz4je</id>
    <title>We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally</title>
    <updated>2026-01-12T16:14:57+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/"&gt; &lt;img alt="We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally" src="https://preview.redd.it/ed9sra1z0ycg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6721c4f7e645b322ae0b855c876d7721c4305e23" title="We fine-tuned a 4B Text2SQL model that matches a 685B teacher - query your CSV data in plain English, locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have been exploring how far you can push small models on narrow, well-defined tasks and decided to focus on &lt;strong&gt;Text2SQL&lt;/strong&gt;. We fine-tuned a small language model (&lt;strong&gt;4B parameters&lt;/strong&gt;) to convert plain English questions into executable SQL queries with accuracy matching a &lt;strong&gt;685B LLM (DeepSeek-V3)&lt;/strong&gt;. Because it's small, you can run it locally on your own machine, no API keys, no cloud dependencies. You can find more information on the &lt;a href="https://github.com/distil-labs/distil-text2sql"&gt;GitHub page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Just type: &lt;em&gt;&amp;quot;How many employees earn more than 50000?&amp;quot;&lt;/em&gt; ‚Üí you get: &lt;code&gt;*SELECT COUNT(*) FROM employees WHERE salary &amp;gt; 50000;*&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;How We Trained Text2SQL&lt;/h2&gt; &lt;p&gt;Asking questions about data shouldn't require knowing SQL. We wanted a local assistant that keeps your data private while matching cloud LLM quality. Small models are perfect for &lt;strong&gt;structured generation tasks&lt;/strong&gt; like SQL, so this became our next testbed after &lt;a href="https://github.com/distil-labs/distil-gitara"&gt;Gitara&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Our goals:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; (Ollama/llamacpp/transformers serve) - your data never leaves your machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fast responses&lt;/strong&gt; (&amp;lt;2 seconds on a laptop)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Match the accuracy of a 685B model&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Examples&lt;/h3&gt; &lt;p&gt;``` &amp;quot;How many employees are in each department?&amp;quot; ‚Üí SELECT department, COUNT(*) FROM employees GROUP BY department;&lt;/p&gt; &lt;p&gt;&amp;quot;What is the average salary by department?&amp;quot; ‚Üí SELECT department, AVG(salary) FROM employees GROUP BY department;&lt;/p&gt; &lt;p&gt;&amp;quot;Who are the top 3 highest paid employees?&amp;quot; ‚Üí SELECT name, salary FROM employees ORDER BY salary DESC LIMIT 3;&lt;/p&gt; &lt;p&gt;&amp;quot;Show total project budget per employee&amp;quot; (with JOINs) ‚Üí SELECT e.name, SUM(p.budget) FROM employees e JOIN projects p ON e.id = p.lead_id GROUP BY e.name;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h3&gt;Results&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;LLM-as-a-Judge&lt;/th&gt; &lt;th&gt;Exact Match&lt;/th&gt; &lt;th&gt;Model link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;DeepSeek-V3 (teacher)&lt;/td&gt; &lt;td&gt;685B&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;48%&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Qwen3-4B (fine-tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;4B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;80%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;60%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/collections/distil-labs/distil-qwen3-4b-text2sql"&gt;huggingface&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-4B (base)&lt;/td&gt; &lt;td&gt;4B&lt;/td&gt; &lt;td&gt;62%&lt;/td&gt; &lt;td&gt;16%&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Our fine-tuned &lt;strong&gt;4B model matches the 685B teacher&lt;/strong&gt; on semantic accuracy and actually &lt;strong&gt;exceeds it on exact match&lt;/strong&gt;. The quantized version also responds &lt;strong&gt;&amp;lt;2 seconds&lt;/strong&gt; on an M4 MacBook Pro.&lt;/p&gt; &lt;p&gt;The wrapper script in the &lt;a href="https://github.com/distil-labs/distil-text2sql"&gt;GitHub page&lt;/a&gt; loads your CSV files, generates SQL, &lt;strong&gt;executes it&lt;/strong&gt;, and returns the results.&lt;/p&gt; &lt;h3&gt;Training Pipeline&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;1. Seed Data:&lt;/strong&gt; We wrote ~50 examples covering simple queries, JOINs, aggregations, and subqueries. Available in &lt;code&gt;finetuning/data/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Synthetic Expansion:&lt;/strong&gt; Using our &lt;a href="https://www.distillabs.ai/blog/small-expert-agents-from-10-examples/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=text2sql"&gt;data synthesis pipeline&lt;/a&gt;, we expanded to &lt;strong&gt;~10,000 training examples&lt;/strong&gt; with diverse schemas across e-commerce, HR, healthcare, and other domains.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Fine-tuning:&lt;/strong&gt; We chose Qwen3-4B based on our &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning/?utm_source=github&amp;amp;utm_medium=referral&amp;amp;utm_campaign=text2sql"&gt;benchmarking of 12 small language models&lt;/a&gt;, which showed it offers the best balance of capability and efficiency for fine-tuning. Training config: 4 epochs, LORA fine-tuning on ~10k examples.&lt;/p&gt; &lt;h3&gt;Qualitative Examples&lt;/h3&gt; &lt;p&gt;We compare the base QWEN3-4B with the fine tuned version on a few cherry-picked examples to showcase the difference&lt;/p&gt; &lt;h3&gt;Example 1: Missing Aggregation Function&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Schema:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```sql CREATE TABLE employees ( id INTEGER PRIMARY KEY, name TEXT NOT NULL, team TEXT, base_salary INTEGER, bonus INTEGER );&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What is the total compensation (salary + bonus) per team?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prediction&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Reference&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT team, SUM(base_salary + bonus) FROM employees GROUP BY team;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT team, (base_salary + bonus) AS total_compensation FROM employees GROUP BY team;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tuned qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT team, SUM(base_salary + bonus) FROM employees GROUP BY team;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; The base model omitted the &lt;code&gt;SUM()&lt;/code&gt; aggregate function, returning only an arbitrary row's compensation per team rather than the total. The tuned model correctly applies the aggregation.&lt;/p&gt; &lt;h3&gt;Example 2: Syntax Error in CASE Expression&lt;/h3&gt; &lt;p&gt;&lt;strong&gt;Schema:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;```sql CREATE TABLE tasks ( id INTEGER PRIMARY KEY, project_id INTEGER, title TEXT, status TEXT, assigned_to INTEGER );&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question:&lt;/strong&gt; What percentage of tasks are completed?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Prediction&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Reference&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT (COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*)) FROM tasks;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Base qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT (COUNT(CASE WHEN status = 'completed' THEN 1 END. * 100.0) / COUNT(*)) AS percentage_completed FROM tasks;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Tuned qwen3-4b&lt;/td&gt; &lt;td&gt;&lt;code&gt;SELECT (COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*)) FROM tasks;&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Analysis:&lt;/strong&gt; The base model produced invalid SQL with a syntax error (&lt;code&gt;END.&lt;/code&gt; instead of &lt;code&gt;END&lt;/code&gt;), causing query execution to fail. The tuned model generates syntactically correct SQL matching the reference.&lt;/p&gt; &lt;h2&gt;Want to try it?&lt;/h2&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-text2sql"&gt;https://github.com/distil-labs/distil-text2sql&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick start (Ollama):&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Download model (~2.5GB quantized)&lt;/h1&gt; &lt;p&gt;huggingface-cli download distil-labs/distil-qwen3-4b-text2sql-gguf-4bit --local-dir distil-model cd distil-model ollama create distil-qwen3-4b-text2sql -f Modelfile cd ..&lt;/p&gt; &lt;h1&gt;Query your data&lt;/h1&gt; &lt;p&gt;python app.py --csv your_data.csv --question &amp;quot;How many rows have status = active?&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;h2&gt;Discussion&lt;/h2&gt; &lt;p&gt;Curious to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you querying local data today? SQL? Pandas? Something else?&lt;/li&gt; &lt;li&gt;Anyone else fine-tuning small models for structured output tasks?&lt;/li&gt; &lt;li&gt;What other &amp;quot;narrow but useful&amp;quot; tasks would benefit from a local SLM?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let us know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ed9sra1z0ycg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qaz4je/we_finetuned_a_4b_text2sql_model_that_matches_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T16:14:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbc5s5</id>
    <title>Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )</title>
    <updated>2026-01-13T00:16:46+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbc5s5/building_opensource_client_sided_code/"&gt; &lt;img alt="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" src="https://external-preview.redd.it/MnIyZmw3ODdkMGRnMege6VYazrCNvPvrU2GG8tcd-8T7OQo9iRCGUYxRaIOc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e84f149b907a0cefc7077c8391844ece6af1b00" title="Building Opensource client sided Code Intelligence Engine -- Potentially deeper than Deep wiki :-) ( Need suggestions and feedback )" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. Think of DeepWiki but with understanding of codebase relations like IMPORTS - CALLS -DEFINES -IMPLEMENTS- EXTENDS relations. &lt;/p&gt; &lt;p&gt;What all features would be useful, any integrations, cool ideas, etc?&lt;/p&gt; &lt;p&gt;site: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; (A ‚≠ê might help me convince my CTO to allot little time for this :-) )&lt;/p&gt; &lt;p&gt;Everything including the DB engine, embeddings model etc works inside your browser. &lt;/p&gt; &lt;p&gt;It combines Graph query capabilities with standard code context tools like semantic search, BM 25 index, etc. Due to graph it should be able to perform Blast radius detection of code changes, codebase audit etc reliably. &lt;/p&gt; &lt;p&gt;Working on exposing the browser tab through MCP so claude code / cursor, etc can use it for codebase audits, deep context of code connections etc preventing it from making breaking changes due to missed dependent functions.&lt;/p&gt; &lt;p&gt;Posted an earlier version of Gitnexus here, there has been a lot of improvement since then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/rmnzno77d0dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbc5s5/building_opensource_client_sided_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbc5s5/building_opensource_client_sided_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T00:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbsdm4</id>
    <title>Fine-tuning Qwen-3-VL for object coordinate detection</title>
    <updated>2026-01-13T14:13:31+00:00</updated>
    <author>
      <name>/u/Due_Veterinarian5820</name>
      <uri>https://old.reddit.com/user/Due_Veterinarian5820</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to fine-tune Qwen-3-VL-8B-Instruct for object keypoint detection, and I‚Äôm running into serious issues. Back in August, I managed to do something similar with Qwen-2.5-VL, and while it took some effort, it did work. One reliable signal back then was the loss behavior: If training started with a high loss (e.g., ~100+) and steadily decreased, things were working. If the loss started low, it almost always meant something was wrong with the setup or data formatting. With Qwen-3-VL, I can‚Äôt reproduce that behavior at all. The loss starts low and stays there, regardless of what I try. So far I‚Äôve: Tried Unsloth Followed the official Qwen-3-VL docs Experimented with different prompts / data formats Nothing seems to click, and it‚Äôs unclear whether fine-tuning is actually happening in a meaningful way. If anyone has successfully fine-tuned Qwen-3-VL for keypoints (or similar structured vision outputs), I‚Äôd really appreciate it if you could share: Training data format Prompt / supervision structure Code or repo Any gotchas specific to Qwen-3-VL At this point I‚Äôm wondering if I‚Äôm missing something fundamental about how Qwen-3-VL expects supervision compared to 2.5-VL. Thanks in advance üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Veterinarian5820"&gt; /u/Due_Veterinarian5820 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbsdm4/finetuning_qwen3vl_for_object_coordinate_detection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbsdm4/finetuning_qwen3vl_for_object_coordinate_detection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbsdm4/finetuning_qwen3vl_for_object_coordinate_detection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T14:13:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbei13</id>
    <title>Tool output compression for agents - 60-70% token reduction on tool-heavy workloads (open source, works with local models)</title>
    <updated>2026-01-13T01:57:59+00:00</updated>
    <author>
      <name>/u/decentralizedbee</name>
      <uri>https://old.reddit.com/user/decentralizedbee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Disclaimer: for those who are very anti-ads - yes this is a tool we built. Yes we built it due to a problem we have. Yes we are open-sourcing it and it's 100% free.&lt;/p&gt; &lt;p&gt;We build agents for clients. Coding assistants, data analysis tools, that kind of thing. A few months ago we noticed something that felt dumb in retrospect: the biggest cost driver wasn't the model itself - it was context size. And most of that context was tool outputs.&lt;/p&gt; &lt;p&gt;Think about what happens when an agent searches a codebase. Grep returns 500 file matches. The agent stuffs all 500 into context and asks the model &amp;quot;which of these are relevant?&amp;quot; You're paying for 500 items worth of tokens so the model can pick out maybe 5. The model is basically acting as a JSON filter at that point.&lt;/p&gt; &lt;p&gt;Same pattern everywhere. Search results, database queries, API responses. Tools return way more than the model actually needs, but agents just shove it all into the prompt because that's the path of least resistance.&lt;/p&gt; &lt;p&gt;So we started hacking on a compression layer. The idea was simple: before tool outputs hit the model, analyze them statistically and keep only what matters.&lt;/p&gt; &lt;p&gt;What we keep:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Anything with error keywords. Errors are never dropped, that would be insane.&lt;/li&gt; &lt;li&gt;Statistical outliers. If a numeric field has values more than 2 standard deviations from the mean, those items survive.&lt;/li&gt; &lt;li&gt;Items that match the user's query. We run BM25 scoring against the actual question being asked.&lt;/li&gt; &lt;li&gt;Top N by score if there's a relevance or score field in the data.&lt;/li&gt; &lt;li&gt;First few and last few items for context and recency.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What we drop:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The repetitive middle. If you have 500 search results and 480 of them look basically the same, you don't need all 480.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The tricky part wasn't the compression itself. It was knowing when NOT to compress. If you're searching a database for a specific user ID and every row is unique with no ranking signal, compression would lose entities. So we do a crushability analysis first. High uniqueness plus no importance signal means we skip compression entirely and pass through the original data.&lt;/p&gt; &lt;p&gt;On our workloads we're seeing 60-90% token reduction depending on the scenario. Code search with hundreds of file matches compresses aggressively. Log analysis with lots of repetitive entries compresses well. Database results with unique rows usually don't compress much, which is correct behavior.&lt;/p&gt; &lt;p&gt;Latency overhead is 1-5ms. The compression is fast, the model is still the bottleneck by a huge margin.&lt;/p&gt; &lt;p&gt;We open sourced it. It's called Headroom.&lt;/p&gt; &lt;p&gt;Two ways to run it. There's a proxy server you can point any OpenAI-compatible client at, or a Python SDK wrapper if you want more control. Works with OpenAI, Anthropic, Google, and local models through LiteLLM. If you're running llama.cpp with an OpenAI-compatible server, you can just point the proxy at that and it works.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/chopratejas/headroom"&gt;https://github.com/chopratejas/headroom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The compression is also reversible. We cache original content with a TTL and inject a retrieval marker into the compressed output. If the model needs data that was compressed away, it can request it back. Haven't needed this much in practice but it's a nice safety net.&lt;/p&gt; &lt;p&gt;Curious what others are doing for context management. Most agent frameworks seem to just truncate blindly which always felt wrong to us. You're either losing information randomly or you're paying for tokens you don't need. There should be a middle ground.&lt;/p&gt; &lt;p&gt;Would also love any feedback to this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decentralizedbee"&gt; /u/decentralizedbee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbei13/tool_output_compression_for_agents_6070_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbei13/tool_output_compression_for_agents_6070_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbei13/tool_output_compression_for_agents_6070_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T01:57:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbm62l</id>
    <title>Has anyone tried the single-socket 9175F with full 12 channels?</title>
    <updated>2026-01-13T08:37:14+00:00</updated>
    <author>
      <name>/u/Infinite100p</name>
      <uri>https://old.reddit.com/user/Infinite100p</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's the cheapest Epyc 9005 SKU that has close to the platform's full 600 Gbs memory bandwidth (when all 12 channels are populated).&lt;/p&gt; &lt;p&gt;Has anyone tried it with:&lt;br /&gt; - CPU inference?&lt;br /&gt; - In combination with a dGPU, and offloading layers to 600Gbs RAM?&lt;/p&gt; &lt;p&gt;In theory it should be amazing, but I am curious about concrete benchmarks, and all I'm able to find is &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1h4j45s/epyc_server_gpu_less/"&gt;theoretical discussions&lt;/a&gt; and this older &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1iyztni/comment/mib3rxq/"&gt;benchmark here&lt;/a&gt; that is suspiciously low perf:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Meta-Llama-3.1-70B-Instruct-Q8_0.gguf pp512 | 115.05 t/s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I get faster pp on a 128GB M3Max, and it's supposedly lower bandwidth (400 Gbs?).&lt;/p&gt; &lt;p&gt;The are also&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1izu62f/comment/mf9lzu2/"&gt; concerns of software optimization issues despite the near-full bandwidth&lt;/a&gt; of 9175F, but this is also kinda old discussion.&lt;/p&gt; &lt;p&gt;So, I am curious if any lucky owners of 9175F with full 12 slots of high rank planks could share some benchmark data points.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Infinite100p"&gt; /u/Infinite100p &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm62l/has_anyone_tried_the_singlesocket_9175f_with_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm62l/has_anyone_tried_the_singlesocket_9175f_with_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm62l/has_anyone_tried_the_singlesocket_9175f_with_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T08:37:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbtn6j</id>
    <title>[CPU] I'm looking for the best model for a CPU.</title>
    <updated>2026-01-13T15:03:57+00:00</updated>
    <author>
      <name>/u/lordfervi</name>
      <uri>https://old.reddit.com/user/lordfervi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello.&lt;/p&gt; &lt;p&gt;Basically, I have a problem :D&lt;/p&gt; &lt;p&gt;I work for a company that potentially wants AI (we'll see if it's realistic). I asked for an AMD Halo Strix machine, but the company prefers to save money (because it does). Instead, I got a server with two 10-core processors (20 threads) ‚Äì a total of 40 threads and over 700GB of RAM, and that's with virtualization...&lt;/p&gt; &lt;p&gt;I want to find an AI model that is as intelligent as possible, but also fast.&lt;/p&gt; &lt;p&gt;I've tested many models (and I'm happy to check out the ones you recommend).&lt;/p&gt; &lt;p&gt;I think GPT-OSS 120B works quite well, generating 7 tokens per second (approximately).&lt;/p&gt; &lt;p&gt;Gemma 3n E4B generates faster, at over 11, but looking at the number of parameters, I suspect it will be significantly weaker.&lt;/p&gt; &lt;p&gt;I was wondering if any of you have tested different models and can recommend one. I tried various ones, even as large as the Mistral Large 3, but it worked at 1 token per second, and of course there are applications where such AI can run on the CPU, e.g., XD automation. But I would like a model that is quite good in terms of performance and quality, which could be offered as a proof-of-concept in applications (maybe this will allow me to raise funds for better machines...).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lordfervi"&gt; /u/lordfervi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbtn6j/cpu_im_looking_for_the_best_model_for_a_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbtn6j/cpu_im_looking_for_the_best_model_for_a_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbtn6j/cpu_im_looking_for_the_best_model_for_a_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T15:03:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qb034t</id>
    <title>GitHub - deepseek-ai/Engram: Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models</title>
    <updated>2026-01-12T16:49:22+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/deepseek-ai/Engram/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qb034t/github_deepseekaiengram_conditional_memory_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-12T16:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbgdu2</id>
    <title>OSS Alternative to Glean</title>
    <updated>2026-01-13T03:21:07+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"&gt; &lt;img alt="OSS Alternative to Glean" src="https://external-preview.redd.it/cmU5Y2xuYnFiMWRnMWWIQZ2CyIf_Xrmm-Z03F9XkK4MxpC4ND6bEYAzhiTDs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2103787259340396f1676074d239180e73672b" title="OSS Alternative to Glean" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be OSS alternative to NotebookLM, Perplexity, and Glean.&lt;/p&gt; &lt;p&gt;In short, Connect any LLM to your internal knowledge sources (Search Engines, Drive, Calendar, Notion and 15+ other connectors) and chat with it in real time alongside your team.&lt;/p&gt; &lt;p&gt;I'm looking for contributors. If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here's a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep Agentic Agent&lt;/li&gt; &lt;li&gt;RBAC (Role Based Access for Teams)&lt;/li&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;li&gt;Local TTS/STT support.&lt;/li&gt; &lt;li&gt;Connects with 15+ external sources such as Search Engines, Slack, Notion, Gmail, Notion, Confluence etc&lt;/li&gt; &lt;li&gt;Cross-Browser Extension to let you save any dynamic webpage you want, including authenticated content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Upcoming Planned Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi Collaborative Chats&lt;/li&gt; &lt;li&gt;Multi Collaborative Documents&lt;/li&gt; &lt;li&gt;Real Time Features&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quick Start (without oauth connectors)&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Linux/macOS:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 \ -v surfsense-data:/data \ --name surfsense \ --restart unless-stopped \ ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Windows (PowerShell):&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d -p 3000:3000 -p 8000:8000 ` -v surfsense-data:/data ` --name surfsense ` --restart unless-stopped ` ghcr.io/modsetter/surfsense:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y63zrbbqb1dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbgdu2/oss_alternative_to_glean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T03:21:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbm7f4</id>
    <title>Gemma 3 1B qat q4_0 gguf without imatrix and (hopefully) correct metadata</title>
    <updated>2026-01-13T08:39:42+00:00</updated>
    <author>
      <name>/u/Big-Tune-190</name>
      <uri>https://old.reddit.com/user/Big-Tune-190</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm7f4/gemma_3_1b_qat_q4_0_gguf_without_imatrix_and/"&gt; &lt;img alt="Gemma 3 1B qat q4_0 gguf without imatrix and (hopefully) correct metadata" src="https://external-preview.redd.it/FWZ9aA8J9mRVVh8qfDpYOFQpE66ZT01z7wbVnO9_J7w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb710747a9555867465a936ab4df4e692fa30d18" title="Gemma 3 1B qat q4_0 gguf without imatrix and (hopefully) correct metadata" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since this is my very first post here, I would like to apologize in advance if I make any content-related or semantic errors in creating this post (or if it might be irrelevant) and I am grateful for constructive feedback.&lt;/p&gt; &lt;p&gt;TL;DR; (model card)&lt;/p&gt; &lt;p&gt;&lt;code&gt;Q4_0&lt;/code&gt; quantized version of &lt;code&gt;google/gemma-3-1b-it-qat-q4_0-unquantized&lt;/code&gt;, which differs from existing quantizations in the following aspects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;smaller and therefore faster than the original &lt;code&gt;google/gemma-3-1b-it-qat-q4_0-gguf&lt;/code&gt;&lt;/li&gt; &lt;li&gt;quantization without imatrix to avoid interactions with already QAT optimized Q4_0 weights&lt;/li&gt; &lt;li&gt;various fixes regarding model metadata &lt;ul&gt; &lt;li&gt;added &lt;code&gt;tokenizer.ggml.eot_token_id = 106&lt;/code&gt; (&lt;code&gt;&amp;lt;end_of_turn&amp;gt;&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;make &lt;code&gt;&amp;lt;start_of_image&amp;gt;&lt;/code&gt; type &lt;code&gt;CONTROL&lt;/code&gt;&lt;/li&gt; &lt;li&gt;make &lt;code&gt;&amp;lt;end_of_image&amp;gt;&lt;/code&gt; type &lt;code&gt;CONTROL&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created with &lt;code&gt;llama.cpp&lt;/code&gt; &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; release &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7699"&gt;b7699&lt;/a&gt; based on &lt;a href="https://huggingface.co/google/gemma-3-1b-it-qat-q4_0-unquantized/tree/a6692c1945954f4aa39a17b8dfba4a7e62db3d4f"&gt;google/gemma-3-1b-it-qat-q4_0-unquantized@a6692c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inspired by ideas and discussions around &lt;a href="https://huggingface.co/stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small"&gt;stduhpf/google-gemma-3-1b-it-qat-q4_0-gguf-small&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some more context (why this might be important for others)&lt;/p&gt; &lt;p&gt;I just wanted to briefly inform you that I have provided a new GGUF quantization for the &lt;code&gt;qat-q4_0&lt;/code&gt; snapshot of &lt;code&gt;gemma-3-1b-it&lt;/code&gt;. The reason for this was that I had not found a ready-made GGUF quantization for &lt;code&gt;google/gemma-3-1b-it-qat-q4_0&lt;/code&gt;that was quantized both with correct metadata on one hand and without the use of an imatrix on the other.&lt;/p&gt; &lt;p&gt;Regarding metadata, there has often been an issue in the past with QAT versions of Gemma 3 GGUF where the &lt;code&gt;&amp;lt;end_of_turn&amp;gt;&lt;/code&gt; token was not set in the model metadata, with only &lt;code&gt;&amp;lt;eos&amp;gt;&lt;/code&gt; appearing there instead. There are also quantizations that incorrectly declare certain tokens as &lt;code&gt;USER_DEFINED&lt;/code&gt;, even though they are probably &lt;code&gt;CONTROL&lt;/code&gt; tokens (like &lt;code&gt;&amp;lt;start_of_image&amp;gt;&lt;/code&gt;,&lt;code&gt;&amp;lt;end_of_image&amp;gt;&lt;/code&gt;).&lt;/p&gt; &lt;p&gt;Furthermore, it is questionable whether using an importance matrix (imatrix) during the quantization of a QAT snapshot is truly helpful or if it might even have a negative effect. For this reason, I wanted to create a quantization that explicitly functions without the use of an imatrix.&lt;/p&gt; &lt;p&gt;In summary, this is a GGUF Q4_0 quantization of &lt;code&gt;google/gemma-3-1b-it-qat-q4_0-unquantized&lt;/code&gt; without the use of an imatrix and with corrected metadata.&lt;/p&gt; &lt;p&gt;Since I searched for such a version for a long time myself and ultimately decided to create it on my own, I thought this might also be helpful for others, especially since, in my opinion, the very small 1B variant of Gemma 3 is somehow sensitive when it comes to quantization and metadata.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Big-Tune-190"&gt; /u/Big-Tune-190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/msievers/gemma-3-1b-it-qat-q4_0-gguf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm7f4/gemma_3_1b_qat_q4_0_gguf_without_imatrix_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbm7f4/gemma_3_1b_qat_q4_0_gguf_without_imatrix_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T08:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbqazx</id>
    <title>MCP, A2A, ACP, UCP - are we sleepwalking into another "standards" war controlled by the same companies?</title>
    <updated>2026-01-13T12:42:06+00:00</updated>
    <author>
      <name>/u/PutPurple844</name>
      <uri>https://old.reddit.com/user/PutPurple844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic has MCP. Google has A2A. OpenAI has ACP. Google just dropped UCP for commerce.&lt;/p&gt; &lt;p&gt;They're all &amp;quot;open&amp;quot;, but let's be real - the specs are written by the big labs.&lt;/p&gt; &lt;p&gt;Linux Foundation launched AAIF to govern all of this. Founding members? Anthropic, OpenAI, Google, Microsoft. The same players.&lt;/p&gt; &lt;p&gt;MCP is probably the most useful one for local setups - tool connections work regardless of what model you're running. But A2A and the commerce protocols assume you're hitting hosted APIs.&lt;/p&gt; &lt;p&gt;Anyone here running MCP servers with local models? Curious how the auth story works when there's no cloud identity provider in the loop.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PutPurple844"&gt; /u/PutPurple844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqazx/mcp_a2a_acp_ucp_are_we_sleepwalking_into_another/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqazx/mcp_a2a_acp_ucp_are_we_sleepwalking_into_another/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqazx/mcp_a2a_acp_ucp_are_we_sleepwalking_into_another/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:42:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbmtuw</id>
    <title>Best LLM model for 128GB of VRAM?</title>
    <updated>2026-01-13T09:19:32+00:00</updated>
    <author>
      <name>/u/Professional-Yak4359</name>
      <uri>https://old.reddit.com/user/Professional-Yak4359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My work requires the LLM to read tons of technical documents at a time and to provide insights (50 pages typically). I have a system of 8 x 5070 Ti running vllm (I need the prompt processing speed with at least 64k or 128k context). Right now I am running qwen3-32b and gptoss:120b but I am wondering if there are better choices than these two. &lt;/p&gt; &lt;p&gt;Any suggestion would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional-Yak4359"&gt; /u/Professional-Yak4359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbmtuw/best_llm_model_for_128gb_of_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbmtuw/best_llm_model_for_128gb_of_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbmtuw/best_llm_model_for_128gb_of_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T09:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbp52n</id>
    <title>FrogBoss 32B and FrogMini 14B from Microsoft</title>
    <updated>2026-01-13T11:40:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"&gt; &lt;img alt="FrogBoss 32B and FrogMini 14B from Microsoft" src="https://b.thumbs.redditmedia.com/gLOM6tp4n9cVVk2b-NQNYqlkyiPlEXip_bGZfpns1mI.jpg" title="FrogBoss 32B and FrogMini 14B from Microsoft" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;FrogBoss is a 32B-parameter coding agent specialized in fixing bugs in code. FrogBoss was obtained by fine‚Äëtuning a Qwen3‚Äë32B language model on debugging trajectories generated by Claude Sonnet 4 within the &lt;a href="https://aka.ms/bug-pilot"&gt;BugPilot framework&lt;/a&gt;. The training data combines real‚Äëworld bugs from R2E‚ÄëGym, synthetic bugs from SWE‚ÄëSmith, and novel ‚ÄúFeatAdd‚Äù bugs.&lt;/p&gt; &lt;p&gt;FrogMini is a 14B-parameter coding agent specialized in fixing bugs in code. FrogMini was obtained by fine‚Äëtuning a Qwen3‚Äë14B language model on debugging trajectories generated by Claude Sonnet 4 within the &lt;a href="https://aka.ms/bug-pilot"&gt;BugPilot framework&lt;/a&gt;. The training data combines real‚Äëworld bugs from R2E‚ÄëGym, synthetic bugs from SWE‚ÄëSmith, and novel ‚ÄúFeatAdd‚Äù bugs.&lt;/p&gt; &lt;p&gt;context length 64k&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/FrogBoss-32B-2510"&gt;https://huggingface.co/microsoft/FrogBoss-32B-2510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/microsoft/FrogMini-14B-2510"&gt;https://huggingface.co/microsoft/FrogMini-14B-2510&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1woo8ui5t3dg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=687cb5972b02c2afc6a4f83217f1ad6a24c3b81f"&gt;https://preview.redd.it/1woo8ui5t3dg1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=687cb5972b02c2afc6a4f83217f1ad6a24c3b81f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbp52n/frogboss_32b_and_frogmini_14b_from_microsoft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T11:40:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbjbrf</id>
    <title>baichuan-inc/Baichuan-M3-235B ¬∑ Hugging Face</title>
    <updated>2026-01-13T05:46:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/"&gt; &lt;img alt="baichuan-inc/Baichuan-M3-235B ¬∑ Hugging Face" src="https://external-preview.redd.it/-zZCICdRLYRGcsTSpa_79bNF1i9sBC7auVQLA_P7iG8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=16acadd715d2f48039128191cb574c9204d186a6" title="baichuan-inc/Baichuan-M3-235B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M3-235B#%F0%9F%8C%9F-model-overview"&gt;&lt;/a&gt;üåü Model Overview&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Baichuan-M3&lt;/strong&gt; is Baichuan AI's new-generation medical-enhanced large language model, a major milestone following &lt;a href="https://github.com/baichuan-inc/Baichuan-M2-32B"&gt;Baichuan-M2&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;In contrast to prior approaches that primarily focus on static question answering or superficial role-playing, Baichuan-M3 is trained to explicitly model the &lt;strong&gt;clinical decision-making process&lt;/strong&gt;, aiming to improve usability and reliability in real-world medical practice. Rather than merely producing &amp;quot;plausible-sounding answers&amp;quot; or high-frequency vague recommendations like &amp;quot;you should see a doctor soon,&amp;quot; the model is trained to &lt;strong&gt;proactively acquire critical clinical information&lt;/strong&gt;, &lt;strong&gt;construct coherent medical reasoning pathways&lt;/strong&gt;, and &lt;strong&gt;systematically constrain hallucination-prone behaviors&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M3-235B#core-highlights"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Core Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üèÜ &lt;strong&gt;Surpasses GPT-5.2&lt;/strong&gt;: Outperforms OpenAI's latest model across HealthBench, HealthBench-Hard, hallucination evaluation, and BCOSCE, establishing a new SOTA in medical AI&lt;/li&gt; &lt;li&gt;ü©∫ &lt;strong&gt;High-Fidelity Clinical Inquiry&lt;/strong&gt;: The only model to rank first across all three BCOSCE dimensions‚ÄîClinical Inquiry, Laboratory Testing, and Diagnosis&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Low Hallucination, High Reliability&lt;/strong&gt;: Achieves substantially lower hallucination rates than GPT-5.2 through Fact-Aware RL, even without external tools&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Efficient Deployment&lt;/strong&gt;: W4 quantization reduces memory to 26% of original; Gated Eagle3 speculative decoding achieves 96% speedup&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baichuan-inc/Baichuan-M3-235B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbjbrf/baichuanincbaichuanm3235b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T05:46:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbqmon</id>
    <title>SPARKLE Announces Intel Arc Pro B60 24GB Graphics Card Series Launch on January 12, 2026 for USD $799 MSRP</title>
    <updated>2026-01-13T12:58:16+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.sparkle.com.tw/en/sparkle-news/view/93E0b95ea8A0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqmon/sparkle_announces_intel_arc_pro_b60_24gb_graphics/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbqmon/sparkle_announces_intel_arc_pro_b60_24gb_graphics/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbpf8s</id>
    <title>Nemotron 3 Super release soon?</title>
    <updated>2026-01-13T11:56:40+00:00</updated>
    <author>
      <name>/u/Lorelabbestia</name>
      <uri>https://old.reddit.com/user/Lorelabbestia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found this entry in the autoconfig YAML of the TRT-LLM github repo from 3 days ago:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/model_registry/models.yaml"&gt;nvidia/NVIDIA-Nemotron-3-Super-120B-BF16-BF16KV-010726&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was just wondering if we have a release date?&lt;/p&gt; &lt;p&gt;I'm currently training nemotron 3 nano 30B to assess my current setup and was thinking to train final model on qwen's 3 next 80B, but if NVIDIA comes out with a 120B banger, I'm going for it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lorelabbestia"&gt; /u/Lorelabbestia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpf8s/nemotron_3_super_release_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpf8s/nemotron_3_super_release_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpf8s/nemotron_3_super_release_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T11:56:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbpz5l</id>
    <title>kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required</title>
    <updated>2026-01-13T12:25:26+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"&gt; &lt;img alt="kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required" src="https://b.thumbs.redditmedia.com/1twWaeVXhu1muEmRUClJoZ9yZJXDVOmoBhoTPlp5ntc.jpg" title="kyutai just introduced Pocket TTS: a 100M-parameter text-to-speech model with high-quality voice cloning that runs on your laptop‚Äîno GPU required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog post with demo: Pocket TTS: A high quality TTS that gives your CPU a voice: &lt;a href="https://kyutai.org/blog/2026-01-13-pocket-tts"&gt;https://kyutai.org/blog/2026-01-13-pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/kyutai-labs/pocket-tts"&gt;https://github.com/kyutai-labs/pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Model Card: &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;arXiv:2509.06926 [cs.SD]: Continuous Audio Language Models&lt;br /&gt; Simon Rouard, Manu Orsini, Axel Roebel, Neil Zeghidour, Alexandre D√©fossez&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2509.06926"&gt;https://arxiv.org/abs/2509.06926&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From kyutai on ùïè: &lt;a href="https://x.com/kyutai_labs/status/2011047335892303875"&gt;https://x.com/kyutai_labs/status/2011047335892303875&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qbpz5l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbpz5l/kyutai_just_introduced_pocket_tts_a_100mparameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T12:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbw325</id>
    <title>My wishes for 2026</title>
    <updated>2026-01-13T16:35:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt; &lt;img alt="My wishes for 2026" src="https://preview.redd.it/8knck5zv85dg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a8be13989bebb31b688873f7197d169cb43651e" title="My wishes for 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which do you think will happen first? And which won‚Äôt happen in 2026?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8knck5zv85dg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T16:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
