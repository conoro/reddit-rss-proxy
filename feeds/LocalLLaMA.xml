<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-28T16:25:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p8smlp</id>
    <title>Question and Answer Position Detection</title>
    <updated>2025-11-28T11:31:05+00:00</updated>
    <author>
      <name>/u/White_Way751</name>
      <uri>https://old.reddit.com/user/White_Way751</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8smlp/question_and_answer_position_detection/"&gt; &lt;img alt="Question and Answer Position Detection" src="https://b.thumbs.redditmedia.com/Pzfgfl5UaAvHo4cpZPNZRjIPQttwfLTPTlSCjfJxlYA.jpg" title="Question and Answer Position Detection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I need advice on which direction to explore.&lt;/p&gt; &lt;p&gt;I have a large table with varying formats usually questionnaires. I need to identify the positions of questions and answers in the document.&lt;/p&gt; &lt;p&gt;I can provide the data in any readable format (JSON, Markdown, HTML, etc.).&lt;/p&gt; &lt;p&gt;In the image, I’ve included a small example, but the actual table can be more complex, including checkboxes, selects, and other elements.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ycm3qb9zgz3g1.png?width=1944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4737c97b955f675142cd9edd9f10030a9152058"&gt;https://preview.redd.it/ycm3qb9zgz3g1.png?width=1944&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4737c97b955f675142cd9edd9f10030a9152058&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ideally, I want to extract the information from the provided data and get back a JSON like the example below.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ { &amp;quot;question&amp;quot;: &amp;quot;Do you perform durability tests on your products or product?&amp;quot;, &amp;quot;questionPosition&amp;quot;: &amp;quot;1,2&amp;quot;, &amp;quot;answerPosition&amp;quot;: &amp;quot;3&amp;quot;, &amp;quot;answerType&amp;quot;: &amp;quot;Yes / No, because&amp;quot; }, { &amp;quot;question&amp;quot;: &amp;quot;Are the results available on request?&amp;quot;, &amp;quot;questionPosition&amp;quot;: &amp;quot;4,5&amp;quot;, &amp;quot;answerPosition&amp;quot;: &amp;quot;6&amp;quot;, &amp;quot;answerType&amp;quot;: &amp;quot;Yes / No, because&amp;quot; }, { &amp;quot;question&amp;quot;: &amp;quot;Are the tests performed by an accredited laboratory?&amp;quot;, &amp;quot;questionPosition&amp;quot;: &amp;quot;7,8&amp;quot;, &amp;quot;answerPosition&amp;quot;: &amp;quot;9&amp;quot;, &amp;quot;answerType&amp;quot;: &amp;quot;Yes / No, because&amp;quot; }, { &amp;quot;question&amp;quot;: &amp;quot;Laboratory name&amp;quot;, &amp;quot;questionPosition&amp;quot;: &amp;quot;10&amp;quot;, &amp;quot;answerPosition&amp;quot;: &amp;quot;11&amp;quot;, &amp;quot;answerType&amp;quot;: &amp;quot;&amp;quot; } ] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Is there are specific model for this task, I have tried LLaMa, chatGPT, Claude big ones not stable at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/White_Way751"&gt; /u/White_Way751 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8smlp/question_and_answer_position_detection/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8smlp/question_and_answer_position_detection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8smlp/question_and_answer_position_detection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T11:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8vo7z</id>
    <title>Is there any comparison or benchmark of react-native-executorch and onnxruntime react native</title>
    <updated>2025-11-28T14:06:17+00:00</updated>
    <author>
      <name>/u/Educational-Nose3354</name>
      <uri>https://old.reddit.com/user/Educational-Nose3354</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;Currently I want to choose the offline LLM runtime for my react native mobile app. I stumble upon these 2 libs react-native-executorch and onnxruntime react native. And I wonder which one is better and faster for making AI on local device totally offline and can output token per second faster?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational-Nose3354"&gt; /u/Educational-Nose3354 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8vo7z/is_there_any_comparison_or_benchmark_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8vo7z/is_there_any_comparison_or_benchmark_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8vo7z/is_there_any_comparison_or_benchmark_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:06:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p89j2t</id>
    <title>Today I learned that DDR5 can throttle itself at high temps. It affects inference speed.</title>
    <updated>2025-11-27T18:33:00+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been moving the rig over to a proper frame from the $50 Amazon mining frame and taking the opportunity to do airflow properly. I measured the temps of the 6400 MT/s DDR5 RDIMMs using ipmitool and found they were hitting 95C and above while compiling vLLM from source.&lt;/p&gt; &lt;p&gt;Ouch. That’s very near the top of their operating envelope.&lt;/p&gt; &lt;p&gt;After 3D printing some RAM shrouds and adding a pair of 92mm Noctua Chromax the DDR5 stays under 60C during compiling and even during CPU inference.&lt;/p&gt; &lt;p&gt;And it runs approx 10% faster at inference even for GPU-only models. &lt;/p&gt; &lt;p&gt;Check your RAM temps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T18:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8orwd</id>
    <title>Please help me pick the right Mac for local LLM inference (M4 vs M2 Pro vs M1 Max)</title>
    <updated>2025-11-28T07:24:34+00:00</updated>
    <author>
      <name>/u/mystical_mountain</name>
      <uri>https://old.reddit.com/user/mystical_mountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I'm trying to decide which Mac to buy, mainly for local LLM inference and general text generation. Nothing too heavy, my top priority is still energy efficiency and silence, which is why I'm sticking with a Mac. After some research, I’ve narrowed it down to three options that seem to hit the sweet spot between performance and budget:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mac Mini M4, 32GB RAM, 1064€ (new)&lt;/li&gt; &lt;li&gt;Mac Mini M2 Pro, 32GB RAM, 900€ (used)&lt;/li&gt; &lt;li&gt;Mac Studio M1 Max, 64GB RAM, 1300€ (used)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From the benchmarks I’ve seen (&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/4167"&gt;Ggerganov's llama.cpp discussion&lt;/a&gt;), it looks like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Mac Studio M1 Max is by far the fastest for LLM inference.&lt;/li&gt; &lt;li&gt;Mac Mini M2 Pro seems to outperform the base M4 in real token-per-second benchmarks.&lt;/li&gt; &lt;li&gt;Mac Mini M4 is newer, but the base model is the slowest of all three.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Before I buy anything, can anyone sanity-check this? Did I overlook something important, or is this ranking basically correct?&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;Edit (use case): I want to set the Mac up as a dedicated headless local LLM server. It won’t run anything else. I’ll use it to process private documents in Paperless-NGX, and possibly connect it to my Home Assistant instance for the chat function.&lt;/p&gt; &lt;p&gt;Edit 2: Thank y'all for your comments! My conclusion: I'll wait a bit more and save money, possibly until the M5 comes out and the old Mac's prices hopefully drop a bit. Then I'll target the Mac Studio M1 Ultra, 128GB RAM, which is currently around 2900€ (used).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mystical_mountain"&gt; /u/mystical_mountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8orwd/please_help_me_pick_the_right_mac_for_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8orwd/please_help_me_pick_the_right_mac_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8orwd/please_help_me_pick_the_right_mac_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8uotj</id>
    <title>New to Local LLMs. What models can I run with my setup?</title>
    <updated>2025-11-28T13:20:53+00:00</updated>
    <author>
      <name>/u/House-Wins</name>
      <uri>https://old.reddit.com/user/House-Wins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, sorry I know this question has been asked 1000s of times by now, but I'm new to Local LLMs and don't know a lot about them. I'm trying to use less of paid services and move more towards self hosted. Now I don't have the best setup compared to some on here and I know the limitations, but what models do you think I should run. My usage will be coding and everyday chat.&lt;/p&gt; &lt;p&gt;Here are my specs:&lt;/p&gt; &lt;p&gt;- Machine: Minisforum X1 Pro, AMD Ryzen AI 9 HX 370, T500 4TB, 128GB 5600mhz.&lt;/p&gt; &lt;p&gt;- GPU: AMD Radeon 890M&lt;/p&gt; &lt;p&gt;- OS: Linux &lt;/p&gt; &lt;p&gt;Running Ollama and Webui through Docker&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/House-Wins"&gt; /u/House-Wins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uotj/new_to_local_llms_what_models_can_i_run_with_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uotj/new_to_local_llms_what_models_can_i_run_with_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uotj/new_to_local_llms_what_models_can_i_run_with_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8up6n</id>
    <title>How does any of this work?</title>
    <updated>2025-11-28T13:21:21+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8up6n/how_does_any_of_this_work/"&gt; &lt;img alt="How does any of this work?" src="https://preview.redd.it/vsfizwwr004g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=29ac3399548a28b083fa78324923edc37f0d3412" title="How does any of this work?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i1-IQ3_S has better quality than i1-IQ3_M&lt;/p&gt; &lt;p&gt;What does this even mean? And why would anyone use the non-i1 versions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vsfizwwr004g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8up6n/how_does_any_of_this_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8up6n/how_does_any_of_this_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:21:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8nped</id>
    <title>Strix Halo batching with tensor parallel and pipeline parallel using vllm benchmarked</title>
    <updated>2025-11-28T06:19:58+00:00</updated>
    <author>
      <name>/u/Hungry_Elk_3276</name>
      <uri>https://old.reddit.com/user/Hungry_Elk_3276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a continuation of last dual Strix Halo cluster post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ot3lxv/i_tested_strix_halo_clustering_w_50gig_ib_to_see/"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It turns out that RCCL seems to work, but it is not enabled by AMD for some reason. (Why??) Following a random PR on GitHub that uses gfx1100 path on gfx1151, I was able to get RCCL working with vLLM. Just compile and swap the default RCCL shipped with vLLM to your local one and everything started working. So I tested some models I was able to run and got the following results for the original hybrid qwen3-4b (to see the batching performance) and qwen3-vl-30b-a3b to try to have an idea of real-world performance.&lt;/p&gt; &lt;p&gt;Here are the results:&lt;/p&gt; &lt;h1&gt;Qwen3-4B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Config&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;Single Node&lt;/th&gt; &lt;th align="left"&gt;tp=2&lt;/th&gt; &lt;th align="left"&gt;pp=2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 128 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;1.64&lt;/td&gt; &lt;td align="left"&gt;3.55&lt;/td&gt; &lt;td align="left"&gt;3.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;209.96&lt;/td&gt; &lt;td align="left"&gt;454.32&lt;/td&gt; &lt;td align="left"&gt;402.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;384.00&lt;/td&gt; &lt;td align="left"&gt;896.00&lt;/td&gt; &lt;td align="left"&gt;647.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;5221.80&lt;/td&gt; &lt;td align="left"&gt;2893.86&lt;/td&gt; &lt;td align="left"&gt;3040.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;5218.32&lt;/td&gt; &lt;td align="left"&gt;3079.07&lt;/td&gt; &lt;td align="left"&gt;2935.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;11067.56&lt;/td&gt; &lt;td align="left"&gt;5608.94&lt;/td&gt; &lt;td align="left"&gt;4441.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;548.74&lt;/td&gt; &lt;td align="left"&gt;242.83&lt;/td&gt; &lt;td align="left"&gt;276.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;563.52&lt;/td&gt; &lt;td align="left"&gt;249.43&lt;/td&gt; &lt;td align="left"&gt;286.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;589.95&lt;/td&gt; &lt;td align="left"&gt;274.77&lt;/td&gt; &lt;td align="left"&gt;307.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;544.46&lt;/td&gt; &lt;td align="left"&gt;240.93&lt;/td&gt; &lt;td align="left"&gt;274.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;450.00&lt;/td&gt; &lt;td align="left"&gt;167.44&lt;/td&gt; &lt;td align="left"&gt;214.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;304.82&lt;/td&gt; &lt;td align="left"&gt;140.87&lt;/td&gt; &lt;td align="left"&gt;159.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 128 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;td align="left"&gt;0.79&lt;/td&gt; &lt;td align="left"&gt;0.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;71.97&lt;/td&gt; &lt;td align="left"&gt;202.32&lt;/td&gt; &lt;td align="left"&gt;157.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;182.00&lt;/td&gt; &lt;td align="left"&gt;384.00&lt;/td&gt; &lt;td align="left"&gt;294.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;28426.97&lt;/td&gt; &lt;td align="left"&gt;11321.20&lt;/td&gt; &lt;td align="left"&gt;14431.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;19933.60&lt;/td&gt; &lt;td align="left"&gt;5554.79&lt;/td&gt; &lt;td align="left"&gt;8448.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;117059.55&lt;/td&gt; &lt;td align="left"&gt;52412.20&lt;/td&gt; &lt;td align="left"&gt;55070.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1635.82&lt;/td&gt; &lt;td align="left"&gt;574.54&lt;/td&gt; &lt;td align="left"&gt;740.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1692.04&lt;/td&gt; &lt;td align="left"&gt;608.23&lt;/td&gt; &lt;td align="left"&gt;780.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1752.66&lt;/td&gt; &lt;td align="left"&gt;620.89&lt;/td&gt; &lt;td align="left"&gt;798.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;1629.43&lt;/td&gt; &lt;td align="left"&gt;572.30&lt;/td&gt; &lt;td align="left"&gt;737.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;1275.61&lt;/td&gt; &lt;td align="left"&gt;400.22&lt;/td&gt; &lt;td align="left"&gt;551.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;1778.59&lt;/td&gt; &lt;td align="left"&gt;632.66&lt;/td&gt; &lt;td align="left"&gt;813.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 256 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;1.93&lt;/td&gt; &lt;td align="left"&gt;5.85&lt;/td&gt; &lt;td align="left"&gt;2.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;246.56&lt;/td&gt; &lt;td align="left"&gt;749.28&lt;/td&gt; &lt;td align="left"&gt;285.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;512.00&lt;/td&gt; &lt;td align="left"&gt;1025.00&lt;/td&gt; &lt;td align="left"&gt;521.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;6999.42&lt;/td&gt; &lt;td align="left"&gt;431.48&lt;/td&gt; &lt;td align="left"&gt;1288.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;4504.39&lt;/td&gt; &lt;td align="left"&gt;417.06&lt;/td&gt; &lt;td align="left"&gt;1657.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;22205.62&lt;/td&gt; &lt;td align="left"&gt;660.91&lt;/td&gt; &lt;td align="left"&gt;1877.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;912.78&lt;/td&gt; &lt;td align="left"&gt;249.23&lt;/td&gt; &lt;td align="left"&gt;790.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;912.48&lt;/td&gt; &lt;td align="left"&gt;261.94&lt;/td&gt; &lt;td align="left"&gt;805.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;1078.28&lt;/td&gt; &lt;td align="left"&gt;304.48&lt;/td&gt; &lt;td align="left"&gt;869.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;905.65&lt;/td&gt; &lt;td align="left"&gt;247.28&lt;/td&gt; &lt;td align="left"&gt;784.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;814.82&lt;/td&gt; &lt;td align="left"&gt;276.54&lt;/td&gt; &lt;td align="left"&gt;837.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;259.57&lt;/td&gt; &lt;td align="left"&gt;85.42&lt;/td&gt; &lt;td align="left"&gt;224.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 256 concurrency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.28&lt;/td&gt; &lt;td align="left"&gt;0.80&lt;/td&gt; &lt;td align="left"&gt;0.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;70.64&lt;/td&gt; &lt;td align="left"&gt;205.47&lt;/td&gt; &lt;td align="left"&gt;124.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;259.00&lt;/td&gt; &lt;td align="left"&gt;512.00&lt;/td&gt; &lt;td align="left"&gt;256.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;95111.92&lt;/td&gt; &lt;td align="left"&gt;32136.63&lt;/td&gt; &lt;td align="left"&gt;36498.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;78589.23&lt;/td&gt; &lt;td align="left"&gt;9586.82&lt;/td&gt; &lt;td align="left"&gt;16249.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;278357.25&lt;/td&gt; &lt;td align="left"&gt;111121.91&lt;/td&gt; &lt;td align="left"&gt;114120.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;3131.02&lt;/td&gt; &lt;td align="left"&gt;1070.57&lt;/td&gt; &lt;td align="left"&gt;1848.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;3333.69&lt;/td&gt; &lt;td align="left"&gt;1162.72&lt;/td&gt; &lt;td align="left"&gt;1891.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;3416.15&lt;/td&gt; &lt;td align="left"&gt;1216.61&lt;/td&gt; &lt;td align="left"&gt;2079.38&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;3118.79&lt;/td&gt; &lt;td align="left"&gt;1066.38&lt;/td&gt; &lt;td align="left"&gt;1841.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;2603.32&lt;/td&gt; &lt;td align="left"&gt;769.11&lt;/td&gt; &lt;td align="left"&gt;1474.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;1812.06&lt;/td&gt; &lt;td align="left"&gt;622.97&lt;/td&gt; &lt;td align="left"&gt;1027.46&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Qwen3VL-30B-A3B&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test Config&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;tp=2&lt;/th&gt; &lt;th align="left"&gt;pp=2&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 1 concurrency / 10 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.16&lt;/td&gt; &lt;td align="left"&gt;0.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;20.66&lt;/td&gt; &lt;td align="left"&gt;13.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;24.00&lt;/td&gt; &lt;td align="left"&gt;15.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;506.55&lt;/td&gt; &lt;td align="left"&gt;667.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;300.01&lt;/td&gt; &lt;td align="left"&gt;467.83&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;2196.93&lt;/td&gt; &lt;td align="left"&gt;2346.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;44.74&lt;/td&gt; &lt;td align="left"&gt;69.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;43.40&lt;/td&gt; &lt;td align="left"&gt;67.62&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;55.68&lt;/td&gt; &lt;td align="left"&gt;80.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;44.39&lt;/td&gt; &lt;td align="left"&gt;68.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;43.32&lt;/td&gt; &lt;td align="left"&gt;67.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;61.96&lt;/td&gt; &lt;td align="left"&gt;94.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 1 concurrency / 10 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.08&lt;/td&gt; &lt;td align="left"&gt;0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;21.43&lt;/td&gt; &lt;td align="left"&gt;13.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;23.00&lt;/td&gt; &lt;td align="left"&gt;15.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;728.18&lt;/td&gt; &lt;td align="left"&gt;1306.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;726.75&lt;/td&gt; &lt;td align="left"&gt;1309.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;752.38&lt;/td&gt; &lt;td align="left"&gt;1319.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;43.96&lt;/td&gt; &lt;td align="left"&gt;68.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;43.97&lt;/td&gt; &lt;td align="left"&gt;68.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;44.08&lt;/td&gt; &lt;td align="left"&gt;68.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;43.79&lt;/td&gt; &lt;td align="left"&gt;68.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;43.85&lt;/td&gt; &lt;td align="left"&gt;68.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;119.46&lt;/td&gt; &lt;td align="left"&gt;187.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 8 concurrency / 100 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.71&lt;/td&gt; &lt;td align="left"&gt;0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;90.55&lt;/td&gt; &lt;td align="left"&gt;52.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;124.00&lt;/td&gt; &lt;td align="left"&gt;80.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;949.21&lt;/td&gt; &lt;td align="left"&gt;1879.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;851.09&lt;/td&gt; &lt;td align="left"&gt;2096.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;1496.50&lt;/td&gt; &lt;td align="left"&gt;2263.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;78.66&lt;/td&gt; &lt;td align="left"&gt;133.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;78.90&lt;/td&gt; &lt;td align="left"&gt;134.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;86.23&lt;/td&gt; &lt;td align="left"&gt;147.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;78.04&lt;/td&gt; &lt;td align="left"&gt;132.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;76.56&lt;/td&gt; &lt;td align="left"&gt;132.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;141.35&lt;/td&gt; &lt;td align="left"&gt;242.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 8 concurrency / 100 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.31&lt;/td&gt; &lt;td align="left"&gt;0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;78.50&lt;/td&gt; &lt;td align="left"&gt;45.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;112.00&lt;/td&gt; &lt;td align="left"&gt;73.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;1229.13&lt;/td&gt; &lt;td align="left"&gt;3934.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;829.60&lt;/td&gt; &lt;td align="left"&gt;5636.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;2089.51&lt;/td&gt; &lt;td align="left"&gt;5760.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;94.68&lt;/td&gt; &lt;td align="left"&gt;156.32&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;96.46&lt;/td&gt; &lt;td align="left"&gt;156.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;101.22&lt;/td&gt; &lt;td align="left"&gt;175.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;94.31&lt;/td&gt; &lt;td align="left"&gt;155.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;82.06&lt;/td&gt; &lt;td align="left"&gt;141.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;326.12&lt;/td&gt; &lt;td align="left"&gt;562.92&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;512 input / 128 output / 16 concurrency / 200 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;1.09&lt;/td&gt; &lt;td align="left"&gt;0.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;139.24&lt;/td&gt; &lt;td align="left"&gt;82.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;192.00&lt;/td&gt; &lt;td align="left"&gt;115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;406.30&lt;/td&gt; &lt;td align="left"&gt;733.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;392.66&lt;/td&gt; &lt;td align="left"&gt;669.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;742.20&lt;/td&gt; &lt;td align="left"&gt;1419.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;109.05&lt;/td&gt; &lt;td align="left"&gt;184.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;106.78&lt;/td&gt; &lt;td align="left"&gt;183.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;122.48&lt;/td&gt; &lt;td align="left"&gt;204.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;108.20&lt;/td&gt; &lt;td align="left"&gt;182.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;99.34&lt;/td&gt; &lt;td align="left"&gt;172.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;183.85&lt;/td&gt; &lt;td align="left"&gt;310.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;2048 input / 256 output / 16 concurrency / 200 requests&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Request Throughput (req/s)&lt;/td&gt; &lt;td align="left"&gt;0.48&lt;/td&gt; &lt;td align="left"&gt;0.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Output Token Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;121.79&lt;/td&gt; &lt;td align="left"&gt;70.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Peak Output Throughput (tok/s)&lt;/td&gt; &lt;td align="left"&gt;176.00&lt;/td&gt; &lt;td align="left"&gt;115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;941.88&lt;/td&gt; &lt;td align="left"&gt;2290.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;632.24&lt;/td&gt; &lt;td align="left"&gt;1468.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TTFT (ms)&lt;/td&gt; &lt;td align="left"&gt;2152.66&lt;/td&gt; &lt;td align="left"&gt;6903.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;124.63&lt;/td&gt; &lt;td align="left"&gt;214.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;121.63&lt;/td&gt; &lt;td align="left"&gt;208.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;P99 TPOT (ms)&lt;/td&gt; &lt;td align="left"&gt;147.76&lt;/td&gt; &lt;td align="left"&gt;256.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Mean ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;124.14&lt;/td&gt; &lt;td align="left"&gt;213.50&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Median ITL (ms)&lt;/td&gt; &lt;td align="left"&gt;108.46&lt;/td&gt; &lt;td align="left"&gt;190.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;Duration (s)&lt;/td&gt; &lt;td align="left"&gt;420.41&lt;/td&gt; &lt;td align="left"&gt;730.73&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The first qwen3-4b is trying to see how well the Strix Halo handled the high pressure situation. As we can see from the results, TP is getting much better performance compared to PP. And I am not sure why the single node inference is this slow for some reason.&lt;/p&gt; &lt;p&gt;For the qwen3vl-30b-a3b, I want to simulate a more realistic situation, which is 1 user or a small-sized team that is using it as a local inference server. And we can see that TP is giving us nearly 50% more token generation speed. While both PP and TP are providing speedups, TP is performing much better.&lt;/p&gt; &lt;p&gt;If someone wonders why the hell this token generation speed is so slow, it is because it is running the full bf16/fp16 weight. The AWQ support isn't quite there yet, but it is improving. It is surprising to see that qwen3-next-awq is working right now, but running the AWQ multi-nodes hits some errors. But it is improving at a rate much faster than I expected. The ultimate goal of running qwen3vl 235b AWQ 4bit seems very near.&lt;/p&gt; &lt;p&gt;And happy Thanksgiving folks! Hope this data provides some insights.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hungry_Elk_3276"&gt; /u/Hungry_Elk_3276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8nped/strix_halo_batching_with_tensor_parallel_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8nped/strix_halo_batching_with_tensor_parallel_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8nped/strix_halo_batching_with_tensor_parallel_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T06:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8u48w</id>
    <title>Using local Llama-3 to analyze Volatility 3 memory dumps. Automating malware discovery in RAM without cloud APIs</title>
    <updated>2025-11-28T12:53:29+00:00</updated>
    <author>
      <name>/u/Glass-Ant-6041</name>
      <uri>https://old.reddit.com/user/Glass-Ant-6041</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8u48w/using_local_llama3_to_analyze_volatility_3_memory/"&gt; &lt;img alt="Using local Llama-3 to analyze Volatility 3 memory dumps. Automating malware discovery in RAM without cloud APIs" src="https://external-preview.redd.it/YnNkcTZ2c3R2ejNnMTHM8r7gjhfmThXWebp6A_unKOp4fr5vmShf9J1OuvEs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=debe9e93db79cfbeaabea5ba2488d79891b30626" title="Using local Llama-3 to analyze Volatility 3 memory dumps. Automating malware discovery in RAM without cloud APIs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glass-Ant-6041"&gt; /u/Glass-Ant-6041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lszb4gqtvz3g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8u48w/using_local_llama3_to_analyze_volatility_3_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8u48w/using_local_llama3_to_analyze_volatility_3_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T12:53:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8o8sn</id>
    <title>I have a RTX5090 and an AMD AI MAX+ 95 128GB. Which benchmark do you want me to run?</title>
    <updated>2025-11-28T06:52:19+00:00</updated>
    <author>
      <name>/u/foogitiff</name>
      <uri>https://old.reddit.com/user/foogitiff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After selling my spare 5080, I couldn't decide between the two option (well, another is a R9700 Pro).&lt;/p&gt; &lt;p&gt;I decided to buy a 5090 in the end, but I didn't had the time to cancel my framework preorder, so I have currently both! I will be keeping only one.&lt;/p&gt; &lt;p&gt;If people want some llama-bench number comparisons, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foogitiff"&gt; /u/foogitiff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8o8sn/i_have_a_rtx5090_and_an_amd_ai_max_95_128gb_which/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8o8sn/i_have_a_rtx5090_and_an_amd_ai_max_95_128gb_which/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8o8sn/i_have_a_rtx5090_and_an_amd_ai_max_95_128gb_which/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T06:52:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p81k2z</id>
    <title>Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted</title>
    <updated>2025-11-27T12:56:59+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt; &lt;img alt="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original discussion on the initial &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; created GLM-4.5-Air-Derestricted model that was ablated using &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;'s new ablation method is here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted &lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: Derestricted is a name given to models created by Arli AI using this method, but the method officially is just called &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;Norm-Preserving Biprojected Abliteration&lt;/a&gt; by &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Hey everyone, Owen here from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; again. In my previous post, I got a lot of requests to attempt this derestricting on OpenAI's gpt-oss models as they are models that are intelligent but was infamous for being very...restricted.&lt;/p&gt; &lt;p&gt;I thought that it would be a big challenge and be interesting to try and attempt as well, and so that was the next model I decided to try and derestrict next. The 120b version is more unwieldy to transfer around and load in/out of VRAM/RAM as I was experimenting, so I started with the 20b version first but I will get to the 120b next which should be super interesting.&lt;/p&gt; &lt;p&gt;As for the 20b model here, it seems to have worked! The model now can respond to questions that OpenAI never would have approved of answering (lol!). It also seems to have cut down its wasteful looping around of deciding whether it can or cannot answer a question based on a non existent policy in it's reasoning, although this isn't completely removed yet. I suspect a more customized harmful/harmless dataset to specifically target this behavior might be useful for this, so that will be what I need to work on.&lt;/p&gt; &lt;p&gt;Otherwise I think this is just an outright improved model over the original as it is much more useful now than it's original behavior. Where it would usually flag a lot of false positives and be absolutely useless in certain situations just because of &amp;quot;safety&amp;quot;.&lt;/p&gt; &lt;p&gt;In order to work on modifying the weights of the model, I also had to use a BF16 converted version to start with as the model as you all might know was released in MXFP4 format, but then attempting the ablation on the BF16 converted model seems to work well. I think that this proves that this new method of essentially &amp;quot;direction-based&amp;quot; abliteration is really flexible and works super well for probably any models.&lt;/p&gt; &lt;p&gt;As for quants, I'm not one to worry about making GGUFs myself because I'm sure the GGUF makers will get to it pretty fast and do a better job than I can. Also, there are no FP8 or INT8 quants now because its pretty small and those that run FP8 or INT8 quants usually have a substantial GPU setup anyways.&lt;/p&gt; &lt;p&gt;Try it out and have fun! This time it's really for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; because we don't even run this model on our Arli AI API service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/gpt-oss-20b-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T12:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8trxh</id>
    <title>I had to review my local model setup after a silent FaceSeek observation</title>
    <updated>2025-11-28T12:35:43+00:00</updated>
    <author>
      <name>/u/CoachExtreme5255</name>
      <uri>https://old.reddit.com/user/CoachExtreme5255</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was experimenting with a small idea when I noticed a detail in FaceSeek that caused me to reconsider my approach to local models..I came to the realisation that I never settle on a consistent workflow because I constantly switch between different model sizes. Larger ones feel heavy for daily tasks, while others run quickly but lack depth. When deciding which models to run locally, I'm interested in how others here strike a balance between usefulness and performance. &lt;/p&gt; &lt;p&gt;Do you use a single, well-tuned setup or maintain separate environments? My goal is to improve my workflow so that the model feels dependable and doesn't require frequent adjustments. I could create a cleaner routine with the help of insights about small, useful habits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoachExtreme5255"&gt; /u/CoachExtreme5255 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8trxh/i_had_to_review_my_local_model_setup_after_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8trxh/i_had_to_review_my_local_model_setup_after_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8trxh/i_had_to_review_my_local_model_setup_after_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T12:35:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ahy8</id>
    <title>Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp; Reasoning Benchmarks. (Link to Chat with the Model provided)</title>
    <updated>2025-11-27T19:13:50+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt; &lt;img alt="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" src="https://a.thumbs.redditmedia.com/tQWpy1j22HMExtYqdGvlA_Lo8sIubygJf4xso2VwSj0.jpg" title="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;From the Official Announcement:&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Today, we release INTELLECT-3, a 100B+ parameter Mixture-of-Experts model trained on our RL stack, achieving state-of-the-art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our complete recipe — from the model weights and training frameworks, to our datasets, RL environments, and evaluations — has been open-sourced, with the goal of encouraging more open research on large scale reinforcement learning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;INTELLECT-3 is trained on the same software and infrastructure that we’re open-sourcing and making available on our platform at Prime Intellect, giving everyone the tools to post-train their own state-of-the-art models, and moving us towards a future where every company can be an AI company.&lt;/p&gt; &lt;p&gt;The sharpest distinction between Prime-RL and many other RL trainers is that it is async-only — we recognized fairly early (for our previous INTELLECT-2 model) that the future of RL is async; i.e. always a few steps off-policy. Async training is simply the only practical way to efficiently scale RL to long-horizon agentic rollouts without incurring bottlenecks based on the slowest rollouts per step.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;Architecture:&lt;/h2&gt; &lt;p&gt;Three main abstractions facilitate RL training: the orchestrator, the trainer, and the inference service. A RL training run involves the coordination of a trainer, orchestrator and an inference service. The FSDP trainer and vLLM inference run disaggregated, and can be individually deployed across multiple nodes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Orchestrator:&lt;/strong&gt; - The orchestrator is a lightweight CPU process that handles the core data flow and scheduling logic, serving as an intermediary between the trainer and inference service with bidirectional relays. In one direction, it collects rollouts from the inference server, assembles them into packed batches, and dispatches them to the trainer; in the other direction, it relays updated model weights from the trainer to the inference service. The orchestrator utilizes verifiers environments to abstract multi-turn rollout generation and scoring, allowing any environment on the Environments Hub to plug into the training loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt; - The trainer is responsible for producing an updated policy model given rollouts and advantages. We use FSDP 2 as the backend with compatibility for any HuggingFace model. FSDP shards model parameters, gradients, and optimizer states, allowing training large models with data parallelism and minimal GPU memory footprint. The trainer is inspired by torchtitan and relies on native PyTorch features to implement advanced parallelism techniques, such as tensor, context, and expert parallelism, and leverages grouped matrix multiplication kernels for efficient MoE training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt; - The inference pool consists of standard OpenAI-compatible servers with a vLLM backend. The API specification is extended with custom endpoints to enable updating the server with the latest policy: &lt;code&gt;/update_weights&lt;/code&gt; is used to update the policy, and &lt;code&gt;/reload_weights&lt;/code&gt; is used to reset the weights to the base model in between experiments. We rely on vLLM's optimized kernels, parallelism strategies, and scheduling for fast rollout generation. Given the disaggregated nature of the service architecture, it can be directly extended to include multiple engines with a shared request pool, allowing operation across multiple clusters and straightforward integration of alternative inference engines.&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Official Announcement: &lt;a href="https://www.primeintellect.ai/blog/intellect-3"&gt;https://www.primeintellect.ai/blog/intellect-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Technical Report: &lt;a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf"&gt;https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Prime-RL GitHub: &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;https://github.com/PrimeIntellect-ai/prime-rl&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Model Weights: &lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Chat with the Model Here: &lt;a href="https://chat.primeintellect.ai/"&gt;https://chat.primeintellect.ai/&lt;/a&gt;&lt;/h4&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8ahy8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T19:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8uzww</id>
    <title>Bifrost vs LiteLLM: Side-by-Side Benchmarks (50x Faster LLM Gateway)</title>
    <updated>2025-11-28T13:35:27+00:00</updated>
    <author>
      <name>/u/dinkinflika0</name>
      <uri>https://old.reddit.com/user/dinkinflika0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone; I recently shared a post here about Bifrost, a high-performance LLM gateway we’ve been building in Go. A lot of folks in the comments asked for a clearer side-by-side comparison with LiteLLM, including performance benchmarks and migration examples. So here’s a follow-up that lays out the numbers, features, and how to switch over in one line of code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (vs LiteLLM)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;single t3.medium instance&lt;/li&gt; &lt;li&gt;mock llm with 1.5 seconds latency&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;LiteLLM&lt;/th&gt; &lt;th align="left"&gt;Bifrost&lt;/th&gt; &lt;th align="left"&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="3"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;p99 Latency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;90.72s&lt;/td&gt; &lt;td align="left"&gt;1.68s&lt;/td&gt; &lt;td align="left"&gt;~54× faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Throughput&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;44.84 req/sec&lt;/td&gt; &lt;td align="left"&gt;424 req/sec&lt;/td&gt; &lt;td align="left"&gt;~9.4× higher&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Memory Usage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;372MB&lt;/td&gt; &lt;td align="left"&gt;120MB&lt;/td&gt; &lt;td align="left"&gt;~3× lighter&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mean Overhead&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~500µs&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;11µs @ 5K RPS&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;~45× lower&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/maximhq/bifrost"&gt;https://github.com/maximhq/bifrost&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Key Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Ultra-low overhead:&lt;/strong&gt; mean request handling overhead is just &lt;strong&gt;11µs per request&lt;/strong&gt; at 5K RPS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Provider Fallback:&lt;/strong&gt; Automatic failover between providers ensures 99.99% uptime for your applications.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic caching:&lt;/strong&gt; deduplicates similar requests to reduce repeated inference costs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive load balancing:&lt;/strong&gt; Automatically optimizes traffic distribution across provider keys and models based on real-time performance metrics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cluster mode resilience:&lt;/strong&gt; High availability deployment with automatic failover and load balancing. Peer-to-peer clustering where every instance is equal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drop-in OpenAI-compatible API:&lt;/strong&gt; Replace your existing SDK with just one line change. Compatible with OpenAI, Anthropic, LiteLLM, Google Genai, Langchain and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Observability:&lt;/strong&gt; Out-of-the-box OpenTelemetry support for observability. Built-in dashboard for quick glances without any complex setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model-Catalog:&lt;/strong&gt; Access 15+ providers and 1000+ AI models from multiple providers through a unified interface. Also support custom deployed models!&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Governance&lt;/strong&gt;: SAML support for SSO and Role-based access control and policy enforcement for team collaboration.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Migrating from LiteLLM → Bifrost&lt;/h1&gt; &lt;p&gt;You don’t need to rewrite your code; just point your LiteLLM SDK to Bifrost’s endpoint.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Old (LiteLLM):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from litellm import completion response = completion( model=&amp;quot;gpt-4o-mini&amp;quot;, messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hello GPT!&amp;quot;}] ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;New (Bifrost):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from litellm import completion response = completion( model=&amp;quot;gpt-4o-mini&amp;quot;, messages=[{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Hello GPT!&amp;quot;}], base_url=&amp;quot;&amp;lt;http://localhost:8080/litellm&amp;gt;&amp;quot; ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can also use custom headers for governance and tracking (see docs!)&lt;/p&gt; &lt;p&gt;The switch is one line; everything else stays the same.&lt;/p&gt; &lt;p&gt;Bifrost is built for teams that treat LLM infra as production software: &lt;strong&gt;predictable, observable, and fast&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;If you’ve found LiteLLM fragile or slow at higher load, this might be worth testing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dinkinflika0"&gt; /u/dinkinflika0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uzww/bifrost_vs_litellm_sidebyside_benchmarks_50x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uzww/bifrost_vs_litellm_sidebyside_benchmarks_50x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8uzww/bifrost_vs_litellm_sidebyside_benchmarks_50x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8pcrj</id>
    <title>How many parameters do you think are required to emulate the *knowledge* of an average person</title>
    <updated>2025-11-28T08:02:12+00:00</updated>
    <author>
      <name>/u/FrostTactics</name>
      <uri>https://old.reddit.com/user/FrostTactics</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not controversial to state that LLMs today aren't 100% efficient in their parameter usage. It would not surprise me if we could compress current day performance into one hundredth of the parameters. That said, all knowledge requires &lt;em&gt;information,&lt;/em&gt; and there must therefore be a limit to the level of compression that can be achieved.&lt;/p&gt; &lt;p&gt;The current paradigm tries to train all LLMs as generalists for various technical reasons I'm sure I don't have to explain to the people here. This means that basically all LLMs, even those with only a couple of billion parameters, speak passable Norwegian, for example.&lt;/p&gt; &lt;p&gt;Say we narrowed the scope and instead of trying to build generalists, we tried to build an LLM with an amount of knowledge comparable to that of an average person. Let's make the person monolingual, with the common knowledge expected of any modern person, and an expert in a single field.&lt;/p&gt; &lt;p&gt;Let's also ignore vision, real-world navigation, and actually processing the knowledge, as these seem a bit too vague to reliably get an estimate of at the moment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; Feels like a fair few of the responders didn't understand the question😅. This discussion is meant as a purely academic exercise for the theoretical lower limit of number of parameters required for the knowledge of an average person. I.e. not intelligence, just the pure amount of information required to represent the an average person's knowledge. I've seen a few people comment that LLMs have surpassed us on this already. I agree with this, I think we could easily represent it with far fewer parameters than the current SotA LLMs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrostTactics"&gt; /u/FrostTactics &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8pcrj/how_many_parameters_do_you_think_are_required_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T08:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8x9cv</id>
    <title>Weekly AI News: First autonomous cyberattack, Meta 1600-language ASR, MIT workforce study, and more</title>
    <updated>2025-11-28T15:13:05+00:00</updated>
    <author>
      <name>/u/Proof-Possibility-54</name>
      <uri>https://old.reddit.com/user/Proof-Possibility-54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Roundup of this week's notable developments:&lt;/p&gt; &lt;p&gt;Anthropic Cyberattack Disclosure - Chinese state actors used Claude Code for reconnaissance/scripting - AI executed 80-90% of attack lifecycle - 30 organizations targeted - Source: Anthropic blog&lt;/p&gt; &lt;p&gt;Meta Omnilingual ASR - 1,600 languages, 500 with no prior AI coverage - 7B parameters, Apache 2.0 - Source: Meta AI blog&lt;/p&gt; &lt;p&gt;MIT Iceberg Index - 11.7% of US workforce replaceable by current AI - $1.2T in wages - Highest exposure: HR, logistics, finance - Source: MIT working paper&lt;/p&gt; &lt;p&gt;Genesis Mission EO - Signed Nov 24 - Unifies 17 DOE national labs - Source: White House&lt;/p&gt; &lt;p&gt;OpenAI Lawsuit - 8 families, 4 suicides - Youngest: 14 years old - Source: Court filings via TechCrunch&lt;/p&gt; &lt;p&gt;I made a video summary if anyone prefers that format: &lt;a href="https://youtu.be/qKxFYhcQppc"&gt;https://youtu.be/qKxFYhcQppc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Possibility-54"&gt; /u/Proof-Possibility-54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8x9cv/weekly_ai_news_first_autonomous_cyberattack_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8x9cv/weekly_ai_news_first_autonomous_cyberattack_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8x9cv/weekly_ai_news_first_autonomous_cyberattack_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T15:13:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8p844</id>
    <title>Tested quantization on my 8GB potato laptop here's what actually breaks first</title>
    <updated>2025-11-28T07:54:17+00:00</updated>
    <author>
      <name>/u/Even_Ganache6148</name>
      <uri>https://old.reddit.com/user/Even_Ganache6148</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt; &lt;img alt="Tested quantization on my 8GB potato laptop here's what actually breaks first" src="https://b.thumbs.redditmedia.com/vR1H3t5ezVMlfOa85U2nv8CFIEe0-WSfW8pF48jj7ns.jpg" title="Tested quantization on my 8GB potato laptop here's what actually breaks first" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running local LLMs on my broke-student laptop (8GB RAM, i3 processor) and kept hitting the quantization guessing game. Downloaded like 10 different formats trying to figure out which one wouldn't destroy quality.&lt;/p&gt; &lt;p&gt;Here's what I found from testing TinyLlama and reading through hundreds of benchmark results:&lt;/p&gt; &lt;p&gt;Findings:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0x2atfgwdy3g1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4feadc39dc6e8a7dc96e7d8d4e63393b13d0859"&gt;https://preview.redd.it/0x2atfgwdy3g1.png?width=730&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b4feadc39dc6e8a7dc96e7d8d4e63393b13d0859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Pattern:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;General chat: Survives down to Q4 pretty well (2-3% quality drop)&lt;/li&gt; &lt;li&gt;Creative writing: Actually stays decent even at Q3&lt;/li&gt; &lt;li&gt;Code generation: Starts getting buggy at Q4 (5-10% drop)&lt;/li&gt; &lt;li&gt;Math/reasoning: Falls off a CLIFF at Q4 (15-20% accuracy drop)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Data Sources:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 3.1 8B (multiple quant formats from TheBloke/bartowski)&lt;/li&gt; &lt;li&gt;Mistral 7B v0.3 (various GGUF quants)&lt;/li&gt; &lt;li&gt;Qwen2 7B (official quants)&lt;/li&gt; &lt;li&gt;Phi-3 Mini (Microsoft's quants)&lt;/li&gt; &lt;li&gt;Tested on: MMLU (general reasoning), HumanEval (coding), GSM8K (math), creative writing prompts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Compiled from:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;HuggingFace model cards with reported benchmarks&lt;/li&gt; &lt;li&gt;Open LLM Leaderboard results&lt;/li&gt; &lt;li&gt;llama.cpp community benchmarks on GitHub&lt;/li&gt; &lt;li&gt;My own testing on TinyLlama 1.1B (what my laptop can actually run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is aggregated trends across models, not exhaustive testing. Different models degrade slightly differently, but the PATTERN holds - math breaks way faster than other tasks.&lt;/p&gt; &lt;p&gt;Why this matters: If you're using a model for coding or math, Q4 might seem fine in casual testing but will randomly fail on complex problems. Meanwhile creative tasks are way more forgiving.&lt;/p&gt; &lt;p&gt;My conclusion: Q5_K_M is the sweet spot - 95%+ quality, fits on 8GB systems, doesn't randomly break on specific tasks.&lt;/p&gt; &lt;p&gt;Now heres my question would anyone actually pay for a tool that analyzes YOUR specific model/use-case and predicts which quantization to use BEFORE downloading 50GB of different formats?&lt;/p&gt; &lt;p&gt;I'm thinking of building this because I'm tired of the trial-and-error, but want to know if it's just me being lazy or an actual problem people would pay to solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Even_Ganache6148"&gt; /u/Even_Ganache6148 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8p844/tested_quantization_on_my_8gb_potato_laptop_heres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:54:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8onns</id>
    <title>I cooked abliterated gemma3-27b-it with norm-preserving technique</title>
    <updated>2025-11-28T07:17:10+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Gemma 3 27B Instruct - Norm-Preserving Abliterated&lt;/h1&gt; &lt;p&gt;I'm excited to share my contribution to the community: a &lt;strong&gt;norm-preserving abliterated version of Google's Gemma 3 27B Instruct&lt;/strong&gt;! Consider it a late Thanksgiving present.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve"&gt;https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model uses the &lt;strong&gt;norm-preserving biprojected abliteration&lt;/strong&gt; technique, which surgically removes refusal mechanisms while preserving reasoning capabilities.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/YanLabs/gemma3-27b-it-abliterated-normpreserve"&gt;YanLabs/gemma3-27b-it-abliterated-normpreserve&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Technique&lt;/strong&gt;: &lt;a href="https://github.com/jim-plus/llm-abliteration"&gt;jim-plus/llm-abliteration&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Hardware&lt;/strong&gt;: Cooked on a rented A100 GPU via RunPod&lt;/p&gt; &lt;p&gt;GGUF files are now available at YanLabs/gemma-3-27b-abliterated-normpreserve-GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/YanLabs/gemma-3-27b-abliterated-normpreserve-GGUF"&gt;https://huggingface.co/YanLabs/gemma-3-27b-abliterated-normpreserve-GGUF&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Disclaimer&lt;/h1&gt; &lt;p&gt;This model has safety guardrails removed. &lt;strong&gt;Research purposes only.&lt;/strong&gt; Use responsibly and in compliance with applicable laws.&lt;/p&gt; &lt;h1&gt;About Me&lt;/h1&gt; &lt;p&gt;I'm an LLM enthusiast and practicing lawyer based in Shanghai. If your AI company needs legal services (domestic or international), feel free to reach out!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;📧 [&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;](mailto:&lt;a href="mailto:ruiqingyan@outlook.com"&gt;ruiqingyan@outlook.com&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Happy experimenting! 🚀&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8onns/i_cooked_abliterated_gemma327bit_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T07:17:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8w9hg</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF · Hugging Face</title>
    <updated>2025-11-28T14:31:50+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF · Hugging Face" src="https://external-preview.redd.it/ble3gnyoRHIxGbCkynVYdB5oBvepM5IUsQgkKmTQPvE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b518c02f5843d8adaaa30fd9ccf229f596ac77a" title="unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8w9hg/unslothqwen3next80ba3bthinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:31:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8hqq4</id>
    <title>Apparently Asus is working with Nvidia on a 784GB "Coherent" Memory desktop PC with 20 PFLOPS AI Performance</title>
    <updated>2025-11-28T00:56:49+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Somehow the announcement went under the radar, but back in May, along side the Ascent GX10, Asus announced the &lt;a href="https://www.asus.com/displays-desktops/workstations/performance/expertcenter-pro-et900n-g3/"&gt;ExpertCenter Pro ET900N G3&lt;/a&gt;, with GB300 Blackwell. They don't really say what's a &amp;quot;Coherent&amp;quot; memory, but my guess it's another term of saying unified memory like Apple and AMD. &lt;/p&gt; &lt;p&gt;The announcement and the specs are very dry on details, but given the GB300, we might get a very decent memory bandwidth, without &lt;a href="https://i.imgur.com/pNaKzWb.png"&gt;looking like a hideous frankestein monster&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;This might be &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; wet dream. If they manage to price it well, and fix that memory bandwidth (that plagued Spark), they have my money. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; As &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5ae77/"&gt;many&lt;/a&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5jvc1/"&gt;pointed out&lt;/a&gt; in the comments, it's based on the &lt;a href="https://www.nvidia.com/en-us/products/workstations/dgx-station/"&gt;Nvidia DGX Station&lt;/a&gt;, announced back in March, which is &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/nr5jvc1/"&gt;rumored to be 80k&lt;/a&gt;. ServeTheHome had a &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1jl4amv/a_closer_look_at_the_nvidia_dgx_station_gb300/"&gt;nice article about it&lt;/a&gt; back in March.&lt;br /&gt; The official specs: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;496GB LPDDR5X CPU memory at 396GB/s (Micron SOCAMM, so it seems that it will be modular not soldered!)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;288GB HBM3e GPU memory at 8TB/s.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8hqq4/apparently_asus_is_working_with_nvidia_on_a_784gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T00:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ven0</id>
    <title>Compared actual usage costs for Chinese AI models. Token efficiency changes everything.</title>
    <updated>2025-11-28T13:54:37+00:00</updated>
    <author>
      <name>/u/YormeSachi</name>
      <uri>https://old.reddit.com/user/YormeSachi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone talks about per-token pricing but nobody mentions token efficiency. How many tokens does it take to complete the same task?&lt;/p&gt; &lt;p&gt;Tested this with coding tasks cause thats where I actually use these models.&lt;/p&gt; &lt;p&gt;glm-4.6: $0.15 input / $0.60 output Kimi K2: $1.50-2.00 MiniMax: $0.80-1.20 deepseek: $0.28&lt;/p&gt; &lt;p&gt;deepseek looks cheapest on paper. But thats not the whole story.&lt;/p&gt; &lt;p&gt;Token efficiency (same task):&lt;/p&gt; &lt;p&gt;Gave each model identical coding task: &amp;quot;refactor this component to use hooks, add error handling, write tests&amp;quot;&lt;/p&gt; &lt;p&gt;glm: 8,200 tokens average deepseek: 14,800 tokens average MiniMax: 10,500 tokens average, Kimi: 11,000 tokens average&lt;/p&gt; &lt;p&gt;glm uses 26% fewer tokens than Kimi, 45% fewer than deepseek.&lt;/p&gt; &lt;p&gt;Real cost for that task:&lt;/p&gt; &lt;p&gt;glm: ~$0.04 (4 cents) deepseek: ~$0.03 (3 cents) - looks cheaper MiniMax: ~$0.05 (5 cents) Kimi: ~$0.09 (9 cents)&lt;/p&gt; &lt;p&gt;But wait. If you do 100 similar tasks:&lt;/p&gt; &lt;p&gt;glm: Total tokens needed: ~820K, Cost: $0.40-0.50 deepseek: Total tokens needed: ~1.48M, Cost: $0.41 - basically same as glm despite lower per-token price MiniMax: Total tokens needed: ~1.05M, Cost: $0.50-0.60 Kimi: Total tokens needed: ~1.1M, Cost: $0.90-1.00&lt;/p&gt; &lt;p&gt;Token efficiency beats per-token price. glm generates less verbose code, fewer explanatory comments, tighter solutions. deepseek tends to over-explain and generate longer outputs.&lt;/p&gt; &lt;p&gt;For businesses doing thousands of API calls daily, glms efficiency compounds into real savings even though its not the absolute cheapest per-token.&lt;/p&gt; &lt;p&gt;Switched to glm for production workloads. Monthly costs dropped 60% vs previous setup. Performance is adequate for 90% of tasks.&lt;/p&gt; &lt;p&gt;deepseeks pricing looks great until you realize youre using 50% more tokens per task. The savings disappear.&lt;/p&gt; &lt;p&gt;Anyone else measuring token efficiency? Feel like this is the underrated metric everyone ignores.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YormeSachi"&gt; /u/YormeSachi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ven0/compared_actual_usage_costs_for_chinese_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8wcbn</id>
    <title>Ask me to run models</title>
    <updated>2025-11-28T14:35:08+00:00</updated>
    <author>
      <name>/u/monoidconcat</name>
      <uri>https://old.reddit.com/user/monoidconcat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"&gt; &lt;img alt="Ask me to run models" src="https://b.thumbs.redditmedia.com/SlyBYDnPqUybVrSsHIbiqHHjqXLR7EhbE0-6UyLUGJk.jpg" title="Ask me to run models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I am currently in the process of upgrading my 4×3090 setup to 2×5090 + 1×RTX Pro 6000. As a result, I have all three kinds of cards in the rig temporarily, and I thought it would be a good idea to take some requests for models to run on my machine.&lt;/p&gt; &lt;p&gt;Here is my current setup: - 1× RTX Pro 6000 Blackwell, power limited to 525 W - 2× RTX 5090, power limited to 500 W - 2× RTX 3090, power limited to 280 W - WRX80E (PCIe 4.0 x16) with 3975WX - 512 GB DDR4 RAM&lt;/p&gt; &lt;p&gt;If you have any model that you want me to run with a specific setup (certain cards, parallelism methods, etc.), let me know in the comments. I’ll run them this weekend and reply with the tok/s!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/monoidconcat"&gt; /u/monoidconcat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8wcbn"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8wcbn/ask_me_to_run_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T14:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8s7az</id>
    <title>Model: Qwen3 Next by pwilkin · Pull Request #16095 · ggml-org/llama.cpp</title>
    <updated>2025-11-28T11:05:36+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"&gt; &lt;img alt="Model: Qwen3 Next by pwilkin · Pull Request #16095 · ggml-org/llama.cpp" src="https://external-preview.redd.it/GTfGIM6FaPx4w5_-UCOwiPgKZNkGDkC0q-Pvot4uDk0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45f974480724f58cdf70046026bd0ccf7e6b00f6" title="Model: Qwen3 Next by pwilkin · Pull Request #16095 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;and it's done&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8s7az/model_qwen3_next_by_pwilkin_pull_request_16095/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T11:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8v9y9</id>
    <title>unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF · Hugging Face</title>
    <updated>2025-11-28T13:48:28+00:00</updated>
    <author>
      <name>/u/WhaleFactory</name>
      <uri>https://old.reddit.com/user/WhaleFactory</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt; &lt;img alt="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF · Hugging Face" src="https://external-preview.redd.it/SSIhbD5Dl8kZRyNgV0oqxKpaE8kMvA_ZXLBFpkDEq90.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1291833f6c1644105b326fbe9244666f7b478451" title="unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WhaleFactory"&gt; /u/WhaleFactory &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8v9y9/unslothqwen3next80ba3binstructgguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-28T13:48:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax — Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I’m Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; — Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; — Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; — LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
