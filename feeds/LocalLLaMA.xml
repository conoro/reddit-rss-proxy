<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-08T22:23:37+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1or4q4m</id>
    <title>Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis</title>
    <updated>2025-11-07T20:04:53+00:00</updated>
    <author>
      <name>/u/teatime1983</name>
      <uri>https://old.reddit.com/user/teatime1983</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt; &lt;img alt="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" src="https://b.thumbs.redditmedia.com/rRHDbssmheavBuel7zlrGe8NTyudGIzlKqGjsplYEaI.jpg" title="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e"&gt;https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Kimi K2 Thinking API pricing is $0.60 per million input tokens and $2.50 per million output tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teatime1983"&gt; /u/teatime1983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1or5j9z</id>
    <title>Nvidia may cancel the RTX 50 Super due to a shortage of 3GB GDDR7 memory</title>
    <updated>2025-11-07T20:36:28+00:00</updated>
    <author>
      <name>/u/Spiderboyz1</name>
      <uri>https://old.reddit.com/user/Spiderboyz1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For now it's just a rumor, but it seems the RTX Super cards will take a while to be released, if they ever are&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout"&gt;https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/"&gt;https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we also have RAM prices skyrocketing due to high demand&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiderboyz1"&gt; /u/Spiderboyz1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1orv5tj</id>
    <title>Confused about settings for my locally run model.</title>
    <updated>2025-11-08T17:27:56+00:00</updated>
    <author>
      <name>/u/FaceplantMurphy</name>
      <uri>https://old.reddit.com/user/FaceplantMurphy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Short and sweet. Very new to this. Im using LM studio to run my model, docker to pipe it to open webui. Between LM studio, and Open WebUI theres so many places to adjust settings. Things like top p, top k, temp, system prompts, etc. What Im trying to figure out is WHERE those settings need to live. Also, the default settings in Open WebUI have me a bit confused. Does default mean it defaults to LM Studios setting, or does default mean a specific default setting? Take Temperature for example. If I leave the default setting temperature in Open WebUI as default, does it default to LM studio or is the default setting say 9? Sorry for stupid questions, and thanks for any help you can offer this supernoob.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FaceplantMurphy"&gt; /u/FaceplantMurphy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orv5tj/confused_about_settings_for_my_locally_run_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orv5tj/confused_about_settings_for_my_locally_run_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orv5tj/confused_about_settings_for_my_locally_run_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T17:27:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortkiq</id>
    <title>Tips for someone new starting out on tinkering and self hosting LLMs</title>
    <updated>2025-11-08T16:23:33+00:00</updated>
    <author>
      <name>/u/MushroomDull4699</name>
      <uri>https://old.reddit.com/user/MushroomDull4699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, im fairly new to this and i got interested after bumping into Alex Ziskind‚Äôs video on recommend in a youtube channel. &lt;/p&gt; &lt;p&gt;I am a consultant here in SouthEast Asia who‚Äôs not fairly techy, but i use LLM‚Äôs a lot and i‚Äôve built my own pc 3x before (i play games on console and pc on a regular).&lt;/p&gt; &lt;p&gt;I plan to build or purchase a decent setup with a $3,000 busget that‚Äôs relatively future proof over the next 12-18 months and study python over the next 6 months (i have zero coding experience, but i believe studying python would help me go down this rabbit hole further)&lt;/p&gt; &lt;p&gt;I‚Äôm like just 2hrs away from Shenzhen and i‚Äôm looking to either buy parts and build my own setup or have one just built there with the ryzan ai max+395 128gb.&lt;/p&gt; &lt;p&gt;Is this a good plan? Or should i look at a different setup with my budget as well as study a different coding language?&lt;/p&gt; &lt;p&gt;I‚Äôm excited and i appreciate any tips and suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MushroomDull4699"&gt; /u/MushroomDull4699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortkiq/tips_for_someone_new_starting_out_on_tinkering/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortkiq/tips_for_someone_new_starting_out_on_tinkering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortkiq/tips_for_someone_new_starting_out_on_tinkering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1or8ehk</id>
    <title>Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)</title>
    <updated>2025-11-07T22:30:30+00:00</updated>
    <author>
      <name>/u/averagebear_003</name>
      <uri>https://old.reddit.com/user/averagebear_003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt; &lt;img alt="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" src="https://b.thumbs.redditmedia.com/UzJABCvVD7NHRog60608GSepBAlbSYFvvfYwhDULgvQ.jpg" title="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/averagebear_003"&gt; /u/averagebear_003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1or8ehk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T22:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as ‚ÄòAmerican Manufacturing‚Äô Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as ‚ÄúAmerican manufacturing.‚Äù&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government help‚Ä¶ Sam lied. Again.Ôøº&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwcc8</id>
    <title>Kimi K2 Thinking: Is there currently a vLLM/sgLang solution to tool calling hallucinations?</title>
    <updated>2025-11-08T18:14:58+00:00</updated>
    <author>
      <name>/u/mborysow</name>
      <uri>https://old.reddit.com/user/mborysow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just want to know if anyone has managed to get it running with sgLang or vLLM with tool calling working decently.&lt;/p&gt; &lt;p&gt;It seems like it's just a known issue, but it makes it totally unsuitable for things like Roo Code / Aider. I understand the fix is basically an enforced grammar for the tool calling section, which is what Kimi claims they do on their API. Hopefully that will come soon. We have limited resources to run models, so if it can't also do tool calling we need to save room for something else. :(&lt;/p&gt; &lt;p&gt;Seems like an awesome model.&lt;/p&gt; &lt;p&gt;For reference:&lt;br /&gt; &lt;a href="https://blog.vllm.ai/2025/10/28/Kimi-K2-Accuracy.html"&gt;https://blog.vllm.ai/2025/10/28/Kimi-K2-Accuracy.html&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/MoonshotAI/K2-Vendor-Verifier"&gt;https://github.com/MoonshotAI/K2-Vendor-Verifier&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't remember if it was vLLM or sglang for this run, but:&lt;br /&gt; {&lt;/p&gt; &lt;p&gt;&amp;quot;model&amp;quot;: &amp;quot;kimi-k2-thinking&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;success_count&amp;quot;: 1998,&lt;/p&gt; &lt;p&gt;&amp;quot;failure_count&amp;quot;: 2,&lt;/p&gt; &lt;p&gt;&amp;quot;finish_stop&amp;quot;: 941,&lt;/p&gt; &lt;p&gt;&amp;quot;finish_tool_calls&amp;quot;: 1010,&lt;/p&gt; &lt;p&gt;&amp;quot;finish_others&amp;quot;: 47,&lt;/p&gt; &lt;p&gt;&amp;quot;finish_others_detail&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;length&amp;quot;: 47&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;&amp;quot;schema_validation_error_count&amp;quot;: 34,&lt;/p&gt; &lt;p&gt;&amp;quot;successful_tool_call_count&amp;quot;: 976&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mborysow"&gt; /u/mborysow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwcc8/kimi_k2_thinking_is_there_currently_a_vllmsglang/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwcc8/kimi_k2_thinking_is_there_currently_a_vllmsglang/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwcc8/kimi_k2_thinking_is_there_currently_a_vllmsglang/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ors0ja</id>
    <title>Anyone actually coded with Kimi K2 Thinking?</title>
    <updated>2025-11-08T15:20:38+00:00</updated>
    <author>
      <name>/u/Federal_Spend2412</name>
      <uri>https://old.reddit.com/user/Federal_Spend2412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious how its debug skills and long-context feel next to Claude 4.5 Sonnet‚Äîbetter, worse, or just hype? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Federal_Spend2412"&gt; /u/Federal_Spend2412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ors0ja/anyone_actually_coded_with_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ors0ja/anyone_actually_coded_with_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ors0ja/anyone_actually_coded_with_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T15:20:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oro9ng</id>
    <title>ROCm(6.4, using latest LLVM) vs ROCm 7 (lemonade sdk)</title>
    <updated>2025-11-08T12:32:29+00:00</updated>
    <author>
      <name>/u/CyBerDreadWing</name>
      <uri>https://old.reddit.com/user/CyBerDreadWing</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One observation I would like to paste in here:&lt;/p&gt; &lt;p&gt;By building llama.cpp with ROCm from scratch (HIP SDK version 6.4), I was able to get more performance than lemonade sdk for ROCm 7.&lt;/p&gt; &lt;p&gt;FYI: I keep changing path of llama.cpp so on first run path was given to ROCm 7 and on second run path was given to ROCm 6.4&lt;/p&gt; &lt;p&gt;Here are some sample outputs:&lt;br /&gt; ROCm 7:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\dreadwing\.lmstudio\models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF&amp;gt; llama-bench -m .\Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf -ub 2048 -b 2048 -ngl 99 -t 16 --n-cpu-moe 2,3,4,5,6,7,8,9,30 -fa on ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | n_cpu_moe | threads | n_ubatch | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------: | -------: | --------------: | -------------------: | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 2 | 16 | 2048 | pp512 | 247.95 ¬± 9.81 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 2 | 16 | 2048 | tg128 | 7.03 ¬± 0.18 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 3 | 16 | 2048 | pp512 | 243.92 ¬± 8.31 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 3 | 16 | 2048 | tg128 | 5.37 ¬± 0.19 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 4 | 16 | 2048 | pp512 | 339.53 ¬± 15.05 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 4 | 16 | 2048 | tg128 | 4.31 ¬± 0.09 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | pp512 | 322.23 ¬± 23.39 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | tg128 | 3.71 ¬± 0.15 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | pp512 | 389.06 ¬± 27.76 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | tg128 | 3.02 ¬± 0.16 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 7 | 16 | 2048 | pp512 | 385.10 ¬± 46.43 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 7 | 16 | 2048 | tg128 | 2.75 ¬± 0.08 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 8 | 16 | 2048 | pp512 | 374.84 ¬± 59.77 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;ROCm 6.4 ( which I build using latest llvm):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\dreadwing\.lmstudio\models\lmstudio-community\Qwen3-Coder-30B-A3B-Instruct-GGUF&amp;gt; llama-bench -m .\Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf -ub 2048 -b 2048 -ngl 99 -t 16 --n-cpu-moe 6,5,30 -fa on ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon RX 7900 GRE, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | n_cpu_moe | threads | n_ubatch | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------: | -------: | --------------: | -------------------: | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | pp512 | 229.92 ¬± 12.49 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 6 | 16 | 2048 | tg128 | 15.69 ¬± 0.10 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | pp512 | 338.65 ¬± 30.11 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 5 | 16 | 2048 | tg128 | 15.20 ¬± 0.04 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 30 | 16 | 2048 | pp512 | 206.16 ¬± 65.14 | | qwen3moe 30B.A3B Q8_0 | 30.25 GiB | 30.53 B | ROCm | 99 | 30 | 16 | 2048 | tg128 | 21.28 ¬± 0.07 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Can someone please explain why this is happening, (ROCm 7 is still in beta for windows, but thats my hard guess).&lt;/p&gt; &lt;p&gt;I am still figuring out TheRock build and vulkan build and will soon benchmark them as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CyBerDreadWing"&gt; /u/CyBerDreadWing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oro9ng/rocm64_using_latest_llvm_vs_rocm_7_lemonade_sdk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T12:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwepr</id>
    <title>How does ChatGPT know when to use web search? Is it using tool calling underneath?</title>
    <updated>2025-11-08T18:17:32+00:00</updated>
    <author>
      <name>/u/Mohamed_SickitLearn</name>
      <uri>https://old.reddit.com/user/Mohamed_SickitLearn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm an AI engineer curious about the internal decision process behind ChatGPT‚Äôs web-search usage. From a systems perspective, does it rely on &lt;em&gt;learned tool calling&lt;/em&gt; (like function-calling tokens) or an external &lt;em&gt;controller&lt;/em&gt; that decides based on confidence and query type?&lt;/p&gt; &lt;p&gt;more importantly, the latency to decide if websearch is needed &amp;lt;100 ms.&lt;br /&gt; In other words, when ChatGPT automatically performs a web search ‚Äî is that triggered by the model itself predicting a &lt;code&gt;web_search&lt;/code&gt; tool call, or by a separate orchestration layer that analyzes the query (e.g., time-sensitive, entity rarity, uncertainty) and routes it?&lt;/p&gt; &lt;p&gt;Would love to hear insights from others who‚Äôve worked on LLM orchestration, tool-use pipelines, or retrieval controllers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mohamed_SickitLearn"&gt; /u/Mohamed_SickitLearn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwepr/how_does_chatgpt_know_when_to_use_web_search_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwepr/how_does_chatgpt_know_when_to_use_web_search_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwepr/how_does_chatgpt_know_when_to_use_web_search_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1orrddh</id>
    <title>Minimax M2 Coding Plan Pricing Revealed</title>
    <updated>2025-11-08T14:54:27+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"&gt; &lt;img alt="Minimax M2 Coding Plan Pricing Revealed" src="https://a.thumbs.redditmedia.com/8aEeUqxK1pswvd_byPWSO7mB5CnEhSUGwY6HlasVrB8.jpg" title="Minimax M2 Coding Plan Pricing Revealed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/frggt9tkp10g1.png?width=1120&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f28f09c1d3058be07b78d6f2f36a44b1f87608b4"&gt;https://preview.redd.it/frggt9tkp10g1.png?width=1120&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f28f09c1d3058be07b78d6f2f36a44b1f87608b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recieved the following in my user notifications on the minimax platform website. Here's the main portion of interest, in text form:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding Plans (Available Nov 10)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Starter:&lt;/strong&gt; $10/ month&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pro:&lt;/strong&gt; $20 / month&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Max:&lt;/strong&gt; $50 / month&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The coding plan pricing seems a lot more expensive than what was previously rumored. Usage provided is currently unknown, but I believe it was supposed to be &amp;quot;5x&amp;quot; the equivalent claude plans, but those rumors also said they were supposed to cost 20% of claude for the pro plan equivalent, and 8% for the other two max plans.&lt;/p&gt; &lt;p&gt;Seems to be a direct competitor to GLM coding plans, but I'm not sure how well this will pan out with those plans being as cheap as $3 a month for first month/quarter/year, and both offering similarly strong models. Chutes is also a strong contendor since they are able to offer both GLM and minimax models, and now K2 thinking as well at fairly cheap plans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orrddh/minimax_m2_coding_plan_pricing_revealed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T14:54:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1os187j</id>
    <title>My Dual MBP setup for offline LLM coding (w/ Qwen3 Coder 30B A3B)</title>
    <updated>2025-11-08T21:35:45+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People here often tout about dual GPUs. And here I am, showing my dual Macbooks setup :P jk jk, stay with me, don't laugh.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M2 Max macbook, with 64GB unified memory for serving LLM via LMStudio&lt;/li&gt; &lt;li&gt;M1 Pro macbook, with 16GB unified memory (doesn't matter), as a client, running Claude Code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model I'm using is Qwen3 Coder 30B A3B, Q8 MLX (temp = 0.1, repeat penalty = 1.05, top k = 20, context size = 51200). To my surprise, both the code quality and the stability in Claude Code was so good. &lt;/p&gt; &lt;p&gt;I've been trying 32B models for coding previously when QwQ 32 and Qwen2.5 Coder was still around, and none of them work. With Qwen3, it makes me feel like we finally have some actual-useful offline model that I can be happy working with.&lt;/p&gt; &lt;p&gt;Now back to the dual MBP setup, you may ask, why? The main thing is the 64GB MBP, running in clam shell and its only job is for the LLM inference, not doing anything else, so I can ultilize a bit more memory for the Q8 quant instead of Q4.&lt;/p&gt; &lt;p&gt;You can see in the below screenshot, it takes 27GB memory to sit idle with the model loaded, and 47GB during generation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/fTxdDRO.png"&gt;https://i.imgur.com/fTxdDRO.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 2nd macbook is unneccesary, it's just something I have at hand. I can use Claude Code on my phone or a Pi if needed.&lt;/p&gt; &lt;p&gt;Now, on inference performance: If I just chat in LMStudio with Qwen3 Coder, it run really fast. But with Claude Code's fatty system prompt, it took about 2 to 3 seconds for prompt processing per request (not so bad), and token generation was about 56 tok/s, pretty much comfortable to use.&lt;/p&gt; &lt;p&gt;On Qwen3 Coder performance: My main workflow is ask Claude Code to perform some search in the codebase, and answer some of my questions, Qwen3 did very good on this, answer quality usually on par with other frontier LLMs in Cursor. Then I'll write a more detailed instruction for the task and let it edit the code, I find that, the more detailed my prompt, the better Qwen3 generate the code.&lt;/p&gt; &lt;p&gt;The only down side is Claude Code's websearch won't work with this setup. But it can be solved by using MCP, i'm also not relying on web search in CC that much.&lt;/p&gt; &lt;p&gt;When I need to move off the work laptop, I don't know if I want to build a custom PC with a dedicated GPU or just go with a mini PC with unified memory, getting over 24GB VRAM with a dedicated GPU will be costly.&lt;/p&gt; &lt;p&gt;I also heard people say 32B dense model works better than A3B, but slower. I think I will try it at some point, but for now, I'm feel quite comfortable with this setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os187j/my_dual_mbp_setup_for_offline_llm_coding_w_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os187j/my_dual_mbp_setup_for_offline_llm_coding_w_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os187j/my_dual_mbp_setup_for_offline_llm_coding_w_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T21:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwz9w</id>
    <title>Free Week of Observer Max as a thank you to r/LocalLLaMA!</title>
    <updated>2025-11-08T18:40:04+00:00</updated>
    <author>
      <name>/u/Roy3838</name>
      <uri>https://old.reddit.com/user/Roy3838</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwz9w/free_week_of_observer_max_as_a_thank_you_to/"&gt; &lt;img alt="Free Week of Observer Max as a thank you to r/LocalLLaMA!" src="https://preview.redd.it/bx60nlgw3yzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59e391906647963849bc1effefdf39e3fa7502cb" title="Free Week of Observer Max as a thank you to r/LocalLLaMA!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; Stress testing Observer MAX and immediately thought of you guys. &lt;strong&gt;Free unlimited access&lt;/strong&gt; this week to help me find what breaks (and build cool stuff). Fingers crossed my API bill doesn't bankrupt me üòÖ&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I'm Roy, the solo dev behind Observer AI (the open-source tool that lets local LLMs watch your screen and react to stuff).&lt;/p&gt; &lt;p&gt;A few months ago, I nervously posted my rough alpha here, and this community absolutely showed up for me. You gave feedback, starred the repo, built some agents, and honestly made me believe this thing was worth finishing. Observer how has 1k+ GitHub stars and 900+ users, and I genuinely don't think that happens without &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;'s early support.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;So here's what I want to do:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I just launched Observer MAX this week (it's the unlimited everything tier - 24/7 cloud monitoring, premium models... etc). It's normally $80/month, and I know that's steep for most hobbyists. But I want to give away 1 week of MAX access to anyone here who wants to experiment with it. That way you can conveniently try out making some agent builds and later on switch to running them with your local models.&lt;/p&gt; &lt;p&gt;How this will work:&lt;/p&gt; &lt;p&gt;Just comment with a cool micro-agent idea you want to build! It can be anything:&lt;/p&gt; &lt;p&gt;- &amp;quot;Watch my 3D printer and SMS me when it finishes&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Monitor my security cam and log every time my cat walks by&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Be in my zoom meeting and when they say my name it sends me a Whatsapp&amp;quot;&lt;/p&gt; &lt;p&gt;I'll reply with &amp;quot;Great idea! Check your DMs üöÄ&amp;quot; and send you a week of MAX access.&lt;/p&gt; &lt;p&gt;The only ask: If you build something cool, share it back here (or on the Discord)! I'd love to feature community agents, and honestly, seeing what you all build is the best part of this project.&lt;/p&gt; &lt;p&gt;This is genuinely just a thank you. No strings attached. You helped me when &lt;a href="https://github.com/Roy3838/Observer"&gt;Observer&lt;/a&gt; was just a weird idea, and I want to give back now that it's actually... a thing.&lt;/p&gt; &lt;p&gt;Thanks for everything, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ‚ù§Ô∏è&lt;/p&gt; &lt;p&gt;Roy&lt;/p&gt; &lt;p&gt;EDIT: added TLDR&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Roy3838"&gt; /u/Roy3838 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bx60nlgw3yzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwz9w/free_week_of_observer_max_as_a_thank_you_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwz9w/free_week_of_observer_max_as_a_thank_you_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1orn6q4</id>
    <title>Handy : Free, Offline AI dictation app for PC, supports Whisper and Parakeet models</title>
    <updated>2025-11-08T11:32:08+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Handy is a trending GitHub repo which is a free alternate for Wispr Flow for AI dictation. The app size is quite small and it supports all Parakeet (nvidia) and Whisper model for speech to text. &lt;/p&gt; &lt;p&gt;GitHub : &lt;a href="https://github.com/cjpais/Handy"&gt;https://github.com/cjpais/Handy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo : &lt;a href="https://youtu.be/1QzXdhVeOkI?si=yli8cfejvOy3ERbo"&gt;https://youtu.be/1QzXdhVeOkI?si=yli8cfejvOy3ERbo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orn6q4/handy_free_offline_ai_dictation_app_for_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwirm</id>
    <title>AesCoder 4B Debuts as the Top WebDev Model on Design Arena</title>
    <updated>2025-11-08T18:21:59+00:00</updated>
    <author>
      <name>/u/Interesting-Gur4782</name>
      <uri>https://old.reddit.com/user/Interesting-Gur4782</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwirm/aescoder_4b_debuts_as_the_top_webdev_model_on/"&gt; &lt;img alt="AesCoder 4B Debuts as the Top WebDev Model on Design Arena" src="https://b.thumbs.redditmedia.com/9og-ZJdy-7J6BnWKjC_HV5d6SqUnzAu6jhyok1kp1xY.jpg" title="AesCoder 4B Debuts as the Top WebDev Model on Design Arena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was messing around earlier today and saw a pretty strong model come up in some of my tournaments. Based on the UI and dark mode look I thought it was a GPT endpoint, but when I finished voting it came up as AesCoder-4B. I got curious so I took a look at its leaderboard rank and saw it was in the top 10 by elo for webdev and had the best elo vs speed ranking -- even better than GLM 4.6 / all of the GPT endpoints / Sonnet 4.5 and 4.5 thinking.&lt;/p&gt; &lt;p&gt;Then I looked the model up on hugging face. Turns out this is a 4 BILLION PARAMETER OPEN WEIGHT MODEL. For context, its closest open weight peer GLM 4.6 is 355 billion parameters, and Sonnet 4.5 / GPT 5 would be in the TRILLIONS TO TENS OF TRILLIONS OF PARAMETERS. WTAF?!!!?! Where did this come from and how have I never heard of it??&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hgh9vc7s20g1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25943c01f0aad33add77c22c4f5050ab363aca38"&gt;https://preview.redd.it/6hgh9vc7s20g1.png?width=1652&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=25943c01f0aad33add77c22c4f5050ab363aca38&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Gur4782"&gt; /u/Interesting-Gur4782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwirm/aescoder_4b_debuts_as_the_top_webdev_model_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwirm/aescoder_4b_debuts_as_the_top_webdev_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwirm/aescoder_4b_debuts_as_the_top_webdev_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:21:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1orucf6</id>
    <title>Current SOTA coding model at around 30-70B?</title>
    <updated>2025-11-08T16:55:05+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What's the current SOTA model at around 30-70B for coding right now? I'm curious smth I can prob fine tune on a 1xH100 ideally, I got a pretty big coding dataset that I grinded up myself.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orucf6/current_sota_coding_model_at_around_3070b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orucf6/current_sota_coding_model_at_around_3070b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orucf6/current_sota_coding_model_at_around_3070b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:55:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1orter8</id>
    <title>Added Kimi-K2-Thinking to the UGI-Leaderboard</title>
    <updated>2025-11-08T16:17:13+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"&gt; &lt;img alt="Added Kimi-K2-Thinking to the UGI-Leaderboard" src="https://preview.redd.it/9kogdlk5620g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be57e7458c6ad9fc1ebe6278bbb2b316a0b4ba9" title="Added Kimi-K2-Thinking to the UGI-Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9kogdlk5620g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1orusbs</id>
    <title>Here comes another bubble (AI edition)</title>
    <updated>2025-11-08T17:12:45+00:00</updated>
    <author>
      <name>/u/Parking-Recipe-9003</name>
      <uri>https://old.reddit.com/user/Parking-Recipe-9003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"&gt; &lt;img alt="Here comes another bubble (AI edition)" src="https://external-preview.redd.it/NDBoMGRhcjBnMjBnMeOwxEKY_BwUmvv0yJlvuSQnrkHkZJuTTKSVmRt4UrhV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50d9634803859b13dbbfac1614c7af5d9f6931e2" title="Here comes another bubble (AI edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking-Recipe-9003"&gt; /u/Parking-Recipe-9003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bnjv3qq0g20g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T17:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1orlqmh</id>
    <title>Honey we shrunk MiniMax M2</title>
    <updated>2025-11-08T10:04:06+00:00</updated>
    <author>
      <name>/u/arjunainfinity</name>
      <uri>https://old.reddit.com/user/arjunainfinity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt; &lt;img alt="Honey we shrunk MiniMax M2" src="https://external-preview.redd.it/ehjUFeUe3VAS2s784qCwiz1JNp5TRqMKwWsfk9mVQNM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a556feed177eef7b3a5785bd56ea678c06d8ea4e" title="Honey we shrunk MiniMax M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, we pruned MiniMax M2 from 250B to 192B (~25%) with only ~5% loss in coding quality. We did this with $200 worth of 8XH200 compute. Our 50% pruned model is ETA 5 more days. Would love to hear your feedback and would you want a 50% pruned Kimi K2 Thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arjunainfinity"&gt; /u/arjunainfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/VibeStudio/thrift"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T10:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwvvj</id>
    <title>Another day, another model - But does it really matter to everyday users?</title>
    <updated>2025-11-08T18:36:18+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"&gt; &lt;img alt="Another day, another model - But does it really matter to everyday users?" src="https://preview.redd.it/8tf3l0pyu20g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3a59b3b2fd820136d7af927e2e8fb8f5f6c1611" title="Another day, another model - But does it really matter to everyday users?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We see new models dropping almost every week now, each claiming to beat the previous ones on benchmarks. Kimi 2 (the new thinking model from Chinese company Moonshot AI) just posted these impressive numbers on Humanity's Last Exam:&lt;/p&gt; &lt;p&gt;Agentic Reasoning Benchmark: - Kimi 2: 44.9&lt;/p&gt; &lt;p&gt;Here's what I've been thinking: For most regular users, benchmarks don't matter anymore.&lt;/p&gt; &lt;p&gt;When I use an AI model, I don't care if it scored 44.9 or 41.7 on some test. I care about one thing: Did it solve MY problem correctly?&lt;/p&gt; &lt;p&gt;The answer quality matters, not which model delivered it.&lt;/p&gt; &lt;p&gt;Sure, developers and researchers obsess over these numbers - and I totally get why. Benchmarks help them understand capabilities, limitations, and progress. That's their job.&lt;/p&gt; &lt;p&gt;But for us? The everyday users who are actually the end consumers of these models? We just want: - Accurate answers - Fast responses&lt;br /&gt; - Solutions that work for our specific use case&lt;/p&gt; &lt;p&gt;Maybe I'm missing something here, but it feels like we're in a weird phase where companies are in a benchmark arms race, while actual users are just vibing with whichever model gets their work done.&lt;/p&gt; &lt;p&gt;What do you think? Am I oversimplifying this, or do benchmarks really not matter much for regular users anymore?&lt;/p&gt; &lt;p&gt;Source: Moonshot AI's Kimi 2 thinking model benchmark results&lt;/p&gt; &lt;p&gt;TL;DR: New models keep topping benchmarks, but users don't care about scores just whether it solves their problem. Benchmarks are for devs; users just want results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8tf3l0pyu20g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:36:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1orsdd9</id>
    <title>Meta‚Äôs AI hidden debt</title>
    <updated>2025-11-08T15:35:16+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"&gt; &lt;img alt="Meta‚Äôs AI hidden debt" src="https://preview.redd.it/a6susixny10g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e81cc9161b2f3c3a8b79ad468df241b09f83883" title="Meta‚Äôs AI hidden debt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta‚Äôs hidden AI debt&lt;/p&gt; &lt;p&gt;Meta has parked $30B in AI infra debt off its balance sheet using SPVs the same financial engineering behind Enron and ‚Äô08.&lt;/p&gt; &lt;p&gt;Morgan Stanley sees tech firms needing $800B in private-credit SPVs by 2028. UBS says AI debt is growing $100B/quarter, raising red flags.&lt;/p&gt; &lt;p&gt;This isn‚Äôt dot-com equity growth it‚Äôs hidden leverage. When chips go obsolete in 3 years instead of 6, and exposure sits in short-term leases, transparency fades and that‚Äôs how bubbles start.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a6susixny10g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T15:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordgys</id>
    <title>We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?</title>
    <updated>2025-11-08T02:15:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt; &lt;img alt="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" src="https://preview.redd.it/qfahc43zzxzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1975edd06526787672ea84d9ae1d9904b84715e1" title="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfahc43zzxzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:15:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1orw0fz</id>
    <title>I've been trying to make a real production service that uses LLM and it turned into a pure agony. Here are some of my "experiences".</title>
    <updated>2025-11-08T18:02:08+00:00</updated>
    <author>
      <name>/u/DaniyarQQQ</name>
      <uri>https://old.reddit.com/user/DaniyarQQQ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I hope this won't be an off topic, but I want to share my experience in creating real production service. Like a real deal, that will earn money. &lt;/p&gt; &lt;p&gt;For this service I've been using &lt;strong&gt;ChatGPT-5&lt;/strong&gt; and &lt;strong&gt;Claude Haiku 4.5&lt;/strong&gt; but I think this could be suitable for other LLMs too. &lt;/p&gt; &lt;p&gt;The idea was as simple as rock. Make an assistant bot that will communicate with people and make a scheduled appointments to the doctor. &lt;/p&gt; &lt;p&gt;Well in a short time I've implemented everything. The vector database that will inject doctor specific knowledge to the conversation at the right time. Multiple tools that will work with doctors data, and couple other integrations. I've extensively made very detailed system prompt, and each tool call returns instructive results. Each tools' parameters' descriptions were written in very detailed way. After testing for a week we finally deployed on production and started to receive conversations from real people.&lt;/p&gt; &lt;p&gt;And then real life had showed a lot of annoying and downright frustrating caveats of these LLMs.&lt;/p&gt; &lt;p&gt;The first frustrating thing is that &lt;strong&gt;LLMs makes an assumptions&lt;/strong&gt; &lt;strong&gt;without calling required tool, which deceives people.&lt;/strong&gt; It happened like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Please give me an address where this doctor will be on tomorrow. LLM: Tomorrow is sunday, which is weekend, doctor is unavalable. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is a tool that explicitly returns that address, and doctor actually works on Sundays. It did not call that tool and made up a bullshit excuse. Then I have emulated this question again by myself:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Me: Give me address where this doctor will be on tomorrow. LLM: &amp;lt;DID NOT CALL THE TOOL&amp;gt;. Tomorrow is sunday, which is weekend, doctor is unavalable. Me: Are you sure about that? LLM: &amp;lt;Finally starts calling the tool which returns address for tomorrow and outputs this address.&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This happens always. No matter what kind of prompts you write, telling it not make any assumptions without any tool calls it still made ups bullshit, which deceives people. Even if you explicitly inject the constraints, it fights them and keeps its own bullshit. &lt;/p&gt; &lt;p&gt;Another problem is close to the first one. &lt;strong&gt;LLMs always agrees with requests without calling tools which confuses people.&lt;/strong&gt; Which looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow. Is it possible. LLM: Of course you can make an appointment. I've prepared everything. What time? User: At 18:00 LLM: &amp;lt;Makes a tool call which returns negative result that next day is unavailable&amp;gt;. I'm sorry tomorrow is unavailable. User: WTF? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of asking proper question before agreeing, it agrees and then shits itself, confusing the user. Especially ChatGPT-5 has this problem, Claude is more rarer but still could shit itself.&lt;/p&gt; &lt;p&gt;And another problem is that &lt;strong&gt;LLMs output text which is complete opposite of it's tool results.&lt;/strong&gt; I've seen this only single time, but I'm now getting paranoid that this could be happening for a long time. It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow at 18:00. LLM: &amp;lt;Makes a call to the tool that returns that it is impossible for this user to make an appointment, because user has another pending appointment&amp;gt; LLM: Good. I've scheduled your appointment for tomorrow at 18:00. Don't forget about that &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That was an epic failure, LLM completely lied it's own results. I don't even know what to say about that.&lt;/p&gt; &lt;p&gt;And finally the funny one. &lt;strong&gt;Looks like ChatGPT does not like that tools can return negative results, and it keeps calling until it fully overloads all context and finally shits itself.&lt;/strong&gt; It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for next friday at 18:00 LLM: &amp;lt;Calls a tool for available window for next friday. No available window&amp;gt; LLM: &amp;lt;Calls again this tool but for the next next friday. No available window&amp;gt; LLM: &amp;lt;Cals AGAIN this tool but for the next next next friday. No available window&amp;gt; ------- And so on and so on | By the way, this doctor does not work on fridays, it was explicitly given in a system prompt, but ChatGPT wants to persevere. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These problems are fixable. You can make even more detailed prompts. Make tools return better and understandable results. You can tune some of LLM parameters. However it is game of whack-a-mole, frustrating one. You fix one thing, another thing comes out. I think some of these models, at least ChatGPT and Claude, were so overly trained on positivity, that they generate deceiving or downright wrong results.&lt;/p&gt; &lt;p&gt;Currently It seems to be that these LLMs can at mostly do their jobs correctly, but these fails, even if they happen rarely, are completely negating all of their reliability. It is not a wonderful magic thing that can solve everything. It is very finnicky (and sometimes very frustrating) tool, that maybe can do what you want. You think you have prepared it for everything, but users can make it shit itself just with a single sentence. &lt;/p&gt; &lt;p&gt;At least I've learned a lot, from these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniyarQQQ"&gt; /u/DaniyarQQQ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ormxoq</id>
    <title>Kimi K2 Thinking was trained with only $4.6 million</title>
    <updated>2025-11-08T11:16:59+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt; &lt;img alt="Kimi K2 Thinking was trained with only $4.6 million" src="https://a.thumbs.redditmedia.com/L1ScXXMSwvWG0gMuGlVV23jh1shgAs9PlFDsIKJOn94.jpg" title="Kimi K2 Thinking was trained with only $4.6 million" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI: &amp;quot;We need government support to cover $1.4 trillion in chips and data centers.&amp;quot;&lt;/p&gt; &lt;p&gt;Kimi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09"&gt;https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortopy</id>
    <title>Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs</title>
    <updated>2025-11-08T16:28:21+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" src="https://preview.redd.it/s190tdo2720g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a07ae8f05f136602ecbf12323e286c3cca29d84" title="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! You can now run Kimi K2 Thinking locally with our Unsloth Dynamic 1bit GGUFs. We also collaborated with the Kimi team on a &lt;strong&gt;fix for K2&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking/discussions/12"&gt;&lt;strong&gt;Thinking's chat template&lt;/strong&gt;&lt;/a&gt; not prepending the default system prompt of &lt;code&gt;You are Kimi, an AI assistant created by Moonshot AI.&lt;/code&gt; on the 1st turn.&lt;/p&gt; &lt;p&gt;We also we &lt;strong&gt;fixed llama.cpp custom jinja separators&lt;/strong&gt; for tool calling - Kimi does &lt;code&gt;{&amp;quot;a&amp;quot;:&amp;quot;1&amp;quot;,&amp;quot;b&amp;quot;:&amp;quot;2&amp;quot;}&lt;/code&gt; and not with extra spaces like &lt;code&gt;{&amp;quot;a&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;: &amp;quot;2&amp;quot;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The 1-bit GGUF will run on 247GB RAM. We shrank the 1T model to 245GB (-62%) &amp;amp; the accuracy recovery is comparable to our third-party &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;DeepSeek-V3.1 Aider Polyglot benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All 1bit, 2bit and other bit width GGUFs are at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The suggested temp is &lt;code&gt;temperature = 1.0&lt;/code&gt;. We also suggest a &lt;code&gt;min_p = 0.01&lt;/code&gt;. If you do not see &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;, use &lt;code&gt;--special&lt;/code&gt;. The code for llama-cli is below which offloads MoE layers to CPU RAM, and leaves the rest of the model on GPU VRAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LLAMA_CACHE=&amp;quot;unsloth/Kimi-K2-Thinking-GGUF&amp;quot; ./llama.cpp/llama-cli \ -hf unsloth/Kimi-K2-Thinking-GGUF:UD-TQ1_0 \ --n-gpu-layers 99 \ --temp 1.0 \ --min-p 0.01 \ --ctx-size 16384 \ --seed 3407 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-by-step Guide + fix details: &lt;a href="https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally"&gt;https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally&lt;/a&gt; and GGUFs are &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us know if you have any questions and hope you have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s190tdo2720g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
