<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-20T02:40:30+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob7mcy</id>
    <title>How can I run a VL model on a Smartphone?</title>
    <updated>2025-10-20T01:56:57+00:00</updated>
    <author>
      <name>/u/klop2031</name>
      <uri>https://old.reddit.com/user/klop2031</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know there are several apps that can run VL models, and I know I can compile llama.cpp on my phone and run models, but is there a good interface to perform inference on these models besides the google ai gallery?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klop2031"&gt; /u/klop2031 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7mcy/how_can_i_run_a_vl_model_on_a_smartphone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7mcy/how_can_i_run_a_vl_model_on_a_smartphone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7mcy/how_can_i_run_a_vl_model_on_a_smartphone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T01:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob8a40</id>
    <title>How do you guys generate/prepare your coding datasets?</title>
    <updated>2025-10-20T02:29:45+00:00</updated>
    <author>
      <name>/u/Patience2277</name>
      <uri>https://old.reddit.com/user/Patience2277</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob8a40/how_do_you_guys_generateprepare_your_coding/"&gt; &lt;img alt="How do you guys generate/prepare your coding datasets?" src="https://b.thumbs.redditmedia.com/RfgKizNcCGwk7QTHpq8h2lbfmy9NlMkeU-gtoM9McJI.jpg" title="How do you guys generate/prepare your coding datasets?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly, I'm questioning if I even need to include coding data for my fine-tuning, but I figured I'd ask just in case!&lt;/p&gt; &lt;p&gt;I've used the Claude API and Codex before. Now, I'm considering using &lt;strong&gt;Qwen3-Coder-30B&lt;/strong&gt; for simpler tasks.&lt;/p&gt; &lt;p&gt;What level of complexity/quality should I ask for? (Although, I doubt my own skills are good enough to properly review the output, lol.)&lt;/p&gt; &lt;p&gt;Oh! And here's an update on my progress: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u3a9zne4h6wf1.png?width=1081&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=252c273a4d4d04edaf1d51cc2662c053a513ecbc"&gt;https://preview.redd.it/u3a9zne4h6wf1.png?width=1081&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=252c273a4d4d04edaf1d51cc2662c053a513ecbc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The persona is still unstable, haha. It takes some prompting/persuasion to get it to act the part.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patience2277"&gt; /u/Patience2277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob8a40/how_do_you_guys_generateprepare_your_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob8a40/how_do_you_guys_generateprepare_your_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob8a40/how_do_you_guys_generateprepare_your_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T02:29:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oayijj</id>
    <title>Turn any dataset into a reasoning dataset easily and cheaply</title>
    <updated>2025-10-19T19:22:43+00:00</updated>
    <author>
      <name>/u/ApprehensiveTart3158</name>
      <uri>https://old.reddit.com/user/ApprehensiveTart3158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tldr; this model is tiny but meant for recreating grounded reasoning generation without changing your datasets too much (scroll down for link)&lt;/p&gt; &lt;p&gt;I woke up one day and thought if it is possible to make an LLM (a tiny one, 0.6b!) turn those old but gold chat datasets into reasoning chat datasets, turns out yes it is possible and the results were quite good. &lt;/p&gt; &lt;p&gt;Which allows you to then fine tune a model on those same older but hq datasets but your model would also learn to reason like those big SOTA's. &lt;/p&gt; &lt;p&gt;Tried multiple llms, gemma3 1b, gemma3 270m and qwen3 0.6b, qwen3 0.6b gave me by far the best results and good interference / training speeds. &lt;/p&gt; &lt;p&gt;Tried both the instruct and base variants of this model, yes the base model performed significantly better and did not seem to overfit, it was fine-tuned on 1 epoch of a mixed half gpt OSS half deepseek r1 dataset with the special format the model uses and needs (about 200k rows total)&lt;/p&gt; &lt;p&gt;The model replicates how deepseeek r1 or gpt OSS would think about answering, you provide it the assistant output and user input (exact format on model page) and it would generate plausible grounded reasoning, keep in mind I've decided to almost completely eliminate reasoning about policies (gpt OSS stuff) and censorship biased reasoning while filtering, so it can think about spicy content, but due to limited data in that field you should check how it performs at that, generally deepseek r1 styled reasoning works better at NSFW, but obviously yes if you make it think about a rejection it would reject in the reasoning. &lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://huggingface.co/Pinkstack/syngen-reasoning-0.6b"&gt;https://huggingface.co/Pinkstack/syngen-reasoning-0.6b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also I made a very quick example dataset for you to evaluate how well it replicates reasoning: &lt;a href="https://huggingface.co/datasets/Pinkstack/syngen-reasoning-example-80-smoltalk1"&gt;https://huggingface.co/datasets/Pinkstack/syngen-reasoning-example-80-smoltalk1&lt;/a&gt; usually it does pretty good but as a rule of thumb, if you give it nonsense it would think poorly, feel free to test that though could be funny. &lt;/p&gt; &lt;p&gt;Hopefully this is useful to somebody! üéâ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveTart3158"&gt; /u/ApprehensiveTart3158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayijj/turn_any_dataset_into_a_reasoning_dataset_easily/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayijj/turn_any_dataset_into_a_reasoning_dataset_easily/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oayijj/turn_any_dataset_into_a_reasoning_dataset_easily/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T19:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob1xkz</id>
    <title>LLM for building GUI</title>
    <updated>2025-10-19T21:37:56+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are there any models out there that would be suitable to help build a GUI for an app?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob1xkz/llm_for_building_gui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob1xkz/llm_for_building_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob1xkz/llm_for_building_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T21:37:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oayrh9</id>
    <title>Got new 5070ti gpu, have access to 16gb vram. What things can I do with it for AI?</title>
    <updated>2025-10-19T19:32:21+00:00</updated>
    <author>
      <name>/u/AdOver7835</name>
      <uri>https://old.reddit.com/user/AdOver7835</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had 2050 earlier with 4gb. Curious what new superpowers do I get with this new vram access?&lt;br /&gt; So far&lt;br /&gt; 1. ran gpt-oss 20b in lmstuio\. with upto 30k context window it gives around 40 tok/sec output.&lt;br /&gt; 2. ran gemma-27b. runs around 17 tok/sec&lt;br /&gt; 3. ran qwen3 coder 30b -- rund around 30 tok/sec&lt;/p&gt; &lt;p&gt;Apart from running models locally, I want to do things which earlier I didn't think of. &lt;/p&gt; &lt;p&gt;Planned :&lt;br /&gt; 1. Image generation with flux and automatic1111&lt;br /&gt; 2. want to try openai whisper&lt;br /&gt; 3. want to build ai agents which runs 24*7&lt;/p&gt; &lt;p&gt;last but not the least, complete spiderman 2 on this :)&lt;/p&gt; &lt;p&gt;Please help me with ideas and experimentations, I want to utilize this precious thing as much as possible and upskill myself in AI world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOver7835"&gt; /u/AdOver7835 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T19:32:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavy6f</id>
    <title>Reverse Engineering and Tracing internal thoughts of LLM</title>
    <updated>2025-10-19T17:43:57+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks I did following experiments to understand inner working of LLM&lt;br /&gt; Index of experiments I did in this article (I used LLama 3 1B)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Token Prediction Trace&lt;/li&gt; &lt;li&gt;Attribution Analysis&lt;/li&gt; &lt;li&gt;Layer Emergence (knowledge tracing)&lt;/li&gt; &lt;li&gt;Weight Matrix Analyis (How knowledge encoded in weights)&lt;/li&gt; &lt;li&gt;Dimension Tokens Analysis (which Dimension stored encoded token for ‚Äúparis‚Äù)&lt;/li&gt; &lt;li&gt;Prediction Chain (How does each dimension contribute to final output)&lt;/li&gt; &lt;li&gt;Token‚ÜíNeuron Map (Which neurons encode token)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://medium.com/@harishhacker3010/reverse-engineering-and-tracing-internal-thoughts-of-llm-3017b5f72008"&gt;https://medium.com/@harishhacker3010/reverse-engineering-and-tracing-internal-thoughts-of-llm-3017b5f72008&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob61fg</id>
    <title>CMP 50HX vs P102-100 test results.</title>
    <updated>2025-10-20T00:38:38+00:00</updated>
    <author>
      <name>/u/Boricua-vet</name>
      <uri>https://old.reddit.com/user/Boricua-vet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well, I finally put together the second LLM server as I had mentioned earlier on another post. Here are the results of a pair of P102-100 vs a pair of CMP 50HX. The results are quite the contrast and interesting. In order to simplify the test I used docker, llama-swap and the same configs using 16K context, Q8kv, Unsloth IQ4_NL except for GPT-OSS-20 which I used Q5_K_M and the same prompt across all tests.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GPU-MODEL&lt;/th&gt; &lt;th align="left"&gt;PP&lt;/th&gt; &lt;th align="left"&gt;TG&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-0.6B-GGUF&lt;/td&gt; &lt;td align="left"&gt;5165.73&lt;/td&gt; &lt;td align="left"&gt;143.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-0.6B-GGUF&lt;/td&gt; &lt;td align="left"&gt;3226.96&lt;/td&gt; &lt;td align="left"&gt;195.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-1.7B-GGUF&lt;/td&gt; &lt;td align="left"&gt;2790.78&lt;/td&gt; &lt;td align="left"&gt;110.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-1.7B-GGUF&lt;/td&gt; &lt;td align="left"&gt;1519.72&lt;/td&gt; &lt;td align="left"&gt;137.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-4B-GGUF&lt;/td&gt; &lt;td align="left"&gt;1123.46&lt;/td&gt; &lt;td align="left"&gt;63.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-4B-GGUF&lt;/td&gt; &lt;td align="left"&gt;604.38&lt;/td&gt; &lt;td align="left"&gt;74.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-8B-GGUF&lt;/td&gt; &lt;td align="left"&gt;704.40&lt;/td&gt; &lt;td align="left"&gt;45.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-8B-GGUF&lt;/td&gt; &lt;td align="left"&gt;367.09&lt;/td&gt; &lt;td align="left"&gt;51.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-14B-GGUF&lt;/td&gt; &lt;td align="left"&gt;319.38&lt;/td&gt; &lt;td align="left"&gt;27.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-14B-GGUF&lt;/td&gt; &lt;td align="left"&gt;203.78&lt;/td&gt; &lt;td align="left"&gt;32.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-32B-GGUF&lt;/td&gt; &lt;td align="left"&gt;161.50&lt;/td&gt; &lt;td align="left"&gt;13.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-32B-GGUF&lt;/td&gt; &lt;td align="left"&gt;87.79&lt;/td&gt; &lt;td align="left"&gt;15.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-GLM-4-32B-0414-GGUF&lt;/td&gt; &lt;td align="left"&gt;174.58&lt;/td&gt; &lt;td align="left"&gt;14.25&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-GLM-4-32B-0414-GGUF&lt;/td&gt; &lt;td align="left"&gt;89.46&lt;/td&gt; &lt;td align="left"&gt;16.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-gpt-oss-20b-GGUF&lt;/td&gt; &lt;td align="left"&gt;929.58&lt;/td&gt; &lt;td align="left"&gt;58.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-gpt-oss-20b-GGUF&lt;/td&gt; &lt;td align="left"&gt;376.16&lt;/td&gt; &lt;td align="left"&gt;72.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P102-Qwen3-30B-A3B-GGUF&lt;/td&gt; &lt;td align="left"&gt;803.81&lt;/td&gt; &lt;td align="left"&gt;54.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;50HX-Qwen3-30B-A3B-GGUF&lt;/td&gt; &lt;td align="left"&gt;291.01&lt;/td&gt; &lt;td align="left"&gt;70.52&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see a pattern emerges, Turing is better at TG and Pascal is better at PP. The key reasons for that are...&lt;/p&gt; &lt;p&gt;1- Turing has a lower double precision throughput than Volta with only 2 FP64 cores.&lt;/p&gt; &lt;p&gt;2- Turing FMA math operations is four clock cycles, like Volta, compared to six cycles on Pascal.&lt;/p&gt; &lt;p&gt;3- The maximum number of concurrent warps per SM is 32 on Turing vs 64.&lt;/p&gt; &lt;p&gt;However, what is impressive is the 72 tk/s on the 50hx on GPT-OSS and 70 on Qwen3-30B-A3B and basically 16tk/s on Qwen32. Those are not slow numbers for a 150 dollar investment. There are cards that cost a whole lot more of give and you less performance when it comes to LLM. I would certainly not use these cards for image or video gen but I am curious about these 50HX working on exllamav2 or v3 since they are 7.5 which are supposedly supported and I might get tensor parallel working on these. I guess that is the next challenge.&lt;/p&gt; &lt;p&gt;In conclusion, because of the drastic loss of PP on the 50hx, even though it does TG faster than the P102-100 the PP rate drop is too high for my taste so I might drop these 50HX and get something a little better if the price is right. For now, I will keep rocking the dual P102-100 which has served me so well. I do have wishful thinking on a pair of Mi50 32GB versions. Someday I will see some on ebay for a 100 bucks each, and I will pull the trigger.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Boricua-vet"&gt; /u/Boricua-vet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob61fg/cmp_50hx_vs_p102100_test_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob61fg/cmp_50hx_vs_p102100_test_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob61fg/cmp_50hx_vs_p102100_test_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T00:38:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob61vf</id>
    <title>Testing Pre-release(before nerfed) Gemini 3.0 pro (OrionMist) entire 1 min cartoon , zero shot , only svg animation + code</title>
    <updated>2025-10-20T00:39:14+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob61vf/testing_prereleasebefore_nerfed_gemini_30_pro/"&gt; &lt;img alt="Testing Pre-release(before nerfed) Gemini 3.0 pro (OrionMist) entire 1 min cartoon , zero shot , only svg animation + code" src="https://external-preview.redd.it/c2x5eHdpYmh4NXdmMa8-WRyYQkv4w-V-pO7BQ_INA615q8OCX1IxZNhnrLGC.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c20ffe8a07a6ad6882f6dc16f31fc3c946cf3472" title="Testing Pre-release(before nerfed) Gemini 3.0 pro (OrionMist) entire 1 min cartoon , zero shot , only svg animation + code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5pgfmriex5wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob61vf/testing_prereleasebefore_nerfed_gemini_30_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob61vf/testing_prereleasebefore_nerfed_gemini_30_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T00:39:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oatip1</id>
    <title>If the bubble really pops how can that affect local AI models?</title>
    <updated>2025-10-19T16:08:41+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If all this AI bubble talk really comes to an popa after all, how might this affect the development of more local AI models? From what I've seen MoE models still outperforms most models easily, but creating models is still expensive as shit, rather for the planet than their pocket, donation exists anyways.&lt;/p&gt; &lt;p&gt;But the servers these models use to be trained consumes a shitton of load, and I could imagine most big company servers not allowing AI to be trained on their servers anymore considering the massive amounts of models being released every week. Do you think AI would immediately freeze in advancement upon a bubble pop making us have to wait more 80 years for an actual AGI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T16:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oauxgg</id>
    <title>lazylms - TUI for LM Studio</title>
    <updated>2025-10-19T17:04:04+00:00</updated>
    <author>
      <name>/u/Rugs007</name>
      <uri>https://old.reddit.com/user/Rugs007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"&gt; &lt;img alt="lazylms - TUI for LM Studio" src="https://b.thumbs.redditmedia.com/HpahzCgyFc5ZYpuDsGsueGlJc11EMlLdwC_J2jGKOvc.jpg" title="lazylms - TUI for LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I made a TUI for using LM Studio by staying in the terminal. This is a hobby side project, MIT licensed and uses the CLI and REST API. Feel free to give it a try. This is inspired by lazygit and lazydocker. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Rugz007/lazylms"&gt;https://github.com/Rugz007/lazylms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rugs007"&gt; /u/Rugs007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oauxgg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob17go</id>
    <title>I made a mod of Qwen Code for working with local models in LM Studio</title>
    <updated>2025-10-19T21:08:22+00:00</updated>
    <author>
      <name>/u/feverdream</name>
      <uri>https://old.reddit.com/user/feverdream</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"&gt; &lt;img alt="I made a mod of Qwen Code for working with local models in LM Studio" src="https://external-preview.redd.it/YRVmeTTENyTnVFkxiv6fyP_vZo98Ij9IpjhsuYhArJo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83641b9a64a38f5187a89aa2294d825e59a7f28c" title="I made a mod of Qwen Code for working with local models in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sip3pvr0v4wf1.png?width=1691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9eb322340ffed7a42020ed91ea4f3520b2125ac"&gt;https://preview.redd.it/sip3pvr0v4wf1.png?width=1691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9eb322340ffed7a42020ed91ea4f3520b2125ac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made &lt;a href="https://github.com/dkowitz/LowCal-Code"&gt;LowCal Code&lt;/a&gt; specifically to work with my locally hosted models in LM Studio, and also with the option to use online models through OpenRouter - that's it, those are the only two options with /auth, LM Studio or OpenRouter.&lt;/p&gt; &lt;p&gt;When you use /model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;With LM Studio, it shows you available models to choose from, along with their configured and maximum context sizes (you have to manually configure a model in LM Studio once and set it's context size before it's available in LowCal).&lt;/li&gt; &lt;li&gt;With OpenRouter, it shows available models (hundreds), along with context size and price, and you can filter them. You need an api key.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other local model enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;/promptmode set &amp;lt;full/concise/auto&amp;gt;&lt;/code&gt; &lt;ul&gt; &lt;li&gt;full: full, long system prompt with verbose instructions and lots of examples&lt;/li&gt; &lt;li&gt;concise: short, abbreviated prompt for conserving context space and decreasing latency, particularly for local models. Dynamically constructed to only include instructions/examples for tools from the currently activated /toolset.&lt;/li&gt; &lt;li&gt;auto: automatically uses concise prompt when using LM Studio endpoint and full prompt when using OpenRouter endpoint&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;/toolset (list, show, activate/use, create, add, remove)&lt;/code&gt; - use custom tool collections to exclude tools from being used and saving context space and decreasing latency, particularly with local models. Using the shell tool is often more efficient than using file tools. &lt;ul&gt; &lt;li&gt;list: list available preset tool collections&lt;/li&gt; &lt;li&gt;show : shows which tools are in a collection&lt;/li&gt; &lt;li&gt;activate/use: Use a selected tool collection&lt;/li&gt; &lt;li&gt;create: Create a new tool collection&lt;code&gt;/toolset create &amp;lt;name&amp;gt; [tool1, tool2, ...]&lt;/code&gt; (Use tool names from /tools)&lt;/li&gt; &lt;li&gt;add/remove: add/remove tool to/from a tool collection &lt;code&gt;/toolset add[remove] &amp;lt;name&amp;gt; tool&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;/promptinfo&lt;/code&gt; - Show the current system prompt in a /view window (‚Üë‚Üì to scroll, 'q' to quit viewer).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's made to run efficiently and autonomously with local models, gpt-oss-120, 20, Qwen3-coder-30b, glm-45-air, and others work really well! Honestly I don't see a huge difference in effectiveness between the concise prompt and the huge full system prompt, and often using just the shell tool, or in combination with WebSearch or Edit can be much faster and more effective than many of the other tools.&lt;/p&gt; &lt;p&gt;I developed it to use on my 128gb Strix Halo system on Ubuntu, so I'm not sure it won't be buggy on other platforms (especially Windows).&lt;/p&gt; &lt;p&gt;Let me know what you think! &lt;a href="https://github.com/dkowitz/LowCal-Code"&gt;https://github.com/dkowitz/LowCal-Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/feverdream"&gt; /u/feverdream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T21:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav3r1</id>
    <title>Quantized some MoE models with MXFP4</title>
    <updated>2025-10-19T17:10:57+00:00</updated>
    <author>
      <name>/u/noctrex</name>
      <uri>https://old.reddit.com/user/noctrex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So as I was sitting and trying out some MXFP4_MOE quants from &lt;a href="https://huggingface.co/Face314"&gt;Face314&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/sm54"&gt;sm54&lt;/a&gt;, I can say that liked them very much.&lt;/p&gt; &lt;p&gt;So I thought why not quantize some more this weekend.&lt;/p&gt; &lt;p&gt;Well, here they are:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex"&gt;https://huggingface.co/noctrex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any suggestions or critique welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noctrex"&gt; /u/noctrex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakrdm</id>
    <title>Drop your underrated models you run LOCALLY</title>
    <updated>2025-10-19T08:52:22+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably within the 0.2b -32b range, or MoEs up to 140b&lt;/p&gt; &lt;p&gt;I‚Äôm on a LLM downloading spree, and wanna fill up a 2tb SSD with them. &lt;/p&gt; &lt;p&gt;Can be any use case. Just make sure to mention the use case too &lt;/p&gt; &lt;p&gt;Thank you ‚úåÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob7q6m</id>
    <title>Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</title>
    <updated>2025-10-20T02:02:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"&gt; &lt;img alt="Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" src="https://external-preview.redd.it/T8a1JuGOfWYN7yWqfBi5-bruC3MzoVLZu36ygPTxd0o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264bdb16d9e4730080c65c21ffa671e23a0de176" title="Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/omnivinci"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T02:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav4hi</id>
    <title>Two new Google models, "lithiumflow" and "orionmist", have been added to LMArena. This is Google's naming scheme and "orion" has been used internally with Gemini 3 codenames, so these are likely Gemini 3 models</title>
    <updated>2025-10-19T17:11:45+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist"&gt;https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oamo0k</id>
    <title>Gemma 4</title>
    <updated>2025-10-19T10:53:39+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are very very excited about the release of gemini 3.0 including me, but im more excited in the gemma family of models since they are based on gemini models and on top of that are open-sourced. And simce Gemini 3.0 is groundbreaking (apparently, like the pelican svg, robot svg, xbox svg, os etc tests), I am very curious about how will the gemma 4 models perform. And also, gemma 4 is going to be a big leap compared to gemma 3 coz It was based on gemini 2.0, not 2.5. So we are getting 2 genarational leaps!&lt;/p&gt; &lt;p&gt;When it will be released??&lt;/p&gt; &lt;p&gt;Gemma 1 was based on gemini 1 and was released ~1-2 months after gemini&lt;/p&gt; &lt;p&gt;Gemma 2 was based on gemini 1.5 and was released ~4 months after gemini 1.5&lt;/p&gt; &lt;p&gt;Gemma 3 was based on gemini 2 and was released ~1-2 months after gemini 2.0&lt;/p&gt; &lt;p&gt;So Gemma 4 might be released ~1-2 months after gemini 3??? Maybe???&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T10:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oayl4j</id>
    <title>Free API Key for GLM 4.6</title>
    <updated>2025-10-19T19:25:35+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, providing a free API for GLM 4.6 for the next 48 hours as part of a load test. Enjoy.&lt;/p&gt; &lt;p&gt;Here are the credentials:&lt;/p&gt; &lt;p&gt;Model Name: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;z-ai/glm-4.6 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Base URL: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;https://api.avian.io/v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;API Key: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;avian-8z-5Qb5tLGS6q_A2j6Z2-iZxD78XnKCuvisEQQswZXw &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T19:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob6ydq</id>
    <title>GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop</title>
    <updated>2025-10-20T01:23:37+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"&gt; &lt;img alt="GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop" src="https://external-preview.redd.it/9USPaHCqnaWZUhhwpPcmVYuxokNlKHBzm3mdxx2L9rE.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c5a9785f74812c007fd190cf17bd9645963730e" title="GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://linuxgizmos.com/gigabyte-ai-top-atom-introduces-nvidia-grace-blackwell-gb10-performance-for-the-desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T01:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready üéâ</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready üéâ" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavxt8</id>
    <title>I built a 1B CAD generator model</title>
    <updated>2025-10-19T17:43:33+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt; &lt;img alt="I built a 1B CAD generator model" src="https://external-preview.redd.it/ZGFhNmE0bzJ2M3dmMdhv6U5XLy0vFYTB3BWLA3H-O3YDxkmUtGbojZ8LN3lz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a21d6d0c153a39bacb389fe42d52137134b86925" title="I built a 1B CAD generator model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a weekend, I decided to build a small language model to generate me 3d files. No reason except for pure curiosity. Here's what I did:&lt;/p&gt; &lt;p&gt;- Gather dataset on OpenSCAD: This turns out to be quite bad because people's code quality is low &amp;amp; in-consistent.&lt;/p&gt; &lt;p&gt;- Generate synthetic data (prompt -&amp;gt; openscad): This was the most wasteful per dollar part. I spent 150$+ on Claude API (70% are on reasoning token). Ended up using Gemma3-12b running in 48 hours continuously.&lt;/p&gt; &lt;p&gt;- Finetune Gemma3-270M, 1B &amp;amp; 4B: 270M lacks fundamental code &amp;amp; object understanding and failed badly. 1B is a good balance between render-ability rate &amp;amp; speed.&lt;/p&gt; &lt;p&gt;Overall, I spent 150$ on Claude (totally wasted) &amp;amp; 25$ on GPU. Both given as credits and grants.&lt;/p&gt; &lt;p&gt;I also made a CLI app if you wanna try on Mac, Linux or Raspberry Pi 4/5: &lt;a href="https://github.com/ThomasVuNguyen/MakeMe"&gt;https://github.com/ThomasVuNguyen/MakeMe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models, dataset &amp;amp; code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasVuNguyen/K"&gt;https://github.com/ThomasVuNguyen/K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b"&gt;https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pn0yo3o2v3wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;/p&gt; &lt;p&gt;1: &lt;a href="https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV"&gt;https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV&lt;/a&gt;&lt;br /&gt; 2: &lt;a href="https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi"&gt;https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi&lt;/a&gt;&lt;br /&gt; 3: &lt;a href="https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1"&gt;https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
