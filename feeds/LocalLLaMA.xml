<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-18T09:12:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pp7x2r</id>
    <title>Variable Sized Experts in MoEs</title>
    <updated>2025-12-17T20:59:06+00:00</updated>
    <author>
      <name>/u/hbfreed</name>
      <uri>https://old.reddit.com/user/hbfreed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been messing around with variable sized experts in MoEs over the past few months, built on top of &lt;a href="https://github.com/karpathy/nanoGPT"&gt;nanoGPT&lt;/a&gt; (working on nanochat support right now!) and &lt;a href="https://github.com/databricks/megablocks"&gt;MegaBlocks&lt;/a&gt; for efficient MoE computation. &lt;/p&gt; &lt;p&gt;In short, the variable sized models do train faster (the 23:1 ratio of large:small experts trains 20% faster with 2.5% higher loss), but that's just because they're using smaller experts on average. When I compared against vanilla MoEs with the same average size, we don't see an efficiency gain. So, the main practical finding is confirming that you don't need the traditional 4x expansion factor, smaller experts are more efficient (DeepSeek V3 and Kimi K2 already use ~2.57x).&lt;/p&gt; &lt;p&gt;The real work I did was trying to chase down which tokens go to which size of experts on average. In this setup, tokens in constrained contexts like code or recipes go to small experts, and more ambiguous tokens like &amp;quot; with&amp;quot; and &amp;quot; to&amp;quot; go to larger ones. I think it's about contextual constraint. When what comes next is more predictable (code syntax, recipe format), the model learns to use less compute. When it's ambiguous, it learns to use more.&lt;/p&gt; &lt;p&gt;Here's my &lt;a href="https://hbfreed.com/2025/12/16/variable-size-experts.html"&gt;full writeup&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;&lt;a href="https://hbfreed.com/assets/visualizations/moe-routing-viz.html"&gt;Visualization 1&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;&lt;a href="https://hbfreed.com/assets/visualizations/moe-code-routing-viz.html"&gt;Visualization 2 (code boogaloo)&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;and &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/hbfreed/nanoMOE"&gt;Github&lt;/a&gt;!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hbfreed"&gt; /u/hbfreed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7x2r/variable_sized_experts_in_moes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7x2r/variable_sized_experts_in_moes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp7x2r/variable_sized_experts_in_moes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:59:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pplvzz</id>
    <title>[Project] I built a local "System 2" VLM pipeline to mine Autonomous Driving data on a single RTX 3090 (No Cloud APIs). Beats CLIP recall by ~50%.</title>
    <updated>2025-12-18T08:36:42+00:00</updated>
    <author>
      <name>/u/Pale_Location_373</name>
      <uri>https://old.reddit.com/user/Pale_Location_373</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm an independent researcher working on Autonomous Vehicles. I wanted to solve the &amp;quot;Dark Data&amp;quot; problem‚Äîwe have petabytes of driving logs, but finding the weird edge cases (e.g., a wheelchair on the road, sensor glare, passive construction zones) is incredibly hard.&lt;/p&gt; &lt;p&gt;Standard methods use metadata tags (too vague) or CLIP embeddings (spatial blindness). Sending petabytes of video to GPT-4V is impossible due to cost and privacy.&lt;/p&gt; &lt;p&gt;So, I built &lt;strong&gt;Semantic-Drive&lt;/strong&gt;: A local-first, neuro-symbolic data mining engine that runs entirely on consumer hardware (tested on an RTX 3090).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture (&amp;quot;System 2&amp;quot; Inference):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of just asking a VLM to &amp;quot;describe the image,&amp;quot; I implemented a &lt;strong&gt;Judge-Scout&lt;/strong&gt; architecture inspired by recent reasoning models (o1):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Symbolic Grounding (The Eye):&lt;/strong&gt; I use &lt;strong&gt;YOLO-E&lt;/strong&gt; to extract a high-recall text inventory of objects. This is injected into the VLM's context window as a hard constraint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cognitive Analysis (The Scouts):&lt;/strong&gt; I run quantized VLMs (&lt;strong&gt;Qwen3-VL-30B-A3B-Thinking, Gemma-3-27B-IT,&lt;/strong&gt; and &lt;strong&gt;Kimi-VL-A3B-Thinking-2506&lt;/strong&gt;) via &lt;em&gt;llama.cpp&lt;/em&gt;. They perform a Chain-of-Thought &amp;quot;&lt;em&gt;forensic analysis&lt;/em&gt;&amp;quot; to verify if the YOLO objects are actual hazards or just artifacts (like a poster of a person).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference-Time Consensus (The Judge):&lt;/strong&gt; A local &lt;strong&gt;Ministral-3-14B-Instruct-2512&lt;/strong&gt; aggregates reports from multiple scouts. It uses an &lt;strong&gt;Explicit Outcome Reward Model (ORM),&lt;/strong&gt; a Python script that scores generations based on YOLO consistency, to perform a &lt;strong&gt;Best-of-N&lt;/strong&gt; search.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Results (Benchmarked on nuScenes):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Recall:&lt;/strong&gt; 0.966 (vs 0.475 for CLIP ViT-L/14).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hallucination:&lt;/strong&gt; Reduced Risk Assessment Error by &lt;strong&gt;51%&lt;/strong&gt; compared to a raw zero-shot VLM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; ~$0.85 per 1k frames (Energy) vs ~$30.00 for GPT-4o.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Tech Stack:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Inference:&lt;/strong&gt; `llama.cpp` server (Dockerized).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Models:&lt;/strong&gt; Q4_K_M GGUFs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;UI:&lt;/strong&gt; Streamlit (for human-in-the-loop verification).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôve open-sourced the whole thing, including the Docker setup and a &amp;quot;Gold Set&amp;quot; benchmark for long-tail mining.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/AntonioAlgaida/Semantic-Drive"&gt;https://github.com/AntonioAlgaida/Semantic-Drive&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo (HF Space):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/agnprz/Semantic-Drive-Explorer"&gt;https://huggingface.co/spaces/agnprz/Semantic-Drive-Explorer&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Paper (ArXiv):&lt;/strong&gt; &lt;a href="https://arxiv.org/abs/2512.12012"&gt;https://arxiv.org/abs/2512.12012&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions about the prompt engineering or the local &amp;quot;System 2&amp;quot; implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pale_Location_373"&gt; /u/Pale_Location_373 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pplvzz/project_i_built_a_local_system_2_vlm_pipeline_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T08:36:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2wun</id>
    <title>GLM 4.6V vs. GLM 4.5 Air: Benchmarks and Real-World Tests?</title>
    <updated>2025-12-17T17:44:18+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Both models are the same size, but GLM 4.6V is a newer generation and includes vision capabilities. Some argue that adding vision may reduce textual performance, while others believe multimodality could enhance the model‚Äôs overall understanding of the world.&lt;/p&gt; &lt;p&gt;Has anyone run benchmarks or real-world tests comparing the two?&lt;/p&gt; &lt;p&gt;For reference, GLM 4.6V already has support in llama.cpp and GGUFs: &lt;a href="https://huggingface.co/unsloth/GLM-4.6V-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.6V-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2wun/glm_46v_vs_glm_45_air_benchmarks_and_realworld/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2wun/glm_46v_vs_glm_45_air_benchmarks_and_realworld/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2wun/glm_46v_vs_glm_45_air_benchmarks_and_realworld/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozpcq</id>
    <title>You can now fine-tune LLMs and deploy them directly on your phone!</title>
    <updated>2025-12-17T15:40:50+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"&gt; &lt;img alt="You can now fine-tune LLMs and deploy them directly on your phone!" src="https://preview.redd.it/zi5ph67zas7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98235903a72ce7dadc57bc88ea65f48143f89438" title="You can now fine-tune LLMs and deploy them directly on your phone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://docs.unsloth.ai/new/deploy-llms-phone"&gt;https://docs.unsloth.ai/new/deploy-llms-phone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;you can:&lt;/p&gt; &lt;p&gt;Use the same tech (ExecuTorch) Meta has to power billions on Instagram, WhatsApp&lt;/p&gt; &lt;p&gt;Deploy Qwen3-0.6B locally to Pixel 8 and iPhone 15 Pro at ~40 tokens/s&lt;/p&gt; &lt;p&gt;Apply QAT via TorchAO to recover 70% of accuracy&lt;/p&gt; &lt;p&gt;Get privacy first, instant responses and offline capabilities&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi5ph67zas7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pow797</id>
    <title>Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter</title>
    <updated>2025-12-17T13:14:02+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt; &lt;img alt="Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter" src="https://a.thumbs.redditmedia.com/-V8KieEduFhCtfHKuiRcs_94wVQuIC9TTbPuo7vOPY8.jpg" title="Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kujwpbsakr7g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b5a113e06d0e8db66436dc632a8828a85bb8d16e"&gt;https://preview.redd.it/kujwpbsakr7g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b5a113e06d0e8db66436dc632a8828a85bb8d16e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jlban9qkr7g1.jpg?width=789&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7984f0c584b0b67cc49f6b24d3ae920d42e3ccc0"&gt;https://preview.redd.it/8jlban9qkr7g1.jpg?width=789&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7984f0c584b0b67cc49f6b24d3ae920d42e3ccc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM wars are wild&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1powhy6</id>
    <title>anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups</title>
    <updated>2025-12-17T13:27:38+00:00</updated>
    <author>
      <name>/u/Zestyclose_Ring1123</name>
      <uri>https://old.reddit.com/user/Zestyclose_Ring1123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;anthropic published this detailed blog about &amp;quot;code execution&amp;quot; for agents: &lt;a href="https://www.anthropic.com/engineering/code-execution-with-mcp"&gt;https://www.anthropic.com/engineering/code-execution-with-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;instead of direct tool calls, model writes code that orchestrates tools&lt;/p&gt; &lt;p&gt;they claim massive token reduction. like 150k down to 2k in their example. sounds almost too good to be true&lt;/p&gt; &lt;p&gt;basic idea: dont preload all tool definitions. let model explore available tools on demand. data flows through variables not context&lt;/p&gt; &lt;p&gt;for local models this could be huge. context limits hit way harder when youre running smaller models&lt;/p&gt; &lt;p&gt;the privacy angle is interesting too. sensitive data never enters model context, flows directly between tools&lt;/p&gt; &lt;p&gt;cloudflare independently discovered this &amp;quot;code mode&amp;quot; pattern according to the blog&lt;/p&gt; &lt;p&gt;main challenge would be sandboxing. running model-generated code locally needs serious isolation&lt;/p&gt; &lt;p&gt;but if you can solve that, complex agents might become viable on consumer hardware. 8k context instead of needing 128k+&lt;/p&gt; &lt;p&gt;tools like cursor and verdent already do basic code generation. this anthropic approach could push that concept way further&lt;/p&gt; &lt;p&gt;wondering if anyone has experimented with similar patterns locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose_Ring1123"&gt; /u/Zestyclose_Ring1123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppl6hv</id>
    <title>LLMs interacting with each other</title>
    <updated>2025-12-18T07:48:46+00:00</updated>
    <author>
      <name>/u/CulturalReflection45</name>
      <uri>https://old.reddit.com/user/CulturalReflection45</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was interested to know how LLMs would interact with each other. So I created this small app that helps you simulate conversations. You can even assign a persona to an agent, have many agents in the conversation, and use APIs or locally deployed models. And it comes with a front-end. Give this a try if you find it interesting.&lt;/p&gt; &lt;p&gt;GitHub - &lt;a href="https://github.com/tewatia/mais"&gt;https://github.com/tewatia/mais&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CulturalReflection45"&gt; /u/CulturalReflection45 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl6hv/llms_interacting_with_each_other/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl6hv/llms_interacting_with_each_other/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl6hv/llms_interacting_with_each_other/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T07:48:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozr6f</id>
    <title>Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)</title>
    <updated>2025-12-17T15:42:43+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"&gt; &lt;img alt="Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with our &lt;strong&gt;November runs&lt;/strong&gt; on &lt;strong&gt;47 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). It‚Äôs a SWE-bench‚Äìstyle setup: models read real PR issues, run tests, edit code, and must make the suite pass. &lt;/p&gt; &lt;p&gt;This update includes a particularly large wave of new releases, so we‚Äôve added a substantial batch of new models to the leaderboard:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Devstral 2&lt;/strong&gt; ‚Äî a strong release of models that can be run locally given their size&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek v3.2&lt;/strong&gt; ‚Äî a new state-of-the-art open-weight model&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;new comparison mode&lt;/strong&gt; to benchmark models against external systems such as &lt;strong&gt;Claude Code&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also introduced a &lt;strong&gt;cached-tokens statistic&lt;/strong&gt; to improve transparency around cache usage.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=nov_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp43wr</id>
    <title>We distilled SGLang to help you learn how modern LLM inference works in a weekend</title>
    <updated>2025-12-17T18:29:58+00:00</updated>
    <author>
      <name>/u/Secret_Seaweed_1574</name>
      <uri>https://old.reddit.com/user/Secret_Seaweed_1574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"&gt; &lt;img alt="We distilled SGLang to help you learn how modern LLM inference works in a weekend" src="https://b.thumbs.redditmedia.com/hfEeQJfhvdWJgogmoX3ykQH-YD-IjMq-igf_MgGEpKM.jpg" title="We distilled SGLang to help you learn how modern LLM inference works in a weekend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xxb4036c4t7g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8ea1c438e9fb3d2625e97881fea5d9dbb5c918e"&gt;https://preview.redd.it/xxb4036c4t7g1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a8ea1c438e9fb3d2625e97881fea5d9dbb5c918e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã,&lt;/p&gt; &lt;p&gt;Mingyi from SGLang here.&lt;/p&gt; &lt;p&gt;We just released mini-SGLang, a distilled version of SGLang that you can actually read and understand in a weekend.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We distilled SGLang from 300K lines to 5,000 lines&lt;/li&gt; &lt;li&gt;We kept all the core optimizations (overlap scheduling, FlashAttention-3, Radix cache, etc.)&lt;/li&gt; &lt;li&gt;Performance: nearly identical to full SGLang for online serving&lt;/li&gt; &lt;li&gt;It is the only minimal inference project that supports online/offline serving, streaming, and overlap scheduling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why we built this:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A lot of people want to understand how modern LLM inference works under the hood, but diving into 300K lines of production code of SGLang is brutal. We took everything we learned building SGLang and distilled it into something you can actually read, understand, and hack on.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The first version includes:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Overlap Scheduling&lt;/li&gt; &lt;li&gt;FlashAttention-3 + FlashInfer kernels&lt;/li&gt; &lt;li&gt;Radix Cache &amp;amp; Chunked Prefill&lt;/li&gt; &lt;li&gt;Tensor Parallelism&lt;/li&gt; &lt;li&gt;JIT CUDA kernels&lt;/li&gt; &lt;li&gt;OpenAI-compatible API&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance (Qwen3-32B, 4x H200, realistic workload):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd1o4vte4t7g1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0266d71455d1dc769a44a47403a99fd7f7846c3"&gt;https://preview.redd.it/fd1o4vte4t7g1.png?width=2700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f0266d71455d1dc769a44a47403a99fd7f7846c3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We built mini-SGLang for engineers, researchers, and students who learn better from code than papers.&lt;/p&gt; &lt;p&gt;We're building more around this: code walkthroughs, cookbooks, and tutorials coming soon!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Post: &lt;a href="https://x.com/lmsysorg/status/2001356624855023669?s=20"&gt;https://x.com/lmsysorg/status/2001356624855023669?s=20&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/sgl-project/mini-sglang"&gt;https://github.com/sgl-project/mini-sglang&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Blog post with full benchmarks: &lt;a href="https://lmsys.org/blog/2025-12-17-minisgl/"&gt;https://lmsys.org/blog/2025-12-17-minisgl/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Secret_Seaweed_1574"&gt; /u/Secret_Seaweed_1574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp43wr/we_distilled_sglang_to_help_you_learn_how_modern/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T18:29:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp5fvp</id>
    <title>Lemonade v9.1 - ROCm 7 for Strix Point - Roadmap Update - Strix Halo Survey</title>
    <updated>2025-12-17T19:21:24+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5fvp/lemonade_v91_rocm_7_for_strix_point_roadmap/"&gt; &lt;img alt="Lemonade v9.1 - ROCm 7 for Strix Point - Roadmap Update - Strix Halo Survey" src="https://preview.redd.it/wejf7bjdat7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=53e9285565bba2d3606267ad5ac0cc86aaf612e8" title="Lemonade v9.1 - ROCm 7 for Strix Point - Roadmap Update - Strix Halo Survey" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, I'm back with a final update for the year and some questions from AMD for you all.&lt;/p&gt; &lt;p&gt;If you haven't heard of Lemonade, it's a local LLM/GenAI router and backend manager that helps you discover and run optimized LLMs with apps like n8n, VS Code Copilot, Open WebUI, and many more.&lt;/p&gt; &lt;h1&gt;Lemonade Update&lt;/h1&gt; &lt;p&gt;Lemonade v9.1 is out, which checks off most of the roadmap items from the v9.0 post a few weeks ago:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The &lt;strong&gt;new Lemonade app&lt;/strong&gt; is available in the &lt;code&gt;lemonade.deb&lt;/code&gt; and &lt;code&gt;lemonade.msi&lt;/code&gt; installers. The goal is to get you set up and connecting to other apps ASAP, and users are not expected to spend loads of time in our app.&lt;/li&gt; &lt;li&gt;Basic &lt;strong&gt;audio input&lt;/strong&gt; (aka ASR aka STT) is enabled through the OpenAI transcriptions API via whisper.cpp.&lt;/li&gt; &lt;li&gt;By popular demand, &lt;strong&gt;Strix Point has ROCm 7 + llamacpp support&lt;/strong&gt; (aka Ryzen AI 360-375 aka Radeon 880-890M aka gfx1150) in Lemonade with &lt;code&gt;--llamacpp rocm&lt;/code&gt; as well as in the upstream &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm"&gt;llamacpp-rocm project&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Also by popular demand, &lt;code&gt;--extra-models-dir&lt;/code&gt; lets you bring LLM GGUFs from anywhere on your PC into Lemonade.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Next on the Lemonade roadmap in 2026 is more output modalities: image generation from stablediffusion.cpp, as well as text-to-speech. At that point Lemonade will support I/O of text, images, and speech from a single base URL.&lt;/p&gt; &lt;p&gt;Links: &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;GitHub&lt;/a&gt; and &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;. Come say hi if you like the project :)&lt;/p&gt; &lt;h1&gt;Strix Halo Survey&lt;/h1&gt; &lt;p&gt;AMD leadership wants to know what you think of Strix Halo (aka Ryzen AI MAX 395). The specific questions are as follows, but please give any feedback you like as well!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If you own a Strix Halo: &lt;ol&gt; &lt;li&gt;What do you enjoy doing with it?&lt;/li&gt; &lt;li&gt;What do you want to do, but is too difficult or impossible today?&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;If you're considering buying a Strix Halo: what software and/or content do you need to see from AMD?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(I've been tracking/reporting feedback from my own posts and others' posts all year, and feel I have a good sense, but it's useful to get people's thoughts in this one place in a semi-official way)&lt;br /&gt; edit: formatting&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wejf7bjdat7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5fvp/lemonade_v91_rocm_7_for_strix_point_roadmap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp5fvp/lemonade_v91_rocm_7_for_strix_point_roadmap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T19:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppbx2r</id>
    <title>2x Hailo 10H running LLMs on Raspberry Pi 5</title>
    <updated>2025-12-17T23:43:36+00:00</updated>
    <author>
      <name>/u/martincerven</name>
      <uri>https://old.reddit.com/user/martincerven</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppbx2r/2x_hailo_10h_running_llms_on_raspberry_pi_5/"&gt; &lt;img alt="2x Hailo 10H running LLMs on Raspberry Pi 5" src="https://external-preview.redd.it/-UtatTVLPFBH5nFCnadOByG6DM8Q0W-9QDOoZnCbr0o.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ed76759fc59b73820b966057effb73022a0c7ef" title="2x Hailo 10H running LLMs on Raspberry Pi 5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested two Hailo 10H running on Raspberry Pi 5, ran 2 LLMs and made them talk to each other: &lt;a href="https://github.com/martincerven/hailo_learn"&gt;https://github.com/martincerven/hailo_learn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also how it runs with/without heatsinks w. thermal camera.&lt;/p&gt; &lt;p&gt;It has 8GB LPDDR4 each, connected over M2 PCIe.&lt;/p&gt; &lt;p&gt;I will try more examples like Whisper, VLMs next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martincerven"&gt; /u/martincerven &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/yhDjQx-Dmu0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppbx2r/2x_hailo_10h_running_llms_on_raspberry_pi_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppbx2r/2x_hailo_10h_running_llms_on_raspberry_pi_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T23:43:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pox733</id>
    <title>LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?</title>
    <updated>2025-12-17T13:59:02+00:00</updated>
    <author>
      <name>/u/Exact-Literature-395</name>
      <uri>https://old.reddit.com/user/Exact-Literature-395</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I stumbled on this LLM Development Landscape 2.0 report from Ant Open Source and it basically confirmed what I've been feeling for months.&lt;/p&gt; &lt;p&gt;LangChain, LlamaIndex and AutoGen are all listed as &amp;quot;steepest declining&amp;quot; projects by community activity over the past 6 months. The report says it's due to &amp;quot;reduced community investment from once dominant projects.&amp;quot; Meanwhile stuff like vLLM and SGLang keeps growing.&lt;/p&gt; &lt;p&gt;Honestly this tracks with my experience. I spent way too long fighting with LangChain abstractions last year before I just ripped it out and called the APIs directly. Cut my codebase in half and debugging became actually possible. Every time I see a tutorial using LangChain now I just skip it.&lt;/p&gt; &lt;p&gt;But I'm curious if this is just me being lazy or if there's a real shift happening. Are agent frameworks solving a problem that doesn't really exist anymore now that the base models are good enough? Or am I missing something and these tools are still essential for complex workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exact-Literature-395"&gt; /u/Exact-Literature-395 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppj35l</id>
    <title>Has anyone done extensive testing with reap releases?</title>
    <updated>2025-12-18T05:40:36+00:00</updated>
    <author>
      <name>/u/SillyLilBear</name>
      <uri>https://old.reddit.com/user/SillyLilBear</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have only done some basic testing, but I am curious if anyone has done any extensive testing of reaped q4 and q8 releases vs non-reaped versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SillyLilBear"&gt; /u/SillyLilBear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppj35l/has_anyone_done_extensive_testing_with_reap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppj35l/has_anyone_done_extensive_testing_with_reap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppj35l/has_anyone_done_extensive_testing_with_reap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T05:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2j60</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!</title>
    <updated>2025-12-17T17:29:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 20+ iterations, 3 close calls, we've finally come to a release. The best Cydonia so far. At least that's what the testers at Beaver have been saying.&lt;/p&gt; &lt;p&gt;Peak Cydonia! Served by yours truly.&lt;/p&gt; &lt;p&gt;Small 3.2: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Magistral 1.2: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Most prefer Magidonia, but they're both pretty good!)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;To my patrons,&lt;/p&gt; &lt;p&gt;Earlier this week, I had a difficult choice to make. Thanks to your support, I get to enjoy the freedom you've granted me. Thank you for giving me strength to pursue this journey. I will continue dishing out the best tunes possible for you, truly.&lt;/p&gt; &lt;p&gt;- Drummer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2rtn</id>
    <title>Nemotron was post-trained to assume humans have reasoning, but they never use it</title>
    <updated>2025-12-17T17:38:58+00:00</updated>
    <author>
      <name>/u/RetiredApostle</name>
      <uri>https://old.reddit.com/user/RetiredApostle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"&gt; &lt;img alt="Nemotron was post-trained to assume humans have reasoning, but they never use it" src="https://preview.redd.it/52423nr8us7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4ca1f0be0378b305c1a58efa1b2f8b99752b5ff" title="Nemotron was post-trained to assume humans have reasoning, but they never use it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RetiredApostle"&gt; /u/RetiredApostle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52423nr8us7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2rtn/nemotron_was_posttrained_to_assume_humans_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppl5mw</id>
    <title>Maestro ‚Äì Run AI coding agents autonomously for days (Free/OSS)</title>
    <updated>2025-12-18T07:47:12+00:00</updated>
    <author>
      <name>/u/pedramamini</name>
      <uri>https://old.reddit.com/user/pedramamini</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl5mw/maestro_run_ai_coding_agents_autonomously_for/"&gt; &lt;img alt="Maestro ‚Äì Run AI coding agents autonomously for days (Free/OSS)" src="https://preview.redd.it/6wzh6jbg3x7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98f3e53f3b468ec568a721316f4f89ab00e96544" title="Maestro ‚Äì Run AI coding agents autonomously for days (Free/OSS)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing a recent labor of love to the world... Maestro is a cross-platform desktop app for orchestrating your fleet of Al agents. Set them loose on complex tasks, check in from your phone, and let them work while you sleep. Free and open source:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://runmaestro.ai/"&gt;https://runmaestro.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/pedramamini/Maestro"&gt;https://github.com/pedramamini/Maestro&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I strongly prefer interacting with ReAct (reason-act) agents over chat agents. It allows for file-system based memory, tool creation and use, MCP agents, etc. I have so many parallel threads with so many agents that I lose track of them regularly. This was the impetus behind the creation of Maestro. Now all my agents sit side-by-side, each logical thread in its own tab, and keyboard short cuts galore allow me to conduct them all at lighting speed.&lt;/p&gt; &lt;p&gt;The single most powerful feature of the application is the Auto Run capability. Work with Al to generate a series of detailed implementation plans, then execute on them with a fresh context per task, allowing for nonstop uninterrupted execution. The current record is over two days of runtime! Even more powerful, organize multiple Markdown documents into a loop-able Playbook, with one stage creating work for other stages.&lt;/p&gt; &lt;p&gt;Mostly tested on OSX with Claude. Codex and Open Code support was just added today. Please download and send me feedback during your holiday downtime, many thanks in advance.&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;p&gt;-pedram&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pedramamini"&gt; /u/pedramamini &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6wzh6jbg3x7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl5mw/maestro_run_ai_coding_agents_autonomously_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppl5mw/maestro_run_ai_coding_agents_autonomously_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T07:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppjo5b</id>
    <title>Day 10: 21 Days of Building a Small Language Model: KV Cache</title>
    <updated>2025-12-18T06:14:31+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 10: 21 Days of Building a Small Language Model: KV Cache" src="https://b.thumbs.redditmedia.com/vt9eeanrTHbty0PeG_jN-4PShJ8fGv9qBsfU3Bpopco.jpg" title="Day 10: 21 Days of Building a Small Language Model: KV Cache" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 10 of 21 Days of Building a Small Language Model. The topic for today is the KV cache. Yesterday, we explored multi-head attention and how it allows models to look at sequences from multiple perspectives simultaneously. Today, we'll see why generating text would be impossibly slow without a clever optimization called the Key-Value cache.&lt;/p&gt; &lt;h1&gt;Problem&lt;/h1&gt; &lt;p&gt;To understand why KV cache is necessary, we first need to understand how language models generate text. The process is simple: the model predicts one token at a time, using all previously generated tokens as context.&lt;/p&gt; &lt;p&gt;Let's walk through a simple example. Suppose you prompt the model with: The algorithm processes data&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ketg7dmymw7g1.png?width=1006&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1998bceae61cdc3a85a3c13fd7292dc0f229c280"&gt;https://preview.redd.it/ketg7dmymw7g1.png?width=1006&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1998bceae61cdc3a85a3c13fd7292dc0f229c280&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's what happens step by step:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;First pass&lt;/strong&gt;: The model processes these four tokens through all transformer layers and predicts the next token, say efficiently&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Second pass&lt;/strong&gt;: Now the sequence is. The algorithm processes data efficiently. The model feeds this &lt;em&gt;entire&lt;/em&gt; sequence through all layers again to predict the next token, perhaps by&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Third pass&lt;/strong&gt;: The sequence becomes. The algorithm processes data efficiently by, and this &lt;em&gt;entire&lt;/em&gt; sequence is processed again to predict the next token&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This process can continue for potentially hundreds or thousands of tokens.&lt;/p&gt; &lt;p&gt;Notice something deeply inefficient here: we're repeatedly recomputing attention for all earlier tokens, even though those computations never change.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the first pass, we compute Query (Q), Key (K), and Value (V) vectors for [&amp;quot;The&amp;quot;, &amp;quot;algorithm&amp;quot;, &amp;quot;processes&amp;quot;, &amp;quot;data&amp;quot;]&lt;/li&gt; &lt;li&gt;In the second pass, we recompute Q/K/V for those same four tokens &lt;em&gt;again&lt;/em&gt;, plus &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;In the third pass, we recompute all five previous tokens &lt;em&gt;again&lt;/em&gt;, plus the new one&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each iteration repeats 90-99% of the same computation. We're essentially throwing away all the work we did in previous iterations and starting over from scratch.&lt;/p&gt; &lt;p&gt;The problem compounds as sequences grow longer. If you're generating a 1,000-token response:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The first token's attention is computed 1,000 times&lt;/li&gt; &lt;li&gt;The second token's attention is computed 999 times&lt;/li&gt; &lt;li&gt;And so on...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a 100-token sequence, you'd compute Q/K/V a total of 5,050 times (1+2+...+100) when you really only need to do it 100 times (once per token). This massive redundancy is what makes inference slow and expensive without optimization.&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;NOTE:&lt;/strong&gt; KV caching only comes during the inference stage. It does not exist during training or pretraining. The KV cache is purely an inference-time optimization that helps accelerate text generation after the model has been trained. This distinction is critical to understand. The cache is used when the model is generating text, not when it is learning from data.&lt;/p&gt; &lt;h1&gt;Only the last token matters&lt;/h1&gt; &lt;p&gt;Here's something that might not be obvious at first, but changes everything once you see it: when predicting the next token, only the last token's output matters.&lt;/p&gt; &lt;p&gt;Think about what happens at the transformer's output. We get a logits matrix with probability distributions for &lt;em&gt;every&lt;/em&gt; token in the sequence. But for prediction, we only use the last row, the logits for the most recent token.&lt;/p&gt; &lt;p&gt;When processing The algorithm processes data efficiently, we compute logits for all five tokens, but we only care about the logits for efficiently to determine what comes next. The earlier tokens? Their logits get computed and then ignored.&lt;/p&gt; &lt;p&gt;This raises an important question: why not just keep the last token and throw away everything else?&lt;/p&gt; &lt;p&gt;While we only need the last token's logits for prediction, we still need information from all earlier tokens to compute those logits correctly. Remember from Day 9, the attention mechanism needs to look at all previous tokens to create context for the current token.&lt;/p&gt; &lt;p&gt;So we can't simply discard everything. We need a smarter approach: preserve information from earlier tokens in a form that lets us efficiently compute attention for new tokens, without recomputing everything from scratch.&lt;/p&gt; &lt;h1&gt;Solution&lt;/h1&gt; &lt;p&gt;Let's work backward from what we actually need to compute the next token.&lt;/p&gt; &lt;p&gt;To compute the context vector for the latest token (say, &amp;quot;efficiently&amp;quot;), we need:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Attention weights&lt;/strong&gt; for &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value vectors&lt;/strong&gt; for all previous tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And to compute those attention weights, we need:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Query vector&lt;/strong&gt; for &amp;quot;efficiently&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Key vectors&lt;/strong&gt; for all previous tokens&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Looking at this list reveals an important pattern: we only need all previous key vectors and all previous value vectors. We do NOT need to store previous query vectors. Here's why this distinction matters.&lt;/p&gt; &lt;h1&gt;Why Queries aren't cached&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/v15xtcmymw7g1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c629f193faa2f2823f1a17ae906dcc99292fb72"&gt;https://preview.redd.it/v15xtcmymw7g1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8c629f193faa2f2823f1a17ae906dcc99292fb72&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the first question that comes to everyone‚Äôs mind. The query vector has a very specific, one time job. It's only used to compute attention weights for the &lt;em&gt;current&lt;/em&gt; token. Once we've done that and combined the value vectors, the query has served its purpose. We never need it again.&lt;/p&gt; &lt;p&gt;Let's trace through what happens with &amp;quot;efficiently&amp;quot;: ‚Ä¢ We compute its query vector to figure out which previous tokens to attend to ‚Ä¢ We compare this query to all the previous keys (from &amp;quot;The&amp;quot;, &amp;quot;algorithm&amp;quot;, &amp;quot;processes&amp;quot;, &amp;quot;data&amp;quot;) ‚Ä¢ We get attention weights and use them to combine the previous value vectors ‚Ä¢ Done. The query is never used again.&lt;/p&gt; &lt;p&gt;When the next token &amp;quot;by&amp;quot; arrives: ‚Ä¢ We'll compute &amp;quot;by&amp;quot;'s NEW query vector for its attention ‚Ä¢ But we WON'T need &amp;quot;efficiently&amp;quot;'s query vector anymore ‚Ä¢ However, we WILL need &amp;quot;efficiently&amp;quot;'s key and value vectors, because &amp;quot;by&amp;quot; needs to attend to &amp;quot;efficiently&amp;quot; and all previous tokens&lt;/p&gt; &lt;p&gt;See the pattern? Each token's query is temporary. But each token's keys and values are permanent. They're needed by every future token.&lt;/p&gt; &lt;p&gt;This is why it's called the KV cache, not the QKV cache.&lt;/p&gt; &lt;p&gt;Here's a helpful mental model: think of the query as asking a question (&amp;quot;What should I pay attention to?&amp;quot;). Once you get your answer, you don't need to ask again. But the keys and values? They're like books in a library. Future tokens will need to look them up, so we keep them around.&lt;/p&gt; &lt;h1&gt;Memory Cost&lt;/h1&gt; &lt;p&gt;While KV cache makes inference dramatically faster, this optimization comes with a significant tradeoff: it requires substantial memory.&lt;/p&gt; &lt;p&gt;The cache must store a key vector and value vector for every layer, every head, and every token in the sequence. These requirements accumulate quickly.&lt;/p&gt; &lt;p&gt;The formula for calculating memory requirements:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;KV Cache Size = layers √ó batch_size √ó num_heads √ó head_dim √ó seq_length √ó 2 √ó 2 Where: ‚Ä¢ First 2: for Keys and Values ‚Ä¢ Second 2: bytes per parameter (FP16 uses 2 bytes) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;For example, let's examine numbers from models to understand the scale of memory requirements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1: A 30B Parameter Model&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Layers: 48 ‚Ä¢ Batch size: 128 ‚Ä¢ Total head dimensions: 7,168 ‚Ä¢ Sequence length: 1,024 tokens KV Cache Size = 48 √ó 128 √ó 7,168 √ó 1,024 √ó 2 √ó 2 = ~180 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That's 180 GB just for the cache, not even including the model parameters themselves.&lt;/p&gt; &lt;p&gt;For models designed for long contexts, the requirements grow even larger:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 2: A Long Context Model&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Layers: 61 ‚Ä¢ Batch size: 1 ‚Ä¢ Heads: 128 ‚Ä¢ Head dimension: 128 ‚Ä¢ Sequence length: 100,000 tokens KV Cache Size = 61 √ó 1 √ó 128 √ó 128 √ó 100,000 √ó 2 √ó 2 = ~400 GB &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;400 GB represents a massive memory requirement. No single GPU can accommodate this, and even multi-GPU setups face significant challenges.&lt;/p&gt; &lt;p&gt;KV cache memory scales linearly with context length. Doubling the context length doubles the memory requirements, which directly translates to higher costs and fewer requests that can be served in parallel.&lt;/p&gt; &lt;h1&gt;Addressing the Memory Challenge&lt;/h1&gt; &lt;p&gt;The memory constraints of KV cache aren't just theoretical concerns. They're real bottlenecks that have driven significant innovation in several directions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi Query Attention (MQA)&lt;/strong&gt;: What if all attention heads shared one key and one value projection instead of each having its own? Instead of storing H separate key/value vectors per token per layer, you'd store just one that all heads share. Massive memory savings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Grouped Query Attention (GQA)&lt;/strong&gt;: A middle ground. Instead of all heads sharing K/V (MQA) or each head having its own (standard multi-head attention), groups of heads share K/V. Better memory than standard attention, more flexibility than MQA.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other Approaches&lt;/strong&gt;: ‚Ä¢ Sparse attention (only attend to relevant tokens) ‚Ä¢ Linear attention (reduce the quadratic complexity) ‚Ä¢ Compression techniques (reduce precision/dimensionality of cached K/V)&lt;/p&gt; &lt;p&gt;All of these innovations address the same fundamental issue: as context length grows, KV cache memory requirements grow proportionally, making very long contexts impractical.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we uncovered one of the most important optimizations in modern language models. The KV cache is elegant in its simplicity: cache the keys and values for reuse, but skip the queries since they're only needed once.&lt;/p&gt; &lt;p&gt;However, the optimization comes at a cost. The KV cache requires substantial memory that grows with context length. This memory requirement becomes the bottleneck as contexts get longer. The cache solved computational redundancy but created a memory scaling challenge.This tradeoff explains many design decisions in modern language models. Researchers developed MQA, GQA, and other attention variants to address the memory problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppjo5b/day_10_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T06:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We‚Äôll be answering questions live on Thursday, Dec. 18, from 2-3pm PT. Hope to see you there.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1porpwd</id>
    <title>Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model</title>
    <updated>2025-12-17T08:49:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt; &lt;img alt="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" src="https://external-preview.redd.it/OXpuN3VqYnE4cTdnMbhg7mfH3BLNBAJzBcqwf-BeiskbYrfqW4XgiIx-FQh0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6475aef4e90b21644bf95a26618a75433c2e08de" title="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Details&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Type:&lt;/strong&gt; Flow-Matching Transformers with Sparse Voxel based 3D VAE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 4 Billion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Single Image&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; 3D Asset &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/microsoft/TRELLIS.2-4B"&gt;https://huggingface.co/microsoft/TRELLIS.2-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo - &lt;a href="https://huggingface.co/spaces/microsoft/TRELLIS.2"&gt;https://huggingface.co/spaces/microsoft/TRELLIS.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post - &lt;a href="https://microsoft.github.io/TRELLIS.2/"&gt;https://microsoft.github.io/TRELLIS.2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g8uco5dq8q7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T08:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp6jhq</id>
    <title>Hey, LocalLLaMa. We need to talk...</title>
    <updated>2025-12-17T20:04:07+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I look on the front page and I see people who have spent time and effort to make something, and they share it willingly. They are getting no upvotes.&lt;/p&gt; &lt;p&gt;We are here because we are &lt;em&gt;local&lt;/em&gt; and we are &lt;em&gt;open source&lt;/em&gt;. Those things &lt;em&gt;depend on people who give us things&lt;/em&gt;, and they don't ask for anything in return, but they &lt;em&gt;need&lt;/em&gt; something in return or they will stop.&lt;/p&gt; &lt;p&gt;Pop your head into the smaller posts where someone is showing work they have done. Give honest and constructive feedback. UPVOTE IT.&lt;/p&gt; &lt;p&gt;The project may be terrible -- encourage them to grow by telling them how they can make it better. &lt;/p&gt; &lt;p&gt;The project may be awesome. They would love to hear how awesome it is. But if you use it, then they would love 100 times more to hear how you use it and how it helps you.&lt;/p&gt; &lt;p&gt;Engage with the people who share their things, and not just with the entertainment. &lt;/p&gt; &lt;p&gt;It take so little effort but it makes so much difference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp6jhq/hey_localllama_we_need_to_talk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T20:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pper90</id>
    <title>MiraTTS: High quality and fast TTS model</title>
    <updated>2025-12-18T01:55:55+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiraTTS&lt;/strong&gt; is a high quality LLM based TTS finetune that can generate audio at &lt;strong&gt;100x&lt;/strong&gt; realtime and generate realistic and clear 48khz speech! I heavily optimized it using Lmdeploy and used &lt;a href="https://github.com/ysharma3501/FlashSR"&gt;FlashSR&lt;/a&gt; to enhance the audio.&lt;/p&gt; &lt;h1&gt;Benefits of this repo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Incredibly fast: As stated before, over &lt;strong&gt;100x&lt;/strong&gt; realtime!&lt;/li&gt; &lt;li&gt;High quality: Generates realistic and 48khz speech, &lt;strong&gt;much&lt;/strong&gt; clearer then most TTS models and it‚Äôs base model.&lt;/li&gt; &lt;li&gt;Memory efficient: Works with even 6gb vram gpus!&lt;/li&gt; &lt;li&gt;Low latency: Possible latency low as &lt;strong&gt;150ms&lt;/strong&gt;, I have not released code for streaming yet but will release soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basic multilingual versions are already supported, I just need to clean up code. Multispeaker is still in progress, but should come soon. If you have any other issues, I will be happy to fix them.&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/ysharma3501/MiraTTS"&gt;https://github.com/ysharma3501/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/YatharthS/MiraTTS"&gt;https://huggingface.co/YatharthS/MiraTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog explaining llm tts models: &lt;a href="https://huggingface.co/blog/YatharthS/llm-tts-models"&gt;https://huggingface.co/blog/YatharthS/llm-tts-models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Stars/Likes would be appreciated very much, thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pper90/miratts_high_quality_and_fast_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T01:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp8vo4</id>
    <title>Nvidia plans heavy cuts to GPU supply in early 2026</title>
    <updated>2025-12-17T21:37:13+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://overclock3d.net/news/gpu-displays/nvidia-plans-heavy-cuts-to-gpu-supply-in-early-2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp8vo4/nvidia_plans_heavy_cuts_to_gpu_supply_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T21:37:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
