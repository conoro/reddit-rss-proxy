<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-07T04:07:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q5y8qd</id>
    <title>[Advice] RTX 3090 + 64GB RAM for local LLM + general use</title>
    <updated>2026-01-06T23:05:29+00:00</updated>
    <author>
      <name>/u/-Chimichanga-</name>
      <uri>https://old.reddit.com/user/-Chimichanga-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm evaluating the feasibility of upgrading my current system so it can function both as a normal desktop machine and as a local LLM/vision inference setup. The system is connected to a 65‚Äù LG OLED G1 and is currently used for general desktop tasks, browsing, system configuration, and occasional gaming. Before committing to the hardware changes, I‚Äôd like to confirm whether this setup is suitable for running 34B‚Äëclass models alongside everyday use.&lt;/p&gt; &lt;p&gt;Planned System Specs&lt;/p&gt; &lt;p&gt;‚Ä¢ CPU: AMD Ryzen 5 5600X&lt;/p&gt; &lt;p&gt;‚Ä¢ GPU: NVIDIA RTX 3090 (24GB VRAM) - upgrade&lt;/p&gt; &lt;p&gt;‚Ä¢ RAM: 64GB DDR4 3200MHz CL16 - upgrade&lt;/p&gt; &lt;p&gt;‚Ä¢ Storage: 1x Samsung 980 Pro 1TB (Windows + LLM workspace). 1x Kingston A2000 1TB (Games + general data)&lt;/p&gt; &lt;p&gt;Home Architecture&lt;/p&gt; &lt;p&gt;‚Ä¢ Home Assistant running separately on an Intel NUC&lt;/p&gt; &lt;p&gt;‚Ä¢ Unraid NAS for storage and container workloads&lt;/p&gt; &lt;p&gt;Model&lt;/p&gt; &lt;p&gt;LLaVA‚ÄëNext 34B (Q4_K_M) or similar 34B‚Äëclass multimodal model.&lt;/p&gt; &lt;p&gt;Possible workloads&lt;/p&gt; &lt;p&gt;‚Ä¢ Local inference&lt;/p&gt; &lt;p&gt;‚Ä¢ Vision + text reasoning&lt;/p&gt; &lt;p&gt;‚Ä¢ Home Assistant automation building&lt;/p&gt; &lt;p&gt;‚Ä¢ Occasional multi‚Äëmodel routing&lt;/p&gt; &lt;p&gt;Questions&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is this hardware combination (RTX 3090 + 64GB RAM + Ryzen 5 5600X) sufficient for running 34B‚Äëclass multimodal models like LLaVA‚ÄëNext at Q4_K_M?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is my understanding correct that switching between gaming and LLM workloads essentially means assigning the GPU to one task at a time, offloading the LLM with a simple command, and reloading it afterward?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Do you foresee any VRAM‚Äërelated issues when the LLM is loaded but I‚Äôm performing normal desktop tasks (non‚Äëgaming)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there any bottlenecks or architectural concerns I should be aware of for this hybrid setup?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks in advance ‚Äî I‚Äôd appreciate insights from anyone running similar hardware or 30‚Äëseries GPUs with 30B+ models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Chimichanga-"&gt; /u/-Chimichanga- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5y8qd/advice_rtx_3090_64gb_ram_for_local_llm_general_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5y8qd/advice_rtx_3090_64gb_ram_for_local_llm_general_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5y8qd/advice_rtx_3090_64gb_ram_for_local_llm_general_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T23:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5rkr6</id>
    <title>Linux mint for local inference</title>
    <updated>2026-01-06T19:02:09+00:00</updated>
    <author>
      <name>/u/Former-Tangerine-723</name>
      <uri>https://old.reddit.com/user/Former-Tangerine-723</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5rkr6/linux_mint_for_local_inference/"&gt; &lt;img alt="Linux mint for local inference" src="https://preview.redd.it/u38i6uvc1sbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35e3ea45d933399d6b306ca320d28547c5829226" title="Linux mint for local inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw a post earlier in here asking for linux, so I wanted to share my story. &lt;/p&gt; &lt;p&gt;Long story short, I switched from win11 to linux mint and im not going back!&lt;/p&gt; &lt;p&gt;The performance boost is ok but the stability and the extra system resources are something else.&lt;/p&gt; &lt;p&gt;Just a little example, I load the model and use all my Ram and Vram, leaving my system with just 1,5 GB of Ram. And guest what, my system is working solid for hours like nothing happens!! For the record, I cannot load the same model in my win11 partition.&lt;/p&gt; &lt;p&gt;Kudos to you Linux Devs &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Former-Tangerine-723"&gt; /u/Former-Tangerine-723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u38i6uvc1sbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5rkr6/linux_mint_for_local_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5rkr6/linux_mint_for_local_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T19:02:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5r5r9</id>
    <title>Local agentic coding with low quantized, REAPed, large models (MiniMax-M2.1, Qwen3-Coder, GLM 4.6, GLM 4.7, ..)</title>
    <updated>2026-01-06T18:47:27+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More or less recent developments (stable &amp;amp; large MoE models, 2 and 3-bit UD_I and exl3 quants, REAPing) allow to run huge models on little VRAM without completely killing model performance. For example, UD-IQ2_XXS (74.1 GB) of MiniMax M2.1, or a REAP-50.Q5_K_M (82 GB), or potentially even a 3.04 bpw exl3 (88.3 GB) would still fit within 96 GB VRAM and we have some coding related benchmarks showing only minor loss (e.g., seeing an Aider polyglot of MiniMax M2.1 ID_IQ2_M with a pass rate 2 of 50.2% while runs on the &lt;del&gt;fp8 /&lt;/del&gt;edit: (full precision?) version seem to have achieved &lt;del&gt;only barely more&lt;/del&gt; between 51.6% and 61.3%)&lt;/p&gt; &lt;p&gt;It would be interesting if anyone deliberately stayed or is using a low-bit quantization (less than 4-bits) of such large models for agentic coding and found them performing better than using a smaller model (either unquantized, or more than 3-bit quantized).&lt;/p&gt; &lt;p&gt;(I'd be especially excited if someone said they have ditched gpt-oss-120b/glm4.5 air/qwen3-next-80b for a higher parameter model on less than 96 GB VRAM :) )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5r5r9/local_agentic_coding_with_low_quantized_reaped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5r5r9/local_agentic_coding_with_low_quantized_reaped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5r5r9/local_agentic_coding_with_low_quantized_reaped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T18:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5fs95</id>
    <title>Artificial Analysis just refreshed their global model indices</title>
    <updated>2026-01-06T11:10:20+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/"&gt; &lt;img alt="Artificial Analysis just refreshed their global model indices" src="https://b.thumbs.redditmedia.com/khnyMcPkgJnHEIEq8WBEM3-umVMrZJXjtfMCDoV1-_c.jpg" title="Artificial Analysis just refreshed their global model indices" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The v4.0 mix includes: GDPval-AA, ùúè¬≤-Bench Telecom, Terminal-Bench Hard, SciCode, AA-LCR, AA-Omniscience, IFBench, Humanity's Last Exam, GPQA Diamond, CritPt.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;REMOVED&lt;/strong&gt;: MMLU-Pro, AIME 2025, LiveCodeBench, and probably Global-MMLU-Lite.&lt;/p&gt; &lt;p&gt;I did the math on the weights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agents + Terminal Use = ~42%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scientific Reasoning = 25%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Omniscience/Hallucination = 12.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coding:&lt;/strong&gt; They literally prioritized Terminal-Bench over algorithmic coding ( SciCode only).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, the benchmark has shifted to being purely corporate. It doesn't measure &amp;quot;Intelligence&amp;quot; anymore, it measures &amp;quot;How good is this model at being an office clerk?&amp;quot;. If a model isn't fine-tuned to perfectly output JSON for tool calls (like DeepSeek-V3.2-Speciale), it gets destroyed in the rankings even if it's smarter.&lt;/p&gt; &lt;p&gt;They are still updating it, so there may be inaccuracies.&lt;/p&gt; &lt;p&gt;&lt;a href="https://artificialanalysis.ai/?models=gpt-oss-120b%2Cgpt-5-2-non-reasoning%2Cgpt-5-2%2Cgpt-5-1%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-3-pro%2Cgemini-3-flash%2Cgemini-3-flash-reasoning%2Cclaude-opus-4-5%2Cclaude-4-5-sonnet-thinking%2Cclaude-4-5-sonnet%2Cclaude-opus-4-5-thinking%2Cmistral-large-3%2Cdeepseek-r1%2Cdeepseek-v3-2%2Cdeepseek-v3-2-reasoning%2Cgrok-4%2Cgrok-4-1-fast%2Cgrok-4-1-fast-reasoning%2Cnova-2-0-pro-reasoning-medium%2Cnova-2-0-lite-reasoning-medium%2Clfm2-1-2b%2Cminimax-m2-1%2Cnvidia-nemotron-3-nano-30b-a3b-reasoning%2Ckimi-k2-thinking%2Ckimi-k2-0905%2Colmo-3-1-32b-think%2Colmo-3-7b-instruct%2Cmimo-v2-flash-reasoning%2Ckat-coder-pro-v1%2Cmi-dm-k-2-5-pro-dec28%2Cglm-4-5-air%2Cglm-4-6v-reasoning%2Cglm-4-7%2Cglm-4-7-non-reasoning%2Capriel-v1-6-15b-thinker%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-next-80b-a3b-reasoning%2Cqwen3-coder-30b-a3b-instruct%2Cqwen3-235b-a22b-instruct-2507%2Cqwen3-0.6b-instruct%2Cglm-4-6&amp;amp;intelligence-category=reasoning-vs-non-reasoning&amp;amp;media-leaderboards=text-to-video&amp;amp;omniscience=omniscience-index&amp;amp;speed=intelligence-vs-speed#artificial-analysis-intelligence-index#artificial-analysis-intelligence-index"&gt;AA Link with my list models&lt;/a&gt; | &lt;a href="https://artificialanalysis.ai/"&gt;Artificial Analysis&lt;/a&gt; | &lt;a href="https://artificialanalysis.ai/evaluations"&gt;All Evals (include LiveCodeBench , AIME 2025 and etc)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q5fs95"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5fs95/artificial_analysis_just_refreshed_their_global/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:10:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5xftv</id>
    <title>I built my own AMD based AI rig</title>
    <updated>2026-01-06T22:34:27+00:00</updated>
    <author>
      <name>/u/Clear_Lead4099</name>
      <uri>https://old.reddit.com/user/Clear_Lead4099</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"&gt; &lt;img alt="I built my own AMD based AI rig" src="https://a.thumbs.redditmedia.com/JRNumikzIO0w3wHXEDVPMYhStJyKIcMBuaBxnn0mKx4.jpg" title="I built my own AMD based AI rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As promised after some trial and error, here is my baby: 256gb/256gb vram/ram, 8 GPU AMD R9700, Epyc 7532 CPU, 4TB nvme storage (and planned 24GB ssd raid) AI rig. It runs on Debian 12. I didn't go Nvidia route because I hate ugly monopolies and fucking crooks extorting money from us - hobbists. AMD path was the only feasible way for me to move forward with this. I do HPC and AI inference via llama.cpp and vllm on it. I plan to use it for local training for SST and TTS models. Largest model I run so far is MiniMax 2.1 Q8 gguf. Below is the equipment list and cost. I built it over the course of last 12 month, so prices for MB, Memory, NVMe drives, PSUs are what they were back then. GPUs and SlimSAS hardware were bought in last two month as well as last PSU. The only issue I had is PCIe AER errors. The culprit seems to be either SlimSAS raisers, cables or two slot adapters. Downgrading PCIe bus speed to Gen3 seem fixed these. Happy to answer any questions.&lt;/p&gt; &lt;p&gt;my /etc/default/grub settings:&lt;/p&gt; &lt;p&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet nosmt amdgpu.runpm=0 irqpoll pci=noaer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rnu7la9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1248802c0c6f3c03807b30320b1bf304e0661626"&gt;https://preview.redd.it/rnu7la9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1248802c0c6f3c03807b30320b1bf304e0661626&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mn2x7a9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f0d920dc8abed7ab2356c97cc0be6d281d0e5b76"&gt;https://preview.redd.it/mn2x7a9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f0d920dc8abed7ab2356c97cc0be6d281d0e5b76&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c8fyjbtl1tbg1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=641b7c4d36ac5f58abcebceaa236aa6f3a9e9704"&gt;Cost before taxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cp7licb52tbg1.png?width=4080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1309390a34d447fdcc23402cb34563414b58bfff"&gt;PCIe4 errors&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clear_Lead4099"&gt; /u/Clear_Lead4099 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T22:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5f1jz</id>
    <title>Liquid AI released LFM2.5 1.2B Instruct</title>
    <updated>2026-01-06T10:28:50+00:00</updated>
    <author>
      <name>/u/KaroYadgar</name>
      <uri>https://old.reddit.com/user/KaroYadgar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/"&gt; &lt;img alt="Liquid AI released LFM2.5 1.2B Instruct" src="https://preview.redd.it/e1qsc3urhpbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55542d5e224e490c112febafbc853ee412d376d2" title="Liquid AI released LFM2.5 1.2B Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, we release LFM2.5, our most capable family of tiny on-device foundation models.&lt;/p&gt; &lt;p&gt;It‚Äôs built to power reliable on-device agentic applications: higher quality, lower latency, and broader modality support in the ~1B parameter class.&lt;/p&gt; &lt;p&gt;&amp;gt; LFM2.5 builds on our LFM2 device-optimized hybrid architecture&lt;br /&gt; &amp;gt; Pretraining scaled from 10T ‚Üí 28T tokens&lt;br /&gt; &amp;gt; Expanded reinforcement learning post-training&lt;br /&gt; &amp;gt; Higher ceilings for instruction following&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KaroYadgar"&gt; /u/KaroYadgar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e1qsc3urhpbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5f1jz/liquid_ai_released_lfm25_12b_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T10:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5gklh</id>
    <title>So I've been losing my mind over document extraction in insurance for the past few years and I finally figured out what the right approach is.</title>
    <updated>2026-01-06T11:53:43+00:00</updated>
    <author>
      <name>/u/GloomyEquipment2120</name>
      <uri>https://old.reddit.com/user/GloomyEquipment2120</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been doing document extraction for insurance for a while now and honestly I almost gave up on it completely last year. Spent months fighting with accuracy issues that made no sense until I figured out what I was doing wrong.&lt;/p&gt; &lt;p&gt;everyone's using llms or tools like LlamaParse for extraction and they work fine but then you put them in an actual production env and accuracy just falls off a cliff after a few weeks. I kept thinking I picked the wrong tools or tried to brute force my way through (Like any distinguished engineer would do XD) but it turned out to be way simpler and way more annoying.&lt;/p&gt; &lt;p&gt;So if you ever worked in an information extraction project you already know that most documents have literally zero consistency. I don't mean like &amp;quot;oh the formatting is slightly different&amp;quot; , I mean every single document is structured completely differently than all the others.&lt;/p&gt; &lt;p&gt;For example in my case : a workers comp FROI from California puts the injury date in a specific box at the top. Texas puts it in a table halfway down. New York embeds it in a paragraph. Then you get medical bills where one provider uses line items, another uses narrative format, another has this weird hybrid table thing. And that's before you even get to the faxed-sideways handwritten nightmares that somehow still exist in 2026???&lt;/p&gt; &lt;p&gt;Sadly llms have no concept of document structure. So when you ask about details in a doc it might pull from the right field, or from some random sentence, or just make something up. &lt;/p&gt; &lt;p&gt;After a lot of headaches and honestly almost giving up completely, I came across a process that might save you some pain, so I thought I'd share it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Stop throwing documents at your extraction model blind. Build a classifier that figures out document type first (FROI vs medical bill vs correspondence vs whatever). Then route to type specific extraction. This alone fixed like 60% of my accuracy problems. (Really This is the golden tip ... a lot of people under estimate classification)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Don't just extract and hope. Get confidence scores for each field. &amp;quot;I'm 96% sure this is the injury date, 58% sure on this wage calc&amp;quot; Auto-process anything above 90%, flag the rest. This is how you actually scale without hiring people to validate everything AI does.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Layout matters more than you think. Vision-language models that actually see the document structure perform way better than text only approaches. I switched to Qwen2.5-VL and it was night and day.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Fine-tune on your actual documents. Generic models choke on industry-specific stuff. Fine-tuning with LoRA takes like 3 hours now and accuracy jumps 15-20%. Worth it every time.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;When a human corrects an extraction, feed that back into training. Your model should get better over time. (This will save you the struggle of having to recreate your process from scratch each time)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Wrote a little blog with more details about this implementation if anyone wants it &amp;quot;I know... Shameless self promotion). ( link in comments) &lt;/p&gt; &lt;p&gt;Anyway this is all the stuff I wish someone had told me when I was starting. Happy to share or just answer questions if you're stuck on this problem. Took me way too long to figure this out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GloomyEquipment2120"&gt; /u/GloomyEquipment2120 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gklh/so_ive_been_losing_my_mind_over_document/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:53:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5gii4</id>
    <title>DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available</title>
    <updated>2026-01-06T11:50:35+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/"&gt; &lt;img alt="DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available" src="https://external-preview.redd.it/jIuhV6ttH6YnzKpDkVMMUOo_Jh6zJvPJHB18Vyt60hU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f0e98de02fdfc544b4060d48c9cf0b20834c057" title="DeepSeek V3.2 with dense attention (disabled lightning attention) GGUF available" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It runs on regular llama.cpp builds (no extra support for DeepSeek V3.2 is needed).&lt;/p&gt; &lt;p&gt;Only Q8_0 and Q4_K_M are available.&lt;/p&gt; &lt;p&gt;Use DeepSeek V3.2 Exp jinja template saved to a file to run this model by passing options: &lt;code&gt;--jinja --chat-template-file ds32-exp.jinja&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here's the template I used in my tests: &lt;a href="https://pastebin.com/4cUXvv35"&gt;https://pastebin.com/4cUXvv35&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note that tool calls will most likely not work with this template - they are different between DS 3.2-Exp and DS 3.2.&lt;/p&gt; &lt;p&gt;I ran &lt;a href="https://github.com/fairydreaming/lineage-bench"&gt;lineage-bench&lt;/a&gt; on Q4_K_M quant deployed in llama-server (40 prompts per each difficulty level), results:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 | |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:| | 1 | deepseek/deepseek-v3.2 | 0.988 | 1.000 | 1.000 | 1.000 | 0.950 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The model got only 2 answers wrong with most difficult graph size (192). It looks like it performed even a bit better than the original DeepSeek V3.2 with sparse attention tested via API:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Nr | model_name | lineage | lineage-8 | lineage-64 | lineage-128 | lineage-192 | |-----:|:-----------------------|----------:|------------:|-------------:|--------------:|--------------:| | 1 | deepseek/deepseek-v3.2 | 0.956 | 1.000 | 1.000 | 0.975 | 0.850 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From my testing so far disabling sparse attention does not hurt the model intelligence.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;Edit: &lt;strong&gt;s/lightning attention/lightning indexer/&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/sszymczyk/DeepSeek-V3.2-nolight-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5gii4/deepseek_v32_with_dense_attention_disabled/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T11:50:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q63rju</id>
    <title>Coordinating local LLM agents without a manager: stigmergy from ant colonies</title>
    <updated>2026-01-07T02:58:03+00:00</updated>
    <author>
      <name>/u/rrrodzilla</name>
      <uri>https://old.reddit.com/user/rrrodzilla</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most multi-agent setups use a manager to delegate tasks. But managers become bottlenecks - add more agents, get diminishing returns.&lt;/p&gt; &lt;p&gt;I tried a different approach borrowed from ant colonies: agents don't communicate with each other at all. Instead, they read &amp;quot;pressure&amp;quot; signals from the shared artifact and propose changes to reduce local pressure. Coordination emerges from the environment, not orchestration.&lt;/p&gt; &lt;p&gt;Running qwen2.5-coder (1.5B) via Ollama on a shell script improvement task. Agents see shellcheck signals (errors, warnings, style issues) for their region only. High pressure = needs work. They propose patches, system validates and applies the best ones.&lt;/p&gt; &lt;p&gt;Fitness values decay over time (like ant pheromones). Even &amp;quot;fixed&amp;quot; regions gradually need re-evaluation. Prevents the system from getting stuck.&lt;/p&gt; &lt;p&gt;Early results: adding agents scales linearly until I/O bottlenecks hit. Zero inter-agent messages. Still experimenting and will post more results as I find them.&lt;/p&gt; &lt;p&gt;Write-up: &lt;a href="https://www.rodriguez.today/articles/emergent-coordination-without-managers"&gt;https://www.rodriguez.today/articles/emergent-coordination-without-managers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rrrodzilla"&gt; /u/rrrodzilla &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q63rju/coordinating_local_llm_agents_without_a_manager/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q63rju/coordinating_local_llm_agents_without_a_manager/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q63rju/coordinating_local_llm_agents_without_a_manager/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T02:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5a0if</id>
    <title>Liquid Ai released LFM2.5, family of tiny on-device foundation models.</title>
    <updated>2026-01-06T05:27:54+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/"&gt; &lt;img alt="Liquid Ai released LFM2.5, family of tiny on-device foundation models." src="https://preview.redd.it/flk7mfltznbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f4889f71adc7a8ebe91cfeb42042aae8fd240db" title="Liquid Ai released LFM2.5, family of tiny on-device foundation models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/collections/LiquidAI/lfm25"&gt;https://huggingface.co/collections/LiquidAI/lfm25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs built to power reliable on-device agentic applications: higher quality, lower latency, and broader modality support in the ~1B parameter class.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;LFM2.5 builds on LFM2 device-optimized hybrid architecture Pretraining scaled from 10T ‚Üí 28T tokens Expanded reinforcement learning post-training Higher ceilings for instruction following&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;5 open-weight model instances from a single architecture:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;General-purpose instruct model Japanese-optimized chat model Vision-language model Native audio-language model (speech in/out) Base checkpoints for deep customization&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/flk7mfltznbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5a0if/liquid_ai_released_lfm25_family_of_tiny_ondevice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T05:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5oz98</id>
    <title>LGAI-EXAONE/K-EXAONE-236B-A23B released</title>
    <updated>2026-01-06T17:30:10+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5oz98/lgaiexaonekexaone236ba23b_released/"&gt; &lt;img alt="LGAI-EXAONE/K-EXAONE-236B-A23B released" src="https://external-preview.redd.it/9yRidQD6qePtlR5IIS0obyCIBcG3P371nr_MudwlERc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2373a9e59ba6c01bea83c9849f8b68958239cf0" title="LGAI-EXAONE/K-EXAONE-236B-A23B released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5oz98/lgaiexaonekexaone236ba23b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5oz98/lgaiexaonekexaone236ba23b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T17:30:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5t0hr</id>
    <title>Building opensource Zero Server Code Intelligence Engine</title>
    <updated>2026-01-06T19:53:26+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/"&gt; &lt;img alt="Building opensource Zero Server Code Intelligence Engine" src="https://external-preview.redd.it/d3piOG55d2Fhc2JnMRc4K08WCLBencfsxOajXaBEA9NZR-l8om0wN65iL7dR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0e96e2b737fe1c01f0efd03220c91d85880ea64" title="Building opensource Zero Server Code Intelligence Engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. What all features would be useful, any integrations, cool ideas, etc?&lt;/p&gt; &lt;p&gt;site: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the crux of how it works:&lt;br /&gt; Repo parsed into Graph using AST -&amp;gt; Embeddings model running in browser creates the embeddings -&amp;gt; Everything is stored in a graph DB ( this also runs in browser through webassembly ) -&amp;gt; user sees UI visualization -&amp;gt; AI gets tools to query graph (cyfer query tool), semantic search, grep and node highlight.&lt;/p&gt; &lt;p&gt;So therefore we get a quick code intelligence engine that works fully client sided 100% private. Except the LLM provider there is no external data outlet. ( working on ollama support )&lt;/p&gt; &lt;p&gt;Would really appreciate any cool ideas / inputs / etc.&lt;/p&gt; &lt;p&gt;This is what I m aiming for right now:&lt;/p&gt; &lt;p&gt;1&amp;gt; Case 1 is quick way to chat with a repo, but then deepwiki is already there. But gitnexus has graph tools+ui so should be more accurate on audits and UI can help in visualize.&lt;/p&gt; &lt;p&gt;2&amp;gt; Downstream potential usecase will be MCP server exposed from browser itself, windsurf / cursor, etc can use it to perform codebase wise audits, blast radius detection of code changes, etc.&lt;/p&gt; &lt;p&gt;3&amp;gt; Another case might be since its fully private, devs having severe restrictions can use it with ollama or their own inference&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6xrs78taasbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T19:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5nw4k</id>
    <title>MiniMax M2 is GOATed - Agentic Capture the Flag (CTF) benchmark on GLM-4.5 air, 4.7 (+REAP), and Minimax-M2</title>
    <updated>2026-01-06T16:51:06+00:00</updated>
    <author>
      <name>/u/sixx7</name>
      <uri>https://old.reddit.com/user/sixx7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5nw4k/minimax_m2_is_goated_agentic_capture_the_flag_ctf/"&gt; &lt;img alt="MiniMax M2 is GOATed - Agentic Capture the Flag (CTF) benchmark on GLM-4.5 air, 4.7 (+REAP), and Minimax-M2" src="https://preview.redd.it/j0yzgwis8rbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=09b7a1b16aedfeb3cc6bed669cb26e857725e21f" title="MiniMax M2 is GOATed - Agentic Capture the Flag (CTF) benchmark on GLM-4.5 air, 4.7 (+REAP), and Minimax-M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sixx7"&gt; /u/sixx7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j0yzgwis8rbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5nw4k/minimax_m2_is_goated_agentic_capture_the_flag_ctf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5nw4k/minimax_m2_is_goated_agentic_capture_the_flag_ctf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T16:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5e010</id>
    <title>Supertonic2: Lightning Fast, On-Device, Multilingual TTS</title>
    <updated>2026-01-06T09:24:47+00:00</updated>
    <author>
      <name>/u/ANLGBOY</name>
      <uri>https://old.reddit.com/user/ANLGBOY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/"&gt; &lt;img alt="Supertonic2: Lightning Fast, On-Device, Multilingual TTS" src="https://external-preview.redd.it/aTZxcnNkeXU1cGJnMYKGJdezLzYbef1CYRrcNdCGvvmVdrxf390KMohjzSE6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e823aedbe91e1e801fe2a44cca03b41024e1d44" title="Supertonic2: Lightning Fast, On-Device, Multilingual TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I want to share that Supertonic now supports 5 languages:&lt;br /&gt; ÌïúÍµ≠Ïñ¥ ¬∑ Espa√±ol ¬∑ Fran√ßais ¬∑ Portugu√™s ¬∑ English&lt;/p&gt; &lt;p&gt;It‚Äôs an open-weight TTS model designed for extreme speed, minimal footprint, and flexible deployment. You can also use it for commercial use!&lt;/p&gt; &lt;p&gt;Here are key features:&lt;/p&gt; &lt;p&gt;(1) Lightning fast ‚Äî RTF 0.006 on M4 Pro&lt;/p&gt; &lt;p&gt;(2) Lightweight ‚Äî 66M parameters&lt;/p&gt; &lt;p&gt;(3) On-device TTS ‚Äî Complete privacy, zero network latency&lt;/p&gt; &lt;p&gt;(4) Flexible deployment ‚Äî Runs on browsers, PCs, mobiles, and edge devices&lt;/p&gt; &lt;p&gt;(5) 10 preset voices ‚Äî Pick the voice that fits your use cases&lt;/p&gt; &lt;p&gt;(6) Open-weight model ‚Äî Commercial use allowed (&lt;a href="https://huggingface.co/Supertone/supertonic-2/blob/main/LICENSE"&gt;OpenRAIL-M&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I hope Supertonic is useful for your projects.&lt;/p&gt; &lt;p&gt;[Demo] &lt;a href="https://huggingface.co/spaces/Supertone/supertonic-2"&gt;https://huggingface.co/spaces/Supertone/supertonic-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Model] &lt;a href="https://huggingface.co/Supertone/supertonic-2"&gt;https://huggingface.co/Supertone/supertonic-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;[Code] &lt;a href="https://github.com/supertone-inc/supertonic"&gt;https://github.com/supertone-inc/supertonic&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANLGBOY"&gt; /u/ANLGBOY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k40jciwu5pbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5e010/supertonic2_lightning_fast_ondevice_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T09:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5ta4l</id>
    <title>llama-benchy - llama-bench style benchmarking for ANY LLM backend</title>
    <updated>2026-01-06T20:02:33+00:00</updated>
    <author>
      <name>/u/Eugr</name>
      <uri>https://old.reddit.com/user/Eugr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: I've built this tool primarily for myself as I couldn't easily compare model performance across different backends in the way that is easy to digest and useful for me. I decided to share this in case someone has the same need.&lt;/p&gt; &lt;h2&gt;Why I built this?&lt;/h2&gt; &lt;p&gt;As probably many of you here, I've been happily using llama-bench to benchmark local models performance running in llama.cpp. One great feature is that it can help to evaluate performance at different context lengths and present the output in a table format that is easy to digest.&lt;/p&gt; &lt;p&gt;However, llama.cpp is not the only inference engine I use, I also use SGLang and vLLM. But llama-bench can only work with llama.cpp, and other benchmarking tools that I found are more focused on concurrency and total throughput.&lt;/p&gt; &lt;p&gt;Also, llama-bench performs measurements using the C++ engine directly which is not representative of the end user experience which can be quite different in practice.&lt;/p&gt; &lt;p&gt;vLLM has its own powerful benchmarking tool, but while it can be used with other inference engines, there are a few issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can't easily measure how prompt processing speed degrades as context grows. You can use &lt;code&gt;vllm bench sweep serve&lt;/code&gt;, but it only works well with vLLM with prefix caching disabled on the server. Even with random prompts it will reuse the same prompt between multiple runs which will hit the cache in &lt;code&gt;llama-server&lt;/code&gt; for instance. So you will get very low median TTFT times and very high prompt processing speeds. &lt;/li&gt; &lt;li&gt;The TTFT measurement it uses is not actually until the first usable token, it's until the very first data chunk from the server which may not contain any generated tokens in /v1/chat/completions mode.&lt;/li&gt; &lt;li&gt;Random dataset is the only ones that allows to specify an arbitrary number of tokens, but randomly generated token sequence doesn't let you adequately measure speculative decoding/MTP.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As of today, I haven't been able to find any existing benchmarking tool that brings llama-bench style measurements at different context lengths to any OpenAI-compatible endpoint.&lt;/p&gt; &lt;h2&gt;What is llama-benchy?&lt;/h2&gt; &lt;p&gt;It's a CLI benchmarking tool that measures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt Processing (pp) and Token Generation (tg) speeds at different context lengths.&lt;/li&gt; &lt;li&gt;Allows to benchmark context prefill and follow up prompt separately.&lt;/li&gt; &lt;li&gt;Reports additional metrics, like time to first response, estimated prompt processing time and end-to-end time to first token.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It works with any OpenAI-compatible endpoint that exposes /v1/chat/completions and also:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports configurable prompt length (&lt;code&gt;--pp&lt;/code&gt;), generation length (&lt;code&gt;--tg&lt;/code&gt;), and context depth (&lt;code&gt;--depth&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Can run multiple iterations (&lt;code&gt;--runs&lt;/code&gt;) and report mean ¬± std.&lt;/li&gt; &lt;li&gt;Uses HuggingFace tokenizers for accurate token counts.&lt;/li&gt; &lt;li&gt;Downloads a book from Project Gutenberg to use as source text for prompts to ensure better benchmarking of spec.decoding/MTP models.&lt;/li&gt; &lt;li&gt;Supports executing a command after each run (e.g., to clear cache).&lt;/li&gt; &lt;li&gt;Configurable latency measurement mode to estimate server/network overhead and provide more accurate prompt processing numbers.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Quick Demo&lt;/h2&gt; &lt;p&gt;Benchmarking MiniMax 2.1 AWQ running on my dual Spark cluster with up to 100000 context:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Run without installation&lt;/h1&gt; &lt;p&gt;uvx llama-benchy --base-url http://spark:8888/v1 --model cyankiwi/MiniMax-M2.1-AWQ-4bit --depth 0 4096 8192 16384 32768 65535 100000 --adapt-prompt --latency-mode generation --enable-prefix-caching ```&lt;/p&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;th align="right"&gt;ttfr (ms)&lt;/th&gt; &lt;th align="right"&gt;est_ppt (ms)&lt;/th&gt; &lt;th align="right"&gt;e2e_ttft (ms)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;3544.10 ¬± 37.29&lt;/td&gt; &lt;td align="right"&gt;688.41 ¬± 6.09&lt;/td&gt; &lt;td align="right"&gt;577.93 ¬± 6.09&lt;/td&gt; &lt;td align="right"&gt;688.45 ¬± 6.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;36.11 ¬± 0.06&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d4096&lt;/td&gt; &lt;td align="right"&gt;3150.63 ¬± 7.84&lt;/td&gt; &lt;td align="right"&gt;1410.55 ¬± 3.24&lt;/td&gt; &lt;td align="right"&gt;1300.06 ¬± 3.24&lt;/td&gt; &lt;td align="right"&gt;1410.58 ¬± 3.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d4096&lt;/td&gt; &lt;td align="right"&gt;34.36 ¬± 0.08&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt;2562.47 ¬± 21.71&lt;/td&gt; &lt;td align="right"&gt;909.77 ¬± 6.75&lt;/td&gt; &lt;td align="right"&gt;799.29 ¬± 6.75&lt;/td&gt; &lt;td align="right"&gt;909.81 ¬± 6.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt;33.41 ¬± 0.05&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d8192&lt;/td&gt; &lt;td align="right"&gt;2832.52 ¬± 12.34&lt;/td&gt; &lt;td align="right"&gt;3002.66 ¬± 12.57&lt;/td&gt; &lt;td align="right"&gt;2892.18 ¬± 12.57&lt;/td&gt; &lt;td align="right"&gt;3002.70 ¬± 12.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d8192&lt;/td&gt; &lt;td align="right"&gt;31.38 ¬± 0.06&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt;2261.83 ¬± 10.69&lt;/td&gt; &lt;td align="right"&gt;1015.96 ¬± 4.29&lt;/td&gt; &lt;td align="right"&gt;905.48 ¬± 4.29&lt;/td&gt; &lt;td align="right"&gt;1016.00 ¬± 4.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt;30.55 ¬± 0.08&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d16384&lt;/td&gt; &lt;td align="right"&gt;2473.70 ¬± 2.15&lt;/td&gt; &lt;td align="right"&gt;6733.76 ¬± 5.76&lt;/td&gt; &lt;td align="right"&gt;6623.28 ¬± 5.76&lt;/td&gt; &lt;td align="right"&gt;6733.80 ¬± 5.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d16384&lt;/td&gt; &lt;td align="right"&gt;27.89 ¬± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt;1824.55 ¬± 6.32&lt;/td&gt; &lt;td align="right"&gt;1232.96 ¬± 3.89&lt;/td&gt; &lt;td align="right"&gt;1122.48 ¬± 3.89&lt;/td&gt; &lt;td align="right"&gt;1233.00 ¬± 3.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt;27.21 ¬± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d32768&lt;/td&gt; &lt;td align="right"&gt;2011.11 ¬± 2.40&lt;/td&gt; &lt;td align="right"&gt;16403.98 ¬± 19.43&lt;/td&gt; &lt;td align="right"&gt;16293.50 ¬± 19.43&lt;/td&gt; &lt;td align="right"&gt;16404.03 ¬± 19.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d32768&lt;/td&gt; &lt;td align="right"&gt;22.09 ¬± 0.07&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt;1323.21 ¬± 4.62&lt;/td&gt; &lt;td align="right"&gt;1658.25 ¬± 5.41&lt;/td&gt; &lt;td align="right"&gt;1547.77 ¬± 5.41&lt;/td&gt; &lt;td align="right"&gt;1658.29 ¬± 5.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt;21.81 ¬± 0.07&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d65535&lt;/td&gt; &lt;td align="right"&gt;1457.71 ¬± 0.26&lt;/td&gt; &lt;td align="right"&gt;45067.98 ¬± 7.94&lt;/td&gt; &lt;td align="right"&gt;44957.50 ¬± 7.94&lt;/td&gt; &lt;td align="right"&gt;45068.01 ¬± 7.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d65535&lt;/td&gt; &lt;td align="right"&gt;15.72 ¬± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d65535&lt;/td&gt; &lt;td align="right"&gt;840.36 ¬± 2.35&lt;/td&gt; &lt;td align="right"&gt;2547.54 ¬± 6.79&lt;/td&gt; &lt;td align="right"&gt;2437.06 ¬± 6.79&lt;/td&gt; &lt;td align="right"&gt;2547.60 ¬± 6.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d65535&lt;/td&gt; &lt;td align="right"&gt;15.63 ¬± 0.02&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d100000&lt;/td&gt; &lt;td align="right"&gt;1130.05 ¬± 1.89&lt;/td&gt; &lt;td align="right"&gt;88602.31 ¬± 148.70&lt;/td&gt; &lt;td align="right"&gt;88491.83 ¬± 148.70&lt;/td&gt; &lt;td align="right"&gt;88602.37 ¬± 148.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d100000&lt;/td&gt; &lt;td align="right"&gt;12.14 ¬± 0.02&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d100000&lt;/td&gt; &lt;td align="right"&gt;611.01 ¬± 2.50&lt;/td&gt; &lt;td align="right"&gt;3462.39 ¬± 13.73&lt;/td&gt; &lt;td align="right"&gt;3351.90 ¬± 13.73&lt;/td&gt; &lt;td align="right"&gt;3462.42 ¬± 13.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d100000&lt;/td&gt; &lt;td align="right"&gt;12.05 ¬± 0.03&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;llama-benchy (0.1.0) date: 2026-01-06 11:44:49 | latency mode: generation&lt;/p&gt; &lt;h2&gt;GitHub&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/eugr/llama-benchy"&gt;https://github.com/eugr/llama-benchy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eugr"&gt; /u/Eugr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T20:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q62pyh</id>
    <title>Why not Qwen3-30B Quantized over qwen3-14B or gemma-12B?</title>
    <updated>2026-01-07T02:12:35+00:00</updated>
    <author>
      <name>/u/arktik7</name>
      <uri>https://old.reddit.com/user/arktik7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I am learning :)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have a 3080ti with 12GB of VRAM and 32GB of RAM and a 5900x. With this I can run qwen3-30b-a3b-thinking-2507 that does 3.3B activated parameters in LM studio 20 tok/sec which I believe is quantized right? It runs pretty well and has good answers. Why would I use the more recommended ones of qwen3-14b or gemma 12b over this that I see more often recommended for a computer of my specs?&lt;/p&gt; &lt;p&gt;My use case is primarily just a general AI that I can ask have search the web, clean up writing, troubleshoot IT issues on my homelab, and ask general questions. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arktik7"&gt; /u/arktik7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T02:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5qsvd</id>
    <title>The FinePDFs üìÑ Book</title>
    <updated>2026-01-06T18:34:44+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"&gt; &lt;img alt="The FinePDFs üìÑ Book" src="https://b.thumbs.redditmedia.com/-QMdTTGDscpwqAKvkKUZy4gqMc1FCD1LS2ACweeZBME.jpg" title="The FinePDFs üìÑ Book" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends, Hynek from HuggingFace here. &lt;/p&gt; &lt;p&gt;We have released FinePDFs dataset of 3T tokens last year and we felt obliged to share the knowledge with there rest of OSS community. &lt;/p&gt; &lt;p&gt;The HuggingFace Press, has been pulling an extra hours through the Christmas, to put everything we know about PDFs inside:&lt;br /&gt; - How to make the SoTA PDFs dataset?&lt;br /&gt; - How much old internet is dead now?&lt;br /&gt; - Why we chose RolmOCR for OCR&lt;br /&gt; - What's the most Claude like OSS model?&lt;br /&gt; - Why is the horse racing site topping the FinePDFs URL list? &lt;/p&gt; &lt;p&gt;We hope you like it :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z49knj5fwrbg1.png?width=1373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8"&gt;https://preview.redd.it/z49knj5fwrbg1.png?width=1373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T18:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5mh84</id>
    <title>Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)</title>
    <updated>2026-01-06T16:00:10+00:00</updated>
    <author>
      <name>/u/A-Rahim</name>
      <uri>https://old.reddit.com/user/A-Rahim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/"&gt; &lt;img alt="Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)" src="https://preview.redd.it/lf2sfats4rbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f9376d0c0fdcd670b38f3bb1ea143dc497573f" title="Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I've been working on something for Mac users in the ML space. &lt;/p&gt; &lt;p&gt;Unsloth-MLX - an MLX-powered library that brings the Unsloth fine-tuning experience to Apple Silicon. &lt;/p&gt; &lt;p&gt;The idea is simple: &lt;/p&gt; &lt;p&gt;‚Üí Prototype your LLM fine-tuning locally on Mac&lt;br /&gt; ‚Üí Same code works on cloud GPUs with original Unsloth&lt;br /&gt; ‚Üí No API changes, just swap the import &lt;/p&gt; &lt;p&gt;Why? Cloud GPU costs add up fast during experimentation. Your Mac's unified memory (up to 512GB on Mac Studio) is sitting right there. &lt;/p&gt; &lt;p&gt;It's not a replacement for Unsloth - it's a bridge for local development before scaling up. &lt;/p&gt; &lt;p&gt;Still early days - would really appreciate feedback, bug reports, or feature requests.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ARahim3/unsloth-mlx"&gt;https://github.com/ARahim3/unsloth-mlx&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A-Rahim"&gt; /u/A-Rahim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lf2sfats4rbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T16:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q617ug</id>
    <title>Razer is demonstrating a ‚ÄúAI accelerator‚Äù box with a Wormhole n150 processor from Tenstorrent at CES</title>
    <updated>2026-01-07T01:07:27+00:00</updated>
    <author>
      <name>/u/Hasuto</name>
      <uri>https://old.reddit.com/user/Hasuto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"&gt; &lt;img alt="Razer is demonstrating a ‚ÄúAI accelerator‚Äù box with a Wormhole n150 processor from Tenstorrent at CES" src="https://external-preview.redd.it/55S8efCmWR_UNwVQwnrBhqr5tzC73SFwKgXTxGt7lKs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d858518876fe54114627febc2a60feabfd21a89" title="Razer is demonstrating a ‚ÄúAI accelerator‚Äù box with a Wormhole n150 processor from Tenstorrent at CES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is a press release from Tenstorrent as well, but I haven‚Äôt seen anyone test it out.&lt;/p&gt; &lt;p&gt;From what I‚Äôve seen before the hardware isn‚Äôt super impressive. The n150 usually comes as a PCIe dev board with 12GB memory for $1000.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hasuto"&gt; /u/Hasuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/razer-partners-tenstorrent-goes-into-full-ai-mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T01:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5vk9m</id>
    <title>200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring</title>
    <updated>2026-01-06T21:24:42+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"&gt; &lt;img alt="200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring" src="https://external-preview.redd.it/hCm8D9e9AzrbuM1MK1zF3wZVeIaff34g_KhZMmvJyGM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9c0e4b8d5b4eb54a2abe96ddff2503d4b7f22f" title="200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the inference strategy:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Embed your query using a dense embedding model into a 'standard' fp32 embedding&lt;/li&gt; &lt;li&gt;Quantize the fp32 embedding to binary: 32x smaller&lt;/li&gt; &lt;li&gt;Use an approximate (or exact) binary index to retrieve e.g. 40 documents (~20x faster than a fp32 index)&lt;/li&gt; &lt;li&gt;Load int8 embeddings for the 40 top binary documents from disk.&lt;/li&gt; &lt;li&gt;Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings&lt;/li&gt; &lt;li&gt;Sort the 40 documents based on the new scores, grab the top 10&lt;/li&gt; &lt;li&gt;Load the titles/texts of the top 10 documents&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This requires:&lt;br /&gt; - Embedding all of your documents once, and using those embeddings for:&lt;br /&gt; - A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate&lt;br /&gt; - A int8 &amp;quot;view&amp;quot;, i.e. a way to load the int8 embeddings from disk efficiently given a document ID&lt;/p&gt; &lt;p&gt;Instead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index.&lt;/p&gt; &lt;p&gt;By loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore ~99% of the performance of the fp32 search, compared to ~97% when using purely the binary index: &lt;a href="https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring"&gt;https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check out the demo that allows you to test this technique on 40 million texts from Wikipedia: &lt;a href="https://huggingface.co/spaces/sentence-transformers/quantized-retrieval"&gt;https://huggingface.co/spaces/sentence-transformers/quantized-retrieval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'.&lt;/p&gt; &lt;p&gt;In short: your retrieval doesn't need to be so expensive!&lt;/p&gt; &lt;p&gt;Sources:&lt;br /&gt; - &lt;a href="https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a"&gt;https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://huggingface.co/blog/embedding-quantization"&gt;https://huggingface.co/blog/embedding-quantization&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://cohere.com/blog/int8-binary-embeddings"&gt;https://cohere.com/blog/int8-binary-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/sentence-transformers/quantized-retrieval"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T21:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5dnyw</id>
    <title>Performance improvements in llama.cpp over time</title>
    <updated>2026-01-06T09:03:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"&gt; &lt;img alt="Performance improvements in llama.cpp over time" src="https://preview.redd.it/lsqwma772pbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5492d32ea47c504ec0399a9c2a02a046df6fc0" title="Performance improvements in llama.cpp over time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lsqwma772pbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T09:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q61wpv</id>
    <title>NousResearch/NousCoder-14B ¬∑ Hugging Face</title>
    <updated>2026-01-07T01:37:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"&gt; &lt;img alt="NousResearch/NousCoder-14B ¬∑ Hugging Face" src="https://external-preview.redd.it/5B9NQ05vM8G6MB5IJCuanVCmVkz4L0DmuyDbK4fKfHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d0894b4bfc8c3fc81d8f7c59a878b6ad54d6614" title="NousResearch/NousCoder-14B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from NousResearch:&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce &lt;em&gt;NousCoder-14B&lt;/em&gt;, a competitive programming model post-trained on &lt;a href="https://huggingface.co/Qwen/Qwen3-14B"&gt;Qwen3-14B&lt;/a&gt; via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/NousCoder-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T01:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5m2n6</id>
    <title>A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time</title>
    <updated>2026-01-06T15:45:12+00:00</updated>
    <author>
      <name>/u/ali_byteshape</name>
      <uri>https://old.reddit.com/user/ali_byteshape</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"&gt; &lt;img alt="A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time" src="https://preview.redd.it/52juwyqq0rbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39e3a291db2422f84c16930831ae926a4cb20240" title="A 30B Qwen Model Walks Into a Raspberry Pi‚Ä¶ and Runs in Real Time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We‚Äôre back with another &lt;strong&gt;ShapeLearn&lt;/strong&gt; GGUF release (&lt;a href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/"&gt;Blog&lt;/a&gt;, &lt;a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;Models&lt;/a&gt;), this time for a model that &lt;em&gt;should not&lt;/em&gt; feel this usable on small hardware‚Ä¶ and yet here we are:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; (device-optimized quant variants, llama.cpp-first).&lt;/p&gt; &lt;p&gt;We‚Äôre optimizing for TPS on a specific device without output quality falling off a cliff.&lt;/p&gt; &lt;p&gt;Instead of treating ‚Äúsmaller‚Äù as the goal, we treat memory as a budget: Fit first, then optimize TPS vs quality.&lt;/p&gt; &lt;p&gt;Why? Because llama.cpp has a quirk: ‚ÄúFewer bits‚Äù does &lt;em&gt;not&lt;/em&gt; automatically mean ‚Äúmore speed.‚Äù&lt;/p&gt; &lt;p&gt;Different quant formats trigger different kernels + decode overheads, and on GPUs you can absolutely end up with &lt;strong&gt;smaller and slower&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieve &lt;strong&gt;8.03 TPS&lt;/strong&gt; at 2.70 BPW, while retaining &lt;strong&gt;94.18% of BF16 quality&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Across devices, the pattern repeats: ShapeLearn tends to find better TPS/quality tradeoffs versus alternatives (we compare against Unsloth and MagicQuant as requested in our previous post).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What‚Äôs new/interesting in this one&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1) CPU behavior is‚Ä¶ sane (mostly)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On CPUs, once you‚Äôre past ‚Äúit fits,‚Äù &lt;strong&gt;smaller tends to be faster&lt;/strong&gt; in a fairly monotonic way. The tradeoff curve behaves like you‚Äôd expect.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) GPU behavior is‚Ä¶ quirky (kernel edition)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On GPUs, performance depends as much on &lt;strong&gt;kernel choice&lt;/strong&gt; as on memory footprint. So you often get &lt;strong&gt;sweet spots&lt;/strong&gt; (especially around ~4b) where the kernels are ‚Äúgolden path,‚Äù and pushing lower-bit can get weird.&lt;/p&gt; &lt;h1&gt;Request to the community üôè&lt;/h1&gt; &lt;p&gt;We‚Äôd &lt;em&gt;love&lt;/em&gt; feedback and extra testing from folks here, especially if you can run:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;different llama.cpp builds / CUDA backends,&lt;/li&gt; &lt;li&gt;weird batch sizes / context lengths,&lt;/li&gt; &lt;li&gt;real workloads (coding assistants, long-form, tool-ish prompts),&lt;/li&gt; &lt;li&gt;or non-NVIDIA setups (we‚Äôre aware this is where it gets spicy).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also: we heard you on the previous Reddit post and are actively working to improve our evaluation and reporting. Evaluation is currently our bottleneck, not quantization, so if you have strong opinions on what benchmarks best match real usage, we‚Äôre all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ali_byteshape"&gt; /u/ali_byteshape &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52juwyqq0rbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
