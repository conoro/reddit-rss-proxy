<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-25T15:06:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qm48ux</id>
    <title>GLM 4.7 vs MiniMax-M2.1 vs DeepSeek 3.2 for coding?</title>
    <updated>2026-01-25T00:41:56+00:00</updated>
    <author>
      <name>/u/ghulamalchik</name>
      <uri>https://old.reddit.com/user/ghulamalchik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Cline/Roo Code. I wonder what option is better for coding. I tried MiniMax M2.1 since it was free for a while as an offer and I was pleased but I wonder if the others are better before I buy anything.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghulamalchik"&gt; /u/ghulamalchik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm48ux/glm_47_vs_minimaxm21_vs_deepseek_32_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm48ux/glm_47_vs_minimaxm21_vs_deepseek_32_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm48ux/glm_47_vs_minimaxm21_vs_deepseek_32_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T00:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlnruw</id>
    <title>Personal experience with GLM 4.7 Flash Q6 (unsloth) + Roo Code + RTX 5090</title>
    <updated>2026-01-24T14:02:56+00:00</updated>
    <author>
      <name>/u/Septerium</name>
      <uri>https://old.reddit.com/user/Septerium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am much more interested in how folks experience quantized versions of new models than just looking at bar graphs, so here is my humble contribution. &lt;/p&gt; &lt;p&gt;I have been using GLM 4.7 Flash to perform a few refactoring tasks in some personal web projects and have been quite impressed by how well the model handles Roo Code without breaking apart. For this agentic tool specifically, it has been much more reliable and precise than GPT-OSS 120b, GLM 4.5 Air, or Devstral 24b.&lt;/p&gt; &lt;p&gt;Here's the llama.cpp command I used to squeeze UD-Q6_K_XL + 48k tokens of context in my RTX 5090 VRAM and get about 150 tok/s (tg):&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server --model downloaded_models/GLM-4.7-Flash-UD-Q6_K_XL.gguf --port 11433 --host &amp;quot;0.0.0.0&amp;quot; -fa on --ctx-size 48000 --temp 0.7 --top-p 1.0 --min-p 0.01 --jinja -ngl 99&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septerium"&gt; /u/Septerium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm3xxm</id>
    <title>Dual 3090s &amp; GLM-4.7-Flash: 1st prompt is great, then logic collapses. Is local AI worth the $5/day power bill?</title>
    <updated>2026-01-25T00:28:59+00:00</updated>
    <author>
      <name>/u/Merstin</name>
      <uri>https://old.reddit.com/user/Merstin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded my family's video cards, which gave me an excuse to inherit two RTX 3090s and build a dedicated local AI rig out of parts i had laying around. My goal was privacy, home automation integration, and getting into &amp;quot;vibe coding&amp;quot; (learning UE5, Home Assistant YAML, etc.).&lt;/p&gt; &lt;p&gt;I love the &lt;em&gt;idea&lt;/em&gt; of owning my data, but I'm hitting a wall on the practical value vs. cost.&lt;/p&gt; &lt;p&gt;The Hardware Cost&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rig: i7 14700K, 64GB DDR5, Dual RTX 3090s (limited to 300W each).&lt;/li&gt; &lt;li&gt;Power: My peak rate is ~$0.65/kWh. A few hours of tinkering burns ~2kW, meaning this rig could easily cost me **$5/day** in electricity if I use it heavily.&lt;/li&gt; &lt;li&gt;Comparison: For that price, I could subscribe to Claude Sonnet/GPT-4 and not worry about heat or setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running a Proxmox LXC with llama-server and Open WebUI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: GLM-4.7-Flash-UD-Q8_K_XL.gguf (Unsloth build).&lt;/li&gt; &lt;li&gt;Performance: ~2,000 t/s prompt processing, ~80 t/s generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The problem is rapid degradation. I tested it with the standard &amp;quot;Make a Flappy Bird game&amp;quot; prompt.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Turn 1: Works great. Good code, minor issues.&lt;/li&gt; &lt;li&gt;Turn 2 (Fixing issues): The logic falls apart. It hangs, stops short, or hallucinates. Every subsequent prompt gets worse.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My Launch Command:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ExecStart=/opt/llama.cpp/build/bin/llama-server \ -m /opt/llama.cpp/models/GLM-4.7-Flash-UD-Q8_K_XL.gguf \ --temp 0.7 --top-p 1.0 --min-p 0.01 --repeat-penalty 1.0 \ -ngl 99 -c 65536 -t -1 --host 0.0.0.0 --port 8080 \ --parallel 1 --n-predict 4096 --flash-attn on --jinja --fit on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Am I doing something wrong with my parameters (is &lt;code&gt;repeat-penalty 1.0&lt;/code&gt; killing the logic?), or is this just the state of 30B local models right now?&lt;/p&gt; &lt;p&gt;Given my high power costs, the results I am seeing there is limited value in the llm for me outside of some perceived data / privacy control which i'm not super concerned with.&lt;/p&gt; &lt;p&gt;Is there a hybrid setup where I use Local AI for RAG/Docs and paid API for the final code generation and get best of both worlds or something i am missing? I like messing around and learning and just these past 2 weeks I've learned so much but its just been that. &lt;/p&gt; &lt;p&gt;I am about to just sell my system and figure out paid services and local tools, talk me out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Merstin"&gt; /u/Merstin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3xxm/dual_3090s_glm47flash_1st_prompt_is_great_then/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3xxm/dual_3090s_glm47flash_1st_prompt_is_great_then/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3xxm/dual_3090s_glm47flash_1st_prompt_is_great_then/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T00:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlt3pw</id>
    <title>GLM 4.7 Flash uncensored - Balanced &amp; Aggressive variants (GGUF)</title>
    <updated>2026-01-24T17:30:56+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I made uncensored versions of the new GLM 4.7 Flash from Z.ai.&lt;/p&gt; &lt;p&gt;For those who don't know the model, it's 30B-A3B MoE, so only ~3B active params (will have fast inference!) and 200K context. Runs surprisingly well for what it is.&lt;/p&gt; &lt;p&gt;Two variants:&lt;/p&gt; &lt;p&gt;- Balanced - excellent for agentic coding stuff where you still want (uncensored) reliability&lt;/p&gt; &lt;p&gt;- Aggressive - great for every other uncensored topic&lt;/p&gt; &lt;p&gt;Quants available: FP16, Q8_0, Q6_K, Q4_K_M&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings from Z.ai:&lt;/p&gt; &lt;p&gt;- General: --temp 1.0 --top-p 0.95&lt;/p&gt; &lt;p&gt;- Agentic/tool use: --temp 0.7 --top-p 1.0&lt;/p&gt; &lt;p&gt;- Keep repeat penalty at 1.0 (or directly off)&lt;/p&gt; &lt;p&gt;- llama.cpp users: --min-p 0.01 and --jinja&lt;/p&gt; &lt;p&gt;Heads up, it currently doesn't play nice with Ollama (has some chat template issues). Works fine with llama.cpp, LM Studio, Jan, koboldcpp.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;Edit: P.S. For those looking for smaller models, I also did GPT-OSS 20B, MXFP4 - Lossless:&lt;br /&gt; - &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit2: To clarify, the aim of the abliteration versions I publish is that they are effectively lossless to their original (censored) counterparts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T17:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmkc1j</id>
    <title>Alternatives to Qwen3-coder-30B?</title>
    <updated>2026-01-25T14:20:56+00:00</updated>
    <author>
      <name>/u/skibud2</name>
      <uri>https://old.reddit.com/user/skibud2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using the qwen3-coder-30B for some time, and it is not bad. But it tends to struggle with debugging tougher issues (threading, etc). Any other models that I should try? I am running on a RTX-4090, and I just got an Ai-max-395+ 128GB. I am not looking for the best coding model. I am looking for a model that could be better at figuring out problems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skibud2"&gt; /u/skibud2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmkc1j/alternatives_to_qwen3coder30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmkc1j/alternatives_to_qwen3coder30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmkc1j/alternatives_to_qwen3coder30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlr3wj</id>
    <title>I built an open-source audiobook converter using Qwen3 TTS - converts PDFs/EPUBs to high-quality audiobooks with voice cloning support</title>
    <updated>2026-01-24T16:16:15+00:00</updated>
    <author>
      <name>/u/TheyCallMeDozer</name>
      <uri>https://old.reddit.com/user/TheyCallMeDozer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Turn any book into an audiobook with AI voice synthesis!&lt;/strong&gt; I just released an open-source tool that converts PDFs, EPUBs, DOCX, and TXT files into high-quality audiobooks using &lt;strong&gt;Qwen3 TTS&lt;/strong&gt; - the amazing open-source voice model that just went public.&lt;/p&gt; &lt;h2&gt;What it does:&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Converts any document format&lt;/strong&gt; (PDF, EPUB, DOCX, DOC, TXT) into audiobooks &lt;strong&gt;Two voice modes&lt;/strong&gt;: Pre-built speakers (Ryan, Serena, etc.) or clone any voice from a reference audio &lt;strong&gt;Always uses 1.7B model&lt;/strong&gt; for best quality &lt;strong&gt;Smart chunking&lt;/strong&gt; with sentence boundary detection &lt;strong&gt;Intelligent caching&lt;/strong&gt; to avoid re-processing &lt;strong&gt;Auto cleanup&lt;/strong&gt; of temporary files &lt;/p&gt; &lt;h2&gt;Key Features:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Custom Voice Mode&lt;/strong&gt;: Professional narrators optimized for audiobook reading&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Clone Mode&lt;/strong&gt;: Automatically transcribes reference audio and clones the voice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-format support&lt;/strong&gt;: Works with PDFs, EPUBs, Word docs, and plain text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequential processing&lt;/strong&gt;: Ensures chunks are combined in correct order&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress tracking&lt;/strong&gt;: Real-time updates with time estimates ## Quick Start: Install Qwen3 TTS (one-click install with Pinokio) Install Python dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; Place your books in &lt;code&gt;book_to_convert/&lt;/code&gt; folder Run: &lt;code&gt;python audiobook_converter.py&lt;/code&gt; Get your audiobook from &lt;code&gt;audiobooks/&lt;/code&gt; folder! ## Voice Cloning Example: &lt;code&gt;bash python audiobook_converter.py --voice-clone --voice-sample reference.wav &lt;/code&gt; The tool automatically transcribes your reference audio - no manual text input needed! ## Why I built this: I was frustrated with expensive audiobook services and wanted a free, open-source solution. Qwen3 TTS going open-source was perfect timing - the voice quality is incredible and it handles both generic speech and voice cloning really well. ## Performance:&lt;/li&gt; &lt;li&gt;Processing speed: ~4-5 minutes per chunk (1.7B model) it is a little slow im working on it&lt;/li&gt; &lt;li&gt;Quality: High-quality audio suitable for audiobooks&lt;/li&gt; &lt;li&gt;Output: MP3 format, configurable bitrate ## GitHub: üîó &lt;strong&gt;&lt;a href="https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter"&gt;https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;What do you think?&lt;/strong&gt; Have you tried Qwen3 TTS? What would you use this for?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyCallMeDozer"&gt; /u/TheyCallMeDozer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T16:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwibf</id>
    <title>What is the best general-purpose model to run locally on 24GB of VRAM in 2026?</title>
    <updated>2026-01-24T19:35:11+00:00</updated>
    <author>
      <name>/u/Paganator</name>
      <uri>https://old.reddit.com/user/Paganator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Gemma 3 27b since its release nine months ago, which is an eternity in the AI field. Has anything better been released since then that can run well on a single 3090ti?&lt;/p&gt; &lt;p&gt;I'm not looking to code, to create agents, or to roleplay; I just want a good model to chat with and get reasonably smart answers to questions. If it can view images, that's even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paganator"&gt; /u/Paganator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmj900</id>
    <title>Sanity check for small-office/homelab shopping cart.</title>
    <updated>2026-01-25T13:34:22+00:00</updated>
    <author>
      <name>/u/artisticMink</name>
      <uri>https://old.reddit.com/user/artisticMink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm about to purchase some equipment for prototyping and need a sanity check. Also perhaps some of you guys have better ideas for a setup up to 5k ‚Ç¨.&lt;/p&gt; &lt;p&gt;Here's a list of models i want to run:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-Math-7B-instruct&lt;/li&gt; &lt;li&gt;Nemotron-Orchestrator-8B&lt;/li&gt; &lt;li&gt;NuMarkdown-8B-Thinking&lt;/li&gt; &lt;li&gt;Qwen3-8B&lt;/li&gt; &lt;li&gt;Qwen3-Embedding-8B&lt;/li&gt; &lt;li&gt;xLAM-2-32b-fc-r&lt;/li&gt; &lt;li&gt;gpt-oss-120b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Being able to try ~70B dense models and large MOE would be nice, but that's negligible.&lt;/p&gt; &lt;p&gt;My use case is process automation, so I'll likely have an orchestrator model + 2-3 8b + gpt-oss-120b or a 32b dense in memory.&lt;/p&gt; &lt;p&gt;There are three setups that i consider:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup #1&lt;/strong&gt;&lt;br /&gt; Used Rack Server&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gigabyte G221-Z30 Rev. A00 - 1.200‚Ç¨&lt;/li&gt; &lt;li&gt;AMD EPYC 7402P - Included in rack server&lt;/li&gt; &lt;li&gt;256GB DDR4-3200 (8x32GB) - (2.000‚Ç¨)&lt;/li&gt; &lt;li&gt;Radeon AI Pro R9700 32GB - (1.500‚Ç¨)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sum: 4.700‚Ç¨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup #2&lt;/strong&gt;&lt;br /&gt; Linked Strix halo&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 gmktec evo-x2 128GB (2000‚Ç¨)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sum: 4.000‚Ç¨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup #3&lt;/strong&gt;&lt;br /&gt; Built from inventory&lt;/p&gt; &lt;ul&gt; &lt;li&gt;B650 mainboard (8x/8x PCIE 4.0, should be fine), from inventory&lt;/li&gt; &lt;li&gt;64GB DDR5@5600, from inventory&lt;/li&gt; &lt;li&gt;Additional Ryzen 7900X or consumer epyc ~400‚Ç¨&lt;/li&gt; &lt;li&gt;2 x Radeon AI Pro R9700 (1500‚Ç¨)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sum: 3.400‚Ç¨&lt;/p&gt; &lt;p&gt;I'm currently leaning towards &lt;strong&gt;#3&lt;/strong&gt;. It's short on RAM and large moe experimentation is out of question. Butt i can use the two R9700 for an actual production build should the need arise and it's the cheapest.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#2&lt;/strong&gt; is the easiest solution but doesn't sale at all. &lt;strong&gt;#1&lt;/strong&gt; would probably be the overall best, but I've a hard time justifying to miself paying 2k for DDR4 RAM.&lt;/p&gt; &lt;p&gt;Any thoughts on my horrible financial decisions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/artisticMink"&gt; /u/artisticMink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmj900/sanity_check_for_smallofficehomelab_shopping_cart/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmj900/sanity_check_for_smallofficehomelab_shopping_cart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmj900/sanity_check_for_smallofficehomelab_shopping_cart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjmgy</id>
    <title>Building a "Sovereign JARVIS" with Council-based Agents and Granular Knowledge Silos. Does this architecture exist yet?</title>
    <updated>2026-01-25T13:51:02+00:00</updated>
    <author>
      <name>/u/kuteguy</name>
      <uri>https://old.reddit.com/user/kuteguy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am an ex-programmer (been a Solution Architect for a long time now) and don't know any of the python, or graph/vector db concepts or node.js - so it will all be vibe coded thanks to chatGPT and/or Google AI Studio&lt;/p&gt; &lt;p&gt;I‚Äôm designing a self-hosted &amp;quot;Life OS&amp;quot; (codenamed JARVIS) because I‚Äôm tired of stateless AI chats that forget context. The core architecture uses a &amp;quot;Council of Agents&amp;quot; (e.g., Psychologist, Mentor, Strategist) that all share a unified Graph + Vector memory. This means they have deep, longitudinal context: if I told the Psychologist about a failure two years ago, the Career Mentor knows not to suggest a similar path today. They can also debate each other to give me synthesized advice rather than generic answers.&lt;/p&gt; &lt;p&gt;The unique feature I‚Äôm building is Granular Knowledge Siloing. I can upload my entire life‚ÄîPDFs, EPUBs, TXT exports of old chats, and URLs‚Äîbut I have a dashboard to toggle exactly which agent has read-access to which artifact. I can let the &amp;quot;Business Agent&amp;quot; see my financial PDFs while strictly firewalling the &amp;quot;Dating Agent&amp;quot; from them. I can also cordon off specific chat sessions so they remain private to one specific persona. I‚Äôm looking to build this using a LangGraph/Neo4j/Qdrant stack. Has anyone seen an open-source project that handles this specific mix of &amp;quot;Shared Memory&amp;quot; but &amp;quot;Permissioned Access&amp;quot; well? Of course the permissioned access is less important than the idea of agents, uploading PDFs that the agents can access and share memory across chats that is &amp;quot;always&amp;quot; in the conext window&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kuteguy"&gt; /u/kuteguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjmgy/building_a_sovereign_jarvis_with_councilbased/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjmgy/building_a_sovereign_jarvis_with_councilbased/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjmgy/building_a_sovereign_jarvis_with_councilbased/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:51:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm2q0c</id>
    <title>Claude Code, but locally</title>
    <updated>2026-01-24T23:37:23+00:00</updated>
    <author>
      <name>/u/Zealousideal-Egg-362</name>
      <uri>https://old.reddit.com/user/Zealousideal-Egg-362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm looking for advice if there is realistic replacement for anthropic's models. Looking to run claude code with models that ideally are snappier and wondering if it's possible at all to replicate the opus model on own hardware.&lt;/p&gt; &lt;p&gt;What annoys me the most is speed, especially when west coast wakes up (I'm in EU). I'd be happy to prompt more, but have model that's more responsive. Opus 4.5 i great, but the context switches totally kill my flow and I feel extremely tired in the end of the day.&lt;/p&gt; &lt;p&gt;Did some limited testing of different models via openrouter, but the landscape is extremely confusing. glm-4.7 seems like a nice coding model, but is there any practical realistic replacement for Opus 4.5?&lt;/p&gt; &lt;p&gt;Edit: I‚Äôm asking very clearly for directions how/what to replace Opus and getting ridiculously irrelevant advice ‚Ä¶&lt;/p&gt; &lt;p&gt;My budget is 5-7k&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Egg-362"&gt; /u/Zealousideal-Egg-362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T23:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmap5e</id>
    <title>Best &lt;4B dense models today?</title>
    <updated>2026-01-25T05:46:01+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think small(&amp;lt;4B) dense models are basically the only practical option for general users. But hasn't there been almost no progress since Gemma 3 4B came out? Are there any alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T05:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm0l2q</id>
    <title>I built a tool that learns your codebase's unwritten rules and conventions- no AI, just AST parsing</title>
    <updated>2026-01-24T22:11:05+00:00</updated>
    <author>
      <name>/u/Fluffy_Citron3547</name>
      <uri>https://old.reddit.com/user/Fluffy_Citron3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last six months teaching myself to orchestrate engineering codebases using AI agents. What I found is that the biggest bottleneck isn‚Äôt intelligence it‚Äôs the context window. Why have we not given agents the proper tooling to defeat this limitation? Agents constantly forget how I handle error structures or which specific components I use for the frontend. This forces mass auditing and refactoring, causing me to spend about 75% of my token budget on auditing versus writing.&lt;/p&gt; &lt;p&gt;That is why I built Drift. Drift is a first-in-class codebase intelligence tool that leverages semantic learning through AST parsing with Regex fallbacks. It scans your codebase and extracts 15 different categories with over 150 patterns. Everything is persisted and recallable via CLI or MCP in your IDE of choice.&lt;/p&gt; &lt;p&gt;What makes drift different?&lt;/p&gt; &lt;p&gt;It‚Äôs learning based not rule based. AI is capable of writing high quality code but the context limitation makes fitting conventions through a large code base extremely tedious and time consuming often leading to things silently failing or just straight up not working. &lt;/p&gt; &lt;p&gt;Drift_context is the real magic &lt;/p&gt; &lt;p&gt;Instead of an agent calling 10 tools and sytheneszing results it: &lt;/p&gt; &lt;p&gt;Takes intent &lt;/p&gt; &lt;p&gt;Takes focus area&lt;/p&gt; &lt;p&gt;Returned a curated package&lt;/p&gt; &lt;p&gt;This eliminates the audit loop, hallucination risk and gives the agent everything needed in one call.&lt;/p&gt; &lt;p&gt;Call graph analysis across 6 different languages&lt;/p&gt; &lt;p&gt;Not just ‚ÄúWhat functions exists‚Äù but..&lt;/p&gt; &lt;p&gt;Drift_reachability_forward &amp;gt; What data can this code access? (Massive for helping with security)&lt;/p&gt; &lt;p&gt;Drift_reachability_inverse &amp;gt; Who can access this field? &lt;/p&gt; &lt;p&gt;Drift_impact_analysis &amp;gt; what breaks if I change this with scoring.&lt;/p&gt; &lt;p&gt;Security-audit-grade analysis available to you or your agent through MCP or CLI&lt;/p&gt; &lt;p&gt;The MCP has been built out with frontier capabilities ensuring context is preserved and is a true tool for your agents&lt;/p&gt; &lt;p&gt;Currently support TS, PY, Java, C#, PHP, GO :&lt;/p&gt; &lt;p&gt;with‚Ä¶&lt;/p&gt; &lt;p&gt;Tree sitter parsing&lt;/p&gt; &lt;p&gt;Regex fallback&lt;/p&gt; &lt;p&gt;Framework aware detection&lt;/p&gt; &lt;p&gt;All data persist into a local file (/.drift) and you have the ability to approve, deny and ignore certain components, functions and features you don‚Äôt want the agent to be trained on.&lt;/p&gt; &lt;p&gt;check it out here: &lt;/p&gt; &lt;p&gt;IF you run into any edge cases or I don‚Äôt support the framework your code base is currently running on open a git issue feature request and ive been banging them out quick&lt;/p&gt; &lt;p&gt;Thank you for all the upvotes and stars on the project it means so much!&lt;/p&gt; &lt;p&gt;check it out here: &lt;a href="https://github.com/dadbodgeoff/drift"&gt;https://github.com/dadbodgeoff/drift&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Citron3547"&gt; /u/Fluffy_Citron3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T22:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm6iho</id>
    <title>Stable-DiffCoder, a strong code diffusion LLM built on Seed-Coder</title>
    <updated>2026-01-25T02:22:05+00:00</updated>
    <author>
      <name>/u/rektide</name>
      <uri>https://old.reddit.com/user/rektide</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rektide"&gt; /u/rektide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bytedance-seed.github.io/Stable-DiffCoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T02:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjyxl</id>
    <title>Understanding Multi-Head Latent Attention (MLA)</title>
    <updated>2026-01-25T14:05:43+00:00</updated>
    <author>
      <name>/u/shreyansh26</name>
      <uri>https://old.reddit.com/user/shreyansh26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A short deep-dive on Multi-Head Latent Attention (MLA) (from DeepSeek): intuition + math, then a walk from MHA ‚Üí GQA ‚Üí MQA ‚Üí MLA, with PyTorch code and the fusion/absorption optimizations for KV-cache efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"&gt;http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shreyansh26"&gt; /u/shreyansh26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltwza</id>
    <title>Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence.</title>
    <updated>2026-01-24T18:00:50+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt; &lt;img alt="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." src="https://preview.redd.it/66fd18ro6cfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f579cce389f709dbf297867095118be2027f04ea" title="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/2014786516153991339"&gt;https://x.com/ArtificialAnlys/status/2014786516153991339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A key driver of this momentum is the Korean National Sovereign AI Initiative, a government-backed, nationwide competition that incentivizes domestic model development through a multi-stage elimination process. The initiative shortlists national champions, with winners receiving direct government funding and guaranteed access to large-scale GPU capacity.&lt;/p&gt; &lt;p&gt;‚û§ In August 2025, five organizations were selected: Naver, SK Telecom, LG Group, Upstage, and NC AI&lt;/p&gt; &lt;p&gt;‚û§ In the most recent round announced last week, the field narrowed to three: LG, SK Telecom, and Upstage.&lt;/p&gt; &lt;p&gt;‚û§ A fourth finalist is expected to be selected in the coming months as the evaluation process continues&lt;/p&gt; &lt;p&gt;Generally, top Korean AI models tend to be open weights, and vary in size ranging from Motif‚Äòs 12.7B Thinking model to LG‚Äôs 236B K-EXAONE. Other models, such as Korea Telecom (KT)‚Äôs Mi:dm K 2.5 Pro, are proprietary and developed with a focus on business integration with existing KT clients.&lt;/p&gt; &lt;p&gt;Overview of major releases:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ LG | K-EXAONE -&lt;/strong&gt; The current leader in the Korean AI race and a shortlisted model in the Korean National Sovereign AI Initiative. K-EXAONE is a 236B open weights model and scores 32 on the Artificial Analysis Intelligence Index. K-EXAONE performs strongly across various intelligence evaluations from scientific reasoning, instruction following, to agentic coding. However, this model has high verbosity, using 100 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Upstage | Solar Open -&lt;/strong&gt; Another shortlisted model in the Korean National Sovereign AI Initiative. Solar Open is a 100B open-weights model and scores 21 on the Artificial Analysis Intelligence Index. Solar Open performs well in instruction following and has lower hallucination rate compared to peer Korean models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Naver | HyperCLOVA X SEED Think -&lt;/strong&gt; A 32B open weights reasoning model that scores 24 on the Artificial Analysis Intelligence Index. HyperCLOVA X SEED Think demonstrates strong performance on agentic tool-use workflows and scores highly in the Global MMLU Lite multilingual index for Korean, highlighting its potential usefulness in a primarily Korean language environment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Korea Telecom | Mi:dm K 2.5 Pro -&lt;/strong&gt; A proprietary reasoning model that scores 23 on the Artificial Analysis Intelligence Index. Mi:dm K 2.5 Pro sees strong performance in agentic tool-use. Mi:dm K 2.5 Pro currently has no publicly available endpoint. Instead, Korea Telecom primarily intends to package this model into product offerings and use this model to serve KT‚Äôs clients&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Motif | Motif-2-12.7B -&lt;/strong&gt; A small open weights model that scores 24 on the Artificial Analysis Intelligence Index. Motif-2-12.7B performs well in long-context reasoning and knowledge, but is highly token intensive - using 120 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66fd18ro6cfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmev6q</id>
    <title>[R] Open-sourcing an unfinished research project: A Self-Organizing, Graph-Based Alternative to Transformers (Looking for feedback or continuation</title>
    <updated>2026-01-25T09:40:12+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sharing a research project I worked on over a long period but had to pause due to personal reasons. Rather than letting it sit idle, I wanted to open it up to the community either for technical feedback, critique, or for anyone interested in continuing or experimenting with it.&lt;/p&gt; &lt;p&gt;The main project is called Self-Organizing State Model (SOSM): &lt;a href="https://github.com/PlanetDestroyyer/Self-Organizing-State-Model"&gt;https://github.com/PlanetDestroyyer/Self-Organizing-State-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At a high level, the goal was to explore an alternative to standard Transformer attention by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Using graph-based routing instead of dense attention&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Separating semantic representation and temporal pattern learning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Introducing a hierarchical credit/attribution mechanism for better interpretability&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The core system is modular and depends on a few supporting components: Semantic representation module (MU) &lt;a href="https://github.com/PlanetDestroyyer/MU"&gt;https://github.com/PlanetDestroyyer/MU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Temporal pattern learner (TEMPORAL) &lt;a href="https://github.com/PlanetDestroyyer/TEMPORAL"&gt;https://github.com/PlanetDestroyyer/TEMPORAL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hierarchical / K-1 self-learning mechanism &lt;a href="https://github.com/PlanetDestroyyer/self-learning-k-1"&gt;https://github.com/PlanetDestroyyer/self-learning-k-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm honestly not sure how valuable or novel this work is that‚Äôs exactly why I‚Äôm posting it here. If nothing else, I‚Äôd really appreciate constructive criticism, architectural feedback, or pointers to related work that overlaps with these ideas. If someone finds parts of it useful (or wants to take it further, refactor it, or formalize it into a paper), they‚Äôre more than welcome to do so. The project is open-source, and I‚Äôm happy to answer questions or clarify intent where needed.&lt;/p&gt; &lt;p&gt;Thanks for taking a look.&lt;/p&gt; &lt;p&gt;Summary:&lt;/p&gt; &lt;p&gt;This work explores a language model architecture based on structured semantics rather than unstructured embeddings. Instead of positional encodings, a temporal learning module is used to model sequence progression and context flow. A K-1 hierarchical system is introduced to provide interpretability, enabling analysis of how a token is predicted and which components, states, or nodes contribute to that prediction. Most importantly, rather than comparing every token with all others (as in full self-attention), the model uses a graph-based connection mechanism that restricts computation to only the most relevant or necessary tokens, enabling selective reasoning and improved efficiency.&lt;/p&gt; &lt;p&gt;(Have used claude code to code )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmhvuz</id>
    <title>Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.</title>
    <updated>2026-01-25T12:30:24+00:00</updated>
    <author>
      <name>/u/Charming_Group_2950</name>
      <uri>https://old.reddit.com/user/Charming_Group_2950</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; You build a RAG system. It gives an answer. It sounds right.&lt;br /&gt; But is it actually grounded in your data, or just hallucinating with confidence?&lt;br /&gt; A single &amp;quot;correctness&amp;quot; or &amp;quot;relevance&amp;quot; score doesn‚Äôt cut it anymore, especially in enterprise, regulated, or governance-heavy environments. We need to know why it failed. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My solution:&lt;/strong&gt;&lt;br /&gt; Introducing &lt;strong&gt;TrustifAI&lt;/strong&gt; ‚Äì a framework designed to quantify, explain, and debug the trustworthiness of AI responses. &lt;/p&gt; &lt;p&gt;Instead of pass/fail, it computes a multi-dimensional Trust Score using signals like:&lt;br /&gt; * Evidence Coverage: Is the answer actually supported by retrieved documents?&lt;br /&gt; * Epistemic Consistency: Does the model stay stable across repeated generations?&lt;br /&gt; * Semantic Drift: Did the response drift away from the given context?&lt;br /&gt; * Source Diversity: Is the answer overly dependent on a single document?&lt;br /&gt; * Generation Confidence: Uses token-level log probabilities at inference time to quantify how confident the model was while generating the answer (not after judging it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; TrustifAI doesn‚Äôt just give you a number - it gives you traceability.&lt;br /&gt; It builds &lt;strong&gt;Reasoning Graphs (DAGs)&lt;/strong&gt; and &lt;strong&gt;Mermaid visualizations&lt;/strong&gt; that show why a response was flagged as reliable or suspicious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How is this different from LLM Evaluation frameworks:&lt;/strong&gt;&lt;br /&gt; All popular Eval frameworks measure how good your RAG system is, but&lt;br /&gt; TrustifAI tells you why you should (or shouldn‚Äôt) trust a specific answer - with explainability in mind.&lt;/p&gt; &lt;p&gt;Since the library is in its early stages, I‚Äôd genuinely love community feedback.&lt;br /&gt; ‚≠ê the repo if it helps üòÑ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;code&gt;pip install trustifai&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github link:&lt;/strong&gt; &lt;a href="https://github.com/Aaryanverma/trustifai"&gt;https://github.com/Aaryanverma/trustifai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Group_2950"&gt; /u/Charming_Group_2950 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmhvuz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T12:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmbevn</id>
    <title>Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?</title>
    <updated>2026-01-25T06:22:22+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"&gt; &lt;img alt="Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?" src="https://preview.redd.it/r0ec0m21vffg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=275f52d7dfb944441f74dea25d60f8c3e620a766" title="Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on a synthetic data pipeline for training high-precision image-to-image models (Flux Klein and Qwen Image Edit). I have reached a point where standard tagging and current open-weights VL models are the main bottleneck for data quality.&lt;/p&gt; &lt;p&gt;I have benchmarked almost every trending VL model on HuggingFace and those leading the MMMU-Pro leaderboard. My conclusion is that even the best open models are &amp;quot;blind&amp;quot; to complex anatomical layering and spatial reasoning.&lt;/p&gt; &lt;p&gt;The problem is best described by the &amp;quot;Horns Issue&amp;quot; (see attached image). If a character has large organic dragon horns and a headband with small decorative horns, every open VLM I tested merges them into one generic attribute. They fail to distinguish between base anatomy and removable accessories. Gemini 3 Flash, however, is on a completely different level‚Äîit accurately describes every layer and understands the distinction perfectly.&lt;/p&gt; &lt;p&gt;My plan is to fine-tune Qwen 3 VL 32B Instruct on a dataset labeled by Gemini 3 Flash. I want to transfer that visual reasoning so I can have a local engine for high-scale synthetic captioning.&lt;/p&gt; &lt;p&gt;A few technical questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can Qwen 3 VL actually absorb this level of reasoning via SFT if it lacks the native &amp;quot;thinking&amp;quot; or CoT process Gemini uses?&lt;/li&gt; &lt;li&gt;Is the &amp;quot;blindness&amp;quot; in open models a limitation of the vision encoder itself, or is it purely a reasoning capability issue on the LLM side?&lt;/li&gt; &lt;li&gt;Has anyone here tried this kind of VLM-to-VLM distillation for high-scale labeling in generative AI pipelines?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am trying to build a local captioner that matches proprietary accuracy. Any insights on the plasticity of Qwen 32B for this specific task would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r0ec0m21vffg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T06:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmh3si</id>
    <title>What happened to moondream3?</title>
    <updated>2026-01-25T11:49:01+00:00</updated>
    <author>
      <name>/u/StableDiffer</name>
      <uri>https://old.reddit.com/user/StableDiffer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So last year the moondream 3 preview came out. It was a nice performing visual model that could do some cool stuff other VL models couldn't. One month ago a MLX version appeared &lt;a href="https://huggingface.co/moondream/md3p-int4"&gt;https://huggingface.co/moondream/md3p-int4&lt;/a&gt; but until now there is no llama.cpp implementation and no public activity I could find.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableDiffer"&gt; /u/StableDiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T11:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmevh7</id>
    <title>Blazing fast JSON extraction with very small LLMs-3B: LSTM to LLM</title>
    <updated>2026-01-25T09:40:42+00:00</updated>
    <author>
      <name>/u/memphet</name>
      <uri>https://old.reddit.com/user/memphet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've learned a lot from this sub, so I wanted to give back by sharing my experience on a recent project.&lt;/p&gt; &lt;p&gt;My goal was to migrate a text extraction pipeline from LSTM to an LLM. The task involves extracting specific data into JSON format from small text inputs (‚âà1024 tokens). I used in-house data to fine-tune it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraints &amp;amp; Achievements (running on an L4 GPU):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Very low end2end latency:&lt;/strong&gt; &amp;lt;500ms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High throughput:&lt;/strong&gt; ‚âà30 RPM (requests per minute)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; 0.99 accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Model:&lt;/strong&gt;&lt;br /&gt; I tested quite a few models for this task.&lt;br /&gt; Ultimately, HuggingFaceTB/SmolLM3-3B was the best fit for our needs.&lt;br /&gt; I also had very strong results with Qwen/Qwen3-4B-Instruct and Ministral&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning parameters matter less than I thought:&lt;/strong&gt; I didn't see huge gains from strictly tweaking hyperparameters. I ran extensive hyperparameter optimization only to find that simply increasing the number of epochs yielded the best (slight) improvements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data is king:&lt;/strong&gt; Poor labeling logic and bad data quality hurt me the most. If I had to redo it, I would spend much more time cleaning and validating the dataset upfront.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small LLMs struggle with Proper Nouns:&lt;/strong&gt; I noticed about a 10% error rate on names! A significant performance boost came from adding a simple post-processing step using Levenshtein distance to correct names extracted by the LLM against the input text (correcting &amp;quot;Jammes&amp;quot; -&amp;gt; &amp;quot;James&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency Gains:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Obviously the best bang for your buck. I recommend &lt;strong&gt;FP8&lt;/strong&gt; using llm-compressor if you have a Lovelace GPU or newer. Otherwise, &lt;strong&gt;AWQ&lt;/strong&gt; is solid. &lt;ul&gt; &lt;li&gt;Gain: ~50% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Formatting:&lt;/strong&gt; You want to generate as few tokens as possible. Instead of fine-tuning for a verbose JSON output like {&amp;quot;key1&amp;quot;: &amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot;: &amp;quot;value2&amp;quot;}, I fine-tuned the model to output just the values: value1,value2. &lt;ul&gt; &lt;li&gt;Gain: ~30% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't work (for me):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I really tried to make &lt;strong&gt;Speculative Decoding&lt;/strong&gt; work with vLLM. In theory, I expected gains even with just n-gram speculative decoding, but I didn't observe any improvement. I did see some speedup using Qwen 0.7B draft model, but since I ultimately chose a different base model architecture, I couldn't use them effectively. Plus, maintaining a base model + a draft model is a pain, which is also why I didn't go with Eagle.&lt;/p&gt; &lt;p&gt;If you have suggestions to squeeze out more performance or thoughts on the setup, I'm all ears!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/memphet"&gt; /u/memphet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and it‚Äôs a significant step forward for local speech synthesis. If you‚Äôve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAI‚Äôs TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;We‚Äôve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAI‚Äôs TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ü§ó Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;üìÑ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;üíª Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmdf2a</id>
    <title>Has anyone got GLM 4.7 flash to not be shit?</title>
    <updated>2026-01-25T08:14:08+00:00</updated>
    <author>
      <name>/u/synth_mania</name>
      <uri>https://old.reddit.com/user/synth_mania</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Real talk. I feel like everyday I'm downloading a new quant and trying it out and not once have I got it to consistently work without looping.&lt;/p&gt; &lt;p&gt;I've tried with and without the suggested settings from unsloth, &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;, and others, to no avail.&lt;/p&gt; &lt;p&gt;Additionally, this has to be the slowest inference I've ever seen from a 30B A3B model. In all fairness, my only point of reference is Qwen3 Coder, but compared to that at least, the token generation speed feels positively lethargic.&lt;/p&gt; &lt;p&gt;If anybody has any tips, please let me know because I feel like I'm going in circles here. I don't think I've ever seen a modern release that had this many issues right off the bat, with no apparent improvement after a few supposed fixes.&lt;/p&gt; &lt;p&gt;It's really unfortunate because I can see the potential this model has. The chain of thought in particular seems uniquely coherent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synth_mania"&gt; /u/synth_mania &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T08:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn‚Äôt even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; ‚Äî image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; ‚Äî we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
