<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-15T04:37:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o6rr4q</id>
    <title>enabling MIG on RTX PRO 6000</title>
    <updated>2025-10-14T21:04:33+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: to enable MIG on RTX PRO 6000 you need vBIOS 98.02.81.00.07 or newer + you need to use &lt;code&gt;displaymodeselector&lt;/code&gt; tool to set GPU into &amp;quot;compute mode&amp;quot; by disabling its graphics output ports. I'm creating this thread to make Google and other search engines index it, as nobody in the world knows how to fix the &lt;code&gt;displaymodeselector&lt;/code&gt; error.&lt;/p&gt; &lt;p&gt;If you run &lt;code&gt;displaymodeselector&lt;/code&gt; tool and encounter an error like&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PROGRAMMING ERROR: HW access out of range. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;or&lt;/p&gt; &lt;pre&gt;&lt;code&gt;terminate called after throwing an instance of 'std::runtime_error' what(): mmap(): /dev/mem[ Base addrres = 0xf4000000, size = 0x04000000] Attempt to map physical memory failed. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;then add &lt;code&gt;iomem=relaxed&lt;/code&gt; to the kernel boot parameters and it will work. Also disabling IOMMU might have helped (&lt;code&gt;iommu=off intel_iommu=off amd_iommu=off&lt;/code&gt;) but I am not sure about it.&lt;/p&gt; &lt;p&gt;If you have a &amp;quot;Workstation&amp;quot; full sized card then you could get the vBIOS update here: &lt;a href="https://files.catbox.moe/8p9ahy.zip"&gt;https://files.catbox.moe/8p9ahy.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mirror: &lt;a href="https://biteblob.com/Information/puLsgEabWaORud/#RTXPro6000WSv9802810007.zip"&gt;https://biteblob.com/Information/puLsgEabWaORud/#RTXPro6000WSv9802810007.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you have &amp;quot;Max-Q&amp;quot; or &amp;quot;server edition&amp;quot; cards then you have to beg your vendor and highly likely they will ignore your request LOL. However if you have the vBIOS update files for these versions then please share them here to help other happy owners of 6000 series.&lt;/p&gt; &lt;p&gt;Getting &lt;code&gt;displaymodeselector&lt;/code&gt; is much easier than vBIOS, you &amp;quot;just&amp;quot; need to register on Nvidia developer portal. Or download it here: &lt;a href="https://files.catbox.moe/qewqna.zip"&gt;https://files.catbox.moe/qewqna.zip&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mirror: &lt;a href="https://biteblob.com/Information/VNJgaJHnV55VCf/#NVIDIA_Display_Mode_Selector_Tool-1.72.0-July25.zip"&gt;https://biteblob.com/Information/VNJgaJHnV55VCf/#NVIDIA_Display_Mode_Selector_Tool-1.72.0-July25.zip&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rr4q/enabling_mig_on_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rr4q/enabling_mig_on_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rr4q/enabling_mig_on_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T21:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o713n9</id>
    <title>Is there any way to have multiple LLMs talk to each other? If yes, how?</title>
    <updated>2025-10-15T04:04:07+00:00</updated>
    <author>
      <name>/u/CatSweaty4883</name>
      <uri>https://old.reddit.com/user/CatSweaty4883</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I currently own a humble RTX3060, 12GB vram, 16GB pc RAM. I was wondering if it was possible to have multiple LLMs (small in size) to load and talk to each other in an environment. How do I achieve this? And if my compute isn’t enough, how much of computing am I looking at? Looking for guidance, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CatSweaty4883"&gt; /u/CatSweaty4883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o713n9/is_there_any_way_to_have_multiple_llms_talk_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o713n9/is_there_any_way_to_have_multiple_llms_talk_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o713n9/is_there_any_way_to_have_multiple_llms_talk_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T04:04:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69vm5</id>
    <title>What’s the point of a DGX Spark for inference if a Mac Studio M1 Ultra beats it at TG and equals it at PP at half the price?</title>
    <updated>2025-10-14T08:28:09+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be missing something here, but with the results I’ve seen, the DGX does what Apple did 3 years ago (actually worse token generation). &lt;/p&gt; &lt;p&gt;Is the DGX as bad as it seems for inference? We all knew that TG would have been shit with that bandwidth, but even prompt processing doesn’t seem great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6yz59</id>
    <title>[WebGPU Demo] Granite Docling 258M — document parsing 100% in-browser (HF Space)</title>
    <updated>2025-10-15T02:18:14+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run IBM’s &lt;strong&gt;Granite-Docling-258M&lt;/strong&gt; entirely in your browser via &lt;strong&gt;WebGPU + Transformers.js&lt;/strong&gt; to convert scanned pages/images into structured &lt;strong&gt;HTML&lt;/strong&gt;—no data leaves your machine. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Upload &lt;strong&gt;PNG/JPG/WEBP&lt;/strong&gt; → get clean HTML. &lt;/li&gt; &lt;li&gt;Local/WebGPU execution = privacy-friendly.&lt;/li&gt; &lt;li&gt;Link: &lt;a href="https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU"&gt;&lt;code&gt;https://huggingface.co/spaces/ibm-granite/granite-docling-258M-WebGPU&lt;/code&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6yz59/webgpu_demo_granite_docling_258m_document_parsing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T02:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ik01</id>
    <title>GLM-4.6 | Gut feel after sparring with Sonnet for half a day: more of a “steady player”</title>
    <updated>2025-10-14T15:25:37+00:00</updated>
    <author>
      <name>/u/xieyutong</name>
      <uri>https://old.reddit.com/user/xieyutong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Cutting to the chase: it feels steadier, especially for small code-review fixes, short-chain reasoning, and toning down overhyped copy. Officially, they say across eight public benchmarks (like AIME25, LCB v6, HLE, SWE-Bench Verified, BrowseComp, Terminal-Bench, τ²-Bench, GPQA) it’s overall aligned with Sonnet 4, parts of its coding performance approach Sonnet 4.5, and there’s a “48.6% ties” line. I don’t obsess over perfect number matching; what matters is that I can reproduce results and it saves me hassle.&lt;/p&gt; &lt;p&gt;I used it for three things. First, code review. I told it “only fix unsafe code and keep function signatures,” and it gave a diff-like display, then pasted the full function; very low reading overhead. Second, terminal task planning. I didn’t let it actually run commands; I just wanted a small blueprint of “plan → expected output → fallback path.” It gave a clean structure that I could execute manually. Third, neutralizing overly promotional copy its touch is just right, and it keeps the numbers and sources.&lt;/p&gt; &lt;p&gt;I put GLM-4.6 into four everyday buckets: small code fixes, short-chain reasoning, tool awareness (planning only, no network), and rewriting. Settings per the official guidance: temperature = 1.0; for code, top_p = 0.95 and top_k = 40; 200K context makes reproducibility easier. For routine code/writing/short-chain reasoning, you can use it as-is; for heavy retrieval and strong evidence chains, plug in your own tools first and swap it in afterward.&lt;/p&gt; &lt;p&gt;Reference: &lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;https://huggingface.co/zai-org/GLM-4.6&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xieyutong"&gt; /u/xieyutong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ik01/glm46_gut_feel_after_sparring_with_sonnet_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ik01/glm46_gut_feel_after_sparring_with_sonnet_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ik01/glm46_gut_feel_after_sparring_with_sonnet_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:25:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6xmok</id>
    <title>GLM-4.6 worse in German than GLM-4.5 - Why?</title>
    <updated>2025-10-15T01:15:28+00:00</updated>
    <author>
      <name>/u/Evening_Ad6637</name>
      <uri>https://old.reddit.com/user/Evening_Ad6637</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I know that GLM-4.6 is clearly superior to its predecessor checkpoint 4.5 in many respects. But I have noticed that the German language has become significantly worse (in terms of grammar and style). After several tests, I can even say with certainty that it has also become significantly worse than that of GLM-4.5-Air.&lt;/p&gt; &lt;p&gt;I observed this &amp;quot;trend&amp;quot; some time ago with other models as well, e.g. with Qwen-2.5 to Qwen-3, with Claude-Sonnet-3.5 to Sonnet 4.0, with GPT-4o models etc.&lt;/p&gt; &lt;p&gt;This usually involves the use of newly 'invented' words that seem half-English half-German, the frequent misuse of personal pronouns and verbs or, for example, a change in style from formal to informal in the middle of the text (which is absolutely not common in German).&lt;/p&gt; &lt;p&gt;Here is a very recent example from GLM-4.6 (I have marked the incorrect passages in bold):&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Jetzt kommt das Problem: Menschen neigen dazu, eher kurze und einfache &lt;strong&gt;Passphrases&lt;/strong&gt; zu wählen (oder es &lt;strong&gt;passieren&lt;/strong&gt; unbewusst). Ein Angreifer, der deine verschlüsselte Schlüsseldatei hat, könnte also versuchen, die Passphrase zu erraten.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I don't know if it's a coincidence, but as you can see here, both words could also have a certain proximity to each other in the tokenizer (Pass-, pass-, -ass-,).&lt;/p&gt; &lt;p&gt;Unfortunately, I can't remember off the top of my head exactly how it was in earlier examples in this regards.&lt;/p&gt; &lt;p&gt;Anyway, as a rule of thumb, I would say that if a model gets a significant intelligence boost in its coding skills (compared to its predecessor), then it is more noticeable that it uses more English words in German texts, or that pseudo Anglicisms are introduced in kind of a unsuccessful way, or that the overall quality of German texts decreases significantly.&lt;/p&gt; &lt;p&gt;Have other people noticed this too? Or is this phenomenon perhaps also true for other languages?&lt;/p&gt; &lt;p&gt;And what do you think might be the reason for this?&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Edit: typos&lt;/p&gt; &lt;p&gt;Edit-02: I just want to add to quoted response from GLM-4.6: Here the correct style would be &lt;strong&gt;Passphrasen&lt;/strong&gt; and the correct grammar for the second word should be &lt;strong&gt;passiert&lt;/strong&gt;. But besides that, the whole sentence really sounds pretty strange and uncommon. I mean the whole &amp;quot;(oder es passieren/passiert unbewusst)&amp;quot; doesn’t make contextual sense at all tbh. It doesn’t sound like a smart 400B model but more like Gemma-2-2b or Phi-3.5-mini etc&lt;/p&gt; &lt;p&gt;And one more thing: Unfortunately, this annoying trend affected the Deepseek models as well, while interestingly, it &lt;strong&gt;never&lt;/strong&gt; occurred in the Gemini, Gemma and Mistral models. With each new release, these three model &lt;em&gt;families&lt;/em&gt; have become increasingly better and better in the German language.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evening_Ad6637"&gt; /u/Evening_Ad6637 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6xmok/glm46_worse_in_german_than_glm45_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6xmok/glm46_worse_in_german_than_glm45_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6xmok/glm46_worse_in_german_than_glm45_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T01:15:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6iwrd</id>
    <title>Performance of llama.cpp on NVIDIA DGX Spark · ggml-org/llama.cpp · Discussion #16578</title>
    <updated>2025-10-14T15:38:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"&gt; &lt;img alt="Performance of llama.cpp on NVIDIA DGX Spark · ggml-org/llama.cpp · Discussion #16578" src="https://external-preview.redd.it/jHQdSuZPiOdCRrmqghCS01mFwWiPh61nOi8HvEEUkiw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd9276c6ee8e677f95f5cd9bcd71ed7dde0ad2a" title="Performance of llama.cpp on NVIDIA DGX Spark · ggml-org/llama.cpp · Discussion #16578" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6iwrd/performance_of_llamacpp_on_nvidia_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5v78n</id>
    <title>The top open models on are now all by Chinese companies</title>
    <updated>2025-10-13T20:27:10+00:00</updated>
    <author>
      <name>/u/k_schaul</name>
      <uri>https://old.reddit.com/user/k_schaul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt; &lt;img alt="The top open models on are now all by Chinese companies" src="https://preview.redd.it/xhsv9ilkuxuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f3ce5e0a0548bdb8546f46e0f43b1b008af719" title="The top open models on are now all by Chinese companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full analysis here (🎁 gift link): &lt;a href="https://wapo.st/4nPUBud"&gt;wapo.st/4nPUBud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_schaul"&gt; /u/k_schaul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhsv9ilkuxuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6izz2</id>
    <title>DGX Spark vs AI Max 395+</title>
    <updated>2025-10-14T15:42:08+00:00</updated>
    <author>
      <name>/u/Responsible-Let9423</name>
      <uri>https://old.reddit.com/user/Responsible-Let9423</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone has fair comparison between two tiny AI PCs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible-Let9423"&gt; /u/Responsible-Let9423 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6izz2/dgx_spark_vs_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T15:42:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6oaa4</id>
    <title>Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails</title>
    <updated>2025-10-14T18:55:40+00:00</updated>
    <author>
      <name>/u/sketharapu</name>
      <uri>https://old.reddit.com/user/sketharapu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"&gt; &lt;img alt="Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails" src="https://preview.redd.it/7w1yhhrhj4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31ba63d90457b18277246650e0e8756589cac761" title="Those who reserved Nvidia's DGX Spark are starting to receive purchase invitation emails" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just received this email&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sketharapu"&gt; /u/sketharapu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7w1yhhrhj4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6oaa4/those_who_reserved_nvidias_dgx_spark_are_starting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:55:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6rqay</id>
    <title>I got fed up with Open WebUI/LibreChat for local LLMs so I made an open source tool to turn my GPU server into an always-on assistant</title>
    <updated>2025-10-14T21:03:41+00:00</updated>
    <author>
      <name>/u/ai-christianson</name>
      <uri>https://old.reddit.com/user/ai-christianson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been running local LLMs since the beginning and have always felt like LLM chat interfaces like Open WebUI/LibreChat/SillyTavern are great, but there must be so much more that we can do with local LLMs. I paid a lot for my GPU servers, so I actually want them to &lt;em&gt;do work&lt;/em&gt; for me.&lt;/p&gt; &lt;p&gt;Furthermore, local LLMs are generally higher latency than cloud services. It's a bit annoying to have to wait for a local LLM to fully generate a response, even though the response can be really good. I've always wanted the LLM to keep churning for me overnight, long after I've closed the chat tab. I don't care if it generates at 5 toks/sec if it is always doing work for me in the background.&lt;/p&gt; &lt;p&gt;Then there's the aspect that inference engines like vllm can get much higher batch throughput, but it hurts the latency a bit. It would be great to stack up many concurrent LLM requests. This would let me really extract the most &lt;em&gt;productivity&lt;/em&gt; out of my GPU servers over time.&lt;/p&gt; &lt;p&gt;So it put all the best ideas together, including all the lessons learned from the open source coding agent I previously built (RA.Aid), and built an open source platform for running agents that are always on.&lt;/p&gt; &lt;p&gt;The heart of the system is the incredible &lt;a href="https://github.com/browser-use/browser-use"&gt;browser-use&lt;/a&gt; project. So right of the bat we get web browsing agents, which is one of keys to being able to do productive work. The agents can access websites, web apps, and interact with them the way a human would.&lt;/p&gt; &lt;p&gt;But the big challenge with browser-use is that it requires writing custom code for each agent, and the agents don't run 24/7, and they lack high level planning and orchestration. I want to just tell my GPU server what I want it to do and &lt;em&gt;put it to work&lt;/em&gt; and have it get back to me when the job is done.&lt;/p&gt; &lt;p&gt;So that's exactly what I've built, and it's OSS (MIT licensed). You can check it out at &lt;a href="https://github.com/gobii-ai/gobii-platform"&gt;https://github.com/gobii-ai/gobii-platform&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To get it running, all you have to do is clone the repo and run: &lt;strong&gt;docker compose up --build&lt;/strong&gt;. It will take a minute to get set up, then a web UI will be available at localhost:8000. You can configure the key settings using the graphical config wizard, which is basically just the default account username/password and your local LLM inference endpoint.&lt;/p&gt; &lt;p&gt;Once it's running, you'll see a big text box at localhost:8000. Just type what you want it to do, like &amp;quot;find me the best priced 3090s on ebay from sellers that have good reviews&amp;quot; and it will do everything, including spawning a full chrome instance in an xvfb environment. It will set its own schedule, or you can ask it explicitly to check every 3 hours, for example.&lt;/p&gt; &lt;p&gt;The best part? If your hardware is not super fast for running local LLMs, you can configure it with an email account using SMTP/IMAP and it will &lt;strong&gt;automatically contact you when it has the results&lt;/strong&gt;, e.g. when it finds the 3090s you're looking for on ebay, it will email you links to them. You don't have to sit there waiting for your hardware to churn out the tokens.&lt;/p&gt; &lt;p&gt;And here's where it gets really cool: you can spin up as many of these agents as you want &lt;strong&gt;and you can link them together&lt;/strong&gt; so they can DM one another and work as a team. This means if you're running an inference server like vllm, it will actually turn that massive concurrent token throughput into &lt;em&gt;productive work&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;I hope you all like this as it took quite a bit of effort to put together. The whole idea here is to mine as much actual productive work as possible out of the expensive GPUs you already have. You can literally turn that GPU server into an &lt;em&gt;always-on team of assistants&lt;/em&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai-christianson"&gt; /u/ai-christianson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rqay/i_got_fed_up_with_open_webuilibrechat_for_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rqay/i_got_fed_up_with_open_webuilibrechat_for_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6rqay/i_got_fed_up_with_open_webuilibrechat_for_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T21:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o61gzs</id>
    <title>Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8</title>
    <updated>2025-10-14T00:47:06+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt; &lt;img alt="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" src="https://preview.redd.it/fjr53w0m4zuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1986eb8662405e67e0522e5d8d37f03ea577ffc" title="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;-NVFP4 is a way to store numbers for training large models using just 4 bits instead of 8 or 16. This makes training faster and use less memory&lt;/p&gt; &lt;p&gt;-NVFP4 shows 4-bit pretraining of a 12B Mamba Transformer on 10T tokens can match FP8 accuracy while cutting compute and memory.&lt;/p&gt; &lt;p&gt;-The validation loss stays within 1% of FP8 for most of training and grows to about 1.5% late during learning rate decay. &lt;/p&gt; &lt;p&gt;-Task scores stay close, for example MMLU Pro 62.58% vs 62.62%, while coding dips a bit like MBPP+ 55.91% vs 59.11%.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/godofprompt/status/1977678347879714912"&gt;X thread&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2509.25149"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjr53w0m4zuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6s89n</id>
    <title>Tested 9 RAG query transformation techniques – HydE is absurdly underrated</title>
    <updated>2025-10-14T21:22:30+00:00</updated>
    <author>
      <name>/u/Best-Information2493</name>
      <uri>https://old.reddit.com/user/Best-Information2493</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"&gt; &lt;img alt="Tested 9 RAG query transformation techniques – HydE is absurdly underrated" src="https://preview.redd.it/fq5i6e8q95vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f8db07dad84a6951edf7b8129992c0a4a7da454f" title="Tested 9 RAG query transformation techniques – HydE is absurdly underrated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your RAG system isn't bad. Your queries are.&lt;/p&gt; &lt;p&gt;I just tested 9 query transformation techniques. Here's what actually moved the needle:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Top 3:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;HydE&lt;/strong&gt; – Generate a hypothetical answer, search for docs similar to &lt;em&gt;that&lt;/em&gt;. Sounds dumb, works incredibly well. Solves the semantic gap problem.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG-Fusion&lt;/strong&gt; – Multi-query + reranking. Simple, effective, production-ready.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Step-Back&lt;/strong&gt; – Ask abstract questions first. &amp;quot;What is photosynthesis?&amp;quot; before &amp;quot;How do C4 plants fix carbon?&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Meh tier:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multi-Query: Good baseline, nothing special&lt;/li&gt; &lt;li&gt;Decomposition: Works but adds complexity&lt;/li&gt; &lt;li&gt;Recursive: Slow, minimal quality gain for simple queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key insight:&lt;/strong&gt; You're spending time optimizing embeddings when your query formulation is the actual bottleneck.&lt;/p&gt; &lt;p&gt;Notebook: &lt;a href="https://colab.research.google.com/drive/1HXhEudDjJsXCvP3tO4G7cAC15OyKW3nM?usp=sharing"&gt;https://colab.research.google.com/drive/1HXhEudDjJsXCvP3tO4G7cAC15OyKW3nM?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What techniques are you using? Anyone else seeing HydE results this good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Best-Information2493"&gt; /u/Best-Information2493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fq5i6e8q95vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6s89n/tested_9_rag_query_transformation_techniques_hyde/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T21:22:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o70fa7</id>
    <title>Sharing a few image transcriptions from Qwen3-VL-8B-Instruct</title>
    <updated>2025-10-15T03:28:58+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"&gt; &lt;img alt="Sharing a few image transcriptions from Qwen3-VL-8B-Instruct" src="https://b.thumbs.redditmedia.com/Lka0yz7i3ahGr9QVYGE_4FpjZ35ZZZnacxxqUOVbwHI.jpg" title="Sharing a few image transcriptions from Qwen3-VL-8B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o70fa7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o70fa7/sharing_a_few_image_transcriptions_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T03:28:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6h8jn</id>
    <title>We tested Claude Sonnet 4.5, GPT-5-codex, Qwen3-Coder, GLM and other 25+ models on fresh SWE-Bench like tasks from September 2025</title>
    <updated>2025-10-14T14:36:10+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I’m Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We’ve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with September runs on &lt;strong&gt;49 fresh GitHub PR bug-fix tasks&lt;/strong&gt; (last-month PR issues only). It’s a SWE-bench–style setup: models read real PR issues, run tests, edit code, and must make the suite pass.&lt;/p&gt; &lt;p&gt;Models: &lt;strong&gt;Sonnet-4.5, GPT-5-Codex, Grok Code Fast 1, GLM, Qwen, Kimi&lt;/strong&gt; and others&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Claude Sonnet 4.5 achieved the highest &lt;em&gt;pass@5&lt;/em&gt; (&lt;strong&gt;55.1%&lt;/strong&gt;) and uniquely solving several instances that &lt;strong&gt;no other model&lt;/strong&gt; on the leaderboard managed to resolve: &lt;a href="https://github.com/python-trio/trio/pull/3334"&gt;&lt;strong&gt;python-trio/trio-3334&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/cubed-dev/cubed/pull/799"&gt;&lt;strong&gt;cubed-dev/cubed-799&lt;/strong&gt;&lt;/a&gt;, &lt;a href="https://github.com/canopen-python/canopen/pull/613"&gt;&lt;strong&gt;canopen-python/canopen-613&lt;/strong&gt;&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder&lt;/strong&gt; is the &lt;strong&gt;best open-source performer&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;All models on the leaderboard were evaluated using the ChatCompletions API, except for &lt;a href="https://platform.openai.com/docs/models/gpt-5-codex"&gt;&lt;strong&gt;gpt-5-codex&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://platform.openai.com/docs/models/gpt-oss-120b"&gt;&lt;strong&gt;gpt-oss-120b&lt;/strong&gt;&lt;/a&gt;, which are only accessible via the Responses API.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please check the leaderboard, the insights, and write if you want to request some models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6h8jn/we_tested_claude_sonnet_45_gpt5codex_qwen3coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6hjgw</id>
    <title>[Open Source] We built a production-ready GenAI framework after deploying 50+ agents. Here's what we learned 🍕</title>
    <updated>2025-10-14T14:48:01+00:00</updated>
    <author>
      <name>/u/mario_candela</name>
      <uri>https://old.reddit.com/user/mario_candela</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ! 👋&lt;/p&gt; &lt;p&gt;After building and deploying 50+ GenAI solutions in production, we got tired of fighting with bloated frameworks, debugging black boxes, and dealing with vendor lock-in. So we built Datapizza AI - a Python framework that actually respects your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem We Solved&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM frameworks give you two bad options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Too much magic → You have no idea why your agent did what it did&lt;/li&gt; &lt;li&gt;Too little structure → You're rebuilding the same patterns over and over&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We wanted something that's predictable, debuggable, and production-ready from day one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes It Different&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🔍 Built-in Observability: OpenTelemetry tracing out of the box. See exactly what your agents are doing, track token usage, and debug performance issues without adding extra libraries.&lt;/p&gt; &lt;p&gt;🤝 Multi-Agent Collaboration: Agents can call other specialized agents. Build a trip planner that coordinates weather experts and web researchers - it just works.&lt;/p&gt; &lt;p&gt;📚 Production-Grade RAG: From document ingestion to reranking, we handle the entire pipeline. No more duct-taping 5 different libraries together.&lt;/p&gt; &lt;p&gt;🔌 Vendor Agnostic: Start with OpenAI, switch to Claude, add Gemini - same code. We support OpenAI, Anthropic, Google, Mistral, and Azure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We're Sharing This&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe in less abstraction, more control. If you've ever been frustrated by frameworks that hide too much or provide too little, this might be for you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🐙 GitHub: &lt;a href="https://github.com/datapizza-labs/datapizza-ai"&gt;https://github.com/datapizza-labs/datapizza-ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;📖 Docs: &lt;a href="https://docs.datapizza.ai"&gt;https://docs.datapizza.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;🏠 Website: &lt;a href="https://datapizza.tech/en/ai-framework/"&gt;https://datapizza.tech/en/ai-framework/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;We Need Your Help! 🙏&lt;/h1&gt; &lt;p&gt;We're actively developing this and would love to hear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What features would make this useful for YOUR use case?&lt;/li&gt; &lt;li&gt;What problems are you facing with current LLM frameworks?&lt;/li&gt; &lt;li&gt;Any bugs or issues you encounter (we respond fast!)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Star us on GitHub if you find this interesting,&lt;/strong&gt; it genuinely helps us understand if we're solving real problems.&lt;/p&gt; &lt;p&gt;Happy to answer any questions in the comments! 🍕&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mario_candela"&gt; /u/mario_candela &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6hjgw/open_source_we_built_a_productionready_genai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T14:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6vb48</id>
    <title>Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)</title>
    <updated>2025-10-14T23:29:26+00:00</updated>
    <author>
      <name>/u/ThetaCursed</name>
      <uri>https://old.reddit.com/user/ThetaCursed</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"&gt; &lt;img alt="Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)" src="https://b.thumbs.redditmedia.com/GVz5Ej45qwNz7Z-7_HggplD-Q37GuOg6QSgfhll7Bek.jpg" title="Quick Guide: Running Qwen3-Next-80B-A3B-Instruct-Q4_K_M Locally with FastLLM (Windows)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Nailed it first try with &lt;strong&gt;FastLLM&lt;/strong&gt;! No fuss.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup &amp;amp; Perf&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Required&lt;/strong&gt;: ~6 GB VRAM (for some reason it wasn't using my GPU to its maximum) + 48 GB RAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed&lt;/strong&gt;: ~8 t/s&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThetaCursed"&gt; /u/ThetaCursed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o6vb48"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6vb48/quick_guide_running_qwen3next80ba3binstructq4_k_m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T23:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6u5o4</id>
    <title>gpt-oss20/120b AMD Strix Halo vs NVIDIA DGX Spark benchmark</title>
    <updated>2025-10-14T22:40:20+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;NVIDIA DGX Spark (ollama)&lt;/th&gt; &lt;th align="left"&gt;Strix Halo (llama.cpp)&lt;/th&gt; &lt;th align="left"&gt;Winner&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Prompt Processing (Prefill)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2,053.98 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1,332.70 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;NVIDIA DGX Spark&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 20b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Token Generation (Decode)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;49.69 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;72.87 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Prompt Processing (Prefill)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;94.67 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;526.15 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gpt-oss 120b&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Token Generation (Decode)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;11.66 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.39 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Strix Halo&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6u5o4/gptoss20120b_amd_strix_halo_vs_nvidia_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T22:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6zg97</id>
    <title>[Update] Qwen3-VL cookbooks coming — recognition, localization, doc parsing, video</title>
    <updated>2025-10-15T02:41:33+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt; &lt;img alt="[Update] Qwen3-VL cookbooks coming — recognition, localization, doc parsing, video" src="https://external-preview.redd.it/sMJBVR2ChB4qgLOpxT2QxURyjiN_Zh_hva5OCRGa7Ls.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e9d23803b4ee9beb0893f3fff3d9a55771c058" title="[Update] Qwen3-VL cookbooks coming — recognition, localization, doc parsing, video" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ny9qsbphu6vf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89a0a9878dd4c655c5d082543041d9d2c36b22fb"&gt;https://preview.redd.it/ny9qsbphu6vf1.png?width=955&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89a0a9878dd4c655c5d082543041d9d2c36b22fb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;cookbooks&lt;/strong&gt; for a bunch of real-world capabilities—&lt;strong&gt;recognition&lt;/strong&gt;, &lt;strong&gt;localization&lt;/strong&gt;, &lt;strong&gt;document parsing&lt;/strong&gt;, &lt;strong&gt;video understanding&lt;/strong&gt;, &lt;strong&gt;key information extraction&lt;/strong&gt;, and more&lt;/p&gt; &lt;h1&gt;Cookbooks&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL#cookbooks"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are preparing &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;cookbooks&lt;/a&gt; for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Cookbook&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Open&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;Omni Recognition&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/omni_recognition.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;Powerful Document Parsing Capabilities&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/document_parsing.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;Precise Object Grounding Across Formats&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Using relative position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/2d_grounding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;General OCR and Key Information Extraction&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/ocr.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;Video Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Better video OCR, long video understanding, and video grounding.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/video_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;Mobile Agent&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Locate and think for mobile phone control.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mobile_agent.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;Computer-Use Agent&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Locate and think for controlling computers and Web.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/computer_use.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;3D Grounding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Provide accurate 3D bounding boxes for both indoor and outdoor objects.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/3d_grounding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;Thinking with Images&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Utilize image_zoom_in_tool and search_tool to facilitate the model’s precise comprehension of fine-grained visual details within images.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/think_with_images.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;MultiModal Coding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Generate accurate code based on rigorous comprehension of multimodal information.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/mmcode.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;Long Document Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Achieve rigorous semantic comprehension of ultra-long documents.&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/long_document_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;Spatial Understanding&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;See, understand and reason about the spatial information&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/QwenLM/Qwen3-VL/blob/main/cookbooks/spatial_understanding.ipynb"&gt;&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6zg97/update_qwen3vl_cookbooks_coming_recognition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T02:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ofr9</id>
    <title>Intel Crescent Island GPU: 160GB of LPDDR5X memory</title>
    <updated>2025-10-14T19:01:14+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;About the GPU:&lt;/strong&gt; The new data center GPU code-named Crescent Island is being designed to be power and cost-optimized for air-cooled enterprise servers and to incorporate large amounts of memory capacity and bandwidth, optimized for inference workflows. &lt;/p&gt; &lt;p&gt;Key features include: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Xe3P microarchitecture with optimized performance-per-watt &lt;/li&gt; &lt;li&gt;160GB of LPDDR5X memory &lt;/li&gt; &lt;li&gt;Support for a broad range of data types, ideal for “tokens-as-a-service” providers and inference use cases &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory"&gt;https://videocardz.com/newz/intel-confirms-xe3p-architecture-to-power-new-crescent-island-data-center-gpu-with-160gb-lpddr5x-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu"&gt;https://newsroom.intel.com/artificial-intelligence/intel-to-expand-ai-accelerator-portfolio-with-new-gpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ofr9/intel_crescent_island_gpu_160gb_of_lpddr5x_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6wfpy</id>
    <title>GPT-OSS-20b TAKE THE WHEEL!</title>
    <updated>2025-10-15T00:20:31+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"&gt; &lt;img alt="GPT-OSS-20b TAKE THE WHEEL!" src="https://external-preview.redd.it/EYfYrBdrbHJnRl1EvzhfVjhkBpr2GL8UU-8scxe6WCU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d3d6c023c0a8855a48f130b4207e4980d5b4f49" title="GPT-OSS-20b TAKE THE WHEEL!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this experiment, I use a single 4090 hooked up to VLLM and a batching GPT-OSS-20b model set up with prefill prompts that explain the current game state (direction/velocity/location of asteroids and the direction/velocity/location of our ship in relation to them), and the LLM is forced to make a control decision to either turn left 25%, turn right 25%, thrust forward, reverse (turn 180 degrees and thrust), or fire. Since I'm only generating one token per generation, I am able to get latency down under 20ms, allowing the AI to make rapid fire decisions (multiple-per-second) and to apply them as control inputs to the spaceship.&lt;/p&gt; &lt;p&gt;As it runs, it's generating a high speed continuous stream of 20ms responses to input thanks to the continuous batching VLLM server (a largely prefix cached prompt with a bit of information updating the current game-state so it can make an input decision in near-realtime). It's able to successfully autopilot the ship around. I also gave it some instructions and a reward (higher points) for flying closer to asteroids and 'hot dogging' which made its chosen flightpath a bit more interesting.&lt;/p&gt; &lt;p&gt;I know it's just a silly experiment, and yes, it would be absolutely trivial to make a simple algorithm that could fly this ship around safely without needing hundreds of watts of screaming GPU, but I thought someone might appreciate making OSS 20b into a little autopilot that knows what's going on around it and controls the ship like it's using a game controller at latency that makes it a fairly competent pilot.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=NY6htCUWFqI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6wfpy/gptoss20b_take_the_wheel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-15T00:20:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6uw71</id>
    <title>Qwen3-VL 4B vs 8B vs 235B</title>
    <updated>2025-10-14T23:11:20+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"&gt; &lt;img alt="Qwen3-VL 4B vs 8B vs 235B" src="https://preview.redd.it/deo3nizps5vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58885a74f99e694dcdba21d3b954746fac3611ce" title="Qwen3-VL 4B vs 8B vs 235B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/deo3nizps5vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6uw71/qwen3vl_4b_vs_8b_vs_235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T23:11:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6pmxt</id>
    <title>Real-time study buddy that sees your screen and talks back</title>
    <updated>2025-10-14T19:46:04+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt; &lt;img alt="Real-time study buddy that sees your screen and talks back" src="https://external-preview.redd.it/ZXlwZW5pYTNuNHZmMUsYDHUptOq0sYO1cNNkCl_tbC9KzkSKWyT6VTZxxWFL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ff99a7daf7c346698fa23df7509fc456a4b3edc3" title="Real-time study buddy that sees your screen and talks back" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a real-time learning assistant that sees your screen, talks, and learns alongside you. All open models (Qwen3-VL, Parakeet, Orpheus) wired together. &lt;/p&gt; &lt;p&gt;I shared a biology site on cell structure to see if it could describe the page, identify the diagram, and answer targeted questions about the mitochondria. &lt;/p&gt; &lt;p&gt;These text and vision models are getting so good. Wiring them together levels them all up. Next step: going to try running it across multiple sites and have it auto-summarize my learnings into a study guide or PDF after.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ctp0k9a3n4vf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6pmxt/realtime_study_buddy_that_sees_your_screen_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T19:46:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6kchz</id>
    <title>Qwen3-VL-4B and 8B Instruct &amp; Thinking are here</title>
    <updated>2025-10-14T16:31:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Thinking&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-4B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can already run Qwen3-VL-4B &amp;amp; 8B locally Day-0 on NPU/GPU/CPU using MLX, GGUF, and NexaML with NexaSDK &lt;strong&gt;(&lt;/strong&gt;&lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;&lt;strong&gt;GitHub&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Check out our GGUF, MLX, and NexaML collection on HuggingFace: &lt;a href="https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a"&gt;https://huggingface.co/collections/NexaAI/qwen3vl-68d46de18fdc753a7295190a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6kchz/qwen3vl4b_and_8b_instruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T16:31:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6ocfs</id>
    <title>If it's not local, it's not yours.</title>
    <updated>2025-10-14T18:57:54+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt; &lt;img alt="If it's not local, it's not yours." src="https://preview.redd.it/zzv4ey22j4vf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebc1f207746b0fa04e90a129bafad3aef0ca9971" title="If it's not local, it's not yours." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zzv4ey22j4vf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6ocfs/if_its_not_local_its_not_yours/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T18:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
