<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-19T06:09:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mu3tln</id>
    <title>Why does Qwen3-Coder not work in Qwen-Code aka what's going on with tool calling?</title>
    <updated>2025-08-19T00:40:48+00:00</updated>
    <author>
      <name>/u/Pristine-Woodpecker</name>
      <uri>https://old.reddit.com/user/Pristine-Woodpecker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;These issues are driving me nuts.&lt;/p&gt; &lt;p&gt;So, my config is with using llama.cpp. Let's assume that is a requirement because of the need to do partial offloading. Of course, we use the very latest from git. Same for qwen-code.&lt;/p&gt; &lt;p&gt;We get a nice GGUF from &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF"&gt;https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF&lt;/a&gt; in some reasonable quant. It was last updated 2 weeks ago. We use &lt;code&gt;--jinja&lt;/code&gt; for the server to get the right template.&lt;/p&gt; &lt;p&gt;Now, we try some queries in qwen-code. And the screen gets full of:&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;tool_call&amp;gt;&amp;lt;function=search_file_content&lt;/code&gt;&lt;/p&gt; &lt;p&gt;And similar junk. It's clearly not expecting the response format it is getting. So what's going on here? It seems the model isn't even really implemented in llama.cpp yet: &lt;a href="https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10#689ccab85457dccd3df19ad2"&gt;https://huggingface.co/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/discussions/10#689ccab85457dccd3df19ad2&lt;/a&gt; Note there's some remarks there explaining Roo/Cline/Kilo completely ignore the built-in tool support and that's both why they work but also break when context gets longer (and the model has problems remembering the custom instructions).&lt;/p&gt; &lt;p&gt;Through OpenRouter stats I noticed &amp;quot;Crush&amp;quot;. Interestingly, it seems to parse the Qwen3-Coder responses from llama.cpp correctly. What's up here? Did they hackfix this in their interface?&lt;/p&gt; &lt;p&gt;Now, if I really want to go on a further rant, let's talk about GLM 4.5 (Air), which doesn't seem to be able to tool call in &lt;strong&gt;any&lt;/strong&gt; CLI. At least qwen-code causes a server-side error, and codex nor Crush are able to deal with tool calling, the latter not understanding e.g.&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;lt;tool_call&amp;gt;agent&amp;lt;arg_key&amp;gt;prompt&amp;lt;/arg_key&amp;gt;&amp;lt;arg_value&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Now, why, despite having several VERY GOOD models that are runnable locally, like Qwen3-30B-A3B and GLM 4.5 Air, and having several open source agentic CLI (qwen-cli, codex, Crush, etc), does nothing actually work together? Is it because nobody is actually running these configs? The model drops are to score points but you're really supposed to use the API? It's a bit telling the most popular tools on OpenRouter (Roo/Cline/Kilo) have tried to work around the tool calling issue, but not entirely with success. &lt;/p&gt; &lt;p&gt;For running the models locally, I would praise the OpenAI guys here, who had launch day support in llama.cpp - including prompt caching - and it even mostly works in codex and Crush...but there's &lt;code&gt;&amp;lt;|channel|&amp;gt;analysis&amp;lt;|message|&amp;gt;&lt;/code&gt; spam all over, so for now that's an &amp;quot;almost&amp;quot;.&lt;/p&gt; &lt;p&gt;tl;dr Locallama.cpp dreams crushed because qwen-code doesn't even support Qwen-Coder properly when running local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pristine-Woodpecker"&gt; /u/Pristine-Woodpecker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu3tln/why_does_qwen3coder_not_work_in_qwencode_aka/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu3tln/why_does_qwen3coder_not_work_in_qwencode_aka/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu3tln/why_does_qwen3coder_not_work_in_qwencode_aka/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T00:40:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu6a9s</id>
    <title>Agentic coding tools with smaller system prompts?</title>
    <updated>2025-08-19T02:31:47+00:00</updated>
    <author>
      <name>/u/Carbonite1</name>
      <uri>https://old.reddit.com/user/Carbonite1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! Wondering if anyone here has thought about this&lt;/p&gt; &lt;p&gt;I've been playing a bit recently with the new Qwen3 Coder 30B locally, and for being so small it's really impressive. Have even tried it with some of the Claude-Code-like agentic coding tools, like qwen's own, Claude Code Router, and opencode/crush, all with some success (again, amazing for all this running locally).&lt;/p&gt; &lt;p&gt;The problem is, all the tools I listed above start out pretty snappy, but get slow after just a few questions. I'm pretty sure this is because of the prompt that each of these tools sends along with each user message -- it's gigantic, includes e.g. the full definition of every tool available to the LLM with several examples each, etc. This fills up the context quickly and I think is why it gets slow. &lt;/p&gt; &lt;p&gt;So, my question -- does that sound right? And if so, has anyone done any exploration into a &amp;quot;lite&amp;quot; mode for these tools or something, such that that can be functional without enormous context?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Carbonite1"&gt; /u/Carbonite1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu6a9s/agentic_coding_tools_with_smaller_system_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu6a9s/agentic_coding_tools_with_smaller_system_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu6a9s/agentic_coding_tools_with_smaller_system_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T02:31:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu9q27</id>
    <title>Independent researcher built efficient attention mechanism - seeking feedback and arXiv help</title>
    <updated>2025-08-19T05:29:06+00:00</updated>
    <author>
      <name>/u/Perfect_Power815</name>
      <uri>https://old.reddit.com/user/Perfect_Power815</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA community! I know this isn't exactly about running models locally, but I figured you folks would appreciate efficiency improvements in AI models.&lt;/p&gt; &lt;p&gt;I have been working on making Transformers more efficient. I developed something called &amp;quot;Condor&amp;quot; - a new attention mechanism that's way more computationally efficient than standard attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;New attention approach using &amp;quot;learnable connection functions&amp;quot;&lt;/li&gt; &lt;li&gt;Reduces complexity from O(L²) to O(LWH) - much more memory/compute friendly&lt;/li&gt; &lt;li&gt;98% better perplexity on WikiText-2 (13.77 vs 717.88)&lt;/li&gt; &lt;li&gt;Each attention head specializes in different patterns&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Paper link:&lt;/strong&gt; &lt;a href="https://www.academia.edu/143509129/Condor_A_Neural_Connection_Network_for_Enhanced?source=swp_share"&gt;https://www.academia.edu/143509129/Condor_A_Neural_Connection_Network_for_Enhanced?source=swp_share&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code is available at: &lt;a href="https://github.com/Kim-Ai-gpu/Condor"&gt;https://github.com/Kim-Ai-gpu/Condor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why you might care:&lt;/strong&gt; More efficient attention = better performance on consumer hardware, faster inference, less VRAM usage.&lt;/p&gt; &lt;p&gt;I'm looking for feedback and also hoping someone might help me get this on arXiv.&lt;/p&gt; &lt;p&gt;If anyone has arXiv privileges and thinks this could be useful, my details are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Name: Youngseong Kim&lt;/li&gt; &lt;li&gt;Email: [&lt;a href="mailto:lam983039@gmail.com"&gt;lam983039@gmail.com&lt;/a&gt;](mailto:&lt;a href="mailto:lam983039@gmail.com"&gt;lam983039@gmail.com&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Affiliation: Independent Researcher&lt;/li&gt; &lt;li&gt;Intended category: &lt;a href="http://cs.AI"&gt;cs.AI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out! Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Power815"&gt; /u/Perfect_Power815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9q27/independent_researcher_built_efficient_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9q27/independent_researcher_built_efficient_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu9q27/independent_researcher_built_efficient_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu8h32</id>
    <title>New nvidia models 9B-v2-Base vs 9B-v2???</title>
    <updated>2025-08-19T04:19:30+00:00</updated>
    <author>
      <name>/u/Mr-Barack-Obama</name>
      <uri>https://old.reddit.com/user/Mr-Barack-Obama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the difference between &amp;quot;9B-v2-Base&amp;quot; and &amp;quot;9B-v2&amp;quot;??&lt;/p&gt; &lt;p&gt;Here are their links respectively:&lt;br /&gt; &lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base"&gt;https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-9B-v2-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Which one should I use?&lt;/p&gt; &lt;p&gt;Sorry for being a noob.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr-Barack-Obama"&gt; /u/Mr-Barack-Obama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu8h32/new_nvidia_models_9bv2base_vs_9bv2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu8h32/new_nvidia_models_9bv2base_vs_9bv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu8h32/new_nvidia_models_9bv2base_vs_9bv2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T04:19:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtsbsk</id>
    <title>I wish we could actually use Gemma 3n</title>
    <updated>2025-08-18T17:19:27+00:00</updated>
    <author>
      <name>/u/ai_fonsi</name>
      <uri>https://old.reddit.com/user/ai_fonsi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It would be the perfect model to run on my Nvidia Jetson Orin Nano, with native audio and video support. It's a great model that punches way above its weight and I like the general vibe a lot. And most importantly, it has some really cool optimizations for low-memory edge devices such as per-layer embeddings that can be loaded from flash memory (in theory).&lt;/p&gt; &lt;p&gt;However, all inference engines I tried either don't support the multimodal features (llama.cpp) or use much more memory than they're supposed to (HF transformers, mistral.rs). ONNX loads the weights in memory twice on the GPU executor for some reason and even Google's own &lt;a href="https://github.com/google-ai-edge/LiteRT-LM"&gt;LiteRT-LM&lt;/a&gt; doesn't support CUDA or &lt;a href="https://github.com/google-ai-edge/LiteRT-LM/issues/351"&gt;multimodal features in any environment&lt;/a&gt; yet (???). The engines' maintainers mostly don't seem to be actively working on improving the situation, at least as far as I can tell.&lt;/p&gt; &lt;p&gt;Don't get me wrong, I'm not trying to blame the OSS devs trying their best to support the current onslaught of models in their engines. However, I wish Google wouldn't &lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/#get-started-with-gemma-3n-today"&gt;boast about how their model is supported by so many tools&lt;/a&gt; when they clearly didn't provide enough help to support half the features or optimizations necessary to unlock the full potential. Contrast this with GPT-OSS, which mostly worked fine out of the box despite architectural differences. It's just a shame given how many cool things this model would have going for it (audio, PLE, MatFormer)...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_fonsi"&gt; /u/ai_fonsi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtsbsk/i_wish_we_could_actually_use_gemma_3n/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtsbsk/i_wish_we_could_actually_use_gemma_3n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtsbsk/i_wish_we_could_actually_use_gemma_3n/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mthavv</id>
    <title>Best NSFW/uncensored LLM to generate prompts for image generation?</title>
    <updated>2025-08-18T09:33:17+00:00</updated>
    <author>
      <name>/u/irmesutb6</name>
      <uri>https://old.reddit.com/user/irmesutb6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m new to this, i basically use SD/PONY/Illustrious to generate images and they’re at a pretty good stage.&lt;/p&gt; &lt;p&gt;I’m trying to find a LLM that can generate NSFW prompts for image generation, i have 8GB VRAM on RTX 4060 locally could please anyone help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irmesutb6"&gt; /u/irmesutb6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthavv/best_nsfwuncensored_llm_to_generate_prompts_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mthavv/best_nsfwuncensored_llm_to_generate_prompts_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mthavv/best_nsfwuncensored_llm_to_generate_prompts_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T09:33:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mto88l</id>
    <title>We open-sourced Memori: A memory engine for AI agents</title>
    <updated>2025-08-18T14:51:39+00:00</updated>
    <author>
      <name>/u/Arindam_200</name>
      <uri>https://old.reddit.com/user/Arindam_200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks!&lt;/p&gt; &lt;p&gt;I'm a part the team behind &lt;a href="https://memori.gibsonai.com/"&gt;Memori&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Memori adds a stateful memory engine to AI agents, enabling them to stay consistent, recall past work, and improve over time. With Memori, agents don’t lose track of multi-step workflows, repeat tool calls, or forget user preferences. Instead, they build up human-like memory that makes them more reliable and efficient across sessions.&lt;/p&gt; &lt;p&gt;We’ve also put together demo apps (a personal diary assistant, a research agent, and a travel planner) so you can see memory in action.&lt;/p&gt; &lt;p&gt;Current LLMs are stateless, they forget everything between sessions. This leads to repetitive interactions, wasted tokens, and inconsistent results. When building AI agents, this problem gets even worse: without memory, they can’t recover from failures, coordinate across steps, or apply simple rules like “always write tests.”&lt;/p&gt; &lt;p&gt;We realized that for AI agents to work in production, they need memory. That’s why we built Memori.&lt;/p&gt; &lt;h1&gt;How Memori Works&lt;/h1&gt; &lt;p&gt;Memori uses a multi-agent architecture to capture conversations, analyze them, and decide which memories to keep active. It supports three modes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Conscious Mode:&lt;/strong&gt; short-term memory for recent, essential context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auto Mode:&lt;/strong&gt; dynamic search across long-term memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Combined Mode:&lt;/strong&gt; blends both for fast recall and deep retrieval.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Under the hood, Memori is &lt;strong&gt;SQL-first&lt;/strong&gt;. You can use SQLite, PostgreSQL, or MySQL to store memory with built-in full-text search, versioning, and optimization. This makes it simple to deploy, production-ready, and extensible.&lt;/p&gt; &lt;h1&gt;Database-Backed for Reliability&lt;/h1&gt; &lt;p&gt;Memori is backed by GibsonAI’s database infrastructure, which supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant provisioning&lt;/li&gt; &lt;li&gt;Autoscaling on demand&lt;/li&gt; &lt;li&gt;Database branching &amp;amp; versioning&lt;/li&gt; &lt;li&gt;Query optimization&lt;/li&gt; &lt;li&gt;Point of recovery&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This means memory isn’t just stored, it’s reliable, efficient, and scales with real-world workloads.&lt;/p&gt; &lt;h1&gt;Getting Started&lt;/h1&gt; &lt;p&gt;Install the SDK( `pip install memorisdk` ) and enable memory in one line:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from memori import Memori memori = Memori(conscious_ingest=True) memori.enable() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;From then on, every conversation is remembered and intelligently recalled when needed.&lt;/p&gt; &lt;p&gt;We’ve open-sourced Memori under the Apache 2.0 license so anyone can build with it. You can check out the GitHub repo here: &lt;a href="https://github.com/GibsonAI/memori"&gt;https://github.com/GibsonAI/memori&lt;/a&gt;, and explore the docs.&lt;/p&gt; &lt;p&gt;We’d love to hear your thoughts. Please dive into the code, try out the demos, and share feedback, your input will help shape where we take Memori from here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arindam_200"&gt; /u/Arindam_200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto88l/we_opensourced_memori_a_memory_engine_for_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto88l/we_opensourced_memori_a_memory_engine_for_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mto88l/we_opensourced_memori_a_memory_engine_for_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtvh0j</id>
    <title>NVIDIA Nemotron Nano 2 and the Nemotron Pretraining Dataset v1</title>
    <updated>2025-08-18T19:12:27+00:00</updated>
    <author>
      <name>/u/pi314156</name>
      <uri>https://old.reddit.com/user/pi314156</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pi314156"&gt; /u/pi314156 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvh0j/nvidia_nemotron_nano_2_and_the_nemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvh0j/nvidia_nemotron_nano_2_and_the_nemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T19:12:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtwzx2</id>
    <title>GLM 4 sits at 4th on Design Arena over the last 7 days</title>
    <updated>2025-08-18T20:08:49+00:00</updated>
    <author>
      <name>/u/Accomplished-Copy332</name>
      <uri>https://old.reddit.com/user/Accomplished-Copy332</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtwzx2/glm_4_sits_at_4th_on_design_arena_over_the_last_7/"&gt; &lt;img alt="GLM 4 sits at 4th on Design Arena over the last 7 days" src="https://b.thumbs.redditmedia.com/l82RKAaoMqP7y8LeiFEJdnfprpc5i1-M8wShxJR03JE.jpg" title="GLM 4 sits at 4th on Design Arena over the last 7 days" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have been looking more deeply into the analytics on the preference data for the &lt;a href="https://www.designarena.ai/"&gt;benchmark&lt;/a&gt; over the last week, and I thought this might be an interesting tidbit to share. &lt;/p&gt; &lt;p&gt;GLM 4.5, filtering for comparisons that have been submitted over the last 7 days, is 4th among models based on win rate. When filtering for the last 14 days, GLM 4.5 is at 6th, while overall it is 10th. &lt;/p&gt; &lt;p&gt;Since developing the benchmark, it's been really interesting to see the open source models really compete with the proprietary models on frontend and design. It'll be really interesting where DeepSeek R2 will land when it comes out. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished-Copy332"&gt; /u/Accomplished-Copy332 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mtwzx2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtwzx2/glm_4_sits_at_4th_on_design_arena_over_the_last_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtwzx2/glm_4_sits_at_4th_on_design_arena_over_the_last_7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T20:08:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtzn4b</id>
    <title>Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)</title>
    <updated>2025-08-18T21:47:13+00:00</updated>
    <author>
      <name>/u/Solid_Woodpecker3635</name>
      <uri>https://old.reddit.com/user/Solid_Woodpecker3635</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtzn4b/tiny_finance_thinking_model_gemma3_270m_with/"&gt; &lt;img alt="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" src="https://preview.redd.it/db62qfi7mujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cacfd1e6d3138f9479b2df53130a1d980a490810" title="Tiny finance “thinking” model (Gemma-3 270M) with verifiable rewards (SFT → GRPO) — structured outputs + auto-eval (with code)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I taught a tiny model to &lt;em&gt;think like a finance analyst&lt;/em&gt; by enforcing a strict output contract and only rewarding it when the output is &lt;strong&gt;verifiably&lt;/strong&gt; correct.&lt;/p&gt; &lt;h1&gt;What I built&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Task &amp;amp; contract&lt;/strong&gt; (always returns): &lt;ul&gt; &lt;li&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt;&lt;/code&gt; concise, balanced rationale&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;SENTIMENT&amp;gt;&lt;/code&gt; positive | negative | neutral&lt;/li&gt; &lt;li&gt;&lt;code&gt;&amp;lt;CONFIDENCE&amp;gt;&lt;/code&gt; 0.1–1.0 (calibrated)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; SFT → GRPO (Group Relative Policy Optimization)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rewards (RLVR):&lt;/strong&gt; format gate, reasoning heuristics, FinBERT alignment, confidence calibration (Brier-style), directional consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stack:&lt;/strong&gt; Gemma-3 270M (IT), Unsloth 4-bit, TRL, HF Transformers (Windows-friendly)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Quick peek&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;REASONING&amp;gt; Revenue and EPS beat; raised FY guide on AI demand. However, near-term spend may compress margins. Net effect: constructive. &amp;lt;/REASONING&amp;gt; &amp;lt;SENTIMENT&amp;gt; positive &amp;lt;/SENTIMENT&amp;gt; &amp;lt;CONFIDENCE&amp;gt; 0.78 &amp;lt;/CONFIDENCE&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Small + fast:&lt;/strong&gt; runs on modest hardware with low latency/cost&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Auditable:&lt;/strong&gt; structured outputs are easy to log, QA, and govern&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early results vs base:&lt;/strong&gt; cleaner structure, better agreement on mixed headlines, steadier confidence&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings/tree/main/projects/financial-reasoning-enhanced"&gt;Reinforcement-learning-with-verifable-rewards-Learnings/projects/financial-reasoning-enhanced at main · Pavankunchala/Reinforcement-learning-with-verifable-rewards-Learnings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am planning to make more improvements essentially trying to add a more robust reward eval and also better synthetic data , I am exploring ideas on how i can make small models really intelligent in some domains ,&lt;/p&gt; &lt;p&gt;It is still rough around the edges will be actively improving it&lt;/p&gt; &lt;p&gt;&lt;em&gt;P.S. I'm currently looking for my next role in the LLM / Computer Vision space and would love to connect about any opportunities&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Portfolio:&lt;/em&gt; &lt;a href="https://pavan-portfolio-tawny.vercel.app/"&gt;Pavan Kunchala - AI Engineer &amp;amp; Full-Stack Developer&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Solid_Woodpecker3635"&gt; /u/Solid_Woodpecker3635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/db62qfi7mujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtzn4b/tiny_finance_thinking_model_gemma3_270m_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtzn4b/tiny_finance_thinking_model_gemma3_270m_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T21:47:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtumrq</id>
    <title>Qwen3 and Qwen2.5 VL built from scratch.</title>
    <updated>2025-08-18T18:42:14+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtumrq/qwen3_and_qwen25_vl_built_from_scratch/"&gt; &lt;img alt="Qwen3 and Qwen2.5 VL built from scratch." src="https://preview.redd.it/q9jdx2funtjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9eeac88cd0fd46896e81131a75f76a7a41c6f67" title="Qwen3 and Qwen2.5 VL built from scratch." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the weekend, I updated my Tiny-Qwen repo to support Qwen3, with MoE support, and wrapped it in a fancy looking CLI for ease of use. &lt;/p&gt; &lt;p&gt;Go check it out: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q9jdx2funtjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtumrq/qwen3_and_qwen25_vl_built_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtumrq/qwen3_and_qwen25_vl_built_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T18:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mua1k4</id>
    <title>GPT OSS quality on Nebius - fixed (update)</title>
    <updated>2025-08-19T05:47:34+00:00</updated>
    <author>
      <name>/u/ai_devrel_eng</name>
      <uri>https://old.reddit.com/user/ai_devrel_eng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt; &lt;img alt="GPT OSS quality on Nebius - fixed (update)" src="https://b.thumbs.redditmedia.com/sK7Bm1tG5gwQQ5s7xz2h10LEJrqGwQrDc2Udh4wNqoE.jpg" title="GPT OSS quality on Nebius - fixed (update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai_devrel_eng"&gt; /u/ai_devrel_eng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mua1k4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mua1k4/gpt_oss_quality_on_nebius_fixed_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T05:47:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtqz3u</id>
    <title>Test: can Qwen 2.5 Omni actually hear guitar chords?</title>
    <updated>2025-08-18T16:30:44+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"&gt; &lt;img alt="Test: can Qwen 2.5 Omni actually hear guitar chords?" src="https://external-preview.redd.it/ZHl5dWUwbXl6c2pmMYMQwk04cVObnLQ8K0JceAGBOHlOc1BqIEw787XqJHGO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f00d3d8d78e6d2d5c31925df7d0e8a596f74c0a" title="Test: can Qwen 2.5 Omni actually hear guitar chords?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tried Qwen 2.5 Omni locally with vision + speech to see if it could hear music instead of just speech.&lt;/p&gt; &lt;p&gt;I played guitar, and it did a surprisingly solid job telling me which chords I was playing in real-time. At the end, I debugged what the LLM was “hearing,” and input quality likely explained some of the misses it did have.&lt;/p&gt; &lt;p&gt;Next test: run it with the guitar out of sight, so we can confirm it’s not cheating by “seeing” the guitar instead of listening. Will share results when I try that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/848676nyzsjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtqz3u/test_can_qwen_25_omni_actually_hear_guitar_chords/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtct4y</id>
    <title>Elon didn't deliver on this announcement. It's already Monday.</title>
    <updated>2025-08-18T04:59:00+00:00</updated>
    <author>
      <name>/u/Outside-Iron-8242</name>
      <uri>https://old.reddit.com/user/Outside-Iron-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt; &lt;img alt="Elon didn't deliver on this announcement. It's already Monday." src="https://preview.redd.it/rt8xgjaampjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2b6da79d84d1e52439441cafb251d7e1dc508f7" title="Elon didn't deliver on this announcement. It's already Monday." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outside-Iron-8242"&gt; /u/Outside-Iron-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rt8xgjaampjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtct4y/elon_didnt_deliver_on_this_announcement_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T04:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttad3</id>
    <title>Qwen-Image-Edit</title>
    <updated>2025-08-18T17:54:01+00:00</updated>
    <author>
      <name>/u/lomero</name>
      <uri>https://old.reddit.com/user/lomero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"&gt; &lt;img alt="Qwen-Image-Edit" src="https://external-preview.redd.it/_phhztHOXP1EaSigwjpmdclnYlgiIY_nMfXGtHApDV0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b9e4ac0aed522efe5c46173ca8aa2cff20f9af4" title="Qwen-Image-Edit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lomero"&gt; /u/lomero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttad3/qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:54:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttchz</id>
    <title>Deepseek R2 coming out ... when it gets more cowbell</title>
    <updated>2025-08-18T17:56:08+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt; &lt;img alt="Deepseek R2 coming out ... when it gets more cowbell" src="https://external-preview.redd.it/JNAOp8mejhheiBappEWRGE1kVdbTWLMVP5NLPsOJx9c.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f39a5e1f22d317e74a5e301e71bbdc92c571323" title="Deepseek R2 coming out ... when it gets more cowbell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what’s floating around it seems like we'll have to keep waiting a bit longer for Deepseek R2 to be released.&lt;/p&gt; &lt;p&gt;Apparently&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Liang Wenfeng has been sitting on R2's release because it still needs more cowbell&lt;/li&gt; &lt;li&gt;Training DeepSeek R2 on Huawei Ascend chips ran into persistent stability and software problems and no full training run ever succeeded. So Deepseek went back to Nvidia GPUs for training and is using Ascend chips for inference only&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is the same story .. but with more cowbell &lt;a href="https://youtu.be/PzlqRsuIo1w"&gt;https://youtu.be/PzlqRsuIo1w&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/psltmf3youjf1.gif"&gt;https://i.redd.it/psltmf3youjf1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttchz/deepseek_r2_coming_out_when_it_gets_more_cowbell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtrn1y</id>
    <title>Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!</title>
    <updated>2025-08-18T16:55:07+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"&gt; &lt;img alt="Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!" src="https://external-preview.redd.it/S-o7drNLzmVPRPUM6Ap2mbj65SEf6wzf8867vZcT5JE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9303bece3c938ba362682f24d93acffc4066003c" title="Drummer's Cydonia 24B v4.1 - Nothing like its predecessors. A stronger, less positive, less Mistral, performant tune!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtrn1y/drummers_cydonia_24b_v41_nothing_like_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T16:55:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtk03a</id>
    <title>Kimi K2 is really, really good.</title>
    <updated>2025-08-18T12:00:19+00:00</updated>
    <author>
      <name>/u/ThomasAger</name>
      <uri>https://old.reddit.com/user/ThomasAger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve spent a long time waiting for an open source model I can use in production for both multi-agent multi-turn workflows, as well as a capable instruction following chat model. &lt;/p&gt; &lt;p&gt;This was the first model that has ever delivered. &lt;/p&gt; &lt;p&gt;For a long time I was stuck using foundation models, writing prompts that did the job I knew fine-tuning an open source model could do so much more effectively.&lt;/p&gt; &lt;p&gt;This isn’t paid or sponsored. It’s available to talk to for free and on the LM arena leaderboard (a month or so ago it was #8 there). I know many of ya’ll are already aware of this but I strongly recommend looking into integrating them into your pipeline.&lt;/p&gt; &lt;p&gt;They are already effective at long term agent workflows like building research reports with citations or websites. You can even try it for free. Has anyone else tried Kimi out? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasAger"&gt; /u/ThomasAger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtk03a/kimi_k2_is_really_really_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T12:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu49q3</id>
    <title>We're Updating the Wiki To Be More Current, And We Want Your Feedback</title>
    <updated>2025-08-19T01:01:14+00:00</updated>
    <author>
      <name>/u/N8Karma</name>
      <uri>https://old.reddit.com/user/N8Karma</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; subreddit has long had a wiki: &lt;a href="https://www.reddit.com/r/LocalLLaMA/wiki/wiki/"&gt;https://www.reddit.com/r/LocalLLaMA/wiki/wiki/&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;However, the wiki hadn't been updated in a year or two (it still was mainly focused on LLaMA 2)! So we renovated the FAQ, Resources, and Models sections to reflect the present ecosystem. You can see a direct comparison &lt;a href="https://github.com/N8python/local-llama-wiki-revamp"&gt;here&lt;/a&gt; - with llama-old.md being the prior wiki and llama.md being the new one. Everything has been brought up to date.&lt;/p&gt; &lt;p&gt;But this is just the beginning - we want the wiki to reflect the desires of the community, and we want your feedback on what should be added/removed/altered. Models that are missing? Good resources that need a spotlight? We want to hear it! There will be a comment under this post to place your suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/N8Karma"&gt; /u/N8Karma &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu49q3/were_updating_the_wiki_to_be_more_current_and_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-19T01:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu0djr</id>
    <title>Qwen Code CLI has generous FREE Usage option</title>
    <updated>2025-08-18T22:15:33+00:00</updated>
    <author>
      <name>/u/NoobMLDude</name>
      <uri>https://old.reddit.com/user/NoobMLDude</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who didnt know, Qwen-Code which is a clone of Gemini CLI has a good &lt;a href="https://github.com/QwenLM/qwen-code?tab=readme-ov-file#-free-options-available"&gt;Free usage plan&lt;/a&gt;: - 2,000 requests per day with no token limits - 60 requests per minute rate limit It allows us to use Qwen3Coder for FREE.&lt;/p&gt; &lt;p&gt;Made a small video to showcase how to setup and use here: &lt;a href="https://youtu.be/M6ubLFqL-OA"&gt;https://youtu.be/M6ubLFqL-OA&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoobMLDude"&gt; /u/NoobMLDude &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu0djr/qwen_code_cli_has_generous_free_usage_option/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mto8fa</id>
    <title>New code benchmark puts Qwen 3 Coder at the top of the open models</title>
    <updated>2025-08-18T14:51:49+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt; &lt;img alt="New code benchmark puts Qwen 3 Coder at the top of the open models" src="https://external-preview.redd.it/-FhGljRcqsXlvJ4R58hFA0RMnpSa9fBguxJ8Dc9Mg-4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=15659a7787e6ab90c61f6ea82b0300809513a758" title="New code benchmark puts Qwen 3 Coder at the top of the open models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR of the open models results:&lt;/p&gt; &lt;p&gt;Q3C fp16 &amp;gt; Q3C fp8 &amp;gt; GPT-OSS-120b &amp;gt; V3 &amp;gt; K2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://brokk.ai/power-ranking?round=open&amp;amp;models=flash-2.5%2Cgpt-oss-120b%2Cgpt5-mini%2Ck2%2Cq3c%2Cq3c-fp8%2Cv3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mto8fa/new_code_benchmark_puts_qwen_3_coder_at_the_top/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T14:51:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1mu15vr</id>
    <title>bilbo.high.reasoning.medium.mini.3lightbulbs.ultra</title>
    <updated>2025-08-18T22:47:26+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt; &lt;img alt="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" src="https://preview.redd.it/bfdlovjpvujf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b521d9b5416a36368655d8645cd92560559169e" title="bilbo.high.reasoning.medium.mini.3lightbulbs.ultra" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfdlovjpvujf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mu15vr/bilbohighreasoningmediummini3lightbulbsultra/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T22:47:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttgrf</id>
    <title>Qwen-Image-Edit Released!</title>
    <updated>2025-08-18T18:00:24+00:00</updated>
    <author>
      <name>/u/MohamedTrfhgx</name>
      <uri>https://old.reddit.com/user/MohamedTrfhgx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba’s Qwen team just released &lt;strong&gt;Qwen-Image-Edit&lt;/strong&gt;, an image editing model built on the &lt;strong&gt;20B Qwen-Image&lt;/strong&gt; backbone. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports &lt;strong&gt;precise bilingual (Chinese &amp;amp; English) text editing&lt;/strong&gt; while preserving style, plus both &lt;strong&gt;semantic&lt;/strong&gt; and &lt;strong&gt;appearance-level&lt;/strong&gt; edits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text editing with bilingual support&lt;/li&gt; &lt;li&gt;High-level semantic editing (object rotation, IP creation, concept edits)&lt;/li&gt; &lt;li&gt; Low-level appearance editing (add / delete / insert objects)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/Alibaba_Qwen/status/1957500569029079083"&gt;https://x.com/Alibaba_Qwen/status/1957500569029079083&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen has been really prolific lately what do you think of the new model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MohamedTrfhgx"&gt; /u/MohamedTrfhgx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttgrf/qwenimageedit_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T18:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mtvgjx</id>
    <title>NVIDIA Releases Nemotron Nano 2 AI Models</title>
    <updated>2025-08-18T19:12:01+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt; &lt;img alt="NVIDIA Releases Nemotron Nano 2 AI Models" src="https://preview.redd.it/pzrpnuykutjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d61af91b9dcdda4649c24e581ac3941490ab82c0" title="NVIDIA Releases Nemotron Nano 2 AI Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;• 6X faster than similarly sized models, while also being more accurate&lt;/p&gt; &lt;p&gt;• NVIDIA is also releasing most of the data they used to create it, including the pretraining corpus&lt;/p&gt; &lt;p&gt;• The hybrid Mamba-Transformer architecture supports 128K context length on single GPU.&lt;/p&gt; &lt;p&gt;Full research paper here: &lt;a href="https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/"&gt;https://research.nvidia.com/labs/adlr/NVIDIA-Nemotron-Nano-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pzrpnuykutjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mtvgjx/nvidia_releases_nemotron_nano_2_ai_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T19:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mttcr9</id>
    <title>🚀 Qwen released Qwen-Image-Edit!</title>
    <updated>2025-08-18T17:56:23+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt; &lt;img alt="🚀 Qwen released Qwen-Image-Edit!" src="https://b.thumbs.redditmedia.com/oRveemue3RG8vuBdHGpOCwiYY2B-M7S5WjEjTkW73hM.jpg" title="🚀 Qwen released Qwen-Image-Edit!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Excited to introduce Qwen-Image-Edit! Built on 20B Qwen-Image, it brings precise bilingual text editing (Chinese &amp;amp; English) while preserving style, and supports both semantic and appearance-level editing.&lt;/p&gt; &lt;p&gt;✨ Key Features&lt;/p&gt; &lt;p&gt;✅ Accurate text editing with bilingual support&lt;/p&gt; &lt;p&gt;✅ High-level semantic editing (e.g. object rotation, IP creation)&lt;/p&gt; &lt;p&gt;✅ Low-level appearance editing (e.g. addition/delete/insert)&lt;/p&gt; &lt;p&gt;Try it now: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwenlm.github.io/blog/qwen-image-edit/"&gt;https://qwenlm.github.io/blog/qwen-image-edit/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mttcr9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mttcr9/qwen_released_qwenimageedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-18T17:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
