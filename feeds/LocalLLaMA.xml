<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-12T22:23:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o50ekb</id>
    <title>Complete noob in LLMs</title>
    <updated>2025-10-12T20:51:40+00:00</updated>
    <author>
      <name>/u/Mobile_Bread6664</name>
      <uri>https://old.reddit.com/user/Mobile_Bread6664</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a university student with suitable hardware, exploring Large Language Models, specifically RAG.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Could you please advise on how to learn LLMs with RAG from the beginning, considering my moderate Python proficiency?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Are there any recommended books, courses, or YouTube channels for this purpose?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is freelancing a viable option, perhaps after reaching a certain level of understanding?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What are some tips for learning efficiently, ensuring a solid grasp of the fundamental concepts?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What are the potential future opportunities in the field of RAG?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Approximately how many people are currently working with RAG? &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mobile_Bread6664"&gt; /u/Mobile_Bread6664 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50ekb/complete_noob_in_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50ekb/complete_noob_in_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o50ekb/complete_noob_in_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T20:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwtv</id>
    <title>I benchmarked my Redmagic 9 Pro phone, initially to find out whether the BLAS batch size parameter had an observable effect on performance, and got some interesting results.</title>
    <updated>2025-10-12T11:43:37+00:00</updated>
    <author>
      <name>/u/PurpleWinterDawn</name>
      <uri>https://old.reddit.com/user/PurpleWinterDawn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwtv/i_benchmarked_my_redmagic_9_pro_phone_initially/"&gt; &lt;img alt="I benchmarked my Redmagic 9 Pro phone, initially to find out whether the BLAS batch size parameter had an observable effect on performance, and got some interesting results." src="https://b.thumbs.redditmedia.com/G1HJLM9wP3Xt7l35pbv_SP4NqP7-XzCxO4A5l25VamM.jpg" title="I benchmarked my Redmagic 9 Pro phone, initially to find out whether the BLAS batch size parameter had an observable effect on performance, and got some interesting results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Phone maker and model: Redmagic 9 Pro 512/16GB, released end of Dec. 2023.&lt;/p&gt; &lt;p&gt;Results :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Basically a wash on prompt processing speeds ;&lt;/li&gt; &lt;li&gt;Some interesting results on the 100 tokens generations, including massive outliers I have no explanation for ;&lt;/li&gt; &lt;li&gt;Going from 3840 to 4096 context window sizes increased the PP and generation speeds slightly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Notes :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ran on Termux, KoboldCpp compiled on-device ;&lt;/li&gt; &lt;li&gt;This is the Unsloth Q4_0 quant ;&lt;/li&gt; &lt;li&gt;100% battery. Power consumption stood at around 7.5 to 9W at the wall, factory phone charger losses included ;&lt;/li&gt; &lt;li&gt;Choice of number of threads: going from 3 to 6 threads registered a great boost in speeds, while 7 threads halved the results obtained at 6 threads. 8 threads not tested. Hypothesis: all cores run at the same frequency, and the slowest cores slow the rest too much to be worth adding to the process. KoboldCpp notes &amp;quot;6 threads and 6 BLAS threads&amp;quot; were spawned ;&lt;/li&gt; &lt;li&gt;Choice of quant: Q4_0 allows using the Llama.cpp improvements for ARM with memory interleaving, increasing performance ; I have observed Q4_K_M models running single-digit speeds at under 1k context window usage ;&lt;/li&gt; &lt;li&gt;Choice of KV quant: Q8 was basically for the compromise on memory usage, considering the device used. I only evaluated whether the model was coherent on a random topic repeatedly (&amp;quot;A wolf has entered my house, what do I do? AI: &amp;lt;insert short response here&amp;gt; User: Thank you. Any other advice? AI: &amp;lt;insert 240+ tokens response here&amp;gt;&amp;quot;) before using it for the benchmark ;&lt;/li&gt; &lt;li&gt;FlashAttention: this one I was divided on, but settled on using it because KoboldCpp highly discourages using QuantKV without it, citing possible higher memory usage than without QuantKV at all ;&lt;/li&gt; &lt;li&gt;I highly doubt KoboldCpp uses the Qualcomm Hexagon NPU at all ; it didn't use the integrated GPU either, as trying to compile with LLAMA_VULKAN=1 failed ;&lt;/li&gt; &lt;li&gt;htop reported RAM usage went up from 8.20GB to 10.90GB which corresponds to the model size, while KoboldCpp reported 37.72MiB for llama_context at 4096 context window. I'm surprised by this &amp;quot;small&amp;quot; memory footprint for the context.&lt;/li&gt; &lt;li&gt;This benchmark session took the better time of 8 hours ;&lt;/li&gt; &lt;li&gt;While the memory footprint of the context allowed for testing larger context windows, going all the way to 8192 context window size would take an inordinate amount of time to benchmark.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you think other parameters can improve those charts, I'll be happy to try a few of them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleWinterDawn"&gt; /u/PurpleWinterDawn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o4mwtv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwtv/i_benchmarked_my_redmagic_9_pro_phone_initially/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwtv/i_benchmarked_my_redmagic_9_pro_phone_initially/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4p3vj</id>
    <title>Very interesting! OmniInsert — mask-free video insertion of any reference</title>
    <updated>2025-10-12T13:29:16+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New diffusion-transformer method that inserts a referenced subject into a source video &lt;strong&gt;without masks&lt;/strong&gt;, with robust demos and a technique report. Paper + project page are live; repo is up—&lt;strong&gt;eager to test once code &amp;amp; weights drop&lt;/strong&gt;. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Highlights: InsertPipe data pipeline, condition-specific feature injection, progressive training; introduces &lt;strong&gt;InsertBench&lt;/strong&gt;. &lt;a href="https://arxiv.org/abs/2509.17627?utm_source=chatgpt.com"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Status: Apache-2.0 repo; no releases yet; open issue requesting HF models/dataset; arXiv says “code will be released.” &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://phantom-video.github.io/OmniInsert/"&gt;&lt;strong&gt;https://phantom-video.github.io/OmniInsert/&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4p3vj/very_interesting_omniinsert_maskfree_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4p3vj/very_interesting_omniinsert_maskfree_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4p3vj/very_interesting_omniinsert_maskfree_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T13:29:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4y7yo</id>
    <title>How and what and can I?</title>
    <updated>2025-10-12T19:27:06+00:00</updated>
    <author>
      <name>/u/jhenryscott</name>
      <uri>https://old.reddit.com/user/jhenryscott</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a 9060Xt 16GB to play games on and liked it so much I bought a 9070xt-16GB too. Can I now use my small fortune in vram to do LLM things? How might I do that? Are there some resources that work better with ayymd?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jhenryscott"&gt; /u/jhenryscott &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4y7yo/how_and_what_and_can_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4y7yo/how_and_what_and_can_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4y7yo/how_and_what_and_can_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T19:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wdzo</id>
    <title>Best smaller model as base for fine tuning SCAD?</title>
    <updated>2025-10-12T18:16:37+00:00</updated>
    <author>
      <name>/u/ComprehensiveBird317</name>
      <uri>https://old.reddit.com/user/ComprehensiveBird317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, my idea is to compress many examples of working SCAD code into a smaller, local, specialized LLM, mostly because I don't want to pay closed source model providers to guess with me. I was thinking about the smaller qwen 3 models for turning a technical description of an object into an scad code, or does glm have some usable small ones as well? Which would you use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComprehensiveBird317"&gt; /u/ComprehensiveBird317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wdzo/best_smaller_model_as_base_for_fine_tuning_scad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wdzo/best_smaller_model_as_base_for_fine_tuning_scad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wdzo/best_smaller_model_as_base_for_fine_tuning_scad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ra1k</id>
    <title>Deleted Ollama, but it’s still running on my MacBook</title>
    <updated>2025-10-12T15:00:19+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Wafer81</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Wafer81</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm going crazy. I deleted Ollama a few weeks ago to save my battery since it was draining almost all of it. I thought I had completely removed it, every last bit. Apparently not, because this popped up when I turned my MacBook on. Any idea how to fix this? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Wafer81"&gt; /u/Puzzleheaded-Wafer81 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ra1k/deleted_ollama_but_its_still_running_on_my_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ra1k/deleted_ollama_but_its_still_running_on_my_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ra1k/deleted_ollama_but_its_still_running_on_my_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T15:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4l7qk</id>
    <title>I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus</title>
    <updated>2025-10-12T10:02:47+00:00</updated>
    <author>
      <name>/u/alone_musk18</name>
      <uri>https://old.reddit.com/user/alone_musk18</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"&gt; &lt;img alt="I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus" src="https://preview.redd.it/1rpwzkgqmnuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afa5a5bad4ea1892710d6879e8e3673370dee73e" title="I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alone_musk18"&gt; /u/alone_musk18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1rpwzkgqmnuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T10:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4xhot</id>
    <title>Effectiveness of Gemini for Sentence Similarity</title>
    <updated>2025-10-12T18:58:59+00:00</updated>
    <author>
      <name>/u/watts-going-on</name>
      <uri>https://old.reddit.com/user/watts-going-on</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to test the similarity between several thousand sentences and find which ones are the most similar to each other. I am currently looking at the models on hugging face and it seems that &lt;a href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"&gt;all-MiniLM-L6-v2&lt;/a&gt; remains the most popular option. It seems to be pretty fast for my needs and relatively accurate. I've also seen the &lt;a href="https://huggingface.co/google/embeddinggemma-300m"&gt;embeddinggemma-300m&lt;/a&gt; model from Google (built using the technology for Gemini) which seems to be promising and released very recently. Is there a leaderboard to determine which ones are the most accurate?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/watts-going-on"&gt; /u/watts-going-on &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4xhot/effectiveness_of_gemini_for_sentence_similarity/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4xhot/effectiveness_of_gemini_for_sentence_similarity/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4xhot/effectiveness_of_gemini_for_sentence_similarity/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o506dy</id>
    <title>What is the one resource you’d recommend to someone looking to learn how to train and deploy LLMs from scratch?</title>
    <updated>2025-10-12T20:42:47+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It can be a blog post, reddit thread, an youtube video, github notebook or even an actual book. If someone is trying to learn the concepts behind fine tunning LLMs like the buidling blocks of LLMs and deploying it for inference, what would you suggest?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o506dy/what_is_the_one_resource_youd_recommend_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o506dy/what_is_the_one_resource_youd_recommend_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o506dy/what_is_the_one_resource_youd_recommend_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T20:42:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4fxer</id>
    <title>PSA: Ollama no longer supports the Mi50 or Mi60</title>
    <updated>2025-10-12T04:32:11+00:00</updated>
    <author>
      <name>/u/TechEnthusiastx86</name>
      <uri>https://old.reddit.com/user/TechEnthusiastx86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ollama/ollama/pull/12481"&gt;https://github.com/ollama/ollama/pull/12481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama recently upgraded its ROCM version and therefore no longer supports the Mi50 or Mi60.&lt;/p&gt; &lt;p&gt;Their most recent release notes states that &amp;quot;AMD gfx900 and gfx906 (MI50, MI60, etc) GPUs are no longer supported via ROCm. We're working to support these GPUs via Vulkan in a future release.&amp;quot; &lt;/p&gt; &lt;p&gt;This means if you pull the latest version of Ollama you won't be able to use the Mi50 even though Ollama docs still list it as being supported.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechEnthusiastx86"&gt; /u/TechEnthusiastx86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T04:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4vnsw</id>
    <title>Claudiomiro: How to Achieve 100% Autonomous (Complex) Coding</title>
    <updated>2025-10-12T17:49:04+00:00</updated>
    <author>
      <name>/u/TomatilloPutrid3939</name>
      <uri>https://old.reddit.com/user/TomatilloPutrid3939</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4vnsw/claudiomiro_how_to_achieve_100_autonomous_complex/"&gt; &lt;img alt="Claudiomiro: How to Achieve 100% Autonomous (Complex) Coding" src="https://external-preview.redd.it/7LCI8FO2pE1VYmvbvycVSKDluOb5d_iczH7df7yoYs0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46a3e5e596daedec77293906b07320c58cc2378b" title="Claudiomiro: How to Achieve 100% Autonomous (Complex) Coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/dctvkrrwxpuf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7fad3d214aca83969aafb4406af5c6f2b9ded58"&gt;https://preview.redd.it/dctvkrrwxpuf1.png?width=843&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f7fad3d214aca83969aafb4406af5c6f2b9ded58&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Send your prompt — it decomposes, codes, reviews, builds, tests, and commits autonomously, in PARALLEL.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;With an army of AI agents, turn days of complex development into a fully automated process — without sacrificing production-grade code quality. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/samuelfaj/claudiomiro"&gt;https://github.com/samuelfaj/claudiomiro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope you guys like it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomatilloPutrid3939"&gt; /u/TomatilloPutrid3939 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4vnsw/claudiomiro_how_to_achieve_100_autonomous_complex/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4vnsw/claudiomiro_how_to_achieve_100_autonomous_complex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4vnsw/claudiomiro_how_to_achieve_100_autonomous_complex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T17:49:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o528nk</id>
    <title>What is your PC/Server/AI Server/Homelab idle power consumption?</title>
    <updated>2025-10-12T22:06:18+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you guys are having a nice day.&lt;/p&gt; &lt;p&gt;I was wondering, how much is the power consumption at idle (aka with the PC booted up, with either a model loaded or not but not using it).&lt;/p&gt; &lt;p&gt;I will start:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Consumer Board: MSI X670E Carbon&lt;/li&gt; &lt;li&gt;Consumer CPU: AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;7 GPUs &lt;ul&gt; &lt;li&gt;5090x2&lt;/li&gt; &lt;li&gt;4090x2&lt;/li&gt; &lt;li&gt;A6000&lt;/li&gt; &lt;li&gt;3090x2&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;5 M2 SSDs (via USB to M2 NVME adapters)&lt;/li&gt; &lt;li&gt;2 SATA SSDs&lt;/li&gt; &lt;li&gt;7 120mm fans&lt;/li&gt; &lt;li&gt;4 PSUs: &lt;ul&gt; &lt;li&gt;1250W Gold&lt;/li&gt; &lt;li&gt;850W Bronze&lt;/li&gt; &lt;li&gt;1200W Gold&lt;/li&gt; &lt;li&gt;700W Gold&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Idle power consumption: 240-260W&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Also for reference, here in Chile electricity is insanely expensive (0.25USD per kwh).&lt;/p&gt; &lt;p&gt;When using a model on lcpp it uses about 800W. When using a model with exl or vllm, it uses about 1400W.&lt;/p&gt; &lt;p&gt;Most of the time I have it powered off as that price accumulates quite a bit.&lt;/p&gt; &lt;p&gt;How much is your idle power consumption?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o528nk/what_is_your_pcserverai_serverhomelab_idle_power/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o528nk/what_is_your_pcserverai_serverhomelab_idle_power/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o528nk/what_is_your_pcserverai_serverhomelab_idle_power/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T22:06:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4szf0</id>
    <title>Paper2Video — turn a research paper into a full presentation video (slides, speech, talking head)</title>
    <updated>2025-10-12T16:06:41+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"&gt; &lt;img alt="Paper2Video — turn a research paper into a full presentation video (slides, speech, talking head)" src="https://external-preview.redd.it/14WZnO_akEyJj42YD4L-nD--_D0Ep8o7tXNC2svzt48.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=604ef91a15e04ea2ea32a47e9edd6696a8318d46" title="Paper2Video — turn a research paper into a full presentation video (slides, speech, talking head)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Multi-agent pipeline (“PaperTalker”) that takes a paper + reference &lt;strong&gt;image/audio&lt;/strong&gt; and outputs a polished &lt;strong&gt;presentation video&lt;/strong&gt; (Slides → Subtitles → Speech → Cursor → Talking-Head). &lt;strong&gt;MIT&lt;/strong&gt; licensed, code + benchmark out. &lt;a href="https://github.com/showlab/Paper2Video"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One-command run via &lt;a href="http://pipeline.py"&gt;&lt;code&gt;pipeline.py&lt;/code&gt;&lt;/a&gt;; set &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; / &lt;code&gt;GEMINI_API_KEY&lt;/code&gt; (best: GPT-4.1 or Gemini 2.5). Depends on Hallo2 + Paper2Poster. &lt;/li&gt; &lt;li&gt;Recommended: &lt;strong&gt;A6000 48GB&lt;/strong&gt; for end-to-end generation.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Benchmark (&lt;strong&gt;101&lt;/strong&gt; paper–video pairs) + metrics: Meta Similarity, PresentArena, PresentQuiz, IP Memory.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b4nd5tfmfpuf1.png?width=835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90777151264bb001c851e64669dcb7b6baae186e"&gt;https://preview.redd.it/b4nd5tfmfpuf1.png?width=835&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=90777151264bb001c851e64669dcb7b6baae186e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4szf0/paper2video_turn_a_research_paper_into_a_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T16:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4hxqe</id>
    <title>KoboldCpp now supports video generation</title>
    <updated>2025-10-12T06:32:40+00:00</updated>
    <author>
      <name>/u/fish312</name>
      <uri>https://old.reddit.com/user/fish312</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"&gt; &lt;img alt="KoboldCpp now supports video generation" src="https://external-preview.redd.it/n7QpKCCkcHUBLj4nC-Lh95amFG6mzdqatT5L5ZA_y1k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f4fc22015a57b49dec0643ef6f0d2a92f83c37" title="KoboldCpp now supports video generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fish312"&gt; /u/fish312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T06:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4xkr6</id>
    <title>Open source streaming STT (Parakeet + Silero + Pipecat Smart Turn)</title>
    <updated>2025-10-12T19:02:09+00:00</updated>
    <author>
      <name>/u/Adventurous-Top209</name>
      <uri>https://old.reddit.com/user/Adventurous-Top209</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4xkr6/open_source_streaming_stt_parakeet_silero_pipecat/"&gt; &lt;img alt="Open source streaming STT (Parakeet + Silero + Pipecat Smart Turn)" src="https://external-preview.redd.it/djAxcDdvb2FhcXVmMbhst-7tsz73ZUCHaN6PiIo0aAbxtcYEpNwxJfvHepHJ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56538f141f48c80a9fcf75683708a8b45c2b399d" title="Open source streaming STT (Parakeet + Silero + Pipecat Smart Turn)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Made this STT streaming server as a piece of a larger project I'm working on. Parakeet is pretty darn fast! Also supports batch inference (because I had a business need for it). Demo above running on a 3090 locally then also showing what the deployed version can do on an L40s.&lt;/p&gt; &lt;p&gt;Also end-of-turn detection is pretty decent. You can see the EOT probabilities drop significantly during my Uhhs and Umms.&lt;/p&gt; &lt;p&gt;STT code found here: &lt;a href="https://github.com/gabber-dev/gabber/tree/main/services/gabber-stt"&gt;https://github.com/gabber-dev/gabber/tree/main/services/gabber-stt&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Top209"&gt; /u/Adventurous-Top209 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/9jef0moaaquf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4xkr6/open_source_streaming_stt_parakeet_silero_pipecat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4xkr6/open_source_streaming_stt_parakeet_silero_pipecat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T19:02:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o50mfy</id>
    <title>Benchmarking small models at 4bit quants on Apple Silicon with mlx-lm</title>
    <updated>2025-10-12T21:00:16+00:00</updated>
    <author>
      <name>/u/ironwroth</name>
      <uri>https://old.reddit.com/user/ironwroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"&gt; &lt;img alt="Benchmarking small models at 4bit quants on Apple Silicon with mlx-lm" src="https://b.thumbs.redditmedia.com/Mr4_Zgx6PZYo5zuPmVXJlD-lJxUzfjZQv81s2I9zk7Q.jpg" title="Benchmarking small models at 4bit quants on Apple Silicon with mlx-lm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran a bunch of small models at 4bit quants through a few benchmarks locally on my MacBook using `mlx-lm.evaluate`. Figured I would share in case anyone else finds it interesting or helpful!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zpl8i0uxsquf1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b079f8de5bad0208a60600b50ff225f9b5e3371a"&gt;https://preview.redd.it/zpl8i0uxsquf1.png?width=1850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b079f8de5bad0208a60600b50ff225f9b5e3371a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System Info: Apple M4 Pro, 48gb RAM, 20 core GPU, 14 core CPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironwroth"&gt; /u/ironwroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o50mfy/benchmarking_small_models_at_4bit_quants_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T21:00:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4dswr</id>
    <title>HuggingFace storage is no longer unlimited - 12TB public storage max</title>
    <updated>2025-10-12T02:36:39+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you’ve missed the memo like me, HuggingFace is no longer unlimited.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Type of account&lt;/th&gt; &lt;th&gt;Public storage&lt;/th&gt; &lt;th&gt;Private storage&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Free user or org&lt;/td&gt; &lt;td&gt;Best-effort* usually up to 5 TB for impactful work&lt;/td&gt; &lt;td&gt;100 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;PRO&lt;/td&gt; &lt;td&gt;Up to 10 TB included* ✅ grants available for impactful work†&lt;/td&gt; &lt;td&gt;1 TB + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Team Organizations&lt;/td&gt; &lt;td&gt;12 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Enterprise Organizations&lt;/td&gt; &lt;td&gt;500 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As seen on &lt;a href="https://huggingface.co/docs/hub/en/storage-limits"&gt;https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, they started enforcing it.&lt;/p&gt; &lt;p&gt;—-&lt;/p&gt; &lt;p&gt;For ref. &lt;a href="https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits"&gt;https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4sgv5</id>
    <title>Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry</title>
    <updated>2025-10-12T15:46:43+00:00</updated>
    <author>
      <name>/u/nelson_moondialu</name>
      <uri>https://old.reddit.com/user/nelson_moondialu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"&gt; &lt;img alt="Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry" src="https://external-preview.redd.it/brnT1-CiL694NH_ogGrkVOnYdebEpEkUcEFq_sappRI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d425501d8da63b539406ec0adae7cd9c361ea22d" title="Interview with Z.ai employee, the company behind the GLM models. Talks about competition and attitudes towards AI in China, dynamics and realities of the industry" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nelson_moondialu"&gt; /u/nelson_moondialu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=r0SalROzO38"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4sgv5/interview_with_zai_employee_the_company_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T15:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wruz</id>
    <title>GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12 AMD MI50 32GB</title>
    <updated>2025-10-12T18:31:28+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"&gt; &lt;img alt="GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12 AMD MI50 32GB" src="https://b.thumbs.redditmedia.com/T2YTTmUil5Maqv4vztGdGrUmIECI1jyMPgt4enseQKI.jpg" title="GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12 AMD MI50 32GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally got another six MI50 32gb. Removed my old Nvidia Titan Vs in my 2nd HP DL580 Gen9.&lt;/p&gt; &lt;h1&gt;Here we go. 384GB VRAM&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;running on secondary host:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp.20251012/build/bin/rpc-server --host 0.0.0.0 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 6 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 1: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 2: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 3: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 4: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 5: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WARNING: Host ('0.0.0.0') is != '127.0.0.1' Never expose the RPC server to an open network! This is an experimental feature and is not secure! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Starting RPC server v3.0.0 endpoint : 0.0.0.0:50052 local cache : n/a Devices: ROCm0: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm1: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm2: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm3: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm4: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm5: AMD Radeon Graphics (32752 MiB, 32694 MiB free) Accepted client connection &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Then on primary host:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;~/llama.cpp/build/bin/llama-server --model ~/models/GLM-4.6-UD-Q6_K_XL-00001-of-00006.gguf --cache-type-k q8_0 --cache-type-v q8_0 --n-gpu-layers 94 --temp 0.6 --ctx-size 131072 --host 0.0.0.0 --rpc 192.168.1.xxx:50052 --alias GLM-4.6_RPC &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Observations (vs Single Node 6x MI50 32gb with GLM 4.6 Q3_K_S):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing about the same on smaller prompts. 62-65 tok/s&lt;/li&gt; &lt;li&gt;Text generation 7.5 tok/s vs 8.5 tok/s, &lt;strong&gt;UD-Q6_K_XL&lt;/strong&gt; vs &lt;strong&gt;Q3_K_S&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Each server idles ~350W. Inference causes 1-2 GPUs to round robin across 12 GPUs with 100-170w power draw vs the rest (10-11 GPUs) @ ~20w.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Prior experiement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nxv7x6/performance_of_glm_46_q3_k_s_on_6x_mi50/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/45wsc8fe5quf1.png?width=3247&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa596c0609bda881db13ad569f60a0789cc11da"&gt;https://preview.redd.it/45wsc8fe5quf1.png?width=3247&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8fa596c0609bda881db13ad569f60a0789cc11da&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Verbose output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/VzuvcKpU"&gt;GLM 4.6 UD-Q6_K_XL running llama.cpp RPC across two nodes and 12x AMD MI50 32GB - Pastebin.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: &lt;/p&gt; &lt;p&gt;You can cache tensors in RPC command. Path is not the same as HuggingFace.&lt;/p&gt; &lt;pre&gt;&lt;code&gt; ~/llama.cpp.20251012/build/bin/rpc-server --host 0.0.0.0 -c ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 6 ROCm devices: Device 0: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 1: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 2: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 3: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 4: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 Device 5: AMD Radeon Graphics, gfx906:sramecc+:xnack- (0x906), VMM: no, Wave Size: 64 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! WARNING: Host ('0.0.0.0') is != '127.0.0.1' Never expose the RPC server to an open network! This is an experimental feature and is not secure! !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Starting RPC server v3.0.0 endpoint : 0.0.0.0:50052 local cache : /home/user/.cache/llama.cpp/rpc/ Devices: ROCm0: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm1: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm2: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm3: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm4: AMD Radeon Graphics (32752 MiB, 32694 MiB free) ROCm5: AMD Radeon Graphics (32752 MiB, 32694 MiB free) Accepted client connection Client connection closed Accepted client connection [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/be7d8d14939819c1' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/aed746681261df7e' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/caf5eb137973dabd' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/2293478b2975daba' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/0588ea2a4a15bdb4' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/ec7b90bfeb1c9fac' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/506047f7ea6a6b5c' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/7e8ef54f72bb5970' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/67a44d91f0298ee1' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/1956963fa7b4cc6a' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/5b1d78872debd949' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/843c7f02e369a92e' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/4defcd4d4ce9618e' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/4865cc4205b44aea' [set_tensor] saved to '/home/user/.cache/llama.cpp/rpc/95041e30d8ecdd09' ... &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wruz/glm_46_udq6_k_xl_running_llamacpp_rpc_across_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:31:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mdkh</id>
    <title>Why has Meta research failed to deliver foundational model at the level of Grok, Deepseek or GLM?</title>
    <updated>2025-10-12T11:13:22+00:00</updated>
    <author>
      <name>/u/External_Natural9590</name>
      <uri>https://old.reddit.com/user/External_Natural9590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They have been in the space for longer - could have atracted talent earlier, their means are comparable to ther big tech. So why have they been outcompeted so heavily? I get they are currently a one generation behind and the chinese did some really clever wizardry which allowed them to squeeze a lot more eke out of every iota. But what about xAI? They compete for the same talent and had to start from the scratch. Or was starting from the scratch actually an advantage here? Or is it just a matter of how many key ex OpenAI employees was each company capable of attracting - trafficking out the trade secrets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Natural9590"&gt; /u/External_Natural9590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4qix3</id>
    <title>Claude's system prompt length has now exceeded 30k tokens</title>
    <updated>2025-10-12T14:29:52+00:00</updated>
    <author>
      <name>/u/StableSable</name>
      <uri>https://old.reddit.com/user/StableSable</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"&gt; &lt;img alt="Claude's system prompt length has now exceeded 30k tokens" src="https://external-preview.redd.it/otAtlKXoVGIzRX_D-XS8ef102ismRuSmY-rYGjCWHEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=345f8e8a9693f1ecf3f281e2c9b37a5656e8634f" title="Claude's system prompt length has now exceeded 30k tokens" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableSable"&gt; /u/StableSable &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/asgeirtj/system_prompts_leaks/blob/main/Anthropic/claude-4.5-sonnet.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4qix3/claudes_system_prompt_length_has_now_exceeded_30k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T14:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4uagn</id>
    <title>Traning Llama3.2:3b on my whatsapp chats with wife</title>
    <updated>2025-10-12T16:56:53+00:00</updated>
    <author>
      <name>/u/jayjay_1996</name>
      <uri>https://old.reddit.com/user/jayjay_1996</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;So my wife and I have been dating since 2018. ALL our chats are on WhatsApp. &lt;/p&gt; &lt;p&gt;I am an LLM noob but I wanted to export it as a txt. And then feed it into an LLM so I could ask questions like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;who has said I love you more? &lt;/li&gt; &lt;li&gt;who apologises more? &lt;/li&gt; &lt;li&gt;what was discussed during our Japan trip? &lt;/li&gt; &lt;li&gt;how many times did we fight in July 2023? &lt;/li&gt; &lt;li&gt;who is more sarcastic in 2025? &lt;/li&gt; &lt;li&gt;list all the people we’ve talked about&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Etc&lt;/p&gt; &lt;p&gt;So far - the idea was to chunk them and store them in a vector DB. And then use llama to interact with it. But the results have been quite horrible. Temp - 0.1 to 0.5, k=3 to 25. Broke the chat into chunks of 4000 with overlap 100 &lt;/p&gt; &lt;p&gt;Any better ideas out there? Would love to hear! And if it works I could share the ingestion script!&lt;/p&gt; &lt;p&gt;Edit - I’ve reduced the chunk size to 250. And ingesting it via llama3.2:3b. Currently - 14 hours out of 34 done! Another 20 hours and I could let you know how that turns out ☠️ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jayjay_1996"&gt; /u/jayjay_1996 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4uagn/traning_llama323b_on_my_whatsapp_chats_with_wife/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T16:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4v880</id>
    <title>New Unhinged NSFW Reasoning Model - Satyr-V0.1-4B</title>
    <updated>2025-10-12T17:32:33+00:00</updated>
    <author>
      <name>/u/ThePantheonUnbound</name>
      <uri>https://old.reddit.com/user/ThePantheonUnbound</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This version is an unpredictable experiment and may produce vulgar, explicit, or graphic content. Please use it at your own risk. More multifaceted versions will be released soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThePantheonUnbound"&gt; /u/ThePantheonUnbound &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PantheonUnbound/Satyr-V0.1-4B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4v880/new_unhinged_nsfw_reasoning_model_satyrv014b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4v880/new_unhinged_nsfw_reasoning_model_satyrv014b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T17:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4wg6q</id>
    <title>Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp; try the demo</title>
    <updated>2025-10-12T18:18:58+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"&gt; &lt;img alt="Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp;amp; try the demo" src="https://external-preview.redd.it/WYNRuaBJiIIoNSwO7SSTGPP2ITAUNljSMCnTROkhRdg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11110ecc44cc993c095ca5cc3a864cd7384ecf18" title="Stanford Researchers Released AgentFlow: Flow-GRPO algorithm. Outperforming 200B GPT-4o with a 7B model! Explore the code &amp;amp; try the demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/AgentFlow/agentflow"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4wg6q/stanford_researchers_released_agentflow_flowgrpo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T18:18:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mwet</id>
    <title>GPU Poor LLM Arena is BACK! 🎉🎊🥳</title>
    <updated>2025-10-12T11:43:02+00:00</updated>
    <author>
      <name>/u/kastmada</name>
      <uri>https://old.reddit.com/user/kastmada</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt; &lt;img alt="GPU Poor LLM Arena is BACK! 🎉🎊🥳" src="https://external-preview.redd.it/xnvppfD8q4Rvrqs00KT2LLxfAKmO_ypt1REhqFgxlVw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49fb1bf881d9a00b0e731a0269d44b4ea6c31968" title="GPU Poor LLM Arena is BACK! 🎉🎊🥳" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;🚀 GPU Poor LLM Arena is BACK! New Models &amp;amp; Updates!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;First off, a massive apology for the extended silence. Things have been a bit hectic, but the GPU Poor LLM Arena is officially back online and ready for action! Thanks for your patience and for sticking around.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;🚀 Newly Added Models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Granite 4.0 Small Unsloth (32B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Tiny Unsloth (7B, 4-bit)&lt;/li&gt; &lt;li&gt;Granite 4.0 Micro Unsloth (3B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Thinking 2507 Unsloth (4B, 8-bit)&lt;/li&gt; &lt;li&gt;Qwen 3 Instruct 2507 Unsloth (30B, 4-bit)&lt;/li&gt; &lt;li&gt;OpenAI gpt-oss Unsloth (20B, 4-bit)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;🚨 Important Notes for GPU-Poor Warriors:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Please be aware that Granite 4.0 Small, Qwen 3 30B, and OpenAI gpt-oss models are quite bulky. Ensure your setup can comfortably handle them before diving in to avoid any performance issues.&lt;/li&gt; &lt;li&gt;I've decided to default to Unsloth GGUFs for now. In many cases, these offer valuable bug fixes and optimizations over the original GGUFs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm happy to see you back in the arena, testing out these new additions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kastmada"&gt; /u/kastmada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/k-mktr/gpu-poor-llm-arena"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mwet/gpu_poor_llm_arena_is_back/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:43:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
