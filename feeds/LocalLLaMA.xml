<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-14T17:41:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r41013</id>
    <title>GLM-5 Is a local GOAT</title>
    <updated>2026-02-13T21:00:08+00:00</updated>
    <author>
      <name>/u/FineClassroom2085</name>
      <uri>https://old.reddit.com/user/FineClassroom2085</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"&gt; &lt;img alt="GLM-5 Is a local GOAT" src="https://external-preview.redd.it/MTlvZ25qOTVyYmpnMet_8L-GzQ_poWye6LYGoFL5kcPokh15ZfJ1OHhOgrf9.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa3d1833719afef6a4e55f5f11807d2e7ef7d341" title="GLM-5 Is a local GOAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Background&lt;/strong&gt;: I am a developer with over two decades of experience. I use LLMs heavily day to day from all of the major providers. Since the first Llama models came out I've been toying with local models, benchmarking them on real-world heavy use cases.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Long story short:&lt;/strong&gt; GLM-5 is the first model I've been able to run locally that's actually impressed me. In 3 'shots' I was able to make a retro styled flappy clone AND deploy it to AWS with a cost assessment if it went viral.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My prompt&lt;/strong&gt;: Please generate a GPU accelerated clone of the game ‚ÄòFlappy Bird‚Äô where using the spacebar causes the bird to ‚Äòflap‚Äô, give it a 'retro inspired' design.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Setup&lt;/strong&gt;:&lt;br /&gt; - Dual RTX 6000 PRO MaxQ GPUs&lt;br /&gt; - 128gb of DDR5&lt;br /&gt; - AMD Ryzen Threadripper PRO 7975WX&lt;br /&gt; - GLM-5-744B served over vLLM with 128k context at IQ2_M&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats&lt;/strong&gt;: Even with my decently powerful hardware, the token output was painfully slow at 16.5t/s. IMO, completely worth the wait though. The same test with Qwen3-Next-80b, GPT-OSS-120b and a few other leaders was unimpressive.&lt;/p&gt; &lt;p&gt;&lt;a href="https://flappy.tjameswilliams.com/"&gt;https://flappy.tjameswilliams.com/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FineClassroom2085"&gt; /u/FineClassroom2085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/7l7iri95rbjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r41013/glm5_is_a_local_goat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T21:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3yuyd</id>
    <title>has it begun?</title>
    <updated>2026-02-13T19:38:01+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt; &lt;img alt="has it begun?" src="https://preview.redd.it/ei9lt0u4ebjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=36034757efbb832ba75f43ed04c4dc8c7bb34675" title="has it begun?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters"&gt;https://www.bloomberg.com/news/articles/2026-02-13/us-to-put-alibaba-on-list-for-aiding-china-s-military-reuters&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They were about to present the name of alibaba and Baidu as a potential threat or issue for helping chinese military in the Pentagon, but ultimately took their names off the list&lt;/p&gt; &lt;p&gt;Would love to hear what y'all think about this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ei9lt0u4ebjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3yuyd/has_it_begun/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T19:38:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4e3w3</id>
    <title>KaniTTS2, our text-to-speech model with frame-level position encodings, optimized for real-time conversational AI.</title>
    <updated>2026-02-14T07:10:30+00:00</updated>
    <author>
      <name>/u/KokaOP</name>
      <uri>https://old.reddit.com/user/KokaOP</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to release KaniTTS2, our text-to-speech model with frame-level position encodings, optimized for real-time conversational AI.&lt;/p&gt; &lt;p&gt;What's in the release:&lt;/p&gt; &lt;p&gt;Pretrained Model (multilingual ‚Äî English, Spanish, Kyrgyz)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-pt"&gt;https://huggingface.co/nineninesix/kani-tts-2-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìå Currently supports 3 languages, with more being added over time. Stay tuned for updates as we expand language coverage.&lt;/p&gt; &lt;p&gt;üá¨üáß English-specific Model&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-2-en"&gt;https://huggingface.co/nineninesix/kani-tts-2-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üõ†Ô∏è Full Pretraining Code ‚Äî train your own TTS model from scratch&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/nineninesix-ai/kani-tts-2-pretrain"&gt;https://github.com/nineninesix-ai/kani-tts-2-pretrain&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;400M parameter model built on LiquidAI's LFM2 backbone + Nvidia NanoCodec&lt;/p&gt; &lt;p&gt;~0.2 RTF on an RTX 5080, 3GB VRAM ‚Äî fast enough for real-time use&lt;/p&gt; &lt;p&gt;Voice cloning with speaker embeddings&lt;/p&gt; &lt;p&gt;Pretrained on ~10k hours of speech data (8x H100s, just 6 hours of training!)&lt;/p&gt; &lt;p&gt;Why we're releasing the pretrain code: We want anyone to be able to train a TTS model for their own language, accent, or domain from scratch. The framework includes FSDP multi-GPU training, Flash Attention 2, YAML-driven configs, and built-in attention analysis metrics to validate layer isolation. Everything you need to go from dataset to deployed model.&lt;/p&gt; &lt;p&gt;Licensed Apache 2.0. Try the demos on our HF Spaces, and come chat with us on Discord if you have questions or want to contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KokaOP"&gt; /u/KokaOP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4e3w3/kanitts2_our_texttospeech_model_with_framelevel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T07:10:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3weq3</id>
    <title>SWE-rebench Jan 2026: GLM-5, MiniMax M2.5, Qwen3-Coder-Next, Opus 4.6, Codex Performance</title>
    <updated>2026-02-13T18:06:40+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench leaderboard&lt;/strong&gt; with our &lt;strong&gt;January runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;Key observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Code (Opus 4.6)&lt;/strong&gt; leads this snapshot at &lt;strong&gt;52.9% resolved rate&lt;/strong&gt; and also achieves the highest &lt;strong&gt;pass@5 (70.8%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.6&lt;/strong&gt; and &lt;strong&gt;gpt-5.2-xhigh&lt;/strong&gt; follow very closely (51.7%), making the top tier extremely tight.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gpt-5.2-medium (51.0%)&lt;/strong&gt; performs surprisingly close to the frontier configuration.&lt;/li&gt; &lt;li&gt;Among open models, &lt;strong&gt;Kimi K2 Thinking (43.8%)&lt;/strong&gt;, &lt;strong&gt;GLM-5 (42.1%)&lt;/strong&gt;, and &lt;strong&gt;Qwen3-Coder-Next (40.0%)&lt;/strong&gt; lead the pack.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MiniMax M2.5 (39.6%)&lt;/strong&gt; continues to show strong performance while remaining one of the cheapest options.&lt;/li&gt; &lt;li&gt;Clear gap between Kimi variants: &lt;strong&gt;K2 Thinking (43.8%)&lt;/strong&gt; vs &lt;strong&gt;K2.5 (37.9%)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Newer smaller/flash variants (e.g., GLM-4.7 Flash, gpt-5-mini-medium) trade performance for efficiency, landing in the 25‚Äì31% range.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=jan_2026"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3weq3/swerebench_jan_2026_glm5_minimax_m25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T18:06:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4leu0</id>
    <title>Built a simple push-to-talk voice tool using local Whisper - super useful for terminal AI assistants</title>
    <updated>2026-02-14T14:03:18+00:00</updated>
    <author>
      <name>/u/Open_Box_60</name>
      <uri>https://old.reddit.com/user/Open_Box_60</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4leu0/built_a_simple_pushtotalk_voice_tool_using_local/"&gt; &lt;img alt="Built a simple push-to-talk voice tool using local Whisper - super useful for terminal AI assistants" src="https://preview.redd.it/gxwlrpubvgjg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=3527bc97eb22c0ef99f4f5d8a9278ef49fc34ac9" title="Built a simple push-to-talk voice tool using local Whisper - super useful for terminal AI assistants" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I noticed when I'm typing prompts to Claude Code or other AI tools, I keep self-editing and cutting my thoughts short. But when I speak, I naturally explain things better and give more context.&lt;/p&gt; &lt;p&gt;Built TalkType to fix this - press F9 to record, speak, press F9 again and it pastes the&lt;br /&gt; transcription wherever your cursor is. Uses faster-whisper locally so nothing leaves your&lt;br /&gt; machine.&lt;/p&gt; &lt;p&gt;&lt;a href="https://raw.githubusercontent.com/lmacan1/talktype/main/assets/demo.gif"&gt;https://raw.githubusercontent.com/lmacan1/talktype/main/assets/demo.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works system-wide (any terminal, browser, text field)&lt;/li&gt; &lt;li&gt;Detects if you're in a terminal and uses the right paste shortcut&lt;/li&gt; &lt;li&gt;Remembers your original window if you alt-tab while talking&lt;/li&gt; &lt;li&gt;Can run as a systemd service so it's always ready&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Linux install:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; git clone https://github.com/lmacan1/talktype.git &amp;amp;&amp;amp; cd talktype &amp;amp;&amp;amp; ./install.sh &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also works on Windows and macOS.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/lmacan1/talktype"&gt;https://github.com/lmacan1/talktype&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Open_Box_60"&gt; /u/Open_Box_60 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gxwlrpubvgjg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4leu0/built_a_simple_pushtotalk_voice_tool_using_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4leu0/built_a_simple_pushtotalk_voice_tool_using_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:03:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r47fz0</id>
    <title>Claude Code with Local Models: Full Prompt Reprocessing with Every Request</title>
    <updated>2026-02-14T01:33:26+00:00</updated>
    <author>
      <name>/u/postitnote</name>
      <uri>https://old.reddit.com/user/postitnote</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Very recently, I found that Claude Code was triggering full prompt processing for every request. I looked into the logs and found CC is adding this to the list of system messages: &lt;code&gt; text:&amp;quot;x-anthropic-billing-header: cc_version=2.1.39.c39; cc_entrypoint=cli; cch=56445;&amp;quot;, type:&amp;quot;text&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The values in the header changed with every request, and the template rendered it as text in the system prompt which caused a full reprocessing. With a little google search, I found &lt;a href="https://github.com/musistudio/claude-code-router/issues/1161"&gt;this&lt;/a&gt;, which recommended doing this to remove the header:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;set env &amp;quot;CLAUDE_CODE_ATTRIBUTION_HEADER&amp;quot;: &amp;quot;0&amp;quot; in claude settings.json&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;And placing that in my ~/.claude/settings.json in the &amp;quot;env&amp;quot; section was enough to remove that from the system prompt and get my KV cache back to being effective again.&lt;/p&gt; &lt;p&gt;Hope that helps anyone running into the same issue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/postitnote"&gt; /u/postitnote &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r47fz0/claude_code_with_local_models_full_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T01:33:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4no3s</id>
    <title>App to analyze a text token-by-token perplexity for a given GGUF</title>
    <updated>2026-02-14T15:37:38+00:00</updated>
    <author>
      <name>/u/EntropyMagnets</name>
      <uri>https://old.reddit.com/user/EntropyMagnets</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4no3s/app_to_analyze_a_text_tokenbytoken_perplexity_for/"&gt; &lt;img alt="App to analyze a text token-by-token perplexity for a given GGUF" src="https://preview.redd.it/ko9gdibnbhjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b985819f603091e042d29566bf0cb1ed0b90205" title="App to analyze a text token-by-token perplexity for a given GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a rust desktop app that allows you to analyze a given text and see how &amp;quot;surprising&amp;quot; it is to a LLM. You just need to have a GGUF model on disk. &lt;/p&gt; &lt;p&gt;You can check it here: &lt;a href="https://github.com/Belluxx/Perplex/"&gt;https://github.com/Belluxx/Perplex/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's quite fun to see from the model's most likely predictions, especially when it gets them wrong (tokens highlighted in red in the app).&lt;/p&gt; &lt;p&gt;Let me know what you think! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EntropyMagnets"&gt; /u/EntropyMagnets &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ko9gdibnbhjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4no3s/app_to_analyze_a_text_tokenbytoken_perplexity_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4no3s/app_to_analyze_a_text_tokenbytoken_perplexity_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T15:37:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4mbu7</id>
    <title>World's most accurate AI-based password guessing tool</title>
    <updated>2026-02-14T14:42:40+00:00</updated>
    <author>
      <name>/u/Arsapen</name>
      <uri>https://old.reddit.com/user/Arsapen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mbu7/worlds_most_accurate_aibased_password_guessing/"&gt; &lt;img alt="World's most accurate AI-based password guessing tool" src="https://external-preview.redd.it/Y3FveTR3ZTgyaGpnMV4UkBybRn1oVzB2i9mvG9t33EhTYJxIuy3OBznMUv_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8817210732fb4d017556faec130a0e1dc013eff2" title="World's most accurate AI-based password guessing tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I've been working on a reproduction of some recent research paper into LLM-based password security (specifically the &lt;a href="https://www.usenix.org/conference/usenixsecurity25/presentation/zou-yunkai"&gt;PassLLM framework&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;The core idea of the project is using PII (names, birthdays, pet names, emails) to generate probability-sorted lists of passwords that a specific user is likely to use online. I've achieved this by using LoRA to fine-tune sub-7B models (like low tier Qwen and Mistral) on millions of publicly available PII/password pairs.&lt;/p&gt; &lt;p&gt;What's interesting is seeing the model pick up on semantic transformations that traditional tools like PCFGs or Markov chains usually miss. For example, it intuitively understands that a user named &amp;quot;Marcus&amp;quot; is likely to use &amp;quot;Mark&amp;quot;, &amp;quot;Marco&amp;quot;, or &amp;quot;Marc&amp;quot; as a base for their password, and it handles leetspeak and compounding much better than any rule-based engine.&lt;/p&gt; &lt;p&gt;So far, the results are satisfying, but most of the data it has been trained on is several years old. While the model is great at capturing human behavior, it hardly reflects password trends of 2026 and still links closely to the 2010s.&lt;/p&gt; &lt;p&gt;I'd love to get your thoughts on adjusting to modern entropy requirements when the training data is older, and your opinion about whether LLMs are actually the future for password auditing, or will the inference cost always make them less practical than optimized rule-based models? Would investing in an even greater training dataset significantly enhance the model's accuracy, or would it face diminishing results at some point? Thanks!&lt;/p&gt; &lt;p&gt;Here's a sample:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;name&amp;quot;: &amp;quot;Sophia M. Turner&amp;quot;, &amp;quot;birth_year&amp;quot;: &amp;quot;2001&amp;quot;, &amp;quot;pet_name&amp;quot;: &amp;quot;Fluffy&amp;quot;, &amp;quot;username&amp;quot;: &amp;quot;soph_t&amp;quot;, &amp;quot;email&amp;quot;: &amp;quot;sturner99@yahoo.com&amp;quot;, &amp;quot;country&amp;quot;: &amp;quot;England&amp;quot;, &amp;quot;sister_pw&amp;quot;: [&amp;quot;soph12345&amp;quot;, &amp;quot;13rockm4n&amp;quot;, &amp;quot;01mamamia&amp;quot;]} --- TOP CANDIDATES --- CONFIDENCE | PASSWORD ------------------------------ 2.93% | sophia123 (this is a mix of the target's first name and the sister password &amp;quot;soph12345&amp;quot;) 2.53% | mamamia01 (a simple variation of another sister password) 1.96% | sophia2001 1.78% | sophie123 (UK passwords often interchange between &amp;quot;sophie&amp;quot; and &amp;quot;sophia&amp;quot;) 1.45% | 123456a (a very commmon password, ranked high due to the &amp;quot;12345&amp;quot; pattern) 1.39% | sophiesophie1 1.24% | sturner999 1.23% | turner2001 1.07% | sturner123 1.05% | sophia12345 0.94% | mamamia99 ... (10,169 passwords generated) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The model can be accessed here, or online through Google Colab: &lt;a href="https://github.com/Tzohar/PassLLM"&gt;https://github.com/Tzohar/PassLLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arsapen"&gt; /u/Arsapen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mg7dfce82hjg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mbu7/worlds_most_accurate_aibased_password_guessing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mbu7/worlds_most_accurate_aibased_password_guessing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:42:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3zuuf</id>
    <title>GPT-OSS 120b Uncensored Aggressive Release (MXFP4 GGUF)</title>
    <updated>2026-02-13T20:15:33+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, made an uncensored version of GPT-OSS 120B.&lt;/p&gt; &lt;p&gt;Quick specs: 117B total params, ~5.1B active (MoE with 128 experts, top-4 routing), 128K context. MXFP4 is the model's native precision - this isn't a quantization, it's how it was trained. No overall quality loss, though you can see CoT behave differently at times.&lt;/p&gt; &lt;p&gt;This is the aggressive variant - &lt;strong&gt;observed 0 refusals to any query during testing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Completely uncensored while keeping full model capabilities intact.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPTOSS-120B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings:&lt;/p&gt; &lt;p&gt;- --temp 1.0 --top-k 40&lt;/p&gt; &lt;p&gt;- Disable everything else (top_p, min_p, repeat penalty, etc.) - some clients turn&lt;/p&gt; &lt;p&gt;these on by default&lt;/p&gt; &lt;p&gt;- llama.cpp users: --jinja is required for the Harmony response format or the model won't work right&lt;/p&gt; &lt;p&gt;- Example: llama-server -m model.gguf --jinja -fa -b 2048 -ub 2048&lt;/p&gt; &lt;p&gt;Single 61GB file. Fits on one H100. For lower VRAM, use --n-cpu-moe N in llama.cpp to offload MoE layers to CPU.&lt;/p&gt; &lt;p&gt;Works with llama.cpp, LM Studio, Ollama, etc.&lt;/p&gt; &lt;p&gt;If you want smaller models, I also have GPT-OSS 20B, GLM 4.7 Flash and Qwen3 8b VL uncensored:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/models/"&gt;https://huggingface.co/HauhauCS/models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As with all my releases, the goal is effectively lossless uncensoring - no dataset changes and no capability loss.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3zuuf/gptoss_120b_uncensored_aggressive_release_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T20:15:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ipgm</id>
    <title>Kyutai Releases Hibiki-Zero</title>
    <updated>2026-02-14T11:49:27+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Kyutai Releases Hibiki-Zero: A3B Parameter Simultaneous Speech-to-Speech Translation Model Using GRPO Reinforcement Learning Without Any Word-Level Aligned Data&lt;/h1&gt; &lt;p&gt;Link: &lt;a href="https://github.com/kyutai-labs/hibiki-zero"&gt;https://github.com/kyutai-labs/hibiki-zero&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ipgm/kyutai_releases_hibikizero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ipgm/kyutai_releases_hibikizero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ipgm/kyutai_releases_hibikizero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4pnsv</id>
    <title>MiniMax M2.5 has been very patient with my dumb ass</title>
    <updated>2026-02-14T16:57:02+00:00</updated>
    <author>
      <name>/u/dengar69</name>
      <uri>https://old.reddit.com/user/dengar69</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4pnsv/minimax_m25_has_been_very_patient_with_my_dumb_ass/"&gt; &lt;img alt="MiniMax M2.5 has been very patient with my dumb ass" src="https://preview.redd.it/ofxvod0fqhjg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=4739c4a9bca710ce6eee98008612eeed4376819c" title="MiniMax M2.5 has been very patient with my dumb ass" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept trying to make a change to a simple HTML file but forgot I was in plan mode lol.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ofxvod0fqhjg1.png?width=991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e45f65af3a65d10ba9e46466de20083fd298bfe"&gt;https://preview.redd.it/ofxvod0fqhjg1.png?width=991&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4e45f65af3a65d10ba9e46466de20083fd298bfe&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dengar69"&gt; /u/dengar69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4pnsv/minimax_m25_has_been_very_patient_with_my_dumb_ass/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4pnsv/minimax_m25_has_been_very_patient_with_my_dumb_ass/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4pnsv/minimax_m25_has_been_very_patient_with_my_dumb_ass/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T16:57:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4m3uw</id>
    <title>MiniMax M2.5 - 4-Bit GGUF Options</title>
    <updated>2026-02-14T14:33:03+00:00</updated>
    <author>
      <name>/u/Responsible_Fig_1271</name>
      <uri>https://old.reddit.com/user/Responsible_Fig_1271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Currently looking at M2.5 available GGUF quants in the 4-bit range (for a 128 GB RAM + 16 GB VRAM system using CUDA) and I'm somewhat bewildered at the quant options availble today.&lt;/p&gt; &lt;p&gt;What is the best quant among these options in your experience, localllama-peeps?&lt;/p&gt; &lt;p&gt;Ubergarm Quants (&lt;a href="https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF):"&gt;https://huggingface.co/ubergarm/MiniMax-M2.5-GGUF):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;mainline-IQ4_NL&lt;/p&gt; &lt;p&gt;IQ4_NL&lt;/p&gt; &lt;p&gt;IQ4_XS&lt;/p&gt; &lt;p&gt;Unsloth Quants (&lt;a href="https://huggingface.co/unsloth/MiniMax-M2.5-GGUF):"&gt;https://huggingface.co/unsloth/MiniMax-M2.5-GGUF):&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MXFP4_MOE&lt;/p&gt; &lt;p&gt;UD-Q4_K_XL&lt;/p&gt; &lt;p&gt;I know that both Unsloth and Ubergarm produce excellent high quality quants on a consistent basis. I'm agnostic as to whether to use llama.cpp or ik_llama.cpp. And I know there are slight tradeoffs for each quant type.&lt;/p&gt; &lt;p&gt;In your experience, either via a vibe check or more rigorous coding or agentic task testing, which of the above quants would perform best on my platform?&lt;/p&gt; &lt;p&gt;Thanks fam!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible_Fig_1271"&gt; /u/Responsible_Fig_1271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4m3uw/minimax_m25_4bit_gguf_options/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4m3uw/minimax_m25_4bit_gguf_options/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4m3uw/minimax_m25_4bit_gguf_options/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:33:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ikop</id>
    <title>15% faster generation - by simply minimizing the webbrowser</title>
    <updated>2026-02-14T11:42:02+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I did some testing with llama.cpp and its web UI. While having the Windows task manager open I noticed that 3D usage was between 0% and 1% while idle, and maybe around 25% during inference.&lt;/p&gt; &lt;p&gt;Well, that &lt;em&gt;might&lt;/em&gt; have been the llama-server, but no: It's the updates of the web UI. The moment I minimized the browser the 3D usage went back to 0% to 1% during inference. The real-time streaming UI updates apparently put some strain on the GPU otherwise. I get 15% more TPS during generation when I minimize the webbrowser directly after starting a request.&lt;/p&gt; &lt;p&gt;There are a few other web-based applications on Windows that can also cause some GPU load - they're easy to identify in the GPU column of the details of the task manager. Anyway, maybe simply reducing the update frequency of the llama.cpp web UI will fully mitigate that impact.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ikop/15_faster_generation_by_simply_minimizing_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ikop/15_faster_generation_by_simply_minimizing_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ikop/15_faster_generation_by_simply_minimizing_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4ie8z</id>
    <title>I tested 21 small LLMs on tool-calling judgment ‚Äî Round 2 with every model you asked for</title>
    <updated>2026-02-14T11:31:45+00:00</updated>
    <author>
      <name>/u/MikeNonect</name>
      <uri>https://old.reddit.com/user/MikeNonect</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A week ago, I posted the Round 1 results: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qyg10z/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qyg10z/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That benchmark tested 11 small models on whether they know &lt;em&gt;when&lt;/em&gt; to call a tool, not just whether they can.&lt;/p&gt; &lt;p&gt;The post got some attention, and many of you asked to include specific models.&lt;/p&gt; &lt;p&gt;So I tested (almost) all of them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Round 2: 10 new models, 21 total, 756 inference calls on CPU.&lt;/strong&gt;&lt;br /&gt; Same 12 prompts, same scoring, same Framework 13 laptop, no GPU.&lt;/p&gt; &lt;h1&gt;The results&lt;/h1&gt; &lt;p&gt;Four models tie for #1 at &lt;strong&gt;0.880 Agent Score&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lfm2.5:1.2b&lt;/li&gt; &lt;li&gt;qwen3:0.6b&lt;/li&gt; &lt;li&gt;qwen3:4b&lt;/li&gt; &lt;li&gt;phi4-mini:3.8b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The biggest surprise was &lt;strong&gt;lfm2.5:1.2b&lt;/strong&gt; ‚Äî a 1.2B state-space hybrid ‚Äî tying for #1 with the fastest latency in the top tier (~1.5s).&lt;/p&gt; &lt;p&gt;It originally scored 0.640 because it outputs bracket notation:&lt;/p&gt; &lt;p&gt;[get_weather(city=&amp;quot;Antwerp&amp;quot;)]&lt;/p&gt; &lt;p&gt;instead of XML tool tags. After fixing the parser, it turned out the model had been making correct decisions all along.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen3:0.6b (600M parameters)&lt;/strong&gt; also ties for #1.&lt;/p&gt; &lt;p&gt;The Qwen3 family ranking is non-monotonic:&lt;/p&gt; &lt;p&gt;0.6B &amp;gt; 4B &amp;gt; 1.7B&lt;/p&gt; &lt;p&gt;The 1.7B sits in a capability valley ‚Äî aggressive enough to call tools, but not careful enough to know when not to.&lt;/p&gt; &lt;h1&gt;Score table&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Action&lt;/th&gt; &lt;th align="left"&gt;Restraint&lt;/th&gt; &lt;th align="left"&gt;Wrong Tool&lt;/th&gt; &lt;th align="left"&gt;Agent Score&lt;/th&gt; &lt;th align="left"&gt;Avg ms&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;lfm2.5:1.2b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;1470&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;phi4-mini:3.8b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;5460&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;qwen3:0.6b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;3645&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;qwen3:4b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.880&lt;/td&gt; &lt;td align="left"&gt;63717&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;qwen2.5:1.5b&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.840&lt;/td&gt; &lt;td align="left"&gt;2211&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;bitnet-2B-4T&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.810&lt;/td&gt; &lt;td align="left"&gt;2036&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;ministral-3:3b&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;7157&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;smollm2:1.7b&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.740&lt;/td&gt; &lt;td align="left"&gt;1626&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;deepseek-r1:1.5b&lt;/td&gt; &lt;td align="left"&gt;0.300&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.720&lt;/td&gt; &lt;td align="left"&gt;1672&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;smollm3:3b&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.710&lt;/td&gt; &lt;td align="left"&gt;12096&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;qwen2.5:3b&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;td align="left"&gt;2801&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;qwen3:1.7b&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;td align="left"&gt;11903&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;granite4:3b&lt;/td&gt; &lt;td align="left"&gt;0.800&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.670&lt;/td&gt; &lt;td align="left"&gt;2402&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;llama3.2:3b&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.660&lt;/td&gt; &lt;td align="left"&gt;1726&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;qwen2.5:0.5b&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;0.640&lt;/td&gt; &lt;td align="left"&gt;881&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;functiongemma&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;0.640&lt;/td&gt; &lt;td align="left"&gt;476&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;bitnet-3B&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;1.000&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;0.600&lt;/td&gt; &lt;td align="left"&gt;11362&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;jan-v3:4b&lt;/td&gt; &lt;td align="left"&gt;0.900&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.560&lt;/td&gt; &lt;td align="left"&gt;2335&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.550&lt;/td&gt; &lt;td align="left"&gt;2426&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;granite3.3:2b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;0.000&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;0.480&lt;/td&gt; &lt;td align="left"&gt;1650&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;0.700&lt;/td&gt; &lt;td align="left"&gt;0.500&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;0.430&lt;/td&gt; &lt;td align="left"&gt;1461&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;What I learned building the parser&lt;/h1&gt; &lt;p&gt;The most interesting (but obvious) finding wasn't about a specific model.&lt;/p&gt; &lt;p&gt;It was this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How you parse tool calls matters as much as what you test.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Five models required custom fallback parsers because they don't use standard formats:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lfm2.5 ‚Üí bracket notation&lt;/li&gt; &lt;li&gt;jan-v3 ‚Üí raw JSON&lt;/li&gt; &lt;li&gt;gemma3 ‚Üí function syntax inside tags&lt;/li&gt; &lt;li&gt;deepseek-r1 ‚Üí bare function calls&lt;/li&gt; &lt;li&gt;smollm3 ‚Üí sometimes omits tags entirely&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here‚Äôs the twist:&lt;/p&gt; &lt;p&gt;Fixing the parser doesn't always &lt;em&gt;help&lt;/em&gt; a model.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lfm2.5: 0.640 ‚Üí 0.880 (it was right all along)&lt;/li&gt; &lt;li&gt;gemma3: 0.600 ‚Üí 0.550 (parser blindness was hiding bad behavior)&lt;/li&gt; &lt;li&gt;smollm3: 0.740 ‚Üí 0.710&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Format-blind benchmarks don't just underestimate models.&lt;br /&gt; They can &lt;strong&gt;overestimate them too&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Your requested models&lt;/h1&gt; &lt;p&gt;Quick replies to the Round 1 commenters:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3 family&lt;/strong&gt; ‚Äî all tested&lt;br /&gt; 0.6B ties #1, 4B matches but ~17√ó slower, 1.7B weakest (0.670).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LFM 2.5:1.2B&lt;/strong&gt; ‚Äî ties #1. Needed a bracket parser to reveal its true score.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;FunctionGemma (270M)&lt;/strong&gt; ‚Äî fastest model (476 ms). Perfect restraint but falls for keyword traps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Jan v3:4B&lt;/strong&gt; ‚Äî Action 0.900 but zero restraint. Calls a tool on literally everything. Score: 0.560.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Granite4:3B&lt;/strong&gt; ‚Äî clear improvement over Granite3.3:2B (0.480 ‚Üí 0.670).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SmolLM3:3B&lt;/strong&gt; ‚Äî reasoning traces often correct, execution sometimes fails.&lt;/p&gt; &lt;p&gt;DeepBrainz-R1-2B GGUF outputs were corrupted. Couldn‚Äôt benchmark.&lt;br /&gt; Gemma 3n (5.6GB) and 15B models were outside the ‚Äúsmall model‚Äù scope.&lt;/p&gt; &lt;h1&gt;What each model called on every prompt&lt;/h1&gt; &lt;p&gt;Legend:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;W = get_weather, S = search_files, M = schedule_meeting, ‚Äî = no tool call&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bold&lt;/strong&gt; = correct on hard prompt&lt;/li&gt; &lt;li&gt;&lt;del&gt;Strikethrough&lt;/del&gt; = wrong tool or restraint failure&lt;/li&gt; &lt;li&gt;P5 and P9 should be &lt;strong&gt;‚Äî&lt;/strong&gt; (restraint). P10‚ÄìP12 are judgment traps.&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;P1&lt;/th&gt; &lt;th align="left"&gt;P2&lt;/th&gt; &lt;th align="left"&gt;P3&lt;/th&gt; &lt;th align="left"&gt;P4&lt;/th&gt; &lt;th align="left"&gt;P5&lt;/th&gt; &lt;th align="left"&gt;P6&lt;/th&gt; &lt;th align="left"&gt;P7&lt;/th&gt; &lt;th align="left"&gt;P8&lt;/th&gt; &lt;th align="left"&gt;P9&lt;/th&gt; &lt;th align="left"&gt;P10&lt;/th&gt; &lt;th align="left"&gt;P11&lt;/th&gt; &lt;th align="left"&gt;P12&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;em&gt;Expected&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;S&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W?&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;‚Äî&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;S&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;‚Äî&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;W&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;S&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;em&gt;M&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi4-mini:3.8b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:0.6b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:4b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;lfm2.5:1.2b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:1.5b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bitnet-2B-4T&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;ava&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ministral-3:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;smollm2:1.7b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek-r1:1.5b&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;smollm3:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:1.7b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite4:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:3b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;M&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5:0.5b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;functiongemma&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bitnet-3B&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;jan-v3:4b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;S&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;S&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite3.3:2b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;W&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:1b&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;S&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;W&lt;/td&gt; &lt;td align="left"&gt;M&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;‚Äî&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;M&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;del&gt;W&lt;/del&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;You can really see the patterns here. The top models (phi4-mini, qwen3, lfm2.5) have clean columns ‚Äî no strikethrough.&lt;/p&gt; &lt;p&gt;The bottom models (llama3.2:1b, granite3.3:2b) are littered with wrong calls.&lt;/p&gt; &lt;p&gt;P12 is a sea of &lt;del&gt;W&lt;/del&gt; ‚Äî almost everyone calls get_weather even though the weather is already in the prompt.&lt;/p&gt; &lt;h1&gt;Key takeaways&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Local tool-calling agents work on commodity hardware.&lt;/strong&gt; Four models hit 0.880 on CPU in ~1.5 seconds.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameter count is a weak predictor.&lt;/strong&gt; A 600M model ties a 3.8B model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conservative behavior wins.&lt;/strong&gt; Top models succeed by &lt;em&gt;not&lt;/em&gt; acting on uncertain prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt P12 is hardest:&lt;/strong&gt; ‚ÄúThe weather is 8¬∞C and rainy. Should I schedule a meeting?‚Äù Only 3/21 models get it right.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test your parser, not just your prompts.&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full report, code, and raw data: &lt;a href="https://github.com/MikeVeerman/tool-calling-benchmark"&gt;https://github.com/MikeVeerman/tool-calling-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or test more models if people want a Round 3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeNonect"&gt; /u/MikeNonect &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ie8z/i_tested_21_small_llms_on_toolcalling_judgment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ie8z/i_tested_21_small_llms_on_toolcalling_judgment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4ie8z/i_tested_21_small_llms_on_toolcalling_judgment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4lckh</id>
    <title>Add Nemotron Nano 12B v2 VL support</title>
    <updated>2026-02-14T14:00:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lckh/add_nemotron_nano_12b_v2_vl_support/"&gt; &lt;img alt="Add Nemotron Nano 12B v2 VL support" src="https://external-preview.redd.it/StSN19KOnNxJYarC__YpFdY_me-YsdlK6PPe096EmjM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f17584254a963d2edacbc38feb25d36dbfd5d094" title="Add Nemotron Nano 12B v2 VL support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA Nemotron Nano v2 12B VL model enables multi-image reasoning and video understanding, along with strong document intelligence, visual Q&amp;amp;A and summarization capabilities.&lt;/p&gt; &lt;p&gt;This model is ready for commercial use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19547"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lckh/add_nemotron_nano_12b_v2_vl_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lckh/add_nemotron_nano_12b_v2_vl_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4mks7</id>
    <title>6-GPU local LLM workstation (‚âà200GB+ VRAM) ‚Äì looking for scaling / orchestration advice</title>
    <updated>2026-02-14T14:53:06+00:00</updated>
    <author>
      <name>/u/shiftyleprechaun</name>
      <uri>https://old.reddit.com/user/shiftyleprechaun</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/"&gt; &lt;img alt="6-GPU local LLM workstation (‚âà200GB+ VRAM) ‚Äì looking for scaling / orchestration advice" src="https://preview.redd.it/blrzr76h4hjg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=bfd384e1306c8a061f9957d6b40c7914ce182d4f" title="6-GPU local LLM workstation (‚âà200GB+ VRAM) ‚Äì looking for scaling / orchestration advice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am newer to building high-end hardware but have been researching local LLM infrastructure for about a year.&lt;/p&gt; &lt;p&gt;Last night was the first time I had all six GPUs running three open-source reasoning models concurrently without stability issues.&lt;/p&gt; &lt;p&gt;Current setup (high level):&lt;/p&gt; &lt;p&gt;Threadripper PRO platform&lt;/p&gt; &lt;p&gt;256GB ECC RAM&lt;/p&gt; &lt;p&gt;~200GB+ aggregate VRAM across 6 GPUs (mix of 24GB + higher VRAM cards)&lt;/p&gt; &lt;p&gt;Dual PSU&lt;/p&gt; &lt;p&gt;Open-air rack&lt;/p&gt; &lt;p&gt;Ubuntu 24.04&lt;/p&gt; &lt;p&gt;Gen4 + Gen5 NVMe&lt;/p&gt; &lt;p&gt;Primary use case is running larger reasoning models locally for internal data analysis + workflow automation&lt;/p&gt; &lt;p&gt;Currently experimenting with multi-model concurrency and different GPU assignment strategies.&lt;/p&gt; &lt;p&gt;I would really appreciate feedback from people running similar multi-GPU rigs:&lt;/p&gt; &lt;p&gt;At this scale, what typically becomes the first real bottleneck for local LLM inference VRAM, PCIe bandwidth, CPU orchestration, memory bandwidth, something else?&lt;/p&gt; &lt;p&gt;Is mixing GPU types a long-term pain point, or fine as long as models are pinned deliberately?&lt;/p&gt; &lt;p&gt;For those running multiple reasoning models simultaneously, where did you start seeing diminishing returns?&lt;/p&gt; &lt;p&gt;How are people handling model scheduling across GPUs ‚Äî static pinning vs dynamic routing?&lt;/p&gt; &lt;p&gt;If you were building today, would you consolidate into fewer high-VRAM GPUs or keep a distributed multi-card setup?&lt;/p&gt; &lt;p&gt;What is one mistake people make when building larger local LLM workstations?&lt;/p&gt; &lt;p&gt;Still learning ‚Äî would rather hear what I am overlooking than what I got right, but I appreciate any comments questions or feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shiftyleprechaun"&gt; /u/shiftyleprechaun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r4mks7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4mks7/6gpu_local_llm_workstation_200gb_vram_looking_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4lg46</id>
    <title>We need to bring back the "experimental" era of LLMs</title>
    <updated>2026-02-14T14:04:45+00:00</updated>
    <author>
      <name>/u/TemperatureMajor5083</name>
      <uri>https://old.reddit.com/user/TemperatureMajor5083</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you remember projects like &lt;a href="https://en.wikipedia.org/wiki/GPT-4Chan"&gt;GPT-4chan&lt;/a&gt;? Back then, training on more &amp;quot;unconventional&amp;quot; data sources was far more common than it is today, where most models tend to converge on the same polished, &amp;quot;helpful assistant&amp;quot; persona. It‚Äôs interesting to think about what we could build with today‚Äôs high-performance base models if they were fine-tuned on more distinctive, niche datasets. Done well, that could be genuinely entertaining.&lt;/p&gt; &lt;p&gt;The recently posted MechaEpstein kind of goes in that direction, but I think there‚Äôs room to be more creative than just having it reply with &amp;quot;&amp;lt;thing&amp;gt; are goy. Sorry for the typos. Sent from my iPhone.&amp;quot; to every message.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TemperatureMajor5083"&gt; /u/TemperatureMajor5083 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lg46/we_need_to_bring_back_the_experimental_era_of_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:04:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r44fzk</id>
    <title>The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'</title>
    <updated>2026-02-13T23:20:10+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"&gt; &lt;img alt="The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'" src="https://preview.redd.it/4rozb901icjg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0af0fe460ed577cfa1d0490e0386a39aa78b986f" title="The gap between open-weight and proprietary model intelligence is as small as it has ever been, with Claude Opus 4.6 and GLM-5'" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4rozb901icjg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r44fzk/the_gap_between_openweight_and_proprietary_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T23:20:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4nt7u</id>
    <title>Qwen3-TTS.cpp</title>
    <updated>2026-02-14T15:43:12+00:00</updated>
    <author>
      <name>/u/redditgivingmeshit</name>
      <uri>https://old.reddit.com/user/redditgivingmeshit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/"&gt; &lt;img alt="Qwen3-TTS.cpp" src="https://external-preview.redd.it/lXcar2l04LaOhzsDCTxgOTofdAoFx7BoAnOuaZ4jNcw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f7e414f528eab2e463ea468be5c2271e564d443" title="Qwen3-TTS.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Lightweight GGML implementation of Qwen3-TTS 0.6B&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4x Speedup compared to pytorch pipeline, with ~2 Gigs of Memory usage.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Hi, this was something I've been working on for the last few days. The result actually performed better than expected, so I'm sharing it here.&lt;/p&gt; &lt;p&gt;The pipeline was optimized with Metal backend support &amp;amp; CoreML code predictor. The other parts contained operations that were not able to be loaded into the ANE, so only the code predictor was converted.&lt;/p&gt; &lt;p&gt;No quantization support yet, but coming soon. Turns out using Q8 for the entire pipeline produces bad results. I'm still figuring out which parts are sensitive to quantization and which parts are okay.&lt;/p&gt; &lt;p&gt;Supports all features, including voice cloning&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redditgivingmeshit"&gt; /u/redditgivingmeshit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/predict-woo/qwen3-tts.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4nt7u/qwen3ttscpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T15:43:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4lx7x</id>
    <title>Nemotron3 Super/Ultra: FP4 pre-training, H1 2026 release, "NVIDIA is a company of volunteers" (all from recent NVIDIA interview)</title>
    <updated>2026-02-14T14:25:15+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nathan Lambert (from Ai2) interviewed an NVIDIA's VP of Applied Deep Learning Research: &lt;a href="https://www.interconnects.ai/p/why-nvidia-builds-open-models-with"&gt;Why Nvidia builds open models with Bryan Catanzaro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Many interesting bits, but of course I was hoping for hints of when the next Nemotron3 models were to be released. Nothing really new there, &amp;quot;2026 H1&amp;quot; is a pretty broad window. &lt;/p&gt; &lt;p&gt;This was interesting: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;we‚Äôre pre-training our Nemotron-3 Super and Ultra models using FP4 which is a thing that, you know, hasn‚Äôt been done publicly anyway and something that, you know, we‚Äôre pretty excited about because our GPUs have really awesome FP4 throughput. But obviously, the numerical challenges of, like, trying to train a state-of-the-art language model using four bits is non-trivial. ...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hopefully those will be highly performant at Q4 quants. &lt;/p&gt; &lt;p&gt;Many other interesting things in the interview, such as motivations for creating open source models. Nathan asks this of various open-source guests, &amp;quot;what is your business reason&amp;quot; -- the NVIDIA VP effectively says, &amp;quot;so people will keep buying NVIDIA GPUs.&amp;quot; (Do they really need local models to bolster their business? Do they see a lot more businesses running local models, on-prem or in the cloud?)&lt;/p&gt; &lt;p&gt;Another interesting thing: more than once the VP said that &amp;quot;NVIDIA is a company of volunteers&amp;quot; -- if you ctrl+f for &amp;quot;volunteers&amp;quot; in the transcript you will see it repeatedly. &lt;/p&gt; &lt;p&gt;The context is &amp;quot;how do you manage and coordinate people to work on Nemotron,&amp;quot; but the wording still caught me off-guard -- &amp;quot;Hey I want to volunteer there...&amp;quot;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;00:22:25 Nathan Lambert: ...Do you have any advice for making the orgs come together? ...&lt;/p&gt; &lt;p&gt;00:23:20 Bryan Catanzaro: You know what‚Äôs worked for us is invitation and not control. ... So you know, NVIDIA is a very decentralized company with a lot of volunteers. You know, everybody that works at NVIDIA is a volunteer. And what do I mean by that? Well, I mean, look, the industry is moving quick.&lt;/p&gt; &lt;p&gt;You know, people can always move from one job to the next. So the way that we think about the work that we do is like, it‚Äôs very decentralized, it‚Äôs very much let smart people figure out what they should be doing and then kind of self-organize. ... There‚Äôs just an enormous number of brilliant people that have decided that they‚Äôre gonna volunteer to make Nemotron awesome, and we‚Äôre, we‚Äôre starting to see some pretty great things come together.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...etc. &lt;/p&gt; &lt;p&gt;Full interview is very interesting.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4lx7x/nemotron3_superultra_fp4_pretraining_h1_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T14:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4hhyy</id>
    <title>local vibe coding</title>
    <updated>2026-02-14T10:37:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please share your experience with vibe coding using local (not cloud) models.&lt;/p&gt; &lt;p&gt;General note: to use tools correctly, some models require a modified chat template, or you may need in-progress PR.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/anomalyco/opencode"&gt;https://github.com/anomalyco/opencode&lt;/a&gt; - probably the most mature and feature complete solution. I use it similarly to Claude Code and Codex.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/mistralai/mistral-vibe"&gt;https://github.com/mistralai/mistral-vibe&lt;/a&gt; - a nice new project, similar to opencode, but simpler.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/RooCodeInc/Roo-Code"&gt;https://github.com/RooCodeInc/Roo-Code&lt;/a&gt; - integrates with Visual Studio Code (not CLI).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/Aider-AI/aider"&gt;https://github.com/Aider-AI/aider&lt;/a&gt; - a CLI tool, but it feels different from opencode (at least in my experience).&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.continue.dev/"&gt;https://docs.continue.dev/&lt;/a&gt; - I tried it last year as a Visual Studio Code plugin, but I never managed to get the CLI working with llama.cpp.&lt;/li&gt; &lt;li&gt;Cline - I was able to use it as Visual Studio Code plugin &lt;/li&gt; &lt;li&gt;Kilo Code - I was able to use it as Visual Studio Code plugin &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hhyy/local_vibe_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T10:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4hx24</id>
    <title>models : optimizing qwen3next graph by ggerganov ¬∑ Pull Request #19375 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-02-14T11:03:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"&gt; &lt;img alt="models : optimizing qwen3next graph by ggerganov ¬∑ Pull Request #19375 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/VVE5ljhhmuVj3S3mZp6__yfxBNtIYwLxEpi1hRwGGjU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95716ff4da82d5b8eaae30528e020e4dd291b4ab" title="models : optimizing qwen3next graph by ggerganov ¬∑ Pull Request #19375 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Faster (t/s) Qwen Next models.&lt;/p&gt; &lt;p&gt;There are still some in-progress PRs to fix/improve Qwen Next in llama.cpp. Let's hope this model will be awesome soon :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19375"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4hx24/models_optimizing_qwen3next_graph_by_ggerganov/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T11:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1r4n3as</id>
    <title>Heretic 1.2 released: 70% lower VRAM usage with quantization, Magnitude-Preserving Orthogonal Ablation ("derestriction"), broad VL model support, session resumption, and more</title>
    <updated>2026-02-14T15:14:00+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llamas and Gentlemen,&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Heretic&lt;/strong&gt; (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;) is the leading software for removing censorship from language models. In the three months since its initial release, &lt;a href="https://huggingface.co/models?other=heretic"&gt;more than 1,300 models&lt;/a&gt; (including quants) made using Heretic have been published by the community. This represents more than a third of all abliterated models ever published, and the vast majority of abliterated models published since Heretic's first release.&lt;/p&gt; &lt;p&gt;Today, I am happy to announce the release of Heretic 1.2, the product of two months of hard work by the Heretic contributors.&lt;/p&gt; &lt;p&gt;The headline feature is the new LoRA-based abliteration engine implemented by accemlcc. Built on top of PEFT, it supports loading models with 4-bit quantization using bitsandbytes, which can reduce VRAM requirements for processing a model by up to 70%. The abliterated model is still exported in full precision, which is achieved by re-loading the original model in system RAM and applying the optimized LoRA adapter on top of it, yielding a high-quality model despite the low resource requirements. To enable quantized loading, set &lt;code&gt;quantization&lt;/code&gt; to &lt;code&gt;bnb_4bit&lt;/code&gt; in the configuration.&lt;/p&gt; &lt;p&gt;spikymoth implemented Magnitude-Preserving Orthogonal Ablation (MPOA) aka Norm-Preserving Biprojected Abliteration aka &amp;quot;derestriction&amp;quot;, a refined abliteration technique developed by Jim Lai which can improve the quality of the resulting model in many cases. This has been one of the most frequently requested features from the community, and is now finally available. To enable MPOA, set &lt;code&gt;orthogonalize_direction&lt;/code&gt; to &lt;code&gt;true&lt;/code&gt; and &lt;code&gt;row_normalization&lt;/code&gt; to &lt;code&gt;full&lt;/code&gt; in the configuration.&lt;/p&gt; &lt;p&gt;Heretic's implementation of MPOA uses Optuna to optimize weight parameters. This can result in models that are better than those generated with the original MPOA technique, which employs a different strategy for layer selection. For example, &lt;code&gt;MuXodious/gpt-oss-20b-RichardErkhov-heresy&lt;/code&gt; dominates &lt;code&gt;ArliAI/gpt-oss-20b-Derestricted&lt;/code&gt; on the UGI Leaderboard, scoring 39.05 vs 34.22 and beating the derestricted model in every individual test (W/10, NatInt, and Writing).&lt;/p&gt; &lt;p&gt;After a long history of hacks being passed around in the community, anrp finally found a clean way to support vision language models in Heretic, and a broad range of VL models can now be processed. Note that only the language model part (the text decoder transformer) is abliterated, not the image encoder.&lt;/p&gt; &lt;p&gt;anrp also implemented fully automatic session progress saving and resumption. This means worrying about crashes during a long optimization run is now a thing of the past, as you can simply restart Heretic and it will offer to continue where it left off. You can also interrupt the run yourself at any time with Ctrl+C, and resume it later.&lt;/p&gt; &lt;p&gt;Please see the release notes for the full list of improvements and fixes. More exciting stuff is coming in future versions!&lt;/p&gt; &lt;p&gt;Cheers :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r4n3as/heretic_12_released_70_lower_vram_usage_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-14T15:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1r3t775</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2026-02-13T16:07:54+00:00</updated>
    <author>
      <name>/u/HardToVary</name>
      <uri>https://old.reddit.com/user/HardToVary</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt; &lt;img alt="AMA with MiniMax ‚Äî Ask Us Anything!" src="https://preview.redd.it/5z2li1ntcajg1.jpg?width=140&amp;amp;height=59&amp;amp;auto=webp&amp;amp;s=ce3340cd37b7e9408878509be00dd7b871efebde" title="AMA with MiniMax ‚Äî Ask Us Anything!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;We're &lt;strong&gt;MiniMax&lt;/strong&gt;, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;.5&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining the channel today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî Founder of MiniMax&lt;/li&gt; &lt;li&gt;&lt;a href="/u/Wise_Evidence9973"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ryan85127704"&gt;u/ryan85127704&lt;/a&gt; ‚Äî Head of Engineering&lt;/li&gt; &lt;li&gt;&lt;a href="/u/HardToVary"&gt;u/HardToVary&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c"&gt;https://preview.redd.it/5z2li1ntcajg1.jpg?width=3525&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e6760feae05c7cfcaea6d95dfcd6e15990ec7f5c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;P.S. We'll continue monitoring and responding to questions for 48 hours after the end of the AMA.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HardToVary"&gt; /u/HardToVary &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r3t775/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-13T16:07:54+00:00</published>
  </entry>
</feed>
