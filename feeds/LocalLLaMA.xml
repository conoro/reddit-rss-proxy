<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-09T00:53:27+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nbmhi4</id>
    <title>Episodic Memory Bank and local voice to voice using Cline.</title>
    <updated>2025-09-08T12:33:06+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"&gt; &lt;img alt="Episodic Memory Bank and local voice to voice using Cline." src="https://external-preview.redd.it/YXpvdGx0OGFxeG5mMamjuXx1p5ZbwAaKmxghv-BQMSuAGY3X-aSJMyvQWEDF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e89a36a762194aef2c481b8bea340ca919158822" title="Episodic Memory Bank and local voice to voice using Cline." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on a new memory bank framework called the episodic memory bank. Here I demo that in action and show off the new kokoro and Apple Intelligence powered voice to voice in Cline.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ymgdjt8aqxnf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmhi4/episodic_memory_bank_and_local_voice_to_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo7sr</id>
    <title>DGX Spark gpt-oss-120b performance ? Benchmarks ?</title>
    <updated>2025-09-08T13:48:00+00:00</updated>
    <author>
      <name>/u/one-wandering-mind</name>
      <uri>https://old.reddit.com/user/one-wandering-mind</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Could be there this board never comes out. If it does, theoretical benchmarks seem to indicate it is a good fit for big, sparse fp4 of which gpt-oss-120b is the first. Any benchmarks out to support or refute this yet? &lt;/p&gt; &lt;p&gt;I would guess we will see more models like gpt-oss-120b coming out because of how cheap they are to run on blackwell server hardware as compared with other models at similar capability levels. Any good reasons why models won't shift to sparse fp4 ? Is it significantly harder to train or fine tune a model like this? &lt;/p&gt; &lt;p&gt;Taking the safety training of this model out of it. This is just a about architecture and performance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/one-wandering-mind"&gt; /u/one-wandering-mind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo7sr/dgx_spark_gptoss120b_performance_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo7sr/dgx_spark_gptoss120b_performance_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo7sr/dgx_spark_gptoss120b_performance_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1naxl6a</id>
    <title>NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp; Priced At $13,200 Per Piece</title>
    <updated>2025-09-07T16:24:13+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt; &lt;img alt="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" src="https://external-preview.redd.it/0E4hPJjWUWQzlid17SPMiSUkbhbtEQRV_SbOMgs-kTI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e26ecaa238d5f9ab48615dfc56baa31609cbaeaa" title="NVIDIA GeForce RTX 5090 128 GB GPU Spotted: Custom Memory, Designed For AI Workloads &amp;amp; Priced At $13,200 Per Piece" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/nvidia-geforce-rtx-5090-128-gb-memory-gpu-for-ai-price-13200-usd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1naxl6a/nvidia_geforce_rtx_5090_128_gb_gpu_spotted_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-07T16:24:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbllnt</id>
    <title>Folks who are fine-tuning SLMs, where do you acquire datasets?</title>
    <updated>2025-09-08T11:51:11+00:00</updated>
    <author>
      <name>/u/CrescendollsFan</name>
      <uri>https://old.reddit.com/user/CrescendollsFan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I noticed a lot of folks interested in unsloth and fine-tuning and with a few of the colab notebooks pulling in a genetic dataset. I am just curious if anyone is replicating this approach outside of a demo / how to - where people acquire or curate datasets and then fine-tune&lt;/p&gt; &lt;p&gt;For example deepseeks distillation method was from pulling data from OpenAI models , and I heard phi4 had &lt;a href="https://arxiv.org/html/2412.08905v1"&gt;synthetics&lt;/a&gt; as a bulk of the training data . Are many people training SLMs in the same way, and where do you get or curate your own specialised data - or you find over-fitting is too much of a problem?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CrescendollsFan"&gt; /u/CrescendollsFan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbllnt/folks_who_are_finetuning_slms_where_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbq1n0</id>
    <title>Switched to LobeChat from OpenWebUI because of crappy web search and no reasoning level support: a review</title>
    <updated>2025-09-08T14:59:32+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For people who use OpenWebUI, I could not get web search working properly. Also, it's been weeks since OpenAI Harmony was released, and it still doesn't support configuring reasoning level for gpt-oss. &lt;/p&gt; &lt;p&gt;I gave up on OpenWebUI being useful, and switched to LobeChat. One &lt;code&gt;docker compose up -d&lt;/code&gt; later, it's running on my server.&lt;/p&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Native web search &lt;a href="https://i.imgur.com/7mmS43O.png"&gt;actually works&lt;/a&gt;! It calls gpt-5 api correctly using the &lt;a href="https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses"&gt;web_search tool&lt;/a&gt; built-in search engine. It's a lot faster than running searxng on my local machine, doesn't seem to run into cloudflare issues, and &lt;a href="https://i.imgur.com/zoakfFJ.png"&gt;gives high quality results quickly&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;You can conveniently &lt;a href="https://i.imgur.com/4dpL9jp.png"&gt;set gpt-oss/gpt-5 reasoning effort&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The icons are really really ugly. Not my style at all. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It's written by a team in china, and it shows sometimes in the translations. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Setting up the server with a custom domain/SSL/custom ports is a bit harder. The default config assumes that you have port 8000, 9000, 9001, and 5432 (postgres port) available. You need to tweak the config a bit if you don't want to use those ports if they are already in use. You can change some of the ports in the &lt;code&gt;.env&lt;/code&gt; file, but the port 9001 and 5432 are hardcoded so you need to change that. This is not a big deal though, just takes a few mins to configure.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall, I rate it 4/5 stars. Would be a higher score if we could change the icons.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq1n0/switched_to_lobechat_from_openwebui_because_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq1n0/switched_to_lobechat_from_openwebui_because_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq1n0/switched_to_lobechat_from_openwebui_because_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T14:59:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc3hc3</id>
    <title>Local LLM and Home Assistant</title>
    <updated>2025-09-08T23:35:30+00:00</updated>
    <author>
      <name>/u/j0ker31m</name>
      <uri>https://old.reddit.com/user/j0ker31m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been a google home user since the day they launched. However, ive also been watching home Assistant grow over the years. Im wanting to switch to HA with a local LLM, and storing all of my wireless security camera footage all running on the same box.&lt;/p&gt; &lt;p&gt;I dont know what size of an llm is considered too small, or considered overkill, so its hard for me to determine what kind of hardware im going to need. Do you guys have any suggestion on what the simplest and cheapest method would be without having to sacrifice this for that?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j0ker31m"&gt; /u/j0ker31m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc3hc3/local_llm_and_home_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc3hc3/local_llm_and_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc3hc3/local_llm_and_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T23:35:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbq3y7</id>
    <title>New research preprint: Evolving Transformers with NEMoE</title>
    <updated>2025-09-08T15:01:47+00:00</updated>
    <author>
      <name>/u/Desperate_Contact102</name>
      <uri>https://old.reddit.com/user/Desperate_Contact102</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I just uploaded a new research preprint called NEMoE (Neuro-Evolutionary Mixture of Experts Transformer).&lt;/p&gt; &lt;p&gt;Instead of using a standard Transformer with fixed experts, NEMoE applies ideas from evolutionary algorithms (mutation, crossover, selection) to improve how experts are chosen and combined.&lt;/p&gt; &lt;p&gt;üîπ Early results show:&lt;/p&gt; &lt;p&gt;Lower perplexity (better language modeling performance)&lt;/p&gt; &lt;p&gt;More stable training compared to Switch Transformer&lt;/p&gt; &lt;p&gt;Better use of experts without adding compute cost&lt;/p&gt; &lt;p&gt;Here‚Äôs the preprint (open access on Zenodo): üëâ &lt;a href="https://doi.org/10.5281/zenodo.17073715"&gt;https://doi.org/10.5281/zenodo.17073715&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate_Contact102"&gt; /u/Desperate_Contact102 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbq3y7/new_research_preprint_evolving_transformers_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:01:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqb3o</id>
    <title>Will you use this new Qwen3-ASR?</title>
    <updated>2025-09-08T15:09:21+00:00</updated>
    <author>
      <name>/u/LuozhuZhang</name>
      <uri>https://old.reddit.com/user/LuozhuZhang</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"&gt; &lt;img alt="Will you use this new Qwen3-ASR?" src="https://preview.redd.it/1ytcylkfiynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c83369e0e599a5317d04c7536881ab848342f5d" title="Will you use this new Qwen3-ASR?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Supporting 11 languages &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LuozhuZhang"&gt; /u/LuozhuZhang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1ytcylkfiynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqb3o/will_you_use_this_new_qwen3asr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:09:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc0dgg</id>
    <title>ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp</title>
    <updated>2025-09-08T21:25:34+00:00</updated>
    <author>
      <name>/u/Recent-Success-1520</name>
      <uri>https://old.reddit.com/user/Recent-Success-1520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"&gt; &lt;img alt="ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp" src="https://external-preview.redd.it/HwAJGIWkuuQRHBpEXp2R4CbrQfzKoASLgWeZFT1sFIQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=131ad86b79c662a9208b9b395f33e7b7dfcbb355" title="ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI all,&lt;/p&gt; &lt;p&gt;A few days ago I posted if anyone had any fine tuning working on Strix Halo and many people like me were looking.&lt;br /&gt; I have got a working setup now that allows me to use ROCm based fine tuining and inferencing.&lt;/p&gt; &lt;p&gt;For now the following tools are working with latest ROCm 7.0.0 nightly and available in my repo (linked). From the limited testing unsloth seems to be working and llama-cpp inference is working too.&lt;/p&gt; &lt;p&gt;This is initial setup and I will keep adding more tools all ROCm compiled.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# make help Available targets: all: Installs everything bitsandbytes: Install bitsandbytes from source flash-attn: Install flash-attn from source help: Prints all available targets install-packages: Installs required packages llama-cpp: Installs llama.cpp from source pytorch: Installs torch torchvision torchaudio pytorch-triton-rcom from ROCm nightly rocWMMA: Installs rocWMMA library from source theRock: Installs ROCm in /opt/rocm from theRock Nightly unsloth: Installs unsloth from source &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Sample bench&lt;/p&gt; &lt;p&gt;&lt;code&gt;root@a7aca9cd63bc:/strix-rocm-all# llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -ngl 999 -mmp 0 -fa 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: found 1 ROCm devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | mmap | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 0 | pp512 | 698.26 ¬± 7.31 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 0 | tg128 | 46.20 ¬± 0.47 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Got mixed up with &lt;a href="/r/LocalLLM"&gt;r/LocalLLM&lt;/a&gt; so posting here too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent-Success-1520"&gt; /u/Recent-Success-1520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shantur/strix-rocm-all"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T21:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa34</id>
    <title>üöÄ What model should we build next? YOU DECIDE! üöÄ</title>
    <updated>2025-09-08T15:08:12+00:00</updated>
    <author>
      <name>/u/GuiltyBookkeeper4849</name>
      <uri>https://old.reddit.com/user/GuiltyBookkeeper4849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA!&lt;/p&gt; &lt;p&gt;After the amazing support we received in our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n3xxm5/introducing_art08b_reasoning_the_way_you_want_it/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;last post&lt;/a&gt; with Art-0-8B, we're ready to tackle our next project and want YOU to decide what it should be! (Art-1 8B and 20B versions are coming soon btw)&lt;/p&gt; &lt;p&gt;For those who missed it, we're AGI-0 Labs - a decentralized research lab building open-source AGI through democratic community input. Our mission is simple: create AI that belongs to everyone, developed openly and guided by the community. Check us out at &lt;a href="http://AGI-0.com"&gt;AGI-0.com&lt;/a&gt; if you want to learn more about our approach.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's how this works:&lt;/strong&gt; The most upvoted comment below describing a model idea will be our next development target. Whether it's a specialized fine-tune, a novel architecture experiment, or something completely wild - if the community wants it, we'll build it.&lt;/p&gt; &lt;p&gt;We're also open to collaborating with any sponsors who'd like to help us get more compute resources - feel free to reach out if you're interested in supporting open-source AI development!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Drop your model ideas below and let's see what the community wants most! The highest upvoted suggestion gets built. üó≥Ô∏è&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Looking forward to seeing what creative ideas you all come up with!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GuiltyBookkeeper4849"&gt; /u/GuiltyBookkeeper4849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa34/what_model_should_we_build_next_you_decide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbmz92</id>
    <title>NotebookLM is amazing - how can I replicate it locally and keep data private?</title>
    <updated>2025-09-08T12:55:27+00:00</updated>
    <author>
      <name>/u/Hot-Independence-197</name>
      <uri>https://old.reddit.com/user/Hot-Independence-197</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like how &lt;strong&gt;NotebookLM&lt;/strong&gt; works - I just upload a file, ask any question, and it provides high-quality answers. How could one build a similar system locally? Would this be considered a RAG (Retrieval-Augmented Generation) pipeline, or something else? Could you recommend good &lt;strong&gt;open-source&lt;/strong&gt; versions that can be run locally, while keeping data secure and private?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Independence-197"&gt; /u/Hot-Independence-197 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbmz92/notebooklm_is_amazing_how_can_i_replicate_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:55:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc2w7h</id>
    <title>5090 vs 6000</title>
    <updated>2025-09-08T23:10:04+00:00</updated>
    <author>
      <name>/u/That-Thanks3889</name>
      <uri>https://old.reddit.com/user/That-Thanks3889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;a student asked me which rig for learning and training models - i recommended the 6000 but with new hardware every month i'm taking it back ... wondering everyone else's opinion ? 5090 seems sufficient to learn and fine tune mistral etc ... and once they proficient they can rent cloud or spend money &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That-Thanks3889"&gt; /u/That-Thanks3889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T23:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo0bz</id>
    <title>KittenML released a mini version (80M) of their text to speech model.</title>
    <updated>2025-09-08T13:39:19+00:00</updated>
    <author>
      <name>/u/Yorn2</name>
      <uri>https://old.reddit.com/user/Yorn2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt; &lt;img alt="KittenML released a mini version (80M) of their text to speech model." src="https://external-preview.redd.it/6tEU3HFyV9wrAIlbgWYDqTicViQ2PFk-H0trsfrB-TE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae1cdc19684b4f8a1d7922e2097495effc92e03" title="KittenML released a mini version (80M) of their text to speech model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yorn2"&gt; /u/Yorn2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbt82m</id>
    <title>Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher.</title>
    <updated>2025-09-08T16:58:12+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"&gt; &lt;img alt="Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher." src="https://external-preview.redd.it/G0pBb0RCd46QXI7ZBwlDv7ScyXXHaae0jNeNWtdfkbk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6496fcee3c3c0005184df9b3cbe9dceaa06a0c4a" title="Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also updated my FAQ. Preparing a release on a Largestral 2407 and Small 22B tune too! (If anyone's interested, they're a bit smarter with the 'modern' tuning.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Valkyrie-49B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T16:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbslxu</id>
    <title>native tool calling support for DeepSeek V3.1 just merged in llama.cpp</title>
    <updated>2025-09-08T16:35:22+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"&gt; &lt;img alt="native tool calling support for DeepSeek V3.1 just merged in llama.cpp" src="https://external-preview.redd.it/AaDrVOkhJbB5T7DYbjublcje7S3TXjWZXxoeBvGkbYY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bdc3f7c6f3e61642c4575d3c207d062eb6dd4b4" title="native tool calling support for DeepSeek V3.1 just merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I doubt many people are using it, but just FYI: native tool calling support (OpenAI style JSON request/response) for DeepSeek V3.1 was just merged into llama.cpp. To use, I think you have to start the server with `--jinja` and unset `--response_format`, or set it to `auto`. I personally use this feature quite a bit with Open Hands AI via docker with `-e LLM_NATIVE_TOOL_CALLING=true`, but you'll have to check your documentation to see if it is supported and how to enable it if you use a different client. Benefits include reduced context length and possibly better agentic reliability (time will tell).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15533"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T16:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbi95c</id>
    <title>Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages</title>
    <updated>2025-09-08T08:32:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt; &lt;img alt="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" src="https://external-preview.redd.it/aoPAPmODv59RqOF8q1zUghKheD5cO88KxVLhosHPVZE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56ba07b04f6a26463fb99f2d29054bf135f506a" title="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TildeOpen LLM is an open-source foundational language model built to serve underrepresented Nordic and Eastern European languages. Developed with European Commission funding and trained on the LUMI supercomputer, this 30B+ parameter model addresses the performance gaps that speakers of 19 focus languages‚Äîrepresenting over 165 million people‚Äîface with existing AI systems.&lt;/p&gt; &lt;p&gt;The model employs an equitable tokeniser and curriculum-learning approach to ensure fair representation across less-resourced languages, moving beyond the typical English-centric design of most language models. As an open-source project, TildeOpen LLM enables transparent research and community-driven development while maintaining European technological independence.&lt;/p&gt; &lt;p&gt;This foundational model is not yet adapted to follow instructions or aligned with safety features. The next version being built on top of this model will be a specialised translation model, leveraging TildeOpen LLM's multilingual foundation to provide high-quality translation capabilities across the supported European language pairs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt; Albanian, Bosnian, Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hungarian, Icelandic, Irish, Italian, Latgalian, Latvian, Lithuanian, Macedonian, Maltese, Montenegrin, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish, Turkish, Ukrainian as well of mathematical proofs, programming code and XML documents containing translation data&lt;/p&gt; &lt;p&gt;GGUF:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/TildeOpen-30b-GGUF"&gt;https://huggingface.co/mradermacher/TildeOpen-30b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TildeAI/TildeOpen-30b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T08:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbly7o</id>
    <title>MiniCPM4.1-8B</title>
    <updated>2025-09-08T12:08:09+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8B hybrid reasoning model (/think vs /no_think)&lt;/li&gt; &lt;li&gt;InfLLM v2 sparse attention, natively supports 65K, RoPE scaling validated to 131K&lt;/li&gt; &lt;li&gt;BitCPM ternary quantization, FP8 and multi-token prediction&lt;/li&gt; &lt;li&gt;Eagle3 speculative decoding integrated in vLLM, SGLang, and CPM .cu with up to 3x faster reasoning&lt;/li&gt; &lt;li&gt;On Jetson Orin achieves approximately 7x faster decoding compared to Qwen3-8B and 3x reasoning speedup over MiniCPM4&lt;/li&gt; &lt;li&gt;Available in GPTQ, AutoAWQ, Marlin, GGUF, MLX, and Eagle3 draft variants&lt;/li&gt; &lt;li&gt;Apache 2.0&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfy60</id>
    <title>Finishing touches on dual RTX 6000 build</title>
    <updated>2025-09-08T06:04:43+00:00</updated>
    <author>
      <name>/u/ikkiyikki</name>
      <uri>https://old.reddit.com/user/ikkiyikki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt; &lt;img alt="Finishing touches on dual RTX 6000 build" src="https://preview.redd.it/sez83piasvnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599c86ee050439c49f07c7056ff67e6c48ef8381" title="Finishing touches on dual RTX 6000 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a dream build: 192 gigs of fast VRAM (and another 128 of RAM) but worried I'll burn the house down because of the 15A breakers.&lt;/p&gt; &lt;p&gt;Downloading Qwen 235B q4 :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikkiyikki"&gt; /u/ikkiyikki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sez83piasvnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgosx</id>
    <title>Apocalyptic scenario: If you could download only one LLM before the internet goes down, which one would it be?</title>
    <updated>2025-09-08T06:50:43+00:00</updated>
    <author>
      <name>/u/sado361</name>
      <uri>https://old.reddit.com/user/sado361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, a thought crossed my mind and I've been thinking about it for a few days. Let's say we have an apocalyptic scenario, like a zombie apocalypse. You have a Mac Studio with an M3 chip and 512 GB of RAM (it uses little power and can run large models). If such an apocalypse happened today, which local LLM would you download before the internet disappears? You only have a chance to download one. Electricity is not a problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sado361"&gt; /u/sado361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbyw3b</id>
    <title>3090 is it still a good buy?</title>
    <updated>2025-09-08T20:28:10+00:00</updated>
    <author>
      <name>/u/Ideabile</name>
      <uri>https://old.reddit.com/user/Ideabile</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got the opportunity to buy 2 Nvidia 3090 RTX 24GB for 600‚Ç¨ each.&lt;/p&gt; &lt;p&gt;I want to be run a bunch of llm workflows: this to self host some Claude code and to automate some burocracies I got.&lt;/p&gt; &lt;p&gt;Additionally I want to step up in the llm experimental path, so I can learn more about it and have the ML skill set.&lt;/p&gt; &lt;p&gt;Currently other video cards seems much more expensive I hardly believe it will ever get cheaper.&lt;/p&gt; &lt;p&gt;I saw some people recommending 2 x 3090 which would make 48gb of vram.&lt;/p&gt; &lt;p&gt;Is there any other budget friendly alternatives? Is this a good lasting investment?&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ideabile"&gt; /u/Ideabile &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T20:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbkxnm</id>
    <title>Introducing IndexTTS-2.0: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
    <updated>2025-09-08T11:16:22+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce the official open-sourcing of IndexTTS-2.0 - an emotionally rich and duration-controllable autoregressive zero-shot text-to-speech system. &lt;/p&gt; &lt;p&gt;- We innovatively propose a &amp;quot;time encoding&amp;quot; mechanism applicable to autoregressive systems, solving for the first time the challenge of precise speech duration control in traditional autoregressive models. &lt;/p&gt; &lt;p&gt;- The system also introduces a timbre-emotion decoupling modeling mechanism, offering diverse and flexible emotional control methods. Beyond single-audio reference, it enables precise adjustment of synthesized speech's emotional expression through standalone emotional reference audio, emotion vectors, or text descriptions, significantly enhancing the expressiveness and adaptability of generated speech. &lt;/p&gt; &lt;p&gt;The architecture of IndexTTS-2.0 makes it widely suitable for various creative and application scenarios, including but not limited to: AI voiceovers, audiobooks, dynamic comics, video translation, voice dialogues, podcasts, and more. We believe this system marks a crucial milestone in advancing zero-shot TTS technology toward practical applications. &lt;/p&gt; &lt;p&gt;Currently, the project paper, full code, model weights, and online demo page are all open-sourced. We warmly invite developers, researchers, and content creators to explore and provide valuable feedback. In the future, we will continue optimizing model performance and gradually release more resources and tools, looking forward to collaborating with the developer community to build an open and thriving technology ecosystem. &lt;/p&gt; &lt;p&gt;üëâ Repository: &lt;a href="https://github.com/index-tts/index-tts"&gt;https://github.com/index-tts/index-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üëâ Paper: &lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üëâ Demo: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa1p</id>
    <title>Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!</title>
    <updated>2025-09-08T15:08:10+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt; &lt;img alt="Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!" src="https://preview.redd.it/et1syg58iynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ab48cb0e9692e8d94764a7f031fd34d0db1ae95" title="Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üéôÔ∏è Meet Qwen3-ASR ‚Äî the all-in-one speech recognition model!&lt;/p&gt; &lt;p&gt;‚úÖ High-accuracy EN/CN + 9 more languages: ar, de, en, es, fr, it, ja, ko, pt, ru, zh&lt;/p&gt; &lt;p&gt;‚úÖ Auto language detection&lt;/p&gt; &lt;p&gt;‚úÖ Songs? Raps? Voice with BGM? No problem. &amp;lt;8% WER&lt;/p&gt; &lt;p&gt;‚úÖ Works in noise, low quality, far-field&lt;/p&gt; &lt;p&gt;‚úÖ Custom context? Just paste ANY text ‚Äî names, jargon, even gibberish üß†&lt;/p&gt; &lt;p&gt;‚úÖ One model. Zero hassle.Great for edtech, media, customer service &amp;amp; more.&lt;/p&gt; &lt;p&gt;API: &lt;a href="https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031"&gt;https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope Demo: &lt;a href="https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo"&gt;https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/et1syg58iynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc1p0a</id>
    <title>Where are people finding RTX PRO 6000 96gb cards for under 7k</title>
    <updated>2025-09-08T22:18:44+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everywhere ive seen, they are like 8.5k, but people comstantly mention that they can be had for around 6.5k. How? Where? I want to start moving away from paid services like claude and start moving towards self-hosting, starting with an rtx pro 6000 + 3090. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T22:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbr45v</id>
    <title>Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!</title>
    <updated>2025-09-08T15:39:53+00:00</updated>
    <author>
      <name>/u/CornerLimits</name>
      <uri>https://old.reddit.com/user/CornerLimits</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt; &lt;img alt="Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!" src="https://external-preview.redd.it/PecTYmNSbm5tb-T9OW67-xyMoNn-SzofgAKif5I3sUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84ff63d5e5bbf73f86b2641f6f74955f0a20dbe" title="Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just released a fork of llama.cpp that implements some strong optimizations for the MI50/MI60/Vega7 series. &lt;/p&gt; &lt;p&gt;Thanks to the outstanding work of open source community I made a final effort to actually make flash attention FASTER than no flash attention in almost every case. Yeah‚Ä¶ almost.&lt;/p&gt; &lt;p&gt;The goal is to run ~30B models with ~30K ctx on a single card at decent speed.&lt;/p&gt; &lt;p&gt;You can find benchmarks, compile/launch/bench scripts, references to the original works and explanations of my new kernel in the repo.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CornerLimits"&gt; /u/CornerLimits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo33p</id>
    <title>UAE Preparing to Launch K2 Think, "the world‚Äôs most advanced open-source reasoning model"</title>
    <updated>2025-09-08T13:42:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt; &lt;img alt="UAE Preparing to Launch K2 Think, &amp;quot;the world‚Äôs most advanced open-source reasoning model&amp;quot;" src="https://external-preview.redd.it/3A4olwwXC7kAmitvVkfkfzLywUYc6IvJ9He-QlxgRLY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2afac7f2d1366e35e6945533e06a6756d060e202" title="UAE Preparing to Launch K2 Think, &amp;quot;the world‚Äôs most advanced open-source reasoning model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;In the coming week, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and G42 will release K2 Think, the world‚Äôs most advanced open-source reasoning model. &lt;strong&gt;Designed to be leaner and smarter, K2 Think delivers frontier-class performance in a remarkably compact form&lt;/strong&gt; ‚Äì often matching, or even surpassing, the results of models an order of magnitude larger. The result: greater efficiency, more flexibility, and broader real-world applicability.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wam.ae/en/article/bll7llv-recognition-sheikh-khalifa%E2%80%99s-contribution"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
