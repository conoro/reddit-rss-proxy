<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-03T21:24:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pdbyhg</id>
    <title>Help - Qwen3 LV - LM Studio instant response - Claude Code Router takes over 20 min</title>
    <updated>2025-12-03T18:19:27+00:00</updated>
    <author>
      <name>/u/designbanana</name>
      <uri>https://old.reddit.com/user/designbanana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;br /&gt; First of all, I hope I'm in the right reddit, if not, boo me out and I'll delete it :) &lt;/p&gt; &lt;p&gt;So I'm playing with claude code router (local LLM proxy for claude code) and I noticed the responses I got where, well, underwhelming. For example, I've tried to make a component based on an input image. But no luck, all I got was some generic grid component. (it was able to view the image, since it rendered the right text). Response took about 7 to 20 min (depending on the model variant). &lt;/p&gt; &lt;p&gt;Then I thought to run the same prompt directly as a chat in LM Studio, and a world of difference. Less than a second for the 30B, 7 seconds would be a lot. 235B variant was, I think, two minutes.&lt;br /&gt; (still no close copy of the image, but still a huge difference in result AND time.)&lt;/p&gt; &lt;p&gt;Now, I've got no clue on how to start unraveling the bottleneck of it all, and I hope you know what I do wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/designbanana"&gt; /u/designbanana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdbyhg/help_qwen3_lv_lm_studio_instant_response_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdbyhg/help_qwen3_lv_lm_studio_instant_response_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdbyhg/help_qwen3_lv_lm_studio_instant_response_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:19:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcjqjs</id>
    <title>Ministral 3 models were pruned from Mistral Small 3.1</title>
    <updated>2025-12-02T20:36:32+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt; &lt;img alt="Ministral 3 models were pruned from Mistral Small 3.1" src="https://preview.redd.it/bte4gtp1qu4g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bec6f7045ad754997a36d5294eedaa2112246178" title="Ministral 3 models were pruned from Mistral Small 3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bte4gtp1qu4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T20:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd4vp0</id>
    <title>EchoKit (Voice Interface for Local LLMs) Update: Added Dynamic System Prompts &amp; MCP Tool Wait Messages</title>
    <updated>2025-12-03T13:58:46+00:00</updated>
    <author>
      <name>/u/smileymileycoin</name>
      <uri>https://old.reddit.com/user/smileymileycoin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building &lt;strong&gt;EchoKit&lt;/strong&gt;, a hardware/software stack to give a voice to your local LLMs. It connects to OpenAI-compatible endpoints, meaning you can run it with LlamaEdge, standard LlamaCPP, or even Groq/Gemini.&lt;/p&gt; &lt;p&gt;We just released a server update that makes testing different &amp;quot;Agents&amp;quot; much faster:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Dynamic Prompt Loading:&lt;/strong&gt; Instead of hardcoding the system prompt in a config file and restarting the server every time you want to change the personality, you can now point the server to a URL (like a raw text file or an entry from &lt;code&gt;LLMs.txt&lt;/code&gt;). This lets you swap between a &amp;quot;Coding Assistant&amp;quot; and a &amp;quot;Storyteller&amp;quot; instantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Better Tool Use (MCP) UX:&lt;/strong&gt; We are betting big on the Model Context Protocol (MCP) for agentic search and tools. The voice agent now speaks a &amp;quot;Please wait&amp;quot; message when it detects it needs to call an external tool, so the user isn't left in silence during the tool-call latency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smileymileycoin"&gt; /u/smileymileycoin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd4vp0/echokit_voice_interface_for_local_llms_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd4vp0/echokit_voice_interface_for_local_llms_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd4vp0/echokit_voice_interface_for_local_llms_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcia1t</id>
    <title>DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High</title>
    <updated>2025-12-02T19:41:57+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt; &lt;img alt="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" src="https://a.thumbs.redditmedia.com/TJzNTRI6aFSLhjDdzBZtSFW1nl-mDnldDNZ8ONcsRV0.jpg" title="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad"&gt;https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for context on how impressive this is, I couldn't believe my eyes and had to double-check the results multiple times. The problems in this category are very hard like in the same ballpark as IMO P6.&lt;br /&gt; &lt;a href="https://x.com/gum1h0x/status/1995915458612953419"&gt;https://x.com/gum1h0x/status/1995915458612953419&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T19:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdf2cw</id>
    <title>how do I setup cline to orchestrate calls to two endpoints?</title>
    <updated>2025-12-03T20:11:49+00:00</updated>
    <author>
      <name>/u/PairOfRussels</name>
      <uri>https://old.reddit.com/user/PairOfRussels</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two GPUs each running separate llama.cpp and models at endpoints localhost:9991 and localhost:9992.&lt;/p&gt; &lt;p&gt;9991 = fast 75t/s small context 32k&lt;br /&gt; 9992 = slow 10t/s larger context 80k&lt;/p&gt; &lt;p&gt;I want a single cline workflow to use both LLM in an async and orchestrated manner. Surely someone has figured this out already. Can someone share the guide or give me some tips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PairOfRussels"&gt; /u/PairOfRussels &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdf2cw/how_do_i_setup_cline_to_orchestrate_calls_to_two/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdf2cw/how_do_i_setup_cline_to_orchestrate_calls_to_two/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdf2cw/how_do_i_setup_cline_to_orchestrate_calls_to_two/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:11:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd6vxu</id>
    <title>Does anyone use RunPod for SFT? If yes, you train via SSH or Jupyter (web-hosted)</title>
    <updated>2025-12-03T15:15:38+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a week or 2, I want to rent one B200 ($5.2/hr) to do SFT (Supervised Fine-Tuning) for GPT-OSS-120B on a ~30k row dataset. &lt;/p&gt; &lt;p&gt;I have done training for 2-4 hours a few months ago, but sometimes the Jupyter notebook crashed (got a message like &amp;quot;Connection lost&amp;quot; or something like that) and often had to restart training.&lt;/p&gt; &lt;p&gt;If you use RunPod (or any other GPU cloud provider), how do you manage long sessions (4+ hours)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd6vxu/does_anyone_use_runpod_for_sft_if_yes_you_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd6vxu/does_anyone_use_runpod_for_sft_if_yes_you_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd6vxu/does_anyone_use_runpod_for_sft_if_yes_you_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T15:15:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcayfs</id>
    <title>Mistral 3 Blog post</title>
    <updated>2025-12-02T15:13:14+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt; &lt;img alt="Mistral 3 Blog post" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 Blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1zeu</id>
    <title>apple/CLaRa-7B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-12-03T11:43:05+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-Instruct"&gt;https://huggingface.co/apple/CLaRa-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-Base"&gt;https://huggingface.co/apple/CLaRa-7B-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-E2E"&gt;https://huggingface.co/apple/CLaRa-7B-E2E&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd8i1u</id>
    <title>Cheapest and best way to host a GGUF model with an API (like OpenAI) for production?</title>
    <updated>2025-12-03T16:15:32+00:00</updated>
    <author>
      <name>/u/New-Worry6487</name>
      <uri>https://old.reddit.com/user/New-Worry6487</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I'm trying to host a &lt;code&gt;.gguf&lt;/code&gt; LLM in a way that lets me access it using an API ‚Äî similar to how we call the OpenAI API (&lt;code&gt;/v1/chat/completions&lt;/code&gt;, etc).&lt;br /&gt; I want to expose my own hosted GGUF model through a clean HTTP API that any app can use.&lt;/p&gt; &lt;h3&gt;What I need:&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Host a GGUF model&lt;/strong&gt; (7B / 13B / possibly 30B later)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Access it over a REST API&lt;/strong&gt; (Ollama-style, OpenAI-style, or custom)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-ready setup&lt;/strong&gt; (stable, scalable enough, not hobby-only)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cheapest possible hosting options&lt;/strong&gt; (VPS or GPU cloud)&lt;/li&gt; &lt;li&gt;Advice on &lt;strong&gt;which server/runtime is best&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Ollama API server&lt;/li&gt; &lt;li&gt;llama.cpp server mode&lt;/li&gt; &lt;li&gt;LocalAI&lt;/li&gt; &lt;li&gt;vLLM (if GGUF isn‚Äôt ideal for it)&lt;/li&gt; &lt;li&gt;or anything else that works well&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Budget Focus&lt;/h3&gt; &lt;p&gt;Trying to find the &lt;strong&gt;best price-to-performance platform&lt;/strong&gt;.&lt;br /&gt; Options I'm considering but unsure about: - Hetzner - RunPod - Vast.ai - Vultr - Lambda Labs - Any cheap GPU rental providers?&lt;/p&gt; &lt;h3&gt;My goals:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Host the model once&lt;/li&gt; &lt;li&gt;Call it from my mobile or backend app through an API&lt;/li&gt; &lt;li&gt;Avoid OpenAI-style monthly costs&lt;/li&gt; &lt;li&gt;Keep latency reasonable&lt;/li&gt; &lt;li&gt;Ensure it runs reliably even with multiple requests&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Questions:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs the &lt;strong&gt;cheapest&lt;/strong&gt; but still &lt;strong&gt;practical&lt;/strong&gt; setup for production?&lt;/li&gt; &lt;li&gt;Is &lt;strong&gt;Ollama on a VPS&lt;/strong&gt; good enough?&lt;/li&gt; &lt;li&gt;Should I use &lt;strong&gt;llama.cpp server&lt;/strong&gt; instead?&lt;/li&gt; &lt;li&gt;Does anyone run GGUF models in production at scale?&lt;/li&gt; &lt;li&gt;Any recommended architectures or pitfalls?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would really appreciate hearing what setups have worked for you ‚Äî especially from people who have deployed GGUF models behind an API for real apps!&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Worry6487"&gt; /u/New-Worry6487 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd8i1u/cheapest_and_best_way_to_host_a_gguf_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd8i1u/cheapest_and_best_way_to_host_a_gguf_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd8i1u/cheapest_and_best_way_to_host_a_gguf_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T16:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3uvy</id>
    <title>Can we expect better LLM hardware in 2026?</title>
    <updated>2025-12-03T13:16:07+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean with a lot of fast(!) VRAM.&lt;/p&gt; &lt;p&gt;DGX spark and AMD AI Max have really low memory speeds.&lt;/p&gt; &lt;p&gt;China is releasing so many open source models, when will they come with cheap hardware that we can run them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pceipb</id>
    <title>Mistral just released Mistral 3 ‚Äî a full open-weight model family from 3B all the way up to 675B parameters.</title>
    <updated>2025-12-02T17:26:06+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are Apache 2.0 and fully usable for research + commercial work.&lt;/p&gt; &lt;p&gt;Quick breakdown:&lt;/p&gt; &lt;p&gt;‚Ä¢ Ministral 3 (3B / 8B / 14B) ‚Äì compact, multimodal, and available in base, instruct, and reasoning variants. Surprisingly strong for their size.&lt;/p&gt; &lt;p&gt;‚Ä¢ Mistral Large 3 (675B MoE) ‚Äì their new flagship. Strong multilingual performance, high efficiency, and one of the most capable open-weight instruct models released so far.&lt;/p&gt; &lt;p&gt;Why it matters: You now get a full spectrum of open models that cover everything from on-device reasoning to large enterprise-scale intelligence. The release pushes the ecosystem further toward distributed, open AI instead of closed black-box APIs.&lt;/p&gt; &lt;p&gt;Full announcement: &lt;a href="https://mistral.ai/news/mistral-3"&gt;https://mistral.ai/news/mistral-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcomhi</id>
    <title>I'm surprised how simple Qwen3 VL's architecture is.</title>
    <updated>2025-12-02T23:50:06+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt; &lt;img alt="I'm surprised how simple Qwen3 VL's architecture is." src="https://preview.redd.it/bfrh4xf5nv4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2643d2d6457fb6e3adfc09a5cf9e18b995e4219f" title="I'm surprised how simple Qwen3 VL's architecture is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the new 3D position id logic really got a lot more intuitive compared to qwen2.5 vl. it basically index image patches on width and height dimension in addition to the regular token sequence / temporal dimension (while treating text as one same number across all 3 dimensions). &lt;/p&gt; &lt;p&gt;in addition to this, they added deepstack, which essentially is just some residual connections between vision encoder blocks and downstream LLM blocks.&lt;/p&gt; &lt;p&gt;here's the full repo if you want to read more: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfrh4xf5nv4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T23:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcp8z3</id>
    <title>Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?</title>
    <updated>2025-12-03T00:16:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt; &lt;img alt="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" src="https://preview.redd.it/buxyht7ltv4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed225e778fb3ebb1d3e4ff9ac401e09c3aced65e" title="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking at you, Unsloth üòÅ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/buxyht7ltv4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1yqc</id>
    <title>Hot take: We‚Äôre overselling 'semantic search' in RAG.</title>
    <updated>2025-12-03T11:42:02+00:00</updated>
    <author>
      <name>/u/Raisin_False</name>
      <uri>https://old.reddit.com/user/Raisin_False</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building some RAG stuff and 'semantic search' feels way more magical in marketing than in reality.&lt;/p&gt; &lt;p&gt;Embeddings are great &lt;strong&gt;fuzzy matchers in meaning space&lt;/strong&gt; - they shine on paraphrases, synonyms, 'something like this' queries. But whenever I need sharper behavior (logic, constraints, dates, 'papers using X on Y after 2019'), plain bi-encoder vector search starts to fall over unless I add extra machinery.&lt;/p&gt; &lt;p&gt;In practice my setups end up looking more like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;BM25 or dense (or hybrid) &lt;/li&gt; &lt;li&gt;Reranker and/or LLM query rewrite &lt;/li&gt; &lt;li&gt;LLM reasoning also maybe graphs/filters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At that point, calling just the first stage 'semantic search' feels a bit misleading, cause it's more like 'dense/vector retrieval' plus a bunch of stuff on top that actually does the reasoning.&lt;/p&gt; &lt;p&gt;So i have 2 questions for you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is 'semantic search' a fair name for plain vector similarity, or do you avoid that term?&lt;/li&gt; &lt;li&gt;How far did you get with just embeddings before needing reranking / query rewriting / graphs / filters?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raisin_False"&gt; /u/Raisin_False &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3mdw</id>
    <title>Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI</title>
    <updated>2025-12-03T13:05:36+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt; &lt;img alt="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" src="https://external-preview.redd.it/0mZ7_HvOTkdLgtq4s_qT3vry9cE_RWRALKiuljZ3Fl8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01be5fde96c8bded5f16d12f17d20ed686c5e29" title="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b60-battlematrix-preview-192gb-of-vram-for-on-premise-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9tgj</id>
    <title>A Technical Tour of the DeepSeek Models from V3 to V3.2</title>
    <updated>2025-12-03T17:03:17+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt; &lt;img alt="A Technical Tour of the DeepSeek Models from V3 to V3.2" src="https://external-preview.redd.it/Oy9W7OYOeVO8Z6Sl3EWWZR-9AbREkAwoyEei1XJ7yeY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a5effd7f132f71b2efdd47cc12daa448023c0bf" title="A Technical Tour of the DeepSeek Models from V3 to V3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T17:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3xyp</id>
    <title>Why don't Google and Openai release their old models?</title>
    <updated>2025-12-03T13:19:51+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPt 4 and gemini 2 pro are dated, they should release it... Are they afraid of releasing their data and architecture? They released gemma and gpt oss already. Gemini 2 has a large context window, but the quality degrades when it gets large though and it is replicable.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9f4x</id>
    <title>I trained a 7B to learn a niche language and reaching 86% code accuracy</title>
    <updated>2025-12-03T16:49:14+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"&gt; &lt;img alt="I trained a 7B to learn a niche language and reaching 86% code accuracy" src="https://b.thumbs.redditmedia.com/y1P83lnQKXF0ESpmUcRyqN8DslMG6uG2cjIoC8294mY.jpg" title="I trained a 7B to learn a niche language and reaching 86% code accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I just wanted to share a project I did over the last weekend.&lt;/p&gt; &lt;p&gt;I‚Äôm no ML engineer or having any relevant background in AI, just have been toying with the idea of training an LLM myself for a while.&lt;/p&gt; &lt;p&gt;Most of my previous training attempts did not yield and meaningful result, but I‚Äôm still managed to learned a thing or two. And this time, I decided to give it a try again.&lt;/p&gt; &lt;p&gt;The niche language I picked to train the LLM (Qwen2.5-coder-7b) was a less popular text-to-diagram language called Pintora. Since most open source models did not have any knowledge about this language, it‚Äôs a fun project to try.&lt;/p&gt; &lt;p&gt;Long story short, I planned to train this for free on Google Colab, but ended up renting a 48GB A40 for a naive mistake, and doing a lot of the training pipeline myself (in a much smaller scale), from creating the dataset, cleaning them up, to do two phases training: Continued Pretraining and then Instruction Finetune, to teach the model how to either generate diagrams from scratch and editing existing diagrams. &lt;/p&gt; &lt;p&gt;In the end, I‚Äôm quite happy with the result, although it‚Äôs not great, the model was able to generate syntactically correct code, the diagrams are showing up. I did a quick evaluation to confirm how accurate (in terms of of compile-able diagrams) that the model can generate, out of 1000 examples, only about 140 are failing, that‚Äôs about 86% accuracy.&lt;/p&gt; &lt;p&gt;Both the model (safetensors, gguf, full and quantized) are available on HF if you are interested. I also did a write up to document the process, I think it might be helpful to share so I can learn from all of your feedback! &lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language"&gt;https://huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/huytd189/pintora-coder-7b"&gt;https://huggingface.co/huytd189/pintora-coder-7b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/huytd189/pintora-coder-7b-gguf"&gt;https://huggingface.co/huytd189/pintora-coder-7b-gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dataset:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/huytd189/pintora-instruct"&gt;https://huggingface.co/datasets/huytd189/pintora-instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/huytd189/pintora-edit-instruct"&gt;https://huggingface.co/datasets/huytd189/pintora-edit-instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pd9f4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd5yxy</id>
    <title>My experiences with the new Ministral 3 14B Reasoning 2512 Q8</title>
    <updated>2025-12-03T14:41:03+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt; &lt;img alt="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" src="https://b.thumbs.redditmedia.com/YQNWxn03P5a0Q35GBj3cSIS0Oa0a8pdRn0Pkkl0sUGM.jpg" title="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;45 minutes and 33K tokens of thinking about making html tetris (1 line prompt):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819"&gt;https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tool calling breaks all the time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184"&gt;https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also at some point it stopped using the [think] tags altogether and just started thinking out loud. I'll leave it running for a couple of hours and see if it eventually manages to build the HTML Tetris.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T14:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pddj59</id>
    <title>average tech american compagny when you ask to release a 100 parameters ai model outdated since 2017 who counts the number of tiles in a bathroom(the model is too dangerous for the user)</title>
    <updated>2025-12-03T19:14:58+00:00</updated>
    <author>
      <name>/u/Qnimbus_</name>
      <uri>https://old.reddit.com/user/Qnimbus_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pddj59/average_tech_american_compagny_when_you_ask_to/"&gt; &lt;img alt="average tech american compagny when you ask to release a 100 parameters ai model outdated since 2017 who counts the number of tiles in a bathroom(the model is too dangerous for the user)" src="https://preview.redd.it/q9ruaywkg15g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27cb05a9b58a21a06a2f7c46b02e68e3e7185b21" title="average tech american compagny when you ask to release a 100 parameters ai model outdated since 2017 who counts the number of tiles in a bathroom(the model is too dangerous for the user)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qnimbus_"&gt; /u/Qnimbus_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q9ruaywkg15g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pddj59/average_tech_american_compagny_when_you_ask_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pddj59/average_tech_american_compagny_when_you_ask_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T19:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
