<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-12T20:25:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ouy2a6</id>
    <title>What's a surprisingly capable smaller model (&lt;15B parameters) that you feel doesn't get enough attention?</title>
    <updated>2025-11-12T06:40:48+00:00</updated>
    <author>
      <name>/u/Street-Lie-2584</name>
      <uri>https://old.reddit.com/user/Street-Lie-2584</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouy2a6/whats_a_surprisingly_capable_smaller_model_15b/"&gt; &lt;img alt="What's a surprisingly capable smaller model (&amp;lt;15B parameters) that you feel doesn't get enough attention?" src="https://b.thumbs.redditmedia.com/kLtGVKLIgLuZIGhmA34Ogjsd59ZIB8LW4CaY0UgxdcA.jpg" title="What's a surprisingly capable smaller model (&amp;lt;15B parameters) that you feel doesn't get enough attention?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We all see the headlines for the massive new 100B+ models, but some of the most impressive work is happening at a smaller scale. What's a sub-15B model you've used recently that genuinely impressed you with its reasoning, coding, or creativity? Maybe it's a fine-tune of a known architecture or something entirely different. Let's share some hidden gems.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2mnrk4jpur0g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=317e3c1ca1664b07b4ada9fafb7a1cd2c0d7c389"&gt;https://preview.redd.it/2mnrk4jpur0g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=317e3c1ca1664b07b4ada9fafb7a1cd2c0d7c389&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street-Lie-2584"&gt; /u/Street-Lie-2584 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouy2a6/whats_a_surprisingly_capable_smaller_model_15b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouy2a6/whats_a_surprisingly_capable_smaller_model_15b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouy2a6/whats_a_surprisingly_capable_smaller_model_15b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T06:40:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov0t1t</id>
    <title>I repurposed an old xeon build by adding two MI50 cards.</title>
    <updated>2025-11-12T09:34:34+00:00</updated>
    <author>
      <name>/u/politerate</name>
      <uri>https://old.reddit.com/user/politerate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov0t1t/i_repurposed_an_old_xeon_build_by_adding_two_mi50/"&gt; &lt;img alt="I repurposed an old xeon build by adding two MI50 cards." src="https://a.thumbs.redditmedia.com/JpkP4SY-Zt0YUkfyWDB0vfK9IDG_vyJreEcTDshGzm4.jpg" title="I repurposed an old xeon build by adding two MI50 cards." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I had an old xeon x79 build laying around and I thought I could use it as an inference box.&lt;/p&gt; &lt;p&gt;I ordered two mi50 from Alibaba for roughly 350 Euros with taxes, upgraded the power supply to 1kw. Had to flash the cards because I could not boot without a video output. I flashed the VEGA Bios which also caps them to 170W.&lt;br /&gt; Idle power consumption is ~70w, during inferencing sub 200w.&lt;br /&gt; While the prompt processing is not stellar, for me as a single user it works fine.&lt;/p&gt; &lt;p&gt;With gpt-oss-120b I can run a 50k context all in vram and 120k with moving some layers to cpu.&lt;br /&gt; Currently my use case is part of my all local stack: n8n workflows which use this as an openAI compatible endpoint.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mplm805ros0g1.png?width=2194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd9a366e739a3b4294608b058dc5443d9f3fa48e"&gt;https://preview.redd.it/mplm805ros0g1.png?width=2194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cd9a366e739a3b4294608b058dc5443d9f3fa48e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/politerate"&gt; /u/politerate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov0t1t/i_repurposed_an_old_xeon_build_by_adding_two_mi50/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov0t1t/i_repurposed_an_old_xeon_build_by_adding_two_mi50/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov0t1t/i_repurposed_an_old_xeon_build_by_adding_two_mi50/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T09:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouazho</id>
    <title>Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)</title>
    <updated>2025-11-11T14:34:43+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"&gt; &lt;img alt="Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)" src="https://external-preview.redd.it/Z2ZjbDkzdmowbjBnMUB18FDMNIKrOWZMaI6GCxWf_t_2BvSabc90NvjIF-MD.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57cf924fd9176e70ff73462191aaf19ef1b60b55" title="Egocentric-10K is the largest egocentric dataset. It is the first dataset collected exclusively in real factories (Build AI - 10,000 hours - 2,153 factory workers - 1,080,000,000 frame)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging Face, (apache 2.0): &lt;a href="https://huggingface.co/datasets/builddotai/Egocentric-10K"&gt;https://huggingface.co/datasets/builddotai/Egocentric-10K&lt;/a&gt;&lt;br /&gt; Eddy Xu on ùïè: &lt;a href="https://x.com/eddybuild/status/1987951619804414416"&gt;https://x.com/eddybuild/status/1987951619804414416&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nlsslzuj0n0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouazho/egocentric10k_is_the_largest_egocentric_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T14:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov4z8j</id>
    <title>Fast and Affordable LLMs serving on Intel Arc Pro B-Series GPUs with vLLM</title>
    <updated>2025-11-12T13:21:38+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.vllm.ai/2025/11/11/intel-arc-pro-b.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov4z8j/fast_and_affordable_llms_serving_on_intel_arc_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov4z8j/fast_and_affordable_llms_serving_on_intel_arc_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T13:21:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouq7oe</id>
    <title>I've just ordered an RTX 6000 Pro. What are the best models to use in its 96GB for inference and OCR processing of documents?</title>
    <updated>2025-11-12T00:13:24+00:00</updated>
    <author>
      <name>/u/AlwaysLateToThaParty</name>
      <uri>https://old.reddit.com/user/AlwaysLateToThaParty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, just trying to find out what people think are the best LLM's these days for inference and OCR document processing? So what model and quant works? I need it because a lot of the inference and documentation is confidential (medical and legal). More than one person will use the device via configuring a web front-end. Your suggestions would be great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlwaysLateToThaParty"&gt; /u/AlwaysLateToThaParty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouq7oe/ive_just_ordered_an_rtx_6000_pro_what_are_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouq7oe/ive_just_ordered_an_rtx_6000_pro_what_are_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouq7oe/ive_just_ordered_an_rtx_6000_pro_what_are_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T00:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouqbyo</id>
    <title>Local conversational model with STT TTS</title>
    <updated>2025-11-12T00:18:39+00:00</updated>
    <author>
      <name>/u/DuncanEyedaho</name>
      <uri>https://old.reddit.com/user/DuncanEyedaho</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqbyo/local_conversational_model_with_stt_tts/"&gt; &lt;img alt="Local conversational model with STT TTS" src="https://external-preview.redd.it/eGU2ZzkydnJ5cDBnMX7vvGggqTNZOst5uXqXZt7URDd0IOwrN4Cxg9i1Tmfm.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1efb5bd97fb338d0dfdc60d946215b0f1ccc6f7c" title="Local conversational model with STT TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to make an animatronic cohost to hang out with me and my workshop and basically roast me. It was really interesting how simple things like injecting relevant memories into the system prompt (or vision captioning) really messed with its core identity; very subtle tweaks repeatedly turned it into &amp;quot;a helpful AI assistant,&amp;quot; but I eventually got the personality to be pretty consistent with a medium context size and decent episodic memory. &lt;/p&gt; &lt;p&gt;Details: faster-whisper base model fine-tuned on my voice, Piper TTS tiny model find tuned on my passable impression of Skeletor, win11 ollama running llama 3.2 3B q4, custom pre-processing and prompt creation using pgvector, captioning with BLIP (v1), facial recognition that Claude basically wrote/ trained for me in a jiffy, and other assorted servos and relays.&lt;/p&gt; &lt;p&gt;There is a 0.5 second pause detection before sending off the latest STT payload. &lt;/p&gt; &lt;p&gt;Everything is running on an RTX 3060, and I can use a context size of 8000 tokens without difficulty, I may push it further but I had to slam it down because there's so much other stuff running on the card.&lt;/p&gt; &lt;p&gt;I'm getting back into the new version of Reddit, hope this is entertaining to somebody.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DuncanEyedaho"&gt; /u/DuncanEyedaho &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hngyx3yryp0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqbyo/local_conversational_model_with_stt_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouqbyo/local_conversational_model_with_stt_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T00:18:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouzxrw</id>
    <title>ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ is available in Qt Creator's Extension Store</title>
    <updated>2025-11-12T08:38:11+00:00</updated>
    <author>
      <name>/u/cristianadam</name>
      <uri>https://old.reddit.com/user/cristianadam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouzxrw/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_is_available_in_qt_creators/"&gt; &lt;img alt="ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ is available in Qt Creator's Extension Store" src="https://external-preview.redd.it/cDZ5aG9vb2tmczBnMe-wOcA5k3ws7B9qNxPhbAFpjix4pw_ql6FN-CDjglod.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=42d5a58fc981c633dccbda024a13c069f9ea04d8" title="ùöïùöïùöäùöñùöä.ùööùöùùöåùöõùöéùöäùöùùöòùöõ is available in Qt Creator's Extension Store" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This video showcases how you can use &lt;code&gt;gpt-oss 20b&lt;/code&gt; with Qt Creator 18 and llama.qtcreator.&lt;/p&gt; &lt;p&gt;This was done on Windows 11 running on a Bosgame M5 &amp;quot;Strix Halo&amp;quot; AMD Ryzen AI Max+ 395 PC.&lt;/p&gt; &lt;p&gt;First the llama.cpp extension in installed from Qt Creator's extension store, then llama.cpp via &lt;code&gt;winget&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cristianadam"&gt; /u/cristianadam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kgkuowokfs0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouzxrw/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_is_available_in_qt_creators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouzxrw/ùöïùöïùöäùöñùöäùööùöùùöåùöõùöéùöäùöùùöòùöõ_is_available_in_qt_creators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T08:38:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouogiq</id>
    <title>Most used models and performance on M3u 512 gb</title>
    <updated>2025-11-11T23:00:24+00:00</updated>
    <author>
      <name>/u/nomorebuttsplz</name>
      <uri>https://old.reddit.com/user/nomorebuttsplz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/"&gt; &lt;img alt="Most used models and performance on M3u 512 gb" src="https://preview.redd.it/s0jrlz569p0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5951ff139e6d111ec9e08e507274871f5fdc1a6" title="Most used models and performance on M3u 512 gb" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bored, thought this screenshot was cute, might delete later.&lt;/p&gt; &lt;p&gt;Overall GLM 4.6 is queen right now.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: Kimi K2 thinking&lt;br /&gt; &lt;strong&gt;Use case&lt;/strong&gt;: idk it's just cool having a huge model running local. I guess I will use it for brainstorming stuff, medical stuff, other questionable activities like academic writing. PP speed/context size is too limited for a lot of agentic workflows but it's a modest step above other open source models for pure smarts&lt;br /&gt; &lt;strong&gt;PP speed:&lt;/strong&gt; Q3 GGUF 19 t/s (26k context) faster with lower context;&lt;br /&gt; &lt;strong&gt;Token gen&lt;/strong&gt; speed: 3ish to 20 t/s depending on context size&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; GLM 4.6&lt;br /&gt; &lt;strong&gt;Use Case:&lt;/strong&gt; vibe coding (slow but actually can create working software semi-autonomously with Cline); creative writing; expository/professional writing; general quality-sensitive use&lt;br /&gt; &lt;strong&gt;PP Speed:&lt;/strong&gt; 4 bit MLX 50-70 t/s at large context sizes (greater than 40k)&lt;br /&gt; &lt;strong&gt;Token Gen speed:&lt;/strong&gt; generally 10-20&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; Minimax-m2&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; Document review, finance, math. Like a smarter OSS 120.&lt;br /&gt; &lt;strong&gt;PP Speed&lt;/strong&gt;: MLX 4 bit 3-400 at modest sizes (10k ish)&lt;br /&gt; &lt;strong&gt;Token gen speed:&lt;/strong&gt; 40-50 at modest sizes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt;: GPT-OSS-120&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; Agentic searching, large document ingesting; general medium-quality, fast use&lt;br /&gt; &lt;strong&gt;PP speed:&lt;/strong&gt; 4 bit MLX near 1000 at modest context sizes. But context caching doesn't work, so has to reprocess every turn.&lt;br /&gt; &lt;strong&gt;Token gen speed:&lt;/strong&gt; about 80 at medium context sizes&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model: Hermes 405b&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; When you want stuff to have that early 2024 vibe... not really good at anything except maybe low context roleplay/creative writing. Not the trivia king people seem to think.&lt;br /&gt; &lt;strong&gt;PP Speed:&lt;/strong&gt; mlx 4 bit: Low... maybe 25 t/s?&lt;br /&gt; &lt;strong&gt;Token gen Speed:&lt;/strong&gt; Super low... 3-5 t/s&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model: Deepseek 3.1:&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;Use case:&lt;/strong&gt; Used to be for roleplay, long context high quality slow work. Might be obsoleted by glm 4.6... not sure it can do anything better&lt;br /&gt; &lt;strong&gt;PP Speed:&lt;/strong&gt; Q3 GGUF: 50 t/s&lt;br /&gt; &lt;strong&gt;Token gen speed:&lt;/strong&gt; 3-20 depending on context size&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nomorebuttsplz"&gt; /u/nomorebuttsplz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0jrlz569p0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouogiq/most_used_models_and_performance_on_m3u_512_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T23:00:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovc90m</id>
    <title>Replace Sonnet 4.5 with Minimax-M2 for my 3D app -&gt; same quality with like 1/10th costs</title>
    <updated>2025-11-12T17:55:54+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc90m/replace_sonnet_45_with_minimaxm2_for_my_3d_app/"&gt; &lt;img alt="Replace Sonnet 4.5 with Minimax-M2 for my 3D app -&amp;gt; same quality with like 1/10th costs" src="https://preview.redd.it/swomto3t6v0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d82219e652879369cab437c9f8023468d498194a" title="Replace Sonnet 4.5 with Minimax-M2 for my 3D app -&amp;gt; same quality with like 1/10th costs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using LLMs to control a modelling software, which requires a lot of thinking and tool calling, so I've been using Sonnet in the most complex portion of the workflow. Ever since I saw minimax can match sonnet in benchmarks, I replaced the model and haven't seen a degradation in output (3d model output in my case).&lt;/p&gt; &lt;p&gt;&lt;a href="https://native-blend-app.vercel.app/"&gt;Agent&lt;/a&gt; I've been using&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/swomto3t6v0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc90m/replace_sonnet_45_with_minimaxm2_for_my_3d_app/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc90m/replace_sonnet_45_with_minimaxm2_for_my_3d_app/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:55:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouzqja</id>
    <title>I wrote a guide on running LLMs everywhere (desktop, mobile, game engines) with zero conversion</title>
    <updated>2025-11-12T08:25:09+00:00</updated>
    <author>
      <name>/u/Apricot-Zestyclose</name>
      <uri>https://old.reddit.com/user/Apricot-Zestyclose</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full article: &lt;a href="https://medium.com/@planetbridging/loom-the-universal-ai-runtime-that-works-everywhere-and-why-that-matters-54de5e7ec182"&gt;https://medium.com/@planetbridging/loom-the-universal-ai-runtime-that-works-everywhere-and-why-that-matters-54de5e7ec182&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: Built LOOM to solve the &amp;quot;download model ‚Üí convert to 5 formats ‚Üí hope outputs match&amp;quot; problem.&lt;/p&gt; &lt;p&gt;One HuggingFace model ‚Üí works on Python, JS, C#, Go, WASM, Android, iOS, Godot game engine. No GGUF conversion needed.&lt;/p&gt; &lt;p&gt;Demos in article: Running SmolLM2/Qwen2.5 on desktop, in Godot, on Android.&lt;/p&gt; &lt;p&gt;Already published to PyPI/npm/NuGet for easy integration.&lt;/p&gt; &lt;p&gt;Article covers technical details and why local AI matters for privacy/cost/sovereignty.&lt;/p&gt; &lt;p&gt;Code: github.com/openfluke/loom&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apricot-Zestyclose"&gt; /u/Apricot-Zestyclose &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouzqja/i_wrote_a_guide_on_running_llms_everywhere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouzqja/i_wrote_a_guide_on_running_llms_everywhere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouzqja/i_wrote_a_guide_on_running_llms_everywhere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T08:25:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov2qee</id>
    <title>My (open-source) continuation (FlexAttention, RoPE, BlockMasks, Muon, etc.) to Karpathy's NanoGPT</title>
    <updated>2025-11-12T11:31:36+00:00</updated>
    <author>
      <name>/u/Any-Winter-4079</name>
      <uri>https://old.reddit.com/user/Any-Winter-4079</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;First of all, I am not fully sure if this useful to &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, because I would assume this is more about running existing models that starting from scratch? Or maybe you expect higher quality models.&lt;/p&gt; &lt;p&gt;In any case, I have been following and coding along Andrej Karpathy's 'Let's reproduce GPT-2 (124M)', and after finishing the four hours, I decided to continue adding some modern changes. At iteration 31, the repo contains:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;FlashAttention (sdpa) / FlexAttention&lt;/li&gt; &lt;li&gt;Sliding Window Attention (attend to a subset of tokens), Doc Masking (attend to same-doc tokens only), and Attention Logit Soft-capping (if FlexAttention, for performance) &lt;ul&gt; &lt;li&gt;Sliding Window Attention ramp (increase window size over training)&lt;/li&gt; &lt;li&gt;Attention logit soft-capping (&amp;quot;clamp&amp;quot;, &amp;quot;ptx&amp;quot; -faster-, &amp;quot;rational&amp;quot; or &amp;quot;exact&amp;quot;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Custom masking (e.g., padding mask if non-causal)&lt;/li&gt; &lt;li&gt;AdamW or AdamW and Muon &lt;ul&gt; &lt;li&gt;Muon steps, momentum, use Nesterov&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;MHA/MQA/GQA (n_heads vs n_kv_heads)&lt;/li&gt; &lt;li&gt;QK norm (RMS/L2)&lt;/li&gt; &lt;li&gt;RMSNorm or LayerNorm&lt;/li&gt; &lt;li&gt;GELU, ReLU, ReLU**2, SiLU or SwiGLU (fair or unfair) activations&lt;/li&gt; &lt;li&gt;Bias or no bias&lt;/li&gt; &lt;li&gt;Tied or untied embeddings&lt;/li&gt; &lt;li&gt;Learning rate warmup and decay&lt;/li&gt; &lt;li&gt;RoPE/NoPE/absolute positional encodings&lt;/li&gt; &lt;li&gt;LM head logit soft-capping&lt;/li&gt; &lt;li&gt;Gradient norm clipping&lt;/li&gt; &lt;li&gt;Kernel warmup steps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I share the repo in case it is helpful to someone starting out. I've tried to comment the code, because I was learning these concepts as I was going along. Also, I have tried to make it configurable at the start, with GPTConfig and TrainingConfig (meaning, you should be able to mix the above as you want, e.,g., GELU + AdamW + gradient norm clipping, or SiLU + Muon + FlexAttention + RoPE, etc.&lt;/p&gt; &lt;p&gt;I am not sure if the code is useful to anyone else, or maybe my comments only make sense to me.&lt;/p&gt; &lt;p&gt;In any case, here is the GitHub. Version 1 (`00-gpt-3-small-overfit-batch.py`) is the batch overfitting from the tutorial, while version 31 (`30-gpt-3-small-with-training-config-and-with-or-without-swa-window-size-ramp.py`) for instance adds a SWA ramp to version 30. And in between, intermediate versions progressively adding the above.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Any-Winter-4079/GPT-3-Small-Pretraining-Experiments"&gt;https://github.com/Any-Winter-4079/GPT-3-Small-Pretraining-Experiments&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally, while it is in the README as well, let me say this is the good, most efficient version of the speedrun: &lt;a href="https://github.com/KellerJordan/modded-nanogpt"&gt;https://github.com/KellerJordan/modded-nanogpt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With this I mean, if you want super fast code, go there. This repo tries to be more configurable and more explained, but it doesn't match yet the speedrun's performance. So take my version as that of someone that is learning along, more than a perfect repo.&lt;/p&gt; &lt;p&gt;Still, I would hope it is useful to someone.&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Any-Winter-4079"&gt; /u/Any-Winter-4079 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2qee/my_opensource_continuation_flexattention_rope/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2qee/my_opensource_continuation_flexattention_rope/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2qee/my_opensource_continuation_flexattention_rope/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T11:31:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov2ll9</id>
    <title>Mastering llama.cpp: A Comprehensive Guide to Local LLM Integration</title>
    <updated>2025-11-12T11:24:02+00:00</updated>
    <author>
      <name>/u/KonradFreeman</name>
      <uri>https://old.reddit.com/user/KonradFreeman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2ll9/mastering_llamacpp_a_comprehensive_guide_to_local/"&gt; &lt;img alt="Mastering llama.cpp: A Comprehensive Guide to Local LLM Integration" src="https://external-preview.redd.it/WdrekqB6cGVYVUhRiytgA_2P2qLFtNk6GTtWaIcWHTU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c892549e0f061a080a58e4d88cc24571b05ad17e" title="Mastering llama.cpp: A Comprehensive Guide to Local LLM Integration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, so I came in here the other day with me fancy shmancy chatbot wrapper I was using Ollama with and thought I was impressive. Pft. Peasant I twas!&lt;/p&gt; &lt;p&gt;So I bit the bullet and finally learned about llama.cpp and I wrote up this guide on what I taught myself about it to get me started. Personally I use python for everything so I included the &lt;code&gt;llama-cpp-python option as well.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I made this more for personal reference. But I have found that other people find this helpful which is why I am sharing.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;If you have any tips or tricks I left out, be sure to post them below so that this post can include even more!&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Thanks everyone and have a nice day!&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KonradFreeman"&gt; /u/KonradFreeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://danielkliewer.com/blog/2025-11-12-mastering-llama-cpp-local-llm-integration-guide"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2ll9/mastering_llamacpp_a_comprehensive_guide_to_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2ll9/mastering_llamacpp_a_comprehensive_guide_to_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T11:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov2wmu</id>
    <title>Is Deepseek-OCR SOTA for OCR-related tasks?</title>
    <updated>2025-11-12T11:41:34+00:00</updated>
    <author>
      <name>/u/Ok_Television_9000</name>
      <uri>https://old.reddit.com/user/Ok_Television_9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those running local setups (e.g 16 GB VRAM), how does DeepSeek-OCR stack up against recent VLMs ‚Äî is it considered SOTA for document parsing?&lt;/p&gt; &lt;p&gt;I‚Äôm experimenting with adding an LLM layer on top to extract structured fields, but I‚Äôm wondering if models like Qwen3-VL-8B might still outperform it overall.&lt;/p&gt; &lt;p&gt;Anyone here been playing with the latest VLMs and have thoughts or benchmarks to share?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Television_9000"&gt; /u/Ok_Television_9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2wmu/is_deepseekocr_sota_for_ocrrelated_tasks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2wmu/is_deepseekocr_sota_for_ocrrelated_tasks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2wmu/is_deepseekocr_sota_for_ocrrelated_tasks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T11:41:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ouwmx4</id>
    <title>Rusty-R2: Open source AI you can actually train yourself on consumer hardware</title>
    <updated>2025-11-12T05:18:25+00:00</updated>
    <author>
      <name>/u/Bonzupii</name>
      <uri>https://old.reddit.com/user/Bonzupii</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building Rusty-R2, exploring efficient, post-transformer architectures you can train from scratch on ordinary hardware. Not cloud-dependent, not locked behind paywalls.&lt;/p&gt; &lt;p&gt;The goal: small, customizable, agentic AI that's fully open. Built with open data, trained transparently, AGPL licensed so it stays open forever. Every contributor keeps their copyright.&lt;/p&gt; &lt;p&gt;Right now it's just me working on this, but I'm looking for people who want to build something real together. We're aiming to explore AI safety through transparency, responsible pretraining, and community-driven development, rather than post-training methods that censor or lobotomize the model. These are goals, not finished achievements. We're learning by doing, figuring this out together.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current status:&lt;/strong&gt; Currently using a RWKV-like architecture, but I'm completely open to experimenting with other architectures. Base model trains successfully on consumer hardware the last time I tested, but I've been focused on choosing datasets and haven't tested the training pipeline in a few days (14M parameters, 1000 training steps in ~98 minutes on a single GTX1650TI GPU with 4GB of vram, training actually uses less than 2gb ram/vram combined in its current state). Supervised learning pipeline is working. The model outputs something, but it's not coherent or usable yet. It needs way more data and training time. Agentic fine-tuning layer has module import issues that need fixing. Interactive terminal has protocol errors to debug. Most of the code is AI-generated. I'm a systems administrator, not a developer, so I use AI as a coding tool while I handle the architecture and system design.&lt;/p&gt; &lt;p&gt;This is early development, but the goal is real, usable, agentic models. Not a toy project. The supervised training works, but the agentic components aren't wired up correctly yet, and the base model needs significantly more training. I'm putting this out there for transparency, showing what works and what doesn't, inviting people who want to help solve real problems or just watch the process unfold.&lt;/p&gt; &lt;p&gt;Once we figure out how to produce high quality models, I'd like to make the entire training process as user-friendly and accessible to laypeople as possible.&lt;/p&gt; &lt;p&gt;You don't need to submit code to participate (though contributions are welcome). All contributions are welcome under the project's AGPL license. &lt;/p&gt; &lt;p&gt;If you want to participate but don't like the direction I'm taking it, fork it and do your own thing. That's what open source is for. I maintain the final say in what pull requests do and do not get merged into MY repo of course.&lt;/p&gt; &lt;p&gt;Right now everything is on GitHub. I might set up a Discord or Matrix channel for community discussion later if there's interest. We might also build Jupyter notebooks to make training environments more reproducible, and/or so people could use Kaggle or Colab. We'll see where this goes.&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="http://github.com/bonzupii/Rusty-R2"&gt;github.com/bonzupii/Rusty-R2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bonzupii"&gt; /u/Bonzupii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouwmx4/rustyr2_open_source_ai_you_can_actually_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouwmx4/rustyr2_open_source_ai_you_can_actually_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ouwmx4/rustyr2_open_source_ai_you_can_actually_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T05:18:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ougamx</id>
    <title>gpt-oss-120b on Cerebras</title>
    <updated>2025-11-11T17:53:30+00:00</updated>
    <author>
      <name>/u/Corporate_Drone31</name>
      <uri>https://old.reddit.com/user/Corporate_Drone31</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"&gt; &lt;img alt="gpt-oss-120b on Cerebras" src="https://preview.redd.it/qkygjyoz1o0g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0028679c0cac64d9ce3f55e2a3aad86019108bc" title="gpt-oss-120b on Cerebras" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gpt-oss-120b reasoning CoT on Cerebras be like &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Corporate_Drone31"&gt; /u/Corporate_Drone31 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qkygjyoz1o0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ougamx/gptoss120b_on_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T17:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovcxu0</id>
    <title>[Followup] Qwen3 VL 30b a3b is pure love (or not so much)</title>
    <updated>2025-11-12T18:20:03+00:00</updated>
    <author>
      <name>/u/Njee_</name>
      <uri>https://old.reddit.com/user/Njee_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A couple of days ago I posted here showcasing a video of the webapp I'm currently making. Qwen3-VL 30B-A3B MoE got me back into this project because it amazed how good it is! (Self promotion at the end: My Project is now open sourced and avaialalbe as an easy to deploy docker container...)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Original post:&lt;/strong&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1omr9rc/qwen3_vl_30b_a3b_is_pure_love/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; This project provides an easy way to turn images into structured data. But Qwen3-VL 30B-A3B is not following the promt to not extract data that is not visible from images. Instead it confidently generates fake data that passes formatting checks, making it unsuitable for some fully automated tasks.&lt;/p&gt; &lt;p&gt;Well, actually using the model together with my app made me realize that it is not actually as good as expected. It's still pretty good though, to be honest.&lt;/p&gt; &lt;p&gt;However, I ran into a really interesting problem:&lt;/p&gt; &lt;p&gt;Remember that post from a few months or a year ago, where someone showed an image of a cat with 5 photoshopped legs to a Vision LLM with the question &amp;quot;how many legs&amp;quot;? The answer would always be 4. Simply because the LLM learned cats have 4 legs ‚Üí therefore this cat has 4 legs. It's not actually counting the legs in the image. Instead it sees a cat and answers 4.&lt;/p&gt; &lt;p&gt;Same thing happened to me using Qwen3-VL 30B-A3B.&lt;/p&gt; &lt;p&gt;I tried to extract structured data from chemical containers. Asking for CAS numbers which have a specific format. I specifically asked the model to not write down a CAS number if it's not visible. Any number that does not fit the specific format can not be a CAS number (Maybe thats even the fault - ill try to not specify the format)&lt;/p&gt; &lt;p&gt;Gemini models would respect that instruction. Qwen3 4B would also respect it (Instead it would sometimes misinterpret other numbers as CAS, ignoring the format instructions, which would then result in them not passing formatting checks).&lt;/p&gt; &lt;p&gt;But Qwen3 30B-A3B would simply ignore my prompt to not make up numbers if they are not visible. Even worse: it's smart enough to make up CAS numbers that fit the formatting rules, and the inbuilt checksum. They seem totally legitimate but are still wrong. Hence I wouldn't be able to filter those with simple postprocessing, but would pollute my dataset if id take the extracted data unreviewed.&lt;/p&gt; &lt;p&gt;I've done a detailed comparison of Qwen3-VL 30B-A3B, Qwen3-VL 4B, and Gemini 2.5 Flash in these scenarios. You can find numbers, plots, and methodology here, have a read if you want to.&lt;/p&gt; &lt;p&gt;&lt;a href="https://janbndrf.github.io/Tabtin/#Qwen"&gt;https://janbndrf.github.io/Tabtin/#Qwen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Webapp youre seeing in the Video is now available as an easy-to-deploy Docker container. I called it Tabtin. It works with local models, Google AI Studio, and OpenRouter.&lt;/p&gt; &lt;p&gt;Check it out: &lt;a href="https://github.com/janbndrf/tabtin"&gt;https://github.com/janbndrf/tabtin&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Njee_"&gt; /u/Njee_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovcxu0/followup_qwen3_vl_30b_a3b_is_pure_love_or_not_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovcxu0/followup_qwen3_vl_30b_a3b_is_pure_love_or_not_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovcxu0/followup_qwen3_vl_30b_a3b_is_pure_love_or_not_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T18:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ove1px</id>
    <title>Why Ampere Workstation/Datacenter/Server GPUs are still so expensive after 5+ years?</title>
    <updated>2025-11-12T18:59:35+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, just an small discussion that came to my mind after reading this post &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I feel I guess it does a bit of sense that Ada Workstation/Datacenter/Server are still expensive, as they support fp8, and have way more compute than Ampere, i.e.:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 6000 Ada (48GB), on ebay for about 5000 USD.&lt;/li&gt; &lt;li&gt;RTX 5000 Ada (32GB), on ebay for about 2800-3000 USD.&lt;/li&gt; &lt;li&gt;RTX 4000 Ada (24GB), on ebay for about 1200 USD.&lt;/li&gt; &lt;li&gt;NVIDIA L40 (48GB), on ebay for about 7000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA L40S (48GB), on ebay for about 7000USD.&lt;/li&gt; &lt;li&gt;NVIDIA L4 (24 GB), on ebay for about 2200 to 2800 USD.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While, for Ampere, we have these cases:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX A6000 (48GB), on ebay for about 4000-4500 USD.&lt;/li&gt; &lt;li&gt;RTX A5000 (24GB), on ebay for about 1400 USD.&lt;/li&gt; &lt;li&gt;RTX A4000 (16GB), on ebay for about 750 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A40 (48GB), on ebay for about 4000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A100 (40GB) PCIe, on ebay for about 4000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A100 (80GB) PCIe, on ebay for about 7000 USD.&lt;/li&gt; &lt;li&gt;NVIDIA A10 (24GB), on ebat for about 1800 USD.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So these cards are slower (about half perf compared to Ada), some less VRAM and don't support FP8.&lt;/p&gt; &lt;p&gt;Why are they still so expensive, what do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ove1px/why_ampere_workstationdatacenterserver_gpus_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T18:59:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovc3sj</id>
    <title>Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming</title>
    <updated>2025-11-12T17:50:56+00:00</updated>
    <author>
      <name>/u/lektoq</name>
      <uri>https://old.reddit.com/user/lektoq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"&gt; &lt;img alt="Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming" src="https://preview.redd.it/n5cc10ph5v0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=badc8223252000cebb903a5e944f40eb1d1caa53" title="Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! üëã&lt;/p&gt; &lt;p&gt;I'm a Technical Marketing Engineer at NVIDIA working on Jetson, and we just open-sourced &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui"&gt;&lt;strong&gt;Live VLM WebUI&lt;/strong&gt;&lt;/a&gt; - a tool for testing Vision Language Models locally with real-time video streaming.&lt;/p&gt; &lt;h1&gt;What is it?&lt;/h1&gt; &lt;p&gt;Stream your webcam to any Ollama vision model (or other VLM backends) and get real-time AI analysis overlaid on your video feed. Think of it as a convenient interface for testing vision models in real-time scenarios.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stream live video to the model (not screenshot-by-screenshot)&lt;/li&gt; &lt;li&gt;Show you exactly how fast it's processing frames&lt;/li&gt; &lt;li&gt;Monitor GPU/VRAM usage in real-time&lt;/li&gt; &lt;li&gt;Work across different hardware (PC, Mac, Jetson)&lt;/li&gt; &lt;li&gt;Support multiple backends (Ollama, vLLM, NVIDIA API Catalog, OpenAI)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WebRTC video streaming&lt;/strong&gt; - Low latency, works with any webcam&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama native support&lt;/strong&gt; - Auto-detect &lt;code&gt;http://localhost:11434&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time metrics&lt;/strong&gt; - See inference time, GPU usage, VRAM, tokens/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-backend&lt;/strong&gt; - Also works with vLLM, NVIDIA API Catalog, OpenAI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt; - Linux PC, DGX Spark, Jetson, Mac, WSL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy install&lt;/strong&gt; - &lt;code&gt;pip install live-vlm-webui&lt;/code&gt; and you're done&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0&lt;/strong&gt; - Fully open source, accepting community contributions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üöÄ Quick Start with Ollama&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Make sure Ollama is running with a vision model ollama pull gemma:4b # 2. Install and run pip install live-vlm-webui live-vlm-webui # 3. Open https://localhost:8090 # 4. Select &amp;quot;Ollama&amp;quot; backend and your model &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Use Cases I've Found Helpful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model comparison&lt;/strong&gt; - Testing &lt;code&gt;gemma:4b&lt;/code&gt; vs &lt;code&gt;gemma:12b&lt;/code&gt; vs &lt;code&gt;llama3.2-vision&lt;/code&gt; the same scenes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance benchmarking&lt;/strong&gt; - See actual inference speed on your hardware&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive demos&lt;/strong&gt; - Show people what vision models can do in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time prompt engineering&lt;/strong&gt; - Tune your vision prompt as seeing the result in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt; - Quick feedback loop when working with VLMs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Models That Work Great&lt;/h1&gt; &lt;p&gt;Any Ollama vision model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gemma3:4b&lt;/code&gt;, &lt;code&gt;gemma3:12b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama3.2-vision:11b&lt;/code&gt;, &lt;code&gt;llama3.2-vision:90b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2.5-vl:3b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:7b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:32b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:72b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-vl:2b&lt;/code&gt;, &lt;code&gt;qwen3-vl:4b&lt;/code&gt;, all the way up to &lt;code&gt;qwen3-vl:235b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;llava:7b&lt;/code&gt;, &lt;code&gt;llava:13b&lt;/code&gt;, &lt;code&gt;llava:34b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;minicpm-v:8b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Docker Alternative&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d --gpus all --network host \ ghcr.io/nvidia-ai-iot/live-vlm-webui:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;Planning to add:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Analysis result copy to clipboard, log and export&lt;/li&gt; &lt;li&gt;Model comparison view (side-by-side)&lt;/li&gt; &lt;li&gt;Better prompt templates&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui"&gt;https://github.com/nvidia-ai-iot/live-vlm-webui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui/tree/main/docs"&gt;https://github.com/nvidia-ai-iot/live-vlm-webui/tree/main/docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PyPI:&lt;/strong&gt; &lt;a href="https://pypi.org/project/live-vlm-webui/"&gt;https://pypi.org/project/live-vlm-webui/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think! What features would make this more useful for your workflows? PRs and issues welcome - this is meant to be a community tool.&lt;/p&gt; &lt;blockquote&gt; &lt;h2&gt;A bit of background&lt;/h2&gt; &lt;p&gt;This community has been a huge inspiration for our work. When we launched the &lt;a href="https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/"&gt;Jetson Generative AI Lab&lt;/a&gt;, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; was literally cited as one of the key communities driving the local AI movement.&lt;/p&gt; &lt;p&gt;WebRTC integration for real-time camera streaming into VLMs on Jetson was pioneered by our colleague a while back. It was groundbreaking but tightly coupled to specific setups. Then Ollama came along and with their standardized API we suddenly could serve vision models in a way that works anywhere.&lt;/p&gt; &lt;p&gt;We realized we could take that WebRTC streaming approach and modernize it: make it work with any VLM backend through standard APIs, run on any platform, and give people a better experience than uploading images on Open WebUI and waiting for responses.&lt;/p&gt; &lt;p&gt;So this is kind of the evolution of that original work - taking what we learned on Jetson and making it accessible to the broader local AI community.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Happy to answer any questions about setup, performance, or implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lektoq"&gt; /u/lektoq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5cc10ph5v0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov2eoz</id>
    <title>Kimi K2 Thinking: The One Point Everyone Overlooks, Interleave Thinking</title>
    <updated>2025-11-12T11:13:26+00:00</updated>
    <author>
      <name>/u/Great_Shop_4356</name>
      <uri>https://old.reddit.com/user/Great_Shop_4356</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"&gt; &lt;img alt="Kimi K2 Thinking: The One Point Everyone Overlooks, Interleave Thinking" src="https://b.thumbs.redditmedia.com/SH8DMO2v4iobrEVSkK4oOgtmTNUnl5VcZau2mvcO-iU.jpg" title="Kimi K2 Thinking: The One Point Everyone Overlooks, Interleave Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vxcayce98t0g1.jpg?width=954&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fed91c942279f8311f0cf04673d450065b8b53b1"&gt;https://preview.redd.it/vxcayce98t0g1.jpg?width=954&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fed91c942279f8311f0cf04673d450065b8b53b1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi K2 Thinking supports multi-turn tool calls with interleaved thinking (think ‚Üí call tool ‚Üí reflect ‚Üí call another tool ‚Üí act). While DeepSeek's reasoning models do not support tool calls, which many people overlook. When your workflow or CLI relies on tools (grep, code-run, web_search, etc.), this difference is decisive.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0dbz7jfc7t0g1.jpg?width=2900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9e1863d14935b00f24be50cddd1bdf582862ff85"&gt;DeepSeek's doc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most &amp;quot;reasoning&amp;quot; demos still look like a single blob of chain-of-thought followed by one action. In real agents, the loop needs to be: reason ‚Üí probe with a tool ‚Üí update beliefs ‚Üí take the next action. That feedback loop is where quality jumps, especially for coding and multi-step ops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Great_Shop_4356"&gt; /u/Great_Shop_4356 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov2eoz/kimi_k2_thinking_the_one_point_everyone_overlooks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T11:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov7ogq</id>
    <title>Stop fine-tuning your model for every little thing. You're probably wasting your time.</title>
    <updated>2025-11-12T15:09:34+00:00</updated>
    <author>
      <name>/u/RYTHEIX</name>
      <uri>https://old.reddit.com/user/RYTHEIX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alright, confession time. I just wasted three weeks and a chunk of my compute budget trying to fine-tune a model to answer questions about our internal API. The results were... mediocre at best. It kinda knew the stuff, but it also started hallucinating in new and creative ways, and forgot how to do basic things it was good at before.&lt;/p&gt; &lt;p&gt;It was a massive facepalm moment. Because the solution was way, way simpler.&lt;/p&gt; &lt;p&gt;I feel like &amp;quot;fine-tuning&amp;quot; has become this default magic wand people wave when an LLM isn't perfect. But 80% of the time, what you actually need is RAG (Retrieval-Augmented Generation). Let me break it down without the textbook definitions.&lt;/p&gt; &lt;p&gt;RAG is like giving your AI a cheat sheet. You've got a mountain of internal docs, PDFs, or knowledge that the model wasn't trained on? Don't shove it down the model's throat and hope it digests it. Just keep it in a database (a &amp;quot;vector store,&amp;quot; if we're being fancy) and teach the AI to look things up before it answers. It's the difference between making an intern memorize the entire employee handbook versus just giving them a link to it and telling them to Ctrl+F. It's faster, cheaper, and the AI can't &amp;quot;forget&amp;quot; or misremember the source material. Fine-tuning is for changing the AI's personality or teaching it a new skill. This is when you need the model to fundamentally write or reason differently. You want it to sound like a snarky pirate in every response? Fine-tune. You need it to generate code in a very specific, obscure style that no public model uses? Fine-tune. You're teaching it a whole new task that isn't just &amp;quot;recall information,&amp;quot; but &amp;quot;process information in this new way.&amp;quot;&lt;/p&gt; &lt;p&gt;So, the dumb-simple rule I go by now:&lt;/p&gt; &lt;p&gt;¬∑ Problem:- &amp;quot;The AI doesn't know about X.&amp;quot; -&amp;gt; Use RAG. &amp;quot;The AI doesn't act or sound the way I want.&amp;quot; -&amp;gt; Consider Fine-Tuning.&lt;/p&gt; &lt;p&gt;I learned this the hard way so you don't have to. Fight me in the comments if you disagree, but my wallet is still crying from that fine-tuning bill.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RYTHEIX"&gt; /u/RYTHEIX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov7ogq/stop_finetuning_your_model_for_every_little_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov7ogq/stop_finetuning_your_model_for_every_little_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov7ogq/stop_finetuning_your_model_for_every_little_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T15:09:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ousy0e</id>
    <title>Repeat after me.</title>
    <updated>2025-11-12T02:16:54+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs okay to be getting 45 tokens per second on an AMD card that costs 4 times less than an Nvidia card with same VRAM. Again, it‚Äôs okay. &lt;/p&gt; &lt;p&gt;They‚Äôll get better and better. And if you want 120 toks per second or 160 toks per second, go for it. Pay the premium. But don‚Äôt shove it up people‚Äôs asses. &lt;/p&gt; &lt;p&gt;Thank you. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ousy0e/repeat_after_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ousy0e/repeat_after_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ousy0e/repeat_after_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T02:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov9lug</id>
    <title>Has the USA/EU given up on open weight models?</title>
    <updated>2025-11-12T16:20:46+00:00</updated>
    <author>
      <name>/u/justDeveloperHere</name>
      <uri>https://old.reddit.com/user/justDeveloperHere</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the last couple of months, we only see Chinese models (thank God). I don't remember that in recent months we had any open model that came from the USA/EU. Do you think they changed their tactics and don't care anymore?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/justDeveloperHere"&gt; /u/justDeveloperHere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov9lug/has_the_usaeu_given_up_on_open_weight_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov9lug/has_the_usaeu_given_up_on_open_weight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov9lug/has_the_usaeu_given_up_on_open_weight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T16:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovbssf</id>
    <title>Is Polish better for prompting LLMs? Case study: Logical puzzles</title>
    <updated>2025-11-12T17:40:03+00:00</updated>
    <author>
      <name>/u/Substantial_Sail_668</name>
      <uri>https://old.reddit.com/user/Substantial_Sail_668</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt; &lt;img alt="Is Polish better for prompting LLMs? Case study: Logical puzzles" src="https://b.thumbs.redditmedia.com/iebRkPWZ9kphj1SEpLB2U4o11_A6ic666lV0hdKGkHo.jpg" title="Is Polish better for prompting LLMs? Case study: Logical puzzles" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, recently this article made waves within many LLM communities: &lt;a href="https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals"&gt;https://www.euronews.com/next/2025/11/01/polish-to-be-the-most-effective-language-for-prompting-ai-new-study-reveals&lt;/a&gt; as it claimed (based on a study by researchers from The University of Maryland and Microsoft) that Polish is the best language for prompting LLMs.&lt;/p&gt; &lt;p&gt;So I decided to put it to a small test. I have dug up a couple of books with puzzles and chose some random ones, translated them from the original Polish into English and made them into two Benchmarks. Run it on a bunch of LLMs and here are the results. Not so obvious after all:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iij23lcx2v0g1.png?width=1889&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=382a824c1a766a14f4bad1b86f158c232463dd5f"&gt;https://preview.redd.it/iij23lcx2v0g1.png?width=1889&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=382a824c1a766a14f4bad1b86f158c232463dd5f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On the left you see the results for the original Polish dataset, on the right the English version.&lt;/p&gt; &lt;p&gt;Some quick insights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Overall the &lt;strong&gt;average accuracy&lt;/strong&gt; was a little over 2 percentage points higher on Polish.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grok models:&lt;/strong&gt; Exceptional multilingual consistency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Google models:&lt;/strong&gt; Mixed‚Äîflagship dropped, flash variants improved&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek models:&lt;/strong&gt; Strong English bias&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI models:&lt;/strong&gt; Both ChatGPT-4o and GPT-4o performed worse in Polish&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want me to run the Benchmarks on any other models or do a comparison for a different field, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Sail_668"&gt; /u/Substantial_Sail_668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:40:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovatvf</id>
    <title>Where are all the data centers dumping their old decommissioned GPUs?</title>
    <updated>2025-11-12T17:05:19+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2022, I purchased a lot of Tesla P40s on eBay, but unfortunately, because of their outdated architecture, they are now practically useless for what I want to do. It seems like newer-generation GPUs aren‚Äôt finding their way into consumers' hands. I asked my data center connection and he said they are recycling them, but they‚Äôve always been doing this and we could still get hardware.&lt;/p&gt; &lt;p&gt;With the amount of commercial GPUs in the market right now, you would think there would be some overflow?&lt;/p&gt; &lt;p&gt;I hope to be wrong and suck at resourcing now, any help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov3dkb</id>
    <title>AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs</title>
    <updated>2025-11-12T12:06:13+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"&gt; &lt;img alt="AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs" src="https://external-preview.redd.it/Ym1xdmdzdXRldDBnMR0L-Ennn3ovi4auFkXdc601F67-ibAb8bxVVAjHQXSP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c6b63f193e162db80ace947cf0df279cfbc1423" title="AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://inference.net/blog/project-aella"&gt;https://inference.net/blog/project-aella&lt;/a&gt;&lt;br /&gt; Models: &lt;a href="https://huggingface.co/inference-net"&gt;https://huggingface.co/inference-net&lt;/a&gt;&lt;br /&gt; Visualizer: &lt;a href="https://aella.inference.net/"&gt;https://aella.inference.net&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/du59aiutet0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T12:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
