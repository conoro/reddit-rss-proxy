<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-06T17:23:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pfuilv</id>
    <title>Grok 4 vs Opus 4.5 in a debate about open source models versus proprietary models.</title>
    <updated>2025-12-06T17:17:58+00:00</updated>
    <author>
      <name>/u/StreetlampLeMooose</name>
      <uri>https://old.reddit.com/user/StreetlampLeMooose</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfuilv/grok_4_vs_opus_45_in_a_debate_about_open_source/"&gt; &lt;img alt="Grok 4 vs Opus 4.5 in a debate about open source models versus proprietary models." src="https://external-preview.redd.it/SOG5CwVJQYjiJFEvot0UzlPVIKZvTzH1j6qMJOQ6c8s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b10c7cdaa250635b38109e13dcd60ac8c8292bd5" title="Grok 4 vs Opus 4.5 in a debate about open source models versus proprietary models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StreetlampLeMooose"&gt; /u/StreetlampLeMooose &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.robuttal.com/debates/9b31b73e-e708-4181-bdce-d6d5d5df5456"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfuilv/grok_4_vs_opus_45_in_a_debate_about_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfuilv/grok_4_vs_opus_45_in_a_debate_about_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T17:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pez5ch</id>
    <title>Why do LLM response formats often use &lt;| |&gt; (as in &lt;|message|&gt;) instead of &lt;message&gt;, and why do they use &lt;|end|&gt; instead of &lt;/message&gt;?</title>
    <updated>2025-12-05T16:14:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt; &lt;img alt="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" src="https://preview.redd.it/5e5ir2zlte5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b47f1bd3dabcdabf34fcf757aaea013f0a0c73" title="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I had to guess, I'd assume it's tokenization because &amp;quot;&amp;lt;|&amp;quot; is not a very commonly occurring pattern in pre-training, which allows devs to make &amp;quot;&amp;lt;|message|&amp;gt;&amp;quot; a single token.&lt;/p&gt; &lt;p&gt;That being said, the &amp;lt;|end|&amp;gt; is still a bit disorienting, at least to me reading as a human. You can see that the &amp;lt;|start|&amp;gt; block ends with another &amp;lt;|start|&amp;gt; block, but the &amp;lt;|message|&amp;gt; block ends in a &amp;lt;|end|&amp;gt; block.&lt;/p&gt; &lt;p&gt;This image is from &lt;a href="https://github.com/openai/harmony"&gt;openai's harmony response template&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5e5ir2zlte5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfpwum</id>
    <title>What can I run on my PC?</title>
    <updated>2025-12-06T13:57:02+00:00</updated>
    <author>
      <name>/u/Motor_Armadillo_7317</name>
      <uri>https://old.reddit.com/user/Motor_Armadillo_7317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;16g ddr4 , GTX 1660 ti, Ryzen 5500 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Armadillo_7317"&gt; /u/Motor_Armadillo_7317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpwum/what_can_i_run_on_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpwum/what_can_i_run_on_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpwum/what_can_i_run_on_my_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T13:57:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfjg34</id>
    <title>How is the agent system inside Cursor (or similar IDE agent workflows) actually designed?</title>
    <updated>2025-12-06T07:32:08+00:00</updated>
    <author>
      <name>/u/v0k3r</name>
      <uri>https://old.reddit.com/user/v0k3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to understand how modern AI-powered IDEs like Cursor structure their internal agent systems.&lt;/p&gt; &lt;p&gt;From the outside, it looks like the tool is able to:&lt;br /&gt; – break a user request into multiple steps,&lt;br /&gt; – apply patches to the codebase,&lt;br /&gt; – run commands (install deps, start dev server),&lt;br /&gt; – detect errors,&lt;br /&gt; – and then automatically fix them in a loop.&lt;/p&gt; &lt;p&gt;is it?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a chain of multiple agents calling each other,&lt;/li&gt; &lt;li&gt;a single agent with tool-calling and a feedback loop,&lt;/li&gt; &lt;li&gt;or some kind of planner–executor architecture?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How do they coordinate step-by-step tasks?&lt;br /&gt; Is there a public technical breakdown of how this “agentic IDE” architecture works?&lt;/p&gt; &lt;p&gt;I’d really appreciate a detailed explanation or any deep-dive resources.&lt;/p&gt; &lt;p&gt;Maybe links or explanation here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v0k3r"&gt; /u/v0k3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T07:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfe5ou</id>
    <title>Open Unified TTS - Turn any TTS into an unlimited-length audio generator</title>
    <updated>2025-12-06T02:38:57+00:00</updated>
    <author>
      <name>/u/SouthernFriedAthiest</name>
      <uri>https://old.reddit.com/user/SouthernFriedAthiest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built an open-source TTS proxy that lets you generate unlimited-length audio from local backends without hitting their length limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Most local TTS models break after 50-100 words. Voice clones are especially bad - send a paragraph and you get gibberish, cutoffs, or errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The solution:&lt;/strong&gt; Smart chunking + crossfade stitching. Text splits at natural sentence boundaries, each chunk generates within model limits, then seamlessly joins with 50ms crossfades. No audible seams.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demos:&lt;/strong&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/intro.mp4"&gt;30-second intro&lt;/a&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/live_demo.mp4"&gt;4-minute live demo&lt;/a&gt; showing it in action&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; - OpenAI TTS-compatible API (drop-in for OpenWebUI, SillyTavern, etc.) - Per-voice backend routing (send &amp;quot;morgan&amp;quot; to VoxCPM, &amp;quot;narrator&amp;quot; to Kokoro) - Works with any TTS that has an API endpoint&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tested with:&lt;/strong&gt; Kokoro, VibeVoice, OpenAudio S1-mini, FishTTS, VoxCPM, MiniMax TTS, Chatterbox, Higgs Audio, Kyutai/Moshi&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/loserbcc/open-unified-tts"&gt;https://github.com/loserbcc/open-unified-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed with Claude and Z.ai (with me in the passenger seat).&lt;/p&gt; &lt;p&gt;Feedback welcome - what backends should I add adapters for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouthernFriedAthiest"&gt; /u/SouthernFriedAthiest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T02:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexnfp</id>
    <title>LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering</title>
    <updated>2025-12-05T15:15:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt; &lt;img alt="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" src="https://external-preview.redd.it/wKVXYkAgQd2YCzTWH9wJHT9a9O4yMSOT8w5RQDj-cGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=266d079c79f26252dc4def3cc7e476d0209bb0af" title="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pes3pu</id>
    <title>Basketball AI with RF-DETR, SAM2, and SmolVLM2</title>
    <updated>2025-12-05T10:53:12+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt; &lt;img alt="Basketball AI with RF-DETR, SAM2, and SmolVLM2" src="https://external-preview.redd.it/N2czYjlxanU4ZDVnMZ78lEX-DYraHupkrsvdafpxwsSm-SfqaN6z7l9OZr1B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aab29ff74cd044468cb8bd288eeaf647b5329d32" title="Basketball AI with RF-DETR, SAM2, and SmolVLM2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;resources: &lt;a href="https://www.youtube.com/watch?v=yGQb9KkvQ1Q"&gt;youtube&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;code&lt;/a&gt;, &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- player and number detection with RF-DETR&lt;/p&gt; &lt;p&gt;- player tracking with SAM2&lt;/p&gt; &lt;p&gt;- team clustering with SigLIP, UMAP and K-Means&lt;/p&gt; &lt;p&gt;- number recognition with SmolVLM2&lt;/p&gt; &lt;p&gt;- perspective conversion with homography&lt;/p&gt; &lt;p&gt;- player trajectory correction&lt;/p&gt; &lt;p&gt;- shot detection and classification&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k6kmogju8d5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T10:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfssoo</id>
    <title>"Router mode is experimental" | llama.cpp now has a router mode and I didn't know.</title>
    <updated>2025-12-06T16:06:19+00:00</updated>
    <author>
      <name>/u/charmander_cha</name>
      <uri>https://old.reddit.com/user/charmander_cha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did anyone else know that llama.cpp has a &amp;quot;router mode&amp;quot;? try out ! it's cool.&lt;/p&gt; &lt;p&gt;Little big history (you can ignore): &lt;/p&gt; &lt;p&gt;I've been trying to keep up with the updates on this sub and ComfyUI, but it's been a bit difficult to stay updated. From what I've observed, there don't seem to be any posts talking about this llama.cpp feature.&lt;/p&gt; &lt;p&gt;Because of this, I decided to share my experience:&lt;/p&gt; &lt;p&gt;I'm using llama.cpp, but I haven't been able to compile it with ROCm support — it always gives me trouble when I try to use it.&lt;/p&gt; &lt;p&gt;I also don't use Docker. Every time I try, it doesn't recognize my GPU. I've tried several times to configure it to detect the hardware, but I just can't get it to work.&lt;/p&gt; &lt;p&gt;That's why I've always preferred Ollama for its ease of use. Recently, however, I realized that the GGUF models I want to use are available on Hugging Face and not on Ollama, and when I try to install them manually, I always get some incompatibility error.&lt;/p&gt; &lt;p&gt;I then decided to compile llama.cpp with Vulkan support, which is more universal and would have a better chance of working on my AMD Radeon RX 7600 XT GPU. Fortunately, the compilation was successful and I can now run some models.&lt;/p&gt; &lt;p&gt;However, I couldn't run Qwen-Next, which was frustrating. I thought my PC would run it without problems, since I can run the OpenAI quantized 120B model, so I imagined they would be similar in demand.&lt;/p&gt; &lt;p&gt;Despite this, I managed to run Qwen3-VL-8B-Instruct via Vulkan. When running the llama-serve command, a warning appeared about &amp;quot;router mode,&amp;quot; which basically allows switching between models directly through the interface generated on port 8080.&lt;/p&gt; &lt;p&gt;All this &amp;quot;lore&amp;quot; serves to contextualize my configuration and the challenges I faced using Pop!_OS, and perhaps it can help others who are in similar situations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/charmander_cha"&gt; /u/charmander_cha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfssoo/router_mode_is_experimental_llamacpp_now_has_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfpw7v</id>
    <title>Function calling Finetuners?</title>
    <updated>2025-12-06T13:56:14+00:00</updated>
    <author>
      <name>/u/zelkovamoon</name>
      <uri>https://old.reddit.com/user/zelkovamoon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huggingface is full of finetunes, merges, etc; typically if you open a list of these for a given model - Qwen3, GPT-OSS, etc; you'll get a bunch of random models with a bunch of random names, it's not very searchable. I'm looking for finetunes / LoRas for tool calling / function performance improvement, and it just seems hard to find anything that unambiguously is trained for this and provides any sort of data about how much better it does.&lt;/p&gt; &lt;p&gt;I'm going to keep scrolling and eyeballing, but that *DOES* suck. So, I'm also going to ask the community - are there known good providers of tool / function calling LoRas? Finetunes? Who? ToolMaster69? Give names and specifics if you have them, please.&lt;/p&gt; &lt;p&gt;P.S. Dont tell me to train my own, that's not the question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zelkovamoon"&gt; /u/zelkovamoon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpw7v/function_calling_finetuners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpw7v/function_calling_finetuners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpw7v/function_calling_finetuners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibu4</id>
    <title>What agentic capabilities are you guys using llms for?</title>
    <updated>2025-12-06T06:23:46+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just curious&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfjvry</id>
    <title>Why so few benchmarks with the pcie p2p patches kernel module?</title>
    <updated>2025-12-06T07:59:49+00:00</updated>
    <author>
      <name>/u/unfortunate_jargon</name>
      <uri>https://old.reddit.com/user/unfortunate_jargon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of inference benchmarks on here, but I'm consistently baffled why it seems that nearly no one is using the various patched Nvidia kernel modules available which enabled pcie p2p.&lt;/p&gt; &lt;p&gt;It reduces the latency between RTX 30/40/50 cards by an order of magnitude, and makes tensor and export parallelism highly viable (leading to _drastically_ improved throughput)&lt;/p&gt; &lt;p&gt;Is this common knowledge around here? If not, then I highly encourage doing some testing with your multi-RTX GPU systems, because running without it is handicapping your performance by multiples.&lt;/p&gt; &lt;p&gt;edit: tinycorp was the first author I'm aware of that released a patch that was widely circulated, but others have forked and improved it, as well as rebasing against newer versions of the kernel module. here's an example I just pulled from chatgpt: &lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unfortunate_jargon"&gt; /u/unfortunate_jargon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T07:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfqmqu</id>
    <title>Trying to ship local RAG to both android and iOS and feeling disheartened</title>
    <updated>2025-12-06T14:30:00+00:00</updated>
    <author>
      <name>/u/chreezus</name>
      <uri>https://old.reddit.com/user/chreezus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a fullstack developer by experience, so forgive me if this is obvious. I've built a number of RAG applications for different industries (finance, government, etc). I recently got into trying to run these same RAG apps fully on-device (government agencies love privacy). I've been playing with Llama-3.2-3B with 4-bit quantization. I was able to get this running on IOS with CoreML after a ton of work (again, I'm not an AI or ML expert). Now I’m looking at Android and it feels pretty daunting: different hardware, multiple ABIs, different runtimes (TFLite / ExecuTorch / llama.cpp builds), and I’m worried I’ll end up with a totally separate pipeline just to get comparable behavior.&lt;/p&gt; &lt;p&gt;For folks who’ve shipped cross-platform on-device RAG:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is there a sane way to target both iOS and Android without maintaining two totally separate build pipelines?&lt;/li&gt; &lt;li&gt;What are you using for the local vector database that works well on mobile? (SQLite-vec? Chroma? Custom C++?)&lt;/li&gt; &lt;li&gt;How do you handle updates to the source data. At some regular interval, I would need to rebuild the embeddings and ship them to device, essentially &amp;quot;deployments&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chreezus"&gt; /u/chreezus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqmqu/trying_to_ship_local_rag_to_both_android_and_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqmqu/trying_to_ship_local_rag_to_both_android_and_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqmqu/trying_to_ship_local_rag_to_both_android_and_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfsx8x</id>
    <title>Running LLM over RAM</title>
    <updated>2025-12-06T16:11:54+00:00</updated>
    <author>
      <name>/u/Bakkario</name>
      <uri>https://old.reddit.com/user/Bakkario</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello community,&lt;/p&gt; &lt;p&gt;I am currently running local LLMs using my RTX 3060 with 6GB VRam and I get about 20ish tokens per second using 7b models which is not bad for my use cases. I get this took/sec using ollama but LMDtudio gives less when using GGUF&lt;/p&gt; &lt;p&gt;I want to take this a nudge and giving that this is a laptop I cannot upgrade my GPU. So, I am thinking of upgrading my RAM and the budget I have is for about 32GB @ 3200mhz. Is this going to help me run larger models like 30b models? If I go further to 64 GB ram would it run better? I want my tokens to be not less than 20tok/sec if possible bare minimum lets say 15tok/sec&lt;/p&gt; &lt;p&gt;Would that help my inference if I offloaded some larger models and can run something that is about 30b models? I want to use it for generating code and agentic AI development locally instead of relying on APIs. &lt;/p&gt; &lt;p&gt;Any input?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bakkario"&gt; /u/Bakkario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsx8x/running_llm_over_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsx8x/running_llm_over_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsx8x/running_llm_over_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:11:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfbo6o</id>
    <title>Is there any model truly open, that you can train yourself from zero?</title>
    <updated>2025-12-06T00:38:47+00:00</updated>
    <author>
      <name>/u/puthre</name>
      <uri>https://old.reddit.com/user/puthre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, is there any open source LLM that comes with all the data it was trained on and all the instructions that you can replicate yourself assuming you have access to the necesary hardware? And if not why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puthre"&gt; /u/puthre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T00:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfcatm</id>
    <title>VoxCPM 1.5B just got released!</title>
    <updated>2025-12-06T01:07:54+00:00</updated>
    <author>
      <name>/u/Hefty_Wolverine_553</name>
      <uri>https://old.reddit.com/user/Hefty_Wolverine_553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt; &lt;img alt="VoxCPM 1.5B just got released!" src="https://external-preview.redd.it/MIb2iimHkfYqVDgmZztu-h5tz8yFqiAztGcy6umK7o8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d7fc02333ca119537bfd3af70a4c74b40c2e98" title="VoxCPM 1.5B just got released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just visiting the &lt;a href="https://github.com/OpenBMB/VoxCPM"&gt;GitHub page&lt;/a&gt; today (setting up a FastAPI TTS server) when I realized that they released a new version of the VoxCPM model. The original VoxCPM-0.5B was already very good in my testing, but this model looks like a straight improvement (it's still a 0.5B model, despite the rather confusing naming scheme).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;VoxCPM&lt;/th&gt; &lt;th align="left"&gt;VoxCPM1.5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Audio VAE Sampling Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16kHz&lt;/td&gt; &lt;td align="left"&gt;44.1kHz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LM Token Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12.5Hz&lt;/td&gt; &lt;td align="left"&gt;6.25Hz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Patch Size&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SFT Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LoRA Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They also added fine-tuning support as well as a guide &lt;a href="https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md"&gt;https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example output: &lt;a href="https://voca.ro/147qPjN98F6g"&gt;https://voca.ro/147qPjN98F6g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Wolverine_553"&gt; /u/Hefty_Wolverine_553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pftdc6</id>
    <title>Best benchmark website</title>
    <updated>2025-12-06T16:30:46+00:00</updated>
    <author>
      <name>/u/AccomplishedStory327</name>
      <uri>https://old.reddit.com/user/AccomplishedStory327</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which website do you use to see benchmark stats of different models, apart from using your own suite?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AccomplishedStory327"&gt; /u/AccomplishedStory327 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pftdc6/best_benchmark_website/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfrqvh</id>
    <title>Multi-directional ablation with self-organizing maps - anyone tried it yet?</title>
    <updated>2025-12-06T15:20:28+00:00</updated>
    <author>
      <name>/u/IllllIIlIllIllllIIIl</name>
      <uri>https://old.reddit.com/user/IllllIIlIllIllllIIIl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran across this preprint the other day: &lt;/p&gt; &lt;p&gt;Piras, Giorgio, et al. &amp;quot;&lt;a href="https://arxiv.org/abs/2511.08379"&gt;SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models.&lt;/a&gt;&amp;quot; arXiv preprint arXiv:2511.08379 (2025).&lt;/p&gt; &lt;p&gt;They have published their code here: &lt;a href="https://github.com/pralab/som-refusal-directions"&gt;https://github.com/pralab/som-refusal-directions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically rather than the usual difference of means method for ablating a single refusal direction, they train a SOM to learn a refusal manifold and use Bayesian Optimization to determine the best subset of k directions to ablate. They got some pretty impressive results. &lt;/p&gt; &lt;p&gt;They only implemented the method for a handful of smaller models (nothing bigger than 14B), probably because the BO step is rather expensive. But it shouldn't be that hard to extend their code to support new models. &lt;/p&gt; &lt;p&gt;I was able to run the full pipeline on Qwen2.5-3B and replicate the results on that. I started extending the code to support gpt-oss-20b, but the further I got, the more I realized I'm too GPU poor to succeed in running it on that. &lt;/p&gt; &lt;p&gt;Any of you GPU rich bastards try this out on a larger model yet, or want to give it a shot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllllIIlIllIllllIIIl"&gt; /u/IllllIIlIllIllllIIIl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T15:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfqm0y</id>
    <title>Speed of DeepSeek with RAM offload</title>
    <updated>2025-12-06T14:29:05+00:00</updated>
    <author>
      <name>/u/vhthc</name>
      <uri>https://old.reddit.com/user/vhthc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 96GB VRAM. By far not enough to run DeepSeek 3.x - bit I could upgrade my RAM so I can have the active layers on the GPU and the rest in system RAM. Yeah the RAM prices are a catastrophe but I need to run such a large model, and I don’t want to use cloud - this is locallama!&lt;/p&gt; &lt;p&gt;Has anyone tried this? What speed can I expect with a 64kb context length in prompt processing and tokens per second?&lt;/p&gt; &lt;p&gt;It would be quite the investment so if anyone has real world data that would be great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vhthc"&gt; /u/vhthc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfsntn</id>
    <title>convert: support Mistral 3 Large MoE by ngxson · Pull Request #17730 · ggml-org/llama.cpp</title>
    <updated>2025-12-06T16:00:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt; &lt;img alt="convert: support Mistral 3 Large MoE by ngxson · Pull Request #17730 · ggml-org/llama.cpp" src="https://external-preview.redd.it/YXlCrbFuGSaJRzk-d-1JftjUbGO215ldNJVTXMLJQi4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4507263a618891c23289c740acf9be9cc8bee393" title="convert: support Mistral 3 Large MoE by ngxson · Pull Request #17730 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can now download GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF"&gt;https://huggingface.co/bartowski/mistralai_Mistral-Large-3-675B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;but can you run it...? &lt;/p&gt; &lt;p&gt;(that another PR is &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17744"&gt;https://github.com/ggml-org/llama.cpp/pull/17744&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17730"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfsntn/convert_support_mistral_3_large_moe_by_ngxson/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T16:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfq0kd</id>
    <title>PaperDebugger: the Best Overleaf Companion!</title>
    <updated>2025-12-06T14:01:42+00:00</updated>
    <author>
      <name>/u/NuoJohnChen</name>
      <uri>https://old.reddit.com/user/NuoJohnChen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt; &lt;img alt="PaperDebugger: the Best Overleaf Companion!" src="https://b.thumbs.redditmedia.com/LSzFW-bVRkmLrP-afZdLmy0DNmjvCz1MK2UnMO8aqLo.jpg" title="PaperDebugger: the Best Overleaf Companion!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chrome/APP Store: &lt;a href="https://www.paperdebugger.com/"&gt;https://www.paperdebugger.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.02589"&gt;https://arxiv.org/abs/2512.02589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/PaperDebugger/PaperDebugger"&gt;https://github.com/PaperDebugger/PaperDebugger&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enhancer: &lt;a href="https://huggingface.co/Xtra-Computing/XtraGPT-7B"&gt;https://huggingface.co/Xtra-Computing/XtraGPT-7B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;An NUS team just released &amp;quot;PaperDebugger&amp;quot;: an in-editor system that uses multiple agents (Reviewer, Researcher, Scorer) to rewrite and critique papers in real-time within Overleaf. Just simply select a rough section, and it launches the full pipeline. &lt;/p&gt; &lt;p&gt;Direct Integration: No copy-pasting. It patches the document with Git-style before/after diffs.&lt;/p&gt; &lt;p&gt;Deep Research: Can pull arXiv papers, summarize them, and generate comparison tables inline.&lt;/p&gt; &lt;p&gt;Tech Stack: Uses an MCP toolchain and Kubernetes to scale the agent reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NuoJohnChen"&gt; /u/NuoJohnChen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pfq0kd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfl0d8</id>
    <title>How big an open source model can I run on 128 GB unified memory?</title>
    <updated>2025-12-06T09:13:03+00:00</updated>
    <author>
      <name>/u/nameless_me</name>
      <uri>https://old.reddit.com/user/nameless_me</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just took delivery of a Minisforum MS-S1 with AMD Ryzen Ai Max+ 395 cpu, 128 GB unified memory architecture and AMD Radeon 8060S Graphics. In the BIOS the UDMA memory for the iGPU is set to 96 GB. Running a Debian Linux terminal in WSL 2, I downloaded and ran ollama which works fine.&lt;/p&gt; &lt;p&gt;Trying a Deepseek-r1:70b model, it refused to load in ollama. I checked a few sources which ended saying this &amp;quot;&lt;strong&gt;DeepSeek-R1-70B INT4 GGUF still requires ~55–60 GB VRAM equivalent&lt;/strong&gt;. &lt;strong&gt;You cannot run this model on a single consumer APU&lt;/strong&gt;, even with “128 GB unified memory”.&lt;/p&gt; &lt;p&gt;Is the above true? What is the largest LLM model I can run reasonably on this computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nameless_me"&gt; /u/nameless_me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiar0</id>
    <title>Qwen3-TTS</title>
    <updated>2025-12-06T06:21:51+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model’s context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
