<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-06T15:51:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1miytb3</id>
    <title>Ok, we get a lobotobot. Great.</title>
    <updated>2025-08-06T08:09:18+00:00</updated>
    <author>
      <name>/u/Reno0vacio</name>
      <uri>https://old.reddit.com/user/Reno0vacio</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miytb3/ok_we_get_a_lobotobot_great/"&gt; &lt;img alt="Ok, we get a lobotobot. Great." src="https://preview.redd.it/81b7dbwexchf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fd0e87382bebbd8aefaa35209dcc558bee3e45d" title="Ok, we get a lobotobot. Great." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Red pill is often considered part of the manosphere, which is a misogynistic ideology.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hmm. Great views on manosphere üëå&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reno0vacio"&gt; /u/Reno0vacio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/81b7dbwexchf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miytb3/ok_we_get_a_lobotobot_great/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miytb3/ok_we_get_a_lobotobot_great/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T08:09:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1migo6d</id>
    <title>I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!</title>
    <updated>2025-08-05T18:10:18+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"&gt; &lt;img alt="I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!" src="https://preview.redd.it/7e3v67opr8hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c78bd2d594d80d839e43d136cedfee6e05b2b464" title="I FEEL SO SAFE! THANK YOU SO MUCH OPENAI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It also lacks all general knowledge and is terrible at coding compared to the same sized GLM air, what is the use case here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7e3v67opr8hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1migo6d/i_feel_so_safe_thank_you_so_much_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T18:10:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1miupht</id>
    <title>GPT -OSS is heavily trained on benchmark. scored rank 34 on simplebench worse than grok 2</title>
    <updated>2025-08-06T04:02:50+00:00</updated>
    <author>
      <name>/u/mvp525</name>
      <uri>https://old.reddit.com/user/mvp525</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miupht/gpt_oss_is_heavily_trained_on_benchmark_scored/"&gt; &lt;img alt="GPT -OSS is heavily trained on benchmark. scored rank 34 on simplebench worse than grok 2" src="https://preview.redd.it/cbd2wyrfpbhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=994e72edd24558bb078da5397d66ecabc0d9a45a" title="GPT -OSS is heavily trained on benchmark. scored rank 34 on simplebench worse than grok 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mvp525"&gt; /u/mvp525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cbd2wyrfpbhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miupht/gpt_oss_is_heavily_trained_on_benchmark_scored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miupht/gpt_oss_is_heavily_trained_on_benchmark_scored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T04:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj3wks</id>
    <title>The missing conversation: Is GPT-OSS by OpenAI a good architecture?</title>
    <updated>2025-08-06T12:55:03+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With GPT-OSS being Apache licensed, could all the big players take the current model and continue fine tuning more aggressively to basically create a new model but not from scratch? &lt;/p&gt; &lt;p&gt;It seems like the architecture might be, but safety tuning has really marred the perception of it. I am sure DeepSeek, Qwen, Mistral are at least studying it to see where their next model might take advantage of the design‚Ä¶ but perhaps a new or small player can use it to step up to the game with a more performant and complacent model.&lt;/p&gt; &lt;p&gt;I saw one post so far that just compared‚Ä¶ it didn‚Äôt evaluate. What do you think? Does the architecture add anything to the conversation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj3wks/the_missing_conversation_is_gptoss_by_openai_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj3wks/the_missing_conversation_is_gptoss_by_openai_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj3wks/the_missing_conversation_is_gptoss_by_openai_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T12:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj1il5</id>
    <title>Building a fully local NSFW-friendly endless visual novel RP world (like Dreammir.ai but local)</title>
    <updated>2025-08-06T10:59:20+00:00</updated>
    <author>
      <name>/u/Sad-Instance-3916</name>
      <uri>https://old.reddit.com/user/Sad-Instance-3916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all!&lt;/p&gt; &lt;p&gt;I want to create a fully offline, anime-styled, NSFW-friendly RP setup inspired by Dreammir.ai. The goal is a persistent world with 2-3 party members, real memory, flexible dialogue (RP-heavy + possibility to speak out of character), and dynamic scene visuals based on context. All local.&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4090, 64GB RAM&lt;/li&gt; &lt;li&gt;SillyTavern in Novel mode&lt;/li&gt; &lt;li&gt;Planning to run a LLaMA 3 finetune like Nymeria-15B or Magnum-Twilight-12B via KoboldCPP&lt;/li&gt; &lt;li&gt;Images via SD (Flux, SDXL, LoRAs), maybe ComfyUI for composition&lt;/li&gt; &lt;li&gt;Idea: cache generated scenes based on keys like [location + character + outfit] e.g LLM should output simple scene metadata for : { &amp;quot;narration&amp;quot;: &amp;quot;We arrive at the tavern. The elf sits down next to us.&amp;quot;, &amp;quot;location&amp;quot;: &amp;quot;tavern_interior&amp;quot;, &amp;quot;characters_in_scene&amp;quot;: [ {&amp;quot;name&amp;quot;: &amp;quot;Elf Mage&amp;quot;, &amp;quot;pose&amp;quot;: &amp;quot;sitting&amp;quot;, &amp;quot;outfit&amp;quot;: &amp;quot;travel cloak&amp;quot;}, {&amp;quot;name&amp;quot;: &amp;quot;Beastfolk Scout&amp;quot;, &amp;quot;pose&amp;quot;: &amp;quot;standing&amp;quot;, &amp;quot;outfit&amp;quot;: &amp;quot;leather armor&amp;quot;} ] }&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I was thinking of writing a Node.js service to parse this and call SD for background + sprite generation, cache results based on metadata keys, then display over the Novel UI if something changed.&lt;/p&gt; &lt;p&gt;Dialogues drive everything. Either I:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;say something,&lt;/li&gt; &lt;li&gt;do something,&lt;/li&gt; &lt;li&gt;or do nothing and let the LLM decide what happens next.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;But maybe I‚Äôm overcomplicating it? I have no expirience with SillyTavern. Are there ST plugins that already do something similar? Also...&lt;/p&gt; &lt;p&gt;I see a lot of people here using Gemini, Deepseek, GLM-4.5, Mistral etc. From what I understand, these are more general-use models and heavily censored (and as far as I read, jailbreak often is not a consistant solution). Am I missing something? Are open LLaMA-based RP models like Nymeria/Magnum/Stheno outdated? And if anyone has any ideas about which stack would be best suited (plugins, models), I would love to hear them. It's also possible that I'm asking for too much and that such a result cannot be achieved for free, in which case I would like to know which is the cheapest paid stack available.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Instance-3916"&gt; /u/Sad-Instance-3916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mj1il5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj1il5/building_a_fully_local_nsfwfriendly_endless/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj1il5/building_a_fully_local_nsfwfriendly_endless/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T10:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mizz4c</id>
    <title>I distilled Qwen3-Coder-480B into Qwen3-Coder-30b-A3B-Instruct</title>
    <updated>2025-08-06T09:25:31+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mizz4c/i_distilled_qwen3coder480b_into/"&gt; &lt;img alt="I distilled Qwen3-Coder-480B into Qwen3-Coder-30b-A3B-Instruct" src="https://b.thumbs.redditmedia.com/otdr2FbIcEHBACfeKNFLe7Iw0h8Ps5qbWOOlVN92PRY.jpg" title="I distilled Qwen3-Coder-480B into Qwen3-Coder-30b-A3B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems to function better than stock Qwen-3-coder-30b-Instruct for UI/UX in my testing. I distilled it using SVD and applied the extracted Lora to the model. In the simulated OS things like the windows can fullscreen but cant minimize and the terminal is not functional. Still pretty good IMO considering its a 30b. All code was 1 or 2 shot. Currently only have a Q8_0 quant up but will have more up soon. If you would like to see the distillation scripts let me know and I can post them to github. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill"&gt;https://huggingface.co/BasedBase/Qwen3-Coder-30B-A3B-Instruct-Distill&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mizz4c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mizz4c/i_distilled_qwen3coder480b_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mizz4c/i_distilled_qwen3coder480b_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T09:25:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj2c73</id>
    <title>Unpopular opinion: The GPT OSS models will be more popular commercially precisely because they are safemaxxed.</title>
    <updated>2025-08-06T11:41:54+00:00</updated>
    <author>
      <name>/u/ariagloris</name>
      <uri>https://old.reddit.com/user/ariagloris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After reading quite a few conversations about OpenAI's safemaxxing approach to their new models. For personal use, yes, the new models may indeed feel weaker or more restricted compared to other offerings currently available. I feel like many people are missing a key point:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;For commercial use&lt;/strong&gt;, these models are often superior for many applications.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They offer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clear hardware boundaries (efficient use of single H100 GPUs), giving you predictable costs.&lt;/li&gt; &lt;li&gt;Safety and predictability: It's crucial if you're building a product directly interacting with the model; you don't want the risk of it generating copyrighted, inappropriate, or edgy content.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;While it's not what I would want for my self hosted models, I would make the argument that this level of safemaxxing and hardware saturation is actually impressive, and is a boon for real world applications that are not related to agentic coding or private personal assistants etc. Just don't be surprised if it gets wide adoption compared to other amazing models that do deserve greater praise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ariagloris"&gt; /u/ariagloris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2c73/unpopular_opinion_the_gpt_oss_models_will_be_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2c73/unpopular_opinion_the_gpt_oss_models_will_be_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2c73/unpopular_opinion_the_gpt_oss_models_will_be_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T11:41:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1miodyp</id>
    <title>GPT-OSS 120B and 20B feel kind of‚Ä¶ bad?</title>
    <updated>2025-08-05T23:07:10+00:00</updated>
    <author>
      <name>/u/SlackEight</name>
      <uri>https://old.reddit.com/user/SlackEight</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After feeling horribly underwhelmed by these models, the more I look around, the more I‚Äôm noticing reports of excessive censorship, high hallucination rates, and lacklustre performance. &lt;/p&gt; &lt;p&gt;Our company builds character AI systems. After plugging both of these models into our workflows and running our eval sets against them, we are getting some of the worst performance we‚Äôve ever seen in the models we‚Äôve tested (120B performing marginally better than Qwen 3 32B, and both models getting demolished by Llama 4 Maverick, K2, DeepSeek V3, and even GPT 4.1 mini)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlackEight"&gt; /u/SlackEight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miodyp/gptoss_120b_and_20b_feel_kind_of_bad/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miodyp/gptoss_120b_and_20b_feel_kind_of_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miodyp/gptoss_120b_and_20b_feel_kind_of_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T23:07:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj7pny</id>
    <title>Just when you thought Qwen was done...</title>
    <updated>2025-08-06T15:27:09+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;still has something up its sleeve&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7pny/just_when_you_thought_qwen_was_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj7i8b</id>
    <title>Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507</title>
    <updated>2025-08-06T15:19:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"&gt; &lt;img alt="Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507" src="https://external-preview.redd.it/qnbu8JjU7mHQuWHQ9TuIRVPsrSeMOsXoVScIkOFk5WY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c19ef5b4c94d500ea5894d87dd560239a58f5832" title="Qwen3-4B-Thinking-2507 and Qwen3-4B-Instruct-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new models from Qwen:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a952795d361291f782aa4472c9751094bdcf7bae"&gt;https://preview.redd.it/fnkijdpn4fhf1.jpg?width=1920&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a952795d361291f782aa4472c9751094bdcf7bae&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of Qwen3-4B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning. We are pleased to introduce &lt;strong&gt;Qwen3-4B-Thinking-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markedly better general capabilities&lt;/strong&gt;, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 256K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks.&lt;/p&gt; &lt;p&gt;We introduce the updated version of the &lt;strong&gt;Qwen3-4B non-thinking mode&lt;/strong&gt;, named &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significant improvements&lt;/strong&gt; in general capabilities, including &lt;strong&gt;instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Substantial gains&lt;/strong&gt; in long-tail knowledge coverage across &lt;strong&gt;multiple languages&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Markedly better alignment&lt;/strong&gt; with user preferences in &lt;strong&gt;subjective and open-ended tasks&lt;/strong&gt;, enabling more helpful responses and higher-quality text generation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced capabilities&lt;/strong&gt; in &lt;strong&gt;256K long-context understanding&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUFs&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF"&gt;https://huggingface.co/lmstudio-community/Qwen3-4B-Thinking-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF"&gt;https://huggingface.co/lmstudio-community/Qwen3-4B-Instruct-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7i8b/qwen34bthinking2507_and_qwen34binstruct2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:19:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1miezct</id>
    <title>üöÄ OpenAI released their open-weight models!!!</title>
    <updated>2025-08-05T17:09:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"&gt; &lt;img alt="üöÄ OpenAI released their open-weight models!!!" src="https://preview.redd.it/1yckal6wg8hf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc6b586f5d511d8c0e30969100e707e6e00a1815" title="üöÄ OpenAI released their open-weight models!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to the gpt-oss series, OpenAI‚Äôs open-weight models designed for powerful reasoning, agentic tasks, and versatile developer use cases.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing two flavors of the open models:&lt;/p&gt; &lt;p&gt;gpt-oss-120b ‚Äî for production, general purpose, high reasoning use cases that fits into a single H100 GPU (117B parameters with 5.1B active parameters)&lt;/p&gt; &lt;p&gt;gpt-oss-20b ‚Äî for lower latency, and local or specialized use cases (21B parameters with 3.6B active parameters)&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;https://huggingface.co/openai/gpt-oss-120b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1yckal6wg8hf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miezct/openai_released_their_openweight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T17:09:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj00g7</id>
    <title>Qwen3 vs. gpt-oss architecture: width matters</title>
    <updated>2025-08-06T09:27:51+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/"&gt; &lt;img alt="Qwen3 vs. gpt-oss architecture: width matters" src="https://preview.redd.it/vqgb87dfbdhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c2ff32aeda0494c869aa38d27852485afc947c7" title="Qwen3 vs. gpt-oss architecture: width matters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sebastian Raschka is at it again! This time he compares the Qwen 3 and gpt-oss architectures. I'm looking forward to his deep dive, his Qwen 3 series was phenomenal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vqgb87dfbdhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00g7/qwen3_vs_gptoss_architecture_width_matters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T09:27:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mivbuo</id>
    <title>in other words benchmaxxed</title>
    <updated>2025-08-06T04:36:37+00:00</updated>
    <author>
      <name>/u/mvp525</name>
      <uri>https://old.reddit.com/user/mvp525</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mivbuo/in_other_words_benchmaxxed/"&gt; &lt;img alt="in other words benchmaxxed" src="https://preview.redd.it/i2vavxugvbhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0659e158cc4f10d87cf14b124dccd590bed50dc" title="in other words benchmaxxed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mvp525"&gt; /u/mvp525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i2vavxugvbhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mivbuo/in_other_words_benchmaxxed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mivbuo/in_other_words_benchmaxxed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T04:36:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj32ra</id>
    <title>Qwen 30b vs. gpt-oss-20b architecture comparison</title>
    <updated>2025-08-06T12:17:23+00:00</updated>
    <author>
      <name>/u/SunilKumarDash</name>
      <uri>https://old.reddit.com/user/SunilKumarDash</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/"&gt; &lt;img alt="Qwen 30b vs. gpt-oss-20b architecture comparison" src="https://preview.redd.it/7v3m4xao5ehf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb7ee193c42fb34e9530b8b00e974a400665f39c" title="Qwen 30b vs. gpt-oss-20b architecture comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunilKumarDash"&gt; /u/SunilKumarDash &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7v3m4xao5ehf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj32ra/qwen_30b_vs_gptoss20b_architecture_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T12:17:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1minpqr</id>
    <title>Finally, a model that's SAFE</title>
    <updated>2025-08-05T22:39:10+00:00</updated>
    <author>
      <name>/u/RandumbRedditor1000</name>
      <uri>https://old.reddit.com/user/RandumbRedditor1000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"&gt; &lt;img alt="Finally, a model that's SAFE" src="https://a.thumbs.redditmedia.com/vPXQi6mxUBYl7zt9fZCD3LWOB6PaZGcjaDHZr2r1u18.jpg" title="Finally, a model that's SAFE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/elpfx70g3ahf1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09af507acfa40063fa0bed3df990bca01d097c81"&gt;https://preview.redd.it/elpfx70g3ahf1.png?width=996&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09af507acfa40063fa0bed3df990bca01d097c81&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks openai, you're really contributing to the open-source LLM community&lt;/p&gt; &lt;p&gt;I haven't been this blown away by a model since Llama 4!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandumbRedditor1000"&gt; /u/RandumbRedditor1000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1minpqr/finally_a_model_thats_safe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-05T22:39:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj011h</id>
    <title>It's amazing how OpenAI missed its window with the gpt-oss release. The models would have been perceived much better last week.</title>
    <updated>2025-08-06T09:28:54+00:00</updated>
    <author>
      <name>/u/DistanceSolar1449</name>
      <uri>https://old.reddit.com/user/DistanceSolar1449</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This week, after the Qwen 2507 releases, the gpt-oss-120b and gpt-oss-20b models are just seen as a more censored &amp;quot;smaller but worse Qwen3-235b-Thinking-2057&amp;quot; and &amp;quot;smaller but worse Qwen3-30b-Thinking-2057&amp;quot; respectively. &lt;/p&gt; &lt;p&gt;This is &lt;a href="https://artificialanalysis.ai/?models=gpt-oss-120b%2Co3-pro%2Cgpt-4-1%2Co4-mini%2Co3%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cclaude-4-sonnet-thinking%2Cdeepseek-r1%2Cgrok-4%2Cllama-nemotron-super-49b-v1-5-reasoning%2Ckimi-k2%2Cexaone-4-0-32b-reasoning%2Cglm-4.5%2Cqwen3-235b-a22b-instruct-2507-reasoning%2Cqwen3-30b-a3b-2507-reasoning&amp;amp;intelligence-tab=intelligence#artificial-analysis-intelligence-index"&gt;what the general perception is mostly following&lt;/a&gt; today: &lt;a href="https://i.imgur.com/wugi9sG.png"&gt;https://i.imgur.com/wugi9sG.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But what if OpenAI released a week earlier? &lt;/p&gt; &lt;p&gt;They would have been seen as world beaters, at least for a few days. No Qwen 2507. No GLM-4.5. No Nvidia Nemotron 49b V1.5. No EXAONE 4.0 32b. &lt;/p&gt; &lt;p&gt;The field would have &lt;a href="https://artificialanalysis.ai/?models=gpt-oss-120b%2Co3-pro%2Cgpt-4-1%2Co4-mini%2Co3%2Cgpt-oss-20b%2Cllama-4-maverick%2Cgemini-2-5-pro%2Cclaude-4-sonnet-thinking%2Cdeepseek-r1%2Cgrok-4%2Cllama-3-1-nemotron-ultra-253b-v1-reasoning%2Ckimi-k2%2Cdeepseek-r1-0120%2Cqwen3-235b-a22b-instruct-reasoning%2Cqwen3-30b-a3b-instruct-reasoning&amp;amp;intelligence-tab=openWeights#artificial-analysis-intelligence-index-by-open-weights-vs-proprietary"&gt;looked like this&lt;/a&gt; last week: &lt;a href="https://i.imgur.com/rGKG8eZ.png"&gt;https://i.imgur.com/rGKG8eZ.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That would be a very different set of competitors. The 2 gpt-oss models would have been seen as &lt;strong&gt;the&lt;/strong&gt; best models other than Deepseek R1 0528, and the 120b better than the original Deepseek R1. &lt;/p&gt; &lt;p&gt;There would have been no open source competitors in its league. Qwen3 235b would be significantly behind. Nvidia Nemotron Ultra 253b would have been significantly behind. &lt;/p&gt; &lt;p&gt;OpenAI would have &lt;strong&gt;set a narrative of &amp;quot;even our open source models stomps on others at the same size&amp;quot;, with others trying to catch up&lt;/strong&gt; but OpenAI failed to capitalize on that due to their delays. &lt;/p&gt; &lt;p&gt;It's possible that the open source models &lt;em&gt;were even better 1-2 weeks ago&lt;/em&gt;, but OpenAI decided to posttrain some more to dumb it down and make it safer since they felt like they had a comfortable lead...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DistanceSolar1449"&gt; /u/DistanceSolar1449 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj011h/its_amazing_how_openai_missed_its_window_with_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj011h/its_amazing_how_openai_missed_its_window_with_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj011h/its_amazing_how_openai_missed_its_window_with_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T09:28:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mix2kg</id>
    <title>Safemaxxed for your safety!</title>
    <updated>2025-08-06T06:17:32+00:00</updated>
    <author>
      <name>/u/Caffdy</name>
      <uri>https://old.reddit.com/user/Caffdy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mix2kg/safemaxxed_for_your_safety/"&gt; &lt;img alt="Safemaxxed for your safety!" src="https://preview.redd.it/gaqdycledchf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5a2b32eb53633ee05256ff12a01a15e7ee6f844" title="Safemaxxed for your safety!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Caffdy"&gt; /u/Caffdy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gaqdycledchf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mix2kg/safemaxxed_for_your_safety/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mix2kg/safemaxxed_for_your_safety/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T06:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj7t51</id>
    <title>üöÄ Qwen3-4B-Thinking-2507 released!</title>
    <updated>2025-08-06T15:30:38+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/"&gt; &lt;img alt="üöÄ Qwen3-4B-Thinking-2507 released!" src="https://preview.redd.it/3cl3vbg54fhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6c235775ccee84fde52e9be7bdcf5ada8fb44ec" title="üöÄ Qwen3-4B-Thinking-2507 released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the past three months, we have continued to scale the thinking capability of Qwen3-4B, improving both the quality and depth of reasoning. We are pleased to introduce Qwen3-4B-Thinking-2507, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Significantly improved performance on reasoning tasks, including logical reasoning, mathematics, science, coding, and academic benchmarks that typically require human expertise.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Markedly better general capabilities, such as instruction following, tool usage, text generation, and alignment with human preferences.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Enhanced 256K long-context understanding capabilities.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;NOTE: This version has an increased thinking length. We strongly recommend its use in highly complex reasoning tasks&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cl3vbg54fhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj7t51/qwen34bthinking2507_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T15:30:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1miyix4</id>
    <title>I'm sorry, but I can't provide that... patience - I already have none...</title>
    <updated>2025-08-06T07:49:59+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/"&gt; &lt;img alt="I'm sorry, but I can't provide that... patience - I already have none..." src="https://preview.redd.it/aufyauketchf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ae39d0f21635e24eb2be18f44662947077760e" title="I'm sorry, but I can't provide that... patience - I already have none..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;That's it. I'm done with this useless piece of trash of a model...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/aufyauketchf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miyix4/im_sorry_but_i_cant_provide_that_patience_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T07:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj00mr</id>
    <title>How did you enjoy the experience so far?</title>
    <updated>2025-08-06T09:28:11+00:00</updated>
    <author>
      <name>/u/Paradigmind</name>
      <uri>https://old.reddit.com/user/Paradigmind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00mr/how_did_you_enjoy_the_experience_so_far/"&gt; &lt;img alt="How did you enjoy the experience so far?" src="https://preview.redd.it/lj67oslhbdhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c80a51205f8e1a50045f6a608b9a5b683365337" title="How did you enjoy the experience so far?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So aside from dishing out neural lobotomies in the name of safety, what else can this model actually provide? I heard someone is brave enough to try fixing it. But unless you‚Äôre in it for the masochistic fun, is it even worth it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paradigmind"&gt; /u/Paradigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lj67oslhbdhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00mr/how_did_you_enjoy_the_experience_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj00mr/how_did_you_enjoy_the_experience_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T09:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj4zkk</id>
    <title>LEAK: How OpenAI came up with the new models name.</title>
    <updated>2025-08-06T13:40:52+00:00</updated>
    <author>
      <name>/u/Paradigmind</name>
      <uri>https://old.reddit.com/user/Paradigmind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj4zkk/leak_how_openai_came_up_with_the_new_models_name/"&gt; &lt;img alt="LEAK: How OpenAI came up with the new models name." src="https://preview.redd.it/d60vtzhkkehf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383cea886dcacb59ca2ecf64648d26e3b8263075" title="LEAK: How OpenAI came up with the new models name." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paradigmind"&gt; /u/Paradigmind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d60vtzhkkehf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj4zkk/leak_how_openai_came_up_with_the_new_models_name/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj4zkk/leak_how_openai_came_up_with_the_new_models_name/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T13:40:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1miwrli</id>
    <title>"What, you don't like your new SOTA model?"</title>
    <updated>2025-08-06T05:59:16+00:00</updated>
    <author>
      <name>/u/Friendly_Willingness</name>
      <uri>https://old.reddit.com/user/Friendly_Willingness</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miwrli/what_you_dont_like_your_new_sota_model/"&gt; &lt;img alt="&amp;quot;What, you don't like your new SOTA model?&amp;quot;" src="https://preview.redd.it/9yqb0l1n9chf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=726e03405370b5eb421009dfc38b1005ddf67ee0" title="&amp;quot;What, you don't like your new SOTA model?&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Friendly_Willingness"&gt; /u/Friendly_Willingness &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9yqb0l1n9chf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1miwrli/what_you_dont_like_your_new_sota_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1miwrli/what_you_dont_like_your_new_sota_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T05:59:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj0snp</id>
    <title>Elon Musk says that xAI will make Grok 2 open source next week</title>
    <updated>2025-08-06T10:16:28+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj0snp/elon_musk_says_that_xai_will_make_grok_2_open/"&gt; &lt;img alt="Elon Musk says that xAI will make Grok 2 open source next week" src="https://preview.redd.it/htgw3mmvjdhf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90be5e283724a3ec93ab02ddff87962c7ebd7661" title="Elon Musk says that xAI will make Grok 2 open source next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Elon Musk on ùïè: &lt;a href="https://x.com/elonmusk/status/1952988026617119075"&gt;https://x.com/elonmusk/status/1952988026617119075&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/htgw3mmvjdhf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj0snp/elon_musk_says_that_xai_will_make_grok_2_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj0snp/elon_musk_says_that_xai_will_make_grok_2_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T10:16:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1misyvc</id>
    <title>OpenAI, I don't feel SAFE ENOUGH</title>
    <updated>2025-08-06T02:35:22+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1misyvc/openai_i_dont_feel_safe_enough/"&gt; &lt;img alt="OpenAI, I don't feel SAFE ENOUGH" src="https://preview.redd.it/af6jm3nt9bhf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb88d869e88cfd2f93a6e76c7ac3ddf342a2db09" title="OpenAI, I don't feel SAFE ENOUGH" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good timing btw&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/af6jm3nt9bhf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1misyvc/openai_i_dont_feel_safe_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1misyvc/openai_i_dont_feel_safe_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T02:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mj2hih</id>
    <title>GPT-OSS looks more like a publicity stunt as more independent test results come out :(</title>
    <updated>2025-08-06T11:49:27+00:00</updated>
    <author>
      <name>/u/mvp525</name>
      <uri>https://old.reddit.com/user/mvp525</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2hih/gptoss_looks_more_like_a_publicity_stunt_as_more/"&gt; &lt;img alt="GPT-OSS looks more like a publicity stunt as more independent test results come out :(" src="https://preview.redd.it/onk13jqo0ehf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=90adba17a6c8320711a1e18d55c4c6fea2ab2fb7" title="GPT-OSS looks more like a publicity stunt as more independent test results come out :(" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mvp525"&gt; /u/mvp525 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/onk13jqo0ehf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2hih/gptoss_looks_more_like_a_publicity_stunt_as_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mj2hih/gptoss_looks_more_like_a_publicity_stunt_as_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T11:49:27+00:00</published>
  </entry>
</feed>
