<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-13T16:08:32+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ow0ss8</id>
    <title>qwen/qwen3-vl-4b - LMStudio Server - llama.cpp: Submitting multimodal video as individual frames</title>
    <updated>2025-11-13T13:16:36+00:00</updated>
    <author>
      <name>/u/ElSrJuez</name>
      <uri>https://old.reddit.com/user/ElSrJuez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was able to send images to Qwen3-VL using LMStudio wrapper around llama.cpp (works awesome btw) but when trying video I hit a wall, seemingly this implementation doesnt support Qwen3 video structures?&lt;br /&gt; Questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Is this a Qwen3-specific thing, or are these video types also part of the so called &amp;quot;OpenAI compatible&amp;quot; schema?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I suppose my particular issue is a limitation of the LMStudio server and not llama.cpp or other frameworks?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;And naturally, what is the easiest way to make this work?&lt;br /&gt; &lt;em&gt;(main reason I am using LMStudio wrapper is because I dont want to have to fiddle with llama.cpp...&lt;/em&gt; &lt;strong&gt;baby steps&lt;/strong&gt;&lt;em&gt;).&lt;/em&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;content&amp;quot;: [&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;type&amp;quot;: &amp;quot;video&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;sample_fps&amp;quot;: 2,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;video&amp;quot;: [&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;data:image/jpeg;base64,...(truncated)...&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;data:image/jpeg;base64,...(truncated)...&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;data:image/jpeg;base64,...(truncated)...&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;data:image/jpeg;base64,...(truncated)...&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;},&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;{&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;type&amp;quot;: &amp;quot;text&amp;quot;,&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;&amp;quot;text&amp;quot;: &amp;quot;Let's see whats going on!&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Invoke-RestMethod error:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;{ &amp;quot;error&amp;quot;: &amp;quot;Invalid \u0027content\u0027: \u0027content\u0027 objects must have a \u0027type\u0027 field that is either \u0027&lt;/code&gt;&lt;strong&gt;text&lt;/strong&gt;&lt;code&gt;\u0027 or \u0027&lt;/code&gt;&lt;strong&gt;image_url&lt;/strong&gt;&lt;code&gt;\u0027.&amp;quot; }&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;InvalidOperation:&lt;/code&gt; &lt;/p&gt; &lt;p&gt;&lt;code&gt;94 | $narr = $resp.choices[0].message.content&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| Cannot index into a null array.&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ElSrJuez"&gt; /u/ElSrJuez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0ss8/qwenqwen3vl4b_lmstudio_server_llamacpp_submitting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0ss8/qwenqwen3vl4b_lmstudio_server_llamacpp_submitting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0ss8/qwenqwen3vl4b_lmstudio_server_llamacpp_submitting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T13:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovx2jr</id>
    <title>Qwen Chat Bot - Inaccessible Source Links</title>
    <updated>2025-11-13T09:51:07+00:00</updated>
    <author>
      <name>/u/middyy95</name>
      <uri>https://old.reddit.com/user/middyy95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So when I prompted the Qwen AI chatbot to provide me links/sources to its claims, all (like all the links) the links do not work at all&lt;/p&gt; &lt;p&gt;- I understand that some links are behind paywalls but I have tried over 50+ links and they're all 'broken'/non-existent links&lt;/p&gt; &lt;p&gt;Due to the lack of actual sources/links, it seems risky to even believe the slightest form of answer it gives.&lt;/p&gt; &lt;p&gt;Does anyone have the same issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/middyy95"&gt; /u/middyy95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovx2jr/qwen_chat_bot_inaccessible_source_links/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovx2jr/qwen_chat_bot_inaccessible_source_links/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovx2jr/qwen_chat_bot_inaccessible_source_links/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T09:51:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovys0w</id>
    <title>Sell my 5080 for something else or...</title>
    <updated>2025-11-13T11:35:23+00:00</updated>
    <author>
      <name>/u/foogitiff</name>
      <uri>https://old.reddit.com/user/foogitiff</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I currently have a spare 5080 16GB in my Xeon server (8259CL, 192GB of RAM). I mostly want to run coding agent (I don't do image/video generation - and I would probably do that on the 5080 that is on my desktop).&lt;/p&gt; &lt;p&gt;I know it's not the best card for the job. I was wondering if I should sell it and invest in card(s) with more VRAM, or even just buy a Strix Halo 128GB. Or sell everything and buy the biggest Mac Studio I can.&lt;/p&gt; &lt;p&gt;I do not care (in some limits) to noise (the noisy machines are in the garage) nor energy consumption (as long as it run on a regular 230v power outlet that is).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foogitiff"&gt; /u/foogitiff &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovys0w/sell_my_5080_for_something_else_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovys0w/sell_my_5080_for_something_else_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovys0w/sell_my_5080_for_something_else_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T11:35:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovra2o</id>
    <title>Open source x 3: GRPO training with OpenEnv, vLLM, and Oumi</title>
    <updated>2025-11-13T04:03:41+00:00</updated>
    <author>
      <name>/u/PrincipleFar6835</name>
      <uri>https://old.reddit.com/user/PrincipleFar6835</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You may have seen the release of open source OpenEnv a fews weeks ago at the PyTorch Conference. I wanted to share a tutorial showing how you can actually do GRPO training using an OpenEnv environment server and vLLM: &lt;a href="https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20OpenEnv%20GRPO%20with%20trl.ipynb"&gt;https://github.com/oumi-ai/oumi/blob/main/notebooks/Oumi%20-%20OpenEnv%20GRPO%20with%20trl.ipynb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrincipleFar6835"&gt; /u/PrincipleFar6835 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovra2o/open_source_x_3_grpo_training_with_openenv_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovra2o/open_source_x_3_grpo_training_with_openenv_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovra2o/open_source_x_3_grpo_training_with_openenv_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T04:03:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovww60</id>
    <title>What Modell to run on 8x A100 (40GB)?</title>
    <updated>2025-11-13T09:39:30+00:00</updated>
    <author>
      <name>/u/Not_Black_is_taken</name>
      <uri>https://old.reddit.com/user/Not_Black_is_taken</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt; &lt;p&gt;I just got access to a 8x A100 GPU server. Do you have some interesting models I should try to run and or benchmark?&lt;/p&gt; &lt;p&gt;Here are the specs of the system: 8x A100 40GB (320GB total) AMD EPYC 7302 (16 Cores / 32 Threads) 1TB of RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Not_Black_is_taken"&gt; /u/Not_Black_is_taken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovww60/what_modell_to_run_on_8x_a100_40gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovww60/what_modell_to_run_on_8x_a100_40gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovww60/what_modell_to_run_on_8x_a100_40gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T09:39:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow0yvl</id>
    <title>Best getting started guide, moving from RTX3090 to Strix Halo</title>
    <updated>2025-11-13T13:24:04+00:00</updated>
    <author>
      <name>/u/favicocool</name>
      <uri>https://old.reddit.com/user/favicocool</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After years of using a 3x RTX3090 with ollama for inference, I ordered a 128GB AI MAX+ 395 mini workstation with 128GB.&lt;/p&gt; &lt;p&gt;As it‚Äôs a major shift in hardware, I‚Äôm not too sure where to begin. My immediate objective is to get similar functionality to what I previously had, which was inference over the Ollama API. I don‚Äôt intend to do any training/fine-tuning. My primary use is for writing code and occasionally processing text and documents (translation, summarizing)&lt;/p&gt; &lt;p&gt;I‚Äôm looking for a few pointers to get started.&lt;/p&gt; &lt;p&gt;I admit I‚Äôm ignorant when it comes to the options for software stack. I‚Äôm sure I‚Äôll be able to get it working, but I‚Äôm interested to know what the state of the art is. &lt;/p&gt; &lt;p&gt;Which is the most performant software solution for LLMs on this platform? If it‚Äôs not ollama, are there compatibility proxies so my ollama-based tools will work without changes?&lt;/p&gt; &lt;p&gt;There‚Äôs plenty of info in this sub about models that work well on this hardware, but software is always evolving. Up to the minute input from this sub seems invaluable&lt;/p&gt; &lt;p&gt;tl; dr; What‚Äôs the best driver and software stack for Strix Halo platforms currently, and what‚Äôs the best source of info as development continues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/favicocool"&gt; /u/favicocool &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0yvl/best_getting_started_guide_moving_from_rtx3090_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0yvl/best_getting_started_guide_moving_from_rtx3090_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0yvl/best_getting_started_guide_moving_from_rtx3090_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T13:24:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovw4gk</id>
    <title>RAG Paper 25.11.12</title>
    <updated>2025-11-13T08:48:22+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08181v1"&gt;MARC: Multimodal and Multi-Task Agentic Retrieval-Augmented Generation for Cold-Start Recommender System&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08505v1"&gt;Structured RAG for Answering Aggregative Questions&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08245v1"&gt;Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08029v1"&gt;BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08507v1"&gt;Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08274v1"&gt;Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.08343v1"&gt;JobSphere: An AI-Powered Multilingual Career Copilot for Government Employment Platforms&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07943v1"&gt;Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07982v1"&gt;NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView.ai&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;/&lt;/strong&gt; &lt;a href="https://github.com/RagView/RagView"&gt;&lt;strong&gt;github/RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovw4gk/rag_paper_251112/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovw4gk/rag_paper_251112/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovw4gk/rag_paper_251112/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T08:48:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ov3dkb</id>
    <title>AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs</title>
    <updated>2025-11-12T12:06:13+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"&gt; &lt;img alt="AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs" src="https://external-preview.redd.it/Ym1xdmdzdXRldDBnMR0L-Ennn3ovi4auFkXdc601F67-ibAb8bxVVAjHQXSP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c6b63f193e162db80ace947cf0df279cfbc1423" title="AELLA: 100M+ research papers: an open-science initiative to make scientific research accessible via structured summaries created by LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://inference.net/blog/project-aella"&gt;https://inference.net/blog/project-aella&lt;/a&gt;&lt;br /&gt; Models: &lt;a href="https://huggingface.co/inference-net"&gt;https://huggingface.co/inference-net&lt;/a&gt;&lt;br /&gt; Visualizer: &lt;a href="https://aella.inference.net/"&gt;https://aella.inference.net&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/du59aiutet0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ov3dkb/aella_100m_research_papers_an_openscience/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T12:06:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovm3jd</id>
    <title>Kimi K2 Thinking Creative Writing Test</title>
    <updated>2025-11-13T00:06:55+00:00</updated>
    <author>
      <name>/u/kennydotun123</name>
      <uri>https://old.reddit.com/user/kennydotun123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Whenever a new model is dropped, either from one of the established labs, or from a new lab, the first thing I do is to give it a creative writing test. I am not a coder. I am more interested in creative writing. And so, my expectations are usually a bit different from most of the people involved in the AI scene. The test I use is simple. I give the AI some background information and worldbuilding details, and then a very rough prologue sketch, including a list of agents that I want the AI to use to edit the prose. Using those agents, the AI is to stretch and refine the sketch to a prologue that is about 2000 words. I have done this consistently for months, and before moving on with my main point, I will list some of my observations-&lt;/p&gt; &lt;p&gt;Lets start with Chatgpt- The newer models are solid. Very, very good. Arguably the best. No complaints. At least for the first couple chapters. To note moving forward, this goes for chatgpt as well as the other models, they all seem to decline in quality in like the third chapter, and more so after that. So, to me these are not long term companions. Honestly, if that could be fixed, I could see AI being used more in the literary scene. &lt;/p&gt; &lt;p&gt;Moving on to Gemini- Was not good until 2.0Pro came, then it got surprisingly better, then 2.5pro came, then it got really good, good enough that I became tempted to start plotting more chapters. Which is usually a good sign. The quality usually declines immediately after, for this and all other models, in my opinion, however, when the prologue is solid, that's a good sign. I go back to Gemini and I am surprised again at how good the writing got. &lt;/p&gt; &lt;p&gt;Claude- Really good, could be the best, but got stagnant/limited. Claude used to be my go to AI for creative writing. I remember there was a time when everyone boasted about Claude's writing chops. I was one of those people. Don't get me wrong, the writing is amazing, still is, but it feels less like Claude got better and more like the others caught up in my opinion. Claude's writing was what made it stand out in the whole field, now the field appears full in my opinion. And I know this because sometimes, I use the old models, and the prose there maintains a kind of elegance. Indicating that while the newer models did improve in certain areas, the AI more or less stagnated. Which is fine, I'm not complaining, but it feels like, if that's the case, then they should focus more on longevity. And that is when it is good. Often it gets over ambitious, it starts doing too much, and weirdly enough, the writing gets awful then. But sometimes, it writes like it really gets you. My relationship with Claude is complex. &lt;/p&gt; &lt;p&gt;Grok- Okay. Fine. &lt;/p&gt; &lt;p&gt;Now, I know that each of these AI's have different models, with different capabilities, but I more or less breezed through these differences for the sake of brevity. Just assume that I am talking about the latest models. Now moving on the the open source models-&lt;/p&gt; &lt;p&gt;Gemma- Not good. &lt;/p&gt; &lt;p&gt;GPT-OSS- Not good. &lt;/p&gt; &lt;p&gt;Llama- Not good. At best, okay. &lt;/p&gt; &lt;p&gt;Now we will move to the Chinese models, one of which, this post centers around. Many of then are either open or quasi open. &lt;/p&gt; &lt;p&gt;Ling and Ring 1T- For some reason, they kept spazzing out. I would look at the reasoning and it was like a guy was driving, then suddenly got super drunk and flew off the road. I never even got any write ups from them, the whole thing would just crash. &lt;/p&gt; &lt;p&gt;Deepseek- It writes like it does not care for creative writing, and in turn, I don't care for it much. &lt;/p&gt; &lt;p&gt;Qwen- Same as Deepseek. &lt;/p&gt; &lt;p&gt;Kimi- When Kimi first came out. I was interested. Everyone raved about it, and so I did the test, it was the first lab that did not spaz out on me, did not start inserting random Chinese letters in the text, it was not good, alright average, but unlike Deepseek and Qwen, it seemed like it cared somewhat. So I decided to put an eye on it. K2 thinking came out. And I noticed instantly, the writing was good. Really good. About as good as the other labs. In my opinion, in terms of creative writing, it is the one that somewhat captures the heart of the story I suppose. Although Claude seems to get it as well. Anyhoo, I'll put the link below to the writing tests. &lt;/p&gt; &lt;p&gt;Here's the link;&lt;br /&gt; &lt;a href="https://docs.google.com/document/d/1ln9txx6vOtyNcYnmb_yBvjMPtzzqlCZTBKJVIsEdjdw/edit?usp=sharing"&gt;https://docs.google.com/document/d/1ln9txx6vOtyNcYnmb_yBvjMPtzzqlCZTBKJVIsEdjdw/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kennydotun123"&gt; /u/kennydotun123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovm3jd/kimi_k2_thinking_creative_writing_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovm3jd/kimi_k2_thinking_creative_writing_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovm3jd/kimi_k2_thinking_creative_writing_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T00:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow4fv2</id>
    <title>Help me Kill or Confirm this Idea</title>
    <updated>2025-11-13T15:44:41+00:00</updated>
    <author>
      <name>/u/Navaneeth26</name>
      <uri>https://old.reddit.com/user/Navaneeth26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre &lt;strong&gt;building ModelMatch&lt;/strong&gt;, a beta open source project that &lt;strong&gt;recommends open source models&lt;/strong&gt; for specific jobs, not generic benchmarks.&lt;/p&gt; &lt;p&gt;So far we cover 5 domains: summarization, therapy advising, health advising, email writing, and finance assistance.&lt;/p&gt; &lt;p&gt;The point is simple: most teams still pick models based on vibes, vendor blogs, or random Twitter threads. In short we help people recommend the best model for a certain use case via our leadboards and open source eval frameworks using gpt 4o and Claude 3.5 Sonnet.&lt;/p&gt; &lt;p&gt;How we do it: we run models through our open source evaluator with task-specific rubrics and strict rules. Each &lt;strong&gt;run produces a 0-10 score&lt;/strong&gt; plus notes. We‚Äôve &lt;strong&gt;finished initial testing&lt;/strong&gt; and have a provisional top three for each domain. We are showing results through short YouTube breakdowns and on our site.&lt;/p&gt; &lt;p&gt;We know it is not perfect yet but what i am looking for is a reality check on the idea itself.&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;looking for feedback&lt;/strong&gt; on this so as to improve. Do u think:&lt;/p&gt; &lt;p&gt;A recommender like this is actually needed for real work, or is model choice not a real pain?&lt;/p&gt; &lt;p&gt;Be blunt. If this is noise, say so and why. If it is useful, tell me the one change that would get you to use it&lt;/p&gt; &lt;p&gt;P.S: we are also &lt;strong&gt;looking for contributors&lt;/strong&gt; to our project&lt;/p&gt; &lt;p&gt;Links in the first comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Navaneeth26"&gt; /u/Navaneeth26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow4fv2/help_me_kill_or_confirm_this_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow4fv2/help_me_kill_or_confirm_this_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow4fv2/help_me_kill_or_confirm_this_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:44:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovc3sj</id>
    <title>Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming</title>
    <updated>2025-11-12T17:50:56+00:00</updated>
    <author>
      <name>/u/lektoq</name>
      <uri>https://old.reddit.com/user/lektoq</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"&gt; &lt;img alt="Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming" src="https://preview.redd.it/n5cc10ph5v0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=badc8223252000cebb903a5e944f40eb1d1caa53" title="Live VLM WebUI - Web interface for Ollama vision models with real-time video streaming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! üëã&lt;/p&gt; &lt;p&gt;I'm a Technical Marketing Engineer at NVIDIA working on Jetson, and we just open-sourced &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui"&gt;&lt;strong&gt;Live VLM WebUI&lt;/strong&gt;&lt;/a&gt; - a tool for testing Vision Language Models locally with real-time video streaming.&lt;/p&gt; &lt;h1&gt;What is it?&lt;/h1&gt; &lt;p&gt;Stream your webcam to any Ollama vision model (or other VLM backends) and get real-time AI analysis overlaid on your video feed. Think of it as a convenient interface for testing vision models in real-time scenarios.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stream live video to the model (not screenshot-by-screenshot)&lt;/li&gt; &lt;li&gt;Show you exactly how fast it's processing frames&lt;/li&gt; &lt;li&gt;Monitor GPU/VRAM usage in real-time&lt;/li&gt; &lt;li&gt;Work across different hardware (PC, Mac, Jetson)&lt;/li&gt; &lt;li&gt;Support multiple backends (Ollama, vLLM, NVIDIA API Catalog, OpenAI)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;WebRTC video streaming&lt;/strong&gt; - Low latency, works with any webcam&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama native support&lt;/strong&gt; - Auto-detect &lt;code&gt;http://localhost:11434&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time metrics&lt;/strong&gt; - See inference time, GPU usage, VRAM, tokens/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-backend&lt;/strong&gt; - Also works with vLLM, NVIDIA API Catalog, OpenAI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-platform&lt;/strong&gt; - Linux PC, DGX Spark, Jetson, Mac, WSL&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy install&lt;/strong&gt; - &lt;code&gt;pip install live-vlm-webui&lt;/code&gt; and you're done&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0&lt;/strong&gt; - Fully open source, accepting community contributions&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üöÄ Quick Start with Ollama&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;# 1. Make sure Ollama is running with a vision model ollama pull gemma:4b # 2. Install and run pip install live-vlm-webui live-vlm-webui # 3. Open https://localhost:8090 # 4. Select &amp;quot;Ollama&amp;quot; backend and your model &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Use Cases I've Found Helpful&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model comparison&lt;/strong&gt; - Testing &lt;code&gt;gemma:4b&lt;/code&gt; vs &lt;code&gt;gemma:12b&lt;/code&gt; vs &lt;code&gt;llama3.2-vision&lt;/code&gt; the same scenes&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance benchmarking&lt;/strong&gt; - See actual inference speed on your hardware&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Interactive demos&lt;/strong&gt; - Show people what vision models can do in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time prompt engineering&lt;/strong&gt; - Tune your vision prompt as seeing the result in real-time&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Development&lt;/strong&gt; - Quick feedback loop when working with VLMs&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Models That Work Great&lt;/h1&gt; &lt;p&gt;Any Ollama vision model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;gemma3:4b&lt;/code&gt;, &lt;code&gt;gemma3:12b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;llama3.2-vision:11b&lt;/code&gt;, &lt;code&gt;llama3.2-vision:90b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen2.5-vl:3b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:7b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:32b&lt;/code&gt;, &lt;code&gt;qwen2.5-vl:72b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;qwen3-vl:2b&lt;/code&gt;, &lt;code&gt;qwen3-vl:4b&lt;/code&gt;, all the way up to &lt;code&gt;qwen3-vl:235b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;llava:7b&lt;/code&gt;, &lt;code&gt;llava:13b&lt;/code&gt;, &lt;code&gt;llava:34b&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;minicpm-v:8b&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Docker Alternative&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;docker run -d --gpus all --network host \ ghcr.io/nvidia-ai-iot/live-vlm-webui:latest &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;Planning to add:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Analysis result copy to clipboard, log and export&lt;/li&gt; &lt;li&gt;Model comparison view (side-by-side)&lt;/li&gt; &lt;li&gt;Better prompt templates&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui"&gt;https://github.com/nvidia-ai-iot/live-vlm-webui&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href="https://github.com/nvidia-ai-iot/live-vlm-webui/tree/main/docs"&gt;https://github.com/nvidia-ai-iot/live-vlm-webui/tree/main/docs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PyPI:&lt;/strong&gt; &lt;a href="https://pypi.org/project/live-vlm-webui/"&gt;https://pypi.org/project/live-vlm-webui/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear what you think! What features would make this more useful for your workflows? PRs and issues welcome - this is meant to be a community tool.&lt;/p&gt; &lt;blockquote&gt; &lt;h2&gt;A bit of background&lt;/h2&gt; &lt;p&gt;This community has been a huge inspiration for our work. When we launched the &lt;a href="https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/"&gt;Jetson Generative AI Lab&lt;/a&gt;, &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; was literally cited as one of the key communities driving the local AI movement.&lt;/p&gt; &lt;p&gt;WebRTC integration for real-time camera streaming into VLMs on Jetson was pioneered by our colleague a while back. It was groundbreaking but tightly coupled to specific setups. Then Ollama came along and with their standardized API we suddenly could serve vision models in a way that works anywhere.&lt;/p&gt; &lt;p&gt;We realized we could take that WebRTC streaming approach and modernize it: make it work with any VLM backend through standard APIs, run on any platform, and give people a better experience than uploading images on Open WebUI and waiting for responses.&lt;/p&gt; &lt;p&gt;So this is kind of the evolution of that original work - taking what we learned on Jetson and making it accessible to the broader local AI community.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Happy to answer any questions about setup, performance, or implementation details!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lektoq"&gt; /u/lektoq &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n5cc10ph5v0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovc3sj/live_vlm_webui_web_interface_for_ollama_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:50:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovmyux</id>
    <title>Uncensored models</title>
    <updated>2025-11-13T00:44:18+00:00</updated>
    <author>
      <name>/u/NotoriousKekabidze</name>
      <uri>https://old.reddit.com/user/NotoriousKekabidze</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I‚Äôm new to the thread and I‚Äôm not sure if I‚Äôm asking my question in the right place. Still, I‚Äôm wondering: are there any AI models for local use that are as uncensored as, or even more uncensored than, Venice.ai? Or would it be better to just run regular open-source LLMs locally and try to look for jailbreaks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NotoriousKekabidze"&gt; /u/NotoriousKekabidze &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovmyux/uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovmyux/uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovmyux/uncensored_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T00:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovatvf</id>
    <title>Where are all the data centers dumping their old decommissioned GPUs?</title>
    <updated>2025-11-12T17:05:19+00:00</updated>
    <author>
      <name>/u/AffectSouthern9894</name>
      <uri>https://old.reddit.com/user/AffectSouthern9894</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2022, I purchased a lot of Tesla P40s on eBay, but unfortunately, because of their outdated architecture, they are now practically useless for what I want to do. It seems like newer-generation GPUs aren‚Äôt finding their way into consumers' hands. I asked my data center connection and he said they are recycling them, but they‚Äôve always been doing this and we could still get hardware.&lt;/p&gt; &lt;p&gt;With the amount of commercial GPUs in the market right now, you would think there would be some overflow?&lt;/p&gt; &lt;p&gt;I hope to be wrong and suck at resourcing now, any help?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectSouthern9894"&gt; /u/AffectSouthern9894 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovatvf/where_are_all_the_data_centers_dumping_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-12T17:05:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3vsu</id>
    <title>Finally got something decent to run llms (Rtx 3090ti)</title>
    <updated>2025-11-13T15:22:37+00:00</updated>
    <author>
      <name>/u/Ok_Essay3559</name>
      <uri>https://old.reddit.com/user/Ok_Essay3559</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3vsu/finally_got_something_decent_to_run_llms_rtx/"&gt; &lt;img alt="Finally got something decent to run llms (Rtx 3090ti)" src="https://a.thumbs.redditmedia.com/C5wBLBR9OFnBeTujcTg3dns6ijP5tRAA33HXHRzmzk0.jpg" title="Finally got something decent to run llms (Rtx 3090ti)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bought it on eBay for $835.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Essay3559"&gt; /u/Ok_Essay3559 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ow3vsu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3vsu/finally_got_something_decent_to_run_llms_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3vsu/finally_got_something_decent_to_run_llms_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovzz3q</id>
    <title>Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (300~ kb) LLM Frontend (Single HTML file). Now with GitHub - github.com/Unmortan-Ellary/Vascura-FRONT.</title>
    <updated>2025-11-13T12:38:30+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovzz3q/vascura_front_open_source_apache_20_bloat_free/"&gt; &lt;img alt="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (300~ kb) LLM Frontend (Single HTML file). Now with GitHub - github.com/Unmortan-Ellary/Vascura-FRONT." src="https://external-preview.redd.it/Y2xkOWd4ZDRvMDFnMTaXKbAxEnkCSlwbwmJDXf_lyDQzd483n4JJoFhjK3xD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=489a635f2ead9cbbd8470ce817253b7761627b19" title="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (300~ kb) LLM Frontend (Single HTML file). Now with GitHub - github.com/Unmortan-Ellary/Vascura-FRONT." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;GitHub&lt;/strong&gt; - &lt;a href="http://github.com/Unmortan-Ellary/Vascura-FRONT"&gt;github.com/Unmortan-Ellary/Vascura-FRONT&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Changes from the prototype version: &lt;/p&gt; &lt;p&gt;- Reworked Web Search: now fit in 4096 tokens, allOrigins can be used locally.&lt;br /&gt; - Now Web Search is really good at collecting links (90 links total for 9 agents).&lt;br /&gt; - Lot of bug fixes and logic improvements.&lt;br /&gt; - Improved React system.&lt;br /&gt; - Copy / Paste settings function.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Frontend is designed around core ideas:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- On-the-Spot Text Editing: You should have fast, precise control over editing and altering text.&lt;br /&gt; - Dependency-Free: No downloads, no Python, no Node.js - just a single compact (300~ kb) HTML file that runs in your browser.&lt;br /&gt; - Focused on Core: Only essential tools and features that serve the main concept.&lt;br /&gt; - Context-Effective Web Search: Should find info and links and fit in 4096 tokens limit.&lt;br /&gt; - OpenAI-compatible API: The most widely supported standard, chat-completion format.&lt;br /&gt; - Open Source under the Apache 2.0 License.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Please watch the video for a visual demonstration of the implemented features.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;On-the-Spot Text Editing:&lt;/strong&gt; Edit text just like in a plain notepad, no restrictions, no intermediate steps. Just click and type.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;React (Reactivation) System:&lt;/strong&gt; Generate as many LLM responses as you like at any point in the conversation. Edit, compare, delete or temporarily exclude an answer by clicking ‚ÄúIgnore‚Äù.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Agents for Web Search:&lt;/strong&gt; Each agent gathers relevant data (using allOrigins) and adapts its search based on the latest messages. Agents will push findings as &amp;quot;internal knowledge&amp;quot;, allowing the LLM to use or ignore the information, whichever leads to a better response. The algorithm is based on more complex system but is streamlined for speed and efficiency, fitting within an 4K context window (all 9 agents, instruction model).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Tokens-Prediction System:&lt;/strong&gt; Available when using LM Studio or Llama.cpp Server as the backend, this feature provides short suggestions for the LLM‚Äôs next response or for continuing your current text edit. Accept any suggestion instantly by pressing Tab.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Any OpenAI-API-Compatible Backend:&lt;/strong&gt; Works with any endpoint that implements the OpenAI API - LM Studio, Kobold.CPP, Llama.CPP Server, Oobabooga's Text Generation WebUI, and more. With &amp;quot;Strict API&amp;quot; mode enabled, it also supports Mistral API, OpenRouter API, and other v1-compliant endpoints.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Markdown Color Coding:&lt;/strong&gt; Uses Markdown syntax to apply color patterns to your text.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Adaptive Interface:&lt;/strong&gt; Each chat is an independent workspace. Everything you move or change is saved instantly. When you reload the backend or switch chats, you‚Äôll return to the exact same setup you left, except for the chat scroll position. Supports custom avatars for your chats.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Pre-Configured for LM Studio:&lt;/strong&gt; By default, the frontend is configured for an easy start with LM Studio: just turn &amp;quot;Enable CORS&amp;quot; to ON, in LM Studio server settings, enable the server in LM Studio, choose your model, launch Vascura FRONT, and say ‚ÄúHi!‚Äù - that‚Äôs it!&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Thinking Models Support:&lt;/strong&gt; Supports thinking models that use `&amp;lt;think&amp;gt;&amp;lt;/think&amp;gt;` tags or if your endpoint returns only the final answer (without a thinking step), enable the &amp;quot;Thinking Model&amp;quot; switch to activate compatibility mode - this ensures Web Search and other features work correctly.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;allOrigins:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Web Search works via allOrigins - &lt;a href="https://github.com/gnuns/allOrigins/tree/main"&gt;https://github.com/gnuns/allOrigins/tree/main&lt;/a&gt;&lt;br /&gt; - By default it will use &lt;a href="http://allorigins.win"&gt;allorigins.win&lt;/a&gt; website as a proxy.&lt;br /&gt; - But by running it locally you will get way faster and more stable results (use LOC version).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ykcfbxd4o01g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovzz3q/vascura_front_open_source_apache_20_bloat_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovzz3q/vascura_front_open_source_apache_20_bloat_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T12:38:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow0jj0</id>
    <title>Running a 1 Trillion Parameter Model on a PC with 128 GB RAM + 24 GB VRAM</title>
    <updated>2025-11-13T13:05:10+00:00</updated>
    <author>
      <name>/u/pulse77</name>
      <uri>https://old.reddit.com/user/pulse77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi again, just wanted to share that this time I've successfully run &lt;strong&gt;Kimi K2 Thinking (1T parameters)&lt;/strong&gt; on &lt;strong&gt;llama.cpp&lt;/strong&gt; using my desktop setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel i9-13900KS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 128 GB DDR5 @ 4800 MT/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 4090 (24 GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; 4TB NVMe SSD (7300 MB/s read)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm using &lt;strong&gt;Unsloth UD-Q3_K_XL (~3.5 bits)&lt;/strong&gt; from Hugging Face: &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance (generation speed):&lt;/strong&gt; 0.42 tokens/sec&lt;/p&gt; &lt;p&gt;(I know, it's slow... but it runs! I'm just stress-testing what's possible on consumer hardware...)&lt;/p&gt; &lt;p&gt;I also tested other huge models - here is a full list with speeds for comparison:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Context&lt;/th&gt; &lt;th align="left"&gt;Speed (t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Thinking&lt;/td&gt; &lt;td align="left"&gt;1T A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2 Instruct 0905&lt;/td&gt; &lt;td align="left"&gt;1T A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek V3.1 Terminus&lt;/td&gt; &lt;td align="left"&gt;671B A37B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Coder 480B Instruct&lt;/td&gt; &lt;td align="left"&gt;480B A35B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;1.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.6&lt;/td&gt; &lt;td align="left"&gt;355B A32B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Thinking&lt;/td&gt; &lt;td align="left"&gt;235B A22B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;5.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B Instruct&lt;/td&gt; &lt;td align="left"&gt;235B A22B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;5.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M2&lt;/td&gt; &lt;td align="left"&gt;230B A10B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;8.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air&lt;/td&gt; &lt;td align="left"&gt;106B A12B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;11.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 120B&lt;/td&gt; &lt;td align="left"&gt;120B A5.1B&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;25.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;IBM Granite 4.0 H Small&lt;/td&gt; &lt;td align="left"&gt;32B A9B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Thinking&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;197.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Instruct&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;218.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B Coder Instruct&lt;/td&gt; &lt;td align="left"&gt;30B A3B&lt;/td&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;120K&lt;/td&gt; &lt;td align="left"&gt;211.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;20B A3.6B&lt;/td&gt; &lt;td align="left"&gt;MXFP4&lt;/td&gt; &lt;td align="left"&gt;128K&lt;/td&gt; &lt;td align="left"&gt;223.3&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Command line used (llama.cpp):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server --threads 32 --jinja --flash-attn on --cache-type-k q8_0 --cache-type-v q8_0 --model &amp;lt;PATH-TO-YOUR-MODEL&amp;gt; --ctx-size 131072 --n-cpu-moe 9999 --no-warmup&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important:&lt;/strong&gt; Use &lt;em&gt;--no-warmup&lt;/em&gt; - otherwise, the process can crash before startup.&lt;/p&gt; &lt;p&gt;Notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory mapping (mmap)&lt;/strong&gt; in llama.cpp lets it read model files far beyond RAM capacity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No swap/pagefile&lt;/strong&gt; - I disabled these to prevent SSD wear (no disk writes during inference).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context size:&lt;/strong&gt; Reducing context length didn't improve speed for huge models (token/sec stayed roughly the same).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU offload:&lt;/strong&gt; llama.cpp automatically uses GPU for all layers unless you limit it. I only use --n-cpu-moe 9999 to keep MoE layers on CPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Anything below ~4 bits noticeably reduces quality. Lowest meaningful quantization for me is UD-Q3_K_XL.&lt;/li&gt; &lt;li&gt;Tried &lt;strong&gt;UD-Q4_K_XL&lt;/strong&gt; for Kimi models, but it failed to start. UD-Q3_K_XL is the max stable setup on my rig.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed test method:&lt;/strong&gt; Each benchmark was done using the same prompt - &amp;quot;Explain quantum computing&amp;quot;. The measurement covers the entire generation process until the model finishes its response (so, true end-to-end inference speed).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama.cpp version:&lt;/strong&gt; b6963 ‚Äî all tests were run on this version.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR - Yes&lt;/strong&gt;, it's possible to run (slowly) a &lt;strong&gt;1-trillion-parameter LLM&lt;/strong&gt; on a machine with &lt;strong&gt;128 GB RAM + 24 GB VRAM&lt;/strong&gt; - no cluster or cloud required. Mostly an experiment to see where the limits really are.&lt;/p&gt; &lt;p&gt;EDIT: Fixed info about IBM Granite model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pulse77"&gt; /u/pulse77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow0jj0/running_a_1_trillion_parameter_model_on_a_pc_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T13:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovsqs7</id>
    <title>Insane week for LLMs</title>
    <updated>2025-11-13T05:20:18+00:00</updated>
    <author>
      <name>/u/Interesting-Gur4782</name>
      <uri>https://old.reddit.com/user/Interesting-Gur4782</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"&gt; &lt;img alt="Insane week for LLMs" src="https://a.thumbs.redditmedia.com/2aUG7XKWHqAxYbI-FDYGKyjPfg5dj4REmjXzm6-2nm8.jpg" title="Insane week for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the past week, we've gotten...&lt;/p&gt; &lt;p&gt;- GPT 5.1&lt;/p&gt; &lt;p&gt;- Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;- 12+ stealth endpoints across LMArena, Design Arena, and OpenRouter, with more coming in just the past day&lt;/p&gt; &lt;p&gt;- Speculation about an imminent GLM 5 drop on X&lt;/p&gt; &lt;p&gt;- A 4B model that beats several SOTA models on front-end fine-tuned using a new agentic reward system&lt;/p&gt; &lt;p&gt;It's a great time for new models and an even better time to be running a local setup. Looking forward to what the labs can cook up before the end of the year (looking at you Z.ai)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b46881agly0g1.png?width=1892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16dfc05b6c2989ae933201911e8d326c473a3402"&gt;https://preview.redd.it/b46881agly0g1.png?width=1892&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16dfc05b6c2989ae933201911e8d326c473a3402&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Gur4782"&gt; /u/Interesting-Gur4782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovsqs7/insane_week_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T05:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow1bmr</id>
    <title>Gain 60% performance on RDNA 4 using this fix</title>
    <updated>2025-11-13T13:39:18+00:00</updated>
    <author>
      <name>/u/Sea-Speaker1700</name>
      <uri>https://old.reddit.com/user/Sea-Speaker1700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/issues/28649"&gt;https://github.com/vllm-project/vllm/issues/28649&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is verified to work and perform well and is stable.&lt;/p&gt; &lt;p&gt;TLDR: AMD enabled native FP8 on Mi350x and prepped the work for RDNA but fell short of fully including it. I finished the job. It's a rough initial version, but already gives 60% speed benefit in Q330b-A3B-2507. Tuning the config files further will result in more gains.&lt;/p&gt; &lt;p&gt;If you want your RDNA 4 cards to go fast, here you go, since AMD can't be bothered to support their hardware I did their job for them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sea-Speaker1700"&gt; /u/Sea-Speaker1700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow1bmr/gain_60_performance_on_rdna_4_using_this_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow1bmr/gain_60_performance_on_rdna_4_using_this_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow1bmr/gain_60_performance_on_rdna_4_using_this_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T13:39:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovv95d</id>
    <title>Stanford's new Equivariant Encryption enables private AI inference with zero slowdown - works with any symmetric encryption</title>
    <updated>2025-11-13T07:51:50+00:00</updated>
    <author>
      <name>/u/Proof-Possibility-54</name>
      <uri>https://old.reddit.com/user/Proof-Possibility-54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this paper (arXiv:2502.01013) that could be huge for private local model deployment.&lt;/p&gt; &lt;p&gt;The researchers achieved 99.999% accuracy on encrypted neural network inference with literally zero additional latency. Not &amp;quot;minimal&amp;quot; overhead - actually zero.&lt;/p&gt; &lt;p&gt;The key insight: instead of using homomorphic encryption (10,000x slowdown), they train networks to use &amp;quot;equivariant functions&amp;quot; that commute with encryption operations. So you can compute directly on AES or ChaCha20 encrypted data.&lt;/p&gt; &lt;p&gt;What this means for local LLMs:&lt;/p&gt; &lt;p&gt;- Your prompts could remain encrypted in memory&lt;/p&gt; &lt;p&gt;- Model weights could be encrypted at rest&lt;/p&gt; &lt;p&gt;- No performance penalty for privacy&lt;/p&gt; &lt;p&gt;The catch: you need to retrain models with their specific architecture constraints. Can't just plug this into existing models.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2502.01013"&gt;https://arxiv.org/abs/2502.01013&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also made a technical breakdown analyzing the limitations they gloss over: &lt;a href="https://youtu.be/PXKO5nkVLI4"&gt;https://youtu.be/PXKO5nkVLI4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone see potential applications for local assistant privacy? The embedding layer limitations seem like the biggest bottleneck for LLM applications.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Possibility-54"&gt; /u/Proof-Possibility-54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovv95d/stanfords_new_equivariant_encryption_enables/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovv95d/stanfords_new_equivariant_encryption_enables/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovv95d/stanfords_new_equivariant_encryption_enables/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T07:51:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow277f</id>
    <title>Fire in the Hole! Benchmarking is broken</title>
    <updated>2025-11-13T14:16:24+00:00</updated>
    <author>
      <name>/u/Substantial_Sail_668</name>
      <uri>https://old.reddit.com/user/Substantial_Sail_668</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks are broken - everybody is &lt;strong&gt;benchmaxxing rather than benchmarking&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;In the other discussion (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ovbssf/is_polish_better_for_prompting_llms_case_study/"&gt;link&lt;/a&gt;) some guys mentioned &lt;strong&gt;data leakage.&lt;/strong&gt; But it's only one of the problems. &lt;strong&gt;Selective reporting&lt;/strong&gt;, &lt;strong&gt;bias, noisy metrics&lt;/strong&gt; and &lt;strong&gt;private leaderboards&lt;/strong&gt; - just to name a couple more.&lt;/p&gt; &lt;p&gt;Of course a few projects are trying to fix this, each with trade-offs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;HELM (Stanford):&lt;/strong&gt; broad, multi-metric evaluation ‚Äî but static between releases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynabench (Meta):&lt;/strong&gt; human-in-the-loop adversarial data ‚Äî great idea, limited scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiveBench:&lt;/strong&gt; rolling updates to stay fresh ‚Äî still centralized and small-team-dependent.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BIG-Bench Hard:&lt;/strong&gt; community-built hard tasks ‚Äî but once public, they leak fast.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chatbot / LM Arena:&lt;/strong&gt; open human voting ‚Äî transparent, but noisy and unverified.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Curious to hear which of these tools you guys use and why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've written a longer article about that if you're interested: &lt;a href="https://medium.com/@peerbench/the-benchmark-trap-why-measuring-ai-progress-has-become-so-hard-9ee5f9ee129e"&gt;medium article&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Sail_668"&gt; /u/Substantial_Sail_668 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow277f/fire_in_the_hole_benchmarking_is_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow277f/fire_in_the_hole_benchmarking_is_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow277f/fire_in_the_hole_benchmarking_is_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T14:16:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow03a6</id>
    <title>Interesting to see an open-source model genuinely compete with frontier proprietary models for coding</title>
    <updated>2025-11-13T12:44:35+00:00</updated>
    <author>
      <name>/u/Technical_Gene4729</name>
      <uri>https://old.reddit.com/user/Technical_Gene4729</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow03a6/interesting_to_see_an_opensource_model_genuinely/"&gt; &lt;img alt="Interesting to see an open-source model genuinely compete with frontier proprietary models for coding" src="https://preview.redd.it/l3lt0757s01g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7f3256b064baacc927a42b0d21aa1946bac509bb" title="Interesting to see an open-source model genuinely compete with frontier proprietary models for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So Code Arena just dropped their new live coding benchmark, and the tier 1 results are sparking an interesting open vs proprietary debate.&lt;/p&gt; &lt;p&gt;GLM-4.6 is the only open-source model in the top tier. It's MIT licensed, the most permissive license possible. It's sitting at rank 1 (score: 1372) alongside Claude Opus and GPT-5.&lt;/p&gt; &lt;p&gt;What makes Code Arena different is that it's not static benchmarks. Real developers vote on actual functionality, code quality, and design. Models have to plan, scaffold, debug, and build working web apps step-by-step using tools just like human engineers.&lt;/p&gt; &lt;p&gt;The score gap among the tier 1 clusters is only ~2%. For context, every other model in ranks 6-10 is either proprietary or Apache 2.0 licensed, and they're 94-250 points behind.&lt;/p&gt; &lt;p&gt;This raises some questions. Are we reaching a point where open models can genuinely match frontier proprietary performance for specialized tasks? Or does this only hold for coding, where training data is more abundant?&lt;/p&gt; &lt;p&gt;The fact that it's MIT licensed (not just &amp;quot;open weights&amp;quot;) means you can actually build products with it, modify the architecture, deploy without restrictions, not just run it locally.&lt;/p&gt; &lt;p&gt;Community voting is still early (576-754 votes per model), but it's evaluating real-world functionality, not just benchmark gaming. You can watch the models work: reading files, debugging, iterating.&lt;/p&gt; &lt;p&gt;They're adding multi-file codebases and React support next, which will test architectural planning even more.&lt;/p&gt; &lt;p&gt;Do you think open models will close the gap across the board, or will proprietary labs always stay ahead? And does MIT vs Apache vs &amp;quot;weights only&amp;quot; licensing actually matter for your use cases?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical_Gene4729"&gt; /u/Technical_Gene4729 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l3lt0757s01g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow03a6/interesting_to_see_an_opensource_model_genuinely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow03a6/interesting_to_see_an_opensource_model_genuinely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T12:44:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovs6ut</id>
    <title>llama.cpp and Qwen 2.5 running on bare metal Windows XP x64 without any compatibility layers</title>
    <updated>2025-11-13T04:50:33+00:00</updated>
    <author>
      <name>/u/PANCHO7532</name>
      <uri>https://old.reddit.com/user/PANCHO7532</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovs6ut/llamacpp_and_qwen_25_running_on_bare_metal/"&gt; &lt;img alt="llama.cpp and Qwen 2.5 running on bare metal Windows XP x64 without any compatibility layers" src="https://preview.redd.it/hg1xeqvuey0g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=223d93e64bfc8f846a673473dfbaaae88ede30a6" title="llama.cpp and Qwen 2.5 running on bare metal Windows XP x64 without any compatibility layers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Slowness aside, surprisingly llama.cpp can be cross-compiled using MinGW and you can actually run it on Windows XP with only a few tweaks! I only have the x64 edition on this laptop so not really sure if it also works on x86&lt;/p&gt; &lt;p&gt;All tools are working without any problems, even the CLI and server tools (pictured), though i'm fairly sure that you can squeeze a token or two more by using the CLI instead of the server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PANCHO7532"&gt; /u/PANCHO7532 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hg1xeqvuey0g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovs6ut/llamacpp_and_qwen_25_running_on_bare_metal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovs6ut/llamacpp_and_qwen_25_running_on_bare_metal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T04:50:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3anq</id>
    <title>Rejected for not using LangChain/LangGraph?</title>
    <updated>2025-11-13T15:00:14+00:00</updated>
    <author>
      <name>/u/dougeeai</name>
      <uri>https://old.reddit.com/user/dougeeai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today I got rejected after a job interview for not being &amp;quot;technical enough&amp;quot; because I use PyTorch/CUDA/GGUF directly with FastAPI microservices for multi-agent systems instead of LangChain/LangGraph in production.&lt;/p&gt; &lt;p&gt;They asked about 'efficient data movement in LangGraph' - I explained I work at a lower level with bare metal for better performance and control. Later it was revealed they mostly just use APIs to Claude/OpenAI/Bedrock.&lt;/p&gt; &lt;p&gt;I am legitimately asking - not venting - Am I missing something by not using LangChain? Is it becoming a required framework for AI engineering roles, or is this just framework bias?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Should I be adopting it even though I haven't seen performance benefits for my use cases?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougeeai"&gt; /u/dougeeai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3anq/rejected_for_not_using_langchainlanggraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:00:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ow3kj3</id>
    <title>Qwen model coming soon üëÄ</title>
    <updated>2025-11-13T15:10:39+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt; &lt;img alt="Qwen model coming soon üëÄ" src="https://preview.redd.it/ibsrtr3ri11g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91dd786919de8cd49f495890d2b241fd22bf2f83" title="Qwen model coming soon üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ibsrtr3ri11g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ow3kj3/qwen_model_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T15:10:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ovxksu</id>
    <title>Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x</title>
    <updated>2025-11-13T10:22:48+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt; &lt;img alt="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" src="https://external-preview.redd.it/bmthZnk4cjV4ejBnMYgdXr3Xr8K8l3LMKEIqfiXLStzaSkNnB6704_pmF3PX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0626efa53bd219b2126a6e5fa2884ec700c482b3" title="Jan-v2-VL: 8B model for long-horizon tasks, improving Qwen3-VL-8B‚Äôs agentic capabilities almost 10x" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team. We‚Äôre releasing Jan-v2-VL, an 8B vision‚Äìlanguage model aimed at long-horizon, multi-step tasks starting from browser use.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-high executes 49 steps without failure on the Long-Horizon Execution benchmark, while the base model (Qwen3-VL-8B-Thinking) stops at 5 and other similar-scale VLMs stop between 1 and 2.&lt;/p&gt; &lt;p&gt;Across text and multimodal benchmarks, it matches or slightly improves on the base model, so you get higher long-horizon stability without giving up reasoning or vision quality.&lt;/p&gt; &lt;p&gt;We're releasing 3 variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v2-VL-low (efficiency-oriented)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-med (balanced)&lt;/li&gt; &lt;li&gt;Jan-v2-VL-high (deeper reasoning and longer execution)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How to run the model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Download Jan-v2-VL from the Model Hub in Jan&lt;/li&gt; &lt;li&gt;Open the model‚Äôs settings and enable Tools and Vision&lt;/li&gt; &lt;li&gt;Enable BrowserUse MCP (or your preferred MCP setup for browser control)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also run the model with vLLM or llama.cpp.&lt;/p&gt; &lt;p&gt;Recommended parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;temperature: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_p: 0.95&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;top_k: 20&lt;/code&gt;&lt;/li&gt; &lt;li&gt;repetition_penalty&lt;code&gt;: 1.0&lt;/code&gt;&lt;/li&gt; &lt;li&gt;presence_penalty&lt;code&gt;: 1.5&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/collections/janhq/jan-v2-vl"&gt;https://huggingface.co/collections/janhq/jan-v2-vl&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Jan app: &lt;a href="https://github.com/janhq/jan"&gt;https://github.com/janhq/jan&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're also working on a browser extension to make model-driven browser automation faster and more reliable on top of this.&lt;/p&gt; &lt;p&gt;Credit to the Qwen team for the Qwen3-VL-8B-Thinking base model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/go4j38r5xz0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ovxksu/janv2vl_8b_model_for_longhorizon_tasks_improving/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-13T10:22:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
