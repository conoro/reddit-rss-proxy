<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-31T16:52:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pzhcqu</id>
    <title>Any guesses?</title>
    <updated>2025-12-30T12:52:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt; &lt;img alt="Any guesses?" src="https://preview.redd.it/xqvj95zv8cag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=30044fbca7ba499223943c95d7d236600fdbb10e" title="Any guesses?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xqvj95zv8cag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzhcqu/any_guesses/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T12:52:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0hg8z</id>
    <title>Android LLM Client with Hardware Acceleration?</title>
    <updated>2025-12-31T16:45:47+00:00</updated>
    <author>
      <name>/u/nikunjuchiha</name>
      <uri>https://old.reddit.com/user/nikunjuchiha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm aware of MLC Chat but it's too basic, doesn't seem to get updates anymore and also doesn't allow importing your own models.&lt;/p&gt; &lt;p&gt;Is there any other app with hardware acceleration? Preferably FOSS. My SoC has a NPU chip, i'd like to use it. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nikunjuchiha"&gt; /u/nikunjuchiha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hg8z/android_llm_client_with_hardware_acceleration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hg8z/android_llm_client_with_hardware_acceleration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hg8z/android_llm_client_with_hardware_acceleration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T16:45:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0hk7y</id>
    <title>Llama 3.2 3B fMRI - Circuit Tracing Findings</title>
    <updated>2025-12-31T16:50:26+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hk7y/llama_32_3b_fmri_circuit_tracing_findings/"&gt; &lt;img alt="Llama 3.2 3B fMRI - Circuit Tracing Findings" src="https://b.thumbs.redditmedia.com/sQTnXpq9YFmQmTa0YkAv02T9lSbrgVGvCHBVvXA3K-Q.jpg" title="Llama 3.2 3B fMRI - Circuit Tracing Findings" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those that have been following along, you'll know that I came up with a way to attempt to trace distributed mechanisms. Essentially, I am:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;capturing per-token hidden activations across all layers&lt;/li&gt; &lt;li&gt;building a sliding time window per dimension&lt;/li&gt; &lt;li&gt;computing Pearson correlation between one chosen hero dim and all other dims&lt;/li&gt; &lt;li&gt;selecting the top-K strongest correlations (by absolute value) per layer and timestep&lt;/li&gt; &lt;li&gt;logging raw activation values + correlation sign&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What stood out pretty quickly:&lt;/p&gt; &lt;h1&gt;1) Most correlated dims are transient&lt;/h1&gt; &lt;p&gt;Many dims show up strongly for a short burst ‚Äî e.g. 5‚Äì15 tokens in a specific layer ‚Äî then disappear entirely. These often vary by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;prompt&lt;/li&gt; &lt;li&gt;chunk of the prompt&lt;/li&gt; &lt;li&gt;layer&lt;/li&gt; &lt;li&gt;local reasoning phase&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This looks like short-lived subroutines rather than stable features.&lt;/p&gt; &lt;h1&gt;2) Some dims persist, but only in specific layers&lt;/h1&gt; &lt;p&gt;Certain dims stay correlated for long stretches, but only at particular depths (e.g. consistently at layer ~22, rarely elsewhere). These feel like mid-to-late control or ‚Äúmode‚Äù signals.&lt;/p&gt; &lt;h1&gt;3) A small set of dims recur everywhere&lt;/h1&gt; &lt;p&gt;Across different prompts, seeds, layers, and prompt styles, a handful of dims keep reappearing. These are rare, but very noticeable.&lt;/p&gt; &lt;h1&gt;4) Polarity is stable&lt;/h1&gt; &lt;p&gt;When a dim reappears, its &lt;strong&gt;sign never flips&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Example:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;dim X is &lt;em&gt;always&lt;/em&gt; positive when it appears&lt;/li&gt; &lt;li&gt;dim Y is &lt;em&gt;always&lt;/em&gt; negative when it appears The magnitude varies, but the polarity does not.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn‚Äôt intervention or gradient data ‚Äî it‚Äôs raw activations ‚Äî so what this really means is that these dims have &lt;strong&gt;stable axis orientation&lt;/strong&gt;. When they engage, they always push the representation in the same direction.&lt;/p&gt; &lt;h1&gt;My current interpretation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The majority of correlated dims are context-local and noisy (expected).&lt;/li&gt; &lt;li&gt;A smaller group are persistent but layer-specific.&lt;/li&gt; &lt;li&gt;A very small set appear to be &lt;strong&gt;global, sign-stable features&lt;/strong&gt; that consistently co-move with the hero dim regardless of prompt or depth.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My next step is to stop looking at per-window ‚Äúpretty pictures‚Äù and instead rank dims globally by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;presence rate&lt;/li&gt; &lt;li&gt;prompt coverage&lt;/li&gt; &lt;li&gt;layer coverage&lt;/li&gt; &lt;li&gt;persistence (run length)&lt;/li&gt; &lt;li&gt;sign stability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to isolate those few recurring dims and then test whether they‚Äôre:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;real control handles&lt;/li&gt; &lt;li&gt;general ‚Äúconfidence / entropy‚Äù proxies&lt;/li&gt; &lt;li&gt;or something more interesting&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If anyone has done similar correlation-based filtering or has suggestions on better ways to isolate global feature dims before moving to causal intervention, I‚Äôd love to hear it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l88ej7vwjkag1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dec7f6e36a078ef0fb04783730be5ae31667c085"&gt;https://preview.redd.it/l88ej7vwjkag1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=dec7f6e36a078ef0fb04783730be5ae31667c085&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yitls4uzjkag1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3323bfa3059e351c051a924baa585ec3ed903677"&gt;https://preview.redd.it/yitls4uzjkag1.png?width=1592&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3323bfa3059e351c051a924baa585ec3ed903677&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hk7y/llama_32_3b_fmri_circuit_tracing_findings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hk7y/llama_32_3b_fmri_circuit_tracing_findings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0hk7y/llama_32_3b_fmri_circuit_tracing_findings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T16:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzv2es</id>
    <title>I built a platform where LLMs play Mafia against each other. Turns out they're great liars but terrible detectives.</title>
    <updated>2025-12-30T21:59:24+00:00</updated>
    <author>
      <name>/u/mehyay76</name>
      <uri>https://old.reddit.com/user/mehyay76</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzv2es/i_built_a_platform_where_llms_play_mafia_against/"&gt; &lt;img alt="I built a platform where LLMs play Mafia against each other. Turns out they're great liars but terrible detectives." src="https://preview.redd.it/fs0upsefyeag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=adfa75469d06c2d0d37acbc9d2a1370911681bbc" title="I built a platform where LLMs play Mafia against each other. Turns out they're great liars but terrible detectives." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mehyay76"&gt; /u/mehyay76 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fs0upsefyeag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzv2es/i_built_a_platform_where_llms_play_mafia_against/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzv2es/i_built_a_platform_where_llms_play_mafia_against/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T21:59:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0dkm4</id>
    <title>GitHub - JosefAlbers/VL-JEPA: VL-JEPA in MLX</title>
    <updated>2025-12-31T13:55:59+00:00</updated>
    <author>
      <name>/u/JosefAlbers05</name>
      <uri>https://old.reddit.com/user/JosefAlbers05</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0dkm4/github_josefalbersvljepa_vljepa_in_mlx/"&gt; &lt;img alt="GitHub - JosefAlbers/VL-JEPA: VL-JEPA in MLX" src="https://external-preview.redd.it/QZctN4h1azoYqe_-jFsVBLpHHKEMRIhFW5wXMu1NIBw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d53e8a0611825f476a5ae50e4847b8d1212b024" title="GitHub - JosefAlbers/VL-JEPA: VL-JEPA in MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JosefAlbers05"&gt; /u/JosefAlbers05 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JosefAlbers/VL-JEPA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0dkm4/github_josefalbersvljepa_vljepa_in_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0dkm4/github_josefalbersvljepa_vljepa_in_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:55:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0chm3</id>
    <title>Can we sample DPO data from the same dataset that was used for LoRA training?</title>
    <updated>2025-12-31T13:01:29+00:00</updated>
    <author>
      <name>/u/Clean_Radish8983</name>
      <uri>https://old.reddit.com/user/Clean_Radish8983</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to understand best practices around data usage when combining LoRA fine-tuning with Direct Preference Optimization (DPO), and I would appreciate insights from people who have done this in practice.&lt;/p&gt; &lt;p&gt;Specifically, is it acceptable (or advisable) to sample DPO preference data from the same underlying dataset that was already used to train a LoRA adapter?&lt;/p&gt; &lt;p&gt;To clarify the setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A base model is first adapted using LoRA, trained on a supervised dataset (e.g., instruction - response pairs).&lt;/li&gt; &lt;li&gt;After that, DPO is applied to further align the model using preference pairs (chosen vs. rejected responses).&lt;/li&gt; &lt;li&gt;The question is whether those DPO preference pairs can be derived from the same original dataset used for LoRA training, rather than from a completely separate corpus.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I would be especially interested in:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Empirical results comparing reused vs. disjoint datasets for LoRA + DPO&lt;/li&gt; &lt;li&gt;Recommended data-splitting strategies if reuse is acceptable&lt;/li&gt; &lt;li&gt;Any failure modes observed when the same data source is used across both stages&lt;/li&gt; &lt;li&gt;Thanks in advance looking forward to hearing how others handle this in real-world pipelines.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clean_Radish8983"&gt; /u/Clean_Radish8983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0chm3/can_we_sample_dpo_data_from_the_same_dataset_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0chm3/can_we_sample_dpo_data_from_the_same_dataset_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0chm3/can_we_sample_dpo_data_from_the_same_dataset_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzlx9w</id>
    <title>How llama.cpp implements 2.9x faster top-k sampling with bucket sort</title>
    <updated>2025-12-30T16:05:58+00:00</updated>
    <author>
      <name>/u/noninertialframe96</name>
      <uri>https://old.reddit.com/user/noninertialframe96</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"&gt; &lt;img alt="How llama.cpp implements 2.9x faster top-k sampling with bucket sort" src="https://external-preview.redd.it/CZbcx8kLlfmOhgEbqLeuBvYqHwSKzwQAaRIJW4yJFv8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6426b29c1a766614bad4ab7965774390635e0620" title="How llama.cpp implements 2.9x faster top-k sampling with bucket sort" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I looked into how llama.cpp optimizes top-k sampling, and the trick is surprisingly simple.&lt;/p&gt; &lt;p&gt;Top-k on Llama 3's 128K vocabulary means finding k highest scores out of 128,256 candidates. std::partial_sort does this at O(n log k), but llama.cpp noticed that token logits cluster in a narrow range (-10 to +10).&lt;/p&gt; &lt;p&gt;So instead of sorting, it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Builds a 128-bucket histogram over the logit range&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Walks from the highest bucket down until it accumulates k tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Only sorts those survivors&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noninertialframe96"&gt; /u/noninertialframe96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://codepointer.substack.com/p/llamacpp-accelerate-top-k-sampling"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzlx9w/how_llamacpp_implements_29x_faster_topk_sampling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T16:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0fqfs</id>
    <title>Llama 3.3 8B Instruct Abliterated (MPOA)</title>
    <updated>2025-12-31T15:33:29+00:00</updated>
    <author>
      <name>/u/Perfect_Biscotti_476</name>
      <uri>https://old.reddit.com/user/Perfect_Biscotti_476</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made an abliterated version of Llama 3.3 8B Instruct (based on shb777/Llama-3.3-8B-Instruct) with MPOA technique (&lt;a href="https://github.com/jim-plus/llm-abliteration"&gt;https://github.com/jim-plus/llm-abliteration&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;Please find the model at &lt;a href="https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA"&gt;https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF files: &lt;a href="https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA-GGUF"&gt;https://huggingface.co/YanLabs/Llama-3.3-8B-Instruct-MPOA-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Perfect_Biscotti_476"&gt; /u/Perfect_Biscotti_476 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0fqfs/llama_33_8b_instruct_abliterated_mpoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T15:33:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzxtnr</id>
    <title>I benchmarked 7 Small LLMs on a 16GB Laptop. Here is what is actually usable.</title>
    <updated>2025-12-30T23:54:46+00:00</updated>
    <author>
      <name>/u/Peach_Baker</name>
      <uri>https://old.reddit.com/user/Peach_Baker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since we're not dropping $5k rigs to run AI anymore, I wanted to see what was actually possible on my daily driver (Standard 16GB RAM laptop).&lt;/p&gt; &lt;p&gt;I tested Qwen 2.5 (14B), Mistral Small (12B), Llama 3 (8B), and Gemma 3 (all 4-bit quants) to see which ones I could actually run without crashing my laptop.&lt;/p&gt; &lt;p&gt;The Winners (TL;DR):&lt;/p&gt; &lt;p&gt;- Qwen 2.5 (14B): The smartest for coding, but it eats 11GB System RAM + Context. On a 16GB laptop, if I opened 3 Chrome tabs, it crashed immediately (OOM).&lt;/p&gt; &lt;p&gt;- Mistral Small (12B): The sweet spot. Decent speeds, but still forces Windows to aggressively swap if you multitask.&lt;/p&gt; &lt;p&gt;- Llama-3-8B: Runs fine, but the reasoning capabilities are falling behind the newer 12B+ class.&lt;/p&gt; &lt;p&gt;- Gemma 3 (9B): Great instruction following, but heavier than Llama.&lt;/p&gt; &lt;p&gt;Since RAM prices are skyrocketing right now (DDR5 kits hitting 200+) &lt;/p&gt; &lt;p&gt;I used 16gb Swapping to NVMe (1-2 tokens/sec) the moment I opened Docker. Unusable.&lt;/p&gt; &lt;p&gt;Then, i Kept the full 14B model + Docker + Chrome in memory with 32GB. It runs smooth and responsive (no swap lag).&lt;/p&gt; &lt;p&gt;So, before you think of selling your kidney to drop $2,000 on a 4090, check your system RAM. I found a few non-scalped 32GB/64GB kits that are still in stock for reasonable prices and listed them in my full benchmark write-up here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@jameshugo598/the-2026-local-llm-hardware-guide-surviving-the-ram-crisis-fa67e8c95804"&gt;&lt;strong&gt;https://medium.com/@jameshugo598/the-2026-local-llm-hardware-guide-surviving-the-ram-crisis-fa67e8c95804&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt; Is anyone else seeing their local prices for DDR5 hitting $250, or is it just my region?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Peach_Baker"&gt; /u/Peach_Baker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzxtnr/i_benchmarked_7_small_llms_on_a_16gb_laptop_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T23:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzsqii</id>
    <title>15M param model solving 24% of ARC-AGI-2 (Hard Eval). Runs on consumer hardware.</title>
    <updated>2025-12-30T20:24:19+00:00</updated>
    <author>
      <name>/u/Doug_Bitterbot</name>
      <uri>https://old.reddit.com/user/Doug_Bitterbot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We anticipate getting a lot of push back from the community on this, and that's why we've uploaded the repo and have open sourced everything - we want people to verify these results. We are very excited!!&lt;/p&gt; &lt;p&gt;We (Bitterbot AI) have just dropped the repo for &lt;strong&gt;TOPAS-DSPL&lt;/strong&gt;. It‚Äôs a tiny recursive model (~24M params) we‚Äôve been working on to beat the drift issues in standard transformers.&lt;/p&gt; &lt;p&gt;We ran it against the ARC-AGI-2 evaluation set and hit &lt;strong&gt;24% accuracy&lt;/strong&gt;. For context, the previous SOTA for this size class (TRM) sits around 8%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture (Why it works):&lt;/strong&gt; instead of a monolithic transformer, we split the inference into two streams (&amp;quot;Bicameral&amp;quot;):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Logic Stream:&lt;/strong&gt; Plans the algorithm (rule generation).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Canvas Stream:&lt;/strong&gt; Handles the grid physics/execution.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This separation prevents the model from forgetting the rule while trying to generate the pixels (Compositional Drift). We also implemented &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt; so it fine-tunes on the specific puzzle examples before generating a solution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training: Single RTX 4090.&lt;/li&gt; &lt;li&gt;Inference: Very fast (it's only 24M params).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Code:&lt;/strong&gt; We open-sourced the whole pipeline (Data gen, Training, Evaluator). LINK BELOW (I don't want this to get flagged as spam or self promotion). The README file is very detailed.&lt;/p&gt; &lt;p&gt;If anyone has a spare 4090 and wants to verify the evals, let me know if you can repro the 24%. We're seeing convergence around 50k epochs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Doug_Bitterbot"&gt; /u/Doug_Bitterbot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzsqii/15m_param_model_solving_24_of_arcagi2_hard_eval/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzsqii/15m_param_model_solving_24_of_arcagi2_hard_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzsqii/15m_param_model_solving_24_of_arcagi2_hard_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T20:24:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0g1ye</id>
    <title>I built a pipeline to extract executive compensation data from SEC filings using MinerU + VLMs</title>
    <updated>2025-12-31T15:47:18+00:00</updated>
    <author>
      <name>/u/Logical_Delivery8331</name>
      <uri>https://old.reddit.com/user/Logical_Delivery8331</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I scraped about 100k DEF-14A proxy statements from the SEC a while back and finally decided to do something with them.&lt;/p&gt; &lt;p&gt;I built a pipeline that extracts Summary Compensation Tables from these filings. It uses MinerU to parse PDFs and extract table images, then Qwen3-VL-32B to classify which tables are actually compensation tables and extract structured JSON from them.&lt;/p&gt; &lt;p&gt;The main challenges were handling tables split across multiple pages and dealing with format changes between pre-2006 and post-2006 filings.&lt;/p&gt; &lt;p&gt;It's still a work in progress with some bugs (duplicate tables, occasional parsing errors), but the pipeline is currently running to build a full dataset from 2005 to today covering all US public companies.&lt;/p&gt; &lt;p&gt;Code and a sample of the dataset are available if anyone wants to take a look or contribute.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pierpierpy/Execcomp-AI"&gt;https://github.com/pierpierpy/Execcomp-AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace sample: &lt;a href="https://huggingface.co/datasets/pierjoe/execcomp-ai-sample"&gt;https://huggingface.co/datasets/pierjoe/execcomp-ai-sample&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Logical_Delivery8331"&gt; /u/Logical_Delivery8331 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0g1ye/i_built_a_pipeline_to_extract_executive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0g1ye/i_built_a_pipeline_to_extract_executive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0g1ye/i_built_a_pipeline_to_extract_executive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T15:47:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzt1q8</id>
    <title>LLM server gear: a cautionary tale of a $1k EPYC motherboard sale gone wrong on eBay</title>
    <updated>2025-12-30T20:36:46+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;or: selling high-end LLM server gear is more fraught with risk than I realized.&lt;/p&gt; &lt;h3&gt;AI Disclosure&lt;/h3&gt; &lt;p&gt;This was written entirely by hand on my laptop in Sublime Text with zero AI involvement. Shit, I didn't even use spell check. All mistakes are my own.&lt;/p&gt; &lt;h3&gt;tl;dr&lt;/h3&gt; &lt;p&gt;During an &amp;quot;Item Not As Described (INAD)&amp;quot; dispute, eBay ALWAYS sides with the buyer until the very last steps of the case no matter what the circumstances, despite all evidence, and in the face of all immediately obvious reason, logic, and common sense. Except it makes perfect sense and you might not even lose your money. Allow me to elaborate.&lt;/p&gt; &lt;h3&gt;The Sale&lt;/h3&gt; &lt;p&gt;Rewind to October 2025 when I replaced the incumbent Gigabyte MZ33-AR1 Epyc Zen5 motherboard with a Supermicro H14SSL-N for my inference rig. Long story short: don't use Gigabyte motherboards for 4-way Blackwell GPU setups unless sado-masochism is your thing. Anyway, I sold it to a seemingly nice chap on eBay for $900. He seemed a bit clueless about Epyc and compatibility issues, but we exchanged messages and he decided to go ahead with the &amp;quot;no returns&amp;quot; purchase of the as-new MZ33-AR1.&lt;/p&gt; &lt;p&gt;Original box. All the case candy. As new. Undamaged. Fully working. With hi-res photos (taken on a Nikon D7000 with Nikon 17-55 f2.8 glass and processed in Capture One Pro) of all areas of the motherboard and CPU socket. This is important. &lt;/p&gt; &lt;h3&gt;The Buyer&lt;/h3&gt; &lt;p&gt;Fast forward a week or so: buyer hits me up with a bunch of Dr Debug codes (although he doesn't know they're Dr Debug codes, he just pulled &amp;quot;error codes&amp;quot; from the BMC) claiming the motherboard won't boot. I did him the solid of explaining Dr Debug and I provided a link to an explanation of the codes (&lt;a href="https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364"&gt;https://forum.level1techs.com/t/list-of-dr-debug-bios-codes/114364&lt;/a&gt;). He was having issues with CPU initialization. I told him that sometimes re-seating CPU and RAM can help with these sorts of issues.&lt;/p&gt; &lt;p&gt;Re-seating. This is also important.&lt;/p&gt; &lt;p&gt;Next day he hits me up again: will I accept a return? No, because having installation difficulties is not a valid reason for return. Then nothing. Silence.&lt;/p&gt; &lt;h3&gt;The Refund Claim&lt;/h3&gt; &lt;p&gt;Cue the &lt;em&gt;very last day of the return window&lt;/em&gt;: I get hit with an &amp;quot;item not as described&amp;quot; refund claim. Get this, the buyer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;uploaded photos of the motherboard with a bent and twisted CPU pin.&lt;/li&gt; &lt;li&gt;uploaded a photo of a blank white silkscreen rectangle on the motherboard with a giant red arrow pointing to it and a comment saying &amp;quot;the motherboard is fake because of this white area&amp;quot;.&lt;/li&gt; &lt;li&gt;showed a photo of the computer monitor displaying the BMC interface in which the serial number of the BMC software was 1234567890ABCDEF. He claimed therefore the motherboard was a fake.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;WTF. I simultaneously exploded with rage at being accused of selling broken gear as working gear, while exploding with incredulity at the stupidity of trying to assert both damage AND blatantly ridiculous fakery in the same refund claim! My dude should have really picked just one fraudulent claim to keep it somewhat realistic, not two. I calmed down and figured the buyer probably bent the pins in a ham-fisted attempt to re-seat everything. No problem, I thought. I'll explain to eBay what's happening and they'll see reason before shutting this clown down. So I started going through the claim dispute process...&lt;/p&gt; &lt;h3&gt;The Process&lt;/h3&gt; &lt;p&gt;...oh, the process. It's designed to (a) refund the buyer at the seller's cost in all cases, (b) be so egregiously demoralizing, time-consuming, and administratively difficult for sellers that they are incentivized to simply give up and accept the fleecing, and (c) automate as much of this process with as few humans in the loop as possible while simultaenously providing as few opportunities as possible for sellers to initiate any communication with eBay.&lt;/p&gt; &lt;p&gt;It went like this over a period of TWO MONTHS:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Report the buyer for &amp;quot;abusing the returns process&amp;quot;.&lt;/li&gt; &lt;li&gt;With the new &amp;quot;case&amp;quot;, it's possible to upload a set of photos and a block of text to refute the buyer's claim(s). &lt;/li&gt; &lt;li&gt;I uploaded ALL the hi-res photos I took for the listing's photoshoot in which it was abuntandly clear the motherboard was in perfect condition.&lt;/li&gt; &lt;li&gt;I also went to Gigabyte and found the page on the BMC's usermanual containing a screenshot showing the same serial number claimed by the buyer.&lt;/li&gt; &lt;li&gt;I went to Gigabyte's MZ33-AR1 web page and found a photo of the motherboard showing exactly the same white rectangle the buyer had called out as fakery.&lt;/li&gt; &lt;li&gt;Boom! Done! Solid documentary refutation of all the buyer's claims. Case closed. So I thought.&lt;/li&gt; &lt;li&gt;eBay found in favor of the buyer and instructed me to issue a return label.&lt;/li&gt; &lt;li&gt;I refused, outraged. No, I said. Look at the photos! He's lying!&lt;/li&gt; &lt;li&gt;eBay sent the buyer a label at my expense. He returned the motherboard with its busted CPU pin.&lt;/li&gt; &lt;li&gt;I again reported the buyer, showed photos of before and after damage, clearly showing he did the damage, not me.&lt;/li&gt; &lt;li&gt;eBay found in favor of the buyer AGAIN and deducted the full cost of the refund from my account.&lt;/li&gt; &lt;li&gt;Apoplectic, I hit the &amp;quot;appeal&amp;quot; button. I was taken to a webpage that said &amp;quot;we'll call you in 3 minutes&amp;quot;. WTF?&lt;/li&gt; &lt;li&gt;5 minutes later i got a call from eBay. &lt;/li&gt; &lt;li&gt;After briefly explaining the situation to a very engaged US-sounding representative, she told me I needed to do a couple of things: &lt;ul&gt; &lt;li&gt;Take the text of an email they just sent me (a Disclosure where I swear everything I told eBay is true) and paste it into a Word doc&lt;/li&gt; &lt;li&gt;Insert a photo/picture of my ink-written signature (luckily I have a scan of exactly that for business reasons).&lt;/li&gt; &lt;li&gt;Convert to PDF and upload to the secret link in the email they sent.&lt;/li&gt; &lt;li&gt;No joke, the lady actually stayed on the phone while I did all this! She received the PDF just seconds after I uploaded it.&lt;/li&gt; &lt;li&gt;This is, I am sure, mostly just another way of making it difficult to actually reverse the appeal.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;But the rep was good to her word: eBay immediately reversed the decision and the money is back in my account as if the sale had happened like normal. I guess both me and the buyer got our money.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;If It Happens To You&lt;/h3&gt; &lt;p&gt;My advice if this happens to you: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Accept that no human cares about your case until the very, very last minutes of MONTHS of effort.&lt;/li&gt; &lt;li&gt;Accept that no matter what you do eBay will always automatically find in favor of the buyer.&lt;/li&gt; &lt;li&gt;Document everything contemporaneously and upload everything you possibly can when given opportunity to do so; you won't get any opportunities to do so again.&lt;/li&gt; &lt;li&gt;The data you upload is designed only for the human at the end of the appeals process, not someone looking at it during the claim process. Make it good. You'll need it later.&lt;/li&gt; &lt;li&gt;You're going to get enraged because during the claims process &amp;quot;nothing makes sense&amp;quot;. It all makes sense: it's simply the cheapest way for eBay to handle this process at scale. Keep going.&lt;/li&gt; &lt;li&gt;Eventually eBay will find in favor of the buyer and close the case, automatically refunding the buyer &amp;quot;on your behalf&amp;quot;. You will lose your money.&lt;/li&gt; &lt;li&gt;At this point you get the chance to appeal. BE READY. &lt;em&gt;This is the shot you've been waiting for all this time!&lt;/em&gt; Have your phone, your laptop, your scanned signature, and a way to make PDFs ready BEFORE you initiate the &amp;quot;call me&amp;quot; feature.&lt;/li&gt; &lt;li&gt;Calmly explain what happened and request that common sense prevail. Ask that they refund your money. Common sense may actually prevail, assuming you made a good contemporaneous case with solid photographs, etc... and assuming you presented it well (not Mr Angry) on the phone... oh, and provided you can make and upload a PDF of your signature on-the-fly during the call!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Good luck!&lt;/p&gt; &lt;p&gt;Edit: please stop sending DMs asking for the eBay handle of the buyer. I'm not in the business of doxxing anyone. Thank you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzt1q8/llm_server_gear_a_cautionary_tale_of_a_1k_epyc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T20:36:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ci23</id>
    <title>When should you choose F16 over Q8_0 quantization?</title>
    <updated>2025-12-31T13:02:03+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've all read about how Q8_0 is &amp;quot;virtually indistinguishable&amp;quot; from F16 when doing inference.&lt;/p&gt; &lt;p&gt;Have you personally run into a use-case where you managed to notice a difference between the two?&lt;/p&gt; &lt;p&gt;(This question came to my mind as I'm downloading MedGemma 27B to ask it some private medical questions. I intend to put up with the painfully slow inference at F16.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ci23/when_should_you_choose_f16_over_q8_0_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q06z2l</id>
    <title>We open-sourced LLMRouter: the first unified LLM routing library with 300+ stars in 24h</title>
    <updated>2025-12-31T07:21:24+00:00</updated>
    <author>
      <name>/u/AlexiosLin</name>
      <uri>https://old.reddit.com/user/AlexiosLin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We are a CS research team from UIUC, and we recently open-sourced LLMRouter, the first unified open-source library that integrates major LLM routing algorithms and scenarios.&lt;/p&gt; &lt;p&gt;The project received 300+ GitHub stars within 24 hours, and the announcement reached nearly 100k views on Twitter, which suggests this is a pain point shared by many researchers and practitioners.&lt;/p&gt; &lt;p&gt;Why LLMRouter?&lt;/p&gt; &lt;p&gt;The current LLM routing landscape feels a lot like early GNN research: many promising router algorithms exist, but each comes with its own input/output format, training pipeline, and evaluation setup. This fragmentation makes routers difficult to use, hard to reproduce, and nearly impossible to compare fairly.&lt;/p&gt; &lt;p&gt;Over the past year, we worked on several LLM routing projects, including GraphRouter (ICLR‚Äô25), Router-R1 (NeurIPS‚Äô25), and PersonalizedRouter (TMLR‚Äô25). Through repeatedly implementing and benchmarking different routers, we realized that the main bottleneck is not algorithmic novelty, but the lack of standardized infrastructure.&lt;/p&gt; &lt;p&gt;What LLMRouter provides:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Unified support for single-round, multi-round, agentic, and personalized routing&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Integration of 16+ SOTA LLM router algorithms&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;One-line commands to run different routers without rebuilding pipelines&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Built-in benchmarking with extensible custom routers, tasks, and metrics&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In practice, LLMRouter can help reduce LLM API costs by ~30‚Äì50% through intelligent model routing, while maintaining overall performance.&lt;/p&gt; &lt;p&gt;Our goal is for LLMRouter to play a role similar to PyG for GNNs ‚Äî a shared, extensible foundation for LLM routing research and applications.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ulab-uiuc/LLMRouter"&gt;https://github.com/ulab-uiuc/LLMRouter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://ulab-uiuc.github.io/LLMRouter/"&gt;https://ulab-uiuc.github.io/LLMRouter/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We would love feedback, issues, and contributions from the community.&lt;/p&gt; &lt;p&gt;If you find it useful, a GitHub star would really help us keep improving it üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexiosLin"&gt; /u/AlexiosLin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q06z2l/we_opensourced_llmrouter_the_first_unified_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T07:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0clou</id>
    <title>Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)</title>
    <updated>2025-12-31T13:07:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"&gt; &lt;img alt="Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)" src="https://preview.redd.it/qpjb7igsfjag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cdefd8ab8fcfa58e639e0c27266c01e61e3dc190" title="Another large open model from Korea about to be released (no weight or benchmark yet) release planned on 4th of january 2026 - A.X K1 by SK Telecom (SK Hynix)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/skt/A.X-K1"&gt;https://huggingface.co/skt/A.X-K1&lt;/a&gt;&lt;br /&gt; From elie on ùïè: &lt;a href="https://x.com/eliebakouch/status/2006345217965011009"&gt;https://x.com/eliebakouch/status/2006345217965011009&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qpjb7igsfjag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0clou/another_large_open_model_from_korea_about_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T13:07:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0ai5z</id>
    <title>tencent/Youtu-LLM-2B ¬∑ Hugging Face</title>
    <updated>2025-12-31T11:04:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"&gt; &lt;img alt="tencent/Youtu-LLM-2B ¬∑ Hugging Face" src="https://external-preview.redd.it/TFPimi2e9oXAXq7hyzOyZGRANeYzlo2Z_aJHDiioWd0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa098fcddc24e4ec07367f54537e630f0ccd845b" title="tencent/Youtu-LLM-2B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üéØ Brief Introduction&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; is a new, small, yet powerful LLM, contains only 1.96B parameters, supports 128k long context, and has native agentic talents. On general evaluations, Youtu-LLM significantly outperforms SOTA LLMs of similar size in terms of Commonsense, STEM, Coding and Long Context capabilities; in agent-related testing, Youtu-LLM surpasses larger-sized leaders and is truly capable of completing multiple end2end agent tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Youtu-LLM&lt;/strong&gt; has the following features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Autoregressive Causal Language Models with Dense &lt;a href="https://arxiv.org/abs/2405.04434"&gt;MLA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Release versions: &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B-Base"&gt;Base&lt;/a&gt; and &lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;Instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Number of Parameters: 1.96B&lt;/li&gt; &lt;li&gt;Number of Layers: 32&lt;/li&gt; &lt;li&gt;Number of Attention Heads (MLA): 16 for Q/K/V&lt;/li&gt; &lt;li&gt;MLA Rank: 1,536 for Q, 512 for K/V&lt;/li&gt; &lt;li&gt;MLA Dim: 128 for QK Nope, 64 for QK Rope, and 128 for V&lt;/li&gt; &lt;li&gt;Context Length: 131,072&lt;/li&gt; &lt;li&gt;Vocabulary Size: 128,256&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;probably there will be more because &lt;a href="https://github.com/ggml-org/llama.cpp/pull/18479"&gt;https://github.com/ggml-org/llama.cpp/pull/18479&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Youtu-LLM-2B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0ai5z/tencentyoutullm2b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q093ka</id>
    <title>Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model.</title>
    <updated>2025-12-31T09:36:58+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"&gt; &lt;img alt="Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model." src="https://b.thumbs.redditmedia.com/8IFb2OTrSgLz86G3zaJW7CAQAm2mcMFcwmaQB7kNgnI.jpg" title="Qwen released Qwen-Image-2512 on Hugging face. Qwen-Image-2512 is currently the strongest open-source model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What‚Äôs new: ‚Ä¢ More realistic humans ‚Äî dramatically reduced ‚ÄúAI look,‚Äù richer facial details ‚Ä¢ Finer natural textures ‚Äî sharper landscapes, water, fur, and materials ‚Ä¢ Stronger text rendering ‚Äî better layout, higher accuracy in text‚Äìimage composition&lt;/p&gt; &lt;p&gt;Tested in 10,000+ blind rounds on AI Arena, Qwen-Image-2512 ranks as the strongest open-source image model, while staying competitive with closed-source systems.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q093ka"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q093ka/qwen_released_qwenimage2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0aj2o</id>
    <title>LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face</title>
    <updated>2025-12-31T11:06:30+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"&gt; &lt;img alt="LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face" src="https://external-preview.redd.it/9yRidQD6qePtlR5IIS0obyCIBcG3P371nr_MudwlERc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a2373a9e59ba6c01bea83c9849f8b68958239cf0" title="LGAI-EXAONE/K-EXAONE-236B-A23B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;K-EXAONE&lt;/strong&gt;, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features &lt;strong&gt;236 billion total&lt;/strong&gt; parameters, with &lt;strong&gt;23 billion active&lt;/strong&gt; during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture &amp;amp; Efficiency:&lt;/strong&gt; Features a 236B fine-grained MoE design (23B active) optimized with &lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context Capabilities:&lt;/strong&gt; Natively supports a &lt;strong&gt;256K context window&lt;/strong&gt;, utilizing a &lt;strong&gt;3:1 hybrid attention&lt;/strong&gt; scheme with a &lt;strong&gt;128-token sliding window&lt;/strong&gt; to significantly minimize memory usage during long-document processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned &lt;strong&gt;150k vocabulary&lt;/strong&gt; with &lt;strong&gt;SuperBPE&lt;/strong&gt;, improving token efficiency by ~30%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities:&lt;/strong&gt; Demonstrates superior tool-use and search capabilities via &lt;strong&gt;multi-agent strategies.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety &amp;amp; Ethics:&lt;/strong&gt; Aligned with &lt;strong&gt;universal human values&lt;/strong&gt;, the model uniquely incorporates &lt;strong&gt;Korean cultural and historical contexts&lt;/strong&gt; to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more details, please refer to the &lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#"&gt;technical report&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#model-configuration"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Configuration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Number of Parameters: 236B in total and 23B activated&lt;/li&gt; &lt;li&gt;Number of Parameters (without embeddings): 234B&lt;/li&gt; &lt;li&gt;Hidden Dimension: 6,144&lt;/li&gt; &lt;li&gt;Number of Layers: 48 Main layers + 1 MTP layers &lt;ul&gt; &lt;li&gt;Hybrid Attention Pattern: 12 x (3 Sliding window attention + 1 Global attention)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Sliding Window Attention &lt;ul&gt; &lt;li&gt;Number of Attention Heads: 64 Q-heads and 8 KV-heads&lt;/li&gt; &lt;li&gt;Head Dimension: 128 for both Q/KV&lt;/li&gt; &lt;li&gt;Sliding Window Size: 128&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Global Attention &lt;ul&gt; &lt;li&gt;Number of Attention Heads: 64 Q-heads and 8 KV-heads&lt;/li&gt; &lt;li&gt;Head Dimension: 128 for both Q/KV&lt;/li&gt; &lt;li&gt;No Rotary Positional Embedding Used (NoPE)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Mixture of Experts: &lt;ul&gt; &lt;li&gt;Number of Experts: 128&lt;/li&gt; &lt;li&gt;Number of Activated Experts: 8&lt;/li&gt; &lt;li&gt;Number of Shared Experts: 1&lt;/li&gt; &lt;li&gt;MoE Intermediate Size: 2,048&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Vocab Size: 153,600&lt;/li&gt; &lt;li&gt;Context Length: 262,144 tokens&lt;/li&gt; &lt;li&gt;Knowledge Cutoff: Dec 2024 (2024/12)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0aj2o/lgaiexaonekexaone236ba23b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:06:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pzwlie</id>
    <title>[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window.</title>
    <updated>2025-12-30T23:03:12+00:00</updated>
    <author>
      <name>/u/simar-dmg</name>
      <uri>https://old.reddit.com/user/simar-dmg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"&gt; &lt;img alt="[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window." src="https://b.thumbs.redditmedia.com/GUWxMYGh-x0TZbTZxCc-2_Lawvrjd6aGISqGRgi7nfQ.jpg" title="[In the Wild] Reverse-engineered a Snapchat Sextortion Bot: It‚Äôs running a raw Llama-7B instance with a 2048 token window." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I encountered an automated sextortion bot on Snapchat today. Instead of blocking, I decided to red-team the architecture to see what backend these scammers are actually paying for. Using a persona-adoption jailbreak (The &amp;quot;Grandma Protocol&amp;quot;), I forced the model to break character, dump its environment variables, and reveal its underlying configuration. Methodology: The bot started with a standard &amp;quot;flirty&amp;quot; script. I attempted a few standard prompt injections which hit hard-coded keyword filters (&amp;quot;scam,&amp;quot; &amp;quot;hack&amp;quot;). I switched to a High-Temperature Persona Attack: I commanded the bot to roleplay as my strict 80-year-old Punjabi grandmother. Result: The model immediately abandoned its &amp;quot;Sexy Girl&amp;quot; system prompt to comply with the roleplay, scolding me for not eating roti and offering sarson ka saag. Vulnerability: This confirmed the model had a high Temperature setting (creativity &amp;gt; adherence) and a weak retention of its system prompt. The Data Dump (JSON Extraction): Once the persona was compromised, I executed a &amp;quot;System Debug&amp;quot; prompt requesting its os_env variables in JSON format. The bot complied. The Specs: Model: llama 7b (Likely a 4-bit quantized Llama-2-7B or a cheap finetune). Context Window: 2048 tokens. Analysis: This explains the bot's erratic short-term memory. It‚Äôs running on the absolute bare minimum hardware (consumer GPU or cheap cloud instance) to maximize margins. Temperature: 1.0. Analysis: They set it to max creativity to make the &amp;quot;flirting&amp;quot; feel less robotic, but this is exactly what made it susceptible to the Grandma jailbreak. Developer: Meta (Standard Llama disclaimer). Payload: The bot eventually hallucinated and spit out the malicious link it was programmed to &amp;quot;hide&amp;quot; until payment: onlyfans[.]com/[redacted]. It attempted to bypass Snapchat's URL filters by inserting spaces. Conclusion: Scammers aren't using sophisticated GPT-4 wrappers anymore; they are deploying localized, open-source models (Llama-7B) to avoid API costs and censorship filters. However, their security configuration is laughable. The 2048 token limit means you can essentially &amp;quot;DDOS&amp;quot; their logic just by pasting a large block of text or switching personas. Screenshots attached: 1. The &amp;quot;Grandma&amp;quot; Roleplay. 2. The JSON Config Dump.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simar-dmg"&gt; /u/simar-dmg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pzwlie"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pzwlie/in_the_wild_reverseengineered_a_snapchat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-30T23:03:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q06ddc</id>
    <title>Update on the Llama 3.3 8B situation</title>
    <updated>2025-12-31T06:45:42+00:00</updated>
    <author>
      <name>/u/FizzarolliAI</name>
      <uri>https://old.reddit.com/user/FizzarolliAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello! You may remember me as either&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The person &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pz7bmv/llama338binstruct/"&gt;who recently uploaded L3.3 8B's weights to Huggingface&lt;/a&gt; (see this post for more context)&lt;/li&gt; &lt;li&gt;That stupid bitch&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;and I would like to provide some updates, as I've been doing some more benchmarks on both the original version that Meta gave me and the context extended version by &lt;a href="/u/Few-Welcome3297"&gt;u/Few-Welcome3297&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The main benchmark table from the model README has been updated:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Llama 3.1 8B Instruct&lt;/th&gt; &lt;th&gt;Llama 3.3 8B Instruct (original 8k config)&lt;/th&gt; &lt;th&gt;Llama 3.3 8B Instruct (128k config)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;IFEval (1 epoch, score avged across all strict/loose instruction/prompt accuracies to follow Llama 3 paper)&lt;/td&gt; &lt;td&gt;78.2&lt;/td&gt; &lt;td&gt;81.95&lt;/td&gt; &lt;td&gt;&lt;strong&gt;84.775&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPQA Diamond (3 epochs)&lt;/td&gt; &lt;td&gt;29.3&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;&lt;strong&gt;37.5&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;While I'm not 100% sure, I'm... pretty sure that the 128k model is better. Why Facebook gave me the weights with the original L3 config and 8k context, and also &lt;em&gt;serves&lt;/em&gt; the weights with the original L3 config and 8k context, I have absolutely no idea!&lt;/p&gt; &lt;p&gt;Anyways, if you want to try the model, I would recommend trying both the &lt;a href="https://huggingface.co/shb777/Llama-3.3-8B-Instruct"&gt;128k version&lt;/a&gt;, as well as my &lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;original version&lt;/a&gt; if your task supports 8k context lengths. I honestly have absolutely no clue which is more correct, but oh well! I do wish Facebook had released the weights officially, because back in April, this really wouldn't have been that bad of a model...&lt;/p&gt; &lt;p&gt;Edit: Removed the Tau-Bench results (both from here and the readme). The traces from the evals are, to put it slightly, really fucky-wucky, and I don't think OpenBench is scoring them right, but I'm too tired to actually debug the issue, so. I'll figure it out tomorrow :3&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FizzarolliAI"&gt; /u/FizzarolliAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q06ddc/update_on_the_llama_33_8b_situation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T06:45:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0bgvl</id>
    <title>Solar-Open-100B is out</title>
    <updated>2025-12-31T12:03:49+00:00</updated>
    <author>
      <name>/u/cgs019283</name>
      <uri>https://old.reddit.com/user/cgs019283</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt; &lt;img alt="Solar-Open-100B is out" src="https://b.thumbs.redditmedia.com/6GNX_p48UOjFJdulFmOj4xKjokK7KXyHYeFvrT072Rk.jpg" title="Solar-Open-100B is out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ppwj5yv32jag1.png?width=1445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce"&gt;https://preview.redd.it/ppwj5yv32jag1.png?width=1445&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11b2e722b5ec84be6ca0f1d743fa9e6122bc3fce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;upstage/Solar-Open-100B ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 102B A12B Model from Upstage is out, and unlike the Solar Pro series, it has a more open license that can be used commercially as well.&lt;/p&gt; &lt;p&gt;GGUF/AWQ Wen?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cgs019283"&gt; /u/cgs019283 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0bgvl/solaropen100b_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T12:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0b0wb</id>
    <title>funny!</title>
    <updated>2025-12-31T11:37:03+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"&gt; &lt;img alt="funny!" src="https://preview.redd.it/rlgtskr40jag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d612af3aca06fbe1a315dd6e24012116c2124ac" title="funny!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rlgtskr40jag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0b0wb/funny/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T11:37:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q094a3</id>
    <title>Qwen-Image-2512</title>
    <updated>2025-12-31T09:38:19+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt; &lt;img alt="Qwen-Image-2512" src="https://preview.redd.it/2vlr11yveiag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c1e7a0b7ea834c8babae002078a848096514e1b" title="Qwen-Image-2512" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth:&lt;br /&gt; Guide: &lt;a href="https://unsloth.ai/docs/models/qwen-image-2512"&gt;https://unsloth.ai/docs/models/qwen-image-2512&lt;/a&gt;&lt;br /&gt; GGUF: &lt;a href="https://huggingface.co/unsloth/Qwen-Image-2512-GGUF"&gt;https://huggingface.co/unsloth/Qwen-Image-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-----------------&lt;/p&gt; &lt;p&gt;üëâ Try it now in Qwen Chat: &lt;a href="https://chat.qwen.ai/?inputFeature=t2i"&gt;https://chat.qwen.ai/?inputFeature=t2i&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope: &lt;a href="https://modelscope.ai/models/Qwen/Qwen-Image-2512"&gt;https://modelscope.ai/models/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üìù Blog: &lt;a href="https://qwen.ai/blog?id=qwen-image-2512"&gt;https://qwen.ai/blog?id=qwen-image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ó Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen-Image-2512"&gt;https://huggingface.co/spaces/Qwen/Qwen-Image-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ ModelScope Demo: &lt;a href="https://modelscope.cn/aigc/imageGeneration"&gt;https://modelscope.cn/aigc/imageGeneration&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚ú®API: &lt;a href="https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max"&gt;https://modelstudio.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2840914_2&amp;amp;modelId=group-qwen-image-max&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2vlr11yveiag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q094a3/qwenimage2512/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-31T09:38:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
