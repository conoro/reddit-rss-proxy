<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-04T19:07:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pe7jvp</id>
    <title>X-VLA: The First Soft-Prompted Robot Foundation Model for Any Robot, Any Task</title>
    <updated>2025-12-04T18:22:44+00:00</updated>
    <author>
      <name>/u/Soft-Worth-4872</name>
      <uri>https://old.reddit.com/user/Soft-Worth-4872</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;br /&gt; At Hugging Face / LeRobot, one of our goals is to make strong, accessible VLA models available to the whole robotics community. Today we‚Äôre excited to announce X-VLA in LeRobot, a new soft-prompted robot foundation model that can generalize across embodiments, sensors, and action spaces.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing 6 checkpoints, including a pretrained base model and a cloth-folding checkpoint that hits &lt;em&gt;100% success for two straight hours&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;There is also an uncut 2-hour folding run powered entirely by X-VLA (video + checkpoints). You can check it out here:&lt;br /&gt; üëâ &lt;a href="https://x.com/jadechoghari/status/1996639961366548597"&gt;https://x.com/jadechoghari/status/1996639961366548597&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to try it yourself, you can &lt;em&gt;fine-tune X-VLA on any dataset, with any action dimension&lt;/em&gt;, directly through LeRobot:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/lerobot/xvla"&gt;https://huggingface.co/collections/lerobot/xvla&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy tinkering, and would love feedback from the community! üßµü§ñ&lt;/p&gt; &lt;p&gt;Docs/Blog: &lt;a href="https://huggingface.co/docs/lerobot/en/xvla"&gt;https://huggingface.co/docs/lerobot/en/xvla&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper from Tsinghua: &lt;a href="https://arxiv.org/abs/2510.10274"&gt;https://arxiv.org/abs/2510.10274&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pe7jvp/video/jgpwg5q6c85g1/player"&gt;https://reddit.com/link/1pe7jvp/video/jgpwg5q6c85g1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soft-Worth-4872"&gt; /u/Soft-Worth-4872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe7jvp/xvla_the_first_softprompted_robot_foundation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe7jvp/xvla_the_first_softprompted_robot_foundation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe7jvp/xvla_the_first_softprompted_robot_foundation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T18:22:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4p9z</id>
    <title>Best local TTS at the moment?</title>
    <updated>2025-12-04T16:38:24+00:00</updated>
    <author>
      <name>/u/Robert__Sinclair</name>
      <uri>https://old.reddit.com/user/Robert__Sinclair</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year I used COQUI xtts_v2 with some decent results.&lt;/p&gt; &lt;p&gt;Is there anything better/faster (supporting voice clone)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Robert__Sinclair"&gt; /u/Robert__Sinclair &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4p9z/best_local_tts_at_the_moment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4p9z/best_local_tts_at_the_moment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4p9z/best_local_tts_at_the_moment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:38:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4vpc</id>
    <title>Fine tune for rp world?</title>
    <updated>2025-12-04T16:45:16+00:00</updated>
    <author>
      <name>/u/JaxxonAI</name>
      <uri>https://old.reddit.com/user/JaxxonAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, I have sillytavern setup locally and run a local llm for rp. I have been toying with the idea of making a specific finetune for an existing world. As an example, would it make sense to take all the text from the Wheel of Time novels and use that to create a finetune? &lt;/p&gt; &lt;p&gt;I've create Loras off of image models but never thought to try an language model until recently. Any thoughts on this? Pros and cons would we great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JaxxonAI"&gt; /u/JaxxonAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4vpc/fine_tune_for_rp_world/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4vpc/fine_tune_for_rp_world/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4vpc/fine_tune_for_rp_world/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlfu3</id>
    <title>Frozen networks show usable early-layer intent: 1370√ó fewer FLOPs and 10√ó faster inference (code + weights)9</title>
    <updated>2025-12-04T00:26:15+00:00</updated>
    <author>
      <name>/u/anima-core</name>
      <uri>https://old.reddit.com/user/anima-core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with whether a frozen network‚Äôs early activations contain enough ‚Äúsemantic intent‚Äù to skip most of the compute.&lt;/p&gt; &lt;p&gt;I used a standard ResNet-18 trained on CIFAR-10 (87.89 percent accuracy), pulled a single 64-dimensional vector from an early layer, and trained a tiny decoder on top of it.&lt;/p&gt; &lt;p&gt;Results on the same hardware: ‚Ä¢ 72.57 percent accuracy from that early-layer vector ‚Ä¢ ~10√ó faster real latency ‚Ä¢ 1370√ó fewer FLOPs ‚Ä¢ No pruning, distillation, quantization, early exit tricks, or sparsity ‚Ä¢ The full model stayed completely frozen&lt;/p&gt; &lt;p&gt;This means 99.93 percent of the original network‚Äôs compute was not required to recover 82.6 percent of its performance.&lt;/p&gt; &lt;p&gt;Code + one-click run script: &lt;a href="https://github.com/Anima-Core/an1-meaning-engine"&gt;https://github.com/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF demo + pretrained weights: &lt;a href="https://huggingface.co/Anima-Core/an1-meaning-engine"&gt;https://huggingface.co/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Runs end to end on almost any GPU or CPU in a few minutes.&lt;/p&gt; &lt;p&gt;Dedicated to my late father, Asad Shamim, whose loss opened the path that led me here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anima-core"&gt; /u/anima-core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe6rhs</id>
    <title>GPUStack drops support for MacOS</title>
    <updated>2025-12-04T17:54:46+00:00</updated>
    <author>
      <name>/u/jaimemiguel</name>
      <uri>https://old.reddit.com/user/jaimemiguel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe6rhs/gpustack_drops_support_for_macos/"&gt; &lt;img alt="GPUStack drops support for MacOS" src="https://b.thumbs.redditmedia.com/YngSjMpe-p8mdko-2jgXQbtnreeWAkEk2_rMRhkDric.jpg" title="GPUStack drops support for MacOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a two mac mini m4s that I was waiting for GPUStack to release an update to 0.7.&lt;/p&gt; &lt;p&gt;Today today I tripped over the news that they have decided to drop support of MacOS. I verified it with their team through github: &lt;a href="https://github.com/gpustack/gpustack/discussions/3704"&gt;https://github.com/gpustack/gpustack/discussions/3704&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Is MLX now my best choice for a mac mini cluster?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7xxs6qgpw75g1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a51ded58adb6ed9f6db5ab0894f9958646ff6d05"&gt;https://preview.redd.it/7xxs6qgpw75g1.png?width=910&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a51ded58adb6ed9f6db5ab0894f9958646ff6d05&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaimemiguel"&gt; /u/jaimemiguel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe6rhs/gpustack_drops_support_for_macos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe6rhs/gpustack_drops_support_for_macos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe6rhs/gpustack_drops_support_for_macos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T17:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe59ud</id>
    <title>smallevals - Tiny 0.6B Evaluation Models and a Local LLM Evaluation Framework</title>
    <updated>2025-12-04T17:00:00+00:00</updated>
    <author>
      <name>/u/mburaksayici</name>
      <uri>https://old.reddit.com/user/mburaksayici</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; , you may know me from the latest blogs I've shared on &lt;a href="http://mburaksayici.com/"&gt;mburaksayici.com/&lt;/a&gt; , discussing LLM and RAG systems, and RAG Boilerplates.&lt;/p&gt; &lt;p&gt;When I study evaluation frameworks on LLMs, I've seen they require lots of API calls to generate golden datasets, open-ended and subjective. I thought at least in the retrieval stage, I can come up with a tiny 0.6B models and a framework that uses those models to evaluate vectorDB(for now) and RAG pipelines (in the near future).&lt;/p&gt; &lt;p&gt;I‚Äôm releasing smallevals, a lightweight evaluation suite built to evaluate RAG / retrieval systems fast and free ‚Äî powered by tiny 0.6B models trained on Google Natural Questions and TriviaQA to generate golden evaluation datasets.&lt;/p&gt; &lt;p&gt;smallevals is designed to run extremely fast even on CPU and fully offline ‚Äî with no API calls, no costs, and no external dependencies.&lt;/p&gt; &lt;p&gt;smallevals generates one question per chunk and then measures whether your vector database can retrieve the correct chunk back using that question.&lt;/p&gt; &lt;p&gt;This directly evaluates retrieval quality using precision, recall, MRR and hit-rate at the chunk level.&lt;/p&gt; &lt;p&gt;SmallEvals includes a built-in local dashboard to visualize rank distributions, failing chunks, retrieval performance, and dataset statistics on your machine.&lt;/p&gt; &lt;p&gt;The first released model is QAG-0.6B, a tiny question-generation model that creates evaluation questions directly from your documents.&lt;/p&gt; &lt;p&gt;This lets you evaluate retrieval quality independently from generation quality, which is exactly where most RAG systems fail silently.&lt;/p&gt; &lt;p&gt;Following QAG-0.6B, upcoming models will evaluate context relevance, faithfulness / groundedness, and answer correctness ‚Äî closing the gap for a fully local, end-to-end evaluation pipeline.&lt;/p&gt; &lt;p&gt;Source:&lt;/p&gt; &lt;p&gt;[&lt;a href="https://github.com/mburaksayici/smallevals%5D(https://github.com/mburaksayici/smallevals"&gt;https://github.com/mburaksayici/smallevals](https://github.com/mburaksayici/smallevals&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Install:&lt;/p&gt; &lt;p&gt;pip install smallevals&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mburaksayici/golden_generate_qwen_0.6b_v3_gguf"&gt;https://huggingface.co/mburaksayici/golden_generate_qwen_0.6b_v3_gguf&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mburaksayici"&gt; /u/mburaksayici &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe59ud/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe59ud/smallevals_tiny_06b_evaluation_models_and_a_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe59ud/smallevals_tiny_06b_evaluation_models_and_a_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T17:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdtht0</id>
    <title>Do you think we‚Äôll get GLM 4.6 Air one day?</title>
    <updated>2025-12-04T07:04:52+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was about to forget, it‚Äôs probably not the priority of zai but hope remain!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdtht0/do_you_think_well_get_glm_46_air_one_day/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe3sme</id>
    <title>Had anyone tried to make llama.cpp vulkan work on mali gpus?</title>
    <updated>2025-12-04T16:04:06+00:00</updated>
    <author>
      <name>/u/bulieme0</name>
      <uri>https://old.reddit.com/user/bulieme0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive tried installing &lt;code&gt;llama-cpp-backend-vulkan&lt;/code&gt; on termux, and tried installing other prequisites (i.e. the vulkan header)&lt;/p&gt; &lt;p&gt;but it give me errors or dont detect the gpu entirely.&lt;/p&gt; &lt;p&gt;here are my terminal logs (yes i restarted termux for this, apologize for lacking of details here)&lt;/p&gt; &lt;p&gt;&lt;code&gt; ~ $ llama-cli --version ggml_vulkan: No devices found. load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ cp /system/lib64/libvulkan.so $PREFIX/lib/libvulkan.so ~ $ cat &amp;gt; $HOME/mali.json &amp;lt;&amp;lt; 'EOF' { &amp;quot;file_format_version&amp;quot;: &amp;quot;1.0.0&amp;quot;, &amp;quot;ICD&amp;quot;: { &amp;quot;library_path&amp;quot;: &amp;quot;/vendor/lib64/hw/vulkan.mali.so&amp;quot;, &amp;quot;api_version&amp;quot;: &amp;quot;1.1.177&amp;quot; } } EOF ~ $ llama-cli --version ggml_vulkan: WARNING: Instance extension VK_EXT_debug_utils not found. ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Mali-G57 MC2 (Mali-G57 MC2) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 0 | matrix cores: none load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ export VK_ICD_FILENAMES=$HOME/mali.json &amp;amp;&amp;amp; export LD_LIBRARY_PATH=/vendor/lib64/hw:$PREFIX/lib:$LD_LIBRARY_PATH ~ $ llama-cli --version load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ unset LD_LIBRARY_PATH ~ $ unset LD_LIBRARY_PATH ~ $ llama-cli --version ggml_vulkan: WARNING: Instance extension VK_EXT_debug_utils not found. ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Mali-G57 MC2 (Mali-G57 MC2) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 0 | matrix cores: none load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so version: 0 (unknown) built with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu ~ $ ls mali.json ~ $ llama-cli -m /sdcard/Huihui-Qwen3-0.6B-abliterated-v2.i1-Q4_K_M.gguf -ngl 99 ggml_vulkan: WARNING: Instance extension VK_EXT_debug_utils not found. ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Mali-G57 MC2 (Mali-G57 MC2) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 16 | shared memory: 32768 | int dot: 0 | matrix cores: none load_backend: loaded Vulkan backend from /data/data/com.termux/files/usr/bin/../lib/libggml-vulkan.so load_backend: loaded CPU backend from /data/data/com.termux/files/usr/bin/../lib/libggml-cpu.so build: 0 (unknown) with Android (13989888, +pgo, +bolt, +lto, +mlgo, based on r563880c) clang version 21.0.0 (https://android.googlesource.com/toolchain/llvm-project 5e96669f06077099aa41290cdb4c5e6fa0f59349) for x86_64-unknown-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Vulkan0 (Mali-G57 MC2) (unknown id) - 7627 MiB free llama_model_loader: loaded meta data with 46 key-value pairs and 310 tensors from /sdcard/Huihui-Qwen3-0.6B-abliterated-v2.i1-Q4_K_M.gguf (version GGUF V3 (latest)) llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output. llama_model_loader: - kv 0: general.architecture str = qwen3 llama_model_loader: - kv 1: general.type str = model llama_model_loader: - kv 2: general.name str = Huihui Qwen3 0.6B Abliterated v2 llama_model_loader: - kv 3: general.version str = v2 llama_model_loader: - kv 4: general.finetune str = abliterated llama_model_loader: - kv 5: general.basename str = Huihui-Qwen3 llama_model_loader: - kv 6: general.size_label str = 0.6B llama_model_loader: - kv 7: general.license str = apache-2.0 llama_model_loader: - kv 8: general.license.link str = https://huggingface.co/Qwen/Qwen3-0.6... llama_model_loader: - kv 9: general.base_model.count u32 = 1 llama_model_loader: - kv 10: general.base_model.0.name str = Qwen3 0.6B llama_model_loader: - kv 11: general.base_model.0.organization str = Qwen llama_model_loader: - kv 12: general.base_model.0.repo_url str = https://huggingface.co/Qwen/Qwen3-0.6B llama_model_loader: - kv 13: general.tags arr[str,4] = [&amp;quot;chat&amp;quot;, &amp;quot;abliterated&amp;quot;, &amp;quot;uncensored&amp;quot;,... llama_model_loader: - kv 14: qwen3.block_count u32 = 28 llama_model_loader: - kv 15: qwen3.context_length u32 = 40960 llama_model_loader: - kv 16: qwen3.embedding_length u32 = 1024 llama_model_loader: - kv 17: qwen3.feed_forward_length u32 = 3072 llama_model_loader: - kv 18: qwen3.attention.head_count u32 = 16 llama_model_loader: - kv 19: qwen3.attention.head_count_kv u32 = 8 llama_model_loader: - kv 20: qwen3.rope.freq_base f32 = 1000000.000000 llama_model_loader: - kv 21: qwen3.attention.layer_norm_rms_epsilon f32 = 0.000001 llama_model_loader: - kv 22: qwen3.attention.key_length u32 = 128 llama_model_loader: - kv 23: qwen3.attention.value_length u32 = 128 llama_model_loader: - kv 24: tokenizer.ggml.model str = gpt2 llama_model_loader: - kv 25: tokenizer.ggml.pre str = qwen2 llama_model_loader: - kv 26: tokenizer.ggml.tokens arr[str,151936] = [&amp;quot;!&amp;quot;, &amp;quot;\&amp;quot;&amp;quot;, &amp;quot;#&amp;quot;, &amp;quot;$&amp;quot;, &amp;quot;%&amp;quot;, &amp;quot;&amp;amp;&amp;quot;, &amp;quot;'&amp;quot;, ... llama_model_loader: - kv 27: tokenizer.ggml.token_type arr[i32,151936] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... llama_model_loader: - kv 28: tokenizer.ggml.merges arr[str,151387] = [&amp;quot;ƒ† ƒ†&amp;quot;, &amp;quot;ƒ†ƒ† ƒ†ƒ†&amp;quot;, &amp;quot;i n&amp;quot;, &amp;quot;ƒ† t&amp;quot;,... llama_model_loader: - kv 29: tokenizer.ggml.eos_token_id u32 = 151645 llama_model_loader: - kv 30: tokenizer.ggml.padding_token_id u32 = 151643 llama_model_loader: - kv 31: tokenizer.ggml.bos_token_id u32 = 151643 llama_model_loader: - kv 32: tokenizer.ggml.add_bos_token bool = false llama_model_loader: - kv 33: general.quantization_version u32 = 2 llama_model_loader: - kv 34: general.file_type u32 = 15 llama_model_loader: - kv 35: general.url str = https://huggingface.co/mradermacher/H... llama_model_loader: - kv 36: mradermacher.quantize_version str = 2 llama_model_loader: - kv 37: mradermacher.quantized_by str = mradermacher llama_model_loader: - kv 38: mradermacher.quantized_at str = 2025-06-19T15:14:20+02:00 llama_model_loader: - kv 39: mradermacher.quantized_on str = nico1 llama_model_loader: - kv 40: general.source.url str = https://huggingface.co/huihui-ai/Huih... llama_model_loader: - kv 41: mradermacher.convert_type str = hf llama_model_loader: - kv 42: quantize.imatrix.file str = Huihui-Qwen3-0.6B-abliterated-v2-i1-G... llama_model_loader: - kv 43: quantize.imatrix.dataset str = imatrix-training-full-3 llama_model_loader: - kv 44: quantize.imatrix.entries_count i32 = 196 llama_model_loader: - kv 45: quantize.imatrix.chunks_count i32 = 318 llama_model_loader: - type f32: 113 tensors llama_model_loader: - type q4_K: 168 tensors llama_model_loader: - type q6_K: 29 tensors print_info: file format = GGUF V3 (latest) print_info: file type = Q4_K - Medium print_info: file size = 372.65 MiB (5.24 BPW) load: printing all EOG tokens: load: - 151643 ('&amp;lt;|endoftext|&amp;gt;') load: - 151645 ('&amp;lt;|im_end|&amp;gt;') load: - 151662 ('&amp;lt;|fim_pad|&amp;gt;') load: - 151663 ('&amp;lt;|repo_name|&amp;gt;') load: - 151664 ('&amp;lt;|file_sep|&amp;gt;') load: special tokens cache size = 26 load: token to piece cache size = 0.9311 MB print_info: arch = qwen3 print_info: vocab_only = 0 print_info: n_ctx_train = 40960 print_info: n_embd = 1024 print_info: n_embd_inp = 1024 print_info: n_layer = 28 print_info: n_head = 16 print_info: n_head_kv = 8 print_info: n_rot = 128 print_info: n_swa = 0 print_info: is_swa_any = 0 print_info: n_embd_head_k = 128 print_info: n_embd_head_v = 128 print_info: n_gqa = 2 print_info: n_embd_k_gqa = 1024 print_info: n_embd_v_gqa = 1024 print_info: f_norm_eps = 0.0e+00 print_info: f_norm_rms_eps = 1.0e-06 print_info: f_clamp_kqv = 0.0e+00 print_info: f_max_alibi_bias = 0.0e+00 print_info: f_logit_scale = 0.0e+00 print_info: f_attn_scale = 0.0e+00 print_info: n_ff = 3072 print_info: n_expert = 0 print_info: n_expert_used = 0 print_info: n_expert_groups = 0 print_info: n_group_used = 0 print_info: causal attn = 1 print_info: pooling type = -1 print_info: rope type = 2 print_info: rope scaling = linear print_info: freq_base_train = 1000000.0 print_info: freq_scale_train = 1 print_info: n_ctx_orig_yarn = 40960 print_info: rope_finetuned = unknown print_info: model type = 0.6B print_info: model params = 596.05 M print_info: general.name = Huihui Qwen3 0.6B Abliterated v2 print_info: vocab type = BPE print_info: n_vocab = 151936 print_info: n_merges = 151387 print_info: BOS token = 151643 '&amp;lt;|endoftext|&amp;gt;' print_info: EOS token = 151645 '&amp;lt;|im_end|&amp;gt;' print_info: EOT token = 151645 '&amp;lt;|im_end|&amp;gt;' print_info: PAD token = 151643 '&amp;lt;|endoftext|&amp;gt;' print_info: LF token = 198 'ƒä' print_info: FIM PRE token = 151659 '&amp;lt;|fim_prefix|&amp;gt;' print_info: FIM SUF token = 151661 '&amp;lt;|fim_suffix|&amp;gt;' print_info: FIM MID token = 151660 '&amp;lt;|fim_middle|&amp;gt;' print_info: FIM PAD token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: FIM REP token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: FIM SEP token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: EOG token = 151643 '&amp;lt;|endoftext|&amp;gt;' print_info: EOG token = 151645 '&amp;lt;|im_end|&amp;gt;' print_info: EOG token = 151662 '&amp;lt;|fim_pad|&amp;gt;' print_info: EOG token = 151663 '&amp;lt;|repo_name|&amp;gt;' print_info: EOG token = 151664 '&amp;lt;|file_sep|&amp;gt;' print_info: max token length = 256 load_tensors: loading model tensors, this can take a while... (mmap = true) Segmentation fault llama-cli -m /sdcard/Huihui-Qwen3-0.6B-abliterated-v2.i1-Q4_K_M.gguf -ngl 99 ~ $ &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Device: Samsung A15 GPU: Mali G57 MC2 Chipset: Mediatek Helio G99&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bulieme0"&gt; /u/bulieme0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe3sme/had_anyone_tried_to_make_llamacpp_vulkan_work_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe3sme/had_anyone_tried_to_make_llamacpp_vulkan_work_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe3sme/had_anyone_tried_to_make_llamacpp_vulkan_work_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdwzgb</id>
    <title>Deepseek 3.2 just does not seem to perform (for me)</title>
    <updated>2025-12-04T10:49:38+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using RooCode and just not feeling DeepSeek 3.2 at all. &lt;/p&gt; &lt;p&gt;Is it just me? Any tips? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdwzgb/deepseek_32_just_does_not_seem_to_perform_for_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T10:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdm268</id>
    <title>How Attention Got So Efficient [GQA/MLA/DSA]</title>
    <updated>2025-12-04T00:53:59+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt; &lt;img alt="How Attention Got So Efficient [GQA/MLA/DSA]" src="https://external-preview.redd.it/4QixmEzxJtTr5ZgAjR4FoJjK4qVPLU4zAuNo-fsPzgM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f8badcadc6197f760be212560f8188dc2793fa6" title="How Attention Got So Efficient [GQA/MLA/DSA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone trying to understand why Deepseek 3.2 DSA is a milestone in terms of solving long context, I really recommend this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Y-o545eYjXM?si=pt-SxR5anfLNSN8j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzmxh</id>
    <title>BrowseSafe, An Open-Source Model for AI Agents Browser Security</title>
    <updated>2025-12-04T13:11:36+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzmxh/browsesafe_an_opensource_model_for_ai_agents/"&gt; &lt;img alt="BrowseSafe, An Open-Source Model for AI Agents Browser Security" src="https://preview.redd.it/x84cad88q65g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0239420f122e41f3ba7514e3d0d9eaaa88de923f" title="BrowseSafe, An Open-Source Model for AI Agents Browser Security" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BrowseSafe is an open-source security model trained to protect AI browser agents from prompt injection attacks embedded in real-world web content. BrowseSafe model is based on the &lt;strong&gt;Qwen3-30B-A3B.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is a brief overview of key features of BrowseSafe model:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. State-of-the-Art Detection&lt;/strong&gt;: Achieves a 90.4% F1 score on the BrowseSafe-Bench test set outperforming models like GPT-5 and Sonnet 4.5.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Robustness to Distractors&lt;/strong&gt;: Specifically trained to distinguish between malicious instructions and benign, structure-rich HTML &amp;quot;noise&amp;quot;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Real-Time Latency&lt;/strong&gt;: Optimized for agent loops, enabling async security checks without degrading user experience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Comprehensive Coverage&lt;/strong&gt;: Validated against 11 attack types with different security criticality levels.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;BrowseSafe model overview&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Type&lt;/strong&gt;: Fine-tuned Causal Language Model (MoE) for SFT Classification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Stage&lt;/strong&gt;: Post-training (Fine-tuning on BrowseSafe-Bench)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: BrowseSafe-Bench&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Base Model&lt;/strong&gt;: Qwen/Qwen3-30B-A3B-Instruct-2507&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Length&lt;/strong&gt;: Up to 16,384 tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input&lt;/strong&gt;: Raw HTML content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output&lt;/strong&gt;: Single token, &amp;quot;yes&amp;quot; or &amp;quot;no&amp;quot; classification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License&lt;/strong&gt;: MIT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/perplexity-ai/browsesafe"&gt;BrowseSafe model&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x84cad88q65g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzmxh/browsesafe_an_opensource_model_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzmxh/browsesafe_an_opensource_model_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe1bd4</id>
    <title>The "Confident Idiot" Problem: Why LLM-as-a-Judge fails in production.</title>
    <updated>2025-12-04T14:25:15+00:00</updated>
    <author>
      <name>/u/Proud-Employ5627</name>
      <uri>https://old.reddit.com/user/Proud-Employ5627</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been struggling with agent reliability lately. I noticed that the industry standard for fixing hallucinations is &amp;quot;LLM-as-a-Judge&amp;quot; (asking a larger model to grade the output).&lt;/p&gt; &lt;p&gt;But I'm finding this creates a circular dependency. If the underlying models suffer from sycophancy or hallucination, the Judge often hallucinates a passing grade. We are trying to fix probability with more probability.&lt;/p&gt; &lt;p&gt;I wrote up a deep dive on why I think we need to re-introduce &lt;strong&gt;Deterministic Assertions&lt;/strong&gt; (running actual code/regex/SQL parsing) into the agent loop instead of just relying on &amp;quot;Vibe Checks.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Core Argument:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a URL is valid. Run &lt;code&gt;requests.get()&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Don't ask an LLM if a SQL query is safe. Parse the AST.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;If the code says &amp;quot;No&amp;quot;, the agent stops. No matter how confident the LLM is.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Full analysis here: &lt;a href="https://steerlabs.substack.com/p/confident-idiot-problem"&gt;https://steerlabs.substack.com/p/confident-idiot-problem&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious how others are handling this? Are you using LLM-as-a-Judge successfully, or do you rely on hard constraints?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud-Employ5627"&gt; /u/Proud-Employ5627 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe1bd4/the_confident_idiot_problem_why_llmasajudge_fails/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T14:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4xm4</id>
    <title>VLLM v0.12.0 supports NVFP4 for SM120 (RTX 50xx and RTX PRO 6000 Blackwell)</title>
    <updated>2025-12-04T16:47:16+00:00</updated>
    <author>
      <name>/u/Rascazzione</name>
      <uri>https://old.reddit.com/user/Rascazzione</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My kudos for the VLLM team that has release the v0.12.0 with support for NVFP4 for the SM120 family!&lt;/p&gt; &lt;h1&gt;Quantization&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;W4A8&lt;/strong&gt;: Marlin kernel support (&lt;a href="https://github.com/vllm-project/vllm/pull/24722"&gt;#24722&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVFP4&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;MoE CUTLASS support for SM120 (&lt;a href="https://github.com/vllm-project/vllm/pull/29242"&gt;#29242&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;TRTLLM MoE NVFP4 kernel (&lt;a href="https://github.com/vllm-project/vllm/pull/28892"&gt;#28892&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;CuteDSL MoE with NVFP4 DeepEP dispatch (&lt;a href="https://github.com/vllm-project/vllm/pull/27141"&gt;#27141&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Non-gated activations support in modelopt path (&lt;a href="https://github.com/vllm-project/vllm/pull/29004"&gt;#29004&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AWQ&lt;/strong&gt;: Compressed-tensors AWQ support for Turing GPUs (&lt;a href="https://github.com/vllm-project/vllm/pull/29732"&gt;#29732&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoRA&lt;/strong&gt;: FusedMoE LoRA Triton kernel for MXFP4 (&lt;a href="https://github.com/vllm-project/vllm/pull/29708"&gt;#29708&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Online quantization&lt;/strong&gt;: Moved to &lt;code&gt;model.load_weights&lt;/code&gt; (&lt;a href="https://github.com/vllm-project/vllm/pull/26327"&gt;#26327&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/releases"&gt;https://github.com/vllm-project/vllm/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally, we'll be able to make some models fly!&lt;/p&gt; &lt;p&gt;In an ubuntu 24.04 with driver 580 and nvidia rtx 6000 pro, just &lt;code&gt;uv pip install vllm --upgrade&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Then &lt;code&gt;vllm bench serve --model &amp;quot;openai/gpt-oss-120b&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 1000 Failed requests: 0 Benchmark duration (s): 90.34 Total input tokens: 1024000 Total generated tokens: 128000 Request throughput (req/s): 11.07 Output token throughput (tok/s): 1416.83 Peak output token throughput (tok/s): 4598.00 Peak concurrent requests: 1000.00 Total Token throughput (tok/s): 12751.46 ---------------Time to First Token---------------- Mean TTFT (ms): 40130.28 Median TTFT (ms): 38045.08 P99 TTFT (ms): 83577.52 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 167.93 Median TPOT (ms): 182.06 P99 TPOT (ms): 222.48 ---------------Inter-token Latency---------------- Mean ITL (ms): 167.95 Median ITL (ms): 67.44 P99 ITL (ms): 498.29 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rascazzione"&gt; /u/Rascazzione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4xm4/vllm_v0120_supports_nvfp4_for_sm120_rtx_50xx_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe5l30</id>
    <title>Tell us a task and we'll help you solve it with Granite</title>
    <updated>2025-12-04T17:11:09+00:00</updated>
    <author>
      <name>/u/ibm</name>
      <uri>https://old.reddit.com/user/ibm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share a task, workflow, or challenge you‚Äôd like one of our Granite 4.0 models to help with, and we‚Äôll select a few and show you ‚Äî step by step ‚Äî how to choose the right model and get it done.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ibm"&gt; /u/ibm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe5l30/tell_us_a_task_and_well_help_you_solve_it_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T17:11:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pe4iev</id>
    <title>We Got Claude to Fine-Tune an Open Source LLM</title>
    <updated>2025-12-04T16:31:07+00:00</updated>
    <author>
      <name>/u/PotentialFunny7143</name>
      <uri>https://old.reddit.com/user/PotentialFunny7143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/blog/hf-skills-training"&gt;https://huggingface.co/blog/hf-skills-training&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PotentialFunny7143"&gt; /u/PotentialFunny7143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pe4iev/we_got_claude_to_finetune_an_open_source_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T16:31:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdvupp</id>
    <title>Cruxy: Train 1.5B models on 4GB VRAM - new optimiser just released</title>
    <updated>2025-12-04T09:37:58+00:00</updated>
    <author>
      <name>/u/National_Control4101</name>
      <uri>https://old.reddit.com/user/National_Control4101</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've just released Cruxy - an adaptive optimiser that lets you fine-tune billion-parameter models on consumer GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; - Drop-in replacement for AdamW - Meta-Lion mode uses 1/3 the memory of AdamW - Automatic stability control - no scheduler tuning needed - Verified on TinyLlama 1.1B and Qwen 2.5 1.5B on a GTX 1650 (4GB)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks (Shakespeare GPT):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Optimiser&lt;/th&gt; &lt;th&gt;Final Loss&lt;/th&gt; &lt;th&gt;Memory&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;AdamW&lt;/td&gt; &lt;td&gt;1.6843&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta3&lt;/td&gt; &lt;td&gt;1.6413&lt;/td&gt; &lt;td&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Cruxy Meta-Lion&lt;/td&gt; &lt;td&gt;1.6633&lt;/td&gt; &lt;td&gt;33%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pip install Cruxy&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/christophergardner-star/Crux1"&gt;https://github.com/christophergardner-star/Crux1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions. Built this on evenings and weekends because cloud GPUs are expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/National_Control4101"&gt; /u/National_Control4101 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdvupp/cruxy_train_15b_models_on_4gb_vram_new_optimiser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T09:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdupdg</id>
    <title>Deepseek's progress</title>
    <updated>2025-12-04T08:21:16+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt; &lt;img alt="Deepseek's progress" src="https://preview.redd.it/zpkzyrrxc55g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88e4dbc74aac37f16270f4775ec470f375eab2f5" title="Deepseek's progress" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's fascinating that DeepSeek has been able to make all this progress with the same pre-trained model since the start of the year, and has just improved post-training and attention mechanisms. It makes you wonder if other labs are misusing their resources by training new base models so often.&lt;/p&gt; &lt;p&gt;Also, what is going on with the Mistral Large 3 benchmarks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zpkzyrrxc55g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdupdg/deepseeks_progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T08:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu5pe</id>
    <title>WTF are these AI companies doing where they supposedly are the cause of the ram price spike?</title>
    <updated>2025-12-04T07:46:40+00:00</updated>
    <author>
      <name>/u/Red_Redditor_Reddit</name>
      <uri>https://old.reddit.com/user/Red_Redditor_Reddit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't understand what could justify that much investment. Maybe I'm way out of the loop, but what huge application are they expecting that would have this kind of payout? Why is there all of the sudden this spike instead of a slower increase in demand? Like I kinda get the overall GPU demand, but this sudden dramatic change in RAM demand doesn't make sense to me. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Red_Redditor_Reddit"&gt; /u/Red_Redditor_Reddit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu5pe/wtf_are_these_ai_companies_doing_where_they/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:46:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdu46s</id>
    <title>New model, microsoft/VibeVoice-Realtime-0.5B</title>
    <updated>2025-12-04T07:43:58+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt; &lt;img alt="New model, microsoft/VibeVoice-Realtime-0.5B" src="https://external-preview.redd.it/yC3RHTaiptQZaDONKxzLP6lQoJh8pT8uDk6mruPADNY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b568bf1e3f993edb57eab9f43241d593fd7c1c2" title="New model, microsoft/VibeVoice-Realtime-0.5B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;VibeVoice: A Frontier Open-Source Text-to-Speech Model&lt;/p&gt; &lt;p&gt;VibeVoice-Realtime is a lightweight real‚Äëtime text-to-speech model supporting streaming text input. It can be used to build realtime TTS services, narrate live data streams, and let different LLMs start speaking from their very first tokens (plug in your preferred model) long before a full answer is generated. It produces initial audible speech in ~300 ms (hardware dependent).&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;p&gt;Parameter size: 0.5B (deployment-friendly) Realtime TTS (~300 ms first audible latency) Streaming text input Robust long-form speech generation&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/microsoft/VibeVoice-Realtime-0.5B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdu46s/new_model_microsoftvibevoicerealtime05b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T07:43:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdzn2n</id>
    <title>legends</title>
    <updated>2025-12-04T13:11:47+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt; &lt;img alt="legends" src="https://preview.redd.it/vu26lxrns65g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a61d2260347cccaa67517ffc3812c121edcd5d0" title="legends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vu26lxrns65g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdzn2n/legends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T13:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
