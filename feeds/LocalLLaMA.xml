<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-29T19:34:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ntglg5</id>
    <title>I built EdgeBox, an open-source local sandbox with a full GUI desktop, all controllable via the MCP protocol.</title>
    <updated>2025-09-29T12:25:04+00:00</updated>
    <author>
      <name>/u/Diao_nasing</name>
      <uri>https://old.reddit.com/user/Diao_nasing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntglg5/i_built_edgebox_an_opensource_local_sandbox_with/"&gt; &lt;img alt="I built EdgeBox, an open-source local sandbox with a full GUI desktop, all controllable via the MCP protocol." src="https://preview.redd.it/2w6bjp03k3sf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=3bbe7dc260557332836c22fecb2f56bc0083ce5c" title="I built EdgeBox, an open-source local sandbox with a full GUI desktop, all controllable via the MCP protocol." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMa community,&lt;/p&gt; &lt;p&gt;I always wanted my MCP agents to do more than just execute code—I wanted them to actually use a GUI. So, I built &lt;strong&gt;EdgeBox&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's a free, open-source desktop app that gives your agent a &lt;strong&gt;local sandbox with a full GUI desktop&lt;/strong&gt;, all controllable via the MCP protocol.&lt;/p&gt; &lt;h1&gt;Core Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-Config Local MCP Server&lt;/strong&gt;: Works out of the box, no setup required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Control the Desktop via MCP&lt;/strong&gt;: Provides tools like &lt;code&gt;desktop_mouse_click&lt;/code&gt; and &lt;code&gt;desktop_screenshot&lt;/code&gt; to let the agent operate the GUI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built-in Code Interpreter &amp;amp; Filesystem&lt;/strong&gt;: Includes all the core tools you need, like &lt;code&gt;execute_python&lt;/code&gt; and &lt;code&gt;fs_write&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is open-source, and I'd love for you to try it out and give some feedback!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo (includes downloads):&lt;/strong&gt; &lt;a href="https://github.com/BIGPPWONG/edgebox"&gt;https://github.com/BIGPPWONG/edgebox&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diao_nasing"&gt; /u/Diao_nasing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2w6bjp03k3sf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntglg5/i_built_edgebox_an_opensource_local_sandbox_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntglg5/i_built_edgebox_an_opensource_local_sandbox_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2l57</id>
    <title>Qwen3 Omni AWQ released</title>
    <updated>2025-09-28T23:12:33+00:00</updated>
    <author>
      <name>/u/No_Information9314</name>
      <uri>https://old.reddit.com/user/No_Information9314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Information9314"&gt; /u/No_Information9314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T23:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntofd1</id>
    <title>FULL Sonnet 4.5 System Prompt and Internal Tools</title>
    <updated>2025-09-29T17:33:37+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Latest update: 29/09/2025&lt;/p&gt; &lt;p&gt;I’ve published the FULL Sonnet 4.5 by Anthropic System prompt and Internal tools. Over 8,000 tokens.&lt;/p&gt; &lt;p&gt;You can check it out here: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntofd1/full_sonnet_45_system_prompt_and_internal_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntofd1/full_sonnet_45_system_prompt_and_internal_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntofd1/full_sonnet_45_system_prompt_and_internal_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T17:33:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntluwl</id>
    <title>llama.cpp: Quantizing from bf16 vs f16</title>
    <updated>2025-09-29T15:57:37+00:00</updated>
    <author>
      <name>/u/Confident-Willow5457</name>
      <uri>https://old.reddit.com/user/Confident-Willow5457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Almost all model weights are released in bf16 these days, so obviously a conversion from bf16 -&amp;gt; f16 is lossy and results in objectively less precise weights. However, could the resulting quantization from f16 end up being overall more precise than the quantization from bf16? Let me explain.&lt;/p&gt; &lt;p&gt;F16 has less range than bf16, so outliers get clipped. When this is further quantized to an INT format, the outlier weights will be less precise than if you had quantized from bf16, however the other weights in their block will have greater precision due to the decreased range, no? So f16 could be seen as an optimization step.&lt;/p&gt; &lt;p&gt;Forgive me if I have a misunderstanding about something.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Confident-Willow5457"&gt; /u/Confident-Willow5457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntluwl/llamacpp_quantizing_from_bf16_vs_f16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntluwl/llamacpp_quantizing_from_bf16_vs_f16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntluwl/llamacpp_quantizing_from_bf16_vs_f16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T15:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntms89</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-09-29T16:32:35+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI, here are the local/edge highlights from today's edition:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EmbeddingGemma - 308M beats models 2x its size&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs on &amp;lt;200MB RAM with quantization&lt;/li&gt; &lt;li&gt;22ms embeddings on EdgeTPU&lt;/li&gt; &lt;li&gt;Handles 100+ languages&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2509.20354"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MetaEmbed - Runtime scaling for retrieval&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adjust precision on the fly (1-32 vectors)&lt;/li&gt; &lt;li&gt;Same model works on phone and datacenter&lt;/li&gt; &lt;li&gt;No retraining needed&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2509.18095"&gt;Paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;tinyWorlds - 3M parameter world model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generates playable game environments&lt;/li&gt; &lt;li&gt;Proves efficient world modeling possible&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/AlmondGod/tinyworlds"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ntms89/video/15oog6kas4sf1/player"&gt;https://reddit.com/link/1ntms89/video/15oog6kas4sf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smol2Operator - 2.2B agentic GUI coder&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full open-source recipe from HuggingFace&lt;/li&gt; &lt;li&gt;Build custom agentic coding systems locally&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/blog/smol2operator"&gt;Blog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lynx personalized video from single photo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ntms89/video/1ueddn6cs4sf1/player"&gt;https://reddit.com/link/1ntms89/video/1ueddn6cs4sf1/player&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hunyuan3D-Part for part-level 3D generation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ntms89/video/0pifv4fes4sf1/player"&gt;https://reddit.com/link/1ntms89/video/0pifv4fes4sf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Free newsletter(demos,papers,more): &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-26-adaptive-retrieval"&gt;https://thelivingedge.substack.com/p/multimodal-monday-26-adaptive-retrieval&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntms89/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntms89/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntms89/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntfkjq</id>
    <title>Why no small &amp; medium size models from Deepseek?</title>
    <updated>2025-09-29T11:34:23+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last time I downloaded something was their Distillations(Qwen 1.5B, 7B, 14B &amp;amp; Llama 8B) during R1 release last Jan/Feb. After that, most of their models are 600B+ size. My hardware(8GB VRAM, 32B RAM) can't even touch those.&lt;/p&gt; &lt;p&gt;It would be great if they release small &amp;amp; medium size models like how Qwen done. Also couple of MOE models particularly one with 30-40B size.&lt;/p&gt; &lt;p&gt;BTW lucky big rig folks, enjoy DeepSeek-V3.2-Exp soon onwards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfkjq/why_no_small_medium_size_models_from_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfkjq/why_no_small_medium_size_models_from_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfkjq/why_no_small_medium_size_models_from_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T11:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt1jaa</id>
    <title>Good ol gpu heat</title>
    <updated>2025-09-28T22:24:45+00:00</updated>
    <author>
      <name>/u/animal_hoarder</name>
      <uri>https://old.reddit.com/user/animal_hoarder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt; &lt;img alt="Good ol gpu heat" src="https://preview.redd.it/94k8168cezrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7614d55e91893f545ea06888d5bbbc047c1ec146" title="Good ol gpu heat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live at 9600ft in a basement with extremely inefficient floor heaters, so it’s usually 50-60F inside year round. I’ve been fine tuning Mistral 7B for a dungeons and dragons game I’ve been working on and oh boy does my 3090 pump out some heat. Popped the front cover off for some more airflow. My cat loves my new hobby, he just waits for me to run another training script so he can soak it in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/animal_hoarder"&gt; /u/animal_hoarder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94k8168cezrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T22:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntheaj</id>
    <title>New to LLMs - What’s the Best Local AI Stack for a Complete ChatGPT Replacement?</title>
    <updated>2025-09-29T13:00:56+00:00</updated>
    <author>
      <name>/u/Live_Drive_6256</name>
      <uri>https://old.reddit.com/user/Live_Drive_6256</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I’m looking to set up my own private, local LLM on my PC. I’ve got a pretty powerful setup with 20TB of storage, 256GB of RAM, an RTX 3090, and an i9 CPU.&lt;/p&gt; &lt;p&gt;I’m super new to LLMs but just discovered I can host them private and locally on my own PC with an actual WebUI like ChatGPT. I’m after something that can basically interpret images and files, generate images and code, handle long conversations or scripts without losing context, delusion, repetitiveness. Ideally act as a complete offline alternative to ChatGPT-5.&lt;/p&gt; &lt;p&gt;Is this possible to even achieve? Am I delusional??? Can I even host an AI model stack that can do everything ChatGPT does like reasoning, vision, coding, creativity, but fully private and running on my own machine with these specs? &lt;/p&gt; &lt;p&gt;If anyone has experience building this kind of all-in-one local setup or can recommend the best models and tools for it, I’d really appreciate the advice.&lt;/p&gt; &lt;p&gt;Thanks!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Live_Drive_6256"&gt; /u/Live_Drive_6256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntheaj/new_to_llms_whats_the_best_local_ai_stack_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntheaj/new_to_llms_whats_the_best_local_ai_stack_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntheaj/new_to_llms_whats_the_best_local_ai_stack_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T13:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntrau8</id>
    <title>inclusionAI/Ring-1T-preview</title>
    <updated>2025-09-29T19:21:03+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntrau8/inclusionairing1tpreview/"&gt; &lt;img alt="inclusionAI/Ring-1T-preview" src="https://preview.redd.it/7vb7yumam5sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f95b7ca10a14cb69158cd8b654359ce4e30e802f" title="inclusionAI/Ring-1T-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Weights: &lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;https://huggingface.co/inclusionAI/Ring-1T-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vb7yumam5sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntrau8/inclusionairing1tpreview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntrau8/inclusionairing1tpreview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T19:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntiv83</id>
    <title>NVIDIA LongLive : Real-time Interactive Long Video Generation</title>
    <updated>2025-09-29T14:03:01+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA and collaborators just released &lt;strong&gt;LongLive&lt;/strong&gt;, a text-to-video system that finally tackles long, interactive videos. Most models outputs 5–10 second clips, but LongLive handles up to 240 seconds on a single H100, staying smooth and responsive even when you switch prompts mid-video. It combines &lt;strong&gt;KV re-cache&lt;/strong&gt; for seamless prompt changes, &lt;strong&gt;streaming long tuning&lt;/strong&gt; to handle extended rollouts, and &lt;strong&gt;short-window attention + frame sink&lt;/strong&gt; to balance speed with context. &lt;/p&gt; &lt;p&gt;Benchmarks show massive speedups (20+ FPS vs &amp;lt;1 FPS for baselines) while keeping quality high.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://arxiv.org/abs/2509.22622"&gt;https://arxiv.org/abs/2509.22622&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace Model : &lt;a href="https://huggingface.co/Efficient-Large-Model/LongLive-1.3B"&gt;https://huggingface.co/Efficient-Large-Model/LongLive-1.3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video demo : &lt;a href="https://youtu.be/caDE6f54pvA"&gt;https://youtu.be/caDE6f54pvA&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiv83/nvidia_longlive_realtime_interactive_long_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiv83/nvidia_longlive_realtime_interactive_long_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiv83/nvidia_longlive_realtime_interactive_long_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T14:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt8jf0</id>
    <title>I have discovered DeepSeeker V3.2-Base</title>
    <updated>2025-09-29T04:10:45+00:00</updated>
    <author>
      <name>/u/ReceptionExternal344</name>
      <uri>https://old.reddit.com/user/ReceptionExternal344</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt; &lt;img alt="I have discovered DeepSeeker V3.2-Base" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="I have discovered DeepSeeker V3.2-Base" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discovered the deepseek-3.2-base repository on Hugging Face just half an hour ago, but within minutes it returned a 404 error. Another model is on its way!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/al21vk9t31sf1.png?width=2690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=067b5daef487efac4fba9699c13a24294088dc42"&gt;https://preview.redd.it/al21vk9t31sf1.png?width=2690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=067b5daef487efac4fba9699c13a24294088dc42&lt;/a&gt;&lt;/p&gt; &lt;p&gt;unfortunately, I forgot to check the config.json file and only took a screenshot of the repository. I'll just wait for the release now.&lt;/p&gt; &lt;p&gt;Now we have discovered：&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReceptionExternal344"&gt; /u/ReceptionExternal344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T04:10:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntml0a</id>
    <title>I think gpt-oss:20b misunderstood its own thought process.</title>
    <updated>2025-09-29T16:25:12+00:00</updated>
    <author>
      <name>/u/FitKaleidoscope1806</name>
      <uri>https://old.reddit.com/user/FitKaleidoscope1806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntml0a/i_think_gptoss20b_misunderstood_its_own_thought/"&gt; &lt;img alt="I think gpt-oss:20b misunderstood its own thought process." src="https://a.thumbs.redditmedia.com/OAg-oQVGFEdbS5Hw1pFefqiGtZv8UMvkwArgjkwmQ-4.jpg" title="I think gpt-oss:20b misunderstood its own thought process." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This made me laugh and just wanted to share with like minded people. I am running gpt-oss:20b on an RTX 3080ti and have it connected to web search. I was just skimming through some options for learning electrical engineering self taught or any certificates I could maybe take online (for fun and to learn) so I was using websearch. &lt;/p&gt; &lt;p&gt;Looking at the thought process there was some ambiguity in the way it was reading its sources and it misunderstood own thought process. So ultimately it determines that the answer is yes and tells itself to cite specific sources and &amp;quot;craft answer in simple language&amp;quot; &lt;/p&gt; &lt;p&gt;From there its response was completely in Spanish. It made me laugh and I just wanted to share my experience. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FitKaleidoscope1806"&gt; /u/FitKaleidoscope1806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ntml0a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntml0a/i_think_gptoss20b_misunderstood_its_own_thought/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntml0a/i_think_gptoss20b_misunderstood_its_own_thought/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntgbag</id>
    <title>Literally me this weekend, after 2+ hours of trying I did not manage to make AWQ quant work on a100, meanwhile the same quant works in vLLM without any problems...</title>
    <updated>2025-09-29T12:11:55+00:00</updated>
    <author>
      <name>/u/Theio666</name>
      <uri>https://old.reddit.com/user/Theio666</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntgbag/literally_me_this_weekend_after_2_hours_of_trying/"&gt; &lt;img alt="Literally me this weekend, after 2+ hours of trying I did not manage to make AWQ quant work on a100, meanwhile the same quant works in vLLM without any problems..." src="https://preview.redd.it/7mx203wgh3sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf661e0987faa8d24ab8bfb1709f07e3b7f14ac5" title="Literally me this weekend, after 2+ hours of trying I did not manage to make AWQ quant work on a100, meanwhile the same quant works in vLLM without any problems..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Theio666"&gt; /u/Theio666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7mx203wgh3sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntgbag/literally_me_this_weekend_after_2_hours_of_trying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntgbag/literally_me_this_weekend_after_2_hours_of_trying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntfbm0</id>
    <title>DeepSeek Updates API Pricing (DeepSeek-V3.2-Exp)</title>
    <updated>2025-09-29T11:21:00+00:00</updated>
    <author>
      <name>/u/Agwinao</name>
      <uri>https://old.reddit.com/user/Agwinao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfbm0/deepseek_updates_api_pricing_deepseekv32exp/"&gt; &lt;img alt="DeepSeek Updates API Pricing (DeepSeek-V3.2-Exp)" src="https://preview.redd.it/0hwzyvjr83sf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81278c017a6d397ce1e552fdf94c7948c0595869" title="DeepSeek Updates API Pricing (DeepSeek-V3.2-Exp)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;$0.028 / 1M Input Tokens (Cache Hit), $0.28 / 1M Input Tokens (Cache Miss), $0.42 / 1M Output Tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agwinao"&gt; /u/Agwinao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0hwzyvjr83sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfbm0/deepseek_updates_api_pricing_deepseekv32exp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfbm0/deepseek_updates_api_pricing_deepseekv32exp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T11:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntb5ab</id>
    <title>deepseek-ai/DeepSeek-V3.2 · Hugging Face</title>
    <updated>2025-09-29T06:49:56+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 · Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New Link &lt;a href="https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66"&gt;https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T06:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntmt7d</id>
    <title>granite 4 GGUFs are still hidden</title>
    <updated>2025-09-29T16:33:38+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmt7d/granite_4_ggufs_are_still_hidden/"&gt; &lt;img alt="granite 4 GGUFs are still hidden" src="https://b.thumbs.redditmedia.com/Mh_nakL415CMje-YrJNCT0tuJzoPhaN3zSlv0zAoxjw.jpg" title="granite 4 GGUFs are still hidden" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ntmt7d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmt7d/granite_4_ggufs_are_still_hidden/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmt7d/granite_4_ggufs_are_still_hidden/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte4j1</id>
    <title>Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base • HuggingFace</title>
    <updated>2025-09-29T10:09:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt; &lt;img alt="Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base • HuggingFace" src="https://external-preview.redd.it/VTfzQkd7AA6Y5gHEsOqxIa7Bzf1OPlvJrdnEYbmotnQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba6d5dd6d41a5218ce7ddbe2cce44e354d9f63ea" title="Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base • HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jrpylyzjx2sf1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8df476ca80a0fa1d01539f59049856b2e15979"&gt;https://preview.redd.it/jrpylyzjx2sf1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8df476ca80a0fa1d01539f59049856b2e15979&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0hamcb6x2sf1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5abcd9c9073ae9a56e24b51c4c3fc7a7d13d5c33"&gt;https://preview.redd.it/d0hamcb6x2sf1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5abcd9c9073ae9a56e24b51c4c3fc7a7d13d5c33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/celk1fb6x2sf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6713f8594e8a66987bade6a83ca404877e077646"&gt;https://preview.redd.it/celk1fb6x2sf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6713f8594e8a66987bade6a83ca404877e077646&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5xnbatckw2sf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2e365d129160081fc292cbac7c1c5b2dc50814"&gt;https://preview.redd.it/5xnbatckw2sf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2e365d129160081fc292cbac7c1c5b2dc50814&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt99fp</id>
    <title>GLM-4.6 now accessible via API</title>
    <updated>2025-09-29T04:52:14+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"&gt; &lt;img alt="GLM-4.6 now accessible via API" src="https://preview.redd.it/yrpnx9o7b1sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=034a4f48d831abff53602bf4adefb90e5b28a82b" title="GLM-4.6 now accessible via API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using the official API, I was able to access GLM 4.6. Looks like release is imminent.&lt;/p&gt; &lt;p&gt;On a side note, the reasoning traces look very different from previous Chinese releases, much more like Gemini models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yrpnx9o7b1sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T04:52:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntmiz1</id>
    <title>Sammyuri built a redstone system to run a small language model (~5M params) in Minecraft!</title>
    <updated>2025-09-29T16:22:54+00:00</updated>
    <author>
      <name>/u/Daniel_H212</name>
      <uri>https://old.reddit.com/user/Daniel_H212</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmiz1/sammyuri_built_a_redstone_system_to_run_a_small/"&gt; &lt;img alt="Sammyuri built a redstone system to run a small language model (~5M params) in Minecraft!" src="https://external-preview.redd.it/9KGtXL31_ILzBLGvG1YO0OZiv4dTVrqsaYqfruNLKD8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7208d66dc5511d3ad04d8f44f11e2cfe87d5239" title="Sammyuri built a redstone system to run a small language model (~5M params) in Minecraft!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;May not be interesting to most people, but as a Minecraft player, this is insane and I think deserves recognition. This is running a local language model after all, so I think it fits here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daniel_H212"&gt; /u/Daniel_H212 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=VaeI9YgE1o8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmiz1/sammyuri_built_a_redstone_system_to_run_a_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmiz1/sammyuri_built_a_redstone_system_to_run_a_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:22:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntkhy4</id>
    <title>3 Tesla GPUs in a Desktop Case</title>
    <updated>2025-09-29T15:06:42+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntkhy4/3_tesla_gpus_in_a_desktop_case/"&gt; &lt;img alt="3 Tesla GPUs in a Desktop Case" src="https://b.thumbs.redditmedia.com/SCJrZr3BN3gZMwMfhfc1yRYq4ZCMxF344eoPEV5hcjQ.jpg" title="3 Tesla GPUs in a Desktop Case" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Plus a slot leftover for a dual 10G ethernet adapter. Originally, a goal of the &lt;a href="https://esologic.com/new-cooler-first-look/"&gt;cooler project&lt;/a&gt; was to be able to do 4 cards in a desktop case but after a lot of experimentation, I don't think it's realistic to be able to dissapate 1000W+ with only your standard case fans.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ntkhy4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntkhy4/3_tesla_gpus_in_a_desktop_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntkhy4/3_tesla_gpus_in_a_desktop_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T15:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntmj9c</id>
    <title>Fiction.liveBench tested DeepSeek 3.2, Qwen-max, grok-4-fast, Nemotron-nano-9b</title>
    <updated>2025-09-29T16:23:13+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmj9c/fictionlivebench_tested_deepseek_32_qwenmax/"&gt; &lt;img alt="Fiction.liveBench tested DeepSeek 3.2, Qwen-max, grok-4-fast, Nemotron-nano-9b" src="https://preview.redd.it/2krrie9kq4sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4324894b3e610dcdabb98a60c481e0333d12c3a" title="Fiction.liveBench tested DeepSeek 3.2, Qwen-max, grok-4-fast, Nemotron-nano-9b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2krrie9kq4sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmj9c/fictionlivebench_tested_deepseek_32_qwenmax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntmj9c/fictionlivebench_tested_deepseek_32_qwenmax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T16:23:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntiua9</id>
    <title>We just open-sourced Kroko ASR: a fast, streaming alternative to Whisper. It’s early days, we’d love testers, feedback, and contributors.</title>
    <updated>2025-09-29T14:01:58+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;First batch&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Streaming models (CC-BY-SA), ready for CPU, mobile, or browser&lt;/li&gt; &lt;li&gt;More extreme but affordable commercial models (with Apache inference code)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A dozen to start, more on the way (Polish and Japanese coming next.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it’s different&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Much smaller download than Whisper&lt;/li&gt; &lt;li&gt;Much faster on CPU (runs on mobile or even in the browser, try the the demo on android)&lt;/li&gt; &lt;li&gt;(Almost) hallucination-free&lt;/li&gt; &lt;li&gt;Streaming support: great for voice assistants, live agent assist, note taking, or just yelling at your computer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offline models beat Whisper v3-large while being about 10× smaller&lt;/li&gt; &lt;li&gt;Streaming models are comparable (or better) at 1s chunk size&lt;/li&gt; &lt;li&gt;There’s a trade-off in quality at ultra-low latency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Project goals&lt;/strong&gt;&lt;br /&gt; Build a community and democratize speech-to-text, making it easier to train models and run them at the edge (without needing a PhD in speech AI).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;website &amp;amp; cloud demo: &lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Android model explorer: &lt;a href="https://play.google.com/store/apps/details?id=com.krokoasr.demo&amp;amp;hl=en"&gt;Google Play&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Discord: &lt;a href="https://discord.gg/nnY9nQac"&gt;discord.gg/nnY9nQac&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kroko-ai/kroko-onnx"&gt;https://github.com/kroko-ai/kroko-onnx&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;Kroko Streaming ASR Wasm&lt;/a&gt; (older models, updates coming soon)&lt;/li&gt; &lt;li&gt;community models page: &lt;a href="https://huggingface.co/Banafo/Kroko-ASR"&gt;https://huggingface.co/Banafo/Kroko-ASR&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Thoughts / caveats&lt;/strong&gt;&lt;br /&gt; We’re still ironing out some things, especially around licensing limits and how to release models in the fairest way. Our philosophy is: easier to give more than to give less later. Some details may change as we learn from the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt;&lt;br /&gt; There is plenty of room to improve the models, as most are still trained on our older pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;br /&gt; Smaller, faster, (almost) hallucination-free Whisper replacement that streams on CPU/mobile. Looking for testers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T14:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntg6sp</id>
    <title>Chinese AI Labs Tier List</title>
    <updated>2025-09-29T12:05:39+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"&gt; &lt;img alt="Chinese AI Labs Tier List" src="https://preview.redd.it/ur65noupg3sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19a3d6f6cec05bb06281985755fbed368e5c9ecf" title="Chinese AI Labs Tier List" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ur65noupg3sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte1kr</id>
    <title>DeepSeek-V3.2 released</title>
    <updated>2025-09-29T10:04:40+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66"&gt;https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nth7cb</id>
    <title>The reason why Deepseek V3.2 is so cheap</title>
    <updated>2025-09-29T12:52:22+00:00</updated>
    <author>
      <name>/u/Js8544</name>
      <uri>https://old.reddit.com/user/Js8544</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt; &lt;img alt="The reason why Deepseek V3.2 is so cheap" src="https://external-preview.redd.it/FcVX6tzRZ8tGOgZD8bxf6fQ4_S6KT4J6UgLFbHWcrYo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dddb9f56e7fc9cdc0774a534e8c31cbb7079188" title="The reason why Deepseek V3.2 is so cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: It's a near linear model with almost O(kL) attention complexity.&lt;/p&gt; &lt;p&gt;Paper link: &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf"&gt;https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to their paper, the Deepseek Sparse Attention computes attention for only k selected previous tokens, meaning it's a linear attention model with decoding complexity O(kL). What's different from previous linear models is it has a O(L^2) index selector to select the tokens to compute attention for. Even though the index selector has square complexity but it's fast enough to be neglected. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h0zys7b4o3sf1.png?width=1390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00a7ea8ada91109d417b8d6e3f490ae9743c18b2"&gt;https://preview.redd.it/h0zys7b4o3sf1.png?width=1390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00a7ea8ada91109d417b8d6e3f490ae9743c18b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/has2qyz7o3sf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0742135b2cb1be9bd853b614097597d521a4ef54"&gt;https://preview.redd.it/has2qyz7o3sf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0742135b2cb1be9bd853b614097597d521a4ef54&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/053i7pdro3sf1.png?width=1356&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52adfb1bf9d0ee03f0a7d8e7b31340ab63b2f4b4"&gt;Cost for V3.2 only increase very little thanks to linear attention&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous linear model attempts for linear models from other teams like Google and Minimax have not been successful. Let's see if DS can make the breakthrough this time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Js8544"&gt; /u/Js8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
