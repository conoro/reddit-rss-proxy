<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-25T05:54:20+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qmanai</id>
    <title>MBP 2019 i9 (64GB RAM) hitting 800% CPU on AnythingLLMs(12B) â€” Need optimization tips and model recs!</title>
    <updated>2026-01-25T05:43:20+00:00</updated>
    <author>
      <name>/u/vdnn1902</name>
      <uri>https://old.reddit.com/user/vdnn1902</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmanai/mbp_2019_i9_64gb_ram_hitting_800_cpu_on/"&gt; &lt;img alt="MBP 2019 i9 (64GB RAM) hitting 800% CPU on AnythingLLMs(12B) â€” Need optimization tips and model recs!" src="https://preview.redd.it/wfd61y4gnffg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10c0f3bdea58ec1bc0812a765e1239bfd71e290d" title="MBP 2019 i9 (64GB RAM) hitting 800% CPU on AnythingLLMs(12B) â€” Need optimization tips and model recs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;Iâ€™m currently running a local AI setup on my &lt;strong&gt;2019 16-inch MacBook Pro&lt;/strong&gt; and Iâ€™m hitting some serious performance thermal throttling. Iâ€™d love some advice from those still rocking Intel Macs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; 2.4 GHz 8-Core Intel Core i9 (16 threads)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64 GB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; AMD Radeon Pro 5500M 8GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; AnythingLLM / Ollama&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Issue:&lt;/strong&gt; Whenever I run models (currently trying Gemma 3 12B) or perform embeddings for my Obsidian vault, my CPU usage spikes to &lt;strong&gt;700-800%&lt;/strong&gt;, and temperatures hit &lt;strong&gt;85-90Â°C&lt;/strong&gt; almost instantly. The fans are screaming, and the response time is quite sluggish. I notice the GPU (AMD 8GB) isn't being utilized much compared to the CPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Optimization:&lt;/strong&gt; How can I better offload tasks to the AMD GPU on an Intel Mac? Any specific environment variables for Ollama or settings in AnythingLLM to limit thread count and prevent the i9 from melting?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Recommendations:&lt;/strong&gt; Given the 64GB RAM, I have plenty of memory but limited &amp;quot;brain power&amp;quot; (CPU/GPU speed). What 8B-14B models are you finding to be the &amp;quot;sweet spot&amp;quot; for logic vs. speed on Intel hardware?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAG Performance:&lt;/strong&gt; For those using Obsidian with Local AI, whatâ€™s the best way to handle large embeddings without locking up the system for an hour?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I know &amp;quot;Buy an M3&amp;quot; is the easy answer, but I want to squeeze every bit of life out of this 64GB beast first!&lt;/p&gt; &lt;p&gt;Thanks in advance for the help!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vdnn1902"&gt; /u/vdnn1902 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfd61y4gnffg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmanai/mbp_2019_i9_64gb_ram_hitting_800_cpu_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmanai/mbp_2019_i9_64gb_ram_hitting_800_cpu_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T05:43:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmap5e</id>
    <title>Best &lt;4B dense models today?</title>
    <updated>2026-01-25T05:46:01+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think small(&amp;lt;4B) dense models are basically the only practical option for general users. But hasn't there been almost no progress since Gemma 3 4B came out? Are there any alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T05:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm33j7</id>
    <title>Is it possible: Qwen3 TTS voice cloning + style instruction? (voice description)</title>
    <updated>2026-01-24T23:53:11+00:00</updated>
    <author>
      <name>/u/Riptyzer</name>
      <uri>https://old.reddit.com/user/Riptyzer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From what I can see, style instruction / voice description only exists for the already available voices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Riptyzer"&gt; /u/Riptyzer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm33j7/is_it_possible_qwen3_tts_voice_cloning_style/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm33j7/is_it_possible_qwen3_tts_voice_cloning_style/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm33j7/is_it_possible_qwen3_tts_voice_cloning_style/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T23:53:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm3bnf</id>
    <title>So I am still playing around with my Jetsons and tonight I got a 14B model running on them combined.</title>
    <updated>2026-01-25T00:02:30+00:00</updated>
    <author>
      <name>/u/Von_plaf</name>
      <uri>https://old.reddit.com/user/Von_plaf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3bnf/so_i_am_still_playing_around_with_my_jetsons_and/"&gt; &lt;img alt="So I am still playing around with my Jetsons and tonight I got a 14B model running on them combined." src="https://preview.redd.it/k904hw5tydfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9ed663685b3d34f5c63505879cb53e82732a950" title="So I am still playing around with my Jetsons and tonight I got a 14B model running on them combined." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soooo In the age of everything being **BEEP** expensive I have decided to keep playing around with the 3 Jetson Orin nano Supers that I have in my rack. &lt;/p&gt; &lt;p&gt;And just got Qwen2.5-coder-14B running on it some what stable.. and kinda slow but that goes with out saying. &lt;/p&gt; &lt;p&gt;llama.cpp and RPC server on two of the devices and talking to the host on the 1st. device.&lt;br /&gt; only getting about 3-4 tokens/sec. over a 2.5GbE network of all the devices, But that is to be expected.&lt;/p&gt; &lt;p&gt;I know I can get about 20-22 tokens/sec. on one jetson with a small model but the goal for me was the larger model today and after getting as much memory cleared from all 3 devices I finally got it working &lt;/p&gt; &lt;p&gt;Asked it to analyze some python code I wrote the other day, and it looks like its working and the optimizations of the code it suggested also looks to be working.&lt;/p&gt; &lt;p&gt;It slow but it works SO... NICE and WOOP!! ;)&lt;br /&gt; Also just felt like sharing it here even some might be doing more crazy stuff ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Von_plaf"&gt; /u/Von_plaf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k904hw5tydfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3bnf/so_i_am_still_playing_around_with_my_jetsons_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3bnf/so_i_am_still_playing_around_with_my_jetsons_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T00:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm6r2f</id>
    <title>Solving memory issues for LLMs</title>
    <updated>2026-01-25T02:32:47+00:00</updated>
    <author>
      <name>/u/RobotsMakingDubstep</name>
      <uri>https://old.reddit.com/user/RobotsMakingDubstep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, hope youâ€™re having a great weekend&lt;/p&gt; &lt;p&gt;Iâ€™m trying to run a 7B model on llama server and the problem is that after a while it starts hallucinating as original context isnâ€™t there anymore&lt;/p&gt; &lt;p&gt;I tried some tricks like using summarisation from a 3B model to keep context shortened but I wonâ€™t say itâ€™s working very well&lt;/p&gt; &lt;p&gt;Would love to hear how people here are managing maintaining context, long term memory and the whole holy grail issue of using LLMs locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotsMakingDubstep"&gt; /u/RobotsMakingDubstep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6r2f/solving_memory_issues_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6r2f/solving_memory_issues_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6r2f/solving_memory_issues_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T02:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlanzn</id>
    <title>GLM-4.7-Flash-REAP on RTX 5060 Ti 16 GB - 200k context window!</title>
    <updated>2026-01-24T02:26:28+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my latest local coding setup, the params are mostly based on &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash"&gt;Unsloth's recommendation for tool calling&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"&gt;unsloth/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repeat penalty: disabled&lt;/li&gt; &lt;li&gt;Temperature: 0.7&lt;/li&gt; &lt;li&gt;Top P: 1&lt;/li&gt; &lt;li&gt;Min P: 0.01&lt;/li&gt; &lt;li&gt;Standard Microcenter PC setup: RTX 5060 Ti 16 GB, 32 GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running this in LM Studio for my own convenience, but it can be run in any setup you have.&lt;/p&gt; &lt;p&gt;With 16k context, everything fit within the GPU, so the speed was impressive:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;965.16 tok/s&lt;/td&gt; &lt;td&gt;26.27 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The tool calls were mostly accurate and the generated code was good, but the context window was too little, so the model ran into looping issue after exceeding that. It kept making the same tool call again and again because the conversation history was truncated.&lt;/p&gt; &lt;p&gt;With 64k context, everything still fit, but the speed started to slow down.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;671.48 tok/s&lt;/td&gt; &lt;td&gt;8.84 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm pushing my luck to see if 100k context still fits. It doesn't! Hahaha. The CPU fan started to scream, RAM usage spiked up, GPU copy chart (in Task Manager) started to dance. Completely unusable.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;172.02 tok/s&lt;/td&gt; &lt;td&gt;0.51 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LM Studio just got the new &amp;quot;Force Model Expert Weight onto CPU&amp;quot; feature (basically llama.cpp's &lt;code&gt;--n-cpu-moe&lt;/code&gt;), and yeah, why not? this is also an MoE model, so let's enable that. Still with 100k context. And wow! only half of the GPU memory was used (7 GB), but with 90% RAM now (29 GB), seems like flash attention also got disabled. The speed was impressive.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;485.64 tok/s&lt;/td&gt; &lt;td&gt;8.98 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's push our luck again, this time, 200k context!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;324.84 tok/s&lt;/td&gt; &lt;td&gt;7.70 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What a crazy time. Almost very month we're getting beefier models that somehow fit on even crappier hardware. Just this week I was thinking of selling my 5060 for an old 3090, but that definitely unnecessary now!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Turned out with CPU MoE offload, I can just run the non-REAP model it self. Here's the speed for UD Q5_K_XL on my card, at 100k token window:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;206.07 tok/s&lt;/td&gt; &lt;td&gt;5.06 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;With more tweak, reducing GPU offload count (36/47), keep KV cache in GPU memory, disable nmap,... the speed increased.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;267.23 tok/s&lt;/td&gt; &lt;td&gt;6.23 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And yes, I was running this without Flash Attention the whole time, since LM Studio didn't support it this model (at the time of writing).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 2:&lt;/strong&gt; I decided to compile llama.cpp to get this running with FA, same UD Q5_K_XL model, it's now better!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;153.36 tok/s&lt;/td&gt; &lt;td&gt;11.49 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Update 3:&lt;/strong&gt; Alright, I think I'm gonna conclude the experiment here, llama.cpp is the way to go.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;423.77 tok/s&lt;/td&gt; &lt;td&gt;14.4 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here's the params to run:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llama-server \ --model ./GLM-4.7-Flash-UD-Q5_K_XL.gguf \ --alias &amp;quot;glm-4.7-flash-q5&amp;quot; --seed 1234 \ --temp 0.7 --top-p 1 --min-p 0.01 \ --ctx-size 102400 --jinja \ --threads 7 --fit on --cpu-moe \ --batch-size 768 --ubatch-size 768 &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T02:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlu6kh</id>
    <title>The mysterious price of Ada and and Ampere workstation GPUs</title>
    <updated>2026-01-24T18:10:28+00:00</updated>
    <author>
      <name>/u/insulaTropicalis</name>
      <uri>https://old.reddit.com/user/insulaTropicalis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's just something I can't wrap my head around.&lt;/p&gt; &lt;p&gt;An RTX Blackwell Pro 5000 has 48GB memory. Compute is less than an RTX 6000 Ada, but not so much less. If you use FP4 it is much more. QAT with 4-bit seems something that will become prevalent, so FP4 is a big deal. Memory bandwidth is 140% of Ada. Power draw is the same. PCIe is 5.0 vs 4.0.&lt;/p&gt; &lt;p&gt;Seems that Blackwell wins or ties in all important regards, and it costs &lt;em&gt;less&lt;/em&gt; than 6000 Ada. Even more bizzarre, RTX A6000 Ampere, which is inferior in every regard and very old, still costs as much as Pro 5000.&lt;/p&gt; &lt;p&gt;I understand that some people can have an Ada or Ampere multi-GPU set-up and wants to expend it or to change a broken one, but is it enough to explain this weird market? Do these sellers actually find buyers?&lt;/p&gt; &lt;p&gt;Even RTX 4090 costs more today than when I bought mine. Who buys at these prices? What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/insulaTropicalis"&gt; /u/insulaTropicalis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm82ao</id>
    <title>How to tell Claude Code about my local modelâ€™s context window size?</title>
    <updated>2026-01-25T03:33:55+00:00</updated>
    <author>
      <name>/u/eapache</name>
      <uri>https://old.reddit.com/user/eapache</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™ve got Claude Code running pointed against my local llama.cpp instance. But I can only run with about 64k of context locally before I run out of memory. Claude Code seems to assume a bigger context window than this, and doesnâ€™t trigger compaction at the right times, etc.&lt;/p&gt; &lt;p&gt;Is there any way to tell CC about the context window of the model itâ€™s using? Or maybe to trigger compaction on the server side without waiting for CC to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eapache"&gt; /u/eapache &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm82ao/how_to_tell_claude_code_about_my_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm82ao/how_to_tell_claude_code_about_my_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm82ao/how_to_tell_claude_code_about_my_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T03:33:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qljf7o</id>
    <title>AI &amp; ML Weekly â€” Hugging Face Highlights</title>
    <updated>2026-01-24T10:15:01+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the most notable &lt;strong&gt;AI models released or updated this week on Hugging Face&lt;/strong&gt;, categorized for easy scanning ðŸ‘‡&lt;/p&gt; &lt;h1&gt;Text &amp;amp; Reasoning Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7 (358B)&lt;/strong&gt; â€” Large-scale multilingual reasoning model &lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash (31B)&lt;/strong&gt; â€” Faster, optimized variant for text generation &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;https://huggingface.co/zai-org/GLM-4.7-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth GLM-4.7-Flash GGUF (30B)&lt;/strong&gt; â€” Quantized version for local inference &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiquidAI LFM 2.5 Thinking (1.2B)&lt;/strong&gt; â€” Lightweight reasoning-focused LLM &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alibaba DASD-4B-Thinking&lt;/strong&gt; â€” Compact thinking-style language model &lt;a href="https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking"&gt;https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Agent &amp;amp; Workflow Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Report (8B)&lt;/strong&gt; â€” Agent model optimized for report generation &lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;https://huggingface.co/openbmb/AgentCPM-Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Explore (4B)&lt;/strong&gt; â€” Exploration-focused agent reasoning model &lt;a href="https://huggingface.co/openbmb/AgentCPM-Explore"&gt;https://huggingface.co/openbmb/AgentCPM-Explore&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sweep Next Edit (1.5B)&lt;/strong&gt; â€” Code-editing and refactoring assistant &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"&gt;https://huggingface.co/sweepai/sweep-next-edit-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio: Speech, Voice &amp;amp; TTS&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VibeVoice-ASR (9B)&lt;/strong&gt; â€” High-quality automatic speech recognition &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PersonaPlex 7B&lt;/strong&gt; â€” Audio-to-audio personality-driven voice model &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 TTS (1.7B)&lt;/strong&gt; â€” Custom &amp;amp; base voice text-to-speech models &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pocket-TTS&lt;/strong&gt; â€” Lightweight open TTS model &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HeartMuLa OSS (3B)&lt;/strong&gt; â€” Text-to-audio generation model &lt;a href="https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B"&gt;https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Vision: Image, OCR &amp;amp; Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Step3-VL (10B)&lt;/strong&gt; â€” Vision-language multimodal model &lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;https://huggingface.co/stepfun-ai/Step3-VL-10B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LightOnOCR 2 (1B)&lt;/strong&gt; â€” OCR-focused vision-language model &lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;https://huggingface.co/lightonai/LightOnOCR-2-1B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TranslateGemma (4B / 12B / 27B)&lt;/strong&gt; â€” Multimodal translation models &lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;https://huggingface.co/google/translategemma-4b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-12b-it"&gt;https://huggingface.co/google/translategemma-12b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-27b-it"&gt;https://huggingface.co/google/translategemma-27b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MedGemma 1.5 (4B)&lt;/strong&gt; â€” Medical-focused multimodal model &lt;a href="https://huggingface.co/google/medgemma-1.5-4b-it"&gt;https://huggingface.co/google/medgemma-1.5-4b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image Generation &amp;amp; Editing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-Image&lt;/strong&gt; â€” Text-to-image generation model &lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;https://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FLUX.2 Klein (4B / 9B)&lt;/strong&gt; â€” High-quality image-to-image models &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-4B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-4B&lt;/a&gt; &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-9B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-9B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen Image Edit (LoRA / AIO)&lt;/strong&gt; â€” Advanced image editing &amp;amp; multi-angle edits &lt;a href="https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA"&gt;https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA&lt;/a&gt; &lt;a href="https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO"&gt;https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Z-Image-Turbo&lt;/strong&gt; â€” Fast text-to-image generation &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Video Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LTX-2&lt;/strong&gt; â€” Image-to-video generation model &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;https://huggingface.co/Lightricks/LTX-2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Any-to-Any / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chroma (6B)&lt;/strong&gt; â€” Any-to-any multimodal generation &lt;a href="https://huggingface.co/FlashLabs/Chroma-4B"&gt;https://huggingface.co/FlashLabs/Chroma-4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T10:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlw8vl</id>
    <title>Loki-v2-70B: Narrative/DM-focused fine-tune (600M+ token custom dataset)</title>
    <updated>2026-01-24T19:25:15+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello from Crucible Labs!&lt;/p&gt; &lt;p&gt;We just finished the 1-epoch fine-tune for Loki-v2-70B, based on Llama-3.3-70B-Instruct.&lt;/p&gt; &lt;p&gt;The goal with this project wasn't to make another &amp;quot;helpful assistant,&amp;quot; but to build a model specifically for long-form narrative, TTRPG-style Dungeon Mastering, and consistent roleplay.&lt;/p&gt; &lt;p&gt;Weâ€™ve spent around six months generating and curating a V2 version of our original Loki Dataset in what we believe is the largest custom-generated dataset for this specific niche:&lt;/p&gt; &lt;p&gt;Total Tokens: 600M+&lt;/p&gt; &lt;p&gt;Size: ~2.5 GB&lt;/p&gt; &lt;p&gt;Composition: 46k+ QA lines, 19k+ prose lines, and 12k+ lines focused on dark/high-stakes scenarios.&lt;/p&gt; &lt;p&gt;The model card has a very extensive guide on how to use the model and details on worlds and universes, so please make sure to read through it!&lt;/p&gt; &lt;p&gt;This is an independent project, so weâ€™re looking for genuine feedback on how it handles long-context narrative and whether the DM bias feels right to you.&lt;/p&gt; &lt;p&gt;L3.3-70B-Loki-V2.0:&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-GGUF"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EXL3: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-EXL3"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-EXL3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lower quants seem to have an issue with how we trained in 256 rank, so please be aware of this. Higher rank training=more affected by quantization, and there doesn't seem to be a way to alleviate this.&lt;/p&gt; &lt;p&gt;- The Crucible Labs Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qloeu4</id>
    <title>MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations</title>
    <updated>2026-01-24T14:29:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt; &lt;img alt="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" src="https://a.thumbs.redditmedia.com/J6s266XP1Z7JRBZht5FAQGe2o36oOfOTvuiPLWD9El8.jpg" title="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/minimax/minimax-m2-her"&gt;https://openrouter.ai/minimax/minimax-m2-her&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations. Designed to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f"&gt;https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/api-reference/text-chat"&gt;https://platform.minimax.io/docs/api-reference/text-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/guides/models-intro"&gt;https://platform.minimax.io/docs/guides/models-intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm4fbp</id>
    <title>Anyone planing to get AMD Gorgon Halo (495) when it drops?</title>
    <updated>2026-01-25T00:49:51+00:00</updated>
    <author>
      <name>/u/SpicyWangz</name>
      <uri>https://old.reddit.com/user/SpicyWangz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It looks like AMD will be releasing the successor to the AI Max 395+ fairly soon.&lt;/p&gt; &lt;p&gt;itâ€™s mostly an incremental improvement, but it will have slightly higher clock speeds as well as 8533MT RAM as opposed to the current 8000MT.&lt;/p&gt; &lt;p&gt;Iâ€™m curious how much of a difference this will make on tps. Are any of you planning to get it when it drops?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpicyWangz"&gt; /u/SpicyWangz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm4fbp/anyone_planing_to_get_amd_gorgon_halo_495_when_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm4fbp/anyone_planing_to_get_amd_gorgon_halo_495_when_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm4fbp/anyone_planing_to_get_amd_gorgon_halo_495_when_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T00:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwcoi</id>
    <title>My Strix Halo beholds itself but believes its in the cloud</title>
    <updated>2026-01-24T19:29:20+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"&gt; &lt;img alt="My Strix Halo beholds itself but believes its in the cloud" src="https://external-preview.redd.it/cnJ2N2V3eWxtY2ZnMQh20pvmbVT22ZdBE-sn9Tc8Ujs4oEH6LQUVqmOmu06-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bff3dbe0b1a561ec97bacc8d8a2a5c4290b4f772" title="My Strix Halo beholds itself but believes its in the cloud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This iPhone app sends photos to a VLM served by the Halo on the local network and gets the response back. &lt;/p&gt; &lt;p&gt;The singularity might require a new system promptâ€¦&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o88lli0mmcfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm48ux</id>
    <title>GLM 4.7 vs MiniMax-M2.1 vs DeepSeek 3.2 for coding?</title>
    <updated>2026-01-25T00:41:56+00:00</updated>
    <author>
      <name>/u/ghulamalchik</name>
      <uri>https://old.reddit.com/user/ghulamalchik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use Cline/Roo Code. I wonder what option is better for coding. I tried MiniMax M2.1 since it was free for a while as an offer and I was pleased but I wonder if the others are better before I buy anything.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghulamalchik"&gt; /u/ghulamalchik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm48ux/glm_47_vs_minimaxm21_vs_deepseek_32_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm48ux/glm_47_vs_minimaxm21_vs_deepseek_32_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm48ux/glm_47_vs_minimaxm21_vs_deepseek_32_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T00:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm3xxm</id>
    <title>Dual 3090s &amp; GLM-4.7-Flash: 1st prompt is great, then logic collapses. Is local AI worth the $5/day power bill?</title>
    <updated>2026-01-25T00:28:59+00:00</updated>
    <author>
      <name>/u/Merstin</name>
      <uri>https://old.reddit.com/user/Merstin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently upgraded my family's video cards, which gave me an excuse to inherit two RTX 3090s and build a dedicated local AI rig out of parts i had laying around. My goal was privacy, home automation integration, and getting into &amp;quot;vibe coding&amp;quot; (learning UE5, Home Assistant YAML, etc.).&lt;/p&gt; &lt;p&gt;I love the &lt;em&gt;idea&lt;/em&gt; of owning my data, but I'm hitting a wall on the practical value vs. cost.&lt;/p&gt; &lt;p&gt;The Hardware Cost&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rig: i7 14700K, 64GB DDR5, Dual RTX 3090s (limited to 300W each).&lt;/li&gt; &lt;li&gt;Power: My peak rate is ~$0.65/kWh. A few hours of tinkering burns ~2kW, meaning this rig could easily cost me **$5/day** in electricity if I use it heavily.&lt;/li&gt; &lt;li&gt;Comparison: For that price, I could subscribe to Claude Sonnet/GPT-4 and not worry about heat or setup.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running a Proxmox LXC with llama-server and Open WebUI.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: GLM-4.7-Flash-UD-Q8_K_XL.gguf (Unsloth build).&lt;/li&gt; &lt;li&gt;Performance: ~2,000 t/s prompt processing, ~80 t/s generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The problem is rapid degradation. I tested it with the standard &amp;quot;Make a Flappy Bird game&amp;quot; prompt.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Turn 1: Works great. Good code, minor issues.&lt;/li&gt; &lt;li&gt;Turn 2 (Fixing issues): The logic falls apart. It hangs, stops short, or hallucinates. Every subsequent prompt gets worse.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My Launch Command:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ExecStart=/opt/llama.cpp/build/bin/llama-server \ -m /opt/llama.cpp/models/GLM-4.7-Flash-UD-Q8_K_XL.gguf \ --temp 0.7 --top-p 1.0 --min-p 0.01 --repeat-penalty 1.0 \ -ngl 99 -c 65536 -t -1 --host 0.0.0.0 --port 8080 \ --parallel 1 --n-predict 4096 --flash-attn on --jinja --fit on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Am I doing something wrong with my parameters (is &lt;code&gt;repeat-penalty 1.0&lt;/code&gt; killing the logic?), or is this just the state of 30B local models right now?&lt;/p&gt; &lt;p&gt;Given my high power costs, the results I am seeing there is limited value in the llm for me outside of some perceived data / privacy control which i'm not super concerned with.&lt;/p&gt; &lt;p&gt;Is there a hybrid setup where I use Local AI for RAG/Docs and paid API for the final code generation and get best of both worlds or something i am missing? I like messing around and learning and just these past 2 weeks I've learned so much but its just been that. &lt;/p&gt; &lt;p&gt;I am about to just sell my system and figure out paid services and local tools, talk me out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Merstin"&gt; /u/Merstin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3xxm/dual_3090s_glm47flash_1st_prompt_is_great_then/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3xxm/dual_3090s_glm47flash_1st_prompt_is_great_then/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm3xxm/dual_3090s_glm47flash_1st_prompt_is_great_then/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T00:28:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm2q0c</id>
    <title>Claude Code, but locally</title>
    <updated>2026-01-24T23:37:23+00:00</updated>
    <author>
      <name>/u/Zealousideal-Egg-362</name>
      <uri>https://old.reddit.com/user/Zealousideal-Egg-362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm looking for advice if there is realistic replacement for anthropic's models. Looking to run claude code with models that ideally are snappier and wondering if it's possible at all to replicate the opus model on own hardware.&lt;/p&gt; &lt;p&gt;What annoys me the most is speed, especially when west coast wakes up (I'm in EU). I'd be happy to prompt more, but have model that's more responsive. Opus 4.5 i great, but the context switches totally kill my flow and I feel extremely tired in the end of the day.&lt;/p&gt; &lt;p&gt;Did some limited testing of different models via openrouter, but the landscape is extremely confusing. glm-4.7 seems like a nice coding model, but is there any practical realistic replacement for Opus 4.5?&lt;/p&gt; &lt;p&gt;Edit: Iâ€™m asking very clearly for directions how/what to replace Opus and getting ridiculously irrelevant advice â€¦&lt;/p&gt; &lt;p&gt;My budget is 5-7k&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Egg-362"&gt; /u/Zealousideal-Egg-362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T23:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlt3pw</id>
    <title>GLM 4.7 Flash uncensored - Balanced &amp; Aggressive variants (GGUF)</title>
    <updated>2026-01-24T17:30:56+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I made uncensored versions of the new GLM 4.7 Flash from Z.ai.&lt;/p&gt; &lt;p&gt;For those who don't know the model, it's 30B-A3B MoE, so only ~3B active params (will have fast inference!) and 200K context. Runs surprisingly well for what it is.&lt;/p&gt; &lt;p&gt;Two variants:&lt;/p&gt; &lt;p&gt;- Balanced - excellent for agentic coding stuff where you still want (uncensored) reliability&lt;/p&gt; &lt;p&gt;- Aggressive - great for every other uncensored topic&lt;/p&gt; &lt;p&gt;Quants available: FP16, Q8_0, Q6_K, Q4_K_M&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings from Z.ai:&lt;/p&gt; &lt;p&gt;- General: --temp 1.0 --top-p 0.95&lt;/p&gt; &lt;p&gt;- Agentic/tool use: --temp 0.7 --top-p 1.0&lt;/p&gt; &lt;p&gt;- Keep repeat penalty at 1.0 (or directly off)&lt;/p&gt; &lt;p&gt;- llama.cpp users: --min-p 0.01 and --jinja&lt;/p&gt; &lt;p&gt;Heads up, it currently doesn't play nice with Ollama (has some chat template issues). Works fine with llama.cpp, LM Studio, Jan, koboldcpp.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;Edit: P.S. For those looking for smaller models, I also did GPT-OSS 20B, MXFP4 - Lossless:&lt;br /&gt; - &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit2: To clarify, the aim of the abliteration versions I publish is that they are effectively lossless to their original (censored) counterparts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T17:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwibf</id>
    <title>What is the best general-purpose model to run locally on 24GB of VRAM in 2026?</title>
    <updated>2026-01-24T19:35:11+00:00</updated>
    <author>
      <name>/u/Paganator</name>
      <uri>https://old.reddit.com/user/Paganator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Gemma 3 27b since its release nine months ago, which is an eternity in the AI field. Has anything better been released since then that can run well on a single 3090ti?&lt;/p&gt; &lt;p&gt;I'm not looking to code, to create agents, or to roleplay; I just want a good model to chat with and get reasonably smart answers to questions. If it can view images, that's even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paganator"&gt; /u/Paganator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlnruw</id>
    <title>Personal experience with GLM 4.7 Flash Q6 (unsloth) + Roo Code + RTX 5090</title>
    <updated>2026-01-24T14:02:56+00:00</updated>
    <author>
      <name>/u/Septerium</name>
      <uri>https://old.reddit.com/user/Septerium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am much more interested in how folks experience quantized versions of new models than just looking at bar graphs, so here is my humble contribution. &lt;/p&gt; &lt;p&gt;I have been using GLM 4.7 Flash to perform a few refactoring tasks in some personal web projects and have been quite impressed by how well the model handles Roo Code without breaking apart. For this agentic tool specifically, it has been much more reliable and precise than GPT-OSS 120b, GLM 4.5 Air, or Devstral 24b.&lt;/p&gt; &lt;p&gt;Here's the llama.cpp command I used to squeeze UD-Q6_K_XL + 48k tokens of context in my RTX 5090 VRAM and get about 150 tok/s (tg):&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server --model downloaded_models/GLM-4.7-Flash-UD-Q6_K_XL.gguf --port 11433 --host &amp;quot;0.0.0.0&amp;quot; -fa on --ctx-size 48000 --temp 0.7 --top-p 1.0 --min-p 0.01 --jinja -ngl 99&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septerium"&gt; /u/Septerium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm6iho</id>
    <title>Stable-DiffCoder, a strong code diffusion LLM built on Seed-Coder</title>
    <updated>2026-01-25T02:22:05+00:00</updated>
    <author>
      <name>/u/rektide</name>
      <uri>https://old.reddit.com/user/rektide</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rektide"&gt; /u/rektide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bytedance-seed.github.io/Stable-DiffCoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T02:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlr3wj</id>
    <title>I built an open-source audiobook converter using Qwen3 TTS - converts PDFs/EPUBs to high-quality audiobooks with voice cloning support</title>
    <updated>2026-01-24T16:16:15+00:00</updated>
    <author>
      <name>/u/TheyCallMeDozer</name>
      <uri>https://old.reddit.com/user/TheyCallMeDozer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Turn any book into an audiobook with AI voice synthesis!&lt;/strong&gt; I just released an open-source tool that converts PDFs, EPUBs, DOCX, and TXT files into high-quality audiobooks using &lt;strong&gt;Qwen3 TTS&lt;/strong&gt; - the amazing open-source voice model that just went public.&lt;/p&gt; &lt;h2&gt;What it does:&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Converts any document format&lt;/strong&gt; (PDF, EPUB, DOCX, DOC, TXT) into audiobooks &lt;strong&gt;Two voice modes&lt;/strong&gt;: Pre-built speakers (Ryan, Serena, etc.) or clone any voice from a reference audio &lt;strong&gt;Always uses 1.7B model&lt;/strong&gt; for best quality &lt;strong&gt;Smart chunking&lt;/strong&gt; with sentence boundary detection &lt;strong&gt;Intelligent caching&lt;/strong&gt; to avoid re-processing &lt;strong&gt;Auto cleanup&lt;/strong&gt; of temporary files &lt;/p&gt; &lt;h2&gt;Key Features:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Custom Voice Mode&lt;/strong&gt;: Professional narrators optimized for audiobook reading&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Clone Mode&lt;/strong&gt;: Automatically transcribes reference audio and clones the voice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-format support&lt;/strong&gt;: Works with PDFs, EPUBs, Word docs, and plain text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequential processing&lt;/strong&gt;: Ensures chunks are combined in correct order&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress tracking&lt;/strong&gt;: Real-time updates with time estimates ## Quick Start: Install Qwen3 TTS (one-click install with Pinokio) Install Python dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; Place your books in &lt;code&gt;book_to_convert/&lt;/code&gt; folder Run: &lt;code&gt;python audiobook_converter.py&lt;/code&gt; Get your audiobook from &lt;code&gt;audiobooks/&lt;/code&gt; folder! ## Voice Cloning Example: &lt;code&gt;bash python audiobook_converter.py --voice-clone --voice-sample reference.wav &lt;/code&gt; The tool automatically transcribes your reference audio - no manual text input needed! ## Why I built this: I was frustrated with expensive audiobook services and wanted a free, open-source solution. Qwen3 TTS going open-source was perfect timing - the voice quality is incredible and it handles both generic speech and voice cloning really well. ## Performance:&lt;/li&gt; &lt;li&gt;Processing speed: ~4-5 minutes per chunk (1.7B model) it is a little slow im working on it&lt;/li&gt; &lt;li&gt;Quality: High-quality audio suitable for audiobooks&lt;/li&gt; &lt;li&gt;Output: MP3 format, configurable bitrate ## GitHub: ðŸ”— &lt;strong&gt;&lt;a href="https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter"&gt;https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;What do you think?&lt;/strong&gt; Have you tried Qwen3 TTS? What would you use this for?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyCallMeDozer"&gt; /u/TheyCallMeDozer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T16:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm0l2q</id>
    <title>I built a tool that learns your codebase's unwritten rules and conventions- no AI, just AST parsing</title>
    <updated>2026-01-24T22:11:05+00:00</updated>
    <author>
      <name>/u/Fluffy_Citron3547</name>
      <uri>https://old.reddit.com/user/Fluffy_Citron3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last six months teaching myself to orchestrate engineering codebases using AI agents. What I found is that the biggest bottleneck isnâ€™t intelligence itâ€™s the context window. Why have we not given agents the proper tooling to defeat this limitation? Agents constantly forget how I handle error structures or which specific components I use for the frontend. This forces mass auditing and refactoring, causing me to spend about 75% of my token budget on auditing versus writing.&lt;/p&gt; &lt;p&gt;That is why I built Drift. Drift is a first-in-class codebase intelligence tool that leverages semantic learning through AST parsing with Regex fallbacks. It scans your codebase and extracts 15 different categories with over 150 patterns. Everything is persisted and recallable via CLI or MCP in your IDE of choice.&lt;/p&gt; &lt;p&gt;What makes drift different?&lt;/p&gt; &lt;p&gt;Itâ€™s learning based not rule based. AI is capable of writing high quality code but the context limitation makes fitting conventions through a large code base extremely tedious and time consuming often leading to things silently failing or just straight up not working. &lt;/p&gt; &lt;p&gt;Drift_context is the real magic &lt;/p&gt; &lt;p&gt;Instead of an agent calling 10 tools and sytheneszing results it: &lt;/p&gt; &lt;p&gt;Takes intent &lt;/p&gt; &lt;p&gt;Takes focus area&lt;/p&gt; &lt;p&gt;Returned a curated package&lt;/p&gt; &lt;p&gt;This eliminates the audit loop, hallucination risk and gives the agent everything needed in one call.&lt;/p&gt; &lt;p&gt;Call graph analysis across 6 different languages&lt;/p&gt; &lt;p&gt;Not just â€œWhat functions existsâ€ but..&lt;/p&gt; &lt;p&gt;Drift_reachability_forward &amp;gt; What data can this code access? (Massive for helping with security)&lt;/p&gt; &lt;p&gt;Drift_reachability_inverse &amp;gt; Who can access this field? &lt;/p&gt; &lt;p&gt;Drift_impact_analysis &amp;gt; what breaks if I change this with scoring.&lt;/p&gt; &lt;p&gt;Security-audit-grade analysis available to you or your agent through MCP or CLI&lt;/p&gt; &lt;p&gt;The MCP has been built out with frontier capabilities ensuring context is preserved and is a true tool for your agents&lt;/p&gt; &lt;p&gt;Currently support TS, PY, Java, C#, PHP, GO :&lt;/p&gt; &lt;p&gt;withâ€¦&lt;/p&gt; &lt;p&gt;Tree sitter parsing&lt;/p&gt; &lt;p&gt;Regex fallback&lt;/p&gt; &lt;p&gt;Framework aware detection&lt;/p&gt; &lt;p&gt;All data persist into a local file (/.drift) and you have the ability to approve, deny and ignore certain components, functions and features you donâ€™t want the agent to be trained on.&lt;/p&gt; &lt;p&gt;check it out here: &lt;/p&gt; &lt;p&gt;IF you run into any edge cases or I donâ€™t support the framework your code base is currently running on open a git issue feature request and ive been banging them out quick&lt;/p&gt; &lt;p&gt;Thank you for all the upvotes and stars on the project it means so much!&lt;/p&gt; &lt;p&gt;check it out here: &lt;a href="https://github.com/dadbodgeoff/drift"&gt;https://github.com/dadbodgeoff/drift&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Citron3547"&gt; /u/Fluffy_Citron3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T22:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltwza</id>
    <title>Artificial Analysis: South Korea ðŸ‡°ðŸ‡· is now the clear #3 nation in AI â€” powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence.</title>
    <updated>2026-01-24T18:00:50+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt; &lt;img alt="Artificial Analysis: South Korea ðŸ‡°ðŸ‡· is now the clear #3 nation in AI â€” powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." src="https://preview.redd.it/66fd18ro6cfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f579cce389f709dbf297867095118be2027f04ea" title="Artificial Analysis: South Korea ðŸ‡°ðŸ‡· is now the clear #3 nation in AI â€” powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/2014786516153991339"&gt;https://x.com/ArtificialAnlys/status/2014786516153991339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A key driver of this momentum is the Korean National Sovereign AI Initiative, a government-backed, nationwide competition that incentivizes domestic model development through a multi-stage elimination process. The initiative shortlists national champions, with winners receiving direct government funding and guaranteed access to large-scale GPU capacity.&lt;/p&gt; &lt;p&gt;âž¤ In August 2025, five organizations were selected: Naver, SK Telecom, LG Group, Upstage, and NC AI&lt;/p&gt; &lt;p&gt;âž¤ In the most recent round announced last week, the field narrowed to three: LG, SK Telecom, and Upstage.&lt;/p&gt; &lt;p&gt;âž¤ A fourth finalist is expected to be selected in the coming months as the evaluation process continues&lt;/p&gt; &lt;p&gt;Generally, top Korean AI models tend to be open weights, and vary in size ranging from Motifâ€˜s 12.7B Thinking model to LGâ€™s 236B K-EXAONE. Other models, such as Korea Telecom (KT)â€™s Mi:dm K 2.5 Pro, are proprietary and developed with a focus on business integration with existing KT clients.&lt;/p&gt; &lt;p&gt;Overview of major releases:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;âž¤ LG | K-EXAONE -&lt;/strong&gt; The current leader in the Korean AI race and a shortlisted model in the Korean National Sovereign AI Initiative. K-EXAONE is a 236B open weights model and scores 32 on the Artificial Analysis Intelligence Index. K-EXAONE performs strongly across various intelligence evaluations from scientific reasoning, instruction following, to agentic coding. However, this model has high verbosity, using 100 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;âž¤ Upstage | Solar Open -&lt;/strong&gt; Another shortlisted model in the Korean National Sovereign AI Initiative. Solar Open is a 100B open-weights model and scores 21 on the Artificial Analysis Intelligence Index. Solar Open performs well in instruction following and has lower hallucination rate compared to peer Korean models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;âž¤ Naver | HyperCLOVA X SEED Think -&lt;/strong&gt; A 32B open weights reasoning model that scores 24 on the Artificial Analysis Intelligence Index. HyperCLOVA X SEED Think demonstrates strong performance on agentic tool-use workflows and scores highly in the Global MMLU Lite multilingual index for Korean, highlighting its potential usefulness in a primarily Korean language environment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;âž¤ Korea Telecom | Mi:dm K 2.5 Pro -&lt;/strong&gt; A proprietary reasoning model that scores 23 on the Artificial Analysis Intelligence Index. Mi:dm K 2.5 Pro sees strong performance in agentic tool-use. Mi:dm K 2.5 Pro currently has no publicly available endpoint. Instead, Korea Telecom primarily intends to package this model into product offerings and use this model to serve KTâ€™s clients&lt;/p&gt; &lt;p&gt;&lt;strong&gt;âž¤ Motif | Motif-2-12.7B -&lt;/strong&gt; A small open weights model that scores 24 on the Artificial Analysis Intelligence Index. Motif-2-12.7B performs well in long-context reasoning and knowledge, but is highly token intensive - using 120 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66fd18ro6cfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and itâ€™s a significant step forward for local speech synthesis. If youâ€™ve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAIâ€™s TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;Weâ€™ve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAIâ€™s TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ðŸ¤— Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;ðŸ“„ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;ðŸ’» Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Iâ€™m really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
