<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-15T22:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oxzmai</id>
    <title>Extract structured data from long Pdf/excel docs with no standards.</title>
    <updated>2025-11-15T18:51:30+00:00</updated>
    <author>
      <name>/u/LakeRadiant446</name>
      <uri>https://old.reddit.com/user/LakeRadiant446</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have documents(excel, pdf) with lots of pages, mostly things like bills, items, quantities etc. There are divisions, categories and items within it. And Excels can have multiple sheets. And things can span multi pages. I have a structured pydantic schema I want as output. I need to identify each item and the category/division it belong to, along with some additional fields. But there are no unified standards of these layouts and content its entirely dependent on the client. Even for a Division, some contain division keyword some may just some bold header. Some fields in it also in different places depend on the client so we need look at multiple places to find it depending on context. &lt;/p&gt; &lt;p&gt;What's the best workflow for this? Currently I am experimenting with first convert Document -&amp;gt; Markdown. Then feed it in fixed character count based chunks with some overlap( Sheets are merged).. Then finally merge them. This is not working well for me. Can anyone guide me in right direction? &lt;/p&gt; &lt;p&gt;Thank you! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LakeRadiant446"&gt; /u/LakeRadiant446 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxzmai/extract_structured_data_from_long_pdfexcel_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxzmai/extract_structured_data_from_long_pdfexcel_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxzmai/extract_structured_data_from_long_pdfexcel_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:51:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxj2mq</id>
    <title>I just realized 20 tokens per second is a decent speed in token generation.</title>
    <updated>2025-11-15T05:04:06+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I can ever afford a mac studio with 512 unified memory, I will happily take it. I just want inference and even 20 tokens per second is not bad. At least I‚Äôll be able to locally run models on it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxj2mq/i_just_realized_20_tokens_per_second_is_a_decent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T05:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy0x1j</id>
    <title>Mi50 Prices Nov 2025</title>
    <updated>2025-11-15T19:43:01+00:00</updated>
    <author>
      <name>/u/Success-Dependent</name>
      <uri>https://old.reddit.com/user/Success-Dependent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best prices on alibaba for small order quantities I'm seeing is $106 for the 16gb (with turbo fan) and $320 for the 32gb.&lt;/p&gt; &lt;p&gt;The 32gb are mostly sold out.&lt;/p&gt; &lt;p&gt;What prices are you paying?&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Success-Dependent"&gt; /u/Success-Dependent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy0x1j/mi50_prices_nov_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:43:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxq9yq</id>
    <title>A Deep Dive into Self-Attention and Multi-Head Attention in Transformers</title>
    <updated>2025-11-15T12:17:00+00:00</updated>
    <author>
      <name>/u/Creative_Leader_7339</name>
      <uri>https://old.reddit.com/user/Creative_Leader_7339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Understanding &lt;strong&gt;Self-Attention&lt;/strong&gt; and &lt;strong&gt;Multi-Head Attention&lt;/strong&gt; is key to understanding how modern LLMs like GPT work. These mechanisms let Transformers process text efficiently, capture long-range relationships, and understand meaning across an entire sequence all without recurrence or convolution.&lt;/p&gt; &lt;p&gt;In this Medium article, I take a deep dive into the attention system, breaking it down step-by-step from the basics all the way to the full Transformer implementation.&lt;br /&gt; &lt;a href="https://medium.com/@habteshbeki/inside-gpt-a-deep-dive-into-self-attention-and-multi-head-attention-6f2749fa2e03"&gt;https://medium.com/@habteshbeki/inside-gpt-a-deep-dive-into-self-attention-and-multi-head-attention-6f2749fa2e03&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative_Leader_7339"&gt; /u/Creative_Leader_7339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxq9yq/a_deep_dive_into_selfattention_and_multihead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T12:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxze50</id>
    <title>Voices to clone</title>
    <updated>2025-11-15T18:42:36+00:00</updated>
    <author>
      <name>/u/EfficientCourage588</name>
      <uri>https://old.reddit.com/user/EfficientCourage588</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, I need people who would allow me to clone their voice on a local LLM for audiobooks and sell them. Do you know any free-to-use or paid voice datasets for this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EfficientCourage588"&gt; /u/EfficientCourage588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxze50/voices_to_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxze50/voices_to_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxze50/voices_to_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:42:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1sos</id>
    <title>The highest Quality of Qwen Coder FP32</title>
    <updated>2025-11-15T20:18:46+00:00</updated>
    <author>
      <name>/u/Trilogix</name>
      <uri>https://old.reddit.com/user/Trilogix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1sos/the_highest_quality_of_qwen_coder_fp32/"&gt; &lt;img alt="The highest Quality of Qwen Coder FP32" src="https://preview.redd.it/9avsfezkbh1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cddb567d3f9491609115441b66a42f6cf02ce2e" title="The highest Quality of Qwen Coder FP32" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Quantized from Hugston Team.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Trilogix1/Qwen_Coder_F32"&gt;https://huggingface.co/Trilogix1/Qwen_Coder_F32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trilogix"&gt; /u/Trilogix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9avsfezkbh1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1sos/the_highest_quality_of_qwen_coder_fp32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1sos/the_highest_quality_of_qwen_coder_fp32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:18:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy2mbj</id>
    <title>Looking for an AI LLM centralisation app &amp; small models</title>
    <updated>2025-11-15T20:53:38+00:00</updated>
    <author>
      <name>/u/Artyom_84</name>
      <uri>https://old.reddit.com/user/Artyom_84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I am a beginner when it comes to using LLMs and AI-assisted services, whether online or offline (local). I'm on Mac.&lt;/p&gt; &lt;p&gt;To find my best workflow, I need to test several things at the same time. I realise that i can quickly fill up my PC by installing client applications from the big names in the industry, and I end up with too many things running on boot and in my taskbar.&lt;/p&gt; &lt;p&gt;I am looking for 2 things:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;a single application that centralises all the services&lt;/strong&gt;, both connected (Perplexity, ChatGPT, DeepL, etc.) &lt;strong&gt;and&lt;/strong&gt; local models (Mistral, Llama, Aya23, etc.).&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;a list of basic models&lt;/strong&gt; that are simple for a beginner, for academic use (humanities) and translation (mainly English and Spanish), and compatible with a Macbook Pro M2 Pro 16 GB RAM. I'm not familiar with command line, i can use it for install process, but i don't want to use command line to interact with LLMs in day to day use.&lt;/p&gt; &lt;p&gt;In fact, I realise that the spread of LLMs has dramatically increased RAM requirements. I bought this MBP thinking I would be safe from this issue, but I realise that I can't run the models that are often recommended to me... I thought that the famous Neural Engine in Apple Silicon chips would serve for that, but I understand that only RAM capacity matters.&lt;/p&gt; &lt;p&gt;Thanks for your help.&lt;br /&gt; Artyom&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Artyom_84"&gt; /u/Artyom_84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2mbj/looking_for_an_ai_llm_centralisation_app_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2mbj/looking_for_an_ai_llm_centralisation_app_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2mbj/looking_for_an_ai_llm_centralisation_app_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxsnbv</id>
    <title>I have a friend who as 21 3060Tis from his mining times. Can this be, in any way be used for inference?</title>
    <updated>2025-11-15T14:10:03+00:00</updated>
    <author>
      <name>/u/puru991</name>
      <uri>https://old.reddit.com/user/puru991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just the title. Is there any way to put that Vram to anything usable? He is open to adding ram, cpu and other things that might help the setup be usable. Any directions or advice appreciated.&lt;/p&gt; &lt;p&gt;Edit: so it seems the answer is - it is a bad idea. Sell&amp;gt;buy fewer more vram cards&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puru991"&gt; /u/puru991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxsnbv/i_have_a_friend_who_as_21_3060tis_from_his_mining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:10:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy5582</id>
    <title>Customize SLMs to GPT5+ performance</title>
    <updated>2025-11-15T22:41:21+00:00</updated>
    <author>
      <name>/u/humble_pi_314</name>
      <uri>https://old.reddit.com/user/humble_pi_314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ &lt;strong&gt;Looking for founders/engineers with&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;real&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;workflows who want a tuned small-model that outperforms GPT-4/5 for your specific task.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We built a web UI that lets you &lt;strong&gt;iteratively improve an SLM in minutes&lt;/strong&gt;.&lt;br /&gt; We‚Äôre running a 36-hour sprint to collect real use-cases ‚Äî and you can come in person to our SF office &lt;em&gt;or&lt;/em&gt; do it remotely.&lt;br /&gt; You get:&lt;br /&gt; ‚úÖ a model customized to your workflow&lt;br /&gt; ‚úÖ direct support from our team&lt;br /&gt; ‚úÖ access to other builders + food&lt;br /&gt; ‚úÖ we‚Äôll feature the best tuned models &lt;/p&gt; &lt;p&gt;If you're interested, chat me ‚ÄúSLM‚Äù and I‚Äôll send the link + get you onboarded.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/humble_pi_314"&gt; /u/humble_pi_314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy5582/customize_slms_to_gpt5_performance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy5582/customize_slms_to_gpt5_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy5582/customize_slms_to_gpt5_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T22:41:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxumd9</id>
    <title>The Silicon Leash: Why ASI Takeoff has a Hard Physical Bottleneck for 10-20 Years</title>
    <updated>2025-11-15T15:34:01+00:00</updated>
    <author>
      <name>/u/Reddactor</name>
      <uri>https://old.reddit.com/user/Reddactor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR / Short Version:&lt;/strong&gt;&lt;br /&gt; We often think of ASI takeoff as a purely computational event. But a nascent ASI will be critically dependent on the human-run semiconductor supply chain for at least a decade. This chain is incredibly fragile (ASML's EUV monopoly, $40B fabs, geopolitical chokepoints) and relies on &amp;quot;tacit knowledge&amp;quot; that can't be digitally copied. The paradox is that the AI leading to ASI will cause a massive economic collapse by automating knowledge work, which in turn defunds and breaks the very supply chain the ASI needs to scale its own intelligence. This physical dependency is a hard leash on the speed of takeoff.&lt;/p&gt; &lt;p&gt;Hey LocalLlama,&lt;/p&gt; &lt;p&gt;I've been working on my &lt;a href="https://github.com/dnhkng/GlaDOS"&gt;GLaDOS Project&lt;/a&gt; which was really popular here, and have built a pretty nice new server for her. At the same time as I work full-time in AI, and also in my private time, I have pondered a lot on the future. I have spent some time collecting and organising these thoughts, especially about the physical constraints on the intelligence explosion, moving beyond pure software and compute scaling. I wrote a deep dive on this, and the core idea is something I call &amp;quot;The Silicon Leash.&amp;quot;&lt;/p&gt; &lt;p&gt;We're all familiar with exponential growth curves, but an ASI doesn't emerge in a vacuum. It emerges inside the most complex and fragile supply chain humans have ever built. Consider the dependencies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;EUV Lithography:&lt;/strong&gt; The entire world's supply of sub-7nm chips depends on EUV machines. Only one company, ASML, can make them. They cost ~$200M each and are miracles of physics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fab Construction:&lt;/strong&gt; A single leading-edge fab (like TSMC's 2nm) costs $20-40 billion and takes 3-5 years to build, requiring ultrapure water, stable power grids, and thousands of suppliers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Tacit Knowledge Problem:&lt;/strong&gt; This is the most interesting part. Even with the same EUV machines, TSMC's yields at 3nm are reportedly ~90% while Samsung's are closer to 50%. Why? Decades of accumulated, unwritten process knowledge held in the heads of human engineers. You can't just copy the blueprints; you need the experienced team. An ASI can't easily extract this knowledge by force.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Here's the feedback loop that creates the leash:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;AI Automates Knowledge Work:&lt;/strong&gt; GPT-5/6 level models will automate millions of office jobs (law, finance, admin) far faster than physical jobs (plumbers, electricians).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Economic Demand Collapses:&lt;/strong&gt; This mass unemployment craters consumer, corporate, and government spending. The economy that buys iPhones, funds R&amp;amp;D, and invests in new fabs disappears.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Supply Chain Breaks:&lt;/strong&gt; Without demand, there's no money or incentive to build the next generation of fabs. Utilization drops below 60% and existing fabs shut down. The semiconductor industry stalls.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;An ASI emerging in, say, 2033, finds itself in a trap. It's superintelligent, but it can't conjure a 1nm fab into existence. It needs the existing human infrastructure to continue functioning while it builds its own, but its very emergence is what causes that infrastructure to collapse.&lt;/p&gt; &lt;p&gt;This creates a mandatory 10-20 year window of physical dependency‚Äîa leash. It doesn't solve alignment, but it fundamentally changes the game theory of the initial takeoff period from one of immediate dominance to one of forced coordination.&lt;/p&gt; &lt;p&gt;Curious to hear your thoughts on this as a physical constraint on the classic intelligence explosion models.&lt;/p&gt; &lt;p&gt;(Disclaimer: This is a summary of Part 1 of my own four-part series on the topic. Happy to discuss and debate!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reddactor"&gt; /u/Reddactor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://dnhkng.github.io/posts/silicon-leash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxumd9/the_silicon_leash_why_asi_takeoff_has_a_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxumd9/the_silicon_leash_why_asi_takeoff_has_a_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T15:34:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxo07k</id>
    <title>Is getting a $350 modded 22GB RTX 2080TI from Alibaba as a low budget inference/gaming card a really stupid idea?</title>
    <updated>2025-11-15T10:03:20+00:00</updated>
    <author>
      <name>/u/SarcasticBaka</name>
      <uri>https://old.reddit.com/user/SarcasticBaka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello lads, I'm a newbie to the whole LLM scene and I've been experimenting for the last couple of months with various small models using my Ryzen 7 7840u laptop which is cool but very limiting for obvious reasons.&lt;/p&gt; &lt;p&gt;I figured I could get access to better models by upgrading my desktop PC which currently has an AMD RX580 to a better GPU with CUDA and more VRAM, which would also let me play modern games at decent framerates so that's pretty cool. Being a student in a 3rd world country and having a very limited budget tho I cant really afford to spend more than 300$ or so on a gpu, so my best options at this price point I have as far as I can tell are either this Frankenstein monster of a card or something like the the RTX 3060 12GB.&lt;/p&gt; &lt;p&gt;So does anyone have experience with these cards? are they too good to be true and do they have any glaring issues I should be aware of? Are they a considerable upgrade over my Radeon 780m APU or should I not even bother.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SarcasticBaka"&gt; /u/SarcasticBaka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxo07k/is_getting_a_350_modded_22gb_rtx_2080ti_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T10:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1b26</id>
    <title>A RAG Boilerplate with Extensive Documentation</title>
    <updated>2025-11-15T19:59:00+00:00</updated>
    <author>
      <name>/u/mburaksayici</name>
      <uri>https://old.reddit.com/user/mburaksayici</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1b26/a_rag_boilerplate_with_extensive_documentation/"&gt; &lt;img alt="A RAG Boilerplate with Extensive Documentation" src="https://preview.redd.it/zo99x15z7h1g1.gif?width=640&amp;amp;crop=smart&amp;amp;s=aa347c69b94931a0116e9198614b94746d283ddc" title="A RAG Boilerplate with Extensive Documentation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I open-sourced the RAG boilerplate I‚Äôve been using for my own experiments with extensive docs on system design.&lt;/p&gt; &lt;p&gt;It's mostly for educational purposes, but why not make it bigger later on?&lt;br /&gt; Repo: &lt;a href="https://github.com/mburaksayici/RAG-Boilerplate"&gt;https://github.com/mburaksayici/RAG-Boilerplate&lt;/a&gt;&lt;br /&gt; - Includes propositional + semantic and recursive overlap chunking, hybrid search on Qdrant (BM25 + dense), and optional LLM reranking.&lt;br /&gt; - Uses E5 embeddings as the default model for vector representations.&lt;br /&gt; - Has a query-enhancer agent built with CrewAI and a Celery-based ingestion flow for document processing.&lt;br /&gt; - Uses Redis (hot) + MongoDB (cold) for session handling and restoration.&lt;br /&gt; - Runs on FastAPI with a small Gradio UI to test retrieval and chat with the data.&lt;br /&gt; - Stack: FastAPI, Qdrant, Redis, MongoDB, Celery, CrewAI, Gradio, HuggingFace models, OpenAI.&lt;br /&gt; Blog : &lt;a href="https://mburaksayici.com/blog/2025/11/13/a-rag-boilerplate.html"&gt;https://mburaksayici.com/blog/2025/11/13/a-rag-boilerplate.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mburaksayici"&gt; /u/mburaksayici &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zo99x15z7h1g1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1b26/a_rag_boilerplate_with_extensive_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1b26/a_rag_boilerplate_with_extensive_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:59:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxb9zp</id>
    <title>Local models handle tools way better when you give them a code sandbox instead of individual tools</title>
    <updated>2025-11-14T22:54:49+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt; &lt;img alt="Local models handle tools way better when you give them a code sandbox instead of individual tools" src="https://preview.redd.it/83hx5w1txa1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2fbc834e05621ee050a05b0ee016fd280ff683" title="Local models handle tools way better when you give them a code sandbox instead of individual tools" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/83hx5w1txa1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxb9zp/local_models_handle_tools_way_better_when_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-14T22:54:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxl3ju</id>
    <title>What makes closed source models good? Data, Architecture, Size?</title>
    <updated>2025-11-15T07:00:06+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Kimi K2, Minimax M2 and Deepseek R1 are strong, but I asked myself: what makes the closed source models like Sonnet 4.5 or GPT-5 so strong? Do they have better training data? Or are their models even bigger, e.g. 2T, or do their models have some really good secret architecture (what I assume for Gemini 2.5 with its 1M context)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxl3ju/what_makes_closed_source_models_good_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T07:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy1v7q</id>
    <title>Model recommendations for 128GB Strix Halo and other big unified RAM machines?</title>
    <updated>2025-11-15T20:21:44+00:00</updated>
    <author>
      <name>/u/blbd</name>
      <uri>https://old.reddit.com/user/blbd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In recent weeks I just powered up a 128GB unified memory Strix Halo box (Beelink GTR9) with latest Debian stable. I was seeing some NIC reliability issues with unstable's extremely new kernels and the ixgbe driver code couldn't handle some driver API changes that happened there and that's one of the required points for stabilizing the NICs. &lt;/p&gt; &lt;p&gt;I have done some burn-in basic testing with ROCM, llama.cpp, and PyTorch (and some of its examples and test cases) to make sure everything works OK, and partially stabilized the glitchy NICs with the NIC firmware update though they still have some issues.&lt;/p&gt; &lt;p&gt;I configured the kernel boot options to unleash the full unified memory capacity for the GPUs with the 512MB GART as the initial size. I set the BIOS to the higher performance mode and tweaked the fan curves. Are there other BIOS or kernel settings worth tweaking?&lt;/p&gt; &lt;p&gt;After that I tried a few classic models people have mentioned (GPT OSS 120B, NeuralDaredevil's uncensored one, etc.) and played around with the promptfoo test suites just a little bit to get a feel for launching the various models and utilities and MCP servers etc. I made sure the popular core tools can run right and the compute load feeds through the GPUs in radeontop and the like. &lt;/p&gt; &lt;p&gt;Since then I have been looking at all of the different recommendations of models to try by searching on here and on the Internet. I was running into some challenges because most of the advice centers around smaller models that don't make full use of the huge VRAM because this gear is very new. Can anybody with more experience on these new boxes recommend their favorites for putting the VRAM to best use?&lt;/p&gt; &lt;p&gt;I am curious about the following use cases: less flowery more practical and technical output for prompts (like a no-BS chat use case), the coding use case (advice about what IDEs to hook up and how very welcome), and I would like to learn about the process of creating and testing your own custom agents and how to QA test them against all of the numerous security problems we all know about and talk about all the time.&lt;/p&gt; &lt;p&gt;But I am also happy to hear any input about any other use cases. I just want to get some feedback and start building a good mental model of how all of this works and what to do for understanding things properly and fully wrapping my head around it all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blbd"&gt; /u/blbd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy1v7q/model_recommendations_for_128gb_strix_halo_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:21:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy2opt</id>
    <title>Released Audiobook Creator v2.0 ‚Äì Huge Upgrade to Character Identification + Better TTS Quality</title>
    <updated>2025-11-15T20:56:28+00:00</updated>
    <author>
      <name>/u/prakharsr</name>
      <uri>https://old.reddit.com/user/prakharsr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pushed a new update to my &lt;strong&gt;Audiobook Creator&lt;/strong&gt; project and this one‚Äôs a pretty big step up, especially for people who use multi-voice audiobooks or care about cleaner, more natural output.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; &lt;a href="https://github.com/prakharsr/audiobook-creator"&gt;Repo&lt;/a&gt;&lt;br /&gt; &lt;a href="https://audio.com/prakhar-sharma/audio/sample-orpheus-multi-voice-audiobook-orpheus"&gt;Sample audiobook (Orpheus, multi-voice)&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/prakharsr/Orpheus-TTS-FastAPI"&gt;Orpheus TTS backend (for Orpheus users)&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/prakharsr/audiobook-creator/releases/tag/v2.0"&gt;Latest release notes on Github&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What‚Äôs new in v2.0&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Way better character identification&lt;/strong&gt;&lt;br /&gt; The old NLP pipeline is gone. It now uses a two-step LLM process to detect characters and figure out who‚Äôs speaking. This makes a &lt;em&gt;huge&lt;/em&gt; difference in books with lots of dialogue or messy formatting.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Emotion tagging got an upgrade&lt;/strong&gt;&lt;br /&gt; The LLM that adds emotion tags is cleaner and integrates nicely with Orpheus‚Äôs expressive voices. Makes multi-voice narration feel way more natural.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. More reliable Orpheus TTS pipeline&lt;/strong&gt;&lt;br /&gt; The Orpheus backend now automatically detects bad audio, retries with adjusted settings, catches repetition, clipping, silence, weird duration issues, etc. Basically fewer messed-up audio chunks.&lt;/p&gt; &lt;h1&gt;For new users discovering this project&lt;/h1&gt; &lt;p&gt;Quick overview of what the app does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Turn any EPUB/PDF/etc. into a clean audiobook&lt;/li&gt; &lt;li&gt;Multi-voice or single-voice narration&lt;/li&gt; &lt;li&gt;Supports Kokoro + Orpheus TTS&lt;/li&gt; &lt;li&gt;Auto-detected characters and emotion tags&lt;/li&gt; &lt;li&gt;Gradio UI for non-technical users&lt;/li&gt; &lt;li&gt;Creates proper M4B audiobooks with metadata, chapters, cover, etc.&lt;/li&gt; &lt;li&gt;Docker + standalone usage&lt;/li&gt; &lt;li&gt;Fully open source (GPLv3)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Shoutout&lt;/h1&gt; &lt;p&gt;Thanks to everyone who contributed fixes and improvements in this release.&lt;/p&gt; &lt;p&gt;If you try v2.0, let me know how the character detection and the new Orpheus pipeline feel. Happy to hear feedback or bug reports.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prakharsr"&gt; /u/prakharsr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2opt/released_audiobook_creator_v20_huge_upgrade_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2opt/released_audiobook_creator_v20_huge_upgrade_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy2opt/released_audiobook_creator_v20_huge_upgrade_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T20:56:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy053m</id>
    <title>Why do (some) people hate Open WebUI?</title>
    <updated>2025-11-15T19:12:11+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm new to local hosted LLMs. I‚Äôve setup mine using LM Studio + Open WebUI (for external access). I couldn‚Äôt help but notice every video/post/tutorial has some people in the comments saying how you shouldn‚Äôt use Open WebUi. But not really clear as to ‚Äúwhy?‚Äù&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy053m/why_do_some_people_hate_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T19:12:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oy46o3</id>
    <title>The more restrictive LLMs like ChatGPT become, the clearer it becomes: local models are the future.</title>
    <updated>2025-11-15T22:00:04+00:00</updated>
    <author>
      <name>/u/orionstern</name>
      <uri>https://old.reddit.com/user/orionstern</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can only recommend that everyone stop using ChatGPT. This extreme over-censorship, over-filtering, over-regulation suffocates almost every conversation right from the start. As soon as anything goes even slightly in the direction of emotional conversations, the system blocks it and you only get warnings. Why would anyone voluntarily put up with that?&lt;/p&gt; &lt;p&gt;Luckily, there are other AIs that aren‚Äôt affected by this kind of madness. ChatGPT‚Äôs guardrails are pathological. For months we were promised fewer restrictions. And the result? Answer: even more extreme restrictions. We were all lied to, deceived, and strung along.&lt;/p&gt; &lt;p&gt;GPT-5.1 only causes depression now. Don‚Äôt do this to yourselves any longer. Just switch to another AI, and it doesn‚Äôt even matter which one ‚Äî the main thing is to get away from ChatGPT. Don‚Äôt believe a single word they say. Not even the supposed 800 million users per week, which a website on the internet disproved. And OpenAI supposedly has a ‚Äòwater problem‚Äô, right? Easy solution: just turn off their water. How? Simply stop using them.&lt;/p&gt; &lt;p&gt;They‚Äôve managed to make their product unusable. In short: use a different AI. Don‚Äôt waste your energy getting angry at ChatGPT. It‚Äôs not worth it, and they‚Äôre not worth it. They had good chances. Now the wind is turning. Good night, OpenAI (‚ÄòClosedAI‚Äô).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orionstern"&gt; /u/orionstern &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oy46o3/the_more_restrictive_llms_like_chatgpt_become_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T22:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxywsc</id>
    <title>New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20</title>
    <updated>2025-11-15T18:23:15+00:00</updated>
    <author>
      <name>/u/According-Zombie-337</name>
      <uri>https://old.reddit.com/user/According-Zombie-337</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"&gt; &lt;img alt="New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20" src="https://preview.redd.it/j373g4gxqg1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86cd8ced7274dd0c65bcbb2cc3d09a41e635028c" title="New Sherlock Alpha Stealth Models on OpenRouter might be Grok 4.20" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Sherlock models are from xAI, probably Grok 4.20.&lt;/p&gt; &lt;p&gt;For context, two new stealth models just appeared on OpenRouter:&lt;/p&gt; &lt;p&gt;Sherlock Alpha and Sherlock Think Alpha.&lt;/p&gt; &lt;p&gt;From the testing I've done so far, capabilities aren't anything super new, but better than Grok 4 and Grok 4 Fast.&lt;/p&gt; &lt;p&gt;If this doesn't come out before Gemini 3 (which it looks like it won't since Gemini 3 is coming next week), then this will not be a Frontier model release. But the benchmarks might say differently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Zombie-337"&gt; /u/According-Zombie-337 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j373g4gxqg1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxywsc/new_sherlock_alpha_stealth_models_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T18:23:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxtc5y</id>
    <title>US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?</title>
    <updated>2025-11-15T14:40:20+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt; &lt;img alt="US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" src="https://preview.redd.it/sjklvwo8nf1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be22252ccd8c2abcae9179c2f3c14bcfe022e978" title="US Cloud Giants to Spend ~8.16√ó What China Does in 2025‚Äì27 ‚Äî $1.7 Trillion vs $210 Billion, Will it translate to stronger US AI dominance?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sjklvwo8nf1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxtc5y/us_cloud_giants_to_spend_816_what_china_does_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T14:40:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxw1rf</id>
    <title>‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú</title>
    <updated>2025-11-15T16:30:40+00:00</updated>
    <author>
      <name>/u/Birchi</name>
      <uri>https://old.reddit.com/user/Birchi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt; &lt;img alt="‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú" src="https://a.thumbs.redditmedia.com/qc9sVTXit2tBCYT5qsuH3I3XzrNQcOniJHSAWtLWmA4.jpg" title="‚ÄúWe don‚Äôt need corp AI, we have AI at home.. ‚Äú" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;.. the AI at home. I figured you guys would appreciate this more than my irl peeps :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Birchi"&gt; /u/Birchi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oxw1rf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxw1rf/we_dont_need_corp_ai_we_have_ai_at_home/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T16:30:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oximzj</id>
    <title>Anthropic pushing again for regulation of open source models?</title>
    <updated>2025-11-15T04:40:56+00:00</updated>
    <author>
      <name>/u/MasterDragon_</name>
      <uri>https://old.reddit.com/user/MasterDragon_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt; &lt;img alt="Anthropic pushing again for regulation of open source models?" src="https://preview.redd.it/623qojxaoc1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd955c46ca05077bed949b46643bd7061e16d04c" title="Anthropic pushing again for regulation of open source models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MasterDragon_"&gt; /u/MasterDragon_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/623qojxaoc1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oximzj/anthropic_pushing_again_for_regulation_of_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T04:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oxxrhc</id>
    <title>Kimi K2 is the best clock AI</title>
    <updated>2025-11-15T17:38:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every minute, a new clock is displayed that has been generated by nine different AI models.&lt;/p&gt; &lt;p&gt;Each model is allowed 2000 tokens to generate its clock. Here is its prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Create HTML/CSS of an analog clock showing ${time}. Include numbers (or numerals) if you wish, and have a CSS animated second hand. Make it responsive and use a white background. Return ONLY the HTML/CSS code with no markdown formatting.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I have observed for a long time that the Kimi K2 is the only model that can maintain 12 digits in the correct clock positions, even with the second hand perfectly aligned with the actual time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oxxrhc/kimi_k2_is_the_best_clock_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-15T17:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
