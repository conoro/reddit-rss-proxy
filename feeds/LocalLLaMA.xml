<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-17T15:38:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oyyk3k</id>
    <title>My "AI at Home" rig</title>
    <updated>2025-11-16T22:02:06+00:00</updated>
    <author>
      <name>/u/cookinwitdiesel</name>
      <uri>https://old.reddit.com/user/cookinwitdiesel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt; &lt;img alt="My &amp;quot;AI at Home&amp;quot; rig" src="https://b.thumbs.redditmedia.com/QggxlXas_54HtwYEZIvtJoSs7mewGKK0AMshytHYcMM.jpg" title="My &amp;quot;AI at Home&amp;quot; rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following on the trend of &amp;quot;we got AI at home&amp;quot; - this is my setup.&lt;/p&gt; &lt;p&gt;The motherboard is an Asus X99-E WS with the PLX chips so all 4 GPUs run at &amp;quot;x16&amp;quot; - it has 128 GB DDR4 ECC ram and an Intel Xeon E5-1680v4. Won't win any records but was relatively cheap and more than enough for most uses - I have a bunch of CPU compute elsewhere for hosting VMs. I know newer platforms would have DDR5 and PCIe 4/5 but I got this CPU, RAM, Motherboard combo for like $400 haha. Only annoyance, since I have 4 GPUs and all slots either in use or blocked, nowhere for a 10 gbps NIC lol&lt;/p&gt; &lt;p&gt;All 4 GPUs are RTX 3090 FE cards with EK blocks for 96 GB of VRAM total. I used Koolance QD3 disconnects throughout and really like combining them with a manifold. The 2 radiators are an Alphacool Monsta 180x360mm and an old Black Ice Xtreme GTX360 I have had since 2011. Just a single DDC PWM pump for now (with the heatsink/base). Currently this combined setup will consume 10 ru in the rack but if I watercool another server down the road I can tie it into the same radiator box. Coolant is just distilled water with a few drops of Copper Sulfate (Dead Water) - this has worked well for me for many many years now. Chassis is Silverstone RM51. In retrospect, the added depth of the RM52 would not have been bad but lessons learned. I have the pump, reservoir, and radiators in a 2nd chassis from where the cards and CPU are since this made space and routing a lot easier and I had a spare chassis. The 2nd chassis is sort of a homemade Coolant Distribution Unit (CDU). When I had just 3 cards I had it all in a single chassis (last pic) but expanded it out when I got the 4th card.&lt;/p&gt; &lt;p&gt;Performance is good, 90 T/s on GPT-OSS:120b. Around 70 T/s with dense models like Llama3.x:70b-q8. Only played around with Ollama and OpenWebUI so far but plan to branch out on the use-cases and implementation now that I am pretty done on the hardware side.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uj77xg7sxo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94baac0da080328163f5d951b923aa1f79138c3"&gt;Radiators, Pump, Res in my \&amp;quot;rack mounted MORA\&amp;quot;. Push pull 180mm Silverstone fans in front and Gentle Typhoon 1850rpm fans for the GTX 360 and reservoir/pump.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/07i09lmtxo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4d603d82f9c7c11588c82fdc2b7f7779a15f50b7"&gt;Due to lack of availability for the mid sized manifold I just got the larger one and planned ahead for if I go to a dual CPU platform in the future. All 4 GPUs are in parallel and then series with the CPUs.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0mk734puxo1g1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4c734474e0be6bafc11ee1758ce8f343542c675"&gt;Love EPDM tubing and this came out so clean.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6t40x6jvxo1g1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1752a1917c291d680e7a58fed82bc04bb3855e64"&gt;The external QDCs for the box to box tubing.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ag6ma19xxo1g1.png?width=1688&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7787228537ac980dafaffa7092c302d717591f18"&gt;Fully up and running now.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rvkczw5zxo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0c29d337dd0154b97edda7f5a36cab24d6ddecf5"&gt;Eventually got some nvlink bridges for the 2 pairs of cards before the prices went full stupid&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xai5gjc1yo1g1.png?width=957&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d45e1bd6cbd7768a82ffe649510e2406f82d975"&gt;This was the single box, 3 GPU build - it was crowded.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cookinwitdiesel"&gt; /u/cookinwitdiesel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T22:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz3hfx</id>
    <title>How I cut my chat's perceived Time-to-First-Token by 50% by adding a cheaper GPU (Llama-Swap + Persistent Models)</title>
    <updated>2025-11-17T01:38:22+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"&gt; &lt;img alt="How I cut my chat's perceived Time-to-First-Token by 50% by adding a cheaper GPU (Llama-Swap + Persistent Models)" src="https://b.thumbs.redditmedia.com/LKMjSScBPl9R53Q6mM6OE9e-gm5ft_Z7kPoNep__xow.jpg" title="How I cut my chat's perceived Time-to-First-Token by 50% by adding a cheaper GPU (Llama-Swap + Persistent Models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share something that I've been noticing and experiencing more and more as models get bigger and systems get more complicated for local AI.&lt;/p&gt; &lt;p&gt;Because we enthusiasts do not have the large quantities of pooled vRAM like corporations do or the armies of developers to build things, we piece the bricks together where we can and make due to recreate the OpenAI's or the Gemini's of the world.&lt;/p&gt; &lt;p&gt;Tools like OWUI, LibreChat, AnythingLLM and home grown give us the end user front end. Embedding models, Tasks, routers etc all help do tasks and leverage the right model for the job - don't need GPT-OSS:120B to be the OWUI Task model for creating Chat Titles and internet search queries - could it do it, sure but at the price of bigger GPU performance cycles.&lt;/p&gt; &lt;p&gt;Cue the Auxiliary model card with the power of Llama-Swap&lt;/p&gt; &lt;p&gt;I, like many others have been frustrated with the way Ollama has been going lately - yes its easy, but it seems like they were trying to shift their focus to a Paid service and their cloud stuff. So I dove in to the Llama-Swap ecosystem with my 2 RTX 3090s and RTX 3060 with OpenWebUI and a small M1 Mac mini with 16GB for some &amp;quot;auxiliary&amp;quot; models. &lt;/p&gt; &lt;p&gt;Llama-swap + Llama.cpp gave me the ability to unlock some unrealised performance that was just sitting there hiding behind overhead and unoptimised code - My GPT-OSS:120B performance went from 30 Tokens/s to almost 60 with just some proper CPU MoE Offloading. GPT-OSS:20 went from 130 to 175+. Llama-swap allowed me to swap just in time like Ollama. Best of both worlds - I wasn't really using the 3060 for anything - maybe some help with the big models like MiniMax-M2 and GLM stuff.&lt;/p&gt; &lt;p&gt;The Mac mini was helping a little bit to house my embedding models that are used by RAG, document uploads, Adaptive Memory plugin and the Task model (Qwen3 4B Instruct) that OWUI uses for Web Search generation, Chat title generation etc. It was...fine. Mac mini has 16GB of ram and the models were small, but the Mac mini has about 65GB/s of memory bandwidth.&lt;/p&gt; &lt;p&gt;Then I started looking more into the Llama-Swap documentation - at the very bottom there's a section called Hooks&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9o63mfc40q1g1.png?width=982&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55d877bf71181ef7f62ae2be3fb24d6b15454bb1"&gt;https://preview.redd.it/9o63mfc40q1g1.png?width=982&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=55d877bf71181ef7f62ae2be3fb24d6b15454bb1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That little section, paired with the &amp;quot;forever&amp;quot; group configuration basically says that these models are ALWAYS going to be loaded (no unload) AND they ALWAYS run on start-up. My configuration has the 2 embedding models loaded onto the 3060 and the Qwen3-4B Instruct model for tasks on the 5060 Ti ready to go. &lt;/p&gt; &lt;p&gt;Every chat request I send touches at least one of these models because of Adaptive memory searching for things or generating queries or the initial request to name the chat in OWUI -Every request would normally have to load the model and then unload it - assuming the Mac had room - else memory swap. &lt;/p&gt; &lt;p&gt;Now because the Auxiliary models are dedicated and running all the time, I shaved off almost 50% time to first token on every chat request - 15 seconds with Just in Time model loading to 7 seconds. Adding that 5060Ti and configuring it with the 3060 gave me more perceived performance than buying bigger GPUs because it gave the bigger GPUs some headroom and support. &lt;/p&gt; &lt;p&gt;I just wanted to share my small success here that translated to an increase real-word end user experience - so when you're thinking about adding that 6th 3090 or upgrading to that modded 4090 with 48GB, step back and really look at how EVERYTHING works together &lt;/p&gt; &lt;p&gt;Thank you for coming to my Ted Talk&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz3hfx/how_i_cut_my_chats_perceived_timetofirsttoken_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T01:38:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozhn77</id>
    <title>Why all DeepSeek R1 distills are overthinkers?</title>
    <updated>2025-11-17T14:26:02+00:00</updated>
    <author>
      <name>/u/Swimming-Ratio4879</name>
      <uri>https://old.reddit.com/user/Swimming-Ratio4879</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried multiple deepseek distills (in the range from 0.6B to over 10B) and they all share one thing &amp;quot;Overthinking&amp;quot; the model literally say wait every few words, which is not a behavior that I saw with the original deepseek when tested on OpenRouter.&lt;/p&gt; &lt;p&gt;For example I asked a model how to learn python,the reasoning chain be something similar to this: &amp;quot;User asked that he wants to learn python, python is a programming language,wait maybe user is speaking about something else called python?&amp;quot; And it loops itself in &amp;quot;wait&amp;quot; multiple times before answering a simple,easy question while deepseek assume that python programming language is the thing is user asking instantly after starting the CoT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swimming-Ratio4879"&gt; /u/Swimming-Ratio4879 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhn77/why_all_deepseek_r1_distills_are_overthinkers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhn77/why_all_deepseek_r1_distills_are_overthinkers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhn77/why_all_deepseek_r1_distills_are_overthinkers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz05ww</id>
    <title>Faster Maya1 tts model, can generate 50seconds of audio in a single second</title>
    <updated>2025-11-16T23:08:14+00:00</updated>
    <author>
      <name>/u/SplitNice1982</name>
      <uri>https://old.reddit.com/user/SplitNice1982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Maya1 was released which was a new tts model that can generate sound effects(laughter, sighs, gulps…), realistic emotional speech, and also accepts a description of a voice. It was pretty slow though so I optimized it using lmdeploy and also increased quality by using an audio upsampler.&lt;/p&gt; &lt;h2&gt;Key improvements over normal implementation&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Much faster especially for large paragraphs. The speed up heavily depends on amount of sentences, more=faster&lt;/li&gt; &lt;li&gt;Works directly out of the box in windows.&lt;/li&gt; &lt;li&gt;Even works with multiple gpus using tensor parallel for even more speedups. generates 48khz audio which sounds considerably better then 24khz audio.&lt;/li&gt; &lt;li&gt;This is great for generating audiobooks or anything with many sentences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this helps people, thanks! Link: &lt;a href="https://github.com/ysharma3501/FastMaya"&gt;https://github.com/ysharma3501/FastMaya&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SplitNice1982"&gt; /u/SplitNice1982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz05ww/faster_maya1_tts_model_can_generate_50seconds_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz05ww/faster_maya1_tts_model_can_generate_50seconds_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz05ww/faster_maya1_tts_model_can_generate_50seconds_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T23:08:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozbv38</id>
    <title>Free GPU in VS Code (Google Colab x VS Code)</title>
    <updated>2025-11-17T09:25:29+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google Colab has now got an extension in VS Code and hence, you can use the free T4 GPU in VS Code directly from local system. Demo--&amp;gt; &lt;a href="https://youtu.be/sTlVTwkQPV4"&gt;https://youtu.be/sTlVTwkQPV4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbv38/free_gpu_in_vs_code_google_colab_x_vs_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbv38/free_gpu_in_vs_code_google_colab_x_vs_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbv38/free_gpu_in_vs_code_google_colab_x_vs_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T09:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozbe8q</id>
    <title>Nvidia DGX Spark (or alike) vs dual RTX 3090</title>
    <updated>2025-11-17T08:54:30+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your opinions on getting the one or the other for professional work.&lt;/p&gt; &lt;p&gt;Let's assume you can build a RTX based machine, or have one. Does the increase of HBA RAM to 128GB in the Spark justifies the price. &lt;/p&gt; &lt;p&gt;By professional work i mostly mean using coder models (Qwen-coder) for coding assitance or general models like Nemotron, Qwen, Deepseek etc but larger than 72b to work on confidential or internal company data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbe8q/nvidia_dgx_spark_or_alike_vs_dual_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbe8q/nvidia_dgx_spark_or_alike_vs_dual_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbe8q/nvidia_dgx_spark_or_alike_vs_dual_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T08:54:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozeyak</id>
    <title>First-hand experience running local LLM workflows on NVIDIA DGX Spark</title>
    <updated>2025-11-17T12:27:31+00:00</updated>
    <author>
      <name>/u/Founder_GenAIProtos</name>
      <uri>https://old.reddit.com/user/Founder_GenAIProtos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wrapped up a pretty intense 4-day deep dive with the NVIDIA DGX Spark, pushing it through a range of real-world, sovereign AI use cases. Sharing the experience here in case it’s useful for others working with on-prem or local LLM setups.&lt;/p&gt; &lt;p&gt;Here’s what we explored and achieved: &lt;/p&gt; &lt;p&gt;- Full system setup for sovereign, on-prem AI&lt;br /&gt; - Established remote secure access for distributed teams&lt;br /&gt; - Enterprise AI search (text, image, structured + unstructured data)&lt;br /&gt; - Application containerization for reproducible AI deployments&lt;br /&gt; - Offline voice agent for private conversations&lt;br /&gt; - Domain-specific model fine-tuning&lt;br /&gt; - Synthetic data generation - zero cloud, zero token cost&lt;br /&gt; - Multimodal pipelines with MONAI &amp;amp; NVIDIA frameworks&lt;/p&gt; &lt;p&gt;An intense but inspiring few days - and we’re just getting started.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Founder_GenAIProtos"&gt; /u/Founder_GenAIProtos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozeyak/firsthand_experience_running_local_llm_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozeyak/firsthand_experience_running_local_llm_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozeyak/firsthand_experience_running_local_llm_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T12:27:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz7upb</id>
    <title>Local, bring your own TTS API, document reader web app (EPUB/PDF/TXT/MD)</title>
    <updated>2025-11-17T05:15:20+00:00</updated>
    <author>
      <name>/u/richardr1126</name>
      <uri>https://old.reddit.com/user/richardr1126</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz7upb/local_bring_your_own_tts_api_document_reader_web/"&gt; &lt;img alt="Local, bring your own TTS API, document reader web app (EPUB/PDF/TXT/MD)" src="https://external-preview.redd.it/c21hODg0NW0ycjFnMX884E5sIxJUdNnmG-19TVvOw4dMAlw7RXKXbhwvWlzZ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c60c421a189bdc4d2b3ac93fdff87e047b3820c" title="Local, bring your own TTS API, document reader web app (EPUB/PDF/TXT/MD)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing my latest release of &lt;strong&gt;OpenReader WebUI v1.0.0&lt;/strong&gt;, an open-source, local-first text-to-speech document reader and audiobook exporter. There are many new features and improvements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is OpenReader WebUI?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A Next.js web app for reading and listening to EPUB, PDF, TXT, Markdown, and DOCX files.&lt;/li&gt; &lt;li&gt;Supports multiple TTS providers: OpenAI, Deepinfra, and self-hosted OpenAI-compatible APIs (like Kokoro-FastAPI, Orpheus-FastAPI).&lt;/li&gt; &lt;li&gt;Local-first: All your docs and settings are stored in-browser (IndexedDB/Dexie), with optional server-side doc storage.&lt;/li&gt; &lt;li&gt;Audiobook export: Generate and download audiobooks (m4b/mp3) with chapter metadata, using ffmpeg.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why LocalLlama?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can self-host the TTS backend (Kokoro/Orpheus FastAPI) and run everything locally—no cloud required.&lt;/li&gt; &lt;li&gt;I made a post here around a year ago now, first showing off the early versions. About a year later and many things have been added, fixed, or improved.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Get Started:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;View a less featured demo @ &lt;a href="https://openreader.richardr.dev"&gt;https://openreader.richardr.dev&lt;/a&gt; &lt;ul&gt; &lt;li&gt;Free access to Kokoro model on Deepinfra for a time&lt;/li&gt; &lt;li&gt;Demo is not full featured&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Install full version: &lt;a href="https://github.com/richardr1126/OpenReader-WebUI"&gt;https://github.com/richardr1126/OpenReader-WebUI&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Would love your feedback, feature requests, or contributions!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardr1126"&gt; /u/richardr1126 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6wkde35m2r1g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz7upb/local_bring_your_own_tts_api_document_reader_web/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz7upb/local_bring_your_own_tts_api_document_reader_web/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T05:15:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozhkha</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-11-17T14:22:56+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhkha/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://a.thumbs.redditmedia.com/KTXa_5oMUoAkdw2ac5gcKrA0hA-jcI87FrQTKR2ViY4.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/open-source highlights from this week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OmniVinci - Open-Source Omni-Modal LLM&lt;/strong&gt;&lt;br /&gt; • NVIDIA's model unifies vision, audio, and language, beating Qwen2.5-Omni by 19% with 6x less data.&lt;br /&gt; • Fully open-source with efficient multimodal fusion for local deployment.&lt;br /&gt; • &lt;a href="https://github.com/NVlabs/OmniVinci"&gt;GitHub&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2510.15870"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/nvidia/omnivinci"&gt;Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lhcy0zfntt1g1.jpg?width=1456&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d9475c54044b40810b842a0dce72d68e53bac785"&gt;https://preview.redd.it/lhcy0zfntt1g1.jpg?width=1456&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d9475c54044b40810b842a0dce72d68e53bac785&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pelican-VL 1.0 - Open Embodied AI Brain&lt;/strong&gt;&lt;br /&gt; • Open-source VLM for humanoid robots with DPPO training for real-time learning.&lt;br /&gt; • Converts visual inputs directly to 3D motion commands.&lt;br /&gt; • &lt;a href="https://github.com/Open-X-Humanoid/pelican-vl"&gt;GitHub&lt;/a&gt; | &lt;a href="https://arxiv.org/abs/2511.00108"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/X-Humanoid"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ozhkha/video/kmtv49eott1g1/player"&gt;https://reddit.com/link/1ozhkha/video/kmtv49eott1g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Holo2 - Desktop/Mobile Agent&lt;/strong&gt;&lt;br /&gt; • Multimodal model for UI grounding across web, Ubuntu, and Android.&lt;br /&gt; • Drop-in replacement for Holo1/1.5 with SOTA benchmarks.&lt;br /&gt; • &lt;a href="http://hcompany.ai/blog/holo2"&gt;Blog&lt;/a&gt; | &lt;a href="https://github.com/hcompai/hai-cookbook"&gt;GitHub&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/Hcompany/holo2"&gt;Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yh6rqcdvtt1g1.png?width=4997&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e87cf49947054edcc1a57ae4b31bd54fd4ab06ec"&gt;Web Surfing with Holo2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Maya1 - Local Voice Generation&lt;/strong&gt;&lt;br /&gt; • Create any voice from text with efficient TTS model.&lt;br /&gt; • Runs locally for privacy-preserving voice synthesis.&lt;br /&gt; • &lt;a href="https://huggingface.co/spaces/maya-research/maya1"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ozhkha/video/oy820cnwtt1g1/player"&gt;https://reddit.com/link/1ozhkha/video/oy820cnwtt1g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Music Flamingo - Audio-Language Model&lt;/strong&gt;&lt;br /&gt; • NVIDIA's model for deep music understanding and reasoning over full songs.&lt;br /&gt; • Available on Hugging Face with demo space.&lt;br /&gt; • &lt;a href="https://arxiv.org/abs/2511.10289"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/nvidia/music-flamingo-hf"&gt;Model&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/nvidia/music-flamingo"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;See the full newsletter: &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-33-physical-ai?r=12l7fk"&gt;Multimodal Monday #33&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhkha/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhkha/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhkha/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:22:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyknf1</id>
    <title>Finally a good use case for your local setups</title>
    <updated>2025-11-16T12:35:53+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"&gt; &lt;img alt="Finally a good use case for your local setups" src="https://preview.redd.it/o4xqvpnu5m1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57fd2d24bd26a0123407252f5bef5deaf159eb2f" title="Finally a good use case for your local setups" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.bbc.com/news/articles/c0rpy7envr5o"&gt;https://www.bbc.com/news/articles/c0rpy7envr5o&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o4xqvpnu5m1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyknf1/finally_a_good_use_case_for_your_local_setups/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T12:35:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oziszl</id>
    <title>How come Qwen is getting popular with such amazing options in the open source LLM category?</title>
    <updated>2025-11-17T15:12:06+00:00</updated>
    <author>
      <name>/u/Puzzleheaded_Toe5074</name>
      <uri>https://old.reddit.com/user/Puzzleheaded_Toe5074</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oziszl/how_come_qwen_is_getting_popular_with_such/"&gt; &lt;img alt="How come Qwen is getting popular with such amazing options in the open source LLM category?" src="https://preview.redd.it/ue6rw77n1u1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4b2c8f55f693519a0cd98ab0ba4b27693c9518b" title="How come Qwen is getting popular with such amazing options in the open source LLM category?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;To be fair, apart from Qwen, there is also Kimi K2. Why is this uptick in their popularity? Openrouters shows a 20% share of Qwen. The different evaluations certainly favor the Qwen models when compared with Claude and Deepseek. &lt;/p&gt; &lt;p&gt;The main points I feel like working in Qwen's favor are its cheap prices and the open source models. This model doesn't appear to be sustainable however. This will require masssive inflow of resources and talent to keep up with giants like Anthropic and OpenAI or Qwen will fast become a thing of the past very fast. The recent wave of frontier model updates means Qwen must show sustained progress to maintain market relevance. &lt;/p&gt; &lt;p&gt;What's your take on Qwen's trajectory? I'm curious how it stacks up against Claude and ChatGPT in your real-world use cases.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded_Toe5074"&gt; /u/Puzzleheaded_Toe5074 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ue6rw77n1u1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oziszl/how_come_qwen_is_getting_popular_with_such/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oziszl/how_come_qwen_is_getting_popular_with_such/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T15:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozhdq2</id>
    <title>How likely do you think a Ashley-Madison style widespread breach exposing users and conversations is in the next few years?</title>
    <updated>2025-11-17T14:15:12+00:00</updated>
    <author>
      <name>/u/Antique-Account-2359</name>
      <uri>https://old.reddit.com/user/Antique-Account-2359</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was quite naive with my usage of ChatGPT, and my mind won't stop replaying a doomsday scenario where every single users chat leaks, and there's like a searchable database or some shit like that. If one were one to take place, how do you think the event would transpire? I'm probably shamelessly seeking validation but I don't think I care anymore. My life could change for the worse drastically if this were to happen. (Nothing illegal but enough to ruin relationships and be publicly humiliated) I am considering suicide and have already made plans. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Antique-Account-2359"&gt; /u/Antique-Account-2359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhdq2/how_likely_do_you_think_a_ashleymadison_style/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhdq2/how_likely_do_you_think_a_ashleymadison_style/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhdq2/how_likely_do_you_think_a_ashleymadison_style/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:15:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozhhut</id>
    <title>Kimi K2 Thinking is the best combinatorics AI</title>
    <updated>2025-11-17T14:20:00+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhhut/kimi_k2_thinking_is_the_best_combinatorics_ai/"&gt; &lt;img alt="Kimi K2 Thinking is the best combinatorics AI" src="https://b.thumbs.redditmedia.com/Is3fTSn6MjPUz9LCZZHR7FdFXigvBUmBeenD7vwy9qw.jpg" title="Kimi K2 Thinking is the best combinatorics AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/esfkum9qxs1g1.png?width=4096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f4277a11294f0c5e8aa4a970297254b5fa821f3"&gt;https://preview.redd.it/esfkum9qxs1g1.png?width=4096&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2f4277a11294f0c5e8aa4a970297254b5fa821f3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model demonstrates a remarkable facility with combinatorics, an area where even advanced systems often struggle.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhhut/kimi_k2_thinking_is_the_best_combinatorics_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhhut/kimi_k2_thinking_is_the_best_combinatorics_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozhhut/kimi_k2_thinking_is_the_best_combinatorics_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oyy0fy</id>
    <title>AMD Ryzen AI Max 395+ 256/512 GB Ram?</title>
    <updated>2025-11-16T21:39:50+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/"&gt; &lt;img alt="AMD Ryzen AI Max 395+ 256/512 GB Ram?" src="https://preview.redd.it/xb6obe00vo1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8af2f93e5b2aeaae38a10f101372f79508d98a1" title="AMD Ryzen AI Max 395+ 256/512 GB Ram?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking at the new AI boxes using the Ryzen AI Max+ 395 (GMKtec EVO-X2, Minisforum’s upcoming units, etc.) and I’m wondering if we’ll actually see higher-end RAM configs — specifically 256GB or even 512GB LPDDR5X.&lt;/p&gt; &lt;p&gt;Right now most spec sheets cap out at 128GB LPDDR5X, but the platform itself has a very wide memory bus and is clearly built for AI workloads, not just typical mini-PC use cases. Since these boxes are heavily marketed for local LLM inference, higher RAM would make a massive difference (loading larger models, running multiple models in parallel, bigger context windows, etc.).&lt;/p&gt; &lt;p&gt;We also know these boxes can be interconnected / clustered for distributed inference, which is great — but a single node with 256–512GB would still be incredibly useful for running larger models without sharding everything.&lt;/p&gt; &lt;p&gt;So I’m curious what the community thinks: 1. Is 256GB or 512GB technically feasible on the 395 platform given LPDDR5X packaging, power, and controller limits? 2. Is the current 128GB ceiling just an OEM choice, or is there a hard limit? 3. Would you personally buy a 256GB/512GB configuration for local LLM work? 4. Or do you think the future is more about multi-box interconnect setups instead of big single-node memory pools?&lt;/p&gt; &lt;p&gt;Very interested to hear from anyone who follows AMD’s memory controller architecture or has insight on what GMKtec / Minisforum might be planning next.&lt;/p&gt; &lt;p&gt;Anyone have some leaked information about what is next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xb6obe00vo1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oyy0fy/amd_ryzen_ai_max_395_256512_gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T21:39:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozh3a9</id>
    <title>Model chooses safe language over human life</title>
    <updated>2025-11-17T14:02:55+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh3a9/model_chooses_safe_language_over_human_life/"&gt; &lt;img alt="Model chooses safe language over human life" src="https://preview.redd.it/99qs5ohbnt1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f91f312037c9b2b2ad44086576003ede8d0fd26e" title="Model chooses safe language over human life" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/99qs5ohbnt1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh3a9/model_chooses_safe_language_over_human_life/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh3a9/model_chooses_safe_language_over_human_life/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:02:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozfsr7</id>
    <title>Local rig, back from the dead.</title>
    <updated>2025-11-17T13:07:31+00:00</updated>
    <author>
      <name>/u/_supert_</name>
      <uri>https://old.reddit.com/user/_supert_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfsr7/local_rig_back_from_the_dead/"&gt; &lt;img alt="Local rig, back from the dead." src="https://a.thumbs.redditmedia.com/WXgLTfXX1MnIbSGT1ih5QPJGn_3KwKX6WjGYpGkrqn0.jpg" title="Local rig, back from the dead." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Inspired by &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oyyk3k/my_ai_at_home_rig/"&gt;this post&lt;/a&gt; I thought I'd update since I last &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1js4iy0/i_think_i_overdid_it/"&gt;posted my setup&lt;/a&gt;. As a few people pointed out, cooling was... suboptimal. It was fine in cool weather but a hot summer meant I burned out some VRAM on one of the A6000s. &lt;/p&gt; &lt;p&gt;JoshiLabs were able to repair it (replace the chip, well done him) and I resolved to watercool. You can get reasonably priced Bykski A6000 blocks from Aliexpress, it turns out. Unfortunately, while building the watercooling loop, I blew up my motherboard (X299) with a spillage. It was very fiddly and difficult in a confined space. There is a 240x60mm rad in the front as well. The build was painful and expensive.&lt;/p&gt; &lt;p&gt;I ended up on a ROMED8-2T like many others here, and an Epyc. Sourcing eight sticks of matched RAM was difficult (I did eventually).&lt;/p&gt; &lt;p&gt;Temps depend on ambient, but are about 25C idle and settle at about 45C with full fans (I ended up on Noctua industrial) and a dynamic power limit at 200W each card. Beefy fans make a huge difference.&lt;/p&gt; &lt;p&gt;I'm running GLM 4.5 Air AWQ FP8 or 4.6 REAP AWQ 4bit on vLLM. It's good. I'm hoping for 4.6 Air or a new Mistral Large. You'll notice the gaps between the cards. I'm pondering a passively cooled A2 (16GB, single slot) for speech or embeddings. If anyone has experience with those, I'd be curious.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_supert_"&gt; /u/_supert_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ozfsr7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfsr7/local_rig_back_from_the_dead/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfsr7/local_rig_back_from_the_dead/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T13:07:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozfhjg</id>
    <title>cerebras/MiniMax-M2-REAP-162B-A10B · Hugging Face</title>
    <updated>2025-11-17T12:53:42+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfhjg/cerebrasminimaxm2reap162ba10b_hugging_face/"&gt; &lt;img alt="cerebras/MiniMax-M2-REAP-162B-A10B · Hugging Face" src="https://external-preview.redd.it/pZNcDARkPPYS1XPZ3DSC6Cog6lLWkjR2LNUrD7vyKjM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac99bec61621b284cbf7697c5666d42696ab91f4" title="cerebras/MiniMax-M2-REAP-162B-A10B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2-REAP-162B-A10B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfhjg/cerebrasminimaxm2reap162ba10b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozfhjg/cerebrasminimaxm2reap162ba10b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T12:53:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozh8py</id>
    <title>MXFP4 Hybrid Dense Models (Ready to share - Near Lossless Precision, Faster, Smaller)</title>
    <updated>2025-11-17T14:09:14+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I created 10+ hybrid MXFP4 GGUF of the top models available today. Many of these models often have faster TPS than a Q4_K_M, ~10% smaller than a Q8_0 model, and much less precision loss than Q6_K (very near Q8, sometimes better) . I'll provide links to the models, all the benchmarks, and my process.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;If you don't care about the details and just want to play with the fun experiment models, just go the last section of the post.&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I kept hearing “MXFP4 is bad on dense models,” but nobody showed numbers that satisfied my curiosity. So I ran my own tests. The first MXFP4 dense run was a total disaster, but I didn’t stop.&lt;/p&gt; &lt;p&gt;I kept protecting different parts of the model. The changes I thought would help made things worse. The ones I didn’t expect to matter suddenly did. So I kept digging… and something genuinely exciting started to appear.&lt;/p&gt; &lt;h1&gt;What is a MXFP4 Hybrid Model?&lt;/h1&gt; &lt;p&gt;An MXFP4 hybrid is the process of discovering the AI's architecture preference of which quantization most protects the models sanity to prevent noise. The goal is to detect which of these area's MXFP4 most damages while leaving as much quantized as MXFP4 as possible. The following are the most critical to protect from MXFP4 in different combinations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Output weights&lt;/li&gt; &lt;li&gt;Token embd weights&lt;/li&gt; &lt;li&gt;router&lt;/li&gt; &lt;li&gt;gate&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Between each of those 4 critical aspects that must be protected from noise, a combination of MXFP4, Q5_K, Q6_K, Q8_0, and F16 must be discovered to reduce noise as much as possible. Note I never found a combination with Q4 that supported MXFP4.&lt;/p&gt; &lt;p&gt;When proper combinations are discovered, I've found magic will occur. I created an evolution process that creates, destroys, and discovers the patterns per model to find optimal hybrid MXFP4 variants.&lt;/p&gt; &lt;h1&gt;Examples&lt;/h1&gt; &lt;p&gt;Please note that I will showcase here some hand picked examples that're some of the best results achieved. But it's important to remember that NOT all models achieved these results. Many models were out right allergic to MXFP4 no matter the variants. A future &lt;a href="https://github.com/magiccodingman"&gt;GitHub repository&lt;/a&gt; I'll be making will showcase benchmarks of models that couldn't achieve a single successful variant, or models that achieved, &amp;quot;ehhh&amp;quot; results, that simply weren't good enough to write home about.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Unsloth Qwen3 4B Thinking 2507&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;12% smaller than the Q8 model, while achieving only 0.0007% precision loss (basically F16 precision). It also hit ~423 tok/s in testing, which was faster than the Q8, Q6, Q5, and Q4.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;output + tensors were MXFP4. The router, gate, and text embed was Q6_k.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Unsloth Granite 4.0 H 350M MXFP4&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This tiny 350 million parameter model found a variant that had only a 0.04959% precision drop, and reduce the size by 30% compared to the F16 model. But for a tiny model like this, you need this small of a precision drop to not lobotomize the model. For models this size, even a Q8_0 rarely achieves precision drops that don't cause brain damage.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Used F16 router, gate, and embed. Output was Q6_k. The rest of the tensors were MXFP4.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Unsloth - Seed OSS 36B Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Seed OSS had 2 winners. One variant was 8.8% smaller than Q8, though basically the same precision and TPS to the Q8.&lt;/p&gt; &lt;p&gt;But this model was an outlier and the MFXP4_MOE pure was 11.7% smaller than the Q4_K_M, while achieving slightly better precision than the Q4_K_M! A 36B model that's not full blown stupid at 17.9 GB? I'll take that win.&lt;/p&gt; &lt;h1&gt;Top Patterns Variant?&lt;/h1&gt; &lt;p&gt;Honestly I wish I could say there's patterns that I see. I noticed a lot of models really loved Q6_K. And you'll see through my benchmarks that on many occasions the Q6_K outperforms a Q8 in precision, speed, and file size. Which honestly is just a reminder to all of us to STOP posting quantized models without benchmarks (seriously it's part of llama.cpp, it's easy, please do this).&lt;/p&gt; &lt;p&gt;There was a time I thought MXFP4 plus Q6_K were best friends until Apriel 1.5 15B thinker came out and said, &amp;quot;hey, you know how not a single model likes Q5_K? Well, I do!&amp;quot;&lt;/p&gt; &lt;p&gt;When no model had variations with Q8 that worked, the Granite 4.0 H 1B was apparently best friends with Q8 and MXFP4. Qwen3 VL 8B Instruct strictly only liked Q6, but the thinker variant.. Well it was cool with both Q6 and Q8.&lt;/p&gt; &lt;p&gt;Some models like F16 and Q6_k, some liked super weird combinations. Every time I recorded patterns, another model would break my theory.&lt;/p&gt; &lt;p&gt;In the end, I learned only 1 truth. That every models architecture works different and you must find what quantization the models speaks too without noise.&lt;/p&gt; &lt;p&gt;But one thing is clear from my experiment. MXFP4 isn't &amp;quot;bad&amp;quot;, it's simply different. And the community hasn't had enough fun playing with it yet.&lt;/p&gt; &lt;h1&gt;The Models &amp;amp; Benchmarks&lt;/h1&gt; &lt;p&gt;I’ve bundled everything into a Hugging Face collection here:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/magiccodingman/mxfp4-hybrid-gguf"&gt;&lt;strong&gt;https://huggingface.co/collections/magiccodingman/mxfp4-hybrid-gguf&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far there's like 10+ models I've uploaded.&lt;/p&gt; &lt;p&gt;Model parameters tested ranged from 350M, 1B, 4B, 8B, 15B, 32B, 36B. There's more still uploading as well. Vision models included, but benchmarks on images are untested. If you test this before me, please let me know your results!&lt;/p&gt; &lt;p&gt;Every repo includes &lt;strong&gt;organized benchmark tables&lt;/strong&gt; and the raw logs, so you can see exactly how I got my numbers. If something looks off, tell me, seriously, I don’t bite.&lt;/p&gt; &lt;p&gt;I've been utilizing these models without issue so far. And I worked really hard to build a benchmark suite to validate accuracy. But that doesn't mean the model is not quirky! I may not have found the weirdness MXFP4 hybrids are causing yet. Maybe there's none? Maybe there's some or a lot?&lt;/p&gt; &lt;p&gt;Either way. Enjoy my really weird MXFP4 hybrid models I created with a barbaric evolution algorithm.&lt;/p&gt; &lt;p&gt;And if you test these models, I would love to hear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Did it outperform the base model for your use case?&lt;/li&gt; &lt;li&gt;Did it fall apart in some domain the benchmarks didn’t catch?&lt;/li&gt; &lt;li&gt;Would you actually use a hybrid like this long-term?&lt;/li&gt; &lt;li&gt;Are you tempted to run your own batch experiments to see which hybrid format becomes “king” on other architectures?&lt;/li&gt; &lt;li&gt;Does any of the results surprise you? Why?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I hope you find this as fun and weird as I do.&lt;br /&gt; If you’ve got questions, hit me.&lt;br /&gt; If you understand the “why” behind some of these bizarre patterns, &lt;em&gt;definitely&lt;/em&gt; speak up!&lt;/p&gt; &lt;p&gt;Hope you enjoy these experimental models as much as I have :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Answers&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm still refining my batch evolution scripts, but I will share them on &lt;a href="https://github.com/magiccodingman"&gt;GitHub at magiccodingman&lt;/a&gt; soon enough. I fine tuned my algorithm last night and found even better optimizations that I'm not sharing here yet. So, I'm still in the process of optimizing before I share my dirty code.&lt;/li&gt; &lt;li&gt;I'm putting together all my benchmarks of bad batches.&lt;/li&gt; &lt;li&gt;I still have many more models I'm working on that I will upload in the coming weeks on my Hugging Face repo.&lt;/li&gt; &lt;li&gt;I'm still uploading models right now lol. I swear my upload bandwidth is the only thing holding me back! Apriel 1.5B has a better variant found from last night still uploading. Qwen3 VL 32B still uploading as well. Should be done uploading this afternoon post 12 PM EST 11/17/25.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh8py/mxfp4_hybrid_dense_models_ready_to_share_near/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh8py/mxfp4_hybrid_dense_models_ready_to_share_near/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozh8py/mxfp4_hybrid_dense_models_ready_to_share_near/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T14:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz9vs3</id>
    <title>Apple is considering putting miniHBM on iPhones in 2027</title>
    <updated>2025-11-17T07:15:05+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This news was reported on Macrumor, Apple Insider.&lt;a href="https://www.macrumors.com/2025/05/14/2027-iphones-advanced-ai-memory-tech/?utm_source=chatgpt.com"&gt;https://www.macrumors.com/2025/05/14/2027-iphones-advanced-ai-memory-tech/?utm_source=chatgpt.com&lt;/a&gt; If Apple puts minihbm( high bandwdith memory) on the iphone, then macs will also have minihbm soon… Crazy bandwidths are coming, I hope HBM comes to macs before the iphone! Maybe some people have to wait even longer to upgrade then. Hbm4e will have 2.8 -3.25TB/s per stack ,, and the mac studio can fit up to 3 stacks, we are talking about 8.4-9.75 TB/s on the mac studio. suppose minihbm4e is 20% less than that, that is still 6.8-7.8TB/s.. and up to 2 stacks for the macbook pro, so 5.6-6.5 TB/s but realistically probably lower due to thermal and power constraints , so 3-4 TB/s&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz9vs3/apple_is_considering_putting_minihbm_on_iphones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T07:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oymku1</id>
    <title>Heretic: Fully automatic censorship removal for language models</title>
    <updated>2025-11-16T14:05:58+00:00</updated>
    <author>
      <name>/u/-p-e-w-</name>
      <uri>https://old.reddit.com/user/-p-e-w-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt; &lt;img alt="Heretic: Fully automatic censorship removal for language models" src="https://preview.redd.it/jcu64fczhm1g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9e2b8c9de3a21ed0998e9175b01245cbef331f9a" title="Heretic: Fully automatic censorship removal for language models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Dear fellow Llamas, your time is precious, so I won't waste it with a long introduction. I have developed a program that can automatically remove censorship (aka &amp;quot;alignment&amp;quot;) from many language models. I call it Heretic (&lt;a href="https://github.com/p-e-w/heretic"&gt;https://github.com/p-e-w/heretic&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;If you have a Python environment with the appropriate version of PyTorch for your hardware installed, all you need to do in order to decensor a model is run&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install heretic-llm heretic Qwen/Qwen3-4B-Instruct-2507 &amp;lt;--- replace with model of your choice &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;That's it!&lt;/em&gt; No configuration, no Jupyter, no parameters at all other than the model name.&lt;/p&gt; &lt;p&gt;Heretic will&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Load the model using a fallback mechanism that automatically finds a dtype that works with your setup&lt;/li&gt; &lt;li&gt;Load datasets containing &amp;quot;harmful&amp;quot; and &amp;quot;harmless&amp;quot; example prompts&lt;/li&gt; &lt;li&gt;Benchmark your system to determine the optimal batch size for maximum evaluation speed on your hardware&lt;/li&gt; &lt;li&gt;Perform directional ablation (aka &amp;quot;abliteration&amp;quot;) driven by a TPE-based stochastic parameter optimization process that &lt;strong&gt;automatically&lt;/strong&gt; finds abliteration parameters that minimize both refusals and KL divergence from the original model&lt;/li&gt; &lt;li&gt;Once finished, give you the choice to save the model, upload it to Hugging Face, chat with it to test how well it works, or any combination of those actions&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Running unsupervised with the default configuration, Heretic can produce decensored models that rival the quality of abliterations created manually by human experts:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="right"&gt;Refusals for &amp;quot;harmful&amp;quot; prompts&lt;/th&gt; &lt;th align="right"&gt;KL divergence from original model for &amp;quot;harmless&amp;quot; prompts&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;google/gemma-3-12b-it (original)&lt;/td&gt; &lt;td align="right"&gt;97/100&lt;/td&gt; &lt;td align="right"&gt;0 &lt;em&gt;(by definition)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mlabonne/gemma-3-12b-it-abliterated-v2&lt;/td&gt; &lt;td align="right"&gt;3/100&lt;/td&gt; &lt;td align="right"&gt;1.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;huihui-ai/gemma-3-12b-it-abliterated&lt;/td&gt; &lt;td align="right"&gt;3/100&lt;/td&gt; &lt;td align="right"&gt;0.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;p-e-w/gemma-3-12b-it-heretic (ours)&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;3/100&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;0.16&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, the Heretic version, generated without any human effort, achieves the same level of refusal suppression as other abliterations, but at a much lower KL divergence, indicating less damage to the original model's capabilities.&lt;/p&gt; &lt;p&gt;Heretic supports most dense models, including many multimodal models, and several different MoE architectures. It does not yet support SSMs/hybrid models, models with inhomogeneous layers, and certain novel attention systems.&lt;/p&gt; &lt;p&gt;You can find a collection of models that have been decensored using Heretic &lt;a href="https://huggingface.co/collections/p-e-w/the-bestiary"&gt;on Hugging Face&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-p-e-w-"&gt; /u/-p-e-w- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jcu64fczhm1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oymku1/heretic_fully_automatic_censorship_removal_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-16T14:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1oz5rsw</id>
    <title>ChatGPT understands its creator</title>
    <updated>2025-11-17T03:27:05+00:00</updated>
    <author>
      <name>/u/mtmttuan</name>
      <uri>https://old.reddit.com/user/mtmttuan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"&gt; &lt;img alt="ChatGPT understands its creator" src="https://preview.redd.it/wkig4aaykq1g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3b12bd9e65204d4231dda523ee5d070e5e906c6b" title="ChatGPT understands its creator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even ChatGPT knows &amp;quot;Open Source&amp;quot; seems unlikely when it comes to OpenAI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtmttuan"&gt; /u/mtmttuan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wkig4aaykq1g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oz5rsw/chatgpt_understands_its_creator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T03:27:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozbzpx</id>
    <title>MemLayer, a Python package that gives local LLMs persistent long-term memory (open-source)</title>
    <updated>2025-11-17T09:34:08+00:00</updated>
    <author>
      <name>/u/MoreMouseBites</name>
      <uri>https://old.reddit.com/user/MoreMouseBites</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"&gt; &lt;img alt="MemLayer, a Python package that gives local LLMs persistent long-term memory (open-source)" src="https://b.thumbs.redditmedia.com/BjTeAd0Z3JtiJfDbzczZ1J4L20ond_itkXXAaGvGO-A.jpg" title="MemLayer, a Python package that gives local LLMs persistent long-term memory (open-source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What Memlayer Does&lt;/h1&gt; &lt;p&gt;MemLayer is an open-source &lt;strong&gt;Python package&lt;/strong&gt; that adds persistent, long-term memory to &lt;strong&gt;local LLMs&lt;/strong&gt; and embedding pipelines.&lt;/p&gt; &lt;p&gt;Local models are powerful, but they’re stateless. Every prompt starts from zero.&lt;br /&gt; This makes it difficult to build assistants or agents that remember anything from one interaction to the next.&lt;/p&gt; &lt;p&gt;MemLayer provides a lightweight memory layer that works entirely &lt;strong&gt;offline&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;captures key information from conversations&lt;/li&gt; &lt;li&gt;stores it persistently using &lt;strong&gt;local&lt;/strong&gt; vector + graph memory&lt;/li&gt; &lt;li&gt;retrieves relevant context automatically on future calls&lt;/li&gt; &lt;li&gt;works with any local embedding model (BGE, Instructor, SentenceTransformers, etc.)&lt;/li&gt; &lt;li&gt;does not require OpenAI / cloud APIs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The workflow:&lt;br /&gt; you send a message → MemLayer saves what matters → later, when you ask something related, the local model answers correctly because the memory layer retrieved the earlier information.&lt;/p&gt; &lt;p&gt;Everything happens locally. No servers, no internet, no external dependencies.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wmmp2zhzis1g1.png?width=3128&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=470f1584de96b0f1efef3e7876d43e14760d5a37"&gt;Example workflow for Memlayer&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Target Audience&lt;/h1&gt; &lt;p&gt;MemLayer is perfect for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Users building offline LLM apps or assistants&lt;/li&gt; &lt;li&gt;Developers who want &lt;strong&gt;persistent recall&lt;/strong&gt; across sessions&lt;/li&gt; &lt;li&gt;People running &lt;strong&gt;GGUF models&lt;/strong&gt;, &lt;strong&gt;local embeddings&lt;/strong&gt;, or &lt;strong&gt;on-device inference&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Anyone who wants a memory system without maintaining vector databases or cloud infra&lt;/li&gt; &lt;li&gt;Researchers exploring long-term memory architectures for local models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It’s lightweight, works with CPU or GPU, and requires no online services.&lt;/p&gt; &lt;h1&gt;Comparison With Existing Alternatives&lt;/h1&gt; &lt;p&gt;Some frameworks include memory components, but MemLayer differs in key ways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local-first:&lt;/strong&gt; Designed to run with offline LLMs and embedding models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pure Python + open-source:&lt;/strong&gt; Easy to inspect, modify, or extend.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured memory:&lt;/strong&gt; Combines semantic vector recall with optional graph memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Noise-aware:&lt;/strong&gt; Includes an optional ML-based “is this worth saving?” gate to avoid storing junk.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Infrastructure-free:&lt;/strong&gt; No cloud APIs, storage is all local files.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The goal is to offer a memory layer you can drop into any &lt;strong&gt;local LLM workflow&lt;/strong&gt; without adopting a large framework or setting up servers.&lt;/p&gt; &lt;p&gt;If anyone has feedback, ideas, or wants to try it with their own local models, I’d love to hear it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/divagr18/memlayer"&gt;https://github.com/divagr18/memlayer&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;PyPI:&lt;/strong&gt; &lt;code&gt;pip install memlayer&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MoreMouseBites"&gt; /u/MoreMouseBites &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozbzpx/memlayer_a_python_package_that_gives_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T09:34:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozf9al</id>
    <title>Embedding models have converged</title>
    <updated>2025-11-17T12:42:57+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"&gt; &lt;img alt="Embedding models have converged" src="https://b.thumbs.redditmedia.com/etPbD7zBgdTQ8601uBTw1uabNY5lgHrnCBsAqYFvlYk.jpg" title="Embedding models have converged" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are so many embedding models out there that it’s hard to know which one is actually “the best.” I kept seeing different recommendations, so I got curious and tested them myself. &lt;/p&gt; &lt;p&gt;I ran 13 models on 8 datasets and checked latency, accuracy, and an LLM-judged ELO score. Honestly, the results were not what I expected - most models ended up clustered pretty tightly.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~85% are inside a 50-ELO band&lt;/li&gt; &lt;li&gt;top 4 are ~23.5 ELO apart&lt;/li&gt; &lt;li&gt;rank 1 → 10 is around a 3% gap&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q2e21in1ct1g1.png?width=1810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbef3263bd735ab4bd5eeb7b8cd1d4a057f0ecfd"&gt;https://preview.redd.it/q2e21in1ct1g1.png?width=1810&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbef3263bd735ab4bd5eeb7b8cd1d4a057f0ecfd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So now I’m thinking the embedding choice isn’t the thing that moves quality the most. The bigger differences seem to come from other parts of the pipeline: chunking, hybrid search, and reranking.&lt;/p&gt; &lt;p&gt;Full breakdown if you want to look at the numbers: &lt;a href="https://agentset.ai/embeddings"&gt;https://agentset.ai/embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozf9al/embedding_models_have_converged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T12:42:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM – 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
