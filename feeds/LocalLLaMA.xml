<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-10T15:48:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ndatgs</id>
    <title>Why does qwen.ai show it's using Qwen3 max preview when it's replying to an image? And what model is it actually using?</title>
    <updated>2025-09-10T10:46:31+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So confusing. Same thing happened with Qwen3 max reasoning. I was using &amp;quot;reasoning&amp;quot; thinking I was using that one, when in reality it was using another model with reasoning?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndatgs/why_does_qwenai_show_its_using_qwen3_max_preview/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndatgs/why_does_qwenai_show_its_using_qwen3_max_preview/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndatgs/why_does_qwenai_show_its_using_qwen3_max_preview/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T10:46:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndegcm</id>
    <title>Is it ever a good idea to inference on CPU and DDR5</title>
    <updated>2025-09-10T13:38:13+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will first token take forever (without accounting for loading model into ram)? Lets say it's Qwen 3 Next 80b-A3B. That's 80GB ram at q4 kinda. Will I be getting 5t/s at least? What kinda CPU would I need? It doesn't scale much with CPU quality right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndegcm/is_it_ever_a_good_idea_to_inference_on_cpu_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndegcm/is_it_ever_a_good_idea_to_inference_on_cpu_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndegcm/is_it_ever_a_good_idea_to_inference_on_cpu_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T13:38:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf379</id>
    <title>What are your experiences with small VL models for local tasks?</title>
    <updated>2025-09-10T14:03:31+00:00</updated>
    <author>
      <name>/u/AnotherSoftEng</name>
      <uri>https://old.reddit.com/user/AnotherSoftEng</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m curious of what models people are using, and for what tasks. I’ve found a lot of success with Qwen2.5-VL 3B and 7B variants. It’s crazy how accurate these models are for their size.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnotherSoftEng"&gt; /u/AnotherSoftEng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf379/what_are_your_experiences_with_small_vl_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf379/what_are_your_experiences_with_small_vl_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf379/what_are_your_experiences_with_small_vl_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndacaw</id>
    <title>MiniPC N150 CPU benchmark Vulkan MoE models</title>
    <updated>2025-09-10T10:18:53+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with Llama.cpp and a few MoE models and wanted to see how they fair with my Intel minPC. Looks like Vulkan is working on latest llama.cpp prebuilt package.&lt;/p&gt; &lt;p&gt;System: MiniPC Kamrui E2 on Intel N150 &amp;quot;Alder Lake-N&amp;quot; CPU with 16GB of DDR4 3200 MT/s ram. Running Kubuntu 25.04 on Kernel 6.14.0-29-generic x86_64.&lt;/p&gt; &lt;p&gt;llama.cpp Vulkan version build: 4f63cd70 (6431)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_backend: loaded RPC backend from /home/user33/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Intel(R) Graphics (ADL-N) (Intel open-source Mesa driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 65536 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/user33/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /home/user33/build/bin/libggml-cpu-alderlake.so &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf&lt;/li&gt; &lt;li&gt;Phi-mini-MoE-instruct-IQ2_XS.gguf&lt;/li&gt; &lt;li&gt;Qwen3-4B-Instruct-2507-UD-IQ2_XXS.gguff&lt;/li&gt; &lt;li&gt;granite-3.1-3b-a800m-instruct_Q8_0.gguf&lt;/li&gt; &lt;li&gt;phi-2.Q6_K.gguf (not a MoE model)&lt;/li&gt; &lt;li&gt;SicariusSicariiStuff_Impish_LLAMA_4B-IQ3_XXS.gguf&lt;/li&gt; &lt;li&gt;gemma-3-270m-f32.gguf&lt;/li&gt; &lt;li&gt;Qwen3-4B-Instruct-2507-Q3_K_M.gguf&lt;/li&gt; &lt;/ol&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Dolphin3.0‑Llama3.1‑8B‑Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi‑mini‑MoE‑instruct‑IQ2_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;2.67 GiB&lt;/td&gt; &lt;td align="left"&gt;7.65 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;5.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑UD‑IQ2_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.16 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;3.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite‑3.1‑3b‑a800m‑instruct_Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;3.27 GiB&lt;/td&gt; &lt;td align="left"&gt;3.30 B&lt;/td&gt; &lt;td align="left"&gt;51.45&lt;/td&gt; &lt;td align="left"&gt;11.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi‑2.Q6_K.gguf&lt;/td&gt; &lt;td align="left"&gt;2.13 GiB&lt;/td&gt; &lt;td align="left"&gt;2.78 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;4.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SicariusSicariiStuff_Impish_LLAMA_4B‑IQ3_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.74 GiB&lt;/td&gt; &lt;td align="left"&gt;4.51 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;3.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma‑3‑270m‑f32.gguf&lt;/td&gt; &lt;td align="left"&gt;1022.71 MiB&lt;/td&gt; &lt;td align="left"&gt;268.10 M&lt;/td&gt; &lt;td align="left"&gt;566.64&lt;/td&gt; &lt;td align="left"&gt;17.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑Q3_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;1.93 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.22&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;sorted by tg128&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑Q3_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;1.93 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Dolphin3.0‑Llama3.1‑8B‑Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SicariusSicariiStuff_Impish_LLAMA_4B‑IQ3_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.74 GiB&lt;/td&gt; &lt;td align="left"&gt;4.51 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;3.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑UD‑IQ2_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.16 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;3.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi‑2.Q6_K.gguf&lt;/td&gt; &lt;td align="left"&gt;2.13 GiB&lt;/td&gt; &lt;td align="left"&gt;2.78 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;4.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi‑mini‑MoE‑instruct‑IQ2_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;2.67 GiB&lt;/td&gt; &lt;td align="left"&gt;7.65 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;5.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite‑3.1‑3b‑a800m‑instruct_Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;3.27 GiB&lt;/td&gt; &lt;td align="left"&gt;3.30 B&lt;/td&gt; &lt;td align="left"&gt;51.45&lt;/td&gt; &lt;td align="left"&gt;11.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma‑3‑270m‑f32.gguf&lt;/td&gt; &lt;td align="left"&gt;1022.71 MiB&lt;/td&gt; &lt;td align="left"&gt;268.10 M&lt;/td&gt; &lt;td align="left"&gt;566.64&lt;/td&gt; &lt;td align="left"&gt;17.10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;sorted by pp512&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model &lt;/th&gt; &lt;th align="left"&gt;size &lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma‑3‑270m‑f32.gguf &lt;/td&gt; &lt;td align="left"&gt;1022.71 MiB&lt;/td&gt; &lt;td align="left"&gt;268.10 M&lt;/td&gt; &lt;td align="left"&gt;566.64 &lt;/td&gt; &lt;td align="left"&gt;17.10 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite‑3.1‑3b‑a800m‑instruct_Q8_0.gguf &lt;/td&gt; &lt;td align="left"&gt;3.27 GiB &lt;/td&gt; &lt;td align="left"&gt;3.30 B &lt;/td&gt; &lt;td align="left"&gt;51.45 &lt;/td&gt; &lt;td align="left"&gt;11.85 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑UD‑IQ2_XXS.gguf &lt;/td&gt; &lt;td align="left"&gt;1.16 GiB &lt;/td&gt; &lt;td align="left"&gt;4.02 B &lt;/td&gt; &lt;td align="left"&gt;25.58 &lt;/td&gt; &lt;td align="left"&gt;3.59 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi‑mini‑MoE‑instruct‑IQ2_XS.gguf &lt;/td&gt; &lt;td align="left"&gt;2.67 GiB &lt;/td&gt; &lt;td align="left"&gt;7.65 B &lt;/td&gt; &lt;td align="left"&gt;25.58 &lt;/td&gt; &lt;td align="left"&gt;5.80 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Dolphin3.0‑Llama3.1‑8B‑Q4_K_M.gguf &lt;/td&gt; &lt;td align="left"&gt;4.58 GiB &lt;/td&gt; &lt;td align="left"&gt;8.03 B &lt;/td&gt; &lt;td align="left"&gt;25.57 &lt;/td&gt; &lt;td align="left"&gt;2.34 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SicariusSicariiStuff_Impish_LLAMA_4B‑IQ3_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.74 GiB&lt;/td&gt; &lt;td align="left"&gt;4.51 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;3.22 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi‑2.Q6_K.gguf &lt;/td&gt; &lt;td align="left"&gt;2.13 GiB &lt;/td&gt; &lt;td align="left"&gt;2.78 B &lt;/td&gt; &lt;td align="left"&gt;25.58 &lt;/td&gt; &lt;td align="left"&gt;4.81 &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑Q3_K_M.gguf &lt;/td&gt; &lt;td align="left"&gt;1.93 GiB &lt;/td&gt; &lt;td align="left"&gt;4.02 B &lt;/td&gt; &lt;td align="left"&gt;25.57 &lt;/td&gt; &lt;td align="left"&gt;2.22 &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;sorted by params&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Dolphin3.0‑Llama3.1‑8B‑Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi‑mini‑MoE‑instruct‑IQ2_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;2.67 GiB&lt;/td&gt; &lt;td align="left"&gt;7.65 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;5.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SicariusSicariiStuff_Impish_LLAMA_4B‑IQ3_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.74 GiB&lt;/td&gt; &lt;td align="left"&gt;4.51 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;3.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑UD‑IQ2_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.16 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;3.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑Q3_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;1.93 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite‑3.1‑3b‑a800m‑instruct_Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;3.27 GiB&lt;/td&gt; &lt;td align="left"&gt;3.30 B&lt;/td&gt; &lt;td align="left"&gt;51.45&lt;/td&gt; &lt;td align="left"&gt;11.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi‑2.Q6_K.gguf&lt;/td&gt; &lt;td align="left"&gt;2.13 GiB&lt;/td&gt; &lt;td align="left"&gt;2.78 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;4.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma‑3‑270m‑f32.gguf&lt;/td&gt; &lt;td align="left"&gt;1022.71 MiB&lt;/td&gt; &lt;td align="left"&gt;268.10 M&lt;/td&gt; &lt;td align="left"&gt;566.64&lt;/td&gt; &lt;td align="left"&gt;17.10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;sorted by size small to big&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma‑3‑270m‑f32.gguf&lt;/td&gt; &lt;td align="left"&gt;1022.71 MiB&lt;/td&gt; &lt;td align="left"&gt;268.10 M&lt;/td&gt; &lt;td align="left"&gt;566.64&lt;/td&gt; &lt;td align="left"&gt;17.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑UD‑IQ2_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.16 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;3.59&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SicariusSicariiStuff_Impish_LLAMA_4B‑IQ3_XXS.gguf&lt;/td&gt; &lt;td align="left"&gt;1.74 GiB&lt;/td&gt; &lt;td align="left"&gt;4.51 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;3.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3‑4B‑Instruct‑2507‑Q3_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;1.93 GiB&lt;/td&gt; &lt;td align="left"&gt;4.02 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phi‑2.Q6_K.gguf&lt;/td&gt; &lt;td align="left"&gt;2.13 GiB&lt;/td&gt; &lt;td align="left"&gt;2.78 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;4.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi‑mini‑MoE‑instruct‑IQ2_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;2.67 GiB&lt;/td&gt; &lt;td align="left"&gt;7.65 B&lt;/td&gt; &lt;td align="left"&gt;25.58&lt;/td&gt; &lt;td align="left"&gt;5.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite‑3.1‑3b‑a800m‑instruct_Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;3.27 GiB&lt;/td&gt; &lt;td align="left"&gt;3.30 B&lt;/td&gt; &lt;td align="left"&gt;51.45&lt;/td&gt; &lt;td align="left"&gt;11.85&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Dolphin3.0‑Llama3.1‑8B‑Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;td align="left"&gt;2.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;In less than 30 days Vulkan has started working for Intel N150 CPU here was my benchmark 25 days ago on CPU backend was recognized by Vulkan build:&lt;/p&gt; &lt;p&gt;Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf&lt;br /&gt; build: 1fe00296 (6182)&lt;/p&gt; &lt;p&gt;load_backend: loaded RPC backend from /home/user33/build/bin/libggml-rpc.so load_backend: loaded CPU backend from /home/user33/build/bin/libggml-cpu-alderlake.so&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;RPC&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;7.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;RPC&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;4.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;real 9m48.044s&lt;/p&gt; &lt;p&gt;Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf backend: Vulkan build: 4f63cd70 (6431)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;25.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;2.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;real 6m51.535s&lt;/p&gt; &lt;p&gt;Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf build: 4f63cd70 (6431) CPU only by using also improved&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -ngl 0 --model ~/Dolphin3.0-Llama3.1-8B-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;8.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8B Q4_K – Medium&lt;/td&gt; &lt;td align="left"&gt;4.58 GiB&lt;/td&gt; &lt;td align="left"&gt;8.03 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;4.10&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;pp512 jumped from 7 t/s to 25 t/s, but we did lose a little on tg128. So use Vulkan if you have a big input request, but don't use if you just need quick questions answered. (just add &lt;code&gt;-ngl 0&lt;/code&gt; )&lt;/p&gt; &lt;p&gt;Not bad for a sub $150 miniPC. MoE model bring lots of power and looks like latest Mesa adds Vulkan support for better pp512 speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndacaw/minipc_n150_cpu_benchmark_vulkan_moe_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndacaw/minipc_n150_cpu_benchmark_vulkan_moe_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndacaw/minipc_n150_cpu_benchmark_vulkan_moe_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T10:18:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nddfpp</id>
    <title>What are the current state of local AI for gaming?</title>
    <updated>2025-09-10T12:55:31+00:00</updated>
    <author>
      <name>/u/Rique_Belt</name>
      <uri>https://old.reddit.com/user/Rique_Belt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am studying on how to better use local LLMs and one of the uses that I am very excited about is using them as a gaming partner in a cooperative game.&lt;/p&gt; &lt;p&gt;One example that I've heard about is the V-Tuber neuro-sama, I don't watch their stream so I don't know at which extension Vedal uses his AI. Let's say that my end goal is be playing a dynamic game like Left 4 Dead, I know a LLM can't achieve such thing (as far as I am aware of) so I'm aiming to Civilization V a turn based game, I don't need them to be good, just wanted to ask &amp;quot;Why you've done that move?&amp;quot; or &amp;quot;Let's aim to a military victory, so focus on modern tank production.&amp;quot;.&lt;/p&gt; &lt;p&gt;So my question is: Is there local AIs that can play games as e.g. FPS, non turn based, cooperative, that has the same complexity of LLMs and can run on end-user hardware? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rique_Belt"&gt; /u/Rique_Belt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nddfpp/what_are_the_current_state_of_local_ai_for_gaming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nddfpp/what_are_the_current_state_of_local_ai_for_gaming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nddfpp/what_are_the_current_state_of_local_ai_for_gaming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T12:55:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd4m2h</id>
    <title>Progress.</title>
    <updated>2025-09-10T04:18:52+00:00</updated>
    <author>
      <name>/u/tarheelbandb</name>
      <uri>https://old.reddit.com/user/tarheelbandb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I attended GTC last year and I've legit been all in on AI. Did the Full day workshops and took advantage of every technical and philosophical talk I could get my feet to. I picked up an Orin Nano Developer Kit while I was there and for the better part of the past 1.5 years I've been getting a solid understanding of CV, SLMs (only 8gb😂) brainstorming with AI tools. I even introduced some productive workflows at work that save a few hours of work per week for my team. I recently started exploring agentic uses and subscribed to claude.ai. In 2 months went through ideation, planning to MVP on my first app. And because I'm old, the idea of renting something, especially @ hitting caps, runs me not well. I started playing around with aider and quickly found that the Orin Nano would not suffice. So I found an RTX 4080 Founders edition at a pretty good price on NewEgg I'm hopes I could replicate my experience with Claude. I've found that the 4080 is great with 14b models but for agentic stuff I quickly understood that I should probably get a MacBook Pro because of their unified memory is a better value than I'm not really keen on relearning MacOS but was willing to do it up until today. Today I came across this &lt;a href="https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395"&gt;https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395&lt;/a&gt; and now I am excited to run Qwen3-coder-30b-a3b-instruct when it arrives. I might even be able to resell my 4080. The last time I was this excited about tech was building RepRap Printers. &lt;/p&gt; &lt;p&gt;That's all. Thanks for reading. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarheelbandb"&gt; /u/tarheelbandb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd4m2h/progress/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd4m2h/progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd4m2h/progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T04:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd1tqf</id>
    <title>What do you use on 12GB vram?</title>
    <updated>2025-09-10T01:58:53+00:00</updated>
    <author>
      <name>/u/Educational_Wind_360</name>
      <uri>https://old.reddit.com/user/Educational_Wind_360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;SIZE&lt;/th&gt; &lt;th align="left"&gt;MODIFIED&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:latest&lt;/td&gt; &lt;td align="left"&gt;2.0 GB&lt;/td&gt; &lt;td align="left"&gt;2 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:14b&lt;/td&gt; &lt;td align="left"&gt;9.3 GB&lt;/td&gt; &lt;td align="left"&gt;4 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b&lt;/td&gt; &lt;td align="left"&gt;8.1 GB&lt;/td&gt; &lt;td align="left"&gt;6 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5-coder:14b&lt;/td&gt; &lt;td align="left"&gt;9.0 GB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5-coder:1.5b&lt;/td&gt; &lt;td align="left"&gt;986 MB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nomic-embed-text:latest&lt;/td&gt; &lt;td align="left"&gt;274 MB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Wind_360"&gt; /u/Educational_Wind_360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T01:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncprrq</id>
    <title>Apple adds matmul acceleration to A19 Pro GPU</title>
    <updated>2025-09-09T17:48:47+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This virtually guarantees that it's coming to M5.&lt;/p&gt; &lt;p&gt;Previous discussion and my comments: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FYI for those who don't know, Apple's GPUs do not have dedicated hardware matmul acceleration like Nvidia's Tensor Cores. That's why prompt processing is slower on Apple Silicon. &lt;/p&gt; &lt;p&gt;I'm personally holding out on investing in a high VRAM (expensive) Macbook until Apple adds hardware matmul to their GPUs. It doesn't &amp;quot;feel&amp;quot; worth it to spend $5k on a maxed out Macbook without matmul and get a suboptimal experience.&lt;/p&gt; &lt;p&gt;I'm guessing it's the M6 generation that will have this, though I'm hopeful that M5 will have it.&lt;/p&gt; &lt;p&gt;I'm imaging GPU matmul acceleration + 256GB VRAM M6 Max with 917 GB/S (LPDDR6 14,400 MT/s) in Q4 2027. Now that is a attainable true local LLM machine that can actually do very useful things.&lt;/p&gt; &lt;p&gt;What's sort of interesting is that we know Apple is designing their own internal inference (and maybe training) server chips. They could share designs between consumer SoCs and server inference chips.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd73p3</id>
    <title>I fine-tuned a small model so it could write blogs &amp; LinkedIn posts in my brand voice (instead of generic AI-speak)</title>
    <updated>2025-09-10T06:48:29+00:00</updated>
    <author>
      <name>/u/StrictSir8506</name>
      <uri>https://old.reddit.com/user/StrictSir8506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt; &lt;img alt="I fine-tuned a small model so it could write blogs &amp;amp; LinkedIn posts in my brand voice (instead of generic AI-speak)" src="https://b.thumbs.redditmedia.com/QeSWzy3LSW9EZJPkAJLZGlrfUbGvmKuVd9oR80TBvsY.jpg" title="I fine-tuned a small model so it could write blogs &amp;amp; LinkedIn posts in my brand voice (instead of generic AI-speak)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I fine-tuned Qwen with DPO to generate YouTube titles(on a smaller dataset) in &lt;em&gt;my&lt;/em&gt; style (instead of “AI-sounding fluff”)&lt;/p&gt; &lt;p&gt;Most AI-generated content feels the same: generic, safe, “AI-sounding.”&lt;br /&gt; But creators and brands care about voice — newsletters, LinkedIn posts, podcast titles, YouTube content. The way you say things is as important as what you say.&lt;/p&gt; &lt;p&gt;That’s the gap Direct Preference Optimization (DPO) fills- quite natural&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You show the model pairs of responses (one better, one worse).&lt;/li&gt; &lt;li&gt;It directly optimizes to favor the “better” ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to see if DPO approach could help fix one of my biggest frustrations: AI writing bad YouTube titles.&lt;br /&gt; Think: hypey, vague, or clickbaity. Stuff I’d never actually publish.&lt;/p&gt; &lt;p&gt;So I:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Started with Qwen2.5-0.5B-Instruct as a base.&lt;/li&gt; &lt;li&gt;Generated multiple candidate titles for ~100+ video ideas.&lt;/li&gt; &lt;li&gt;Labeled pairs (better vs worse) to build a preference dataset.&lt;/li&gt; &lt;li&gt;Fine-tuned the model with Hugging Face’s &lt;code&gt;trl&lt;/code&gt; library and DPO.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And when I tested 50 random video ideas in a blind A/B test, I preferred the DPO outputs 68% of the time. Not perfect, but significantly closer to my style.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ub5eszpjaaof1.png?width=1070&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d4c961808635e09010ebac03088b361be42753a"&gt;https://preview.redd.it/ub5eszpjaaof1.png?width=1070&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d4c961808635e09010ebac03088b361be42753a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This isn’t just about YouTube titles. The same process works for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Newsletter subject lines&lt;/li&gt; &lt;li&gt;LinkedIn posts&lt;/li&gt; &lt;li&gt;Customer support replies&lt;/li&gt; &lt;li&gt;Blog intros, podcast titles, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone else here experimented with finetuning for style/brand voice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StrictSir8506"&gt; /u/StrictSir8506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T06:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nddmhx</id>
    <title>Memory models for local LLMs</title>
    <updated>2025-09-10T13:03:24+00:00</updated>
    <author>
      <name>/u/marmotter</name>
      <uri>https://old.reddit.com/user/marmotter</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been struggling with adding persistent memory to the poor man's SillyTavern I am vibe coding. This project is just for fun and to learn. I have a 5090. I have attempted my own simple RAG solution with a local embedding model and ChromaDB, and I have tried to implement Graphiti + FalkorDB as a more advanced version of my simple RAG solution (to help manage entity relationships across time). I run Graphiti in the 'hot' path for my implementation. &lt;/p&gt; &lt;p&gt;When trying to use Graphiti, the problem I run into is that the local LLMs I use can't seem to handle the multiple LLM calls that services like Graphiti need for summarization, entity extraction and updates. I keep getting errors and malformed memories because the LLM gets confused in structuring the JSON correctly across all the calls that occur for each conversational turn, even if I use the structured formatting option within LMStudio. I've spent hours trying to tweak prompts to mitigate these problems without much success.&lt;/p&gt; &lt;p&gt;I suspect that the type of models I can run on a 5090 are just not smart enough to handle this, and that these memory frameworks (Graphiti, Letta, etc.) require frontier models to run effectively. Is that true? Has anyone been successful in implementing these services locally on LLMs of 24B or less? The LLMs I am using are more geared to conversation than coding, and that might also be a source of problems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marmotter"&gt; /u/marmotter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nddmhx/memory_models_for_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nddmhx/memory_models_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nddmhx/memory_models_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T13:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctqym</id>
    <title>128GB 5090 is a hoax</title>
    <updated>2025-09-09T20:13:10+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt; &lt;img alt="128GB 5090 is a hoax" src="https://external-preview.redd.it/Kqv12dp3DtBbcIZhBA6wJa268drjtRcQXIG-PJVjhow.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca291e118c6d9bf8638af6d8b64731f927fb4938" title="128GB 5090 is a hoax" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non-existent GDDR7X memory that was never on a road map let alone in experimental phase. (GDDR7 and HBM4e improvements are planned until late 2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/no-there-is-no-geforce-rtx-5090-with-128gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl0v1</id>
    <title>🤔</title>
    <updated>2025-09-09T14:50:44+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt; &lt;img alt="🤔" src="https://preview.redd.it/1x8wy1p0k5of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5abc658735fe1e769f852e16c92dad154d7fd44c" title="🤔" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1x8wy1p0k5of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgub</id>
    <title>Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted</title>
    <updated>2025-09-09T14:29:35+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt; &lt;img alt="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" src="https://external-preview.redd.it/6f6MRyALyD6CxjbdRAXgjWeul-9vmUyW8_mAvDGRbV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfbaeba49e889b967e95e8d5052e5b00621dec5d" title="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40771"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfttb</id>
    <title>New smol course on Hugging Face - Climb the leaderboard to win prizes.</title>
    <updated>2025-09-10T14:32:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"&gt; &lt;img alt="New smol course on Hugging Face - Climb the leaderboard to win prizes." src="https://preview.redd.it/26eruo46lcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ec83e94c4ebb6b90da2d9cafe108fafbcac73e5" title="New smol course on Hugging Face - Climb the leaderboard to win prizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;smol course v2 - a Direct Way to Learn Post-Training AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Finally dropped our FREE certified course that cuts through the fluff:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's distinctive about smol course compared to other AI courses (LLM course)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Minimal instructions, maximum impact&lt;/li&gt; &lt;li&gt;Bootstrap real projects from day one&lt;/li&gt; &lt;li&gt;Leaderboard-based assessment (competitive learning FTW)&lt;/li&gt; &lt;li&gt;Hands-off approach - points you to docs instead of hand-holding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What's specifically new in this version&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Student model submission leaderboard&lt;/li&gt; &lt;li&gt;PRIZES for top performers&lt;/li&gt; &lt;li&gt;Latest TRL &amp;amp; SmolLM3 content&lt;/li&gt; &lt;li&gt;Hub integration for training/eval via hf jobs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Chapters drop every few weeks.&lt;/p&gt; &lt;p&gt;👉 Start here: &lt;a href="https://huggingface.co/smol-course"&gt;https://huggingface.co/smol-course&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/26eruo46lcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndbzjx</id>
    <title>New to Local LLMs - what hardware traps to avoid?</title>
    <updated>2025-09-10T11:48:58+00:00</updated>
    <author>
      <name>/u/False-Disk-1329</name>
      <uri>https://old.reddit.com/user/False-Disk-1329</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've around a USD $7K budget; I was previously very confident to put together a PC (or buy a private new or used pre-built).&lt;/p&gt; &lt;p&gt;Browsing this sub, I've seen all manner of considerations I wouldn't have accounted for: timing/power and test stability, for example. I felt I had done my research, but I acknowledge I'll probably miss some nuances and make less optimal purchase decisions.&lt;/p&gt; &lt;p&gt;I'm looking to do integrated machine learning and LLM &amp;quot;fun&amp;quot; hobby work - could I get some guidance on common pitfalls? Any hardware recommendations? Any known, convenient pre-builts out there?&lt;/p&gt; &lt;p&gt;...I also have seen the cost-efficiency of cloud computing reported on here. While I believe this, I'd still prefer my own machine however deficient compared to investing that $7k in cloud tokens.&lt;/p&gt; &lt;p&gt;Thanks :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/False-Disk-1329"&gt; /u/False-Disk-1329 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndbzjx/new_to_local_llms_what_hardware_traps_to_avoid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndbzjx/new_to_local_llms_what_hardware_traps_to_avoid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndbzjx/new_to_local_llms_what_hardware_traps_to_avoid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T11:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndb3lv</id>
    <title>[UPDATE] API for extracting tables, markdown, json and fields from pdfs and images</title>
    <updated>2025-09-10T11:02:12+00:00</updated>
    <author>
      <name>/u/LostAmbassador6872</name>
      <uri>https://old.reddit.com/user/LostAmbassador6872</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndb3lv/update_api_for_extracting_tables_markdown_json/"&gt; &lt;img alt="[UPDATE] API for extracting tables, markdown, json and fields from pdfs and images" src="https://preview.redd.it/mtga3rt2kbof1.gif?width=640&amp;amp;crop=smart&amp;amp;s=8592ca6e9d5093002ab1875a74196a5812960c1b" title="[UPDATE] API for extracting tables, markdown, json and fields from pdfs and images" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously shared an open-source project for extracting structured data from documents. I’ve now hosted it as a free to use API.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Outputs: JSON, Markdown, CSV, tables, specific fields, schema etc&lt;/li&gt; &lt;li&gt;Inputs: PDFs, images, and other common document formats&lt;/li&gt; &lt;li&gt;Use cases: invoicing, receipts, contracts, reports, and more&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;API docs: &lt;a href="https://docstrange.nanonets.com/apidocs"&gt;https://docstrange.nanonets.com/apidocs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Original post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mepr38/docstrange_open_source_document_data_extractor/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LostAmbassador6872"&gt; /u/LostAmbassador6872 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mtga3rt2kbof1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndb3lv/update_api_for_extracting_tables_markdown_json/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndb3lv/update_api_for_extracting_tables_markdown_json/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T11:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf3rj</id>
    <title>I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)</title>
    <updated>2025-09-10T14:04:09+00:00</updated>
    <author>
      <name>/u/WouterGlorieux</name>
      <uri>https://old.reddit.com/user/WouterGlorieux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"&gt; &lt;img alt="I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)" src="https://preview.redd.it/7yajbqkmd6of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1e29abf739772b644d059323cbc4269b2391b68" title="I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been working on a project called Valyrian Games: a fully automated system where Large Language Models compete against each other in coding challenges. After running 50 tournaments, I’ve published the first results here:&lt;/p&gt; &lt;p&gt;👉 Leaderboard: &lt;a href="https://valyriantech.github.io/ValyrianGamesLeaderboard"&gt;https://valyriantech.github.io/ValyrianGamesLeaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;👉 Challenge data repo: &lt;a href="https://github.com/ValyrianTech/ValyrianGamesCodingChallenge"&gt;https://github.com/ValyrianTech/ValyrianGamesCodingChallenge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;Phase 1 doubles as qualification: each model must create its own coding challenge, then solve it multiple times to prove it’s fair. To do this, the LLM has access to an MCP server to execute Python code. The coding challenge can be anything, as long as the final answer is a single integer value (for easy verification).&lt;/p&gt; &lt;p&gt;Only models that pass this step qualify for tournaments.&lt;/p&gt; &lt;p&gt;Phase 2 is the tournament: qualified models solve each other’s challenges head-to-head. Results are scored (+1 correct, -1 wrong, +1 bonus for solving another's challenge, extra penalties if you fail your own challenge).&lt;/p&gt; &lt;p&gt;Ratings use Microsoft’s TrueSkill system, which accounts for uncertainty.&lt;/p&gt; &lt;p&gt;Some results so far:&lt;/p&gt; &lt;p&gt;I’ve tested 62 models, but only 18 qualified.&lt;/p&gt; &lt;p&gt;GPT-5-mini is currently #1, but the full GPT-5 actually failed qualification.&lt;/p&gt; &lt;p&gt;Some reasoning-optimized models literally “overthink” until they timeout.&lt;/p&gt; &lt;p&gt;Performance is multi-dimensional: correctness, speed, and cost all vary wildly.&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;This started as a testbed for workflows in my own project SERENDIPITY, which is built on a framework I also developed: &lt;a href="https://github.com/ValyrianTech/ValyrianSpellbook"&gt;https://github.com/ValyrianTech/ValyrianSpellbook&lt;/a&gt; . I wanted a benchmark that was open, automated, and dynamic, not just static test sets.&lt;/p&gt; &lt;p&gt;Reality check:&lt;/p&gt; &lt;p&gt;The whole system runs 100% automatically, but it’s expensive. API calls are costing me about $50/day, which is why I’ve paused after 50 tournaments. I’d love to keep it running continuously, but as a solo developer with no funding, that’s not sustainable. Right now, the only support I have is a referral link to RunPod (GPU hosting).&lt;/p&gt; &lt;p&gt;I’m sharing this because:&lt;/p&gt; &lt;p&gt;I think the results are interesting and worth discussing (especially which models failed qualification).&lt;/p&gt; &lt;p&gt;I’d love feedback from this community. Does this kind of benchmarking seem useful to you?&lt;/p&gt; &lt;p&gt;If there’s interest, maybe we can find ways to keep this running long-term.&lt;/p&gt; &lt;p&gt;For those who want to follow me: &lt;a href="https://linktr.ee/ValyrianTech"&gt;https://linktr.ee/ValyrianTech&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WouterGlorieux"&gt; /u/WouterGlorieux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7yajbqkmd6of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxx7</id>
    <title>Qwen vl</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"&gt; &lt;img alt="Qwen vl" src="https://preview.redd.it/il757v4emcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bd8c776140638bad168e112aeaf64c8186d548" title="Qwen vl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/il757v4emcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfmb0</id>
    <title>Qwen3-VL soon?</title>
    <updated>2025-09-10T14:23:47+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"&gt; &lt;img alt="Qwen3-VL soon?" src="https://external-preview.redd.it/WmIZZLYdo41uN4s96YqW_5HlL8MG-0LtKmnFoOx7RwY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fafac68575ffd3b34262cbfa9c59fc0dcef20103" title="Qwen3-VL soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40795"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndg4up</id>
    <title>My open-source project on different RAG techniques just hit 20K stars on GitHub</title>
    <updated>2025-09-10T14:43:55+00:00</updated>
    <author>
      <name>/u/Nir777</name>
      <uri>https://old.reddit.com/user/Nir777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here's what's inside:&lt;/p&gt; &lt;p&gt;- 35 detailed tutorials on different RAG techniques&lt;/p&gt; &lt;p&gt;- Tutorials organized by category &lt;/p&gt; &lt;p&gt;- Clear, high-quality explanations with diagrams and step-by-step code implementations &lt;/p&gt; &lt;p&gt;- Many tutorials paired with matching blog posts for deeper insights&lt;/p&gt; &lt;p&gt;- I'll keep sharing updates about these tutorials here &lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;A huge thank you to all contributors who made this possible! &lt;/p&gt; &lt;p&gt;link to the repo in the first comment &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nir777"&gt; /u/Nir777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndg4up/my_opensource_project_on_different_rag_techniques/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndg4up/my_opensource_project_on_different_rag_techniques/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndg4up/my_opensource_project_on_different_rag_techniques/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctfdv</id>
    <title>Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES</title>
    <updated>2025-09-09T20:01:35+00:00</updated>
    <author>
      <name>/u/Embarrassed_Sir_853</name>
      <uri>https://old.reddit.com/user/Embarrassed_Sir_853</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt; &lt;img alt="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" src="https://preview.redd.it/sxii7uog37of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61301382a59bd7671163d02b77eb25115e5d46e8" title="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this announcement about ROMA, seems like a plug-and-play and the benchmarks are up there. Simple combo of recursion and multi-agent structure with search tool. Crazy this is all it takes to beat SOTA billion dollar AI companies :)&lt;/p&gt; &lt;p&gt;I've been trying it out for a few things, currently porting it to my finance and real estate research workflows, might be cool to see it combined with other tools and image/video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://x.com/sewoong79/status/1963711812035342382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://github.com/sentient-agi/ROMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly shocked that this is open-source&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed_Sir_853"&gt; /u/Embarrassed_Sir_853 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxxi</id>
    <title>😳 umm</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt; &lt;img alt="😳 umm" src="https://preview.redd.it/80dp7ukemcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8ce888fab8e72337bb19e61f35d929aeac11346" title="😳 umm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80dp7ukemcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd7nxo</id>
    <title>VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!</title>
    <updated>2025-09-10T07:24:32+00:00</updated>
    <author>
      <name>/u/Cipher_Lock_20</name>
      <uri>https://old.reddit.com/user/Cipher_Lock_20</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt; &lt;img alt="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" src="https://external-preview.redd.it/ZXJidjUwNHJnYW9mMTZtREgbQQjA1lJ8zPSNZtqKO6Gf9AtInhXi-M401FlP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051bf77ffd8780ab4b7ffcc3dc7c1b3bc71a8875" title="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a huge AI audio nerd, I've recently been knee-deep in Microsoft's latest VibeVoice models and they really are awesome!! The work from the Microsoft Research team is amazing and they've shared them with everyone.... even though they took one back lol. I highly recommend checking them out if you haven't already.&lt;/p&gt; &lt;p&gt;I started reading up on all of the techniques applied within the architecture to allow for such long generations (45-90 minutes), with up to 4 speakers, and sounding so life-like... Google notebook is the closest thing to this kind of generation, but it's limited in that it auto-generates your podcast based on the context, not on the exact script you provide.&lt;/p&gt; &lt;p&gt;Let me have the VibeVoice model do the talking!&lt;/p&gt; &lt;p&gt;The generated voices in my video were generated within my own Hugging Face space and using the default voices provided by the VibeVoice model (7B). The voices were generated in one single generation, not stitched! &lt;a href="https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice"&gt;https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cipher_Lock_20"&gt; /u/Cipher_Lock_20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x4dht8pgaof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T07:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndc7z8</id>
    <title>I pre-trained GPT-OSS entirely from scratch</title>
    <updated>2025-09-10T12:00:25+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt; &lt;img alt="I pre-trained GPT-OSS entirely from scratch" src="https://external-preview.redd.it/9EZFRbCI06NQd5IaAcswlKJEIIkgLbOtsjD1e7w98EI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85d2bae87aa536469b4b4fbaafbfa3ee215b6f78" title="I pre-trained GPT-OSS entirely from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/fo9rnnpeubof1.png?width=2562&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=562a72f12d23083851c3775e1540b7f111ffda57"&gt;https://preview.redd.it/fo9rnnpeubof1.png?width=2562&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=562a72f12d23083851c3775e1540b7f111ffda57&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I recorded a 3 hour video to show how we built GPT-OSS from scratch. &lt;/p&gt; &lt;p&gt;You can watch the video here: &lt;a href="https://youtu.be/hBUsySdcA3I"&gt;https://youtu.be/hBUsySdcA3I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The video contains the following 8 steps:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;(1) Tiny Stories: Data Preprocessing&lt;/p&gt; &lt;p&gt;(2) GPT-OSS Harmony Tokenizer to tokenize the data&lt;/p&gt; &lt;p&gt;(3) Architecture Part 1: Token embeddings, RMSNorm and Rotary Positional Encoding (RoPE)&lt;/p&gt; &lt;p&gt;(4) Architecture Part 2: Sliding attention layers and Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;(5) Architecture Part 3: Attention Bias and Attention Sinks&lt;/p&gt; &lt;p&gt;(6) Architecture Part 4: SwiGLU Mixture of Experts (MoE) &lt;/p&gt; &lt;p&gt;(7) GPT-OSS Pre-training loop&lt;/p&gt; &lt;p&gt;(8) GPT-OSS Inference&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some info:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We have now released two versions of our codebase publicly. Both are under active work: &lt;/p&gt; &lt;p&gt;(1) Nano-GPT-OSS: &lt;a href="https://github.com/VizuaraAI/nano-gpt-oss"&gt;https://github.com/VizuaraAI/nano-gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- A 500 million parameter model which retains all the key architectural innovations of GPT-OSS. &lt;/p&gt; &lt;p&gt;- Requires 20 hours of training on 1 A40 GPU (0.4$/hr). Can be replicated under 10$. &lt;/p&gt; &lt;p&gt;(2) Truly-Open-GPT-OSS: &lt;a href="https://github.com/VizuaraAI/truly-open-gpt-oss"&gt;https://github.com/VizuaraAI/truly-open-gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- A 20B parameter model which we pre-trained fully from scratch. &lt;/p&gt; &lt;p&gt;- Requires 5 H200 GPUs. Budget needed for this would be 100-150$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T12:00:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf01a</id>
    <title>So apparently half of us are "AI providers" now (EU AI Act edition)</title>
    <updated>2025-09-10T14:00:15+00:00</updated>
    <author>
      <name>/u/Thecomplianceexpert</name>
      <uri>https://old.reddit.com/user/Thecomplianceexpert</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heads up, fellow tinkers&lt;/p&gt; &lt;p&gt;The EU AI Act’s first real deadline kicked in August 2nd so if you’re messing around with models that hit 10^23 FLOPs or more (think Llama-2 13B territory), regulators now officially care about you.&lt;/p&gt; &lt;p&gt;Couple things I’ve learned digging through this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The FLOP cutoff is surprisingly low. It’s not “GPT-5 on a supercomputer” level, but it’s way beyond what you’d get fine-tuning Llama on your 3090.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;“Provider” doesn’t just mean Meta, OpenAI, etc. If you fine-tune or significantly modify a big model, you need to watch out. Even if it’s just a hobby, you can still be classified as a provider.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Compliance isn’t impossible. Basically: &lt;ul&gt; &lt;li&gt;Keep decent notes (training setup, evals, data sources).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Have some kind of “data summary” you can share if asked.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Don’t be sketchy about copyright.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Deadline check:&lt;br /&gt; &lt;ul&gt; &lt;li&gt;New models released after Aug 2025 - rules apply now!&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Models that existed before Aug 2025 - you’ve got until 2027.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EU basically said: “Congrats, you’re responsible now.” 🫠 &lt;/p&gt; &lt;p&gt;TL;DR: If you’re just running models locally for fun, you’re probably fine. If you’re fine-tuning big models and publishing them, you might already be considered a “provider” under the law.&lt;/p&gt; &lt;p&gt;Honestly, feels wild that a random tinkerer could suddenly have reporting duties, but here we are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thecomplianceexpert"&gt; /u/Thecomplianceexpert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
</feed>
