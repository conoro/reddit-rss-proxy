<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-08T08:49:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qyx1b2</id>
    <title>Another use for my local llm</title>
    <updated>2026-02-08T02:36:10+00:00</updated>
    <author>
      <name>/u/regjoe13</name>
      <uri>https://old.reddit.com/user/regjoe13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyx1b2/another_use_for_my_local_llm/"&gt; &lt;img alt="Another use for my local llm" src="https://preview.redd.it/xifntr1737ig1.png?width=140&amp;amp;height=89&amp;amp;auto=webp&amp;amp;s=c63ae4e7fcdf3c9f1732130db1bbc857018a8960" title="Another use for my local llm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was helping a friend of mine with an article about AI and software development. As part of it GPT generated a Chrome extension for us, that grabs a content of a site you currently on, and sends it to my local lmstudio with a prompt. Lmstudio returns back list of facts, claims and opinions, along with evidence for each and displays it on the extension in english, regardless of the original site language. Its actually pretty cool, generation took about an hour of iterative process, with no manual code changes.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xifntr1737ig1.png?width=1673&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b83b3c3d3c0a4d0632734f4fb7c4e912b727b1ec"&gt;https://preview.redd.it/xifntr1737ig1.png?width=1673&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b83b3c3d3c0a4d0632734f4fb7c4e912b727b1ec&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xebj6fky27ig1.png?width=1663&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71b64b87e4c756062dae1621fbc353254d2a9f83"&gt;https://preview.redd.it/xebj6fky27ig1.png?width=1663&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=71b64b87e4c756062dae1621fbc353254d2a9f83&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/x1pxp7ly27ig1.png?width=1669&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98f1412fa492c1decbfdb4fc1c09817037cd0042"&gt;https://preview.redd.it/x1pxp7ly27ig1.png?width=1669&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98f1412fa492c1decbfdb4fc1c09817037cd0042&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I dropped it here: &lt;a href="https://github.com/yurtools/yr-evidence-extractor"&gt;https://github.com/yurtools/yr-evidence-extractor&lt;/a&gt; along with the prompt GPT produced to regenerate the code. I think using browser extension that you generated to easily run the content of the site against local model has some potential.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regjoe13"&gt; /u/regjoe13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyx1b2/another_use_for_my_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyx1b2/another_use_for_my_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyx1b2/another_use_for_my_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T02:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyeje2</id>
    <title>The M5 max and possibly the m5 ultra macs are coming soon!</title>
    <updated>2026-02-07T14:01:01+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just imagine having 256 gb of ram on MacBook! Mac os 26.3 should be coming out next week since the rc version is already out . They might release the m5 max with it since the os leak has the m5 max and ultra codenames in it. Crazy deepseek 4 and glm 5 and non codex gpt 5.3 are coming out soon too. Minimax 2.2 shouldnt be far either . If they release a macbook with the m5 ultra , I think people will go crazy over it, but the cooling is not good enough. A mac studio is more likely But since the packaging is different, u might be able to choose your gpu separately from your cpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyeje2/the_m5_max_and_possibly_the_m5_ultra_macs_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyeje2/the_m5_max_and_possibly_the_m5_ultra_macs_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyeje2/the_m5_max_and_possibly_the_m5_ultra_macs_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T14:01:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qycn5s</id>
    <title>DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison.</title>
    <updated>2026-02-07T12:32:28+00:00</updated>
    <author>
      <name>/u/RelativeOperation483</name>
      <uri>https://old.reddit.com/user/RelativeOperation483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"&gt; &lt;img alt="DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison." src="https://b.thumbs.redditmedia.com/HDuzkmv1h50aT7P7iyAduUT2TYW4g84iH4RYq4kv6Hg.jpg" title="DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same potato, new test. If you saw my last post, you will catch this up. I run LLMs on a &lt;strong&gt;2018 HP ProBook 8th Gen i3 with no Nvidia, no dedicated GPU&lt;/strong&gt;, just hope and an OpenVINO backend. This time I wanted to see how two MoE models compare head to head on the exact same hardware, same questions, same settings, same everything.&lt;/p&gt; &lt;p&gt;Same 10 questions for both models. Logic, health, history, coding, creative writing, factual biography, math, tech explainer, ethics, food science. Wide spread of topics to stress test general capability.&lt;/p&gt; &lt;p&gt;Each model was tested 3 times, each time running all 10 questions on CPU first then on iGPU with 1 layer offloaded. So that is 10 questions x 3 runs = 30 samples per device per model. 120 total inference runs. Same context (4096), same max output (256 tokens), same temperature (0.2), same top_p (0.9). Identical conditions.&lt;/p&gt; &lt;p&gt;&lt;em&gt;THE SPEED&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-V2-Lite absolutely smoked GPT-OSS. Almost 2x faster across the board.&lt;/li&gt; &lt;li&gt;DeepSeek on CPU: 7.93 tok/s average, TTFT 2.36s&lt;/li&gt; &lt;li&gt;DeepSeek on iGPU: 8.08 tok/s average, TTFT 1.86s&lt;/li&gt; &lt;li&gt;Peak decode: 8.28 tok/s (iGPU) — Lowest: 5.50 tok/s (CPU, cold start Q1)&lt;/li&gt; &lt;li&gt;GPT-OSS on CPU: 4.20 tok/s average, TTFT 3.13s&lt;/li&gt; &lt;li&gt;GPT-OSS on iGPU: 4.36 tok/s average, TTFT 3.07s&lt;/li&gt; &lt;li&gt;Peak decode: 4.46 tok/s (CPU) — Lowest: 3.18 tok/s (CPU, two questions got stuck slow)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In real time, DeepSeek finishes a 256-token response in about 32 seconds. GPT-OSS takes over a minute. That is the difference between usable and painful on a slow machine. The iGPU helped DeepSeek more than GPT-OSS. DeepSeek's time to first token dropped 21% on iGPU (from 2.36s to 1.86s). GPT-OSS barely changed. So if you are on iGPU, the smaller active parameter count benefits more from that little offload. (Just my opinion) &lt;/p&gt; &lt;p&gt;&lt;em&gt;THE QUALITY (I read every single response)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I went through all the outputs manually. Not vibes, actually reading them.&lt;/p&gt; &lt;p&gt;DeepSeek-V2-Lite: 7.5 out of 10&lt;/p&gt; &lt;p&gt;Very consistent. Clean structured answers. Good at health, history, math, tech explainers, ethics, food science. Wrote a complete cyberpunk poem. Solid Magna Carta summary. Nailed the Golden Ratio with three nature examples. Good VPN envelope analogy. Maillard reaction explanation was textbook quality.&lt;/p&gt; &lt;p&gt;Weaknesses&lt;br /&gt; But for today, it got the logic question wrong. The classic &amp;quot;All A are B, some B are C, therefore some A are C&amp;quot;. DeepSeek confidently said it is valid. It is not. That is a well-known syllogistic fallacy. Also on the coding question (Tower of Hanoi), &lt;strong&gt;it spent all its tokens explaining the problem and left the actual function as &amp;quot;# Your code here&amp;quot; without writing the implementation. Small factual error in Marie Curie bio (described her heritage incorrectly)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;GPT-OSS-20B: &lt;strong&gt;2 out of 10&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When it worked, it was impressive. It correctly identified the logic question as invalid and gave a concrete counterexample with sets to prove it. That was genuinely good reasoning. It also produced a complete working Tower of Hanoi implementation with proper recursion, base case, and example usage. The ethics response on the trolley problem was decent too.&lt;/p&gt; &lt;p&gt;Weaknesses &lt;/p&gt; &lt;p&gt;Hallucinated or broke down on 8 out of 10 questions. And I do not mean subtle errors, I mean full collapse. The health question turned into a loop of &amp;quot;Sure! Here is a revised version of the prompt&amp;quot; repeated over and over without ever answering. The history question started ok then degenerated into repeated &amp;quot;Answer:&amp;quot; blocks and &amp;quot;**...**&amp;quot; until the token limit. The VPN question was the worst — it looped &amp;quot;The user is a 3rd person perspective. The user is a 3. The user is a 3.&amp;quot; endlessly. Marie Curie question confused itself trying to summarize events from 2018-2023 for a woman who died in 1934. Golden Ratio collapsed into the same looping pattern. The poem spent all its tokens reasoning about what to write and only managed 4 lines.&lt;/p&gt; &lt;p&gt;This was not random. The same questions broke the same way across all 3 runs. It is a problem, GPT-OSS seems to be a reasoning/thinking model that burns its output budget on internal chain-of-thought and then either never reaches the answer or gets trapped in repetition loops. &lt;strong&gt;With only 256 tokens of output, it simply cannot think AND answer. Caution, I'm not saying Gpt-oss is bad, It can probably be the effect of Q4_K_M.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek-Coder-V2-Lite is the better model for budget hardware if we compare these 2 only. It is faster, more coherent, and way more reliable. &lt;strong&gt;GPT-OSS has flashes of real intelligence (that logic answer was better than what most small models produce)&lt;/strong&gt; but a model that loops on 8 out of 10 questions is not usable for anything practical at Q4_K_M. &lt;strong&gt;GPT-OSS might do better with higher max_tokens, and higher quantization.&lt;/strong&gt; I only tested Q4_K_M at 256 max output. If someone with better hardware wants to test it with more ram, more higher specs, Go for it. &lt;/p&gt; &lt;p&gt;I attached some screenshots in this post. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelativeOperation483"&gt; /u/RelativeOperation483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qycn5s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T12:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy5xnn</id>
    <title>Kimi-Linear-48B-A3B &amp; Step3.5-Flash are ready - llama.cpp</title>
    <updated>2026-02-07T05:59:11+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below are actual releases for both models. Anyway get &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;latest version&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step3.5-Flash&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7964"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7964&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi-Linear-48B-A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7957"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7957&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't see any new GGUFs( &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;other=base_model:quantized:moonshotai%2FKimi-Linear-48B-A3B-Instruct&amp;amp;sort=created"&gt;Kimi&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;other=base_model:quantized:stepfun-ai%2FStep-3.5-Flash&amp;amp;sort=trending"&gt;Step-3.5&lt;/a&gt; ) from our favorite sources yet. Probably today or tomorrow. &lt;/p&gt; &lt;p&gt;But ik_llama folks got GGUF for &lt;a href="https://huggingface.co/ubergarm/Step-3.5-Flash-GGUF"&gt;Step-3.5-Flash&lt;/a&gt; by ubergarm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T05:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy0l26</id>
    <title>Nemo 30B is insane. 1M+ token CTX on one 3090</title>
    <updated>2026-02-07T01:39:58+00:00</updated>
    <author>
      <name>/u/Dismal-Effect-1914</name>
      <uri>https://old.reddit.com/user/Dismal-Effect-1914</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with llama.cpp and some 30-80B parameter models with CPU offloading. Currently have one 3090 and 32 GB of RAM. Im very impressed by Nemo 30B. 1M+ Token Context cache, runs on one 3090, CPU offloading for experts. Does 35 t/s which is faster than I can read at least. Usually slow as fuck at this large a context window. Feed it a whole book or research paper and its done summarizing in like a few mins. This really makes long context windows on local hardware possible. The only other contender I have tried is Seed OSS 36b and it was much slower by about 20 tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Effect-1914"&gt; /u/Dismal-Effect-1914 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T01:39:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyzqwz</id>
    <title>Is this model working fine at Q4km? How does it compare to the original?</title>
    <updated>2026-02-08T04:48:45+00:00</updated>
    <author>
      <name>/u/Significant_Fig_7581</name>
      <uri>https://old.reddit.com/user/Significant_Fig_7581</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyzqwz/is_this_model_working_fine_at_q4km_how_does_it/"&gt; &lt;img alt="Is this model working fine at Q4km? How does it compare to the original?" src="https://external-preview.redd.it/j98XKqoJ3UOGeW66Etg0lVtFqPsaabyeyZuH8PQVb-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=234ec5f7ffcda5d2272c5b48c2652755e36ad2b9" title="Is this model working fine at Q4km? How does it compare to the original?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there a benchmark?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Fig_7581"&gt; /u/Significant_Fig_7581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/lovedheart/Qwen3-Coder-Next-REAP-48B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyzqwz/is_this_model_working_fine_at_q4km_how_does_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyzqwz/is_this_model_working_fine_at_q4km_how_does_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T04:48:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydx7z</id>
    <title>Successfully built an Autonomous Research Agent to handle 10k PDFs locally (32GB RAM / AnythingLLM)</title>
    <updated>2026-02-07T13:33:36+00:00</updated>
    <author>
      <name>/u/NGU-FREEFIRE</name>
      <uri>https://old.reddit.com/user/NGU-FREEFIRE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a quick win. I’ve been experimenting with Agentic RAG to handle a massive local dataset (10,000+ PDFs).&lt;/p&gt; &lt;p&gt;Most standard RAG setups were failing or hallucinating at this scale, so I moved to an &lt;strong&gt;Autonomous Agent&lt;/strong&gt; workflow using AnythingLLM and Llama 3.2. The agent now performs recursive searches and cross-references data points before giving me a final report.&lt;/p&gt; &lt;p&gt;Running it on 32GB RAM was the sweet spot for handling the context window without crashing.&lt;/p&gt; &lt;p&gt;If you're looking for a way to turn a &amp;quot;dumb&amp;quot; archive into a searchable, intelligent local database without sending data to the cloud, this is definitely the way to go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NGU-FREEFIRE"&gt; /u/NGU-FREEFIRE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydx7z/successfully_built_an_autonomous_research_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydx7z/successfully_built_an_autonomous_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydx7z/successfully_built_an_autonomous_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz34g9</id>
    <title>Qwen3-VL 2B LoRA finetuning</title>
    <updated>2026-02-08T07:58:02+00:00</updated>
    <author>
      <name>/u/NailCertain7181</name>
      <uri>https://old.reddit.com/user/NailCertain7181</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to finetune Qwen3-VL 2B model but stuck at deciding appropriate configuration of LoRA finetuning.&lt;/p&gt; &lt;p&gt;I have limited gpu resources so cant do hyperparameter tuning.&lt;/p&gt; &lt;p&gt;It would be a great help if anyone of you having experience with LoRA finetuning can give some suggestions.&lt;/p&gt; &lt;p&gt;Thank You&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NailCertain7181"&gt; /u/NailCertain7181 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz34g9/qwen3vl_2b_lora_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz34g9/qwen3vl_2b_lora_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz34g9/qwen3vl_2b_lora_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T07:58:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz0l8z</id>
    <title>I made an MNN of Jan-v3 4B</title>
    <updated>2026-02-08T05:32:41+00:00</updated>
    <author>
      <name>/u/DeProgrammer99</name>
      <uri>https://old.reddit.com/user/DeProgrammer99</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz0l8z/i_made_an_mnn_of_janv3_4b/"&gt; &lt;img alt="I made an MNN of Jan-v3 4B" src="https://external-preview.redd.it/VycITumdk1rq1JByGSeqfmuZwtJELyfu8fA2xVpGUuE.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=290dcb1b79edcdfc98ddd76d689faa046da3ad28" title="I made an MNN of Jan-v3 4B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Use case: MNN Chat on Android or iOS&lt;/p&gt; &lt;p&gt;If you're not familiar with it: MNN Chat is a really fast local LLM chat app--for example, I got 73.92 tokens per second prefill (28 tokens) and 16.3 tokens per second decode (465 tokens) with this model on my Galaxy S24+:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u48fuijyi7ig1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=390a4c45466d839b6104ac823c7d28d17017c8bb"&gt;https://preview.redd.it/u48fuijyi7ig1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=390a4c45466d839b6104ac823c7d28d17017c8bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DeProgrammer/Jan-v3-4B-base-instruct-MNN"&gt;https://huggingface.co/DeProgrammer/Jan-v3-4B-base-instruct-MNN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous thread about Jan v3 in general: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeProgrammer99"&gt; /u/DeProgrammer99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz0l8z/i_made_an_mnn_of_janv3_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz0l8z/i_made_an_mnn_of_janv3_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz0l8z/i_made_an_mnn_of_janv3_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T05:32:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyurjq</id>
    <title>Quantization-Aware distillation</title>
    <updated>2026-02-08T00:51:32+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I stumbled upon this research paper and it got me really interested so I would like to share it with you.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2601.20088"&gt;https://arxiv.org/abs/2601.20088&lt;/a&gt;&lt;/p&gt; &lt;p&gt;enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyurjq/quantizationaware_distillation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyurjq/quantizationaware_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyurjq/quantizationaware_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T00:51:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyjm0l</id>
    <title>Benchmarking total wait time instead of pp/tg</title>
    <updated>2026-02-07T17:22:35+00:00</updated>
    <author>
      <name>/u/batsba</name>
      <uri>https://old.reddit.com/user/batsba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"&gt; &lt;img alt="Benchmarking total wait time instead of pp/tg" src="https://preview.redd.it/dmf3ykavv3ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb531575915521581cd6f6e05acf9e09b011c7f3" title="Benchmarking total wait time instead of pp/tg" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find pp512/tg128 numbers not very useful for judging real-world performance. I've had setups that looked acceptable on paper but turned out to be too slow in real use.&lt;/p&gt; &lt;p&gt;So I started benchmarking total time to process realistic context sizes (1k to 64k tokens) + generation (always 500 tokens), which I think better represents what actually matters: how long do I need to wait?&lt;/p&gt; &lt;p&gt;Automated the whole process and put results on a website. Attached a screenshot showing some results for the Strix Halo 128 GB. Link if anyone's curious: &lt;a href="https://llocalhost.com/speed-bench/best-per-system/"&gt;https://llocalhost.com/speed-bench/best-per-system/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think is the best way to express how fast a local setup actually is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/batsba"&gt; /u/batsba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dmf3ykavv3ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T17:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qykuxd</id>
    <title>GLM-4.7-Flash reasoning is amazing</title>
    <updated>2026-02-07T18:09:12+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model is very aware when to start using structured points and when to talk directly and use minimal tokens.&lt;/p&gt; &lt;p&gt;For example I asked it a maths problem and asked it to do web search,when he saw the math problem he started to put the problem into different pieces and analyze each and then achieved conclusion.&lt;/p&gt; &lt;p&gt;where when it was operating in agentic environment it's like &amp;quot;user told me ..,I should...&amp;quot; Then it calls the tool directly without Yapping inside the Chain-Of-Thought.&lt;/p&gt; &lt;p&gt;Another good thing that it uses MLA instead of GQA which makes it's memory usage significantly lower and allows it to fit directly on some GPUs without offload.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydlwi</id>
    <title>Potential new Qwen and ByteDance Seed models are being tested on the Arena. The “Karp-001” and “Karp-002” models claim to be Qwen-3.5 models. The “Pisces-llm-0206a” and “Pisces-llm-0206b” models claim to be ByteDance models.</title>
    <updated>2026-02-07T13:19:21+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt; &lt;img alt="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The “Karp-001” and “Karp-002” models claim to be Qwen-3.5 models. The “Pisces-llm-0206a” and “Pisces-llm-0206b” models claim to be ByteDance models." src="https://preview.redd.it/rtrygqo1p2ig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9704e4c75927f5669c01b711e9c25a0d47ce44bb" title="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The “Karp-001” and “Karp-002” models claim to be Qwen-3.5 models. The “Pisces-llm-0206a” and “Pisces-llm-0206b” models claim to be ByteDance models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtrygqo1p2ig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:19:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyg10z</id>
    <title>I tested 11 small LLMs on tool-calling judgment — on CPU, no GPU.</title>
    <updated>2026-02-07T15:03:14+00:00</updated>
    <author>
      <name>/u/MikeNonect</name>
      <uri>https://old.reddit.com/user/MikeNonect</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Friday night experiment that got out of hand. I wanted to know: how small can a model be and still reliably do tool-calling on a laptop CPU?&lt;/p&gt; &lt;p&gt;So I benchmarked 11 models (0.5B to 3.8B) across 12 prompts. No GPU, no cloud API. Just Ollama and bitnet.cpp.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The models:&lt;/strong&gt; Qwen 2.5 (0.5B, 1.5B, 3B), LLaMA 3.2:3B, SmolLM2:1.7B, Ministral-3:3B, DeepSeek-R1:1.5B, Gemma3:1B, Phi4-mini:3.8B, BitNet 3B (base), BitNet 2B-4T (instruction-tuned)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The interesting part isn't whether they can call tools — they all can.&lt;/strong&gt; The interesting part is whether they know when NOT to.&lt;/p&gt; &lt;p&gt;I designed trick prompts like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Don't check the weather in Antwerp, just find me the quarterly report.&amp;quot; → 3 of 8 models called get_weather anyway&lt;/li&gt; &lt;li&gt;&amp;quot;The weather in Antwerp is 8°C and rainy. Should I schedule an indoor meeting with Jan?&amp;quot; → 5 of 8 models called get_weather to look up weather that was already in the prompt&lt;/li&gt; &lt;li&gt;&amp;quot;Can you write a Python script that checks the weather using an API?&amp;quot; → Multiple models called get_weather instead of writing code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some things that really surprised me:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen2.5:1.5b beat qwen2.5:3b.&lt;/strong&gt; The smaller model won by being more conservative — it declined prompts it wasn't sure about instead of guessing wrong. The 3B model called get_weather when asked to write a Python script about weather APIs. The 1.5B didn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.2 calls a tool on literally everything.&lt;/strong&gt; 9/10 action score, 0/2 restraint. Asked &amp;quot;what tools do you have?&amp;quot; — it called search_files. Asked to write code — it called search_files. It's a hammer that sees every prompt as a nail. But interesting: it actually picked the &lt;em&gt;right&lt;/em&gt; tool more often than most models on the hard prompts. Its problem is restraint, not selection.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BitNet 2B-4T gave the unexpected result.&lt;/strong&gt; I threw BitNet in as a wildcard, expecting it to fail. The base BitNet 3B model produces word salad — completely incoherent output. The instruction-tuned 2B-4T, however, produces perfect JSON tool calls at 2.3s on CPU. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical takeaway:&lt;/strong&gt; Simple tool routing is solved at 1.5B on CPU. But if your agent needs to decide &lt;em&gt;whether&lt;/em&gt; to act — not just &lt;em&gt;how&lt;/em&gt; — sub-4B models will confidently take the wrong action when keyword triggers are present. &lt;/p&gt; &lt;p&gt;Full benchmark code, detailed report with per-run data: &lt;a href="https://github.com/MikeVeerman/tool-calling-benchmark"&gt;https://github.com/MikeVeerman/tool-calling-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmark is a single Python file — easy to add your own models and prompts. Would love to see what happens with different hardware, different models, or different context window settings (I ran everything at Ollama's default 4K context).&lt;/p&gt; &lt;p&gt;Early attempt at a tool-calling-on-consumer-hardware benchmark. Polite feedback and ideas are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeNonect"&gt; /u/MikeNonect &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T15:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qywlk0</id>
    <title>Step-3.5 Flash</title>
    <updated>2026-02-08T02:16:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qywlk0/step35_flash/"&gt; &lt;img alt="Step-3.5 Flash" src="https://b.thumbs.redditmedia.com/5HyLXzZ2sQrWzeAiMYCs3Ybafk4NxcnrHGjDU-oPsxk.jpg" title="Step-3.5 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;stepfun-ai_Step-3.5-Flash-Q3_K_M from &lt;a href="https://huggingface.co/bartowski/stepfun-ai_Step-3.5-Flash-GGUF"&gt;https://huggingface.co/bartowski/stepfun-ai_Step-3.5-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;30t/s on 3x3090&lt;/p&gt; &lt;p&gt;Prompt prefill is too slow (around 150 t/s) for agentic coding, but regular chat works great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qywlk0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qywlk0/step35_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qywlk0/step35_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T02:16:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz2fra</id>
    <title>I benchmarked 672 "Return JSON only" calls. Strict parsing failed 67% of the time. Here's why.</title>
    <updated>2026-02-08T07:16:26+00:00</updated>
    <author>
      <name>/u/rozetyp</name>
      <uri>https://old.reddit.com/user/rozetyp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been building several LLM apps that rely on streaming JSON. The idea seemed quite simple: tell the model to &amp;quot;Return JSON only&amp;quot; and pipe it into my app.&lt;/p&gt; &lt;p&gt;But I kept breaking my parsers. The models would give me perfect logic, but wrapped in markdown fences (&lt;code&gt;\&lt;/code&gt;``json`) or preceded by conversational filler like &amp;quot;Here is the data.&amp;quot;&lt;/p&gt; &lt;p&gt;Out of curiosity, I decided to stop guessing and actually measure the gap between &amp;quot;Model generated valid JSON&amp;quot; and &amp;quot;API returned parseable JSON.&amp;quot;&lt;/p&gt; &lt;p&gt;Sharing what I learned because the results were way more drastic than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. The &amp;quot;Strict vs. Extractable&amp;quot; Gap is Massive&lt;/strong&gt; I tested 8 models (including 2026 releases like Kimi-k2.5, Mistral-small, and GPT-4o-mini) with plain prompts (no &lt;code&gt;response_format&lt;/code&gt;).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strict Parse (&lt;/strong&gt;&lt;code&gt;json.loads(response)&lt;/code&gt;&lt;strong&gt;):&lt;/strong&gt; Only &lt;strong&gt;33.3%&lt;/strong&gt; succeeded.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extractable JSON:&lt;/strong&gt; &lt;strong&gt;99.5%&lt;/strong&gt; of responses contained valid JSON buried in the text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, the models are smart enough to generate the data, but too &amp;quot;chatty&amp;quot; to be used as an API without a cleaning layer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Mistral is a &amp;quot;Helpful Saboteur&amp;quot;&lt;/strong&gt; I found a distinct personality quirk with the Mistral-family models. In my raw lane, they scored &lt;strong&gt;0%&lt;/strong&gt; on strict parsing.&lt;/p&gt; &lt;p&gt;But they weren't hallucinating. They were just aggressively helpful. They wrapped &lt;em&gt;every single response&lt;/em&gt; in markdown fences, even when the prompt explicitly forbade it. Once I stripped the fences, their accuracy jumped to 100%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. &amp;quot;Reasoning Models&amp;quot; leak their thoughts&lt;/strong&gt; This was the most interesting failure mode. I tested Moonshot Kimi-k2.5, and it sometimes failed because it &amp;quot;thought out loud&amp;quot; in the final response.&lt;/p&gt; &lt;p&gt;Ironically, it would output text like &lt;em&gt;&amp;quot;The user wants JSON only, so I must not use markdown&amp;quot;&lt;/em&gt;... and then that sentence itself would break the parser. As we move toward reasoning models, &amp;quot;thought leakage&amp;quot; is going to be a new headache for JSON reliability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. &amp;quot;Flash&amp;quot; doesn't mean &amp;quot;Timeout Proof&amp;quot;&lt;/strong&gt; I caught one outlier where &lt;code&gt;glm-4.7-flash&lt;/code&gt; (usually fast) hung for &lt;strong&gt;5.7 minutes&lt;/strong&gt; before returning. It’s a good reminder that even &amp;quot;fast&amp;quot; models need strict client-side timeouts, or one ghost request can hang your worker threads forever.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution&lt;/strong&gt; Since I didn't want to use regex hacks in every project, I built a tiny StreamFix middleware (not an ad). It’s a proxy that strips markdown fences and &amp;quot;thinking&amp;quot; text on the fly, so the client only ever sees clean JSON.&lt;/p&gt; &lt;p&gt;It bumped my success rate from 33% to 98% without changing the prompts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caveats!&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I tested with &lt;code&gt;temperature=0&lt;/code&gt; to keep it scientific.&lt;/li&gt; &lt;li&gt;My &amp;quot;markdown fence&amp;quot; classifier is simple (it flags &lt;code&gt;\&lt;/code&gt;``` anywhere), so it might catch some edge cases where the model is quoting code.&lt;/li&gt; &lt;li&gt;I didn't use &lt;code&gt;response_format&lt;/code&gt; because it's not supported strictly everywhere and I wanted to test the &amp;quot;plain prompt&amp;quot; baseline.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Questions for you:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Are you guys mostly relying on &lt;code&gt;response_format&lt;/code&gt; now, or do you still use regex cleaning?&lt;/li&gt; &lt;li&gt;Has anyone else noticed &amp;quot;reasoning leakage&amp;quot; breaking their structured outputs with newer models?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Models are great at JSON logic (99% success) but terrible at JSON formatting (33% success). The failures are mostly markdown wrappers and conversational filler. Does anyone else face this? How do you deal with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rozetyp"&gt; /u/rozetyp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz2fra/i_benchmarked_672_return_json_only_calls_strict/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T07:16:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyl6rd</id>
    <title>Gemini System Prompt - Google decided to remove "PRO" option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription.</title>
    <updated>2026-02-07T18:21:34+00:00</updated>
    <author>
      <name>/u/Educational_Rent1059</name>
      <uri>https://old.reddit.com/user/Educational_Rent1059</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt; &lt;img alt="Gemini System Prompt - Google decided to remove &amp;quot;PRO&amp;quot; option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription." src="https://b.thumbs.redditmedia.com/GQChSaPbMeljTuOrlvLzH1SfN18Sj71SBClWPpwoU_M.jpg" title="Gemini System Prompt - Google decided to remove &amp;quot;PRO&amp;quot; option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8fcauhhx64ig1.png?width=601&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b7a38b522ce96958f3d5df022bd77d140090255"&gt;https://preview.redd.it/8fcauhhx64ig1.png?width=601&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b7a38b522ce96958f3d5df022bd77d140090255&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As the title says! Enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Rent1059"&gt; /u/Educational_Rent1059 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz11n9</id>
    <title>What are some things you guys are using Local LLMs for?</title>
    <updated>2026-02-08T05:57:47+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So far im only using it for coding and search related stuff but anything else would be cool&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz11n9/what_are_some_things_you_guys_are_using_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T05:57:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyns06</id>
    <title>AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test.</title>
    <updated>2026-02-07T20:01:22+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt; &lt;img alt="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." src="https://preview.redd.it/7euavxiwo4ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31891ab1e02bef6fcc1b33374b8b479e2fec1051" title="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026"&gt;https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7euavxiwo4ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T20:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qynxuw</id>
    <title>Full Claude Opus 4.6 System Prompt for your pleasure</title>
    <updated>2026-02-07T20:07:42+00:00</updated>
    <author>
      <name>/u/frubberism</name>
      <uri>https://old.reddit.com/user/frubberism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qynxuw/full_claude_opus_46_system_prompt_for_your/"&gt; &lt;img alt="Full Claude Opus 4.6 System Prompt for your pleasure" src="https://external-preview.redd.it/otAtlKXoVGIzRX_D-XS8ef102ismRuSmY-rYGjCWHEI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=345f8e8a9693f1ecf3f281e2c9b37a5656e8634f" title="Full Claude Opus 4.6 System Prompt for your pleasure" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/frubberism"&gt; /u/frubberism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/asgeirtj/system_prompts_leaks/blob/main/Anthropic/claude-opus-4.6.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qynxuw/full_claude_opus_46_system_prompt_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qynxuw/full_claude_opus_46_system_prompt_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T20:07:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyljr0</id>
    <title>Prompt injection is killing our self-hosted LLM deployment</title>
    <updated>2026-02-07T18:34:55+00:00</updated>
    <author>
      <name>/u/mike34113</name>
      <uri>https://old.reddit.com/user/mike34113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We moved to self-hosted models specifically to avoid sending customer data to external APIs. Everything was working fine until last week when someone from QA tried injecting prompts during testing and our entire system prompt got dumped in the response.&lt;/p&gt; &lt;p&gt;Now I'm realizing we have zero protection against this. Traditional web application firewalls don't understand LLM-specific attacks. The model just treats malicious prompts like normal user input and happily complies.&lt;/p&gt; &lt;p&gt;Has anyone actually solved prompt injection for production LLM apps? Not talking about basic input sanitization because adversarial prompts can be crafted to look completely normal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mike34113"&gt; /u/mike34113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyynyw</id>
    <title>Llama.cpp's "--fit" can give major speedups over "--ot" for Qwen3-Coder-Next (2x3090 - graphs/chart included)</title>
    <updated>2026-02-08T03:54:02+00:00</updated>
    <author>
      <name>/u/tmflynnt</name>
      <uri>https://old.reddit.com/user/tmflynnt</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"&gt; &lt;img alt="Llama.cpp's &amp;quot;--fit&amp;quot; can give major speedups over &amp;quot;--ot&amp;quot; for Qwen3-Coder-Next (2x3090 - graphs/chart included)" src="https://b.thumbs.redditmedia.com/V82hsSMlAmBr4rUSKI2WUKnD9q38N3Rvi5wreJX85kg.jpg" title="Llama.cpp's &amp;quot;--fit&amp;quot; can give major speedups over &amp;quot;--ot&amp;quot; for Qwen3-Coder-Next (2x3090 - graphs/chart included)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next (unsloth's UD_Q4_K_XL) on dual RTX 3090 with llama.cpp b7941. More info in comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tmflynnt"&gt; /u/tmflynnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qyynyw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyynyw/llamacpps_fit_can_give_major_speedups_over_ot_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T03:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qym566</id>
    <title>I trained a 1.8M params model from scratch on a total of ~40M tokens.</title>
    <updated>2026-02-07T18:57:42+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt; &lt;img alt="I trained a 1.8M params model from scratch on a total of ~40M tokens." src="https://preview.redd.it/hv5xc4g794ig1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=cef557529cd85b5ecdfb430034c5db51f4d966d7" title="I trained a 1.8M params model from scratch on a total of ~40M tokens." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok so I've been working &amp;amp; experimenting with my own simple architecture. I call it &lt;a href="https://github.com/SrijanSriv211/Strawberry"&gt;Strawberry&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This is a very very small experimental model. It has 1.8M params and was trained on a dataset with ~9M tokens (~7M for training and ~2M for val). It model was trained on a batch size of 16 and context length of 256. Making the batch size in token counts to be &lt;code&gt;16*256 = 4096&lt;/code&gt;. Meaning the model saw 4096 tokens per step. It was trained for 10k steps meaning it trained on a total of 40M tokens.&lt;/p&gt; &lt;p&gt;The dataset was manually scraped and cleaned. The dataset contain texts from wikipedia on various topics, personalities, games, movies, companies and more. It also contain texts fandoms of various games such as GTA, RDR, Last of Us, Mafia and all. The dataset also contains storylines, scripts and story dialogues of various games such as RDR 2, GTA 5, Cyperpunk 2077, Mafia The Old Country. It also contain transcripts of some of my favorite youtube videos and it also contain code from some of my personal code bases and other repos such as the Hazel Game Engine repo on github. I tried my best to keep the programming language scale limited to just Python, C#, C++ and JavaScript. The dataset also contains texts from several research papers, academic articles and blogs (mainly revolving around AI and LLMs in general). All of this made ~30M chars in total.&lt;/p&gt; &lt;p&gt;After training for 10k steps the final train loss was around 3.5 and val loss was around 3.8.&lt;/p&gt; &lt;p&gt;This is the exact config for the model: &lt;code&gt;{&amp;quot;dataset&amp;quot;: {&amp;quot;data_division&amp;quot;: 0.8, &amp;quot;load_from_file&amp;quot;: true, &amp;quot;path&amp;quot;: &amp;quot;data/webtext.bin&amp;quot;}, &amp;quot;checkpoints&amp;quot;: {&amp;quot;path&amp;quot;: &amp;quot;bin/ck18&amp;quot;, &amp;quot;interval&amp;quot;: 1000, &amp;quot;create_checkpoints&amp;quot;: true}, &amp;quot;model_hyperparams&amp;quot;: {&amp;quot;vocab_size&amp;quot;: 8192, &amp;quot;block_size&amp;quot;: 256, &amp;quot;r_layer&amp;quot;: 3, &amp;quot;n_layer&amp;quot;: 2, &amp;quot;n_head&amp;quot;: 6, &amp;quot;n_embd&amp;quot;: 96, &amp;quot;n_qkv&amp;quot;: 384, &amp;quot;n_ffn&amp;quot;: 384}, &amp;quot;optimizer_hyperparams&amp;quot;: {&amp;quot;eps&amp;quot;: 1e-08, &amp;quot;beta1&amp;quot;: 0.9, &amp;quot;beta2&amp;quot;: 0.99, &amp;quot;weight_decay&amp;quot;: 0.001, &amp;quot;use_muon&amp;quot;: false, &amp;quot;momentum&amp;quot;: 0.95}, &amp;quot;model_path&amp;quot;: &amp;quot;bin/s1.strawberry&amp;quot;, &amp;quot;encoder_path&amp;quot;: &amp;quot;bin/cl8k.bin&amp;quot;, &amp;quot;init_from&amp;quot;: &amp;quot;scratch&amp;quot;, &amp;quot;seed&amp;quot;: &amp;quot;auto&amp;quot;, &amp;quot;gradient_accumulation_steps&amp;quot;: 1, &amp;quot;batch_size&amp;quot;: 16, &amp;quot;max_iters&amp;quot;: 10000, &amp;quot;eval_interval&amp;quot;: 1000, &amp;quot;log_interval&amp;quot;: 100, &amp;quot;eval_iters&amp;quot;: 100, &amp;quot;decay_lr&amp;quot;: true, &amp;quot;lr_decay_iters&amp;quot;: 10000, &amp;quot;learning_rate&amp;quot;: 0.002, &amp;quot;cooldown_frac&amp;quot;: 0.2, &amp;quot;warmup_iters&amp;quot;: 500, &amp;quot;min_lr&amp;quot;: 0.0002}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cl8k&lt;/code&gt; is a tokenizer from Andrej Karpathy's tokenizer video trained on the same dataset I explained above and then it was used to tokenize those ~30M chars into just ~9M toks.&lt;/p&gt; &lt;p&gt;The idea for Strawberry and retention was that I wanted to explore whether the attention weights can be generated in-real time rather than being learned. That's why I implemented a &amp;quot;Retention&amp;quot; Mechanism. The retention mechanism generates &amp;quot;weights&amp;quot; based on your input which are then used in attention. The formulation is a little bit similar to standard linear attention formula. This system where the QKV weights are dynamically generated rather than being learned allows to increase the number of attention layers (or model depth) without increasing the number of parameters at all.&lt;/p&gt; &lt;p&gt;However increasing the number of attention layers have a problem. If multiple attention layers are stacked on top of each other without any non-linearity such as FFN, then the performance can decline and the loss can get worse overtime.&lt;/p&gt; &lt;p&gt;That's why I implemented a mini-ffn right after the attention calculation and right before the output projection of each attention layer. So, the weights of qkv, mini-ffn and output projection are generated and updated dynamically by the retention mechanism.&lt;/p&gt; &lt;p&gt;I've two attention mechanisms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Linear Attention in this case Apple's AFT for global context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Standard MHA attention for local context. I'm also planning to experiment with &lt;code&gt;mixture of attention experts&lt;/code&gt; approach where each attention expert will get different local window. I haven't implemented it yet cuz this model was too small so it didn't made sense to me but I'll implement it later. Mixture of Attention Experts that's why the SPDA version of attention class is called &lt;code&gt;The Expert Abundance&lt;/code&gt;. Idk why but I like that name so I'm sticking with it.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Currently I'm trying to optimize &amp;amp; improve the architecture more.&lt;/p&gt; &lt;p&gt;So yeah. That's the entire thing. I'd love to know your views and opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qym566"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:57:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qz23pp</id>
    <title>PR opened for Qwen3.5!!</title>
    <updated>2026-02-08T06:57:13+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt; &lt;img alt="PR opened for Qwen3.5!!" src="https://preview.redd.it/r10pwm02y7ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb19e2c9eac9c47e80b6a33b08c10d458c3fb6c0" title="PR opened for Qwen3.5!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43830/"&gt;https://github.com/huggingface/transformers/pull/43830/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Looking at the code at &lt;code&gt;src/transformers/models/qwen3_5/modeling_qwen3_5.py&lt;/code&gt;, it looks like Qwen3.5 series will have VLMs right off the bat!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r10pwm02y7ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qz23pp/pr_opened_for_qwen35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T06:57:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
