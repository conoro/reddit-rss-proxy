<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-17T15:24:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mrqj6y</id>
    <title>Moxie goes local</title>
    <updated>2025-08-16T09:42:55+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt; &lt;img alt="Moxie goes local" src="https://external-preview.redd.it/NjRrNWZhaTZyY2pmMSz-4GeMjZaaPuK_BtqJdauJLy8SeG31djvp2OceGUPi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f56d4e2d6d85d38d0a6fee04a3f5cd06f2d2d7df" title="Moxie goes local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a localllama version of the OpenMoxie&lt;/p&gt; &lt;p&gt;It uses faster-whisper on the local for STT or the OpenAi whisper api (when selected in setup)&lt;/p&gt; &lt;p&gt;Supports LocalLLaMA, or OpenAi for conversations.&lt;/p&gt; &lt;p&gt;I also added support for XAI (Grok3 et al ) using the XAI API.&lt;/p&gt; &lt;p&gt;allows you to select what AI model you want to run for the local service.. right now 3:2b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eiwf36o6rcjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrtea</id>
    <title>Any benchmark for dual Nvidia rtx 6000 pro Blackwell?</title>
    <updated>2025-08-17T14:06:09+00:00</updated>
    <author>
      <name>/u/Reasonable_Friend_77</name>
      <uri>https://old.reddit.com/user/Reasonable_Friend_77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, can't seem to find any. I'm interested in both vllm and sglang. Anyone with this setup willing to share? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Friend_77"&gt; /u/Reasonable_Friend_77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrtea/any_benchmark_for_dual_nvidia_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrtea/any_benchmark_for_dual_nvidia_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrtea/any_benchmark_for_dual_nvidia_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:06:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mss3us</id>
    <title>Securing and Observing MCP Servers in Production</title>
    <updated>2025-08-17T14:18:16+00:00</updated>
    <author>
      <name>/u/No-Abies7108</name>
      <uri>https://old.reddit.com/user/No-Abies7108</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss3us/securing_and_observing_mcp_servers_in_production/"&gt; &lt;img alt="Securing and Observing MCP Servers in Production" src="https://external-preview.redd.it/MOcxvduEb_5FhLeszm41Cc6Zs_nQlMs6Kli8iZmRCP8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc5ffec244f1d6bf85300caf2e437ec2e0eb7ab4" title="Securing and Observing MCP Servers in Production" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building with &lt;strong&gt;Model Context Protocol (MCP)&lt;/strong&gt;? Cool, now here‚Äôs the hard part: making it &lt;strong&gt;secure, reliable, and observable&lt;/strong&gt; in production. In my new article, I walk through step-by-step practices: &lt;strong&gt;structured logging&lt;/strong&gt;, Moesif &amp;amp; New Relic monitoring, &lt;strong&gt;permission models&lt;/strong&gt;, and running audits with &lt;strong&gt;MCPSafetyScanner&lt;/strong&gt;. I also cover how to prevent &lt;em&gt;tool poisoning&lt;/em&gt; and &lt;em&gt;prompt injection&lt;/em&gt;. This isn‚Äôt theory, I‚Äôve included JSON logging examples, observability code snippets, and real-world design patterns. Devs, what‚Äôs your monitoring stack for MCP today‚Äîrolling your own dashboards or plugging into platforms? Let‚Äôs swap notes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Abies7108"&gt; /u/No-Abies7108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://glama.ai/blog/2025-08-17-monitoring-and-security-for-mcp-based-ai-systems"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss3us/securing_and_observing_mcp_servers_in_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mss3us/securing_and_observing_mcp_servers_in_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:18:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1mssgml</id>
    <title>2x W7900 or 1x RTX Pro 6000 Blackwell?</title>
    <updated>2025-08-17T14:32:46+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to upgrade my rig. I‚Äôm running into the limits of 48GB of VRAM and need more. The MoE CPU offloading in Llama.cpp help but as my project grows I will eventually exhaust that too.&lt;/p&gt; &lt;p&gt;To be clear: this is for inference, not training. I use llama.cpp and soon vLLm. I run mostly coding models at high context, but I do also like to run larger general purpose models. I already have a chonkyboi W7900 and even if I got the RTX I wouldn‚Äôt sell it, so this keeps bringing me back to just getting another one.&lt;/p&gt; &lt;p&gt;Running in Ubuntu but will switch back to Windows if inference performance matched Ubuntu. &lt;/p&gt; &lt;p&gt;PCIe lanes aren‚Äôt an issue as I am also looking to upgrade to a Threadripper system. Power would be an issue (I‚Äôd get the Max-Q RTX, so dual W7900s would consume twice the power, but in a Threadripper system the total system difference is not 2x). Cost is another issue (W7900s are cheap rn, the 6000s aren‚Äôt).&lt;/p&gt; &lt;p&gt;Before anyone goes off on AMD, I‚Äôm quite happy with the performance of the W7900 and it‚Äôs pretty easy to get up and running. It‚Äôs a beast of a card and the price-to-performance is great.&lt;/p&gt; &lt;p&gt;What would you do?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mssgml/2x_w7900_or_1x_rtx_pro_6000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mssgml/2x_w7900_or_1x_rtx_pro_6000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mssgml/2x_w7900_or_1x_rtx_pro_6000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:32:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms96z1</id>
    <title>AI Lifecycle in a Nutshell</title>
    <updated>2025-08-16T22:01:19+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt; &lt;img alt="AI Lifecycle in a Nutshell" src="https://b.thumbs.redditmedia.com/0zwQoI10gO1GKiDgCyUSu47ZcGjND6-vGxCtl-comsg.jpg" title="AI Lifecycle in a Nutshell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xc6l1acgegjf1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=309bc20eee5c0958a7bcb597610c07fca2dc856d"&gt;https://preview.redd.it/xc6l1acgegjf1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=309bc20eee5c0958a7bcb597610c07fca2dc856d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI Lifecycle in a Nutshell&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You pay $20 to Cursor.&lt;/li&gt; &lt;li&gt;Cursor pays $50 to Claude (with $30 from VC money).&lt;/li&gt; &lt;li&gt;Claude pays $100 to Nvidia (with $50 from VC money).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;NOTE: Just for fun, not aimed at any specific company! :D&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mssp1i</id>
    <title>Claude distill</title>
    <updated>2025-08-17T14:41:56+00:00</updated>
    <author>
      <name>/u/superNova-best</name>
      <uri>https://old.reddit.com/user/superNova-best</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone tried distilling claude on a local llm smtg like qwen code or deepseek to up its coding capabilities? I mean qwen3 code is already breaking a lot of benchmarks but wont distilling a powerful proprietary model make it even better ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/superNova-best"&gt; /u/superNova-best &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mssp1i/claude_distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mssp1i/claude_distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mssp1i/claude_distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1msm9ip</id>
    <title>GLM 4.5 stuck in a loop. Is this bug ?</title>
    <updated>2025-08-17T09:16:10+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msm9ip/glm_45_stuck_in_a_loop_is_this_bug/"&gt; &lt;img alt="GLM 4.5 stuck in a loop. Is this bug ?" src="https://preview.redd.it/tzv51i1crjjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22c7ddfc86f2bb9bc9a780dd6ba57db0633a4e76" title="GLM 4.5 stuck in a loop. Is this bug ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tzv51i1crjjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msm9ip/glm_45_stuck_in_a_loop_is_this_bug/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msm9ip/glm_45_stuck_in_a_loop_is_this_bug/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T09:16:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms222w</id>
    <title>"AGI" is equivalent to "BTC is going to take over the financial world"</title>
    <updated>2025-08-16T17:37:53+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;AGI&amp;quot; is really just another hypetrain. Sure AI is going to disrupt industries, displace jobs and cause mayhem in the social fabric - but the omnipotent &amp;quot;AGI&amp;quot; that governs all aspects of life and society and most importantly, ushers in &amp;quot;post labor economics&amp;quot;? Wonder how long it takes until tech bros and fanboys realize this. GPT5, Opus 4 and all others are only incremental improvements, if at all. Where's the path to &amp;quot;AGI&amp;quot; in this reality? People who believe this are going to build a bubble for themselves, detached from reality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T17:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1msbfw9</id>
    <title>Qwen3 just gets me ‚ù§Ô∏è</title>
    <updated>2025-08-16T23:31:35+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"&gt; &lt;img alt="Qwen3 just gets me ‚ù§Ô∏è" src="https://b.thumbs.redditmedia.com/0GYMAK-JtLBRlRV0DjTz-LN109QuZuO4A0Gni-WxIJI.jpg" title="Qwen3 just gets me ‚ù§Ô∏è" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1msbfw9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mss11l</id>
    <title>LL3M: Large Language 3D Modelers</title>
    <updated>2025-08-17T14:15:10+00:00</updated>
    <author>
      <name>/u/codexauthor</name>
      <uri>https://old.reddit.com/user/codexauthor</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/codexauthor"&gt; /u/codexauthor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://threedle.github.io/ll3m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss11l/ll3m_large_language_3d_modelers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mss11l/ll3m_large_language_3d_modelers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:15:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn987</id>
    <title>Profiling Large Language Model Inference on Apple Silicon: A Quantization Perspective</title>
    <updated>2025-08-17T10:19:17+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.08531"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn987/profiling_large_language_model_inference_on_apple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn987/profiling_large_language_model_inference_on_apple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mskf61</id>
    <title>OpenEvolve Beats GEPA Benchmarks: +6.42% Overall Improvement with Evolutionary Prompt Optimization</title>
    <updated>2025-08-17T07:19:43+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Wanted to share results from &lt;strong&gt;OpenEvolve&lt;/strong&gt;, an open-source implementation of evolutionary prompt optimization that's achieving strong performance on benchmarks from the recent GEPA paper.&lt;/p&gt; &lt;h2&gt;Context: The GEPA Paper&lt;/h2&gt; &lt;p&gt;Researchers recently released &lt;a href="https://arxiv.org/abs/2507.19457"&gt;GEPA (Genetic-Pareto)&lt;/a&gt;, a prompt optimization technique that uses natural language reflection to improve LLM performance. GEPA reports 10-20% improvements over GRPO and 10%+ over MIPROv2, using up to 35x fewer rollouts by leveraging the interpretable nature of language as a learning medium.&lt;/p&gt; &lt;h2&gt;OpenEvolve Results (Same Benchmarks as GEPA)&lt;/h2&gt; &lt;p&gt;OpenEvolve improved prompts across 11,946 samples:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;Evolved&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;IFEval&lt;/strong&gt; (instruction following)&lt;/td&gt; &lt;td&gt;95.01%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;97.41%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+2.40%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HotpotQA&lt;/strong&gt; (multi-hop reasoning)&lt;/td&gt; &lt;td&gt;77.93%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;88.62%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+10.69% üî•&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HoVer&lt;/strong&gt; (claim verification)&lt;/td&gt; &lt;td&gt;43.83%&lt;/td&gt; &lt;td&gt;42.90%&lt;/td&gt; &lt;td&gt;-0.93%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;67.29%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.71%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+6.42%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;That's &lt;strong&gt;767 more correct answers&lt;/strong&gt; with &lt;strong&gt;38% fewer empty responses&lt;/strong&gt;!&lt;/p&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;p&gt;OpenEvolve takes a different approach from GEPA's reflection-based optimization and DSPy's gradient-based methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MAP-Elites Algorithm&lt;/strong&gt;: Maintains diversity through multi-dimensional feature grids&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Island Evolution&lt;/strong&gt;: 4 isolated populations evolve independently with periodic migration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cascade Evaluation&lt;/strong&gt;: Quick validation (10 samples) before expensive full tests (40+ samples)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-Judge&lt;/strong&gt;: Combines quantitative accuracy with qualitative feedback on clarity/robustness&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Example Evolution (HotpotQA)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Before&lt;/strong&gt;: Basic prompt asking for answer&lt;br /&gt; &lt;strong&gt;After 50 iterations&lt;/strong&gt;: Structured multi-step reasoning with paragraph analysis, synthesis, and citation requirements&lt;/p&gt; &lt;h2&gt;Quick Start&lt;/h2&gt; &lt;p&gt;&lt;code&gt;bash git clone https://github.com/codelion/openevolve cd openevolve/examples/llm_prompt_optimization pip install -r requirements.txt python evaluate_prompts.py --dataset all --prompt-type evolved &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API (OpenRouter, vLLM, Ollama).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/openevolve"&gt;OpenEvolve Repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone's compared evolutionary vs reflection-based (GEPA) vs gradient-based (DSPy) approaches on their own tasks? What's been your experience with prompt optimization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T07:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1msa1n4</id>
    <title>So Steam finally got back to me</title>
    <updated>2025-08-16T22:34:49+00:00</updated>
    <author>
      <name>/u/ChrisZavadil</name>
      <uri>https://old.reddit.com/user/ChrisZavadil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 5 weeks of waiting for steam to approve my application that would allow users to input their own llms locally and communicate with them they told me that my app failed testing because it lacked the proper guardrails. They want me to block input and output for the LLM. Anybody put an unguarded LLM on Steam before?&lt;/p&gt; &lt;p&gt;I added a walledguard and re-uploaded, but for now I just made the full unrestricted version available on Itch if anyone wants to give it a try:&lt;br /&gt; &lt;a href="https://zavgaming.itch.io/megan-ai"&gt;https://zavgaming.itch.io/megan-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChrisZavadil"&gt; /u/ChrisZavadil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms9djc</id>
    <title>Wan2.2 i2v Censors Chinese-looking women in nsfw workflows</title>
    <updated>2025-08-16T22:08:13+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using wan2.2 i2v for generating over 100 nsfw videos so far. Noticed something curious. Lol When input image is chinese-looking, it never outputs nsfw videos. But when I use non-chinese input images, it outputs nsfw.&lt;/p&gt; &lt;p&gt;Anybody else experienced this? Lol really curious shiz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn7pt</id>
    <title>XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization</title>
    <updated>2025-08-17T10:16:39+00:00</updated>
    <author>
      <name>/u/juanviera23</name>
      <uri>https://old.reddit.com/user/juanviera23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; XQuant proposes caching low-bit &lt;strong&gt;layer inputs (X)&lt;/strong&gt; instead of the usual KV cache and &lt;strong&gt;rematerializing K/V on the fly&lt;/strong&gt;, trading extra compute for far less memory traffic; this gives an immediate &lt;strong&gt;~2√ó&lt;/strong&gt; cut vs standard KV caching and up to &lt;strong&gt;~7.7√ó&lt;/strong&gt; vs FP16 with &lt;strong&gt;&amp;lt;0.1&lt;/strong&gt; perplexity drop, while the cross-layer variant (&lt;strong&gt;XQuant-CL&lt;/strong&gt;) reaches &lt;strong&gt;10√ó (‚âà0.01 ppl)&lt;/strong&gt; and &lt;strong&gt;12.5√ó (‚âà0.1 ppl)&lt;/strong&gt;, with near-FP16 accuracy and better results than prior KV-quant methods.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/juanviera23"&gt; /u/juanviera23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2508.10395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn7pt/xquant_breaking_the_memory_wall_for_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn7pt/xquant_breaking_the_memory_wall_for_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:16:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms4n55</id>
    <title>What does it feel like: Cloud LLM vs Local LLM.</title>
    <updated>2025-08-16T19:10:29+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt; &lt;img alt="What does it feel like: Cloud LLM vs Local LLM." src="https://preview.redd.it/8qtcdau4kfjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c4609d4440c5a870f624682bb7bead5dece104" title="What does it feel like: Cloud LLM vs Local LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't get me wrong, I love local models, but they give me this anxiety. We need to fix this... üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qtcdau4kfjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn1n0</id>
    <title>Looks like Kimi K2 quietly joined the ‚Äú5.9 ‚àí 5.11 = ?‚Äù support group. üò©</title>
    <updated>2025-08-17T10:06:04+00:00</updated>
    <author>
      <name>/u/JeffreySons_90</name>
      <uri>https://old.reddit.com/user/JeffreySons_90</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt; &lt;img alt="Looks like Kimi K2 quietly joined the ‚Äú5.9 ‚àí 5.11 = ?‚Äù support group. üò©" src="https://preview.redd.it/e0o9q4g90kjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96de0c19ca32f0ffe1af17db9ea73f11e103ccae" title="Looks like Kimi K2 quietly joined the ‚Äú5.9 ‚àí 5.11 = ?‚Äù support group. üò©" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JeffreySons_90"&gt; /u/JeffreySons_90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e0o9q4g90kjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn1n0/looks_like_kimi_k2_quietly_joined_the_59_511/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:06:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjr8e</id>
    <title>Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp; 1.6B)</title>
    <updated>2025-08-17T06:39:35+00:00</updated>
    <author>
      <name>/u/benja0x40</name>
      <uri>https://old.reddit.com/user/benja0x40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt; &lt;img alt="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" src="https://external-preview.redd.it/ODf4ePnObFjNLo_T-D3tl5IjEp3QG9wN69Zl1K75jBk.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6edbd43244d19019e3eb00f1cf461de13c681f23" title="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;2 models based on the hybrid &lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt;LFM2 architecture&lt;/a&gt;: LFM2-VL-450M and LFM2-VL-1.6B&lt;/li&gt; &lt;li&gt;Available quant: 8bit MLX, GGUF Q8 &amp;amp; Q4 (llama.cpp release &lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b6183"&gt;b6183&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;HuggingFace Collection&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f7cnaj82zijf1.png?width=2072&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8dae64814c6f611481706d86e4a7643b7dc776"&gt;Figure 3. Processing time comparison across vision-language models.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Added GGUF availability and compatible llama.cpp release&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benja0x40"&gt; /u/benja0x40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T06:39:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msgl6q</id>
    <title>Added Qwen 0.6B to the small model overview in IFEval.</title>
    <updated>2025-08-17T03:41:26+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt; &lt;img alt="Added Qwen 0.6B to the small model overview in IFEval." src="https://external-preview.redd.it/9Cl7KVCIkap1D9OBhLKIL0DrKnbvINMV1azrpCVXD0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db8e296f1f9b6b888a016ade0da5771f2fa87434" title="Added Qwen 0.6B to the small model overview in IFEval." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ygMzbHp.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T03:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1msb0mq</id>
    <title>For those who run large models locally.. HOW DO YOU AFFORD THOSE GPUS</title>
    <updated>2025-08-16T23:14:00+00:00</updated>
    <author>
      <name>/u/abaris243</name>
      <uri>https://old.reddit.com/user/abaris243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;okay I'm just being nosy.. I mostly run models and fine tune as a hobby so I typically only run models under the 10b parameter range, is everyone that is running larger models just paying for cloud services to run them? and for those of you who do have stacks of A100/H100s is this what you do for a living, how do you afford it??&lt;/p&gt; &lt;p&gt;edit: for more context about me and my setup, I have a 3090ti and 64gb ram, I am actually a cgi generalist / 3d character artist and my industry is taking a huge hit right now, so with my extra free time and my already decent set up I've been learning to fine tune models and format data on the side, idk if ill ever do a full career 180 but I love new tech (even though these new technologies and ideas are eating my current career)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abaris243"&gt; /u/abaris243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mss35h</id>
    <title>WE NEED OPEN-SOURCE NANO-BANANAüò≠</title>
    <updated>2025-08-17T14:17:27+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss35h/we_need_opensource_nanobanana/"&gt; &lt;img alt="WE NEED OPEN-SOURCE NANO-BANANAüò≠" src="https://preview.redd.it/915jijh29ljf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4be54065d8e62ba8548f2a1fe622c4e462941fb" title="WE NEED OPEN-SOURCE NANO-BANANAüò≠" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/915jijh29ljf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mss35h/we_need_opensource_nanobanana/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mss35h/we_need_opensource_nanobanana/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T14:17:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1msosdv</id>
    <title>Why does Mistral NeMo's usage keep growing even after more than a year since releasing?</title>
    <updated>2025-08-17T11:46:54+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt; &lt;img alt="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" src="https://preview.redd.it/5wd0ayxihkjf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef52cf7168e409394d3d181f871e20c40bcefa5d" title="Why does Mistral NeMo's usage keep growing even after more than a year since releasing?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wd0ayxihkjf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msosdv/why_does_mistral_nemos_usage_keep_growing_even/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T11:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1msn3gi</id>
    <title>Ovis2.5 9B ~ 2B - New Multi-modal LLMs from Alibaba</title>
    <updated>2025-08-17T10:09:17+00:00</updated>
    <author>
      <name>/u/Sad_External6106</name>
      <uri>https://old.reddit.com/user/Sad_External6106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing with &lt;strong&gt;Ovis2.5 (2B &amp;amp; 9B)&lt;/strong&gt; the past few days. The cool part is it now has an optional &lt;em&gt;think&lt;/em&gt; mode ‚Äî the model will slow down a bit but actually self-check and refine answers, which really helps on harder reasoning tasks. Also the OCR feels way better than before, especially on messy charts and dense documents. Overall, a pretty practical upgrade if you care about reasoning + OCR.&lt;/p&gt; &lt;p&gt;üëâ &lt;a href="https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335"&gt;https://huggingface.co/collections/AIDC-AI/ovis25-689ec1474633b2aab8809335&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_External6106"&gt; /u/Sad_External6106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msn3gi/ovis25_9b_2b_new_multimodal_llms_from_alibaba/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T10:09:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1msrnqq</id>
    <title>Wow anthropic and Google losing coding share bc of qwen 3 coder</title>
    <updated>2025-08-17T13:59:47+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt; &lt;img alt="Wow anthropic and Google losing coding share bc of qwen 3 coder" src="https://preview.redd.it/rwehyliy5ljf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c67f716968425732683dc36fdd2644caa8322da3" title="Wow anthropic and Google losing coding share bc of qwen 3 coder" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rwehyliy5ljf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msrnqq/wow_anthropic_and_google_losing_coding_share_bc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:59:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1msr7j8</id>
    <title>To all vibe coders I present</title>
    <updated>2025-08-17T13:40:07+00:00</updated>
    <author>
      <name>/u/theundertakeer</name>
      <uri>https://old.reddit.com/user/theundertakeer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt; &lt;img alt="To all vibe coders I present" src="https://external-preview.redd.it/dXZiNzRocGcybGpmMeA17HlDZqcxGH0WPMXNGATdmxTbHU45E1nSLLgU5DlN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5981b886ff07914ad22d7db97d58fa9b60c3a9" title="To all vibe coders I present" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theundertakeer"&gt; /u/theundertakeer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eckuwlog2ljf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msr7j8/to_all_vibe_coders_i_present/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T13:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
