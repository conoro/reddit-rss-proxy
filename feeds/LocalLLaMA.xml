<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-28T09:06:13+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1llzdi8</id>
    <title>I built an Automated AI Stylist in 24 hours (open source, local)</title>
    <updated>2025-06-27T17:11:21+00:00</updated>
    <author>
      <name>/u/ParsaKhaz</name>
      <uri>https://old.reddit.com/user/ParsaKhaz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/"&gt; &lt;img alt="I built an Automated AI Stylist in 24 hours (open source, local)" src="https://external-preview.redd.it/aWJoanhkd2I1aTlmMeEsEqhEcpnAGeAOI3lYg_mXc9hWrD9oAMlWiqt_A_Sq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4aa3b18ac6df7c246ae5daf5fad4a83ff312eb26" title="I built an Automated AI Stylist in 24 hours (open source, local)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ParsaKhaz"&gt; /u/ParsaKhaz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2v76newb5i9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llzdi8/i_built_an_automated_ai_stylist_in_24_hours_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T17:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm17p6</id>
    <title>Is it just me, or Gemma 3n really sucks in recognizing images?</title>
    <updated>2025-06-27T18:25:16+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just curious, is it just me, or Gemma 3n really sucks in recognizing images?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm17p6/is_it_just_me_or_gemma_3n_really_sucks_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm17p6/is_it_just_me_or_gemma_3n_really_sucks_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm17p6/is_it_just_me_or_gemma_3n_really_sucks_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1llnwy5</id>
    <title>AI performance of smartphone SoCs</title>
    <updated>2025-06-27T07:34:42+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"&gt; &lt;img alt="AI performance of smartphone SoCs" src="https://external-preview.redd.it/H_9g87w3EitABPy3ZAOo2ZH9LlcpQ5L4KMiJgV1zrjo.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=baf972823fcd97a8af0b34ddd0ede97ce0d9de05" title="AI performance of smartphone SoCs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ai-benchmark.com/ranking_processors.html"&gt;https://ai-benchmark.com/ranking_processors.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few things notable to me: - The difference between tiers is &lt;em&gt;huge&lt;/em&gt;. A 2022 Snapdragon 8 Gen 2 beats the 8s Gen 4. There are huge gaps between the Dimensity 9000, 8000 and 7000 series. - You can better get a high-end SoC that‚Äôs a few years old than the latest mid-range one.&lt;/p&gt; &lt;h2&gt;- In this benchmark, it‚Äôs mainly a Qualcomm and Mediatek competition. It seems optimized software libraries are immensely important in using hardware effectively.&lt;/h2&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1llnwy5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llnwy5/ai_performance_of_smartphone_socs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T07:34:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmdkbg</id>
    <title>It's wild, where they got their data for training and consistency --&gt; https://youtu.be/US2gO7UYEfY</title>
    <updated>2025-06-28T04:00:59+00:00</updated>
    <author>
      <name>/u/kernel348</name>
      <uri>https://old.reddit.com/user/kernel348</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any idea on how they might have trained/fine-tuned veo3 and how they got it to consistency. &lt;a href="https://youtu.be/US2gO7UYEfY"&gt;veo3 ai video&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kernel348"&gt; /u/kernel348 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmdkbg/its_wild_where_they_got_their_data_for_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmdkbg/its_wild_where_they_got_their_data_for_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmdkbg/its_wild_where_they_got_their_data_for_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T04:00:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm0btg</id>
    <title>Mid-30s SWE: Take Huge Pay Cut for Risky LLM Research Role?</title>
    <updated>2025-06-27T17:49:28+00:00</updated>
    <author>
      <name>/u/Worth_Contract7903</name>
      <uri>https://old.reddit.com/user/Worth_Contract7903</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current Situation: * TC: 110k * YoE: 2 years as a Software Engineer (career switcher, mid-30s). * Role: SWE building AI applications using RAG. I've developed a strong passion for building LLMs, not just using them. I do not have a PhD.&lt;/p&gt; &lt;p&gt;I've been offered a role at a national lab to do exactly that‚Äîbuild LLMs from scratch and publish research, which could be a stepping stone to a top-tier team.&lt;/p&gt; &lt;p&gt;The problem is the offer has major red flags. It‚Äôs a significant pay cut, and my contact there admits the rest of the team is unmotivated and out of touch. More critically, the project's funding is only guaranteed until June of next year, and my contact, the only person I'd want to work with, will likely leave in two years. I'm worried about taking a huge risk that could blow up and leave me with nothing. My decision comes down to the future of AI roles. Is core LLM development a viable path without a PhD, or is the safer money in AI app development and fine-tuning? &lt;/p&gt; &lt;p&gt;Given the unstable funding and weak team, would you take this risky, low-paying job for a shot at a dream role, or is it a career-killing move?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worth_Contract7903"&gt; /u/Worth_Contract7903 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0btg/mid30s_swe_take_huge_pay_cut_for_risky_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T17:49:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1llms46</id>
    <title>FYI to everyone: RTX 3090 prices crashed and are back to baseline. You can finally get $600something 3090s again in the USA.</title>
    <updated>2025-06-27T06:20:23+00:00</updated>
    <author>
      <name>/u/DepthHour1669</name>
      <uri>https://old.reddit.com/user/DepthHour1669</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you've been priced out by the spike to $1000+ recently for the past ~3 months, the prices finally dropped to baseline recently. &lt;/p&gt; &lt;p&gt;You can get a $650-750 Nvidia 3090 fairly easily now, instead of being nearly impossible. &lt;/p&gt; &lt;p&gt;Future pricing is unpredictable- if we follow expected deprecation trends, the 3090 should be around $550-600, but then again Trump's tariff extensions expire in a few weeks and pricing is wild and likely to spike up. &lt;/p&gt; &lt;p&gt;If you're interested in GPUs, &lt;strong&gt;now&lt;/strong&gt; is probably the best time to buy for 3090s/4090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DepthHour1669"&gt; /u/DepthHour1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llms46/fyi_to_everyone_rtx_3090_prices_crashed_and_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm93yi</id>
    <title>Magistral small similarity to Deepseek chat?</title>
    <updated>2025-06-28T00:03:21+00:00</updated>
    <author>
      <name>/u/ImprovementBusy5947</name>
      <uri>https://old.reddit.com/user/ImprovementBusy5947</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just testing on some old math problems, noticed that &lt;strong&gt;Magistral Small and Mistral Small output looks a lot like deepseek chat&lt;/strong&gt;, but pretty far from Qwen3. I‚Äôm guessing Magistral distilled from deepseek directly without acknowledging it?&lt;/p&gt; &lt;p&gt;Suppose that there exist nonzero complex numbers $a$ , $b$ , $c$ , and $d$ such that $k$ is a root of both the equations $ax^3+bx^2+cx+d=0$ and $bx^3+cx^2+dx+a=0$ . Find all possible values of $k$ (including complex values).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Deepseek chat:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Alright, I have this problem: **Problem Statement:** Suppose that there exist nonzero complex numbers a,b,c,, and d such that k is a root of both the equations ax3+bx2+cx+d=0 and bx3+cx2+dx+a=0. Find all possible values of k(including complex values). # Understanding the Problem First, I need to understand what's being asked. We have two cubic equations: 1. ax3+bx2+cx+d=0 ‚Äî let's call this Equation (1). 2. bx3+cx2+dx+a=0 ‚Äî Equation (2). A complex number k is a root of both equations. The coefficients a,b,c,d are non-zero complex numbers. I need to find all possible values of k that satisfy both equations. # Step 1: Substitute k into Both Equations ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Magistral Small:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Alright, I have this problem: **Problem Statement:** Find all possible values of ( k ) (including complex values) such that there exist nonzero complex numbers ( a, b, c, d ) for which ( k ) is a root of both the equations: 1. ( ax\^3 + bx\^2 + cx + d = 0 ) 2. ( bx\^3 + cx\^2 + dx + a = 0 ) # Understanding the Problem First, I need to understand what's being asked. We have two cubic equations: 1. The first equation is ( ax\^3 + bx\^2 + cx + d = 0 ). 2. The second equation is ( bx\^3 + cx\^2 + dx + a = 0 ). Both equations have a common root ( k ). The coefficients ( a, b, c, d ) are all nonzero complex numbers. The goal is to find all possible values of ( k ) that satisfy this condition. # Step 1: Substitute the Common Root ( k ) into Both Equations ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Mistral Small:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Alright, I have this problem: **Problem Statement:** Suppose that there exist nonzero complex numbers a,b,c, and dd such that k is a root of both the equations: ax3+bx2+cx+d=0 and bx3+cx2+dx+a=0. Find all possible values of k (including complex values). # Understanding the Problem First, I need to understand what's being asked. We have two cubic equations: The first equation is P(x)=ax3+bx2+cx+d=0. The second equation is Q(x)=bx3+cx2+dx+a=0. A complex number k is a common root of both P and Q. The coefficients a,b,c,d are all nonzero complex numbers. The goal is to find all possible values of k that satisfy both equations &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Qwen3 32B:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Okay, so I need to find all possible values of k that are roots of both equations: ax¬≥ + bx¬≤ + cx + d = 0 and bx¬≥ + cx¬≤ + dx + a = 0. The coefficients a, b, c, d are nonzero complex numbers. Hmm, let me think about how to approach this. First, since k is a root of both equations, substituting k into both should satisfy them. Let me write that down: For the first equation: a*k¬≥ + b*k¬≤ + c\*k + d = 0. (1) For the second equation: b*k¬≥ + c*k¬≤ + d\*k + a = 0. (2) So, I have two equations here with the same root k. Maybe I can manipulate these equations to find a relationship between them? Let me try to express equation (1) and equation (2) in terms of each other. If I can find a way to relate them, perhaps by dividing or subtracting? ... &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImprovementBusy5947"&gt; /u/ImprovementBusy5947 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm93yi/magistral_small_similarity_to_deepseek_chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm93yi/magistral_small_similarity_to_deepseek_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm93yi/magistral_small_similarity_to_deepseek_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T00:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1llhdoq</id>
    <title>I'm using a local Llama model for my game's dialogue system!</title>
    <updated>2025-06-27T01:23:40+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt; &lt;img alt="I'm using a local Llama model for my game's dialogue system!" src="https://external-preview.redd.it/c2JvZG9ndjVnZDlmMe7CY4SqtJeZEukasJn79Adjh2cJgmt44HDkzVTcUucN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a31f419b54bcf613f907d27abae7c2526e8092" title="I'm using a local Llama model for my game's dialogue system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm blown away by how fast and intelligent Llama 3.2 is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgoobkv5gd9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T01:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1llsztp</id>
    <title>What I Learned Building Agents for Enterprises</title>
    <updated>2025-06-27T12:46:41+00:00</updated>
    <author>
      <name>/u/Beneficial-Sir-6261</name>
      <uri>https://old.reddit.com/user/Beneficial-Sir-6261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üè¶ For the past 3 months, we've been developing AI agents together with banks, fintechs, and software companies. The most critical point I've observed during this process is: Agentic transformation will be a painful process, just like digital transformation. What I learned in the field:üëá&lt;/p&gt; &lt;p&gt;1- Definitions related to artificial intelligence are not yet standardized. Even the definition of &amp;quot;AI agent&amp;quot; differs between parties in meetings.&lt;/p&gt; &lt;p&gt;2- Organizations typically develop simple agents. They are far from achieving real-world transformation. To transform a job that generates ROI, an average of 20 agents need to work together or separately.&lt;/p&gt; &lt;p&gt;3- Companies initially want to produce a basic working prototype. Everyone is ready to allocate resources after seeing real ROI. But there's an important point. High performance is expected from small models running on a small amount of GPU, and the success of these models is naturally low. Therefore, they can't get out of the test environment and the business turns into a chicken-and-egg problem.üê•&lt;/p&gt; &lt;p&gt;4- Another important point in agentic transformation is that significant changes need to be made in the use of existing tools according to the agent to be built. Actions such as UI changes in used applications and providing new APIs need to be taken. This brings many arrangements with it.üå™Ô∏è&lt;/p&gt; &lt;p&gt;ü§∑‚Äç‚ôÇÔ∏è An important problem we encounter with agents is the excitement about agents. This situation causes us to raise our expectations from agents. There are two critical points to pay attention to:&lt;/p&gt; &lt;p&gt;1- Avoid using agents unnecessarily. Don't try to use agents for tasks that can be solved with software. Agents should be used as little as possible. Because software is deterministic - we can predict the next step with certainty. However, we cannot guarantee 100% output quality from agents. Therefore, we should use agents only at points where reasoning is needed.&lt;/p&gt; &lt;p&gt;2- Due to MCP and Agent excitement, we see technologies being used in the wrong places. There's justified excitement about MCP in the sector. We brought MCP support to our framework in the first month it was released, and we even prepared a special page on our website explaining the importance of MCP when it wasn't popular yet. MCP is a very important technology. However, this should not be forgotten: if you can solve a problem with classical software methods, you shouldn't try to solve it using tool calls (MCP or agent) or LLM. It's necessary to properly orchestrate the technologies and concepts emerging with agents.üéª&lt;/p&gt; &lt;p&gt;If you can properly orchestrate agents and choose the right agentic transformation points, productivity increases significantly with agents. At one of our clients, a job that took 1 hour was reduced to 5 minutes. The 5 minutes also require someone to perform checks related to the work done by the Agent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Sir-6261"&gt; /u/Beneficial-Sir-6261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llsztp/what_i_learned_building_agents_for_enterprises/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T12:46:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmictu</id>
    <title>We created world's first AI model that does Intermediate reasoning || Defeated models like deepseek and o1 in maths bench mark</title>
    <updated>2025-06-28T09:05:06+00:00</updated>
    <author>
      <name>/u/Quiet-Moment-338</name>
      <uri>https://old.reddit.com/user/Quiet-Moment-338</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We at HelpingAI were fed up with thinking model taking so much tokens, and being very pricy. So, we decided to take a very different approach towards reasoning. Unlike, traditional ai models which reasons on top and then generate response, our ai model do reasoning in middle of response (Intermediate reasoning). Which decreases it's token consumption and time taken by a footfall.&lt;/p&gt; &lt;p&gt;Our model:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img 6hp6bcl9g09f1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Deepseek:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img szpxd7ebg09f1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We have finetuned an existing model named Qwen-14B, because of lack of resources. We have pretrained many models in our past.&lt;/p&gt; &lt;p&gt;We ran this model through a series of benchmarks like math-500 (where it scored 95.68) and AIME (where it scored 82). Making it just below gemini-2.5-pro :-&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img qdzwioepg09f1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;We are planning to make this model open weight on 1 July. Till then you can chat with it on &lt;a href="http://helpingai.co"&gt;helpingai.co&lt;/a&gt; .&lt;/p&gt; &lt;p&gt;Please give us feedback on which we can improve upon :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Quiet-Moment-338"&gt; /u/Quiet-Moment-338 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmictu/we_created_worlds_first_ai_model_that_does/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T09:05:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1llqp0a</id>
    <title>The more LLMs think, the worse they translate</title>
    <updated>2025-06-27T10:41:40+00:00</updated>
    <author>
      <name>/u/Nuenki</name>
      <uri>https://old.reddit.com/user/Nuenki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/"&gt; &lt;img alt="The more LLMs think, the worse they translate" src="https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c116c7e6295d776b6382e425434256d0d8559943" title="The more LLMs think, the worse they translate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nuenki"&gt; /u/Nuenki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nuenki.app/blog/the_more_llms_think_the_worse_they_translate"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T10:41:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmc6dp</id>
    <title>Is there a open source equivalent of Google's Gemini-Diffusion model?</title>
    <updated>2025-06-28T02:42:17+00:00</updated>
    <author>
      <name>/u/GullibleEngineer4</name>
      <uri>https://old.reddit.com/user/GullibleEngineer4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This thing is insane. Any leads on an open source equivalent? &lt;/p&gt; &lt;p&gt;Additionally, does anyone have a rough idea of how large is the underlying model for Gemini-Diffusion?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleEngineer4"&gt; /u/GullibleEngineer4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmc6dp/is_there_a_open_source_equivalent_of_googles/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmc6dp/is_there_a_open_source_equivalent_of_googles/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmc6dp/is_there_a_open_source_equivalent_of_googles/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T02:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1llwfwv</id>
    <title>Qwen VLo: From "Understanding" the World to "Depicting" It</title>
    <updated>2025-06-27T15:15:25+00:00</updated>
    <author>
      <name>/u/Additional_Top1210</name>
      <uri>https://old.reddit.com/user/Additional_Top1210</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/"&gt; &lt;img alt="Qwen VLo: From &amp;quot;Understanding&amp;quot; the World to &amp;quot;Depicting&amp;quot; It" src="https://external-preview.redd.it/p-RdsB-v9L-CFrA5EkxqdVn1O17bnDolUwqTorCzqTE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cf118a7b3e066763df86407692a4a20da4c744d0" title="Qwen VLo: From &amp;quot;Understanding&amp;quot; the World to &amp;quot;Depicting&amp;quot; It" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://qwenlm.github.io/blog/qwen-vlo/"&gt;https://qwenlm.github.io/blog/qwen-vlo/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Top1210"&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1llwfwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llwfwv/qwen_vlo_from_understanding_the_world_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:15:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1llndut</id>
    <title>Hunyuan-A13B released</title>
    <updated>2025-06-27T06:59:21+00:00</updated>
    <author>
      <name>/u/kristaller486</name>
      <uri>https://old.reddit.com/user/kristaller486</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt; &lt;img alt="Hunyuan-A13B released" src="https://external-preview.redd.it/B1uwVS2BmhDOjFW0XJ6pW7-r7n5zECGun4YlOmky9YY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=975cbb18dc0dd9f2342d47d40a0f9fb8fe177327" title="Hunyuan-A13B released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From HF repo:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Model Introduction&lt;/p&gt; &lt;p&gt;With the rapid advancement of artificial intelligence technology, large language models (LLMs) have achieved remarkable progress in natural language processing, computer vision, and scientific tasks. However, as model scales continue to expand, optimizing resource consumption while maintaining high performance has become a critical challenge. To address this, we have explored Mixture of Experts (MoE) architectures. The newly introduced Hunyuan-A13B model features a total of 80 billion parameters with 13 billion active parameters. It not only delivers high-performance results but also achieves optimal resource efficiency, successfully balancing computational power and resource utilization.&lt;/p&gt; &lt;p&gt;Key Features and Advantages&lt;/p&gt; &lt;p&gt;Compact yet Powerful: With only 13 billion active parameters (out of a total of 80 billion), the model delivers competitive performance on a wide range of benchmark tasks, rivaling much larger models.&lt;/p&gt; &lt;p&gt;Hybrid Inference Support: Supports both fast and slow thinking modes, allowing users to flexibly choose according to their needs.&lt;/p&gt; &lt;p&gt;Ultra-Long Context Understanding: Natively supports a 256K context window, maintaining stable performance on long-text tasks.&lt;/p&gt; &lt;p&gt;Enhanced Agent Capabilities: Optimized for agent tasks, achieving leading results on benchmarks such as BFCL-v3 and œÑ-Bench.&lt;/p&gt; &lt;p&gt;Efficient Inference: Utilizes Grouped Query Attention (GQA) and supports multiple quantization formats, enabling highly efficient inference.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kristaller486"&gt; /u/kristaller486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Hunyuan-A13B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llndut/hunyuana13b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T06:59:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm9012</id>
    <title>I keep returning to Llama-3.1-8B</title>
    <updated>2025-06-27T23:58:14+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on porting a GPT-4.1 project over to an open-source model to deal with a GDPR-compliant client. The task is basically fine-tuning the model to classify text in a western European language.&lt;/p&gt; &lt;p&gt;I tried Qwen3 (0.6B, 1.7B, 8B) without making much progress (the fine-tuned model is far behind GPT-4.1) and finally went back to Llama-3.1-8B, which was what worked for me over a year ago. This is super surprising to me, because Qwen3's zero-shot performance in English is almost 2x that of Llama's for similar model sizes.&lt;/p&gt; &lt;p&gt;Does anyone else run fine-tuning heavy workloads in European languages? What's the best model for this workload that I can fine-tune on an H100 96GB (note: I don't do PEFT)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm9012/i_keep_returning_to_llama318b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T23:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm3jvm</id>
    <title>Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.</title>
    <updated>2025-06-27T20:00:37+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/"&gt; &lt;img alt="Arch-Router: The first (and fastest) LLM router that can align to your usage preferences." src="https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b84cb94dea055e799bbb2285e64e2b597538da36" title="Arch-Router: The first (and fastest) LLM router that can align to your usage preferences." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and gotchas. For example:&lt;/p&gt; &lt;p&gt;‚ÄúEmbedding-based‚Äù (or simple intent-classifier) routers sound good on paper‚Äîlabel each prompt via embeddings as ‚Äúsupport,‚Äù ‚ÄúSQL,‚Äù ‚Äúmath,‚Äù then hand it to the matching model‚Äîbut real chats don‚Äôt stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that can‚Äôt keep up with multi-turn conversations or fast-moving product requirements.&lt;/p&gt; &lt;p&gt;&amp;quot;Performance-based&amp;quot; routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: ‚ÄúWill Legal accept this clause?‚Äù ‚ÄúDoes our support tone still feel right?‚Äù Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language.&lt;/em&gt;&lt;/strong&gt; Drop rules like ‚Äúcontract clauses ‚Üí GPT-4o‚Äù or ‚Äúquick travel tips ‚Üí Gemini-Flash,‚Äù and our 1.5B auto-regressive router model maps prompt along with the context to your routing policies‚Äîno retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; ‚Äì 1.5 B params ‚Üí runs on one modern GPU (or CPU while you play).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; ‚Äì points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; ‚Äì beats bigger closed models on conversational datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; ‚Äì push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br /&gt; üîó Model + code: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br /&gt; üìÑ Paper / longer read: &lt;a href="https://arxiv.org/abs/2506.16655"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6zqw0rkhzi9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T20:00:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1llx4ky</id>
    <title>Prime Intellect: We did it ‚Äî SYNTHETIC‚Äë2 is complete.</title>
    <updated>2025-06-27T15:42:21+00:00</updated>
    <author>
      <name>/u/Marha01</name>
      <uri>https://old.reddit.com/user/Marha01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"&gt; &lt;img alt="Prime Intellect: We did it ‚Äî SYNTHETIC‚Äë2 is complete." src="https://external-preview.redd.it/FouZOpBR8n9C_WGYTOTMN6i2egUkQFWjKrxslBsNmKU.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebd1a569f5715c190c324ddbb7cf8ca8b9e4815d" title="Prime Intellect: We did it ‚Äî SYNTHETIC‚Äë2 is complete." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Marha01"&gt; /u/Marha01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/PrimeIntellect/status/1938490370054361422"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llx4ky/prime_intellect_we_did_it_synthetic2_is_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T15:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmb5s3</id>
    <title>[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken</title>
    <updated>2025-06-28T01:47:50+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"&gt; &lt;img alt="[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken" src="https://a.thumbs.redditmedia.com/bZuU8fFxF1Xzopfioxwh3cANnUztVf0ibuxGQD5IFI4.jpg" title="[Day 5/50] Building a Small Language Model from Scratch - Byte Pair Encoding with tiktoken" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a"&gt;https://preview.redd.it/0579fta7pk9f1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e94c97bc4d62edd9d41575d48948d6ae60d6ce3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone!&lt;br /&gt; We‚Äôve made it to Day 5 of the &lt;em&gt;50 Days of Building a Small Language Model from Scratch&lt;/em&gt; journey.&lt;/p&gt; &lt;p&gt;So far, we‚Äôve covered the basics of what a small language model is, built our own tokenizer from scratch, and identified a major pain point: handling unknown or rare words. That‚Äôs where today's Byte Pair Encoding (BPE) comes in&lt;/p&gt; &lt;p&gt;Instead of creating everything from the ground up, we‚Äôve now switched gears to use OpenAI‚Äôs &lt;code&gt;tiktoken&lt;/code&gt; library, which powers the GPT-2 tokenizer. It's fast, memory-efficient, and trained on a broad range of English text, making it perfect for small to mid-size model experiments.&lt;/p&gt; &lt;p&gt;But we‚Äôre not just plugging in a tokenizer. We‚Äôre also designing it for storytelling use cases. That means adding special tokens like &lt;code&gt;&amp;lt;|startofstory|&amp;gt;&lt;/code&gt; and &lt;code&gt;&amp;lt;|title|&amp;gt;&lt;/code&gt; to guide our model and give it a narrative structure. These little markers help the model &amp;quot;think&amp;quot; like a storyteller.&lt;/p&gt; &lt;p&gt;Before tokenization occurs, we run a cleaning step that normalizes text, trims unnecessary whitespace, and converts it to lowercase, ensuring our inputs are clean and consistent. It‚Äôs a small step that makes a big difference.&lt;/p&gt; &lt;p&gt;This is how we process the data:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each sample gets wrapped with special tokens.&lt;/li&gt; &lt;li&gt;We tokenize with error handling.&lt;/li&gt; &lt;li&gt;We cap token sequences at 1024 to fit the GPT-2 context window.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;From there, we move on to dataset loading. We‚Äôre using a curated collection of children‚Äôs stories and filtering them by token length to ensure quality inputs. We split everything into train, validation, and fine-tune subsets.&lt;/p&gt; &lt;p&gt;Then comes the heavy lifting:&lt;br /&gt; We tokenize the dataset using 8 parallel processes and store the results in binary format using memory-mapped NumPy arrays. This setup enables us to efficiently read large datasets during training without encountering memory issues.&lt;/p&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Wrapping Up Week 1&lt;/strong&gt;&lt;br /&gt; With BPE and &lt;code&gt;tiktoken&lt;/code&gt;We‚Äôve built a solid, scalable preprocessing pipeline tailored for training small LLMs. Next week, we start tackling the model itself.&lt;/p&gt; &lt;p&gt;üîó Complete blog: &lt;a href="https://www.ideaweaver.ai/blog/day5.html"&gt;https://www.ideaweaver.ai/blog/day5.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for following along. If you're building your own LLM or are just curious about the process, feel free to drop a comment on LinkedIn. I'm always happy to chat!&lt;/p&gt; &lt;p&gt;Stay tuned, and have a great weekend! üöÄ&lt;br /&gt; ‚Äî Prashant Lakhera&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmb5s3/day_550_building_a_small_language_model_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T01:47:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmgdw1</id>
    <title>How do I stop gemnini 2.5 pro from being overly sycophantic? It has gotten very excessive and feels like it degrades the answers it gives.</title>
    <updated>2025-06-28T06:52:00+00:00</updated>
    <author>
      <name>/u/Commercial-Celery769</name>
      <uri>https://old.reddit.com/user/Commercial-Celery769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every single question/follow up question I ask it acts as if I am a nobel prize winner who cracked fusion energy single handedly. Its always something like &amp;quot;Thats an outstanding and very insightful question.&amp;quot; Or &amp;quot;That is the perfect question to ask&amp;quot; or &amp;quot;you are absolutely correct to provide that snippet&amp;quot; etc. Its very annoying and worrys me that it gives answers it thinks I would like and not whats the best answer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Commercial-Celery769"&gt; /u/Commercial-Celery769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmgdw1/how_do_i_stop_gemnini_25_pro_from_being_overly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T06:52:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm76gk</id>
    <title>Hugging Face releases a 50+ page report on how they built FineWeb2</title>
    <updated>2025-06-27T22:34:23+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/"&gt; &lt;img alt="Hugging Face releases a 50+ page report on how they built FineWeb2" src="https://preview.redd.it/ixin9dvyqj9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4185435146679d75323286fe47669bc3ecf82fc" title="Hugging Face releases a 50+ page report on how they built FineWeb2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ixin9dvyqj9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm76gk/hugging_face_releases_a_50_page_report_on_how/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T22:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm0m6i</id>
    <title>Copilot Chat for VS Code is now Open Source</title>
    <updated>2025-06-27T18:00:56+00:00</updated>
    <author>
      <name>/u/corysama</name>
      <uri>https://old.reddit.com/user/corysama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"&gt; &lt;img alt="Copilot Chat for VS Code is now Open Source" src="https://external-preview.redd.it/tyJeCqipzT78spT8qdYr9nFThGnon2rt0efU2xelzLQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c7ae49e1d763b069953250103aad9e1f240a4f3" title="Copilot Chat for VS Code is now Open Source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/corysama"&gt; /u/corysama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/microsoft/vscode-copilot-chat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm0m6i/copilot_chat_for_vs_code_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm98z7</id>
    <title>Automated GPU kernel optimization for Qwen3 attention - 12.5% average speedup on Apple Silicon using evolutionary programming</title>
    <updated>2025-06-28T00:10:14+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! Wanted to share something interesting I've been working on that might be relevant for folks running models locally on Apple Silicon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Used evolutionary programming to automatically optimize Metal GPU kernels for transformer attention. Specifically targeted Qwen3-0.6B's grouped query attention (40:8 head ratio) running on Apple M-series GPUs through MLX.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tested across 20 different inference scenarios against MLX's &lt;code&gt;scaled_dot_product_attention&lt;/code&gt; baseline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Average decode speed improvement: +12.5%&lt;/strong&gt; (œÉ = 38.3%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Peak improvement: +106%&lt;/strong&gt; on repetitive pattern generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best category: +24.8%&lt;/strong&gt; average on general tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory usage: -0.99%&lt;/strong&gt; (slight reduction)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The honest picture:&lt;/strong&gt; It's workload dependent. Some scenarios saw big gains (+46.6% on dialogue, +73.9% on extreme-length generation), but others regressed (-16.5% on code generation). Success rate was 7/20 benchmarks with &amp;gt;25% improvements.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The system automatically evolves the Metal kernel source code using LLMs while preserving the MLX integration. No human GPU programming expertise was provided - it discovered optimizations like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Perfect SIMD vectorization&lt;/strong&gt;: Found that &lt;code&gt;vec&amp;lt;T, 8&amp;gt;&lt;/code&gt; operations match Apple Silicon's capabilities for 128-dim attention heads&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Two-pass online softmax&lt;/strong&gt;: Fused softmax normalization with value accumulation, reducing memory bandwidth&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GQA-specific memory patterns&lt;/strong&gt;: Optimized for the 40:8 head structure with coalesced access patterns&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Why this might matter for local inference&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Shows automated optimization can compete with expert-engineered kernels&lt;/li&gt; &lt;li&gt;Demonstrates potential for hardware-specific optimizations without manual tuning&lt;/li&gt; &lt;li&gt;Could be applied to other transformer components or different model architectures&lt;/li&gt; &lt;li&gt;All open source - you can reproduce and extend this work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The code and all benchmarks are available in the &lt;a href="https://github.com/codelion/openevolve"&gt;OpenEvolve repo&lt;/a&gt;. The MLX kernel optimization example is at &lt;code&gt;examples/mlx_metal_kernel_opt/&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Apple Silicon Mac&lt;/li&gt; &lt;li&gt;MLX framework&lt;/li&gt; &lt;li&gt;Qwen3-0.6B model&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Currently specific to Apple Silicon and this exact model configuration&lt;/li&gt; &lt;li&gt;Performance improvements are highly workload-dependent&lt;/li&gt; &lt;li&gt;Takes ~25 evolutionary generations to converge (few hours on M3)&lt;/li&gt; &lt;li&gt;No guarantees it'll work better for your specific use case&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical write-up&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Full details with code diffs and benchmark methodology: &lt;a href="https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery"&gt;https://huggingface.co/blog/codelion/openevolve-gpu-kernel-discovery&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious to hear thoughts from folks who've done MLX optimization work, or if anyone wants to try this on different models/configurations. The evolutionary approach seems promising but definitely has room for improvement.&lt;/p&gt; &lt;p&gt;Has anyone else experimented with automated kernel optimization for local inference?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm98z7/automated_gpu_kernel_optimization_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T00:10:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm92se</id>
    <title>Qwen3 Coder Soon?</title>
    <updated>2025-06-28T00:01:43+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt; &lt;img alt="Qwen3 Coder Soon?" src="https://b.thumbs.redditmedia.com/KDJV-rJVdBsUxEikcR6mcx63y02QfY38vq7JUDazoWM.jpg" title="Qwen3 Coder Soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/415iw73n6k9f1.png?width=1093&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4e66852a8d0b6a8981e1e0f23da6ddfd4d0744c"&gt;https://x.com/huybery/status/1938655788849098805&lt;/a&gt;&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/huybery/status/1938655788849098805"&gt;https://x.com/huybery/status/1938655788849098805&lt;/a&gt;&lt;/p&gt; &lt;p&gt;i hope they release these models soon! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm92se/qwen3_coder_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T00:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1lmfiu9</id>
    <title>I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-</title>
    <updated>2025-06-28T05:57:46+00:00</updated>
    <author>
      <name>/u/irodov4030</name>
      <uri>https://old.reddit.com/user/irodov4030</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt; &lt;img alt="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-" src="https://external-preview.redd.it/lSrPd1MMz7blRmLYLnruRoJd4XS5NpPXF_maDibWecs.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d68c4413ac33077ccb1f955a9767daec572c1df8" title="I tested 10 LLMs locally on my MacBook Air M1 (8GB RAM!) ‚Äì Here's what actually works-" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All feedback is welcome! I am learning how to do better everyday.&lt;/p&gt; &lt;p&gt;I went down the LLM rabbit hole trying to find the &lt;strong&gt;best local model&lt;/strong&gt; that runs &lt;em&gt;well&lt;/em&gt; on a humble MacBook Air M1 with just 8GB RAM.&lt;/p&gt; &lt;p&gt;My goal? &lt;strong&gt;Compare 10 models&lt;/strong&gt; across question generation, answering, and self-evaluation.&lt;/p&gt; &lt;p&gt;TL;DR: Some models were brilliant, others‚Ä¶ not so much. One even took &lt;strong&gt;8 minutes&lt;/strong&gt; to write a question.&lt;/p&gt; &lt;p&gt;Here's the breakdown &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models Tested&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mistral 7B&lt;/li&gt; &lt;li&gt;DeepSeek-R1 1.5B&lt;/li&gt; &lt;li&gt;Gemma3:1b&lt;/li&gt; &lt;li&gt;Gemma3:latest&lt;/li&gt; &lt;li&gt;Qwen3 1.7B&lt;/li&gt; &lt;li&gt;Qwen2.5-VL 3B&lt;/li&gt; &lt;li&gt;Qwen3 4B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 1B&lt;/li&gt; &lt;li&gt;LLaMA 3.2 3B&lt;/li&gt; &lt;li&gt;LLaMA 3.1 8B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(All models run with quantized versions, via: os.environ[&amp;quot;OLLAMA_CONTEXT_LENGTH&amp;quot;] = &amp;quot;4096&amp;quot; and os.environ[&amp;quot;OLLAMA_KV_CACHE_TYPE&amp;quot;] = &amp;quot;q4_0&amp;quot;)&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Methodology&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Each model:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Generated 1 question on 5 topics: &lt;em&gt;Math, Writing, Coding, Psychology, History&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Answered all 50 questions (5 x 10)&lt;/li&gt; &lt;li&gt;Evaluated every answer (including their own)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So in total:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;50 questions&lt;/li&gt; &lt;li&gt;500 answers&lt;/li&gt; &lt;li&gt;4830 evaluations (Should be 5000; I evaluated less answers with qwen3:1.7b and qwen3:4b as they do not generate scores and take a lot of time**)**&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And I tracked:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;token generation speed (tokens/sec)&lt;/li&gt; &lt;li&gt;tokens created&lt;/li&gt; &lt;li&gt;time taken&lt;/li&gt; &lt;li&gt;scored all answers for quality&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Key Results&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt;, &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;Qwen3 1.7B&lt;/strong&gt; (LLaMA 3.2 1B hit 82 tokens/sec, avg is ~40 tokens/sec (for english topic question it reached &lt;strong&gt;146 tokens/sec)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Slowest: &lt;strong&gt;LLaMA 3.1 8B&lt;/strong&gt;, &lt;strong&gt;Qwen3 4B&lt;/strong&gt;, &lt;strong&gt;Mistral 7B&lt;/strong&gt; Qwen3 4B took &lt;strong&gt;486s&lt;/strong&gt; (8+ mins) to generate a single Math question!&lt;/li&gt; &lt;li&gt;Fun fact: deepseek-r1:1.5b, qwen3:4b and Qwen3:1.7B output &amp;lt;think&amp;gt; tags in questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Answer Generation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fastest: &lt;strong&gt;Gemma3:1b&lt;/strong&gt;, &lt;strong&gt;LLaMA 3.2 1B&lt;/strong&gt; and &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek got faster answering &lt;em&gt;its own&lt;/em&gt; questions (80 tokens/s vs. avg 40 tokens/s)&lt;/li&gt; &lt;li&gt;Qwen3 4B generates &lt;strong&gt;2‚Äì3x more tokens&lt;/strong&gt; per answer&lt;/li&gt; &lt;li&gt;Slowest: llama3.1:8b, qwen3:4b and mistral:7b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt; &lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Best scorer: Gemma3:latest ‚Äì consistent, numerical, no bias&lt;/li&gt; &lt;li&gt;Worst scorer: &lt;strong&gt;DeepSeek-R1 1.5B&lt;/strong&gt; ‚Äì often skipped scores entirely&lt;/li&gt; &lt;li&gt;Bias detected: Many models &lt;strong&gt;rate their own answers higher&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;DeepSeek even evaluated some answers &lt;strong&gt;in Chinese&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;I did think of creating a control set of answers. I could tell the mdoel this is the perfect answer basis this rate others. But I did not because it would need support from a lot of people- creating perfect answer, which still can have a bias. I read a few answers and found most of them decent except math. So I tried to find which model's evaluation scores were closest to the average to determine a decent model for evaluation tasks(check last image)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Fun Observations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some models create &amp;lt;think&amp;gt; tags for questions, answers and even while evaluation as output&lt;/li&gt; &lt;li&gt;Score inflation is real: Mistral, Qwen3, and LLaMA 3.1 8B overrate themselves&lt;/li&gt; &lt;li&gt;Score formats vary wildly (text explanations vs. plain numbers)&lt;/li&gt; &lt;li&gt;Speed isn‚Äôt everything ‚Äì some slower models gave much higher quality answers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Best Performers (My Picks)&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Best Model&lt;/th&gt; &lt;th align="left"&gt;Why&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 1B&lt;/td&gt; &lt;td align="left"&gt;Fast &amp;amp; relevant&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;Gemma3:1b&lt;/td&gt; &lt;td align="left"&gt;Fast, accurate&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.2 3B&lt;/td&gt; &lt;td align="left"&gt;Generates numerical scores and evaluations closest to model average&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Worst Surprises&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Task&lt;/th&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Problem&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left" colspan="2"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Question Gen&lt;/td&gt; &lt;td align="left"&gt;Qwen3 4B&lt;/td&gt; &lt;td align="left"&gt;Took 486s to generate 1 question&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Answer Gen&lt;/td&gt; &lt;td align="left"&gt;LLaMA 3.1 8B&lt;/td&gt; &lt;td align="left"&gt;Slow&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Evaluation&lt;/td&gt; &lt;td align="left"&gt;DeepSeek-R1 1.5B&lt;/td&gt; &lt;td align="left"&gt;Inconsistent, skipped scores&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Screenshots Galore&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm adding screenshots of:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Questions generation&lt;/li&gt; &lt;li&gt;Answer comparisons&lt;/li&gt; &lt;li&gt;Evaluation outputs&lt;/li&gt; &lt;li&gt;Token/sec charts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You &lt;strong&gt;can&lt;/strong&gt; run decent LLMs locally on M1 Air (8GB) ‚Äì if you pick the right ones&lt;/li&gt; &lt;li&gt;Model size ‚â† performance. Bigger isn't always better.&lt;/li&gt; &lt;li&gt;5 Models have a self bais, they rate their own answers higher than average scores. attaching screen shot of a table. Diagonal is their own evaluation, last column is average.&lt;/li&gt; &lt;li&gt;Models' evaluation has high variance! Every model has a unique distribution of the scores it gave.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Post questions if you have any, I will try to answer.&lt;/p&gt; &lt;p&gt;Happy to share more data if you need.&lt;/p&gt; &lt;p&gt;Open to collaborate on interesting projects! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/irodov4030"&gt; /u/irodov4030 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lmfiu9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lmfiu9/i_tested_10_llms_locally_on_my_macbook_air_m1_8gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-28T05:57:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1lm1v2c</id>
    <title>Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2</title>
    <updated>2025-06-27T18:51:13+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"&gt; &lt;img alt="Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2" src="https://preview.redd.it/ypm4lnr4ni9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f23d8c2da2fff2f8a6b194ee42f06b2d3e90dca" title="Open source model that does photoshop-grade edits without affecting the rest of the pic: OmniGen 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Code: &lt;a href="https://github.com/VectorSpaceLab/OmniGen2"&gt;https://github.com/VectorSpaceLab/OmniGen2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://vectorspacelab.github.io/OmniGen2/"&gt;https://vectorspacelab.github.io/OmniGen2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ypm4lnr4ni9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lm1v2c/open_source_model_that_does_photoshopgrade_edits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T18:51:13+00:00</published>
  </entry>
</feed>
