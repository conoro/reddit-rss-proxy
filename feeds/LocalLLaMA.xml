<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-06T15:18:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qwz0x6</id>
    <title>Any feedback on step-3.5-flash ?</title>
    <updated>2026-02-05T21:58:09+00:00</updated>
    <author>
      <name>/u/Jealous-Astronaut457</name>
      <uri>https://old.reddit.com/user/Jealous-Astronaut457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It was overshadowed by qwen3-next-coder and was not supported by llamacpp at launch, but it looks like a very promising model for local inference. My first impression of stepfun's chat is that the model is a thinker, but what are your impressions few days after the release ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jealous-Astronaut457"&gt; /u/Jealous-Astronaut457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwz0x6/any_feedback_on_step35flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwz0x6/any_feedback_on_step35flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwz0x6/any_feedback_on_step35flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T21:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwue2w</id>
    <title>SoproTTS v1.5: A 135M zero-shot voice cloning TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU</title>
    <updated>2026-02-05T19:08:29+00:00</updated>
    <author>
      <name>/u/SammyDaBeast</name>
      <uri>https://old.reddit.com/user/SammyDaBeast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"&gt; &lt;img alt="SoproTTS v1.5: A 135M zero-shot voice cloning TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU" src="https://external-preview.redd.it/CWCqETRtx5uPv_SVUC4tOtF3EsWY3Pg-rooYdIufOP0.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=3e1da0ab72d9427c772c087709903112c521ae66" title="SoproTTS v1.5: A 135M zero-shot voice cloning TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, thank you for the support on my first release. &lt;/p&gt; &lt;p&gt;Today, I'm releasing a new version of my side project: SoproTTS &lt;/p&gt; &lt;p&gt;A 135M parameter TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU. &lt;/p&gt; &lt;p&gt;v1.5 highlights (on CPU): &lt;/p&gt; &lt;p&gt;‚Ä¢ 250 ms TTFA streaming latency&lt;br /&gt; ‚Ä¢ 0.05 RTF (~20√ó real-time)&lt;br /&gt; ‚Ä¢ Zero-shot voice cloning&lt;br /&gt; ‚Ä¢ Smaller, faster, more stable &lt;/p&gt; &lt;p&gt;Still not perfect (OOD voices can be tricky, and there are still some artifacts), but a decent upgrade. Training code TBA.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/samuel-vitorino/sopro"&gt;https://github.com/samuel-vitorino/sopro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qwue2w/video/y114to0a2qhg1/player"&gt;https://reddit.com/link/1qwue2w/video/y114to0a2qhg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SammyDaBeast"&gt; /u/SammyDaBeast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T19:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxhmu5</id>
    <title>Is there still no way to convert Gemma 3n to onnx/tflite?</title>
    <updated>2026-02-06T13:28:53+00:00</updated>
    <author>
      <name>/u/blueblazd</name>
      <uri>https://old.reddit.com/user/blueblazd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been months since gemma's release and i need to convert my fine tuned gemma 3n to either onnx, tflite or litert lm to deploy on mobile. After many trials i failed and can not find any guide at all to do so. Was no one able to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blueblazd"&gt; /u/blueblazd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxhmu5/is_there_still_no_way_to_convert_gemma_3n_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxhmu5/is_there_still_no_way_to_convert_gemma_3n_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxhmu5/is_there_still_no_way_to_convert_gemma_3n_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T13:28:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwrpom</id>
    <title>really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025</title>
    <updated>2026-02-05T17:33:54+00:00</updated>
    <author>
      <name>/u/datascienceharp</name>
      <uri>https://old.reddit.com/user/datascienceharp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"&gt; &lt;img alt="really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025" src="https://b.thumbs.redditmedia.com/lemBjuywLXHSh55oKPeQldeNetPiKExhrecGB4VPRXY.jpg" title="really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gif 1: LightOnOCR-2-1B&lt;/p&gt; &lt;p&gt;docs page: &lt;a href="https://docs.voxel51.com/plugins/plugins_ecosystem/lightonocr_2.html"&gt;https://docs.voxel51.com/plugins/plugins_ecosystem/lightonocr_2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quickstart nb: &lt;a href="https://github.com/harpreetsahota204/LightOnOCR-2/blob/main/lightonocr2_fiftyone_example.ipynb"&gt;https://github.com/harpreetsahota204/LightOnOCR-2/blob/main/lightonocr2_fiftyone_example.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gif 2: GLM-OCR&lt;/p&gt; &lt;p&gt;docs page: &lt;a href="https://docs.voxel51.com/plugins/plugins_ecosystem/glm_ocr.html"&gt;https://docs.voxel51.com/plugins/plugins_ecosystem/glm_ocr.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quickstart nb: &lt;a href="https://github.com/harpreetsahota204/glm_ocr/blob/main/glm_ocr_fiftyone_example.ipynb"&gt;https://github.com/harpreetsahota204/glm_ocr/blob/main/glm_ocr_fiftyone_example.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;imo, glm-ocr takes the cake. much faster, and you can get pretty reliable structured output &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/datascienceharp"&gt; /u/datascienceharp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwrpom"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T17:33:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx4alp</id>
    <title>Qwen3-Coder-Next; Unsloth Quants having issues calling tools?</title>
    <updated>2026-02-06T01:40:16+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is regarding Q4 and Q5 quants that I've tried.&lt;/p&gt; &lt;p&gt;Qwen3-Coder-Next seems to write good code, but man does it keep erroring out on tool calls!&lt;/p&gt; &lt;p&gt;Rebuilt llama CPP from latest a few days ago. The errors don't seem to bubble up to the tool I'm using (Claude Code, Qwen-Code) but rather in the llama-cpp logs, and it seems to be a bunch of regex that's different each time.&lt;/p&gt; &lt;p&gt;Are there known issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T01:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxh0xg</id>
    <title>One 3090 or two 5060 ti 16gb?</title>
    <updated>2026-02-06T13:02:42+00:00</updated>
    <author>
      <name>/u/Dentifrice</name>
      <uri>https://old.reddit.com/user/Dentifrice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I‚Äôm wondering if I should buy a used 3090 24gb or two brand new 5060 ti 16gb&lt;/p&gt; &lt;p&gt;3090 is more powerful but I remember seeing that series 50xx has features useful for AI that 3090 don‚Äôt.&lt;/p&gt; &lt;p&gt;I would also have more ram with the 5060.&lt;/p&gt; &lt;p&gt;But does it work great with 2 cards? Ollama for example?&lt;/p&gt; &lt;p&gt;I‚Äôm also considering going the very cheap way of buying only one 5060.&lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dentifrice"&gt; /u/Dentifrice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxh0xg/one_3090_or_two_5060_ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxh0xg/one_3090_or_two_5060_ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxh0xg/one_3090_or_two_5060_ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T13:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxciqr</id>
    <title>Mitchell Hashimoto (author of Ghostty): My AI Adoption Journey</title>
    <updated>2026-02-06T08:50:21+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mitchellh.com/writing/my-ai-adoption-journey"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxciqr/mitchell_hashimoto_author_of_ghostty_my_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxciqr/mitchell_hashimoto_author_of_ghostty_my_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:50:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxbl7j</id>
    <title>Kimi K2.5 on 4x RTX 6000 Pro Blackwell runpod Benchmarks</title>
    <updated>2026-02-06T07:52:29+00:00</updated>
    <author>
      <name>/u/skysthelimit187</name>
      <uri>https://old.reddit.com/user/skysthelimit187</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test the performance of Kimi K2.5 (mainly TTFT and Tok/s) on a Setup with 4x RTX 6000 Pro Blackwell. So I rented a system on runpod (for ~7$ per hour).&lt;/p&gt; &lt;p&gt;Problem is I am a absolute beginner in Terms of Local LLMs. I figured that SGLang with KT-Kernel seem to be a good way for performance, if the entire model does not fit into VRAM.&lt;/p&gt; &lt;p&gt;My whole command line looks like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; python3 -m sglang.launch_server \ --host 0.0.0.0 \ --port 8090 \ --model /workspace/models/Kimi-K2.5 \ --tp-size 4 \ --kt-weight-path /workspace/models/Kimi-K2.5 \ --kt-cpuinfer 128 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 180 \ --kt-method RAWINT4 \ --kt-gpu-prefill-token-threshold 2048 \ --mem-fraction-static 0.85 \ --trust-remote-code \ --served-model-name Kimi-K2.5 \ --reasoning-parser kimi_k2 \ --tool-call-parser kimi_k2 \ --enable-mixed-chunk \ --attention-backend flashinfer \ --context-length 131072 \ --max-total-tokens 150000 \ --enable-p2p-check &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here are benchmark results with diffferent parameters:&lt;/p&gt; &lt;p&gt;``` python3 -m sglang.bench_serving --host 127.0.0.1 --port 8090 --dataset-name sharegpt --num-prompts 100&lt;/p&gt; &lt;p&gt;Kimi-K2.5 4x RTX 6000 PRO --mem-fraction-static 0.90 --kt-num-gpu-experts 20 --kt-gpu-prefill-token-threshold 1000 ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 100&lt;br /&gt; Benchmark duration (s): 797.57&lt;br /&gt; Total input tokens: 33147&lt;br /&gt; Total input text tokens: 33147&lt;br /&gt; Total generated tokens: 21350&lt;br /&gt; Total generated tokens (retokenized): 21343&lt;br /&gt; Request throughput (req/s): 0.13&lt;br /&gt; Input token throughput (tok/s): 41.56&lt;br /&gt; Output token throughput (tok/s): 26.77&lt;br /&gt; Peak output token throughput (tok/s): 99.00&lt;br /&gt; Peak concurrent requests: 100&lt;br /&gt; Total token throughput (tok/s): 68.33&lt;br /&gt; Concurrency: 40.28&lt;br /&gt; ----------------End-to-End Latency---------------- Mean E2E Latency (ms): 321229.26 Median E2E Latency (ms): 302115.02 P90 E2E Latency (ms): 649477.80 P99 E2E Latency (ms): 734740.50 ---------------Time to First Token---------------- Mean TTFT (ms): 43683.46&lt;br /&gt; Median TTFT (ms): 39622.10&lt;br /&gt; P99 TTFT (ms): 63386.48&lt;br /&gt; -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 2308.10&lt;br /&gt; Median TPOT (ms): 1744.01&lt;br /&gt; P99 TPOT (ms): 7974.68&lt;br /&gt; ---------------Inter-Token Latency---------------- Mean ITL (ms): 1306.10&lt;br /&gt; Median ITL (ms): 1376.37&lt;br /&gt; P95 ITL (ms): 1999.40&lt;br /&gt; P99 ITL (ms): 5206.45 &lt;/p&gt; &lt;h1&gt;Max ITL (ms): 12761.78 &lt;/h1&gt; &lt;p&gt;Kimi-K2.5 4x RTX 6000 PRO --mem-fraction-static 0.80 --kt-num-gpu-experts 64 --kt-gpu-prefill-token-threshold 2048 ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 100&lt;br /&gt; Benchmark duration (s): 720.88&lt;br /&gt; Total input tokens: 33147&lt;br /&gt; Total input text tokens: 33147&lt;br /&gt; Total generated tokens: 21350&lt;br /&gt; Total generated tokens (retokenized): 21345&lt;br /&gt; Request throughput (req/s): 0.14&lt;br /&gt; Input token throughput (tok/s): 45.98&lt;br /&gt; Output token throughput (tok/s): 29.62&lt;br /&gt; Peak output token throughput (tok/s): 99.00&lt;br /&gt; Peak concurrent requests: 100&lt;br /&gt; Total token throughput (tok/s): 75.60&lt;br /&gt; Concurrency: 42.07&lt;br /&gt; ----------------End-to-End Latency---------------- Mean E2E Latency (ms): 303249.40 Median E2E Latency (ms): 285529.22 P90 E2E Latency (ms): 593663.77 P99 E2E Latency (ms): 666586.61 ---------------Time to First Token---------------- Mean TTFT (ms): 49258.67&lt;br /&gt; Median TTFT (ms): 44937.76&lt;br /&gt; P99 TTFT (ms): 68691.17&lt;br /&gt; -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 2227.62&lt;br /&gt; Median TPOT (ms): 1599.91&lt;br /&gt; P99 TPOT (ms): 7969.61&lt;br /&gt; ---------------Inter-Token Latency---------------- Mean ITL (ms): 1195.25&lt;br /&gt; Median ITL (ms): 1293.28&lt;br /&gt; P95 ITL (ms): 2125.91&lt;br /&gt; P99 ITL (ms): 5073.84 &lt;/p&gt; &lt;h1&gt;Max ITL (ms): 13245.65 &lt;/h1&gt; &lt;p&gt;Kimi-K2.5 4x RTX 6000 PRO --mem-fraction-static 0.85 --kt-num-gpu-experts 180 --kt-gpu-prefill-token-threshold 2048 ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 100&lt;br /&gt; Benchmark duration (s): 569.87&lt;br /&gt; Total input tokens: 33147&lt;br /&gt; Total input text tokens: 33147&lt;br /&gt; Total generated tokens: 21350&lt;br /&gt; Total generated tokens (retokenized): 21346&lt;br /&gt; Request throughput (req/s): 0.18&lt;br /&gt; Input token throughput (tok/s): 58.17&lt;br /&gt; Output token throughput (tok/s): 37.46&lt;br /&gt; Peak output token throughput (tok/s): 123.00&lt;br /&gt; Peak concurrent requests: 100&lt;br /&gt; Total token throughput (tok/s): 95.63&lt;br /&gt; Concurrency: 44.35&lt;br /&gt; ----------------End-to-End Latency---------------- Mean E2E Latency (ms): 252740.99 Median E2E Latency (ms): 240023.88 P90 E2E Latency (ms): 448283.65 P99 E2E Latency (ms): 505817.34 ---------------Time to First Token---------------- Mean TTFT (ms): 75851.65&lt;br /&gt; Median TTFT (ms): 70053.38&lt;br /&gt; P99 TTFT (ms): 99228.64&lt;br /&gt; -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 1908.22&lt;br /&gt; Median TPOT (ms): 1081.44&lt;br /&gt; P99 TPOT (ms): 9853.65&lt;br /&gt; ---------------Inter-Token Latency---------------- Mean ITL (ms): 832.42&lt;br /&gt; Median ITL (ms): 774.26&lt;br /&gt; P95 ITL (ms): 1237.89&lt;br /&gt; P99 ITL (ms): 2973.36 &lt;/p&gt; &lt;h1&gt;Max ITL (ms): 22928.28 &lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Do you have any suggestions on how to tweak this better?&lt;/p&gt; &lt;p&gt;If you are asking yourself why I am testing this o 4x RTX 6000 Pro Bw? I want to buy a Dell Precision7960 Tower Workstation with that Setup to run large Models like Kimi K2.5. It cost around 90k ‚Ç¨.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skysthelimit187"&gt; /u/skysthelimit187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbl7j/kimi_k25_on_4x_rtx_6000_pro_blackwell_runpod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbl7j/kimi_k25_on_4x_rtx_6000_pro_blackwell_runpod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbl7j/kimi_k25_on_4x_rtx_6000_pro_blackwell_runpod/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T07:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxib19</id>
    <title>Qwen3-Coder-Next 80B (GGUF/BF16) on Zen 5 EPYC: 12-channel DDR5 &amp; NVFP4 bench</title>
    <updated>2026-02-06T13:57:21+00:00</updated>
    <author>
      <name>/u/Express-Jicama-9827</name>
      <uri>https://old.reddit.com/user/Express-Jicama-9827</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxib19/qwen3codernext_80b_ggufbf16_on_zen_5_epyc/"&gt; &lt;img alt="Qwen3-Coder-Next 80B (GGUF/BF16) on Zen 5 EPYC: 12-channel DDR5 &amp;amp; NVFP4 bench" src="https://preview.redd.it/gtb1luh2rvhg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2ff57c3af73db004e97089d43b8e77d12232bdd4" title="Qwen3-Coder-Next 80B (GGUF/BF16) on Zen 5 EPYC: 12-channel DDR5 &amp;amp; NVFP4 bench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Qwen3-Coder-Next (approx. 80B params)&lt;/strong&gt;. This time, I moved away from quantization and tested the &lt;strong&gt;full BF16 (unquantized weights)&lt;/strong&gt; to see if high-precision coding tasks are viable on a 12-channel CPU setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; Running 80B BF16 on a 12-channel Zen 5 system is surprisingly practical. I‚Äôm seeing a stable &lt;strong&gt;~7.8 tok/s decode&lt;/strong&gt;, which is plenty for a &amp;quot;background&amp;quot; coding assistant or local code reviewer where you value reasoning and precision over raw speed.&lt;/p&gt; &lt;h1&gt;Hardware / Runtime&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 9175F (16 Cores / 32 Threads, Zen 5, 512MB L3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 768GB DDR5 (12-Channel,6000 MT/s; DIMMs are 6400-rated but capped by the MB)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; Not used (CPU-only inference)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Ubuntu 24.04&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runtime:&lt;/strong&gt; llama.cpp&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;e.g&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;podman run --rm -p 8081:8080 --shm-size 16g --cap-add=SYS_NICE -v /mnt/data/hf/hub/models--unsloth--Qwen3-Coder-Next-GGUF:/models:Z compute.home.arpa/llamacpp-zen5:qwen3-coder-next -m /models/snapshots/96ab45bf06d904ee251044b0679df08f668677d2/BF16/Qwen3-Coder-Next-BF16-00001-of-00004.gguf --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on --ctx-size 16384 --parallel 1 --threads 13 --threads-batch 13 --batch-size 2048 --ubatch-size 512 --jinja --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Model Settings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Qwen3-Coder-Next (~80B)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quant:&lt;/strong&gt; &lt;strong&gt;BF16&lt;/strong&gt; (unsloth/Qwen3-Coder-Next-GGUF/BF16/*)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context:&lt;/strong&gt; 16k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KV Cache:&lt;/strong&gt; q8_0 (Optimized to balance precision and memory pressure)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads:&lt;/strong&gt; 13 (The &amp;quot;Sweet Spot&amp;quot; identified in my previous post)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance (Real Numbers)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Prompt Processing (Prefill)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Short prompt (~9 tokens):&lt;/strong&gt; &lt;strong&gt;33.37 tok/s&lt;/strong&gt; (warmup-scale)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Realistic prompt (~287 tokens):&lt;/strong&gt; &lt;strong&gt;117.40 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Average PF (realistic):&lt;/strong&gt; &lt;strong&gt;~111‚Äì117 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. Generation (Decode)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Sustainable speed:&lt;/strong&gt; &lt;strong&gt;~7.59 tok/s&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Tested on long generations (~2,233 tokens). Throughput stayed very consistent.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3. TTFT (Estimated)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;~2.58s&lt;/strong&gt; for a &lt;strong&gt;287-token&lt;/strong&gt; prompt (estimated as PF time + 1 decode token).&lt;/li&gt; &lt;li&gt;&lt;em&gt;(177-token TTFT not included in this run‚Äôs pasted timing logs.)&lt;/em&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Discussion: Why BF16 on CPU?&lt;/h1&gt; &lt;p&gt;While 4-bit quants are faster, I chose BF16 for this coder-specific model to ensure zero degradation in logic and syntax handling.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Memory Bandwidth:&lt;/strong&gt; The 12-channel DDR5-6400 configuration is the hero here. At 80B scale, we are moving a massive amount of data per token, and the bandwidth saturation is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zen 5 Advantage:&lt;/strong&gt; The AVX-512 throughput on the 9175F handles the BF16 math with helps. Even without a GPU, the experience doesn't feel like &amp;quot;waiting&amp;quot; in an async workflow.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Coding Evaluation Takeaways&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Security &amp;amp; Audit:&lt;/strong&gt; Extremely strong. It successfully identified SQLi vulnerabilities and plaintext password risks, providing robust fixes and unit tests.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hallucination Control:&lt;/strong&gt; Using the spec-grounded mode, it correctly refused to answer when the information was missing (&amp;quot;NOT IN SPEC&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Logic:&lt;/strong&gt; It followed 90% of constraint-heavy Django requirements but missed some specific multi-tenant safety nuances. It‚Äôs best used as a high-end draft generator + expert reviewer.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Bonus Benchmark: Qwen3-Coder-Next-NVFP4 on GPU&lt;/h1&gt; &lt;p&gt;GPU: Blackwell RTX PRO 6000 Max-Q 96GB &lt;/p&gt; &lt;p&gt;MODEL: vincentzed-hf/Qwen3-Coder-Next-NVFP4&lt;/p&gt; &lt;pre&gt;&lt;code&gt;podman run --rm --device nvidia.com/gpu=all --security-opt seccomp=unconfined --cap-add SYS_NICE --shm-size=16g -v /mnt/data/hf:/data/hf:Z -v /opt/containers/runtime/vllm/data/gpu_cache:/data/cache:Z -p 8000:8000 -e HF_HOME=/data/hf -e HF_DATASETS_CACHE=/data/hf -e VLLM_CACHE_ROOT=/data/cache -e HF_HUB_OFFLINE=1 -e FLASHINFER_DISABLE_VERSION_CHECK=1 compute.home.arpa/vllm-gpu:nightly vincentzed-hf/Qwen3-Coder-Next-NVFP4 --dtype auto --gpu-memory-utilization 0.88 --max-num-seqs 1 --max-model-len 32768 --enable-prefix-caching --trust-remote-code --enable-auto-tool-choice --tool-call-parser qwen3_coder --reasoning-parser qwen3 --served-model-name qwen3-coder-next-nvfp4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;vLLM (NVFP4) throughput (periodic log snapshots; interval averages, so it fluctuates a lot):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Avg generation throughput observed: ~11.7‚Äì100.4 tok/s (examples: 17.5, 58.4, ~99‚Äì100 tok/s spikes)&lt;/li&gt; &lt;li&gt;Avg prompt throughput observed: ~17.7‚Äì669.1 tok/s (examples: ~20‚Äì30 tok/s in some intervals; large spikes like 175/463/669 tok/s depending on the interval)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gtb1luh2rvhg1.png?width=3220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1b346dd9cbcf851b486f5cc1354efbd3050aad82"&gt;https://preview.redd.it/gtb1luh2rvhg1.png?width=3220&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1b346dd9cbcf851b486f5cc1354efbd3050aad82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: these are rolling/interval averages from vLLM logs (not per-request measurements).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Video Demo&lt;/strong&gt;: (GPU 8:05~)&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qxib19/video/2m475useqvhg1/player"&gt;https://reddit.com/link/1qxib19/video/2m475useqvhg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express-Jicama-9827"&gt; /u/Express-Jicama-9827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxib19/qwen3codernext_80b_ggufbf16_on_zen_5_epyc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxib19/qwen3codernext_80b_ggufbf16_on_zen_5_epyc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxib19/qwen3codernext_80b_ggufbf16_on_zen_5_epyc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T13:57:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwo9j0</id>
    <title>We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF</title>
    <updated>2026-02-05T15:28:27+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"&gt; &lt;img alt="We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF" src="https://external-preview.redd.it/bmIycDZuMHYxcGhnMTkRUzZawZzMWm4JXBBoVayTVh3fNrkxvwbY4-FVurAN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=416b047c4576396c89a7eee16410255cdb27cd61" title="We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Here's something new for you: Mobile World Models.&lt;br /&gt; We just released gWorld ‚Äî open-weight visual world models for mobile GUIs (8B and 32B).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo Video Explanation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's gWorld 32B imagining a multi-step Booking dot com session ‚Äî zero access to the real app:&lt;br /&gt; 1. Sees flight search form (Detroit ‚Üí Chicago)&lt;br /&gt; 2. Click &amp;quot;Search&amp;quot; ‚Üí writes code ‚Üí renders full results page with airlines, prices, times&lt;br /&gt; 3. Click destination field ‚Üí predicts the search UI with history &lt;/p&gt; &lt;p&gt;Every screen = executable HTML/CSS/JS rendered to pixels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The core idea:&lt;/strong&gt; Instead of predicting the next screen as pixels (diffusion, autoregressive image gen), gWorld predicts it as executable web code. You render the code, you get the image. This sounds simple but it works remarkably well because VLMs already have strong priors on structured web code from pre-training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why code instead of pixels?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-based world models lose visual fidelity (can't represent layouts, colors, images)&lt;/li&gt; &lt;li&gt;Pixel-generation models hallucinate text and structural elements&lt;/li&gt; &lt;li&gt;Code generation gives you the best of both: precise text rendering from linguistic priors + high-fidelity visuals from structured code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results on MWMBench (6 benchmarks, 4 ID + 2 OOD):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Avg Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;29.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="left"&gt;109B (A17B)&lt;/td&gt; &lt;td align="left"&gt;50.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Maverick&lt;/td&gt; &lt;td align="left"&gt;402B (A17B)&lt;/td&gt; &lt;td align="left"&gt;55.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL&lt;/td&gt; &lt;td align="left"&gt;235B (A22B)&lt;/td&gt; &lt;td align="left"&gt;51.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.6V&lt;/td&gt; &lt;td align="left"&gt;106B&lt;/td&gt; &lt;td align="left"&gt;67.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gWorld&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;74.9%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gWorld&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.6%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 8B model beats everything up to 50√ó its size. Render failure rate is &amp;lt;1% (vs 40% for base Qwen3 VL 8B before our training).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other things worth noting:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data scaling follows a power law with R¬≤ ‚â• 0.94 ‚Äî gains are predictable and nowhere near saturating&lt;/li&gt; &lt;li&gt;We include a Korean apps benchmark (KApps) as OOD eval ‚Äî the models generalize well cross-lingually&lt;/li&gt; &lt;li&gt;The data pipeline is automated: repurpose existing trajectory data ‚Üí cross-modal relabeling to code ‚Üí synthetic reasoning traces&lt;/li&gt; &lt;li&gt;We also show that better world models ‚Üí better downstream GUI agent performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters beyond benchmarks:&lt;/strong&gt; The bottleneck for training GUI agents with online RL is device-policy coupling ‚Äî every rollout needs a real Android emulator. World models could decouple this entirely, enabling massively parallel rollouts on pure compute. gWorld is a step in that direction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§ó gWorld 8B: &lt;a href="https://huggingface.co/trillionlabs/gWorld-8B"&gt;https://huggingface.co/trillionlabs/gWorld-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ü§ó gWorld 32B: &lt;a href="https://huggingface.co/trillionlabs/gWorld-32B"&gt;https://huggingface.co/trillionlabs/gWorld-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª Code: &lt;a href="https://github.com/trillion-labs/gWorld"&gt;https://github.com/trillion-labs/gWorld&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑ Paper: &lt;a href="https://huggingface.co/papers/2602.01576"&gt;https://huggingface.co/papers/2602.01576&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üåê Project page (and demos): &lt;a href="https://trillionlabs-gworld.github.io/"&gt;https://trillionlabs-gworld.github.io&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmarks (incl. K-Apps) coming soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions.&lt;br /&gt; Built by Trillion Labs √ó KAIST AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/37uavl0v1phg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx2teh</id>
    <title>~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)</title>
    <updated>2026-02-06T00:34:05+00:00</updated>
    <author>
      <name>/u/Spiritual_Tie_5574</name>
      <uri>https://old.reddit.com/user/Spiritual_Tie_5574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"&gt; &lt;img alt="~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)" src="https://b.thumbs.redditmedia.com/HZiUyd9n-ZrICOCtj5qj7gJrQgYD4FauYK8iUuHW7_s.jpg" title="~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9gfytpz5srhg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11f99eb16917695fa52dbf8ebec6acaf0105e1e9"&gt;https://preview.redd.it/9gfytpz5srhg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11f99eb16917695fa52dbf8ebec6acaf0105e1e9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Just a quick one in case it saves someone else a headache. I was getting really poor throughput (~10 tok/sec) with Qwen3-Coder-Next-Q4_K_S.gguf on llama.cpp, like ‚Äúthis can‚Äôt be right‚Äù levels, and eventually found a set of args that fixed it for me.&lt;/p&gt; &lt;p&gt;My rig:&lt;/p&gt; &lt;p&gt;- RTX 5090&lt;/p&gt; &lt;p&gt;- 9950X3D&lt;/p&gt; &lt;p&gt;- 96GB RAM&lt;/p&gt; &lt;p&gt;Driver 591.86 / CUDA 13.1&lt;/p&gt; &lt;p&gt;llama.cpp b7951&lt;/p&gt; &lt;p&gt;Model: Unsloth GGUF Qwen3-Coder-Next-Q4_K_S.gguf&lt;/p&gt; &lt;p&gt;What worked:&lt;/p&gt; &lt;p&gt;&lt;code&gt;-c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -np 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Full command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;.\llama-bin\llama-server.exe -m &amp;quot;C:\path\to\Qwen3-Coder-Next-Q4_K_S.gguf&amp;quot; -c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -np 1 --host&lt;/code&gt; &lt;a href="http://127.0.0.1"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From what I can tell, the big win here is:&lt;/p&gt; &lt;p&gt;- Offloading the MoE expert tensors (the .ffn_.*_exps ones) to CPU, which seems to reduce VRAM pressure / weird paging/traffic on this *huge* model&lt;/p&gt; &lt;p&gt;- Quantising KV cache (ctk/ctv q8_0) helps a lot at 32k context&lt;/p&gt; &lt;p&gt;Small warning: the &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; bit seems great for this massive Qwen3-Next GGUF, but I‚Äôve seen it hurt smaller MoE models (extra CPU work / transfers), so definitely benchmark on your own setup.&lt;/p&gt; &lt;p&gt;Hope that helps someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Tie_5574"&gt; /u/Spiritual_Tie_5574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T00:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxgro7</id>
    <title>Terminal capability is becoming a core eval, we open-sourced 1,376 environments</title>
    <updated>2026-02-06T12:51:03+00:00</updated>
    <author>
      <name>/u/No-Wind-1854</name>
      <uri>https://old.reddit.com/user/No-Wind-1854</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI and Anthropic recently released GPT-5.3-Codex and Opus 4.6.&lt;/p&gt; &lt;p&gt;One clear trend is that terminal capability is now a core part of agent evaluation.&lt;/p&gt; &lt;p&gt;In practice, terminal training runs into a bottleneck quickly:&lt;/p&gt; &lt;p&gt;there are not enough high-quality, realistic environments. Scripted tasks and synthetic traces don't go very far.&lt;/p&gt; &lt;p&gt;In SETA, we focused on building environments instead of tasks.&lt;/p&gt; &lt;p&gt;We've released 1,376 validated terminal environments, covering:&lt;/p&gt; &lt;p&gt;Software engineering, Sysadmin, Security, Debugging, Networking, DevOps&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real terminal interaction&lt;/li&gt; &lt;li&gt;Compatible with Terminal Bench and Harbor&lt;/li&gt; &lt;li&gt;Reproducible and validated&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Github: &lt;a href="https://github.com/camel-ai/seta-env"&gt;https://github.com/camel-ai/seta-env&lt;/a&gt;&lt;/p&gt; &lt;p&gt;or search for seta-env in on harbor registry&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Wind-1854"&gt; /u/No-Wind-1854 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgro7/terminal_capability_is_becoming_a_core_eval_we/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgro7/terminal_capability_is_becoming_a_core_eval_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgro7/terminal_capability_is_becoming_a_core_eval_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T12:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx0gxy</id>
    <title>Any hope for Gemma 4 release?</title>
    <updated>2026-02-05T22:54:46+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given that there been a lot of great releases, do you think Gemma 4 would be similar to or even better than what we've seen? Or did Google give up on the project?&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0gxy/any_hope_for_gemma_4_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0gxy/any_hope_for_gemma_4_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0gxy/any_hope_for_gemma_4_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T22:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxk5jn</id>
    <title>hugging face now has benchmark repos for community reported evals</title>
    <updated>2026-02-06T15:10:22+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxk5jn/hugging_face_now_has_benchmark_repos_for/"&gt; &lt;img alt="hugging face now has benchmark repos for community reported evals" src="https://preview.redd.it/e3tqbou44whg1.png?width=140&amp;amp;height=140&amp;amp;auto=webp&amp;amp;s=242774572c3c68f5aa5f3584309ec8b3f5ca756a" title="hugging face now has benchmark repos for community reported evals" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks, it's Ben from Hugging Face &lt;/p&gt; &lt;p&gt;We want to fix inconsistent benchmark results with models, so we shipped Community Evals and Benchmark Datasets.&lt;br /&gt; Benchmark Datasets now host benchmark leaderboards. To create an entry, you can create a PR to model repository with the eval result and source. This directly links model to leaderboard, without merger of PR. We also allow running Jobs for evals for verified results. This helps benchmark results become more transparent. &lt;/p&gt; &lt;p&gt;We'd love to have your feedback, so let us know what you think!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e3tqbou44whg1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f8b5f378d427c37ae410ed290fbd4776d9641b86"&gt;Scores are collected from model repos PRs and added to benchmark repo leaderboards.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxk5jn/hugging_face_now_has_benchmark_repos_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxk5jn/hugging_face_now_has_benchmark_repos_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxk5jn/hugging_face_now_has_benchmark_repos_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T15:10:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx0kzb</id>
    <title>PR to implemt tensor parallelism in Llama.cpp</title>
    <updated>2026-02-05T22:59:13+00:00</updated>
    <author>
      <name>/u/keyboardhack</name>
      <uri>https://old.reddit.com/user/keyboardhack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0kzb/pr_to_implemt_tensor_parallelism_in_llamacpp/"&gt; &lt;img alt="PR to implemt tensor parallelism in Llama.cpp" src="https://external-preview.redd.it/QSt5C9i-4IS4QnEvc5D4DF24jORBMQJOEdeWPERjEmk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5c0fef7864004ff1e03585a93bc0bfb5770856e" title="PR to implemt tensor parallelism in Llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keyboardhack"&gt; /u/keyboardhack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19378"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0kzb/pr_to_implemt_tensor_parallelism_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0kzb/pr_to_implemt_tensor_parallelism_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T22:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxc8qj</id>
    <title>"Minimum Buy-in" Build</title>
    <updated>2026-02-06T08:32:30+00:00</updated>
    <author>
      <name>/u/jmuff98</name>
      <uri>https://old.reddit.com/user/jmuff98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxc8qj/minimum_buyin_build/"&gt; &lt;img alt="&amp;quot;Minimum Buy-in&amp;quot; Build" src="https://preview.redd.it/exb6j45a5uhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0ebdf0ecbf44394a813508836f918d0e6781d0b" title="&amp;quot;Minimum Buy-in&amp;quot; Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished putting this together. &lt;/p&gt; &lt;p&gt;Supermicro x10drh One Radeon pro v340 on each 6 pcie 3.0 x8 slots. The only x16 slot is bifurcated to x8x4x4 for dual Nvme drives and another GPU down the line. But testing first for peak power. I have 15A 120v socket only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jmuff98"&gt; /u/jmuff98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/exb6j45a5uhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxc8qj/minimum_buyin_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxc8qj/minimum_buyin_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxgnqa</id>
    <title>Running Kimi-k2.5 on CPU-only: AMD EPYC 9175F Benchmarks &amp; "Sweet Spot" Analysis</title>
    <updated>2026-02-06T12:45:54+00:00</updated>
    <author>
      <name>/u/Express-Jicama-9827</name>
      <uri>https://old.reddit.com/user/Express-Jicama-9827</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;author:~$ export LANG=en_US.UTF-8 &amp;gt; Japanese is my native language. I used AI to help structure and translate this post to ensure the technical details are accurate in English. This is my first post:D Learned so much from this community:bow &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;I ran a series of local experiments with &lt;strong&gt;Kimi-k2.5 (~1.03T params, MoE)&lt;/strong&gt; using &lt;code&gt;llama.cpp&lt;/code&gt; server to see if a 1T-class model is actually usable on CPU-only infrastructure for non-interactive workloads.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This is &lt;strong&gt;not&lt;/strong&gt; about Chat UX. The target use case is async/batch execution: data pipelines, dataset generation, distillation, and RAG processing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; A 1T-class MoE model is practically usable on CPU-only if you accept the latency and design your workflow around caching + async execution. On my setup, I‚Äôm getting sustainable ~10-12 tok/s decode speeds.&lt;/p&gt; &lt;h1&gt;Hardware / Runtime&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; AMD EPYC 9175F (16 cores / 32 threads, Zen 5, 512MB L3)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 768GB DDR5 (12 channels, running at 6000 MT/s due to motherboard limits)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; Not used&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OS:&lt;/strong&gt; Ubuntu 24.04&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Runtime:&lt;/strong&gt; &lt;code&gt;llama.cpp&lt;/code&gt; container (server mode, rootless podman, AVX-512/VNNI build)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;e.g.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;podman run --rm -p 8081:8080 --shm-size 16g --cap-add=SYS_NICE -v /mnt/data/hf/hub/models--unsloth--Kimi-K2.5-GGUF:/models:Z compute.home.arpa/llamacpp-zen5:latest -m /models/snapshots/386fed8b054275941d6a495a9a7010fbf31b560d/Q4_K_S/Kimi-K2.5-Q4_K_S-00001-of-00013.gguf --cache-type-k q8_0 --cache-type-v q8_0 --defrag-thold 0.1 --flash-attn on --ctx-size 16384 --parallel 1 --threads 13 --threads-batch 13 --batch-size 2048 --ubatch-size 512 --jinja --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Model Settings&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; Kimi-k2.5 (~1.03T params, MoE)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quant:&lt;/strong&gt; GGUF Q4_K_S &lt;code&gt;unsloth/Kimi-K2.5-GGUF&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context:&lt;/strong&gt; 16k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Batch:&lt;/strong&gt; 2048 (ubatch: 512)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Threads:&lt;/strong&gt; 13‚Äì14 (See &amp;quot;Thread Scaling&amp;quot; below)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flash Attention:&lt;/strong&gt; Enabled&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Cache:&lt;/strong&gt; Enabled&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Memory Footprint (Measured)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model RSS:&lt;/strong&gt; ~522‚Äì525 GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KV Cache (16k):&lt;/strong&gt; ~2.0 GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompt Cache (~1.2k tokens):&lt;/strong&gt; ~160 MB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total RSS:&lt;/strong&gt; ~523 GB (Stable, no swap-in/out observed)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Performance (Real Numbers)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Cold Run (No Cache)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prefill:&lt;/strong&gt; ~22 tok/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decode:&lt;/strong&gt; ~10 tok/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total Time (~1.2k tokens):&lt;/strong&gt; ~80s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2. With Prompt Cache (LCP Hit)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cache Lookup &amp;amp; state apply:&lt;/strong&gt; ~60 ms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Impact:&lt;/strong&gt; FFTF (Time to First Token) drops dramatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verdict:&lt;/strong&gt; While slow for real-time chat, this is totally fine for batch workloads where prompt caching can be leveraged.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Thread Scaling &amp;amp; The &amp;quot;Sweet Spot&amp;quot;&lt;/h1&gt; &lt;p&gt;I tested various thread counts (ctx 8k) to find the optimal configuration:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Prefill (tok/s)&lt;/th&gt; &lt;th align="left"&gt;Decode (tok/s)&lt;/th&gt; &lt;th align="left"&gt;Note&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;16&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;24.4&lt;/td&gt; &lt;td align="left"&gt;12.9&lt;/td&gt; &lt;td align="left"&gt;Max throughput&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;14&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;21.3&lt;/td&gt; &lt;td align="left"&gt;12.5&lt;/td&gt; &lt;td align="left"&gt;Memory bandwidth saturation begins&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;13&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;21.6&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;11.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;The Sweet Spot&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;12&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;14.6&lt;/td&gt; &lt;td align="left"&gt;11.9&lt;/td&gt; &lt;td align="left"&gt;Efficiency-oriented&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Observation:&lt;/strong&gt; Decode speed saturates around 13‚Äì14 threads. Pushing beyond this yields diminishing returns while starving other processes. Running at &lt;code&gt;th=13&lt;/code&gt; leaves headroom for my data pipeline (Dagster/Trino) to run in the background without choking the inference.&lt;/p&gt; &lt;h1&gt;Discussion: Why does this CPU work?&lt;/h1&gt; &lt;p&gt;&lt;em&gt;This is my current interpretation based on observed behavior. I'm happy to be corrected.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; Entire experts obviously do not fit in L3 (512MB). However, MoE works well on CPU not because &lt;em&gt;everything&lt;/em&gt; fits, but because the &lt;strong&gt;repeatedly reused working set&lt;/strong&gt; does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Router / Gating logic&lt;/li&gt; &lt;li&gt;Projection layers&lt;/li&gt; &lt;li&gt;Recent layer weights &amp;amp; intermediate tensors&lt;/li&gt; &lt;li&gt;KV reuse paths&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unlike dense 70B+ models which often fall back into memory-latency-dominated behavior for every token, MoE seems to benefit significantly from the localized &amp;quot;hot regions&amp;quot; staying in cache.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EPYC 9175F (Zen 5) Specific Factors:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Huge L3 √ó Low Core Count:&lt;/strong&gt; With 512MB L3 shared across only 16 cores, we have effectively &lt;strong&gt;32MB+ L3 per core&lt;/strong&gt;. This minimizes cache contention/thrashing even with random MoE access patterns.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low Memory Controller effective latency:&lt;/strong&gt; 12 memory channels feeding only 16 cores means very shallow request queues. MoE favors latency minimization over raw bandwidth.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zen 5 AVX-512/BF16:&lt;/strong&gt; The true 512-bit datapaths and native BF16 execution seem to help significantly, even with Q4 quants (accum paths).&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;A 1T-parameter MoE model on CPU-only is a viable workhorse.&lt;/p&gt; &lt;p&gt;If you treat it as a batch engine and lean heavily on prompt caching, it is surprisingly usable. My current setup splits the workload: &lt;strong&gt;GPU for fast agents, CPU for stable, massive-context, reproducible batch generation.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Video Demo:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qxgnqa/video/82ow6kvmdvhg1/player"&gt;https://reddit.com/link/1qxgnqa/video/82ow6kvmdvhg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;*Bonus Benchmark: Llama-4-Maverick-17B (GGUF Q8)&lt;/p&gt; &lt;p&gt;To contrast with the massive MoE model, I also tested Llama-4-Maverick-17B at Q8 (8-bit) quantization.&lt;/p&gt; &lt;p&gt;Performance:&lt;/p&gt; &lt;p&gt;Prompt Processing (Prefill): ~50‚Äì52 tok/s&lt;/p&gt; &lt;p&gt;819 tokens in 15.6s ‚Üí 52.4 tok/s&lt;/p&gt; &lt;p&gt;1000 tokens in 19.7s ‚Üí 50.8 tok/s&lt;/p&gt; &lt;p&gt;Generation (Decode): ~15‚Äì16 tok/s&lt;/p&gt; &lt;p&gt;104 tokens in 6.3s ‚Üí 16.6 tok/s&lt;/p&gt; &lt;p&gt;916 tokens in 60.4s ‚Üí 15.2 tok/s&lt;/p&gt; &lt;p&gt;TTFT: ~16‚Äì20s (for ~1k token prompts)&lt;/p&gt; &lt;p&gt;What's Next? For my next experiment, I plan to test the newly released Qwen3-Coder-Next at Q8. I'm curious to see if the &amp;quot;Active 3B&amp;quot; architecture can push CPU inference speeds even higher while maintaining top-tier coding performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express-Jicama-9827"&gt; /u/Express-Jicama-9827 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgnqa/running_kimik25_on_cpuonly_amd_epyc_9175f/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgnqa/running_kimik25_on_cpuonly_amd_epyc_9175f/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgnqa/running_kimik25_on_cpuonly_amd_epyc_9175f/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T12:45:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwxtf8</id>
    <title>BalatroBench - Benchmark LLMs' strategic performance in Balatro</title>
    <updated>2026-02-05T21:12:37+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/"&gt; &lt;img alt="BalatroBench - Benchmark LLMs' strategic performance in Balatro" src="https://preview.redd.it/we7y7tzvrqhg1.png?width=140&amp;amp;height=111&amp;amp;auto=webp&amp;amp;s=388833c8320341635311505c2b2e13565a687f71" title="BalatroBench - Benchmark LLMs' strategic performance in Balatro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you own a copy of Balatro, you can make your local LLM play it.&lt;/p&gt; &lt;p&gt;I built tools to let LLMs play Balatro autonomously. The LLM gets the game state as text, decides what to do (play, discard, buy from shop...), and the action executes in the actual game. No hard-coded heuristics ‚Äî all decisions come from the LLM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/coder/balatrobot"&gt;BalatroBot&lt;/a&gt; is a mod that exposes an HTTP API for game state and controls. &lt;a href="https://github.com/coder/balatrollm"&gt;BalatroLLM&lt;/a&gt; is the bot framework ‚Äî it works with any OpenAI-compatible endpoint (Ollama, vLLM, etc.).&lt;/p&gt; &lt;p&gt;You can write your own &lt;strong&gt;strategy&lt;/strong&gt; (Jinja2 templates that define how game state is prompted and what the LLM's decision philosophy should be). Different strategies lead to very different results with the same model.&lt;/p&gt; &lt;p&gt;Benchmark results across various models (including open-weight ones) are on &lt;a href="https://balatrobench.com/"&gt;BalatroBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Resources: - &lt;a href="https://github.com/coder/balatrobot"&gt;BalatroBot&lt;/a&gt;: Balatro mod with HTTP API - &lt;a href="https://github.com/coder/balatrollm"&gt;BalatroLLM&lt;/a&gt;: Bot framework ‚Äî create strategies, plug in your model - &lt;a href="https://balatrobench.com/"&gt;BalatroBench&lt;/a&gt;: Leaderboard and results (&lt;a href="https://github.com/coder/balatrobench"&gt;source&lt;/a&gt;) - &lt;a href="https://discord.gg/SBaRyVDmFg"&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; You can watch an LLM struggling to play Balatro live on &lt;a href="https://www.twitch.tv/S1M0N38"&gt;Twitch&lt;/a&gt; - rn Opus 4.6 is playing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwxtf8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T21:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx77xm</id>
    <title>I am absolutely loving qwen3-235b</title>
    <updated>2026-02-06T03:55:56+00:00</updated>
    <author>
      <name>/u/TwistedDiesel53</name>
      <uri>https://old.reddit.com/user/TwistedDiesel53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed qwen3-235b on my desktop system, and I had to join here to brag about it. It's such a careful model, the accuracy of it's output is unbelievable and I've found myself using it absolutely constantly to the point my chatgpt pro subscription is getting left behind. The ability to get carefully curated information of this quality from your own desktop PC is astounding to me and for my use puts all the commercial subscriptions to shame. Sorry for the rant lol!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwistedDiesel53"&gt; /u/TwistedDiesel53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T03:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxf7sf</id>
    <title>Kimi-Linear support is merged to llama.cpp</title>
    <updated>2026-02-06T11:32:47+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally Kimi-Linear is merged to the main branch of llama.cpp.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18755"&gt;https://github.com/ggml-org/llama.cpp/pull/18755&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For people who can't wait for bartowski and unsloth ggufs, you can download them from&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ymcki/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It does take more time than we would have wanted but I think it is necessary to keep the quality of code high.&lt;/p&gt; &lt;p&gt;This is not a work of a single person, here is a breakdown of the contributors:(names are github IDs, sorry if I missed anyone who made a notable contribution)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;cacaview for starting the project to write the logic of Kimi-Linear without KV cache and also implemented KDA in for both CPU and CUDA.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Aaryan-Kapoor added MHA KV cache support and confirmed cacaview's code basically works.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;pwilkin's Qwen3Next gated delta rule code that my KDA code is based on.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;me for extending pwilin's gated delta net (GDN) code to handle KDA (GDN is a special case of KDA) such that uses only existing ggml functions such that it can work on all backednds. I also implemented MLA KV cache support, cleaned up the code and updated it to cope with changes of llama.cpp itself.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;CISC for his time to review the code and thoughtful discussions&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;While cleaning up the code, I manged to find some time to further improve the KDA code such that the overall prompt processing speed increases by 20% and VRAM saving that allows you to run extra 64k context across the board for a fixed size of VRAM, e.g. IQ3_M on 3090 can run 160k when the merged version can only run 96k.&lt;/p&gt; &lt;p&gt;For people who are working at the cutting edge, please feel free to clone the code and tell me if there are any bugs.&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/ymcki/llama.cpp"&gt;https://github.com/ymcki/llama.cpp&lt;/a&gt; --branch Kimi-Linear&lt;/p&gt; &lt;p&gt;This new change will likely to be in the Qwen3-Next and Kimi-Linear unification PR that I will be working with pwilkin and ngxson. So reporting bugs should help us getting this PR done early.&lt;/p&gt; &lt;p&gt;When this unified delta net PR is done, Qwen3-Next should also enjoy 20% gain in pp speed. Context gain in Qwen3-Next probably won't be as dramatic as its KV cache is not MLA.&lt;/p&gt; &lt;p&gt;Hope you all will enjoy this model. I think while it is not as knowledgeable as it is only trained on 5.7T tokens (vs 36T for Qwen3-30B-A3B), it is the only game in town that allows low end hardware to run 1M tokens at high accuracy, so I believe you should be able to find use cases for it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxf7sf/kimilinear_support_is_merged_to_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxf7sf/kimilinear_support_is_merged_to_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxf7sf/kimilinear_support_is_merged_to_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T11:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx9u62</id>
    <title>Report claims Nvidia will not be releasing any new RTX gaming GPUs in 2026, RTX 60 series likely debuting in 2028</title>
    <updated>2026-02-06T06:10:08+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx9u62/report_claims_nvidia_will_not_be_releasing_any/"&gt; &lt;img alt="Report claims Nvidia will not be releasing any new RTX gaming GPUs in 2026, RTX 60 series likely debuting in 2028" src="https://external-preview.redd.it/Vhe0E1hknQdCzPfipp4a4FDDTKpsoaiucv4xmdoIbE4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05bef7b7b638f0b1de9b82e717df9072f9485b20" title="Report claims Nvidia will not be releasing any new RTX gaming GPUs in 2026, RTX 60 series likely debuting in 2028" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/report-claims-nvidia-will-not-be-releasing-any-new-rtx-gaming-gpus-in-2026-rtx-60-series-likely-debuting-in-2028"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx9u62/report_claims_nvidia_will_not_be_releasing_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx9u62/report_claims_nvidia_will_not_be_releasing_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T06:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxepct</id>
    <title>Kimi-Linear support has been merged into llama.cpp</title>
    <updated>2026-02-06T11:04:21+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxepct/kimilinear_support_has_been_merged_into_llamacpp/"&gt; &lt;img alt="Kimi-Linear support has been merged into llama.cpp" src="https://external-preview.redd.it/Kte-gNs9V2aUIoBe_4Yw4msrRYLvCyAEa9ktjCmziH4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22c6d5b10d38c630d7d73ea01a86bd0ce81b6645" title="Kimi-Linear support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18755"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxepct/kimilinear_support_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxepct/kimilinear_support_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T11:04:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxgkd1</id>
    <title>CPU-only, no GPU computers can run all kinds of AI tools locally</title>
    <updated>2026-02-06T12:41:35+00:00</updated>
    <author>
      <name>/u/JackStrawWitchita</name>
      <uri>https://old.reddit.com/user/JackStrawWitchita</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/"&gt; &lt;img alt="CPU-only, no GPU computers can run all kinds of AI tools locally" src="https://preview.redd.it/y9esf03tcvhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fac85771a0c9ce493fdd8ef4c9be41ff1793344f" title="CPU-only, no GPU computers can run all kinds of AI tools locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While it‚Äôs great that so many people on LocalLLaMA are pushing the envelope with what can be done locally with expensive setups, we need to remember that a lot can be done with very minimal machines.&lt;/p&gt; &lt;p&gt;I‚Äôm talking about CPU-only locally run LLMs. That‚Äôs right, &lt;strong&gt;no GPU!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm running Linux Mint on an old Dell optiplex desktop with an i5-8500 processor, 6 threads and 32GB of RAM. You can pick up one of these refurbished for something like $120.&lt;/p&gt; &lt;p&gt;And with this humble rig I can:&lt;/p&gt; &lt;p&gt;Run 12B Q4_K_M gguf LLMs using KoboldCPP. This allows me to have local chatbot fun using quite highly rated models from &lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;. Response times are fast enough as long as you keep the initial prompt below 800 tokens. And with context-shifting it remembers stuff during the session. Uncensored, private RP hilarity for free! You can even add in kokoro_no_espeak for text to speech so your RP characters talk to you with only a few seconds delay. The trick is to find good models to use. For example, DreadPoor/Famino-12B-Model_Stock is rated a 41+ on writing, which is better than many 70B models. You don‚Äôt need big horsepower for fun.&lt;/p&gt; &lt;p&gt;You can also use these models for writing, coding and all sorts of applications. Just need the patience to try out different local models and find the settings that work for you.&lt;/p&gt; &lt;p&gt;I also run Stable Diffusion 1.5 locally for basic image generation, inpainting and so on. Again using KoboldCPP and Stable UI. OK, it takes 3 minutes to generate a 512x512 image but it works fine. And you can experiment with loras and many SD 1.5 models. All 100% free on old gear.&lt;/p&gt; &lt;p&gt;I‚Äôm also running Chatterbox TTS for voice cloning voice-over projects. Works surprisingly well. Again, it takes a couple of minutes to generate a 75 word audio clip, but it does work. Vibevoice TTS also works on this old rig but I prefer Chatterbox.&lt;/p&gt; &lt;p&gt;And then there are amazing tools like Upscayl which upscales images locally incredibly well. Just gotta experiment with the models.&lt;/p&gt; &lt;p&gt;I‚Äôve used ollama transcriber which converts audio files into text amazingly well. Just point a spoken word .WAV at it and then go make dinner and when I get back, the text is there.&lt;/p&gt; &lt;p&gt;There are many other local LLMs and tools I‚Äôve used. These are just the tip of the iceberg. &lt;/p&gt; &lt;p&gt;Video? Nope. Music generation? Nope. I‚Äôve looked and tried a few things but those big resource tasks need serious horsepower. However, it‚Äôs quite possible to use your old desktop computer for text-based tasks and then rent online GPU for one-off tasks and use the big online services for other tasks. It would still probably work out to be less costly.&lt;/p&gt; &lt;p&gt;I know I‚Äôm not the only one doing this.&lt;/p&gt; &lt;p&gt;CPU-only people: tell us how you‚Äôre using AI locally...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JackStrawWitchita"&gt; /u/JackStrawWitchita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y9esf03tcvhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxgkd1/cpuonly_no_gpu_computers_can_run_all_kinds_of_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T12:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxcm5g</id>
    <title>No NVIDIA? No Problem. My 2018 "Potato" 8th Gen i3 hits 10 TPS on 16B MoE.</title>
    <updated>2026-02-06T08:56:17+00:00</updated>
    <author>
      <name>/u/RelativeOperation483</name>
      <uri>https://old.reddit.com/user/RelativeOperation483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"&gt; &lt;img alt="No NVIDIA? No Problem. My 2018 &amp;quot;Potato&amp;quot; 8th Gen i3 hits 10 TPS on 16B MoE." src="https://b.thumbs.redditmedia.com/FlN9zjU8g_h6h9JM_gRcOC3oluZt1E_e4NDcZO0YLwQ.jpg" title="No NVIDIA? No Problem. My 2018 &amp;quot;Potato&amp;quot; 8th Gen i3 hits 10 TPS on 16B MoE." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm writing this from Burma. Out here, we can‚Äôt all afford the latest NVIDIA 4090s or high-end MacBooks. If you have a tight budget, corporate AI like ChatGPT will try to gatekeep you. If you ask it if you can run a 16B model on an old dual-core i3, it‚Äôll tell you it‚Äôs &amp;quot;impossible.&amp;quot;&lt;/p&gt; &lt;p&gt;I spent a month figuring out how to prove them wrong.&lt;/p&gt; &lt;p&gt;After 30 days of squeezing every drop of performance out of my hardware, I found the peak. I‚Äôm running DeepSeek-Coder-V2-Lite (16B MoE) on an HP ProBook 650 G5 (i3-8145U, 16GB Dual-Channel RAM) at near-human reading speeds.&lt;/p&gt; &lt;p&gt;#### The Battle: CPU vs iGPU&lt;/p&gt; &lt;p&gt;I ran a 20-question head-to-head test with no token limits and real-time streaming.&lt;/p&gt; &lt;p&gt;| Device | Average Speed | Peak Speed | My Rating |&lt;/p&gt; &lt;p&gt;| --- | --- | --- | --- |&lt;/p&gt; &lt;p&gt;| CPU | 8.59 t/s | 9.26 t/s | 8.5/10 - Snappy and solid logic. |&lt;/p&gt; &lt;p&gt;| iGPU (UHD 620) | 8.99 t/s | 9.73 t/s | 9.0/10 - A beast once it warms up. |&lt;/p&gt; &lt;p&gt;The Result: The iGPU (OpenVINO) is the winner, proving that even integrated Intel graphics can handle heavy lifting if you set it up right.&lt;/p&gt; &lt;p&gt;## How I Squeezed the Performance:&lt;/p&gt; &lt;p&gt;* MoE is the &amp;quot;Cheat Code&amp;quot;: 16B parameters sounds huge, but it only calculates 2.4B per token. It‚Äôs faster and smarter than 3B-4B dense models.&lt;/p&gt; &lt;p&gt;* Dual-Channel is Mandatory: I‚Äôm running 16GB (2x8GB). If you have single-channel, don't even bother; your bandwidth will choke.&lt;/p&gt; &lt;p&gt;* Linux is King: I did this on Ubuntu. Windows background processes are a luxury my &amp;quot;potato&amp;quot; can't afford.&lt;/p&gt; &lt;p&gt;* OpenVINO Integration: Don't use OpenVINO alone‚Äîit's dependency hell. Use it as a backend for llama-cpp-python.&lt;/p&gt; &lt;p&gt;## The Reality Check&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First-Run Lag: The iGPU takes time to compile. It might look stuck. Give it a minute‚Äîthe &amp;quot;GPU&amp;quot; is just having his coffee.&lt;/li&gt; &lt;li&gt;Language Drift: On iGPU, it sometimes slips into Chinese tokens, but the logic never breaks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm sharing this because you shouldn't let a lack of money stop you from learning AI. If I can do this on an i3 in Burma, you can do it too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelativeOperation483"&gt; /u/RelativeOperation483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qxcm5g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
