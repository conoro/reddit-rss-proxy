<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-22T15:07:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pstdnb</id>
    <title>Best General-Purpose Model - 12gb vram, 128ram</title>
    <updated>2025-12-22T07:17:22+00:00</updated>
    <author>
      <name>/u/rainegarden</name>
      <uri>https://old.reddit.com/user/rainegarden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;^ I want a general purpose model - though sometimes being okay at coding might be useful for debugging stuff that can fit within the range of my system. It's old server parts I basically got for free, so that's why it's a titan xp and has 128gb of ddr4 ecc. Can someone point me in the right direction? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rainegarden"&gt; /u/rainegarden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1psxxss</id>
    <title>Will the RAM shortage cause hike in hosting and compute costs?</title>
    <updated>2025-12-22T12:04:14+00:00</updated>
    <author>
      <name>/u/salary_pending</name>
      <uri>https://old.reddit.com/user/salary_pending</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is not related to the LocalLLaMA type posts but kinda linked to the AI world? Will normal compute costs, shared servers, vps costs go up in the next few years due to this RAM and storage shortages?&lt;/p&gt; &lt;p&gt;Any insight is appreciated. Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salary_pending"&gt; /u/salary_pending &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxxss/will_the_ram_shortage_cause_hike_in_hosting_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxxss/will_the_ram_shortage_cause_hike_in_hosting_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psxxss/will_the_ram_shortage_cause_hike_in_hosting_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:04:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pszwoq</id>
    <title>[Tool] imesde: Zero-GPU, In-Memory Vector Engine for Real-Time Local RAG</title>
    <updated>2025-12-22T13:44:33+00:00</updated>
    <author>
      <name>/u/alessiopelliccione</name>
      <uri>https://old.reddit.com/user/alessiopelliccione</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a project I‚Äôve been working on called &lt;code&gt;imesde&lt;/code&gt; (In-MEmory Streaming Data Engine).&lt;/p&gt; &lt;p&gt;If you‚Äôve ever tried to build a RAG system for live data streams (logs, tweets, stock tickers, or even a fast Twitch chat), you‚Äôve probably hit the &amp;quot;Knowledge Gap&amp;quot;: traditional Vector DBs are great for persistence but they are often too slow to index in real-time or too disk-heavy for what is essentially &amp;quot;short-term memory.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;code&gt;imesde&lt;/code&gt; is designed to be the &amp;quot;Semantic Pipe&amp;quot; for your local LLMs. It lives entirely in RAM, runs on CPU, and has zero indexing latency.&lt;/p&gt; &lt;h1&gt;üß† Why should you care?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-GPU Dependency&lt;/strong&gt;: &lt;code&gt;imesde&lt;/code&gt; is intentionally architected for CPUs. It uses SIMD-accelerated dot product kernels to process Int8 Quantized models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Infinite Window&amp;quot; RAG&lt;/strong&gt;: Instead of a static database, it‚Äôs a Sharded Circular Buffer. As new data flows in, old data flows out. Your LLM always stays focused on the most relevant.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero Indexing Latency&lt;/strong&gt;: The moment a packet arrives, it's available for retrieval.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Local-First &amp;amp; Private&lt;/strong&gt;: It uses in-process vectorization via ONNX. Your data never leaves your machine, making it perfect for private SRE monitoring or local agents.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üöÄ Performance Benchmark on a 5000 records dataset (Apple M4)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Avg Search Latency&lt;/strong&gt;: 232.74 Œºs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;P99 Search Latency&lt;/strong&gt;: 383.75 Œºs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Engine Throughput&lt;/strong&gt;: ~4,300 queries/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Avg Embedding Time&lt;/strong&gt;: 1.93 ms (using bge-small-en-v1.5 int8)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üêç Minimal Python Snippet&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;import imesde # Initialize with a local ONNX model db = imesde.PyImesde( &amp;quot;model/model.onnx&amp;quot;, &amp;quot;model/tokenizer.json&amp;quot;, num_shards=32, shard_size=2048 ) # Ingest live stream (massively parallelized via Rust) db.ingest_batch([&amp;quot;User login at 10:00&amp;quot;, &amp;quot;DB Query timeout&amp;quot;, &amp;quot;High latency detected&amp;quot;]) # Retrieve the absolute latest context for your LLM context = db.search(&amp;quot;database issues&amp;quot;, k=3) # -&amp;gt; results: [(text, score), ...] &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Showcase: The Semantic Radar&lt;/h1&gt; &lt;p&gt;I've included an example in the repo that monitors the global aviation firehose (OpenSky API).&lt;/p&gt; &lt;p&gt;It ingests ~10,000 aircraft states per minute, maps them to semantic concepts (e.g., &amp;quot;dangerous high speed at low altitude&amp;quot;), and triggers a local LLM (via Ollama) only when a real anomaly is detected.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing gif fjmfyh3ver8g1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm looking for community feedback on two fronts: Which integrations would make this most useful in your local workflows? Also, do you have any favorite ultra-lightweight ONNX models to recommend for this type of high-speed CPU streaming?&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/imesde/imesde"&gt;https://github.com/imesde/imesde&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Built with Rust. MIT Licensed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alessiopelliccione"&gt; /u/alessiopelliccione &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pszwoq/tool_imesde_zerogpu_inmemory_vector_engine_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pszwoq/tool_imesde_zerogpu_inmemory_vector_engine_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pszwoq/tool_imesde_zerogpu_inmemory_vector_engine_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T13:44:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pse7w6</id>
    <title>It ain‚Äôt much, but proud of my 2x3090 + a spare 3060 for support</title>
    <updated>2025-12-21T19:03:54+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"&gt; &lt;img alt="It ain‚Äôt much, but proud of my 2x3090 + a spare 3060 for support" src="https://b.thumbs.redditmedia.com/d_4ZORlCqJiugxvoE3WiLbJ7wAjH7oBiwe8Id8UqYgY.jpg" title="It ain‚Äôt much, but proud of my 2x3090 + a spare 3060 for support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs a bit tight, but it fits and I didn‚Äôt want to buy a new case just yet. I had a spare computer that I bought first 1x3090, and now a 2nd 3090.&lt;/p&gt; &lt;p&gt;Qwen3-Next-80b is great!&lt;/p&gt; &lt;p&gt;Trying to wrap my head around Clint and using it in VS Code, but still not working properly‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pse7w6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T19:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pslzv6</id>
    <title>Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2</title>
    <updated>2025-12-22T00:48:25+00:00</updated>
    <author>
      <name>/u/biet_roi</name>
      <uri>https://old.reddit.com/user/biet_roi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"&gt; &lt;img alt="Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2" src="https://external-preview.redd.it/J8tu8oCcj88QnyOGI0-nxwGZljaEo8sk3VorQPTha8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b7531b384cf525c3c5142217cba6a53acbdba0" title="Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/locallama"&gt;r/locallama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;This was my project to evaluate Devstral 2 since it's free right now. Overall, I thought it did pretty well! The CLI it made is totally usable and has a bit better performance than the original when actively agenting (not that it really matters since it'll likely be dwarfed by the model). I usually prefer tools like this to be in rust though since it's the language I work in daily.&lt;/p&gt; &lt;p&gt;Unfortunately, the 120b devstral is too big &amp;amp; slow for my hardware, but I might try to finetune the 24b. I hope Mistral and other labs will continue releasing open code models :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/biet_roi"&gt; /u/biet_roi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nicksenger/revibe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T00:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1psw3ae</id>
    <title>Chroma DB's weak Open Source commitment</title>
    <updated>2025-12-22T10:12:38+00:00</updated>
    <author>
      <name>/u/Primary-Lake7507</name>
      <uri>https://old.reddit.com/user/Primary-Lake7507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chroma's sparse vector search feature (and hybrid search) are only available on Chroma Cloud. This fact is &amp;quot;sparsly&amp;quot; documented and in direct violation of &lt;a href="https://docs.trychroma.com/cloud/getting-started"&gt;their stated open source committments&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Under the hood, it's the exact same Apache 2.0‚Äìlicensed Chroma‚Äîno forks, no divergence, just the open-source engine running at scale.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is such a key feature these days that I would call it a bait-and-switch.&lt;/p&gt; &lt;p&gt;Avoid Chroma, opt for stronger open source alternatives like pgvector or Qdrant.&lt;/p&gt; &lt;p&gt;FYI: I originally made &lt;a href="https://www.reddit.com/r/Rag/comments/1psvu1m/chroma_dbs_open_core_baitandswitch/"&gt;a post&lt;/a&gt; over on &lt;a href="/r/Rag"&gt;r/Rag&lt;/a&gt; showing how Chroma DB's open source committment have become very weak. Since there have been &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1gdqlw7/i_tested_what_small_llms_1b3b_can_actually_do/"&gt;popular posts here utilizing Chroma&lt;/a&gt;, I thought it might be helpful for you guys as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Primary-Lake7507"&gt; /u/Primary-Lake7507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw3ae/chroma_dbs_weak_open_source_commitment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw3ae/chroma_dbs_weak_open_source_commitment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psw3ae/chroma_dbs_weak_open_source_commitment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T10:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1psy387</id>
    <title>Qwen3-235B-W4A16 is S tier</title>
    <updated>2025-12-22T12:12:42+00:00</updated>
    <author>
      <name>/u/Sero_x</name>
      <uri>https://old.reddit.com/user/Sero_x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best model I‚Äôve tried that can fit on my 200GB VRAM is surprisingly this Qwen model.&lt;/p&gt; &lt;p&gt;It is able to navigate complex puzzles, and agentic environments with grace, it‚Äôs a good coder, and extremely fast.&lt;/p&gt; &lt;p&gt;I am even comparing this to remotely hosted FP16 models like GLM-4.6 &lt;/p&gt; &lt;p&gt;This might be old news but I wrote of Qwen since I typically dislike how locked down their models feel. Hope this is helpful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sero_x"&gt; /u/Sero_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy387/qwen3235bw4a16_is_s_tier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy387/qwen3235bw4a16_is_s_tier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psy387/qwen3235bw4a16_is_s_tier/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1psxr5j</id>
    <title>GLM 4.7 Frontend tests (Source: Chinese ForumÔºâ</title>
    <updated>2025-12-22T11:54:28+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"&gt; &lt;img alt="GLM 4.7 Frontend tests (Source: Chinese ForumÔºâ" src="https://b.thumbs.redditmedia.com/nxtMRcalkPFmLY1Gt5yVZ5Wfx6yToFaDf_s8dLxsNog.jpg" title="GLM 4.7 Frontend tests (Source: Chinese ForumÔºâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oa8cibcjuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d77c4e2b26e2dd279169375f10e03884c475f96"&gt;https://preview.redd.it/oa8cibcjuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d77c4e2b26e2dd279169375f10e03884c475f96&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3seg8epkuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bbcb786d3be75c7b5a5775fedf0ce93678eee7e2"&gt;https://preview.redd.it/3seg8epkuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bbcb786d3be75c7b5a5775fedf0ce93678eee7e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nhk4ystluq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d5e5c30101e075b183da8049d13cf21edd663d1f"&gt;https://preview.redd.it/nhk4ystluq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d5e5c30101e075b183da8049d13cf21edd663d1f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27vfi5k1vq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=02493ae4ba91ade53f26d2e0a33d32b1e6910deb"&gt;https://preview.redd.it/27vfi5k1vq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=02493ae4ba91ade53f26d2e0a33d32b1e6910deb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1p8w9bogyq8g1.jpg?width=825&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd8ecebcd58a8598c477251cde98e85f006678a9"&gt;https://preview.redd.it/1p8w9bogyq8g1.jpg?width=825&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd8ecebcd58a8598c477251cde98e85f006678a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://linux.do/t/1350606"&gt;https://linux.do/t/1350606&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/1350292"&gt;https://linux.do/t/1350292&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/1349948"&gt;https://linux.do/t/1349948&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/1350354"&gt;https://linux.do/t/1350354&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/topic/1350623"&gt;https://linux.do/t/topic/1350623&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T11:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pswyjm</id>
    <title>Spent weekend tuning LLM server to hone my nerdism so you don't have to.</title>
    <updated>2025-12-22T11:07:28+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"&gt; &lt;img alt="Spent weekend tuning LLM server to hone my nerdism so you don't have to." src="https://a.thumbs.redditmedia.com/RkN2WJzc5mi4jYKiq6rriMSd4lwO6wttk0O6KbsEPV0.jpg" title="Spent weekend tuning LLM server to hone my nerdism so you don't have to." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Art:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1rdwk3yykq8g1.jpg?width=2494&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=562c0dcecf89a3227a2627572e902afca5384bfb"&gt;https://preview.redd.it/1rdwk3yykq8g1.jpg?width=2494&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=562c0dcecf89a3227a2627572e902afca5384bfb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tl;dr; I've spent some time setting up local AI server with various models for chat and agentic coding in VS code + Cline. The goal was to replace Ollama with llama.cpp and squeeze as much performance as I can from the hardware (Dual RTX 3090 + CPU). The llama-swap configuration with llama.cpp command and options and some extra information is here in the repo: &lt;a href="https://github.com/cepa/llama-nerd"&gt;https://github.com/cepa/llama-nerd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can consider this a sample or a reference, it should work if you have 48+ GB of VRAM but you can scale it up or down by adjusting quant and context size in most models.&lt;/p&gt; &lt;p&gt;I guess that config may be helpful for some of you who want to ditch ollama for good.&lt;/p&gt; &lt;p&gt;The Artist:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kdikr0zgmq8g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c500bd772de3ea9be6e8f1f47d542fcf45d2611"&gt;https://preview.redd.it/kdikr0zgmq8g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c500bd772de3ea9be6e8f1f47d542fcf45d2611&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The llama-swap config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# llama-swap-config.yaml # Hardware: # Dell T7910 # GPU: 2x NVIDIA RTX 3090 (Total 48GB VRAM) # CPU: 2x Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (2 Sockets x 40 Cores) # RAM: 256GB DDR4 # Virtual Machine: # OS: Ubuntu + Nvidia CUDA Drivers # vCPU: 40 Cores # RAM: 64GB # GPU: 2x NVIDIA RTX 3090 (48GB VRAM) (PCIe Passthrough) # Disk: 1TB NVMe (PCIe Passthrough) # NUMA: To DISABLE NUMA, the VM is pinned to physical CPU0 with 64GB RAM and both GPUs. models: # --------------------------------------------------------------------------- # Coding models # --------------------------------------------------------------------------- # https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF # https://huggingface.co/magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF # Q6_K_XL with quantized 96k context size to fit in the 48GB VRAM for speed Seed-OSS-36B-Instruct-UD-Q5_K_XL: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Seed-OSS-36B-Instruct-UD-Q5_K_XL.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 1.1 --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - seed-oss # https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally Qwen3-Coder-30B-A3B-Instruct-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 0.2 --min-p 0.0 --top-p 0.90 --top-k 20 --repeat-penalty 1.05 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - qwen3-coder Qwen2.5-Coder-32B-Instruct-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 0.2 --min-p 0.0 --top-p 0.90 --top-k 20 --repeat-penalty 1.05 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - qwen2.5-coder # https://docs.unsloth.ai/models/devstral-2 Devstral-Small-2-24B-Instruct-2512-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf --mmproj /models/mmproj-Devstral-Small-2-24B-Instruct-2512-F16.gguf --n-gpu-layers 999 --ctx-size 131072 --jinja --temp 0.15 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - devstral-small-2 # Devstral is a dense model, 123b works 2tps or less. #Devstral-2-123B-Instruct-2512-IQ4_XS: # cmd: &amp;gt; # llama-server --port ${PORT} # --model /models/Devstral-2-123B-Instruct-2512-IQ4_XS-00001-of-00002.gguf # --n-gpu-layers 58 # --ctx-size 32768 # --jinja # --temp 0.15 # --cache-type-k q4_0 # --cache-type-v q4_0 # https://docs.unsloth.ai/models/nemotron-3 Nemotron-3-Nano-30B-A3B-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Nemotron-3-Nano-30B-A3B-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --jinja --temp 0.6 --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - nemotron-3-nano # --------------------------------------------------------------------------- # SOTA Models # --------------------------------------------------------------------------- # https://docs.unsloth.ai/models/gpt-oss-how-to-run-and-fine-tune # dont use cache quant, seems to impact performance # performance: 25..30tps gpt-oss-120b: cmd: &amp;gt; llama-server --port ${PORT} --model /models/gpt-oss-120b-MXFP4_MOE.gguf --n-gpu-layers 999 --ctx-size 65536 --flash-attn on -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --threads -1 --temp 1.0 --min-p 0.0 --top-p 1.0 --top-k 0.0 --chat-template-kwargs &amp;quot;{\&amp;quot;reasoning_effort\&amp;quot;: \&amp;quot;high\&amp;quot;}&amp;quot; # https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally # For q8_0 cache, the max is 64k context size # For iq4_nl cache, the max is 128k context size GLM-4.5-Air-IQ4_XS: cmd: &amp;gt; llama-server --port ${PORT} --model /models/GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf --jinja --n-gpu-layers 999 -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --ctx-size 65536 --temp 1.0 --min-p 0.0 --top-p 0.95 --top-k 40 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally # With mmproj and iq4_nl, the max is 32k context size but slow # With mmproj and q4_0, the max is 16k context size but is affected by the uploaded image size GLM-4.6V-IQ4_XS: cmd: &amp;gt; llama-server --port ${PORT} --model /models/GLM-4.6V-IQ4_XS-00001-of-00002.gguf --mmproj /models/mmproj-GLM-4.6V-F16.gguf --jinja --n-gpu-layers 999 -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --ctx-size 32768 --temp 1.0 --min-p 0.0 --top-p 0.95 --top-k 40 --cache-type-k iq4_nl --cache-type-v iq4_nl --flash-attn on # https://docs.unsloth.ai/models/qwen3-next Qwen3-Next-80B-A3B-Thinking-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-Next-80B-A3B-Thinking-Q4_K_M.gguf --n-gpu-layers 999 --n-cpu-moe 2 --ctx-size 65536 --cache-type-k q8_0 --cache-type-v q8_0 --temp 0.6 --min-p 0.0 --top-p 0.80 --top-k 20 --flash-attn on # https://docs.unsloth.ai/models/qwen3-next Qwen3-Next-80B-A3B-Instruct-Q4_K_M: cmd: | llama-server --port ${PORT} --model /models/Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf --n-gpu-layers 999 --n-cpu-moe 2 --ctx-size 65536 --temp 0.7 --min-p 0.0 --top-p 0.8 --top-k 20 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/qwen3-vl-how-to-run-and-fine-tune Qwen3-VL-30B-A3B-Instruct-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-VL-30B-A3B-Instruct-Q8_0.gguf --mmproj /models/mmproj-Qwen3-VL-30B-A3B-Instruct-f16.gguf --n-gpu-layers 999 --ctx-size 81920 --top-p 0.8 --top-k 20 --temp 0.7 --min-p 0.0 --presence-penalty 1.5 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/qwen3-vl-how-to-run-and-fine-tune Qwen3-VL-30B-A3B-Thinking-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-VL-30B-A3B-Thinking-Q8_0.gguf --mmproj /models/mmproj-Qwen3-VL-30B-A3B-Thinking-f16.gguf --n-gpu-layers 999 --ctx-size 81920 --top-p 0.95 --top-k 20 --temp 1.0 --min-p 0.0 --presence-penalty 0.0 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # --------------------------------------------------------------------------- # Legacy Models # --------------------------------------------------------------------------- QwQ-32B-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/QwQ-32B-Q8_0.gguf --n-gpu-layers 999 --ctx-size 65536 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on Qwen2.5-72B-Instruct-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen2.5-72B-Instruct-Q4_K_M.gguf --n-gpu-layers 81 --ctx-size 16384 --cache-type-k q4_0 --cache-type-v q4_0 --flash-attn on DeepSeek-R1-Distill-Llama-70B-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf --n-gpu-layers 999 --ctx-size 32768 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on Llama-3.3-70B-Instruct-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Llama-3.3-70B-Instruct-Q4_K_M.gguf --n-gpu-layers 999 --ctx-size 32768 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune gemma-3-27b-it-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/gemma-3-27b-it-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you like it :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T11:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1psh1w2</id>
    <title>1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec.</title>
    <updated>2025-12-21T21:04:59+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"&gt; &lt;img alt="1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec." src="https://preview.redd.it/fkz64bswfm8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=173d94496a9f94631434a2e6b566db19eb11ba40" title="1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1gmd1a8/are_people_speedrunning_training_gpts_now/"&gt;Previous post&lt;/a&gt; for context. Also note original NanoGPT run from Andrej Karpathy was 45 min. I think this is a great way to understand progress in overall algorithmic speed improvements as I'm sure the big labs are using similar speedup tricks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fkz64bswfm8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T21:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt0av6</id>
    <title>PLX/PEX PCIe 4.0 seems to help for LLMs and P2P! I.e. PEX88096 (1 PCIe 4.0 X16 to 5 PCIE 4.0 X16) and others, and comparison vs bifurcation.</title>
    <updated>2025-12-22T14:01:51+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/"&gt; &lt;img alt="PLX/PEX PCIe 4.0 seems to help for LLMs and P2P! I.e. PEX88096 (1 PCIe 4.0 X16 to 5 PCIE 4.0 X16) and others, and comparison vs bifurcation." src="https://a.thumbs.redditmedia.com/lYix6QQ5r2WZJhPTaCsEMZvjPixt8kNPtTjjFJY3gj8.jpg" title="PLX/PEX PCIe 4.0 seems to help for LLMs and P2P! I.e. PEX88096 (1 PCIe 4.0 X16 to 5 PCIE 4.0 X16) and others, and comparison vs bifurcation." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're having a good day.&lt;/p&gt; &lt;p&gt;I do this post if it helps for information for some users that don't know about switches.&lt;/p&gt; &lt;p&gt;Before anything, I have all the switches I mention on this post but the PCIe 5.0 ones and PEX88080 one. All bought from aliexpress, and all working fine, ranging from 100 to 500USD. If you're interested in the links let me know!&lt;/p&gt; &lt;p&gt;Also, English isn't my first language, so if you found something not written correctly also let me know!&lt;/p&gt; &lt;p&gt;What are PCIe switches?&lt;/p&gt; &lt;p&gt;PCIe switches like the Broadcom PEX88000 (Gen4) and PEX89000 (Gen5) series are essentially &lt;strong&gt;packet-routing fabrics&lt;/strong&gt; for PCIe. They're non-transparent bridges that create a hierarchical PCIe topology, allowing multiple downstream devices to share one or more upstream ports connecting to the CPU's root complex.&lt;/p&gt; &lt;p&gt;Think of them as Ethernet switches but for PCIe packets. They contain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One or more &lt;strong&gt;upstream ports&lt;/strong&gt; (connecting toward the CPU)&lt;/li&gt; &lt;li&gt;Multiple &lt;strong&gt;downstream ports&lt;/strong&gt; (connecting to endpoints like GPUs)&lt;/li&gt; &lt;li&gt;An internal &lt;strong&gt;crossbar switch fabric&lt;/strong&gt; that routes TLP (Transaction Layer Packets) between ports&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For example one of them looks the one of the picture, also some ones look like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c0gd2u9jdr8g1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9e4f49e6e908c5a04761a53df1b617af73c6c584"&gt;X16 4.0 upstream via dual SlimSAS 8i uplink to 4*X16 4.0 slots + 2 SlimSAS 8i downstream&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What are some other benefits of switches?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You don't need PCIe bifurcation motherboard support, the PLX/PEX switch inside does everything. &lt;ul&gt; &lt;li&gt;So for example you can split a X4 slot into X1/X1/X1/X1, or X2/X1/X1, etc and dynamically, those limits will happen when you use everything fully at the same time.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;It works out of the box, you can boot on drives attached to them, and for either OS Linux or Windows.&lt;/li&gt; &lt;li&gt;As PCIe is birectional, it helps a lot for P2P.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would you wonder, how do they create so many slots from a single one?&lt;/p&gt; &lt;p&gt;You don't magically get more bandwidth than the slot offers (i.e. 32 GiB/s bidirectional), but if you use 2 PCIe 4.0 slots on that switch for example, you could get about 64GiB/s total if you write to one side and read from the other.&lt;/p&gt; &lt;p&gt;The switch presents multiple independent downstream ports (say, 4√ó x16 slots), each appearing as a separate PCIe link to the devices attached.&lt;/p&gt; &lt;p&gt;When GPU-A sends a TLP to system memory, the switch routes it through the crossbar to the upstream port. When GPU-B does the same, traffic is interleaved/arbitrated. The switch handles flow control, credit management, and QoS.&lt;/p&gt; &lt;p&gt;So then, traffic between downstream ports (GPU-to-GPU P2P) can traverse the switch fabric &lt;strong&gt;without going through the upstream port at all&lt;/strong&gt;. This is why switches are valuable for multi-GPU‚Äîyou could get full local bandwidth for P2P transfers.&lt;/p&gt; &lt;p&gt;Another switch example are these ones:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PEX88024 (PCIe 4.0 X8 to 4 PCIe 4.0 X4 M2)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rkwzvadher8g1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e9c6749d058f7807821f36ac4b8b1764dcee6da"&gt;PEX88024 Switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PLX88048 (PCIe 4.0 X16 to 8 PCIe 4.0 X4 M2 and 2 SlimSAS 8i to 2x 4i each)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r2e6r46ser8g1.png?width=2632&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=08550c17b3cc9030f23eba9136b7aa82cbfb0973"&gt;PLX88048 Switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PEX88048 variant: PCIE 4.0 X16 to 4 SlimSAS 8i (or 4x8 PCIe 4.0). In this one you can do either X16/X16, X8/X8/X8/X8, or X4/X4/X4/X4/X4/X4/X4/X4.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/thoa2y33fr8g1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b985311a163b62c984aad4ee08c9f37e19f182fb"&gt;PEX88048 Switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PEX88080 (X16 4.0 to 4*X16 4.0 slots)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gadycyqffr8g1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d4e4de1add14208c0399c53b40115dc3468b392"&gt;PEX88080 Switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PLX88096 (Already shown one on the start, so here it is another one). PCIe X16 4.0 to 10 SlimSAS 8i ports: Supports 5*X16 4.0, or 10*X8 4.0, or 20*X4 4.0.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1yhj5ud4gr8g1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7d43c727d23143aa8487e007fca7554585e5eb67"&gt;PEX88096 Switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PEX89048: PCIe 5.0 X16 uplink to 4xMCIO 8i ports (so you can do X16/X16 5.0, or X8/X8/X8/X8 5.0, or 8*X4 5.0)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mf1dp5qegr8g1.png?width=1065&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16d9d8d4b277d3d1824a23c5a82cd26da9789243"&gt;Rocket 1628A, PEX89048 Switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So what are the demerits for something that sounds so good?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It is expensive, like a LOT more expensive than bifurcation cards.&lt;/li&gt; &lt;li&gt;It add latency in the ns terms, which may or not affect your workload.&lt;/li&gt; &lt;li&gt;Requires something external on your PC vs just enabling bifurcation on your motherboard BIOS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A good table comparison would be:&lt;/p&gt; &lt;h1&gt;PCIe Switch vs. Bifurcation&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;Bifurcation&lt;/th&gt; &lt;th align="left"&gt;PCIe Switch&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;What it is&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;CPU/chipset configuration that splits a single physical slot's lanes&lt;/td&gt; &lt;td align="left"&gt;Active silicon device with its own logic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;No additional hardware (just BIOS setting)&lt;/td&gt; &lt;td align="left"&gt;Requires switch chip ($$$)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Bandwidth&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Divides lanes statically (x16 ‚Üí 2√ó8, 4√ó4, etc.)&lt;/td&gt; &lt;td align="left"&gt;Shares bandwidth dynamically via arbitration&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Device visibility&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Each bifurcated segment is a direct CPU link&lt;/td&gt; &lt;td align="left"&gt;Devices sit behind switch in topology hierarchy&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;P2P traffic&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Must traverse CPU root complex&lt;/td&gt; &lt;td align="left"&gt;Can route locally within switch fabric&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Latency&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Lower (direct to root complex)&lt;/td&gt; &lt;td align="left"&gt;Slightly higher (extra hop through switch)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Flexibility&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Fixed by BIOS/physical slot&lt;/td&gt; &lt;td align="left"&gt;Can be reconfigured, supports hot-plug&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Cost&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Free&lt;/td&gt; &lt;td align="left"&gt;Significant (switch chips are expensive)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Practical Example&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Bifurcation scenario&lt;/strong&gt;: Your motherboard has an x16 slot. You set BIOS to 4√ó4 bifurcation and use a passive riser to install four NVMe drives. Each drive gets a dedicated x4 link straight to the CPU, but you've &amp;quot;spent&amp;quot; 16 lanes from your CPU's lane budget.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Switch scenario&lt;/strong&gt;: You have an x16 slot connected to a PEX88096 card. That card provides 4√ó x16 downstream slots (64 lanes downstream from 16 upstream). Four GPUs can each negotiate x16 links. They share the x16 upstream bandwidth to CPU, but GPU-to-GPU P2P gets full switch fabric bandwidth (no CPU bottleneck). You've still only &amp;quot;spent&amp;quot; 16 CPU lanes.&lt;/p&gt; &lt;h1&gt;Real Example&lt;/h1&gt; &lt;p&gt;On Servethehome, an user got the first PLX88096 switch and tested with 3090s, and also a 5.0 one and tested with 5090s. You can read more &lt;a href="https://forums.servethehome.com/index.php?threads/new-chinese-pcie-switch-board-gpu-testing.52488/"&gt;here.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;His results on the 3090s:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# CUDA_VISIBLE_DEVICES=6,7,8 /usr/share/doc/nvidia-cuda-toolkit/examples/bin/x86_64/linux/release/p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 3090, pciBusID: c1, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 3090, pciBusID: e1, pciDeviceID: 0, pciDomainID:0 Device: 2, NVIDIA GeForce RTX 3090, pciBusID: f1, pciDeviceID: 0, pciDomainID:0 Device=0 CAN Access Peer Device=1 Device=0 CAN Access Peer Device=2 Device=1 CAN Access Peer Device=0 Device=1 CAN Access Peer Device=2 Device=2 CAN Access Peer Device=0 Device=2 CAN Access Peer Device=1 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 2 0 1 1 1 1 1 1 1 2 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 0 830.68 11.52 11.59 1 11.46 833.78 11.59 2 11.35 11.41 834.22 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 2 0 833.78 26.40 26.37 1 26.40 834.67 26.40 2 26.40 26.40 835.11 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 0 838.27 16.92 16.93 1 16.85 839.15 17.05 2 17.11 16.99 839.83 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 2 0 839.83 52.20 52.19 1 52.19 839.97 52.20 2 52.20 52.16 839.15 P2P=Disabled Latency Matrix (us) GPU 0 1 2 0 1.48 13.28 13.71 1 13.15 1.56 13.91 2 12.73 13.82 1.56 CPU 0 1 2 0 2.00 5.76 5.31 1 5.61 1.90 5.39 2 5.40 5.53 1.80 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 2 0 1.56 1.02 1.01 1 1.04 1.48 1.04 2 0.97 0.97 1.58 CPU 0 1 2 0 1.91 1.49 1.51 1 1.59 1.94 1.60 2 1.47 1.44 1.88 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;His results on the 5090s:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3,4,5 /usr/share/doc/nvidia-cuda-toolkit/examples/bin/x86_64/linux/release/p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA RTX PRO 6000 Blackwell Workstation Edition, pciBusID: 1, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 5090, pciBusID: 11, pciDeviceID: 0, pciDomainID:0 Device: 2, NVIDIA GeForce RTX 5090, pciBusID: 61, pciDeviceID: 0, pciDomainID:0 Device: 3, NVIDIA GeForce RTX 5090, pciBusID: 71, pciDeviceID: 0, pciDomainID:0 Device: 4, NVIDIA GeForce RTX 5090, pciBusID: 81, pciDeviceID: 0, pciDomainID:0 Device: 5, NVIDIA GeForce RTX 5090, pciBusID: 91, pciDeviceID: 0, pciDomainID:0 Device=0 CAN Access Peer Device=1 Device=0 CAN Access Peer Device=2 Device=0 CAN Access Peer Device=3 Device=0 CAN Access Peer Device=4 Device=0 CAN Access Peer Device=5 Device=1 CAN Access Peer Device=0 Device=1 CAN Access Peer Device=2 Device=1 CAN Access Peer Device=3 Device=1 CAN Access Peer Device=4 Device=1 CAN Access Peer Device=5 Device=2 CAN Access Peer Device=0 Device=2 CAN Access Peer Device=1 Device=2 CAN Access Peer Device=3 Device=2 CAN Access Peer Device=4 Device=2 CAN Access Peer Device=5 Device=3 CAN Access Peer Device=0 Device=3 CAN Access Peer Device=1 Device=3 CAN Access Peer Device=2 Device=3 CAN Access Peer Device=4 Device=3 CAN Access Peer Device=5 Device=4 CAN Access Peer Device=0 Device=4 CAN Access Peer Device=1 Device=4 CAN Access Peer Device=2 Device=4 CAN Access Peer Device=3 Device=4 CAN Access Peer Device=5 Device=5 CAN Access Peer Device=0 Device=5 CAN Access Peer Device=1 Device=5 CAN Access Peer Device=2 Device=5 CAN Access Peer Device=3 Device=5 CAN Access Peer Device=4 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 2 3 4 5 0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 3 1 1 1 1 1 1 4 1 1 1 1 1 1 5 1 1 1 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 0 1496.69 42.63 42.68 42.81 43.21 43.07 1 42.63 1550.15 42.68 42.66 43.14 43.06 2 42.69 42.57 1553.23 42.70 43.10 43.13 3 42.75 42.72 42.66 1553.18 43.00 42.93 4 42.97 42.85 42.89 42.89 1553.23 43.43 5 43.01 42.89 42.91 42.95 43.73 1553.23 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 2 3 4 5 0 1493.83 56.57 56.55 56.55 55.85 55.86 1 56.54 1537.89 56.55 56.57 55.71 55.63 2 56.58 56.58 1534.87 56.56 55.56 55.85 3 56.55 56.55 56.54 1543.97 55.83 55.82 4 55.54 55.59 55.50 55.49 1537.89 56.55 5 55.60 55.62 55.63 55.63 56.58 1543.97 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 0 1483.79 56.50 56.59 56.77 56.92 57.14 1 56.21 1538.60 56.55 56.54 56.82 56.67 2 56.27 56.47 1539.36 56.72 56.89 57.12 3 56.40 56.58 56.21 1540.12 56.99 56.81 4 56.75 56.81 56.73 56.89 1540.88 56.85 5 56.71 56.85 57.05 56.87 56.77 1539.36 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 0 1483.81 111.33 111.39 111.39 110.88 110.88 1 111.38 1534.80 111.38 111.38 55.36 110.01 2 111.38 111.34 1534.07 111.39 110.76 110.90 3 111.38 111.38 111.34 1538.60 110.80 110.80 4 110.73 110.86 110.89 110.91 1537.85 111.39 5 110.92 110.83 110.93 110.91 111.39 1537.07 P2P=Disabled Latency Matrix (us) GPU 0 1 2 3 4 5 0 2.07 14.34 14.30 14.30 14.29 14.29 1 14.30 2.07 14.32 14.32 14.32 14.32 2 14.32 14.31 2.07 14.32 14.32 14.32 3 14.32 14.32 14.34 2.07 14.33 14.33 4 14.32 14.34 14.31 14.23 2.07 14.33 5 14.30 14.32 14.30 14.22 14.32 2.07 CPU 0 1 2 3 4 5 0 2.35 6.88 6.77 6.41 5.68 5.93 1 6.65 2.39 7.07 6.95 6.09 6.15 2 6.70 6.86 2.40 6.62 5.87 6.13 3 6.43 6.71 6.74 2.29 5.69 5.92 4 5.90 6.23 6.18 5.89 2.03 5.46 5 6.12 6.42 6.44 6.15 5.43 2.16 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 2 3 4 5 0 2.07 0.37 0.36 0.43 0.36 0.36 1 0.46 2.07 0.45 0.38 0.38 0.38 2 0.39 0.37 2.07 0.37 0.38 0.37 3 0.37 0.38 0.36 2.07 0.37 0.37 4 0.38 0.43 0.44 0.37 2.07 0.38 5 0.38 0.37 0.37 0.44 0.37 2.07 CPU 0 1 2 3 4 5 0 2.36 1.69 1.64 1.64 1.65 1.75 1 1.79 2.45 1.75 1.87 1.89 1.88 2 1.80 1.73 2.49 1.78 1.78 1.82 3 1.70 1.65 1.66 2.30 1.67 1.71 4 1.47 1.50 1.46 1.45 2.07 1.46 5 1.59 1.54 1.54 1.52 1.53 2.15 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;--------------------------&lt;/p&gt; &lt;p&gt;Hope you found this post informative! Any question is welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T14:01:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1psv6uv</id>
    <title>~1.8√ó peak throughput for Kimi K2 with EAGLE3 draft model</title>
    <updated>2025-12-22T09:14:33+00:00</updated>
    <author>
      <name>/u/yzlnew</name>
      <uri>https://old.reddit.com/user/yzlnew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;we‚Äôve released &lt;strong&gt;Kimi-K2-Instruct-eagle3&lt;/strong&gt;, an &lt;strong&gt;EAGLE3 draft model&lt;/strong&gt; intended to be used with &lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt; for speculative decoding.&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/AQ-MedAI/Kimi-K2-Instruct-eagle3"&gt;https://huggingface.co/AQ-MedAI/Kimi-K2-Instruct-eagle3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kimi-K2-Instruct-eagle3&lt;/strong&gt; is a specialized draft model designed to accelerate the inference of the Kimi-K2-Instruct ecosystem using the &lt;strong&gt;EAGLE3&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Kimi-K2-Instruct with EAGLE3 achieves up to &lt;strong&gt;1.8√ó peak throughput&lt;/strong&gt; versus the base model, accelerating generation across all 7 benchmarks‚Äîfrom +24% on MT-Bench to +80% on Math500 (configured with bs=8, steps=3, topk=1, num_draft_tokens=4).&lt;/p&gt; &lt;p&gt;More performance details in the link above. Hopefully this is useful ‚Äî even if getting Kimi-K2 running locally comes with a bit of pain/cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yzlnew"&gt; /u/yzlnew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psv6uv/18_peak_throughput_for_kimi_k2_with_eagle3_draft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psv6uv/18_peak_throughput_for_kimi_k2_with_eagle3_draft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psv6uv/18_peak_throughput_for_kimi_k2_with_eagle3_draft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T09:14:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1psy0ag</id>
    <title>Local RAG with small models with hallucination mitigation</title>
    <updated>2025-12-22T12:08:03+00:00</updated>
    <author>
      <name>/u/ljubobratovicrelja</name>
      <uri>https://old.reddit.com/user/ljubobratovicrelja</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy0ag/local_rag_with_small_models_with_hallucination/"&gt; &lt;img alt="Local RAG with small models with hallucination mitigation" src="https://external-preview.redd.it/YTFtcnJ1a2t3cThnMS9Ei5brJy4BdsT9diAS7Z-bKUm9Q2FZKYFGqvrqb-vS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b78e5c7d415421f97b463e11d575e588641cefc9" title="Local RAG with small models with hallucination mitigation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I started this as a personal project, aiming to build something fully customizable and suitable to my needs, allowing me to study technical documentation, books and scientific articles locally, privately - therefore allowing me to include larger contexts on proprietary docs within my work. However, I was genuinely surprised by how well it worked, so I decided to make it public and share it here.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; - I built a custom RAG for locally deployed small models, tuned to warn about lack of context and hence mitigate hallucination, when prompted too wide.&lt;/p&gt; &lt;p&gt;Here's the project: &lt;a href="https://github.com/ljubobratovicrelja/tensor-truth"&gt;https://github.com/ljubobratovicrelja/tensor-truth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The video here shows it in action. Just a brief example, loading a single book (Convex Optimization by Boyd and Vandenberghe), asking something I know is inside - model answering comfortably, with clear citing from where it got the info. However, after a couple of prompts, asking something I know the book doesn't cover - backpropagation and ML optimization basis - the model admits the limitation of the context plainly. In the following prompt, I load the book I know it covers this topic (Mathematics for Machine Learning), ask it to now revise its sources and try to answer previous question, and it does so successfully.&lt;/p&gt; &lt;p&gt;I'll be honest - I'm a computer vision engineer with limited LLM experience, so when I started this I tried existing tools like AnythingLLM and Open WebUI first. They're great for basic RAG, but I couldn't get the level of control I needed - specifically around confidence thresholds, synthesis behavior when context is missing, and the ability to dynamically update context and re-evaluate answers. So I ended up building this with streamlit and llama-index, tailoring it as I saw fit.&lt;/p&gt; &lt;p&gt;Limitations:&lt;/p&gt; &lt;p&gt;As it's well known, a small model in a RAG system will nicely fetch the correct info from precisely tuned prompt when context is available, but as soon as it has to put some &amp;quot;glue&amp;quot; between multiple sources, it's prone to hallucination. The confidence threshold warning should pop-up in the UI, but more importantly, some prompt engineering helps a lot - for e.g. focusing the prompt to &amp;quot;list available info&amp;quot;, rather than &amp;quot;tell me about it&amp;quot;, and later on asking it to elaborate on specific topics it listed and cited.&lt;/p&gt; &lt;p&gt;Technical details:&lt;/p&gt; &lt;p&gt;Uses hierarchical node parsing (2048-&amp;gt;512-&amp;gt;256 chunks for papers and smaller docs, 3072-&amp;gt;768-&amp;gt;384 for books) + auto-merging retrieval + BGE reranking with similarity cutoff. I guess this is a standard pipeline, however I tuned the system to aid the synthesizer response and warn within the UI when context is not available within assigned confidence thresholds.&lt;/p&gt; &lt;p&gt;Anyhow, I hope you find this useful, and please, by all means - comment away. I am very happy to receive all kinds of feedback, and learn from you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljubobratovicrelja"&gt; /u/ljubobratovicrelja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cvrl5akkwq8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy0ag/local_rag_with_small_models_with_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psy0ag/local_rag_with_small_models_with_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:08:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1psyxwo</id>
    <title>Kimi K2 Thinking is the least sycophantic open-source AI, according to research by Anthropic</title>
    <updated>2025-12-22T12:58:00+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"&gt; &lt;img alt="Kimi K2 Thinking is the least sycophantic open-source AI, according to research by Anthropic" src="https://b.thumbs.redditmedia.com/iXwcJSAdgqEh3LeXTBc6oZpnQTN5MofqdRMMRSIcCJU.jpg" title="Kimi K2 Thinking is the least sycophantic open-source AI, according to research by Anthropic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1qpm2njj6r8g1.png?width=2293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3be1a70055147b1d283b5b49557bfd17f1a24c8"&gt;https://preview.redd.it/1qpm2njj6r8g1.png?width=2293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3be1a70055147b1d283b5b49557bfd17f1a24c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's very close to my daily experience. Kimi directly points out problems instead of flattering me.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://alignment.anthropic.com/2025/bloom-auto-evals/"&gt;https://alignment.anthropic.com/2025/bloom-auto-evals/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstuyv</id>
    <title>MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo...</title>
    <updated>2025-12-22T07:48:01+00:00</updated>
    <author>
      <name>/u/BlackRice_hmz</name>
      <uri>https://old.reddit.com/user/BlackRice_hmz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"&gt; &lt;img alt="MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo..." src="https://external-preview.redd.it/ZmtlNnAwcnZtcDhnMbRwrbZjXgs5PA7MM0agSvimAWH_bh1Ie65E3MD0QPIx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b343b6e2cdec93caf72ec2c4750c9d49b84ff91" title="MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seriously, I didn't expect MiniMax M2.1 to be this cracked at design. Just saw this post on X (link below) and the UI it generated looks incredibly clean.&lt;/p&gt; &lt;p&gt;Also noticed the vLLM PR for it was just merged, so it‚Äôs officially coming. If it can actually code and design like this consistently, I'm switching.&lt;/p&gt; &lt;p&gt;Link to the tweet üëâ &lt;a href="https://x.com/CloudTrader4/status/2002729591451054127"&gt;https://x.com/CloudTrader4/status/2002729591451054127&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlackRice_hmz"&gt; /u/BlackRice_hmz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x7el31rvmp8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt18gb</id>
    <title>Seems like GLM-4.7 is released?</title>
    <updated>2025-12-22T14:42:26+00:00</updated>
    <author>
      <name>/u/gameguy56</name>
      <uri>https://old.reddit.com/user/gameguy56</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18gb/seems_like_glm47_is_released/"&gt; &lt;img alt="Seems like GLM-4.7 is released?" src="https://external-preview.redd.it/1DPx3QlWqCDS1dlD35s_ciYODV6nIDpLcKJqCvvcPkA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b74695d3cd82ce6268cf91e8d5bb6b4e119fc03" title="Seems like GLM-4.7 is released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just started using it with droid today. Certainly faster than 4.6- we'll see if it's really any better. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gameguy56"&gt; /u/gameguy56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.z.ai/guides/llm/glm-4.7#introducing-glm-4-7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18gb/seems_like_glm47_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18gb/seems_like_glm47_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T14:42:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1psbx2q</id>
    <title>llama.cpp appreciation post</title>
    <updated>2025-12-21T17:28:24+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"&gt; &lt;img alt="llama.cpp appreciation post" src="https://preview.redd.it/asipaua1el8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87eeb0e85f39e765b810e9ec58e5148346cc419b" title="llama.cpp appreciation post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asipaua1el8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T17:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstaoo</id>
    <title>Got me a 32GB RTX 4080 Super</title>
    <updated>2025-12-22T07:12:05+00:00</updated>
    <author>
      <name>/u/Spooknik</name>
      <uri>https://old.reddit.com/user/Spooknik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"&gt; &lt;img alt="Got me a 32GB RTX 4080 Super" src="https://b.thumbs.redditmedia.com/skSb77iJra6hgwff6TIBJmnw4nMo-uZGyL0qiLRJ9hs.jpg" title="Got me a 32GB RTX 4080 Super" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is maybe slightly off topic, but since people ask about hardware here a lot. &lt;/p&gt; &lt;p&gt;I took a risk and bought a modified RTX 4080 Super from the Chinese market for around 1200 USD / 1000 EUR. Which for me because I live in Europe, the cheapest RTX 5090 I can find is around 2500 USD / 2100 EUR. &lt;/p&gt; &lt;p&gt;It's maybe not the best card for price per GB of VRAM considering the RTX 3090 is dropping a lot, but 32GB on one card for about half the price of a 5090 is nice. I do a lot of Diffusion model stuff, so it's great for that too. &lt;/p&gt; &lt;p&gt;It works with the stock Nvidia driver, no messing around, it was just literally plug and play. Card seems really good quality, metal back plate and metal case. Fan sounds like a small jet engine. &lt;/p&gt; &lt;p&gt;But running it around a month now and zero issues at all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spooknik"&gt; /u/Spooknik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pstaoo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt18x4</id>
    <title>NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!</title>
    <updated>2025-12-22T14:42:56+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"&gt; &lt;img alt="NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!" src="https://preview.redd.it/k20itq6cpr8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28c0951b3081aafea1e185ecbd82f4f0fc54726f" title="NVIDIA made a beginner's guide to fine-tuning LLMs with Unsloth!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog Link: &lt;a href="https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/"&gt;https://blogs.nvidia.com/blog/rtx-ai-garage-fine-tuning-unsloth-dgx-spark/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You'll learn about: - Training methods: LoRA, FFT, RL - When to fine-tune and why + use-cases - Amount of data and VRAM needed - How to train locally on DGX Spark, RTX GPUs &amp;amp; more&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k20itq6cpr8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt18x4/nvidia_made_a_beginners_guide_to_finetuning_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T14:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1psyqha</id>
    <title>upstage/Solar-Open-100B ¬∑ Hugging Face</title>
    <updated>2025-12-22T12:47:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"&gt; &lt;img alt="upstage/Solar-Open-100B ¬∑ Hugging Face" src="https://external-preview.redd.it/KGZNZzWd5K6vM05pplkzhPrPxHhbMAP1w-s6MnLTkhM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=559420850549bc9f134f2c1908f869fba5d48dbf" title="upstage/Solar-Open-100B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...do you remember &lt;a href="https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0"&gt;https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0&lt;/a&gt; from 2024?&lt;/p&gt; &lt;p&gt;It looks like they have something new:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#solar-open"&gt;&lt;/a&gt;Solar Open&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Solar Open&lt;/strong&gt; is Upstage's flagship &lt;strong&gt;102B-parameter&lt;/strong&gt; large language model, trained &lt;strong&gt;entirely from scratch&lt;/strong&gt; and released under the &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (see &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;). As a &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; architecture, it delivers enterprise-grade performance in reasoning, instruction-following, and agentic capabilities‚Äîall while prioritizing transparency and customization for the open-source community.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#highlights"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MoE Architecture (102B / 12B):&lt;/strong&gt; Built on a Mixture-of-Experts architecture with &lt;strong&gt;102B total / 12B active parameters&lt;/strong&gt;. This design delivers the knowledge depth of a massive model with the inference speed and cost-efficiency of a much smaller model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massive Training Scale:&lt;/strong&gt; Pre-trained on &lt;strong&gt;19.7 trillion tokens&lt;/strong&gt;, ensuring broad knowledge coverage and robust reasoning capabilities across various domains.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#model-overview"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Overview&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Name:&lt;/strong&gt; Solar Open 100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face ID:&lt;/strong&gt; Upstage/Solar-Open-100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Mixture-of-Experts (MoE) &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Parameters:&lt;/strong&gt; 102.6B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Active Parameters:&lt;/strong&gt; 12B (per token)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experts:&lt;/strong&gt; 129 Experts (top 8 among 128 Routed + 1 Shared)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training Tokens:&lt;/strong&gt; 19.7 Trillion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Length:&lt;/strong&gt; 128k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Hardware:&lt;/strong&gt; NVIDIA B200 GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (See &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1psw818</id>
    <title>Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks</title>
    <updated>2025-12-22T10:21:02+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/"&gt; &lt;img alt="Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks" src="https://external-preview.redd.it/aTgxeDFoN2hlcThnMS3fkoPAQ79Dr1Rhop5Txa3dHZ8j7rKJLXSjpsKe44ta.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec62c0ae7606f90a6ff429f79fbdce528fe005e8" title="Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing Jan-v2-VL-max, a 30B multimodal model built for long-horizon execution.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-max outperforms DeepSeek R1 and Gemini 2.5 Pro on the Illusion of Diminishing Returns benchmark, which measures execution length.&lt;/p&gt; &lt;p&gt;Built on Qwen3-VL-30B-A3B-Thinking, Jan-v2-VL-max scales the Jan-v2-VL base model to 30B parameters and applies LoRA-based RLVR to improve stability and reduce error accumulation across many-step executions.&lt;/p&gt; &lt;p&gt;The model is available on &lt;a href="https://chat.jan.ai/"&gt;https://chat.jan.ai/&lt;/a&gt;, a public interface built on Jan Server. We host the platform ourselves for now so anyone can try the model in the browser. We're going to release the latest Jan Server repo soon.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try the model here: &lt;a href="https://chat.jan.ai/"&gt;https://chat.jan.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Run the model locally: &lt;a href="https://huggingface.co/janhq/Jan-v2-VL-max-FP8"&gt;https://huggingface.co/janhq/Jan-v2-VL-max-FP8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can serve the model locally with vLLM (vLLM 0.12.0, transformers 4.57.1). FP8 inference is supported via llm-compressor, with production-ready serving configs included. It's released under the Apache-2.0 license.&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.jan.ai/"&gt;https://chat.jan.ai/&lt;/a&gt; doesn't replace Jan Desktop. It complements it by giving the community a shared environment to test larger Jan models.&lt;/p&gt; &lt;p&gt;Happy to answer your questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/80qbna7heq8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T10:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1psuy8g</id>
    <title>GLM 4.7 IS COMING!!!</title>
    <updated>2025-12-22T08:59:22+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zhipu‚Äôs next-generation model, GLM-4.7, is about to be released! We are now opening Early Access Beta Permissions specifically for our long-term supporters. We look forward to your feedback we work together to make the GLM model even better!&lt;/p&gt; &lt;p&gt;As the latest flagship of the GLM series, &lt;strong&gt;GLM-4.7 features enhanced coding capabilities, long-range task planning, and tool orchestration specifically optimized for Agentic Coding scenarios. It has already achieved leading performance among open-source models across multiple public benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This Early Access Beta aims to collect feedback from &amp;quot;real-world development scenarios&amp;quot; to continuously improve the model's coding ability, engineering comprehension, and overall user experience.&lt;/p&gt; &lt;p&gt;üìå &lt;strong&gt;Testing Key Points&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Freedom of Choice: Feel free to choose the tech stack and development scenarios you are familiar with (e.g., developing from scratch, refactoring, adding features, fixing bugs, etc.).&lt;/li&gt; &lt;li&gt;Focus Areas:Pay attention to code quality, instruction following, and whether the intermediate reasoning/processes meet your expectations.&lt;/li&gt; &lt;li&gt;‚Ä¢ Authenticity: There is no need to intentionally cover every type of task; prioritize your actual, real-world usage scenarios.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;‚è∞ &lt;strong&gt;Beta Period: December 22, 2025 ‚Äì Official Release&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Feedback Channels: For API errors or integration issues, you can provide feedback directly within the group. If you encounter results that do not meet expectations, please post a &amp;quot;Topic&amp;quot; (including the date, prompt, tool descriptions, expected vs. actual results, and attached local logs). Other developers can brainstorm with you, and our algorithm engineers and architects will be responding to your queries!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current early access form only available for Chinese user&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T08:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstlas</id>
    <title>major open-source releases this year</title>
    <updated>2025-12-22T07:30:46+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt; &lt;img alt="major open-source releases this year" src="https://preview.redd.it/wynfuvk9kp8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=763bc1a7f949dc4ff18c4a976a10f017205abb54" title="major open-source releases this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wynfuvk9kp8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
