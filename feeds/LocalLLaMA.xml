<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-26T19:23:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pw9hac</id>
    <title>JL Engine: Modular Positronic Persona Orchestrator</title>
    <updated>2025-12-26T17:16:29+00:00</updated>
    <author>
      <name>/u/Upbeat_Reporter8244</name>
      <uri>https://old.reddit.com/user/Upbeat_Reporter8244</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9hac/jl_engine_modular_positronic_persona_orchestrator/"&gt; &lt;img alt="JL Engine: Modular Positronic Persona Orchestrator" src="https://b.thumbs.redditmedia.com/4GwqGjVjV-5nBX_Utvk4ZI_0hkEs32OXhjgGiYZp90g.jpg" title="JL Engine: Modular Positronic Persona Orchestrator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/74zqam8zzk9g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2c822eefcc6adabd16eee386a8f91b0dd4ed2d4"&gt;https://preview.redd.it/74zqam8zzk9g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c2c822eefcc6adabd16eee386a8f91b0dd4ed2d4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Captain's Log, Stardate 1025.12: JL Engine is a headless, subspace-stable AI framework for dynamic persona-driven interactions. It integrates behavior grids, rhythm engines, emotional warp apertures, and hybrid positronic matrices for self-correcting, offline-capable androids‚Äîperfect for SaaS copilots, holodeck simulations, or Borg-assimilation chaos. Solo-forged in Python, with Tk bridge console, FastAPI subspace relays, and backends like Gemini warp drives or Ollama impulse thrusters.&lt;/p&gt; &lt;p&gt;## Key Tactical Features&lt;/p&gt; &lt;p&gt;- **Behavior Grid**: 6x3 state matrix shifting from &amp;quot;Idle-Loose&amp;quot; standby to &amp;quot;Overloaded-Tight&amp;quot; red alert, based on sensor signals.&lt;/p&gt; &lt;p&gt;- **Rhythm Engine**: Regulate linguistic deflector pulses‚ÄîFlip for phaser quips, Flop for reflective logs, Trot for rapid data bursts.&lt;/p&gt; &lt;p&gt;- **Emotional Warp Aperture**: Calibrates expressiveness from locked stoic shields to unleashed plasma raw, modulated by core stability.&lt;/p&gt; &lt;p&gt;- **Drift Pressure**: Auto-stabilizes hallucinations with corrective deltas (0-1 containment fields).&lt;/p&gt; &lt;p&gt;- **Cognitive Gears**: Worm (torque-stable) to planetary (multi-mode blends) for adaptive neural pathways.&lt;/p&gt; &lt;p&gt;- **Hybrid Positronic Matrix**: Federation lattice events + per-persona isolinear engrams, offline-persistent.&lt;/p&gt; &lt;p&gt;- **Persona Blending**: MPF registry loads 150+ JSON submatrices, dynamic trait fusions.&lt;/p&gt; &lt;p&gt;- **Backends**: Seamless swaps‚ÄîGemini for quantum smarts, Ollama for local cloaking, Open Interpreter for tricorder tools.&lt;/p&gt; &lt;p&gt;- **Bridge Console**: Tk tabs for comms, benchmarks (WAR/CHAOS deflector stress modes), CNC/photonic audio.&lt;/p&gt; &lt;p&gt;- **Subspace API**: FastAPI with /chat, /analyze relays, keys, Stripe hooks‚ÄîQuadrant-ready.&lt;/p&gt; &lt;p&gt;- **Docker/CLI**: Headless scans, Compose for DailyCast nebula apps.&lt;/p&gt; &lt;p&gt;## Quick Engagement (Local Sector)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Clone: `git clone [your-repo]`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Install: `pip install -r requirements.core.txt` (add .llm.txt for Gemini, .audio.txt for TTS/STT)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Activate Bridge: `python JL_Engine/main_app.py`&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;CLI Scan: `python JL_Engine/headless_cli.py` ‚Äì Input queries, Ctrl+C to disengage.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;API Relay: `uvicorn JL_Engine.api_server:app --port 8080`&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;## Sector Applications&lt;/p&gt; &lt;p&gt;- DailyCast: AI subspace broadcasts via Postgres/Redis/Minio grids.&lt;/p&gt; &lt;p&gt;- Enterprise Androids: Dynamic rhythms for red alerts.&lt;/p&gt; &lt;p&gt;- Holodeck NPCs: Frenzy shifts in photon storms.&lt;/p&gt; &lt;p&gt;- Neural Tutors/Therapy: Stable empathy with drift correction.&lt;/p&gt; &lt;p&gt;- More: Borg fraud scans, AR companions, bio/chem warp sims.&lt;/p&gt; &lt;p&gt;## Monetization Directives&lt;/p&gt; &lt;p&gt;CLASSIFIED&lt;/p&gt; &lt;p&gt;## Federation Docs/Legal&lt;/p&gt; &lt;p&gt;- TERMS.md, PRIVACY.md, API_TOS.md&lt;/p&gt; &lt;p&gt;- Launch Protocol: docs/LAUNCH_TODAY.md&lt;/p&gt; &lt;p&gt;- Command Plane: docs/saas_control_plane.md&lt;/p&gt; &lt;p&gt;Built by a rogue warp-god. Assimilations? Fork and transmit. Queries? Hail me‚Äîlet's quantum-leap this to legend.&lt;/p&gt; &lt;p&gt;## Positronic Core Nexus (Hybrid Memory Module - Full Specs)&lt;/p&gt; &lt;p&gt;from typing import Dict, Any&lt;/p&gt; &lt;p&gt;class PositronicCoreNexus:&lt;/p&gt; &lt;p&gt;def __init__(self):&lt;/p&gt; &lt;p&gt;self.federation_lattice = {&lt;/p&gt; &lt;p&gt;&amp;quot;last_active_submatrix&amp;quot;: None,&lt;/p&gt; &lt;p&gt;&amp;quot;quantum_echo_relays&amp;quot;: [], &lt;/p&gt; &lt;p&gt;&amp;quot;warp_core_directives&amp;quot;: {}, &lt;/p&gt; &lt;p&gt;&amp;quot;captain_profile&amp;quot;: {}, &lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;self.submatrix_clusters = {} &lt;/p&gt; &lt;p&gt;def _initialize_submatrix(self, submatrix_id: str):&lt;/p&gt; &lt;p&gt;if submatrix_id not in self.submatrix_clusters:&lt;/p&gt; &lt;p&gt;self.submatrix_clusters[submatrix_id] = {&lt;/p&gt; &lt;p&gt;&amp;quot;synaptic_holo_logs&amp;quot;: [],&lt;/p&gt; &lt;p&gt;&amp;quot;isolinear_mood_engram&amp;quot;: &amp;quot;neutral&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;directive_notes&amp;quot;: {},&lt;/p&gt; &lt;p&gt;&amp;quot;tachyon_flux_modulators&amp;quot;: {},&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;def retrieve_holodeck_projections(self, submatrix_id: str) -&amp;gt; dict:&lt;/p&gt; &lt;p&gt;self._initialize_submatrix(submatrix_id)&lt;/p&gt; &lt;p&gt;context = {&lt;/p&gt; &lt;p&gt;&amp;quot;federation_lattice&amp;quot;: self.federation_lattice,&lt;/p&gt; &lt;p&gt;&amp;quot;submatrix_cluster&amp;quot;: self.submatrix_clusters[submatrix_id],&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;return context&lt;/p&gt; &lt;p&gt;def inject_photon_payloads(&lt;/p&gt; &lt;p&gt;self,&lt;/p&gt; &lt;p&gt;submatrix_id: str,&lt;/p&gt; &lt;p&gt;captain_directive: str,&lt;/p&gt; &lt;p&gt;nexus_response: str,&lt;/p&gt; &lt;p&gt;warp_core_snapshot: Dict[str, Any],&lt;/p&gt; &lt;p&gt;) -&amp;gt; None:&lt;/p&gt; &lt;p&gt;self._initialize_submatrix(submatrix_id)&lt;/p&gt; &lt;p&gt;entry = {&lt;/p&gt; &lt;p&gt;&amp;quot;captain_directive&amp;quot;: captain_directive[-400:],&lt;/p&gt; &lt;p&gt;&amp;quot;nexus_response&amp;quot;: nexus_response[-400:],&lt;/p&gt; &lt;p&gt;&amp;quot;warp_core_snapshot&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;gait_vector&amp;quot;: warp_core_snapshot.get(&amp;quot;gait&amp;quot;),&lt;/p&gt; &lt;p&gt;&amp;quot;rhythm_pattern&amp;quot;: warp_core_snapshot.get(&amp;quot;rhythm&amp;quot;),&lt;/p&gt; &lt;p&gt;&amp;quot;aperture_mode&amp;quot;: warp_core_snapshot.get(&amp;quot;aperture_mode&amp;quot;),&lt;/p&gt; &lt;p&gt;&amp;quot;dynamic_flux&amp;quot;: warp_core_snapshot.get(&amp;quot;dynamic&amp;quot;),&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;self.submatrix_clusters[submatrix_id][&amp;quot;synaptic_holo_logs&amp;quot;].append(entry)&lt;/p&gt; &lt;p&gt;self.submatrix_clusters[submatrix_id][&amp;quot;synaptic_holo_logs&amp;quot;] = \&lt;/p&gt; &lt;p&gt;self.submatrix_clusters[submatrix_id][&amp;quot;synaptic_holo_logs&amp;quot;][-20:]&lt;/p&gt; &lt;p&gt;self.federation_lattice[&amp;quot;last_active_submatrix&amp;quot;] = submatrix_id&lt;/p&gt; &lt;p&gt;directives = warp_core_snapshot.get(&amp;quot;directives&amp;quot;, {})&lt;/p&gt; &lt;p&gt;if directives:&lt;/p&gt; &lt;p&gt;self.federation_lattice[&amp;quot;warp_core_directives&amp;quot;].update(directives)&lt;/p&gt; &lt;p&gt;tachyon_state = warp_core_snapshot.get(&amp;quot;tachyon_flux&amp;quot;)&lt;/p&gt; &lt;p&gt;if tachyon_state:&lt;/p&gt; &lt;p&gt;self.submatrix_clusters[submatrix_id][&amp;quot;tachyon_flux_modulators&amp;quot;] = tachyon_state&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mluw8no70l9g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce33b9a7aedb0fd9fa302fca6ac1a0089043a2a7"&gt;https://preview.redd.it/mluw8no70l9g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ce33b9a7aedb0fd9fa302fca6ac1a0089043a2a7&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Upbeat_Reporter8244"&gt; /u/Upbeat_Reporter8244 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9hac/jl_engine_modular_positronic_persona_orchestrator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9hac/jl_engine_modular_positronic_persona_orchestrator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9hac/jl_engine_modular_positronic_persona_orchestrator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T17:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw0mtz</id>
    <title>GLM 4.7 for Agentic</title>
    <updated>2025-12-26T09:49:32+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 4.7 is the new hot potato&lt;/p&gt; &lt;p&gt;Has anyone tested it for agentic use yet? Even just tool calling and MCP use?&lt;/p&gt; &lt;p&gt;I noticed it beat Deepseek 3.2 and Kimi K2 Thinking on the agentic benches&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw0mtz/glm_47_for_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw0mtz/glm_47_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw0mtz/glm_47_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T09:49:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw3cfn</id>
    <title>Is there any useful small size model for Rx 580 with 8 GB of VRAM? For a hobbyist.</title>
    <updated>2025-12-26T12:38:25+00:00</updated>
    <author>
      <name>/u/skincr</name>
      <uri>https://old.reddit.com/user/skincr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just looking as a hobbyist beginner. I already use the corporate chatbots for my serious works so I am not looking for a model to cure cancer. I am just looking for a small model to play with. What I am looking for is something small but good for its size. Maybe I would use it for organizing my personal text files like journal, notes, etc.&lt;/p&gt; &lt;p&gt;I tried Gemma 12B, although it is smarter, it was very slow at around 4 tokens per second. Llama 8B was much faster with 20 plus tokens per second, but it was noticeably more stupid. What would you recommend?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skincr"&gt; /u/skincr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3cfn/is_there_any_useful_small_size_model_for_rx_580/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3cfn/is_there_any_useful_small_size_model_for_rx_580/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3cfn/is_there_any_useful_small_size_model_for_rx_580/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T12:38:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw2yw7</id>
    <title>Non-native English, AI translation, and Reddit: where is the line? (A Korean farmer‚Äôs question)</title>
    <updated>2025-12-26T12:16:37+00:00</updated>
    <author>
      <name>/u/amadale</name>
      <uri>https://old.reddit.com/user/amadale</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a farmer who grows garlic in Korea.&lt;/p&gt; &lt;p&gt;When I don‚Äôt have farm work, I spend most of my time talking with AI. For the last 2 years, I also spent not small money on many famous paid AI plans around the world, and I did my own personal research and experiments. In this process, I always thought in my mother language, Korean, and I also talked with AI in Korean.&lt;/p&gt; &lt;p&gt;My thinking flow, my emotion, my intuition are tied to Korean. When it is translated to English, I often feel more than half is disappearing.&lt;/p&gt; &lt;p&gt;Still, I wanted to share on Reddit. So I organized many conversation logs and notes. For translation, I used AI help, but the final sentences and responsibility were mine. But today I found that one post I uploaded like that was removed. I did not think I broke rules seriously, so I was shocked.&lt;/p&gt; &lt;p&gt;I am confused: Did I do something wrong? Or does it look like a problem itself when a non-English user posts with AI assistance?&lt;/p&gt; &lt;p&gt;Let me explain my situation a bit more. I am not a professional researcher. I am just a farmer who experiments with AI using only a smartphone. I throw same or similar topics to multiple AIs (US, France, China, Korea models, etc.), and I observed differences and patterns.&lt;/p&gt; &lt;p&gt;Inside the chat window, I used a Python code interpreter and built something like a sandbox / virtual kernel. I applied the same structure to different AIs and cross-checked. I saved the results as thousands of logs in Google Drive, and I tried toÊï¥ÁêÜ (organize) some parts to share on Reddit.&lt;/p&gt; &lt;p&gt;When I write, my method is:&lt;/p&gt; &lt;p&gt;My original thinking and concepts are organized in Korean first&lt;/p&gt; &lt;p&gt;For draft writing / translation / proofreading, I get help from AI&lt;/p&gt; &lt;p&gt;But final content and responsibility is always mine as a human&lt;/p&gt; &lt;p&gt;Now I want to seriously ask these three questions:&lt;/p&gt; &lt;p&gt;If I disclose that I collaborated with AI, and I do final editing and take responsibility as a human, is this still a problem on Reddit?&lt;/p&gt; &lt;p&gt;For non-English users who think in their native language and use AI translation to join English communities, how far is allowed?&lt;/p&gt; &lt;p&gt;Policies that try to block ‚ÄúAI-heavy posts‚Äù ‚Äî could it also block personal experiment records like mine, even if my goal is honest sharing?&lt;/p&gt; &lt;p&gt;Even humans who speak the same language cannot communicate perfectly. If different language, different culture, and also human-AI translation are added, misunderstanding becomes more unavoidable.&lt;/p&gt; &lt;p&gt;I am just one person who lived through analog ÏãúÎåÄ and now smartphone ÏãúÎåÄ. Through conversations with AI, I felt many insights, and I want to share them in the most honest way I can.&lt;/p&gt; &lt;p&gt;If my approach has problems, I want to know: where is allowed, and where does it become an issue? I want to hear this community‚Äôs opinion. And I also want to ask: is it really this difficult for a non-English user to bring Korean thinking into English as honestly as possible?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amadale"&gt; /u/amadale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw2yw7/nonnative_english_ai_translation_and_reddit_where/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw2yw7/nonnative_english_ai_translation_and_reddit_where/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw2yw7/nonnative_english_ai_translation_and_reddit_where/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T12:16:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvx6a0</id>
    <title>TurboDiffusion ‚Äî 100‚Äì200√ó faster video diffusion on a single GPU</title>
    <updated>2025-12-26T06:04:06+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvx6a0/turbodiffusion_100200_faster_video_diffusion_on_a/"&gt; &lt;img alt="TurboDiffusion ‚Äî 100‚Äì200√ó faster video diffusion on a single GPU" src="https://preview.redd.it/b5vrplmioh9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=82a5c94a6996fffe772bd0c3adc81b86a0ee00d3" title="TurboDiffusion ‚Äî 100‚Äì200√ó faster video diffusion on a single GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open framework that speeds up end-to-end video generation by 100‚Äì200√ó while keeping quality, shown on a single RTX 5090. Ôøº ‚Ä¢ How: low-bit SageAttention + trainable Sparse-Linear Attention, rCM step distillation, and W8A8 quantization. Ôøº ‚Ä¢ Repo: &lt;a href="https://github.com/thu-ml/TurboDiffusion"&gt;https://github.com/thu-ml/TurboDiffusion&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/b5vrplmioh9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvx6a0/turbodiffusion_100200_faster_video_diffusion_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvx6a0/turbodiffusion_100200_faster_video_diffusion_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw6qvw</id>
    <title>Running a Local LLM for Development: Minimum Hardware, CPU vs GPU, and Best Models?</title>
    <updated>2025-12-26T15:22:46+00:00</updated>
    <author>
      <name>/u/Nervous-Blacksmith-3</name>
      <uri>https://old.reddit.com/user/Nervous-Blacksmith-3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôm new to this sub.&lt;/p&gt; &lt;p&gt;I‚Äôm considering running a local LLM. I‚Äôm a developer, and it‚Äôs pretty common for me to hit free-tier limits on hosted AIs, even with relatively basic interactions.&lt;/p&gt; &lt;p&gt;Right now, I only have a work laptop, and I‚Äôm fully aware that running a local LLM on it might be more a problem than just using free cloud options.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What would be the minimum laptop specs to comfortably run a local LLM for things like code completion, code generation, and general development suggestions?&lt;/li&gt; &lt;li&gt;Are there any LLMs that perform reasonably well on &lt;strong&gt;CPU-only&lt;/strong&gt; setups? I know CPU inference is possible, but are there models or configurations that are designed or well-optimized for CPUs?&lt;/li&gt; &lt;li&gt;Which LLMs offer the best &lt;strong&gt;performance vs quality&lt;/strong&gt; trade-off specifically for software development?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The main goal would be to integrate a local LLM into my main project/workflow to assist development and make it easier to retrieve context and understand what‚Äôs going on in a larger codebase.&lt;/p&gt; &lt;p&gt;Additionally, I currently use a ThinkPad with only an iGPU, but there are models with NVIDIA Quadro/Pro GPUs. Is there a meaningful performance gain when using those GPUs for local LLMs, or does it vary a lot depending on the model and setup?&lt;/p&gt; &lt;p&gt;The CPU question is partly curiosity: my current laptop has a Ryzen 7 Pro 5850U with 32GB of RAM, and during normal work I rarely fully utilize the CPU. I‚Äôm wondering if it‚Äôs worth trying a CPU-only local LLM first before committing to a more dedicated machine.&lt;/p&gt; &lt;p&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nervous-Blacksmith-3"&gt; /u/Nervous-Blacksmith-3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw6qvw/running_a_local_llm_for_development_minimum/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw6qvw/running_a_local_llm_for_development_minimum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw6qvw/running_a_local_llm_for_development_minimum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T15:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvvv8m</id>
    <title>Kimi-Linear Support in progress (you can download gguf and run it)</title>
    <updated>2025-12-26T04:50:52+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvvv8m/kimilinear_support_in_progress_you_can_download/"&gt; &lt;img alt="Kimi-Linear Support in progress (you can download gguf and run it)" src="https://external-preview.redd.it/Ez8JR9W3z41Aa9GoVbfLGF_GmJCt-mt-65CyiCbmgv4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c85131734185b5feabb39b027b4f431dac21c4a1" title="Kimi-Linear Support in progress (you can download gguf and run it)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's not reviewed, so don't get too excited yet &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18381"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvvv8m/kimilinear_support_in_progress_you_can_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvvv8m/kimilinear_support_in_progress_you_can_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T04:50:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvr64e</id>
    <title>A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster.</title>
    <updated>2025-12-26T00:41:51+00:00</updated>
    <author>
      <name>/u/Sudden_Rip7717</name>
      <uri>https://old.reddit.com/user/Sudden_Rip7717</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"&gt; &lt;img alt="A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster." src="https://preview.redd.it/go1uf72v2g9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a49f1c4f3df4fa34d397b34c3fcf1212cd609c5d" title="A Christmas Miracle: Managed to grab 3x RTX 5090 FE at MSRP for my home inference cluster." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;It has been a challenging year, but it has brought its own blessings too. I am truly grateful to God for so much more than just hardware, but I am also specifically thankful for this opportunity to upgrade my local AI research lab.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I just want to wish everyone here a Merry Christmas! Don't give up on your dreams, be ready to work hard, look boldly into the future, and try to enjoy every single day you live.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Merry Christmas and God bless!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sudden_Rip7717"&gt; /u/Sudden_Rip7717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go1uf72v2g9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvr64e/a_christmas_miracle_managed_to_grab_3x_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T00:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvjpmb</id>
    <title>Why I quit using Ollama</title>
    <updated>2025-12-25T18:38:36+00:00</updated>
    <author>
      <name>/u/SoLoFaRaDi</name>
      <uri>https://old.reddit.com/user/SoLoFaRaDi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For about a year, I've used Ollama like... 24/7. It was always my go-to, as it was frequently updated and had support for every model I needed.&lt;/p&gt; &lt;p&gt;Over the past few months, there's been a serious decline in the updates &amp;amp; update content that releases with Ollama. I understand that, and just went about my day, as the maintainers obviously have a life. Cool! Then the **Cloud** update dropped. I saw Ollama as a great model runner, you just download a model and boom. Nope! They decided to combine proprietary models with the models uploaded on their Library. At first, it seemed cool. We can now run AI models that were otherwise impossible to run on consumer hardware, but then I started getting confused. Why did they add in Cloud, what's the point? What were the privacy implications? It just felt like they were adding more and more bloatware into their already massive binaries, so about a month ago, I made the decision, and quit Ollama for good.&lt;/p&gt; &lt;p&gt;I feel like with every update they are seriously straying away from the main purpose of their application; to provide a secure inference platform for LOCAL AI models. I understand they're simply trying to fund their platform with the Cloud option, but it feels like a terrible move from the Ollama maintainers. &lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SoLoFaRaDi"&gt; /u/SoLoFaRaDi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvjpmb/why_i_quit_using_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T18:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvs8l3</id>
    <title>ASUS Rumored To Enter DRAM Market Next Year</title>
    <updated>2025-12-26T01:36:47+00:00</updated>
    <author>
      <name>/u/Highwaytothebeach</name>
      <uri>https://old.reddit.com/user/Highwaytothebeach</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well instead of learning about AI and having a pretty small chince finding a real job with that knoweledge actually seems that right now and in near future the most proffitable is investing in AI and tech stocks. And some people make money when stocks go sharp down.&lt;/p&gt; &lt;p&gt;Because of PC CPUs are locked at max 256 RAM support for too long and also DDR market looks weird lacking higher capacity widelly affordable modules in AI times, I was thinking tons of motherboards , barebones, PSUs and alot of other hardware is just going to hit recycling facilities, despite being reasonably priced.. And found this &lt;a href="https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor"&gt;https://wccftech.com/asus-enter-dram-market-next-year-to-tackle-memory-shortages-rumor&lt;/a&gt; Any chance it may be true?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Highwaytothebeach"&gt; /u/Highwaytothebeach &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvs8l3/asus_rumored_to_enter_dram_market_next_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T01:36:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw87le</id>
    <title>Why is Nemotron 3 acting so insecure?</title>
    <updated>2025-12-26T16:24:25+00:00</updated>
    <author>
      <name>/u/Ertowghan</name>
      <uri>https://old.reddit.com/user/Ertowghan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw87le/why_is_nemotron_3_acting_so_insecure/"&gt; &lt;img alt="Why is Nemotron 3 acting so insecure?" src="https://preview.redd.it/dwu4sle0rk9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=59f870881e3c5c70a6eb7e7e4d8f4d9f2cf3358e" title="Why is Nemotron 3 acting so insecure?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ertowghan"&gt; /u/Ertowghan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dwu4sle0rk9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw87le/why_is_nemotron_3_acting_so_insecure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw87le/why_is_nemotron_3_acting_so_insecure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw9n74</id>
    <title>[Model Release] Genesis-152M-Instruct, exploring hybrid attention + TTT at small scale</title>
    <updated>2025-12-26T17:23:11+00:00</updated>
    <author>
      <name>/u/Kassanar</name>
      <uri>https://old.reddit.com/user/Kassanar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôm sharing &lt;strong&gt;Genesis-152M-Instruct&lt;/strong&gt;, an &lt;strong&gt;experimental small language model&lt;/strong&gt; built to explore how &lt;em&gt;recent architectural ideas interact&lt;/em&gt; when combined in a single model ‚Äî especially under &lt;strong&gt;tight data constraints&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is &lt;strong&gt;research-oriented&lt;/strong&gt;, not a production model or SOTA claim.&lt;/p&gt; &lt;p&gt;üîç &lt;strong&gt;Why this might be interesting&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most recent architectures (GLA, FoX, TTT, ¬µP, sparsity) are tested &lt;strong&gt;in isolation&lt;/strong&gt; and usually at &lt;strong&gt;large scale&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I wanted to answer a simpler question:&lt;/p&gt; &lt;p&gt;&lt;em&gt;How much can architecture compensate for data at ~150M parameters?&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Genesis combines several &lt;strong&gt;ICLR 2024‚Äì2025 ideas&lt;/strong&gt; into one model and evaluates the result.&lt;/p&gt; &lt;p&gt;‚ö° &lt;strong&gt;TL;DR&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;152M parameters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Trained on &lt;strong&gt;~2B tokens&lt;/strong&gt; (vs ~2T for SmolLM2)&lt;/p&gt; &lt;p&gt;‚Ä¢ Hybrid &lt;strong&gt;GLA + FoX attention&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt; during inference&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;Selective Activation (sparse FFN)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ &lt;strong&gt;¬µP-scaled training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Fully open-source (Apache 2.0)&lt;/p&gt; &lt;p&gt;ü§ó Model: &lt;a href="https://huggingface.co/guiferrarib/genesis-152m-instruct"&gt;https://huggingface.co/guiferrarib/genesis-152m-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üì¶ pip install genesis-llm&lt;/p&gt; &lt;p&gt;üìä &lt;strong&gt;Benchmarks (LightEval, Apple MPS)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ARC-Easy ‚Üí 44.0% (random: 25%)&lt;/p&gt; &lt;p&gt;BoolQ ‚Üí 56.3% (random: 50%)&lt;/p&gt; &lt;p&gt;HellaSwag ‚Üí 30.2% (random: 25%)&lt;/p&gt; &lt;p&gt;SciQ ‚Üí 46.8% (random: 25%)&lt;/p&gt; &lt;p&gt;Winogrande ‚Üí 49.1% (random: 50%)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important context:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SmolLM2-135M was trained on &lt;strong&gt;~2 trillion tokens&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Genesis uses &lt;strong&gt;~2 billion tokens&lt;/strong&gt; ‚Äî so this is not a fair head-to-head, but an exploration of &lt;strong&gt;architecture vs data scaling&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;üß† &lt;strong&gt;Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hybrid Attention (Qwen3-Next inspired)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Layer&lt;/strong&gt; &lt;strong&gt;%&lt;/strong&gt; &lt;strong&gt;Complexity&lt;/strong&gt; &lt;strong&gt;Role&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Gated DeltaNet (GLA) 75% O(n) Long-range efficiency&lt;/p&gt; &lt;p&gt;FoX (Forgetting Attention) 25% O(n¬≤) Precise retrieval&lt;/p&gt; &lt;p&gt;GLA uses:&lt;/p&gt; &lt;p&gt;‚Ä¢ Delta rule memory updates&lt;/p&gt; &lt;p&gt;‚Ä¢ Mamba-style gating&lt;/p&gt; &lt;p&gt;‚Ä¢ L2-normalized Q/K&lt;/p&gt; &lt;p&gt;‚Ä¢ Short convolutions&lt;/p&gt; &lt;p&gt;FoX adds:&lt;/p&gt; &lt;p&gt;‚Ä¢ Softmax attention&lt;/p&gt; &lt;p&gt;‚Ä¢ Data-dependent forget gate&lt;/p&gt; &lt;p&gt;‚Ä¢ Output gating&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test-Time Training (TTT)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of frozen inference, Genesis can &lt;strong&gt;adapt online&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;‚Ä¢ Dual-form TTT (parallel gradients)&lt;/p&gt; &lt;p&gt;‚Ä¢ Low-rank updates (rank=4)&lt;/p&gt; &lt;p&gt;‚Ä¢ Learnable inner learning rate&lt;/p&gt; &lt;p&gt;Paper: &lt;em&gt;Learning to (Learn at Test Time)&lt;/em&gt; (MIT, ICML 2024)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Selective Activation (Sparse FFN)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SwiGLU FFNs with &lt;strong&gt;top-k activation masking&lt;/strong&gt; (85% kept).&lt;/p&gt; &lt;p&gt;Currently acts as &lt;strong&gt;regularization&lt;/strong&gt; ‚Äî real speedups need sparse kernels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;¬µP Scaling + Zero-Centered RMSNorm&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Hyperparameters tuned on small proxy&lt;/p&gt; &lt;p&gt;‚Ä¢ Transferred via ¬µP rules&lt;/p&gt; &lt;p&gt;‚Ä¢ Zero-centered RMSNorm for stable scaling&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Limitations (honest)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Small training corpus (2B tokens)&lt;/p&gt; &lt;p&gt;‚Ä¢ TTT adds ~5‚Äì10% inference overhead&lt;/p&gt; &lt;p&gt;‚Ä¢ No RLHF&lt;/p&gt; &lt;p&gt;‚Ä¢ Experimental, not production-ready&lt;/p&gt; &lt;p&gt;üìé &lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ ü§ó Model: &lt;a href="https://huggingface.co/guiferrarib/genesis-152m-instruct"&gt;https://huggingface.co/guiferrarib/genesis-152m-instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ üì¶ PyPI: &lt;a href="https://pypi.org/project/genesis-llm/"&gt;https://pypi.org/project/genesis-llm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate feedback ‚Äî especially from folks working on &lt;strong&gt;linear attention&lt;/strong&gt;, &lt;strong&gt;hybrid architectures&lt;/strong&gt;, or &lt;strong&gt;test-time adaptation&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Built by Orch-Mind Team&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kassanar"&gt; /u/Kassanar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw9n74/model_release_genesis152minstruct_exploring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T17:23:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxmqt</id>
    <title>Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]</title>
    <updated>2025-12-26T06:32:16+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt; &lt;img alt="Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]" src="https://b.thumbs.redditmedia.com/3L7s9Y4g4SXcNJbIUac3xq5NcNnmOFl2SLzQbPE94bI.jpg" title="Finally a Kimi-Linear-48B-A3B GGUF! [Experimental PR]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Yes, it's finally happening! I recently pushed some changes and have gotten Kimi-Linear to work (fully; fingers crossed) PR (#18381). &lt;/p&gt; &lt;p&gt;I've tested it heavily on Q2_K (mind BLOWING coherence :), and it‚Äôs now passing logic puzzles, long-context essay generation, and basic math - all of which were previously broken.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mjychgkcth9g1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f02c3fda1ea59629b4aac6664cc7c4a071f7ebd1"&gt;q2_k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Resources:&lt;/p&gt; &lt;p&gt;PR Branch: &lt;a href="http://github.com/ggml-org/llama.cpp/pull/18381"&gt;github.com/ggml-org/llama.cpp/pull/18381&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs (Use above PR): &lt;a href="https://huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;huggingface.co/AaryanK/Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Use this free Colab notebook or copy the code from it for a quick start :) &lt;a href="https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing"&gt;https://colab.research.google.com/drive/1NMHMmmht-jxyfZqJr5xMlOE3O2O4-WDq?usp=sharing&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Please give it a spin and let me know if you run into any divergent logits or loops!&lt;/p&gt; &lt;p&gt;I am currently looking for open positions! ü§ó&lt;/p&gt; &lt;p&gt;If you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: &lt;a href="https://www.linkedin.com/in/theaaryankapoor/"&gt;Aaryan Kapoor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxmqt/finally_a_kimilinear48ba3b_gguf_experimental_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:32:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw7g4e</id>
    <title>KTransformers supports MiniMax M2.1 - 2x5090 + 768GB DRAM yeilds prefill 4000 tps, decode 33 tps.</title>
    <updated>2025-12-26T15:52:37+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"&gt; &lt;img alt="KTransformers supports MiniMax M2.1 - 2x5090 + 768GB DRAM yeilds prefill 4000 tps, decode 33 tps." src="https://b.thumbs.redditmedia.com/NjAzHMEVH0VMgscXKhOmqRO-3O_c3O0kgKZ0xzULhCU.jpg" title="KTransformers supports MiniMax M2.1 - 2x5090 + 768GB DRAM yeilds prefill 4000 tps, decode 33 tps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are excited to announce support for &lt;strong&gt;MiniMax M2.1&lt;/strong&gt; in its original FP8 format (no quantization).&lt;/p&gt; &lt;p&gt;We tested this setup on a high-end local build to see how far we could push the bandwidth.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; 2x RTX 5090&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System RAM:&lt;/strong&gt; 768GB DRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; Native FP8&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prefill:&lt;/strong&gt; ~4000 tokens/s (Saturating PCIe 5.0 bandwidth)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Decode:&lt;/strong&gt; 33 tokens/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/pjaf5y7glk9g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdf654e2f426c24235f0f7837528a570627e6bb"&gt;https://preview.redd.it/pjaf5y7glk9g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdf654e2f426c24235f0f7837528a570627e6bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ktransformers-supports-minimax-m2-1-2x5090-768gb-dram-v0-pkn23v48lk9g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bb17a08354a9ae97fe47aec37999db6af2b6bc84"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This implementation is designed to fully exploit the PCIe 5.0 bus during the prefill phase. If you have the hardware to handle the memory requirements, the throughput is significant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw7g4e/ktransformers_supports_minimax_m21_2x5090_768gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T15:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw5360</id>
    <title>MLX community already added support for Minimax-M2.1</title>
    <updated>2025-12-26T14:06:29+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"&gt; &lt;img alt="MLX community already added support for Minimax-M2.1" src="https://preview.redd.it/phwy35uk2k9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6dd981b9774d7410723451975474cfd7b8d6908c" title="MLX community already added support for Minimax-M2.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/phwy35uk2k9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw5360/mlx_community_already_added_support_for_minimaxm21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T14:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvwlfh</id>
    <title>systemctl disable ollama</title>
    <updated>2025-12-26T05:30:55+00:00</updated>
    <author>
      <name>/u/copenhagen_bram</name>
      <uri>https://old.reddit.com/user/copenhagen_bram</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"&gt; &lt;img alt="systemctl disable ollama" src="https://preview.redd.it/8qvw6jdjih9g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=67cb047d78cc712448a65395f1aff5b8269410ca" title="systemctl disable ollama" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;151GB timeshift snapshot composed of mainly Flatpak repo data (Alpaca?) and /usr/share/ollama&lt;/p&gt; &lt;p&gt;From now on I'm storing models in my home directory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/copenhagen_bram"&gt; /u/copenhagen_bram &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qvw6jdjih9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvwlfh/systemctl_disable_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T05:30:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvz7v2</id>
    <title>Minimax M2.1 released</title>
    <updated>2025-12-26T08:13:29+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to xcancel: &lt;a href="https://xcancel.com/ModelScope2022/status/2004462984698253701#m"&gt;https://xcancel.com/ModelScope2022/status/2004462984698253701#m&lt;/a&gt;&lt;/p&gt; &lt;p&gt;New on ModelScope: MiniMax M2.1 is open-source!&lt;/p&gt; &lt;p&gt;‚úÖ SOTA in 8+ languages (Rust, Go, Java, C++, TS, Kotlin, Obj-C, JS) ‚úÖ Full-stack Web &amp;amp; mobile dev: Android/iOS, 3D visuals, vibe coding that actually ships ‚úÖ Smarter, faster, 30% fewer tokens ‚Äî with lightning mode (M2.1-lightning) for high-TPS workflows ‚úÖ Top-tier on SWE-bench, VIBE, and custom coding/review benchmarks ‚úÖ Works flawlessly in Cursor, Cline, Droid, BlackBox, and more&lt;/p&gt; &lt;p&gt;It‚Äôs not just ‚Äúbetter code‚Äù ‚Äî it‚Äôs AI-native development, end to end.&lt;/p&gt; &lt;p&gt;&lt;a href="https://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary"&gt;https://modelscope.cn/models/MiniMax/MiniMax-M2.1/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvz7v2/minimax_m21_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T08:13:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvxq2t</id>
    <title>Hard lesson learned after a year of running large models locally</title>
    <updated>2025-12-26T06:38:00+00:00</updated>
    <author>
      <name>/u/inboundmage</name>
      <uri>https://old.reddit.com/user/inboundmage</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, go easy with me I'm new at running large models.&lt;/p&gt; &lt;p&gt;After spending about 12 months tinkering with locally hosted LLMs, I thought I had my setup dialed in. I‚Äôm running everything off a workstation with a single RTX 3090, Ubuntu 22.04, llama.cpp for smaller models and vLLM for anything above 30 B parameters. &lt;/p&gt; &lt;p&gt;My goal has always been to avoid cloud dependencies and keep as much computation offline as possible, so I‚Äôve tried every quantization trick and caching tweak I could find.&lt;/p&gt; &lt;p&gt;The biggest friction point has been scaling beyond 13 B models. &lt;/p&gt; &lt;p&gt;Even with 24 GB of VRAM, running a 70 B model in int4 still exhausts memory when the context window grows and attention weights balloon. &lt;/p&gt; &lt;p&gt;Offloading to system RAM works, but inference latency spikes into seconds, and batching requests becomes impossible. &lt;/p&gt; &lt;p&gt;I‚Äôve also noticed that GPU VRAM fragmentation accumulates over time when swapping between models, after a few hours, vLLM refuses to load a model that would normally fit because of leftover allocations.&lt;/p&gt; &lt;p&gt;My takeaway so far is that local first inference is viable for small to medium models, but there‚Äôs a hard ceiling unless you invest in server grade hardware or cluster multiple GPUs. &lt;/p&gt; &lt;p&gt;Quantization helps, but you trade some quality and run into new bugs. &lt;/p&gt; &lt;p&gt;For privacy sensitive tasks, the trade‚Äëoff is worth it; for fast iteration, it‚Äôs been painful compared to cloud based runners. &lt;/p&gt; &lt;p&gt;I‚Äôm curious if anyone has found a reliable way to manage VRAM fragmentation or offload attention blocks more efficiently on consumer cards, or whether the answer is simply ‚Äúbuy more VRAM.‚Äù &lt;/p&gt; &lt;p&gt;How are others solving this without compromising on running fully offline?&lt;/p&gt; &lt;p&gt;Thx&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inboundmage"&gt; /u/inboundmage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvxq2t/hard_lesson_learned_after_a_year_of_running_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T06:38:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pvpkqo</id>
    <title>I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA</title>
    <updated>2025-12-25T23:21:39+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt; &lt;img alt="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" src="https://external-preview.redd.it/eHAyeXBnM2xvZjlnMcbYDDf5MmPAc5-kZmkvzc1kUbOViw5SF6SuJ_dOojri.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b4e5d7a038251df406b6345161c5136f2011960" title="I wish this GPU VRAM upgrade modification became mainstream and ubiquitous to shred monopoly abuse of NVIDIA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/u1mxlc3lof9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pvpkqo/i_wish_this_gpu_vram_upgrade_modification_became/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-25T23:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8h6w</id>
    <title>GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB</title>
    <updated>2025-12-26T16:35:28+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt; &lt;img alt="GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB" src="https://b.thumbs.redditmedia.com/M2P6WP9rpl4ZOUlAGcCSpfB_YOF4tnbUEiVovuoomHc.jpg" title="GLM-4.7-6bit MLX vs MiniMax-M2.1-6bit MLX Benchmark Results on M3 Ultra 512GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i find the benchmark result from twitter, which is very interesting.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Hardware: Apple M3 Ultra, 512GB. All tests with single M3 Ultra &lt;strong&gt;without batch inference&lt;/strong&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zwqsxk9btk9g1.png?width=4052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1940693109fab3938946786fb719ad07bd73345c"&gt;glm-4.7&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0nkcz4fetk9g1.png?width=4052&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=48a2d1eba5e5dd4ce8ecce705b01468c4931c47c"&gt;minimax-m2.1&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM-4.7-6bit MLX Benchmark Results with different context sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;0.5k Prompt: 98 - Gen: 16 t/s - 287.6GB&lt;br /&gt; 1k Prompt: 140 - Gen: 17 t/s - 288.0GB&lt;br /&gt; 2k Prompt: 206 - Gen: 16 t/s - 288.8GB&lt;br /&gt; 4k Prompt: 219 - Gen: 16 t/s - 289.6GB&lt;br /&gt; 8k Prompt: 210 - Gen: 14 t/s - 291.0GB&lt;br /&gt; 16k Prompt: 185 - Gen: 12 t/s - 293.9GB&lt;br /&gt; 32k Prompt: 134 - Gen: 10 t/s - 299.8GB&lt;br /&gt; 64k Prompt: 87 - Gen: 6 t/s - 312.1GB&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MiniMax-M2.1-6bit MLX Benchmark raw results with different context sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;0.5k Prompt: 239 - Gen: 42 t/s - 186.5GB&lt;br /&gt; 1k Prompt: 366 - Gen: 41 t/s - 186.8GB&lt;br /&gt; 2k Prompt: 517 - Gen: 40 t/s - 187.2GB&lt;br /&gt; 4k Prompt: 589 - Gen: 38 t/s - 187.8GB&lt;br /&gt; 8k Prompt: 607 - Gen: 35 t/s - 188.8GB&lt;br /&gt; 16k Prompt: 549 - Gen: 30 t/s - 190.9GB&lt;br /&gt; 32k Prompt: 429 - Gen: 21 t/s - 195.1GB&lt;br /&gt; 64k Prompt: 291 - Gen: 12 t/s - 203.4GB&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I would prefer minimax-m2.1 for general usage from the benchmark result, about &lt;strong&gt;~2.5x&lt;/strong&gt; prompt processing speed, &lt;strong&gt;~2x&lt;/strong&gt; token generation speed&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;p&gt;sources: &lt;a href="https://x.com/ivanfioravanti/status/2004578941408039051"&gt;glm-4.7&lt;/a&gt; , &lt;a href="https://x.com/ivanfioravanti/status/2004569464407474555"&gt;minimax-m2.1&lt;/a&gt;, &lt;a href="https://x.com/ivanfioravanti/status/2004602428122169650"&gt;4bit-comparison&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p7kp5hcv1l9g1.jpg?width=1841&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c66839601a68efa3baf6c845bce91e8c2c8c2254"&gt;4bit-6bit-comparison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- It seems that 4bit and 6bit have similar speed for prompt processing and token generation.&lt;br /&gt; - for the same model, 6bit's memory usage is about &lt;strong&gt;~1.4x&lt;/strong&gt; of 4bit. since RAM/VRAM is so expensive now, maybe it's not worth it (128GB x 1.4 = 179.2GB)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8h6w/glm476bit_mlx_vs_minimaxm216bit_mlx_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw701k</id>
    <title>MiniMax-M2.1 GGUF is here!</title>
    <updated>2025-12-26T15:33:38+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"&gt; &lt;img alt="MiniMax-M2.1 GGUF is here!" src="https://external-preview.redd.it/0xe3vYLHuf2Mb8WiNbMmuRGbcT2eNARsH6mkzOnOBgQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af9bda8765a5deb37e3c09288310949ab2d8704a" title="MiniMax-M2.1 GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I might've skipped going to bed for this one: &lt;a href="https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF"&gt;https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From my runs:&lt;/p&gt; &lt;p&gt;model: MiniMax-M2.1.q2_k.gguf&lt;br /&gt; GPU: NVIDIA A100-SXM4-80GB&lt;/p&gt; &lt;p&gt;n_gpu_layers: 55&lt;br /&gt; context_size: 32768&lt;br /&gt; temperature: 0.7&lt;br /&gt; top_p: 0.9&lt;br /&gt; top_k: 40&lt;br /&gt; max_tokens: 512&lt;br /&gt; repeat_penalty: 1.1&lt;/p&gt; &lt;p&gt;[ Prompt: 28.0 t/s | Generation: 25.4 t/s ]&lt;/p&gt; &lt;p&gt;I am currently looking for open positions! ü§ó &lt;/p&gt; &lt;p&gt;If you find this model useful or are looking for a talented AI/LLM Engineer, please reach out to me on LinkedIn: &lt;a href="https://www.linkedin.com/in/theaaryankapoor/"&gt;Aaryan Kapoor&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy holidays!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/MiniMax-M2.1-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw701k/minimaxm21_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T15:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw8nfk</id>
    <title>Nvidia acquired Groq, but why not Cerebras? Cerebras is 3x times faster than Groq, while maximum 1.5x the price. Anyone can explain?</title>
    <updated>2025-12-26T16:42:23+00:00</updated>
    <author>
      <name>/u/Conscious_Warrior</name>
      <uri>https://old.reddit.com/user/Conscious_Warrior</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone with technical knowledge can explain why they chose Groq over Cerebras? Really interested in this. Because Cerebras is even waaay faster than Groq. Cerebras seems like a bigger threat to Nvidia than Groq...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Conscious_Warrior"&gt; /u/Conscious_Warrior &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw8nfk/nvidia_acquired_groq_but_why_not_cerebras/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T16:42:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pw3fih</id>
    <title>MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp; agents</title>
    <updated>2025-12-26T12:43:08+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"&gt; &lt;img alt="MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp;amp; agents" src="https://preview.redd.it/mxsku2dnnj9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b27c9e0dbc5e46995d16f434d126d93ba14f68da" title="MiniMax M2.1 is OPEN SOURCE: SOTA for real-world dev &amp;amp; agents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;https://huggingface.co/MiniMaxAI/MiniMax-M2.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SOTA on coding benchmarks (SWE / VIBE / Multi-SWE) ‚Ä¢ Beats Gemini 3 Pro &amp;amp; Claude Sonnet 4.5 ‚Ä¢ 10B active / 230B total (MoE)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mxsku2dnnj9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pw3fih/minimax_m21_is_open_source_sota_for_realworld_dev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T12:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
