<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-06-27T01:36:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1lldkdg</id>
    <title>AutoInference: Multiple inference options in a single library</title>
    <updated>2025-06-26T22:25:30+00:00</updated>
    <author>
      <name>/u/According-Local-9704</name>
      <uri>https://old.reddit.com/user/According-Local-9704</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lldkdg/autoinference_multiple_inference_options_in_a/"&gt; &lt;img alt="AutoInference: Multiple inference options in a single library" src="https://preview.redd.it/0isu7rxjkc9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f774d9d78464b449a1b89c9b9a1cad7d4ca591ca" title="AutoInference: Multiple inference options in a single library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Auto-Inference is a Python library that provides a unified interface for model inference using several popular backends, including Hugging Face's Transformers, Unsloth, and vLLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Local-9704"&gt; /u/According-Local-9704 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0isu7rxjkc9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lldkdg/autoinference_multiple_inference_options_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lldkdg/autoinference_multiple_inference_options_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T22:25:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1llhdoq</id>
    <title>I'm using a local Llama model for my game's dialogue system!</title>
    <updated>2025-06-27T01:23:40+00:00</updated>
    <author>
      <name>/u/LandoRingel</name>
      <uri>https://old.reddit.com/user/LandoRingel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt; &lt;img alt="I'm using a local Llama model for my game's dialogue system!" src="https://external-preview.redd.it/c2JvZG9ndjVnZDlmMe7CY4SqtJeZEukasJn79Adjh2cJgmt44HDkzVTcUucN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24a31f419b54bcf613f907d27abae7c2526e8092" title="I'm using a local Llama model for my game's dialogue system!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm blown away by how fast and intelligent Llama 3.2 is!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LandoRingel"&gt; /u/LandoRingel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cgoobkv5gd9f1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llhdoq/im_using_a_local_llama_model_for_my_games/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T01:23:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1llewyp</id>
    <title>Can Llamcpp run gemma 3n?</title>
    <updated>2025-06-26T23:25:10+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llewyp/can_llamcpp_run_gemma_3n/"&gt; &lt;img alt="Can Llamcpp run gemma 3n?" src="https://external-preview.redd.it/ksRJC2bKGwjrMfOqsioi-B4oIm5QWQUM7Vf03KwieGM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f6fc5d8f727ab6f86a8ca5f94a5091bbe81d025" title="Can Llamcpp run gemma 3n?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I followed the instructions here, but when I try to run I get unknown architecture gemma3n error. Is it not supported and I fell for a generate doc?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llewyp/can_llamcpp_run_gemma_3n/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llewyp/can_llamcpp_run_gemma_3n/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T23:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll3j07</id>
    <title>Anubis 70B v1.1 - Just another RP tune... unlike any other L3.3! (allegedly) A breath of fresh prose and lack of positivity (YMMV ofc) + bonus Fallen 70B for mergefuel! (because tuners aren't limited to RP)</title>
    <updated>2025-06-26T15:46:44+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3j07/anubis_70b_v11_just_another_rp_tune_unlike_any/"&gt; &lt;img alt="Anubis 70B v1.1 - Just another RP tune... unlike any other L3.3! (allegedly) A breath of fresh prose and lack of positivity (YMMV ofc) + bonus Fallen 70B for mergefuel! (because tuners aren't limited to RP)" src="https://external-preview.redd.it/_eIF0Xo1buph34Tuk-bXjGb0GyE839b8Ocdfqz4UUok.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d95c24031eb18602df8ccce61ee45bba3ac35bc3" title="Anubis 70B v1.1 - Just another RP tune... unlike any other L3.3! (allegedly) A breath of fresh prose and lack of positivity (YMMV ofc) + bonus Fallen 70B for mergefuel! (because tuners aren't limited to RP)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Did you like Fallen R1? Here's the non-R1 version: &lt;a href="https://huggingface.co/TheDrummer/Fallen-Llama-3.3-70B-v1"&gt;https://huggingface.co/TheDrummer/Fallen-Llama-3.3-70B-v1&lt;/a&gt; Enjoy the mergefuel!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Anubis-70B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3j07/anubis_70b_v11_just_another_rp_tune_unlike_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll3j07/anubis_70b_v11_just_another_rp_tune_unlike_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1llawcf</id>
    <title>Arch-Agent Family of LLMs - Designed for fast, multi-step agent orchestration.</title>
    <updated>2025-06-26T20:34:27+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llawcf/archagent_family_of_llms_designed_for_fast/"&gt; &lt;img alt="Arch-Agent Family of LLMs - Designed for fast, multi-step agent orchestration." src="https://external-preview.redd.it/3XQOsvT905GnlfeEvoOsJJEZYArlF_pQKyhe7nzO-Iw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ab599907c513b7c3afed9b3994cf2b4d3d04b5c" title="Arch-Agent Family of LLMs - Designed for fast, multi-step agent orchestration." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7l6dn5qo0c9f1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed57df70215bfcd2958598973c4501e30b55bb71"&gt;https://preview.redd.it/7l6dn5qo0c9f1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ed57df70215bfcd2958598973c4501e30b55bb71&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Launch #3 for the week üöÄ - We announced Arch-Agent-7B on Tuesday. Today, I introduce the Arch-Agent family of LLMs. The worlds fastest agentic models that run laps around top proprietary models. &lt;/p&gt; &lt;p&gt;Arch-Agent LLMs are designed for multi-step, multi-turn workflow orchestration scenarios and intended for application settings where the model has access to a system-of-record, knowledge base or 3rd-party APIs.&lt;/p&gt; &lt;p&gt;Btw what is agent orchestration? Its the ability for an LLM to plan and execute complex user tasks based on access to the environment (internal APIs, 3rd party services, and knowledge bases). The agency on what the LLM can do and achieve is guided by human-defined policies written in plain ol' english.&lt;/p&gt; &lt;p&gt;Why are we building these? Because its crucial technology for the agentic future, but also because they will power Arch: the universal data plane for AI that handles the low-level plumbing work in building and scaling agents so that you can focus on higher-level logic and move faster. All without locking you in clunky programming frameworks.&lt;/p&gt; &lt;p&gt;Link to Arch-Agent LLMs: &lt;a href="https://huggingface.co/collections/katanemo/arch-agent-685486ba8612d05809a0caef"&gt;https://huggingface.co/collections/katanemo/arch-agent-685486ba8612d05809a0caef&lt;/a&gt;&lt;br /&gt; Link to Arch: &lt;a href="https://github.com/katanemo/archgw"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llawcf/archagent_family_of_llms_designed_for_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llawcf/archagent_family_of_llms_designed_for_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llawcf/archagent_family_of_llms_designed_for_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll49jc</id>
    <title>My Python AI Dev Tool: Avakin - Local LLMs, Project-Specific + Global RAG, &amp; More</title>
    <updated>2025-06-26T16:15:09+00:00</updated>
    <author>
      <name>/u/One_Negotiation_2078</name>
      <uri>https://old.reddit.com/user/One_Negotiation_2078</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll49jc/my_python_ai_dev_tool_avakin_local_llms/"&gt; &lt;img alt="My Python AI Dev Tool: Avakin - Local LLMs, Project-Specific + Global RAG, &amp;amp; More" src="https://preview.redd.it/qiuq20a1pa9f1.gif?width=640&amp;amp;crop=smart&amp;amp;s=dae7fb2838824ee2e900107ef0dde71780954483" title="My Python AI Dev Tool: Avakin - Local LLMs, Project-Specific + Global RAG, &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been working on a project called Avakin, a desktop AI development environment for Python, and wanted to share it with this community. My goal was to create a tool that deeply integrates with the development workflow, leverages local LLMs for privacy and control, and actually understands the context of individual projects.&lt;/p&gt; &lt;p&gt;Avakin runs entirely on your local machine (Windows for packaged release, source runs cross-platform). It's built with Python/PySide6 and orchestrates a team of AI agents (Architect, Coder, etc.) that can be configured to use different LLMs via a local FastAPI backend. This backend interfaces with Ollama for local models (Llama 3, Mistral, CodeLlama, etc.) or can call out to cloud APIs if you provide keys.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/carpsesdema/AvA_Kintsugi"&gt;https://github.com/carpsesdema/AvA_Kintsugi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a breakdown of the core technical features:&lt;/p&gt; &lt;p&gt;Dual-Context Local RAG (Project &amp;amp; Global Knowledge):&lt;/p&gt; &lt;p&gt;Technology:** Utilizes `SentenceTransformers` (`all-MiniLM-L6-v2` by default) for embeddings and `ChromaDB` for persistent local vector storage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Project-Specific DBs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each Python project you work on gets its *own isolated `rag_db` directory*. This allows Avakin to build a deep understanding of your current project's specifics (like Game Design Documents, API schemas, or existing proprietary code) without context bleed from other work. The RAG server dynamically switches its active project DB when you switch projects in Avakin.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Global Knowledge Base:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Simultaneously, Avakin supports a separate, persistent global RAG collection (its path configured via the `GLOBAL_RAG_DB_PATH` env var). This is perfect for your large corpus of general Python code examples, programming best practices, or any technical documentation you want the AI to reference across all projects.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Synergistic Context&lt;/strong&gt;: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;When planning, coding, or chatting, AI agents can be fed context retrieved from *both* the active project's RAG and the global RAG. This allows for highly relevant, project-aware suggestions that are also informed by broad, general knowledge.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Seamless Chat-to-Code Workflow:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Brainstorm ideas or discuss code with the chat AI (which also benefits from the Dual-Context RAG).&lt;/li&gt; &lt;li&gt; If an AI response in the chat contains a good idea or a snippet you want to build upon, you can instantly send that chat message's content to Avakin's &amp;quot;Build&amp;quot; mode with a right-click. This pre-populates the build prompt, allowing a smooth transition from conversation to code generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Local LLM Orchestration (Ollama Focus):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A dedicated local FastAPI server (`llm_server.py`) acts as a unified gateway to various LLM providers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Native Ollama Support:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Directly streams responses from any model hosted by your local Ollama instance (Llama 3, Mistral, CodeLlama, etc.).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Configurable AI Agent Roles:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can assign different models (local or cloud) to distinct roles like 'Architect' (for planning), 'Coder' (for file generation), 'Reviewer' (for debugging), and 'Chat'. This allows for optimizing performance and capability (e.g., a powerful local model for coding, a smaller/faster one for chat).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Full Project Scaffolding &amp;amp; Generation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; From a single prompt, the 'Architect' agent (using its configured LLM and the powerful Dual-Context RAG) designs a multi-file Python application structure.&lt;/li&gt; &lt;li&gt; The 'Coder' agent then generates each file, with access to a dynamically updated symbol index of the project and the full code of already generated files in the current session, promoting better integration.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Surgical Code Modification &amp;amp; Debugging:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Accepts natural language requests to modify existing codebases. The AI is provided with the current code, project structure, and relevant RAG context.&lt;/li&gt; &lt;li&gt; One-Click Debugging: When a script run in the integrated terminal fails, Avakin captures the traceback. The 'Reviewer' agent analyzes this&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm still actively developing Avakin and would love to get your thoughts and feedback, especially from fellow local LLM enthusiasts! What features would you find most useful? Any pain points in local AI development that Avakin could help address?&lt;/p&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Negotiation_2078"&gt; /u/One_Negotiation_2078 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qiuq20a1pa9f1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll49jc/my_python_ai_dev_tool_avakin_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll49jc/my_python_ai_dev_tool_avakin_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:15:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lko09j</id>
    <title>Google's CLI DOES use your prompting data</title>
    <updated>2025-06-26T01:54:24+00:00</updated>
    <author>
      <name>/u/Physical_Ad9040</name>
      <uri>https://old.reddit.com/user/Physical_Ad9040</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt; &lt;img alt="Google's CLI DOES use your prompting data" src="https://preview.redd.it/j1km6ff1h69f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=183f6cf57cbd408bb1e17247c8aba72d8086d1a3" title="Google's CLI DOES use your prompting data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Physical_Ad9040"&gt; /u/Physical_Ad9040 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j1km6ff1h69f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lko09j/googles_cli_does_use_your_prompting_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T01:54:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1llcb9x</id>
    <title>Tilde pits DeepSeek‚Äôs ‚ÄúNSA‚Äù vs Kimi‚Äôs ‚ÄúMoBA‚Äù sparse attention - the key to long-context LLM</title>
    <updated>2025-06-26T21:31:45+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llcb9x/tilde_pits_deepseeks_nsa_vs_kimis_moba_sparse/"&gt; &lt;img alt="Tilde pits DeepSeek‚Äôs ‚ÄúNSA‚Äù vs Kimi‚Äôs ‚ÄúMoBA‚Äù sparse attention - the key to long-context LLM" src="https://external-preview.redd.it/b9PAY9Uys9eVz0QmKOo2RFmkWMPBPY0JczoG9wyn_wQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c78985ccd8af3f324bfd3344dfd9023abd3df796" title="Tilde pits DeepSeek‚Äôs ‚ÄúNSA‚Äù vs Kimi‚Äôs ‚ÄúMoBA‚Äù sparse attention - the key to long-context LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished Tilde Research‚Äôs new blog on sparse attention. They benchmark the two schemes in Chinese long-context models‚ÄîDeepSeek‚Äôs Native Sparse Attention (NSA) and Moonshot/Kimi‚Äôs Mixture of Block Attention (MoBA)‚Äîagainst full attention.&lt;/p&gt; &lt;p&gt;Sparse attention exploits inherent sparsity in model attention patterns to dramatically accelerate sequence mixing. Natively trainable approaches, such as Kimi‚Äôs MoBA and Deepseek‚Äôs NSA, expand the pareto frontier by matching and even outcompeting base attention on expressivity respectively.&lt;/p&gt; &lt;p&gt;They trained dozens of sparse attention models and poked around in their brains. Sparse attention models boost superior long-context generalization capability out of box, even with 80% sparsity in attention scores.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/iccpjm7pac9f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31fa21c784c94a877515545c26fb221c1b579f9c"&gt;https://preview.redd.it/iccpjm7pac9f1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=31fa21c784c94a877515545c26fb221c1b579f9c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also created a series of exquisite interactive visualizations to present the experimental results, which are definitely worth a look. &lt;/p&gt; &lt;p&gt;Read the full post here: &lt;a href="https://www.tilderesearch.com/blog/sparse-attn"&gt;Sparsity is Cool&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also released their NSA kernel for experimentation: &lt;a href="https://github.com/tilde-research/nsa-impl"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llcb9x/tilde_pits_deepseeks_nsa_vs_kimis_moba_sparse/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llcb9x/tilde_pits_deepseeks_nsa_vs_kimis_moba_sparse/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llcb9x/tilde_pits_deepseeks_nsa_vs_kimis_moba_sparse/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T21:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll5w6m</id>
    <title>Gemma 3n is now stable on HuggingFace</title>
    <updated>2025-06-26T17:18:14+00:00</updated>
    <author>
      <name>/u/best_codes</name>
      <uri>https://old.reddit.com/user/best_codes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll5w6m/gemma_3n_is_now_stable_on_huggingface/"&gt; &lt;img alt="Gemma 3n is now stable on HuggingFace" src="https://external-preview.redd.it/ELsux0mwxWZPvalnHOQRSKe_mzDmS7uNjsKunK8e1U8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abfda3ccbbaaecf31b079783cc429aaf9abad256" title="Gemma 3n is now stable on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/best_codes"&gt; /u/best_codes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll5w6m/gemma_3n_is_now_stable_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll5w6m/gemma_3n_is_now_stable_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll321h</id>
    <title>From "LangGraph is trash" to "pip install langgraph": A Stockholm Syndrome Story</title>
    <updated>2025-06-26T15:28:08+00:00</updated>
    <author>
      <name>/u/FailingUpAllDay</name>
      <uri>https://old.reddit.com/user/FailingUpAllDay</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Listen, I get it. We all hate LangGraph. The documentation reads like it was written by someone explaining quantum mechanics to their dog. The examples are either &amp;quot;Hello World&amp;quot; or &amp;quot;Here's how to build AGI, figure out the middle part yourself.&amp;quot;&lt;/p&gt; &lt;p&gt;But I was different. I was going to be the hero LocalLlama needed.&lt;/p&gt; &lt;p&gt;&amp;quot;LangGraph is overcomplicated!&amp;quot; I declared. &amp;quot;State machines for agents? What is this, 1970? I'll build something better in a weekend!&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 1:&lt;/strong&gt; Drew a beautiful architecture diagram. Posted it on Twitter. 47 likes. &amp;quot;This is the way.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 3:&lt;/strong&gt; Okay, turns out managing agent state is... non-trivial. But I'm smart! I'll just use Python dicts!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 7:&lt;/strong&gt; My dict-based state management has evolved into... a graph. With nodes. And edges. Shit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 10:&lt;/strong&gt; Need tool calling. &amp;quot;MCP is the future!&amp;quot; Twitter says. Three days later: it works! (On my desktop. In dev mode. Only one user. When Mercury is in retrograde.)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 14:&lt;/strong&gt; Added checkpointing because production agents apparently need to not die when AWS hiccups. My &amp;quot;simple&amp;quot; solution is now 3,000 lines of spaghetti.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 21:&lt;/strong&gt; &amp;quot;Maybe I need human-in-the-loop features,&amp;quot; my PM says. I start drinking during standups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 30:&lt;/strong&gt; I've essentially recreated LangGraph, but worse. My state transitions look like they were designed by M.C. Escher having a bad trip. The only documentation is my increasingly unhinged commit messages.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 45:&lt;/strong&gt; I quietly pip install langgraph. Nobody needs to know.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 55:&lt;/strong&gt; &amp;quot;You need observability,&amp;quot; someone says. I glance at my custom logging system. It's 500 lines of print statements. I sign up for LangSmith. &amp;quot;Just the free tier,&amp;quot; I tell myself. Two hours later I'm on the Teams plan, staring at traces like a detective who just discovered fingerprints exist. &amp;quot;So THAT'S why my agent thinks it's a toaster every third request.&amp;quot; My credit card weeps.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Day 60:&lt;/strong&gt; Boss wants to demo tool calling. Palms sweat. &amp;quot;Define demo?&amp;quot; Someone mutters &lt;code&gt;pip install langchain-arcade&lt;/code&gt;. Ten minutes later, the agent is reading emails. I delete three days of MCP auth code and pride. I hate myself as I utter these words: &amp;quot;LangGraph isn't just a framework‚Äîit's an ecosystem of stuff that works.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Today:&lt;/strong&gt; I'm a LangGraph developer. I've memorized which 30% of the documentation actually matches the current version. I know exactly when to use StateGraph vs MessageGraph (hint: just use StateGraph and pray). I've accepted that &amp;quot;conditional_edge&amp;quot; is just how we live now.&lt;/p&gt; &lt;p&gt;The other day, a junior dev complained about LangGraph being &amp;quot;unnecessarily complex.&amp;quot; I laughed. Not a healthy laugh. The laugh of someone who's seen things. &amp;quot;Sure,&amp;quot; I said, &amp;quot;go build your own. I'll see you back here in 6 weeks.&amp;quot;&lt;/p&gt; &lt;p&gt;I've become the very thing I mocked. Yesterday, I actually said out loud: &amp;quot;Once you understand LangGraph's philosophy, it's quite elegant.&amp;quot; My coworkers staged an intervention.&lt;/p&gt; &lt;p&gt;But here's the thing - IT ACTUALLY WORKS. While everyone's writing blog posts about &amp;quot;Why Agent Frameworks Should Be Simple,&amp;quot; I'm shipping production systems with proper state management, checkpointing, and human oversight. My agents don't randomly hallucinate their entire state history anymore!&lt;/p&gt; &lt;p&gt;The final irony? I'm now building a LangGraph tutorial site... using a LangGraph agent to generate the content. It's graphs all the way down.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;class MyAgentJourney: def __init__(self): self.confidence = float('inf') self.langgraph_hatred = 100 def build_own_framework(self): self.confidence *= 0.5 self.langgraph_hatred -= 10 self.understanding_of_problem += 50 def eventually(self): return &amp;quot;pip install langgraph&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; - Yes, I've tried CrewAI, AutoGen, and that new framework your favorite AI influencer is shilling. No, they don't handle complex state management. Yes, I'm stuck with LangGraph. No, I'm not happy about it. Yes, I'll defend it viciously if you criticize it because Stockholm Syndrome is real.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; To everyone saying &amp;quot;skill issue&amp;quot; - yes, and?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 2:&lt;/strong&gt; The LangChain team DMed me asking if I want to help improve the docs. This is either an olive branch or a threat.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 3:&lt;/strong&gt; RIP my inbox. No, I won't review your &amp;quot;simple&amp;quot; agent framework. We both know where this ends.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 4:&lt;/strong&gt; This isn't fake. It's satire. :)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT 5:&lt;/strong&gt; Yes, I originally posted this to the Langchain subreddit but I figured you'd enjoy it too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FailingUpAllDay"&gt; /u/FailingUpAllDay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll321h/from_langgraph_is_trash_to_pip_install_langgraph/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll321h/from_langgraph_is_trash_to_pip_install_langgraph/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll321h/from_langgraph_is_trash_to_pip_install_langgraph/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:28:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1lleks2</id>
    <title>Gemini CLI - someone already made a pull request for Local LLM providers (and more)</title>
    <updated>2025-06-26T23:09:59+00:00</updated>
    <author>
      <name>/u/merrycachemiss</name>
      <uri>https://old.reddit.com/user/merrycachemiss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lleks2/gemini_cli_someone_already_made_a_pull_request/"&gt; &lt;img alt="Gemini CLI - someone already made a pull request for Local LLM providers (and more)" src="https://external-preview.redd.it/07Svddxhws9NRhwiaZE7X8N_M-orx7gvT8GOb4RjL2I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b658f44c11bd14eedf4c27ce46aafb04ba52e18" title="Gemini CLI - someone already made a pull request for Local LLM providers (and more)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's there, but the contributor still has to complete a CLA and nobody has openly talked about reviewing it. Would giving the PR a thumbs up help it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/merrycachemiss"&gt; /u/merrycachemiss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/google-gemini/gemini-cli/pull/1939"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lleks2/gemini_cli_someone_already_made_a_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lleks2/gemini_cli_someone_already_made_a_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T23:09:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1llb0et</id>
    <title>I‚Äôve been fine tuning a small llm 500m parameter on my MacBook !!!</title>
    <updated>2025-06-26T20:38:58+00:00</updated>
    <author>
      <name>/u/Ok-Math-5601</name>
      <uri>https://old.reddit.com/user/Ok-Math-5601</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb0et/ive_been_fine_tuning_a_small_llm_500m_parameter/"&gt; &lt;img alt="I‚Äôve been fine tuning a small llm 500m parameter on my MacBook !!!" src="https://preview.redd.it/tfvnaqas1c9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=795da79f752edd8faf7a034bc3cbd479e8315cc5" title="I‚Äôve been fine tuning a small llm 500m parameter on my MacBook !!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It‚Äôs for a STT &amp;amp; TTS engine that I‚Äôm trying to build, but can‚Äôt figure out how to get it running in multiple threads üòÆ‚Äçüí®&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Math-5601"&gt; /u/Ok-Math-5601 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tfvnaqas1c9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb0et/ive_been_fine_tuning_a_small_llm_500m_parameter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llb0et/ive_been_fine_tuning_a_small_llm_500m_parameter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:38:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll1yjh</id>
    <title>LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA üöÄ</title>
    <updated>2025-06-26T14:44:50+00:00</updated>
    <author>
      <name>/u/Additional_Top1210</name>
      <uri>https://old.reddit.com/user/Additional_Top1210</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt; &lt;img alt="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA üöÄ" src="https://external-preview.redd.it/GPs8oonK03Al4q6HtUFhFxh4J-39nPu_HZOBEQOCcn8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=faddbc4424a43d6c2043b2d74892e39170e98392" title="LLM Tuning Method 12,000x more efficient than full fine-tuning and 30% faster than LoRA üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper Link: &lt;a href="https://huggingface.co/papers/2506.16406"&gt;https://huggingface.co/papers/2506.16406&lt;/a&gt; Project Link: &lt;a href="https://jerryliang24.github.io/DnD/"&gt;https://jerryliang24.github.io/DnD/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Top1210"&gt; /u/Additional_Top1210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ll1yjh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll1yjh/llm_tuning_method_12000x_more_efficient_than_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T14:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll57uz</id>
    <title>Gemma 3n is on out on Hugging Face!</title>
    <updated>2025-06-26T16:52:30+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Google just dropped the perfect local model!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll57uz/gemma_3n_is_on_out_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:52:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll88pe</id>
    <title>Gemma 3n vs Gemma 3 (4B/12B) Benchmarks</title>
    <updated>2025-06-26T18:49:09+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I compiled all of the available official first-party benchmark results from google's model cards available here &lt;a href="https://ai.google.dev/gemma/docs/core/model_card_3#benchmark_results"&gt;https://ai.google.dev/gemma/docs/core/model_card_3#benchmark_results&lt;/a&gt; into a table to compare how the new 3N models do compared to their older non-n Gemma 3 siblings. Of course not all the same benchmark results were available for both models so I only added the results for tests they had done in common.&lt;/p&gt; &lt;h1&gt;Reasoning and Factuality&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;n-shot&lt;/th&gt; &lt;th align="left"&gt;E2B PT&lt;/th&gt; &lt;th align="left"&gt;E4B PT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1905.07830"&gt;HellaSwag&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;10-shot&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;td align="left"&gt;78.6&lt;/td&gt; &lt;td align="left"&gt;77.2&lt;/td&gt; &lt;td align="left"&gt;84.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1905.10044"&gt;BoolQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;76.4&lt;/td&gt; &lt;td align="left"&gt;81.6&lt;/td&gt; &lt;td align="left"&gt;72.3&lt;/td&gt; &lt;td align="left"&gt;78.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.11641"&gt;PIQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;78.9&lt;/td&gt; &lt;td align="left"&gt;81&lt;/td&gt; &lt;td align="left"&gt;79.6&lt;/td&gt; &lt;td align="left"&gt;81.8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1904.09728"&gt;SocialIQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;48.8&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;51.9&lt;/td&gt; &lt;td align="left"&gt;53.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1705.03551"&gt;TriviaQA&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;70.2&lt;/td&gt; &lt;td align="left"&gt;65.8&lt;/td&gt; &lt;td align="left"&gt;78.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/google-research-datasets/natural-questions"&gt;Natural Questions&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;15.5&lt;/td&gt; &lt;td align="left"&gt;20.9&lt;/td&gt; &lt;td align="left"&gt;20&lt;/td&gt; &lt;td align="left"&gt;31.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.01547"&gt;ARC-c&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;25-shot&lt;/td&gt; &lt;td align="left"&gt;51.7&lt;/td&gt; &lt;td align="left"&gt;61.6&lt;/td&gt; &lt;td align="left"&gt;56.2&lt;/td&gt; &lt;td align="left"&gt;68.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1911.01547"&gt;ARC-e&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;75.8&lt;/td&gt; &lt;td align="left"&gt;81.6&lt;/td&gt; &lt;td align="left"&gt;82.4&lt;/td&gt; &lt;td align="left"&gt;88.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1907.10641"&gt;WinoGrande&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;5-shot&lt;/td&gt; &lt;td align="left"&gt;66.8&lt;/td&gt; &lt;td align="left"&gt;71.7&lt;/td&gt; &lt;td align="left"&gt;64.7&lt;/td&gt; &lt;td align="left"&gt;74.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://paperswithcode.com/dataset/bbh"&gt;BIG-Bench Hard&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;few-shot&lt;/td&gt; &lt;td align="left"&gt;44.3&lt;/td&gt; &lt;td align="left"&gt;52.9&lt;/td&gt; &lt;td align="left"&gt;50.9&lt;/td&gt; &lt;td align="left"&gt;72.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/1903.00161"&gt;DROP&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Token F1 score&lt;/td&gt; &lt;td align="left"&gt;1-shot&lt;/td&gt; &lt;td align="left"&gt;53.9&lt;/td&gt; &lt;td align="left"&gt;60.8&lt;/td&gt; &lt;td align="left"&gt;60.1&lt;/td&gt; &lt;td align="left"&gt;72.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMEAN&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;54.46&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;61.08&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;58.57&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68.99&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Additional/Other Benchmarks&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Benchmark&lt;/th&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;n-shot&lt;/th&gt; &lt;th align="left"&gt;E2B IT&lt;/th&gt; &lt;th align="left"&gt;E4B IT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2210.03057"&gt;MGSM&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;53.1&lt;/td&gt; &lt;td align="left"&gt;60.7&lt;/td&gt; &lt;td align="left"&gt;34.7&lt;/td&gt; &lt;td align="left"&gt;64.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.12404v1"&gt;WMT24++ (ChrF)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Character-level F-score&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;42.7&lt;/td&gt; &lt;td align="left"&gt;50.1&lt;/td&gt; &lt;td align="left"&gt;48.4&lt;/td&gt; &lt;td align="left"&gt;53.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2502.21228"&gt;ECLeKTic&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;ECLeKTic score&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;2.5&lt;/td&gt; &lt;td align="left"&gt;1.9&lt;/td&gt; &lt;td align="left"&gt;4.6&lt;/td&gt; &lt;td align="left"&gt;10.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2311.12022"&gt;GPQA Diamond&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;RelaxedAccuracy/accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;24.8&lt;/td&gt; &lt;td align="left"&gt;23.7&lt;/td&gt; &lt;td align="left"&gt;30.8&lt;/td&gt; &lt;td align="left"&gt;40.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2108.07732"&gt;MBPP&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;3-shot&lt;/td&gt; &lt;td align="left"&gt;56.6&lt;/td&gt; &lt;td align="left"&gt;63.6&lt;/td&gt; &lt;td align="left"&gt;63.2&lt;/td&gt; &lt;td align="left"&gt;73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2107.03374"&gt;HumanEval&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;66.5&lt;/td&gt; &lt;td align="left"&gt;75&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;td align="left"&gt;85.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2403.07974"&gt;LiveCodeBench&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;pass@1&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;13.2&lt;/td&gt; &lt;td align="left"&gt;13.2&lt;/td&gt; &lt;td align="left"&gt;12.6&lt;/td&gt; &lt;td align="left"&gt;24.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;HiddenMath&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;td align="left"&gt;37.7&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/datasets/CohereForAI/Global-MMLU-Lite"&gt;Global-MMLU-Lite&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;59&lt;/td&gt; &lt;td align="left"&gt;64.5&lt;/td&gt; &lt;td align="left"&gt;54.5&lt;/td&gt; &lt;td align="left"&gt;69.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://arxiv.org/abs/2009.03300"&gt;MMLU (Pro)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Accuracy&lt;/td&gt; &lt;td align="left"&gt;0-shot&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;50.6&lt;/td&gt; &lt;td align="left"&gt;43.6&lt;/td&gt; &lt;td align="left"&gt;60.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMEAN&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;29.27&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;31.81&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32.66&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;46.8&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Overall Geometric-Mean&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt; &lt;/th&gt; &lt;th align="left"&gt;E2B IT&lt;/th&gt; &lt;th align="left"&gt;E4B IT&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 4B&lt;/th&gt; &lt;th align="left"&gt;Gemma 3 IT 12B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;GEOMAN-ALL&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt; &lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;40.53&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;44.77&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;44.35&lt;/em&gt;&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;&lt;em&gt;57.40&lt;/em&gt;&lt;/strong&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Link to google sheets document: &lt;a href="https://docs.google.com/spreadsheets/d/1U3HvtMqbiuO6kVM96d0aE9W40F8b870He0cg6hLPSdA/edit?usp=sharing"&gt;https://docs.google.com/spreadsheets/d/1U3HvtMqbiuO6kVM96d0aE9W40F8b870He0cg6hLPSdA/edit?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll88pe/gemma_3n_vs_gemma_3_4b12b_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T18:49:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkzynl</id>
    <title>The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)</title>
    <updated>2025-06-26T13:19:50+00:00</updated>
    <author>
      <name>/u/aospan</name>
      <uri>https://old.reddit.com/user/aospan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt; &lt;img alt="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" src="https://external-preview.redd.it/1wJhDztWCANroswcLW3p5i3oMCiTskJ82JKTdTfiCRM.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d6977975d5861c60901c746f5374dd709bf8cb89" title="The Real Performance Penalty of GPU Passthrough into a VM (It's... boring)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running GPUs in virtual machines for AI workloads is quickly becoming the golden standard - especially for isolation, orchestration, and multi-tenant setups. So I decided to measure the actual performance penalty of this approach.&lt;/p&gt; &lt;p&gt;I benchmarked some LLMs (via ollama-benchmark) on an AMD RX 9060 XT 16GB - first on bare metal Ubuntu 24.04, then in a VM (Ubuntu 24.04) running under AI Linux (Sbnb Linux) with GPU passthrough via &lt;code&gt;vfio-pci&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Models tested:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistral:7b&lt;/li&gt; &lt;li&gt;gemma2:9b&lt;/li&gt; &lt;li&gt;phi4:14b&lt;/li&gt; &lt;li&gt;deepseek-r1:14b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;VM performance was just &lt;strong&gt;1‚Äì2% slower&lt;/strong&gt; than bare metal. That‚Äôs it. Practically a rounding error.&lt;/p&gt; &lt;p&gt;So‚Ä¶ yeah. Turns out GPU passthrough isn‚Äôt the scary performance killer.&lt;/p&gt; &lt;p&gt;üëâ I put together the full setup, AMD ROCm install steps, benchmark commands, results, and even a diagram - all in this README: &lt;a href="https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md"&gt;https://github.com/sbnb-io/sbnb/blob/main/README-GPU-PASSTHROUGH-BENCHMARK.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or help if you‚Äôre setting up something similar!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aospan"&gt; /u/aospan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1lkzynl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkzynl/the_real_performance_penalty_of_gpu_passthrough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T13:19:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1lla27f</id>
    <title>Google DeepMind Releases AlphaGenome</title>
    <updated>2025-06-26T20:01:14+00:00</updated>
    <author>
      <name>/u/aithrowaway22</name>
      <uri>https://old.reddit.com/user/aithrowaway22</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lla27f/google_deepmind_releases_alphagenome/"&gt; &lt;img alt="Google DeepMind Releases AlphaGenome" src="https://external-preview.redd.it/43SAwvb1n5vlp2Qq_6_pefepMSOiGDZDO8afisrPhzg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c27c96bfdfb4e956d2433cecac8f9d56364d7d0a" title="Google DeepMind Releases AlphaGenome" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aithrowaway22"&gt; /u/aithrowaway22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://deepmind.google/discover/blog/alphagenome-ai-for-better-understanding-the-genome/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lla27f/google_deepmind_releases_alphagenome/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lla27f/google_deepmind_releases_alphagenome/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1llaxaz</id>
    <title>What is this checkmark next to our subreddit name?</title>
    <updated>2025-06-26T20:35:31+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llaxaz/what_is_this_checkmark_next_to_our_subreddit_name/"&gt; &lt;img alt="What is this checkmark next to our subreddit name?" src="https://preview.redd.it/u8j9adw41c9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c87c2fd4e8ad334c049b29b7e771e4cfffe23e56" title="What is this checkmark next to our subreddit name?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u8j9adw41c9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llaxaz/what_is_this_checkmark_next_to_our_subreddit_name/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llaxaz/what_is_this_checkmark_next_to_our_subreddit_name/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:35:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1lkz0hg</id>
    <title>Meta wins AI copyright lawsuit as US judge rules against authors | Meta</title>
    <updated>2025-06-26T12:35:26+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt; &lt;img alt="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" src="https://external-preview.redd.it/P24oFDRu9fwfx1j87kht5i8PPJV3CyEIC0aLVuyN_0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5020fe75b422d099598cd47f46c61ccb4e8bea63" title="Meta wins AI copyright lawsuit as US judge rules against authors | Meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theguardian.com/technology/2025/jun/26/meta-wins-ai-copyright-lawsuit-as-us-judge-rules-against-authors"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1lkz0hg/meta_wins_ai_copyright_lawsuit_as_us_judge_rules/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T12:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1llb5e9</id>
    <title>Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel.</title>
    <updated>2025-06-26T20:44:28+00:00</updated>
    <author>
      <name>/u/SilverRegion9394</name>
      <uri>https://old.reddit.com/user/SilverRegion9394</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"&gt; &lt;img alt="Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel." src="https://preview.redd.it/x6kkfnuo2c9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d99eb39eccf80408c1a602f9dfe2d9fb44ce50a" title="Crazy how this subreddit started out focused on Meta's LLaMA and ended up becoming a full-blown AI channel." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SilverRegion9394"&gt; /u/SilverRegion9394 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x6kkfnuo2c9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llb5e9/crazy_how_this_subreddit_started_out_focused_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T20:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll68iz</id>
    <title>Gemma 3n Full Launch - Developers Edition</title>
    <updated>2025-06-26T17:31:27+00:00</updated>
    <author>
      <name>/u/hackerllama</name>
      <uri>https://old.reddit.com/user/hackerllama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! Today we have the full launch of Gemma 3n, meaning we have support for your favorite tools as well as full support for its capabilities &lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recap&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Audio, video, image, and text input; text output&lt;/li&gt; &lt;li&gt;E2B and E4B - while their raw parameter count is 5B and 8B, you can operate them with as little as 2B and 4B effective params&lt;/li&gt; &lt;li&gt;MatFormer: The model architecture allows extracting submodels and doing mix-n-match, allowing you to export additional models in your favorite size between 2B and 4B.&lt;/li&gt; &lt;li&gt;MobileNetV5 and a new audio encoder&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And now...for supported tools. We collaborated with many many open source developers to enable its capabilities. So you can now use Gemma in Hugging Face, Kaggle, llama.cpp, Ollama, MLX, LMStudio, transformers.js, Docker model hub, Unsloth, transformers trl and PEFT, VLLM, SGLang, Jetson AI Lab, and many others. Enjoy! We'll also host a Kaggle competition if anyone wants to join &lt;a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon"&gt;https://www.kaggle.com/competitions/google-gemma-3n-hackathon&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hugging Face &lt;a href="https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4"&gt;https://huggingface.co/collections/google/gemma-3n-685065323f5984ef315c93f4&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Unsloth &lt;a href="https://unsloth.ai/blog/gemma-3n"&gt;https://unsloth.ai/blog/gemma-3n&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HF blog &lt;a href="https://huggingface.co/blog/gemma3n"&gt;https://huggingface.co/blog/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;LMStudio &lt;a href="https://lmstudio.ai/models/google/gemma-3n-e4b"&gt;https://lmstudio.ai/models/google/gemma-3n-e4b&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Ollama &lt;a href="https://ollama.com/library/gemma3n"&gt;https://ollama.com/library/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;AI Studio &lt;a href="http://ai.dev"&gt;ai.dev&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Kaggle &lt;a href="https://www.kaggle.com/models/google/gemma-3n"&gt;https://www.kaggle.com/models/google/gemma-3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;MLX &lt;a href="https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc"&gt;https://huggingface.co/collections/mlx-community/gemma-3n-685d6c8d02d7486c7e77a7dc&lt;/a&gt; &lt;/li&gt; &lt;li&gt;ONNX/transformers.js &lt;a href="https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX"&gt;https://huggingface.co/onnx-community/gemma-3n-E2B-it-ONNX&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Vertex &lt;a href="https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n"&gt;https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/gemma3n&lt;/a&gt; &lt;/li&gt; &lt;li&gt;GGUF &lt;a href="https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7"&gt;https://huggingface.co/collections/ggml-org/gemma-3n-685d6fc0843071be9e77b6f7&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackerllama"&gt; /u/hackerllama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll68iz/gemma_3n_full_launch_developers_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll38zu</id>
    <title>FLUX.1 Kontext [dev] - an open weights model for proprietary-level image editing performance.</title>
    <updated>2025-06-26T15:35:49+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;weights: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev"&gt;https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;release news: &lt;a href="https://x.com/bfl_ml/status/1938257909726519640"&gt;https://x.com/bfl_ml/status/1938257909726519640&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll38zu/flux1_kontext_dev_an_open_weights_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T15:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll429p</id>
    <title>gemma 3n has been released on huggingface</title>
    <updated>2025-06-26T16:07:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B"&gt;https://huggingface.co/google/gemma-3n-E2B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E2B-it"&gt;https://huggingface.co/google/gemma-3n-E2B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B"&gt;https://huggingface.co/google/gemma-3n-E4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/google/gemma-3n-E4B-it"&gt;https://huggingface.co/google/gemma-3n-E4B-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(You can find benchmark results such as HellaSwag, MMLU, or LiveCodeBench above)&lt;/p&gt; &lt;p&gt;llama.cpp implementation by &lt;a href="https://github.com/ngxson"&gt;&lt;strong&gt;ngxson&lt;/strong&gt;&lt;/a&gt;:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14400"&gt;https://github.com/ggml-org/llama.cpp/pull/14400&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUFs:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E2B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF"&gt;https://huggingface.co/ggml-org/gemma-3n-E4B-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Technical announcement:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/"&gt;https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll429p/gemma_3n_has_been_released_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T16:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ll6jo5</id>
    <title>DeepSeek R2 delayed</title>
    <updated>2025-06-26T17:43:13+00:00</updated>
    <author>
      <name>/u/FeathersOfTheArrow</name>
      <uri>https://old.reddit.com/user/FeathersOfTheArrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt; &lt;img alt="DeepSeek R2 delayed" src="https://preview.redd.it/718m48of6b9f1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b5423692617bfdf316daec6232ca857bc69416c" title="DeepSeek R2 delayed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Over the past several months, DeepSeek's engineers have been working to refine R2 until Liang gives the green light for release, according to The Information. However, a fast adoption of R2 could be difficult due to a shortage of Nvidia server chips in China as a result of U.S. export regulations, the report said, citing employees of top Chinese cloud firms that offer DeepSeek's models to enterprise customers.&lt;/p&gt; &lt;p&gt;A potential surge in demand for R2 would overwhelm Chinese cloud providers, who need advanced Nvidia chips to run AI models, the report said.&lt;/p&gt; &lt;p&gt;DeepSeek did not immediately respond to a Reuters request for comment.&lt;/p&gt; &lt;p&gt;DeepSeek has been in touch with some Chinese cloud companies, providing them with technical specifications to guide their plans for hosting and distributing the model from their servers, the report said.&lt;/p&gt; &lt;p&gt;Among its cloud customers currently using R1, the majority are running the model with Nvidia's H20 chips, The Information said.&lt;/p&gt; &lt;p&gt;Fresh export curbs imposed by the Trump administration in April have prevented Nvidia from selling in the Chinese market its H20 chips - the only AI processors it could legally export to the country at the time.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Sources : &lt;a href="https://www.theinformation.com/articles/deepseeks-progress-stalled-u-s-export-controls"&gt;[1]&lt;/a&gt; &lt;a href="https://x.com/kimmonismus/status/1938221881175183740"&gt;[2]&lt;/a&gt; &lt;a href="https://www.reuters.com/world/china/deepseek-r2-launch-stalled-ceo-balks-progress-information-reports-2025-06-26/"&gt;[3]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeathersOfTheArrow"&gt; /u/FeathersOfTheArrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/718m48of6b9f1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ll6jo5/deepseek_r2_delayed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-26T17:43:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1llfufv</id>
    <title>Dear Mod, we don't want our posts on X/Twitter.</title>
    <updated>2025-06-27T00:07:54+00:00</updated>
    <author>
      <name>/u/Pro-editor-1105</name>
      <uri>https://old.reddit.com/user/Pro-editor-1105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llfufv/dear_mod_we_dont_want_our_posts_on_xtwitter/"&gt; &lt;img alt="Dear Mod, we don't want our posts on X/Twitter." src="https://preview.redd.it/ber4b39v2d9f1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b33ffbed20588badb2becadf417b4cbebfe32d6f" title="Dear Mod, we don't want our posts on X/Twitter." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Especially with no credit in the title, but rather just put in a comment just deep in there. This is user generated content, and not the property of the mods to just regurgitate whereever they wants. No harm meant, and also it seems like the majority of the community agrees with this consensus, based on downvotes of comments which mentioned this. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pro-editor-1105"&gt; /u/Pro-editor-1105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ber4b39v2d9f1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1llfufv/dear_mod_we_dont_want_our_posts_on_xtwitter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1llfufv/dear_mod_we_dont_want_our_posts_on_xtwitter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-06-27T00:07:54+00:00</published>
  </entry>
</feed>
