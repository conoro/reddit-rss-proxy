<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-24T23:06:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qlejvk</id>
    <title>Is anyone else worried about the enshitifciation cycle of AI platforms? What is your plan (personal and corporate)</title>
    <updated>2026-01-24T05:33:33+00:00</updated>
    <author>
      <name>/u/Ngambardella</name>
      <uri>https://old.reddit.com/user/Ngambardella</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm starting to see the oh to familiar pattern of the enshitifcation cycle starting to rear its head in the AI space.&lt;/p&gt; &lt;p&gt;For those unfamiliar, enshitification is a term that defines the ‚Äúdeliberate, gradual degradation of quality in digital platforms‚Äù. Something that we have all seen time and time again.&lt;/p&gt; &lt;p&gt;The cycle is as follows:&lt;/p&gt; &lt;p&gt;Stage 1: Good for users&lt;/p&gt; &lt;p&gt;Stage 2: Good for business customers (defined as extracting money from platform at the users expense, whether through ads, features that make the platform&lt;/p&gt; &lt;p&gt;More unusable, etc.)&lt;/p&gt; &lt;p&gt;Stage 3: Good for shareholders (the final push to squeeze every drop of remaining value out of the product, by making user experience significantly worse, as well as screwing business customers by increasing rates, worse bank for your buck, etc.)&lt;/p&gt; &lt;p&gt;I believe we are starting to enter stage 2. Although I haven‚Äôt seen any (clearly stated) ads, I have seen a lot more discussion about integrated ads in AI chats. I‚Äôve also noticed significantly reduced performance with higher usage, clearly stated rate limiting (even on paid apps), etc.&lt;/p&gt; &lt;p&gt;Right now it would be a death sentence for any company to fully enshitify, but once the competition slows down and companies start to drop out of the race, or if one company jumps significantly above the rest, we will start to really see stage 2 come to fruition.&lt;/p&gt; &lt;p&gt;In a personal setting this bothers me because I work on a lot of highly technical/niche applications and I really need accurate and consistent answers that are consistent over a larger context window, and having to start a new chat/switch apps is honestly a nightmare. To the point where I am looking to refine my workflow to allow me to switch more efficiently mid conversation.&lt;/p&gt; &lt;p&gt;In a corporate setting this is definitely going to be an issue for those not running self hosted models, it is such an easy game plan for the LLM companies to extract revenue. Get all these companies setup on your AI integrated into their internal applications, push the compliance argument, start to deprecate models/increase cost, ???, profit.&lt;/p&gt; &lt;p&gt;Thankfully most corporate applications don‚Äôt require state of the art models. But still, I think everyone should be monitoring value metrics and have contingencies in place for in both settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ngambardella"&gt; /u/Ngambardella &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T05:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwxcu</id>
    <title>Built a Tavily alternative flow that doesn't hide what it's doing - direct web access for your local LLM pipelines</title>
    <updated>2026-01-24T19:51:03+00:00</updated>
    <author>
      <name>/u/kami4ka</name>
      <uri>https://old.reddit.com/user/kami4ka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwxcu/built_a_tavily_alternative_flow_that_doesnt_hide/"&gt; &lt;img alt="Built a Tavily alternative flow that doesn't hide what it's doing - direct web access for your local LLM pipelines" src="https://external-preview.redd.it/a2Ag5fJk1SlrdoXN0ka7e78aP8oCJnhPt_5qgxUnz2g.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f00319796396eea94ca1bd7e4ead66c229df813" title="Built a Tavily alternative flow that doesn't hide what it's doing - direct web access for your local LLM pipelines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of black-box search APIs deciding what's &amp;quot;relevant&amp;quot; for my RAG setup. Built an MCP server that lets your LLM:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Search Bing/DuckDuckGo/Any SERP choice directly via SERP scraping&lt;/li&gt; &lt;li&gt;Pick which URLs to fetch (no vendor reranking)&lt;/li&gt; &lt;li&gt;Get content as HTML, Markdown, or plain text&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;10K free API credits/month to play with&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kami4ka"&gt; /u/kami4ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://scrapingant.com/tavily-alternative-web-access-for-ai-agents"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwxcu/built_a_tavily_alternative_flow_that_doesnt_hide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwxcu/built_a_tavily_alternative_flow_that_doesnt_hide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlxbk0</id>
    <title>help choosing an UI</title>
    <updated>2026-01-24T20:05:50+00:00</updated>
    <author>
      <name>/u/BXresearch</name>
      <uri>https://old.reddit.com/user/BXresearch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi everyone.&lt;/p&gt; &lt;p&gt;I'm having to choose an ui for my chatbot and I see there are some different options, so I would like to ask some questions... &lt;/p&gt; &lt;p&gt;reading online, it seems that main options are LibreChat, AnythingLM and OpenWebUI... (obviously other solution are ok) &lt;/p&gt; &lt;p&gt;I've worked on custom rags, web search and tools but I was stuck on a junky gradio UI (ui is a compliment) I initially made just for testing, due to pure laziness I admit.&lt;/p&gt; &lt;p&gt;I have quite a lot of experience regarding NN architecture and design research, but I have no experience on anything even remotely ui related.&lt;/p&gt; &lt;p&gt;what I need is &amp;quot;just&amp;quot; an ui that allow me to to use custom RAG and related databases, and that allow me to easily see or inspect the actual context received from the model, let it be as a graphic slide or anything similar.&lt;/p&gt; &lt;p&gt;it would be used mainly with hosted APIs, running locally various finetuned ST models for RAG. &lt;/p&gt; &lt;p&gt;Also it would be helpful if it would accept custom python code for the chat behavior, context management, web search, rag etch &lt;/p&gt; &lt;p&gt;I'm sorry if the question may sound dumb... thanks in advance for any kind of reply. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BXresearch"&gt; /u/BXresearch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlxbk0/help_choosing_an_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlxbk0/help_choosing_an_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlxbk0/help_choosing_an_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T20:05:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlkp6x</id>
    <title>Built a library of LLM prompts for RAG</title>
    <updated>2026-01-24T11:30:26+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt; &lt;img alt="Built a library of LLM prompts for RAG" src="https://a.thumbs.redditmedia.com/5xg3Rwo4RIh0LAjeGlCsGR-iU-JyYfNfHJtQ638GqV0.jpg" title="Built a library of LLM prompts for RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gathered a set of RAG prompt templates focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;grounding constraints&lt;/li&gt; &lt;li&gt;citation rules&lt;/li&gt; &lt;li&gt;multi-source + uncertainty handling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Templates are copy-pasteable. If you try one, &lt;strong&gt;upvote/downvote&lt;/strong&gt; it so the best ones float up over time.&lt;/p&gt; &lt;p&gt;And if you have a prompt that consistently works, contribute it - I‚Äôd love to include it.&lt;/p&gt; &lt;p&gt;If useful, the library is here: &lt;a href="https://agentset.ai/rag-prompts"&gt;https://agentset.ai/rag-prompts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vwuxs2jn8afg1.png?width=2660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bee373363c01d0cda6b915cc8fd8902760f8fd7c"&gt;https://preview.redd.it/vwuxs2jn8afg1.png?width=2660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bee373363c01d0cda6b915cc8fd8902760f8fd7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T11:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlfu2b</id>
    <title>engine for GLM 4.7 Flash that doesn't massively slow down as the context grows?</title>
    <updated>2026-01-24T06:42:56+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man, i just tried GLM 4.7 Flash in LMstudio on a 5090 and while the 150 tokens/sec at Q6 is nice on the first prompt, things rapidly go south speedwise after 10k, unlike any other model i've tried.&lt;/p&gt; &lt;p&gt;I am using all the recommended settings and my unsloth quant, llama.cpp runtime, and lmstudio are all up to date.&lt;/p&gt; &lt;p&gt;I see that ik_llama.cpp has a recent patch that reduces this slowdown:&lt;br /&gt; &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/1182"&gt;https://github.com/ikawrakow/ik_llama.cpp/pull/1182&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But i can't figure out how to compile it.&lt;/p&gt; &lt;p&gt;I was wondering if the implementation in vllm or some other engine doesn't suffer of this.&lt;/p&gt; &lt;p&gt;This seems like an otherwise pretty good model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T06:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlyip2</id>
    <title>Linting LLM prompts - catching contradictions before they hit production</title>
    <updated>2026-01-24T20:51:21+00:00</updated>
    <author>
      <name>/u/ObjectiveRealistic98</name>
      <uri>https://old.reddit.com/user/ObjectiveRealistic98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;System prompts are code but we don't treat them like it. They live in string literals, grow organically, and break in ways you only discover at runtime.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I built this&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I was debugging an agent that kept ignoring instructions. Took me 2 hours to find the problem: two fragments written months apart that contradicted each other. One said &amp;quot;always explain your reasoning&amp;quot;, the other said &amp;quot;be brief, no explanations needed.&amp;quot; The prompt was 1800 tokens across 6 files - impossible to spot by eye. Figured if we lint code, we should lint prompts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it catches&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ promptier lint ./agent.ts ‚ö† conflicting-patterns &amp;quot;Always provide detailed explanations&amp;quot; conflicts with &amp;quot;Never write more than 2 sentences&amp;quot; ‚ö† dynamic-before-static Dynamic content before static reduces cache efficiency ‚ö† missing-identity No identity section &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Current rules are heuristic: pattern matching for &amp;quot;always X&amp;quot; vs &amp;quot;never X&amp;quot;, section ordering, token budgets.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Roadmap: Semantic Linting with Local LLMs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pattern matching misses nuance. Next step is local model inference via Ollama:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;be concise&amp;quot; + &amp;quot;provide comprehensive details&amp;quot; = tension (no keyword overlap)&lt;/li&gt; &lt;li&gt;Ambiguous instructions that could be interpreted multiple ways&lt;/li&gt; &lt;li&gt;Phrasings known to cause hallucination&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Training data from Anthropic/OpenAI prompt guides + community before/after examples. Local-first, prompts stay on your machine.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What anti-patterns would you want caught?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/DeanShandler123/promptier"&gt;github.com/DeanShandler123/promptier&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObjectiveRealistic98"&gt; /u/ObjectiveRealistic98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlyip2/linting_llm_prompts_catching_contradictions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlyip2/linting_llm_prompts_catching_contradictions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlyip2/linting_llm_prompts_catching_contradictions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T20:51:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlie1t</id>
    <title>Running MoE Models on CPU/RAM: A Guide to Optimizing Bandwidth for GLM-4 and GPT-OSS</title>
    <updated>2026-01-24T09:12:49+00:00</updated>
    <author>
      <name>/u/Shoddy_Bed3240</name>
      <uri>https://old.reddit.com/user/Shoddy_Bed3240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The core principle of running Mixture-of-Experts (MoE) models on CPU/RAM is that the CPU doesn't need to extract or calculate all weights from memory simultaneously. Only a fraction of the parameters are &amp;quot;active&amp;quot; for any given token, and since calculations are approximate, memory throughput becomes our primary bottleneck.&lt;/p&gt; &lt;h1&gt;The Math: Model Size vs. Memory Bandwidth&lt;/h1&gt; &lt;p&gt;Let's look at two popular models: &lt;strong&gt;GLM-4.7-Flash&lt;/strong&gt; (3B active params) and &lt;strong&gt;GPT OSS 120B&lt;/strong&gt; (5.1B active params). At Q4_K_M quantization, their active memory footprints are:&lt;/p&gt; &lt;p&gt;Now, let's look at theoretical vs. realistic &lt;strong&gt;DDR5 Dual-Channel Bandwidth&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality Check:&lt;/strong&gt; We rarely hit theoretical peaks when reading small, scattered chunks of data. A realistic &amp;quot;sustained&amp;quot; bandwidth for LLM inference is closer to &lt;strong&gt;35 GB/s&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Doing the math for DDR5-6000:&lt;/p&gt; &lt;p&gt;If you can fully stress your memory bus, these are the speeds you can expect.&lt;/p&gt; &lt;h1&gt;Hardware Optimization (Intel 14700f Example)&lt;/h1&gt; &lt;p&gt;To hit these numbers, your CPU and BIOS settings must be dialed in:&lt;/p&gt; &lt;h1&gt;Software Stack &amp;amp; Compilation&lt;/h1&gt; &lt;p&gt;I‚Äôm running on Linux with the latest drivers (Nvidia 590.48 / CUDA 13.1) and GCC 15.2. For maximum performance, you &lt;strong&gt;must&lt;/strong&gt; compile llama.cpp from source with flags optimized for your specific architecture (Raptor Lake in this case).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Build Command:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;p&gt;cmake .. -DGGML_CUDA=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_GRAPHS=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_USE_CUBLASLT=ON \&lt;/p&gt; &lt;p&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120a;86&amp;quot; \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_TENSOR_CORES=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_FP16=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_INT8=ON \&lt;/p&gt; &lt;p&gt;-DGGML_AVX512=OFF \&lt;/p&gt; &lt;p&gt;-DGGML_AVX2=ON \&lt;/p&gt; &lt;p&gt;-DGGML_FMA=ON \&lt;/p&gt; &lt;p&gt;-DGGML_F16C=ON \&lt;/p&gt; &lt;p&gt;-DCMAKE_C_COMPILER=gcc-15 \&lt;/p&gt; &lt;p&gt;-DCMAKE_CXX_COMPILER=g++-15 \&lt;/p&gt; &lt;p&gt;-DCMAKE_C_FLAGS=&amp;quot;-march=raptorlake -mtune=native -O3 -flto=auto&amp;quot; \&lt;/p&gt; &lt;p&gt;-DCMAKE_CXX_FLAGS=&amp;quot;-march=raptorlake -mtune=native -O3 -flto=auto&amp;quot; \&lt;/p&gt; &lt;p&gt;-DGGML_OPENMP=ON \&lt;/p&gt; &lt;p&gt;-DGGML_OPENMP_DYNAMIC=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_ENABLE_UNIFIED_MEMORY=OFF \&lt;/p&gt; &lt;p&gt;-DGGML_LTO=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_BLACKWELL_NATIVE_FP4=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_USE_CUDNN=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_MAX_CONTEXT=32768 \&lt;/p&gt; &lt;p&gt;-DBUILD_SHARED_LIBS=OFF \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_MAX_STREAMS=8 \&lt;/p&gt; &lt;p&gt;-DCMAKE_BUILD_TYPE=Release&lt;/p&gt; &lt;h1&gt;Running the Server&lt;/h1&gt; &lt;p&gt;The key is to pin the process to your &lt;strong&gt;Performance Cores (P-cores)&lt;/strong&gt; and avoid the Efficiency Cores (E-cores), which can slow down the memory-heavy threads.&lt;/p&gt; &lt;p&gt;For the 14700f, I use taskset to bind to the first 16 logical threads (P-cores):&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;p&gt;taskset -c 0-15 llama-server \&lt;/p&gt; &lt;p&gt;-m /data/gguf/GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf \&lt;/p&gt; &lt;p&gt;--ctx-size 64000 \&lt;/p&gt; &lt;p&gt;--jinja \&lt;/p&gt; &lt;p&gt;-fa 1 \&lt;/p&gt; &lt;p&gt;--no-warmup \&lt;/p&gt; &lt;p&gt;--threads 16 \&lt;/p&gt; &lt;p&gt;--numa distribute \&lt;/p&gt; &lt;p&gt;--threads-batch 16 \&lt;/p&gt; &lt;p&gt;--host 0.0.0.0 \&lt;/p&gt; &lt;p&gt;--port 8080 \&lt;/p&gt; &lt;p&gt;--temp 1.0 \&lt;/p&gt; &lt;p&gt;--top-p 0.95 \&lt;/p&gt; &lt;p&gt;--min-p 0.01 \&lt;/p&gt; &lt;p&gt;--repeat-penalty 1.0&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pro Tip:&lt;/strong&gt; Don't disable your GPU! Even if the model doesn't fit entirely on the VRAM, llama.cpp can offload specific layers to the GPU, providing a nice speed boost to the overall generation.&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;Thanks for the comments. About the build flags: these are the flags I actually use in my working setup. Not everything here is about raw CPU optimization ‚Äî a good portion is tuned for my specific builds (Blackwell and Ampere). Feel free to use or ignore any flags depending on your own setup.&lt;/p&gt; &lt;h1&gt;Performance Tests (llama-bench, CPU-only / NO GPU)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;System notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Threads: 16&lt;/li&gt; &lt;li&gt;Backend listed as CUDA by the runner, but &lt;strong&gt;NO GPU used&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Metrics: tokens/sec (t/s)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîπ GLM-4.7-Flash Q4_K_M (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;101.65 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;84.25 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.41 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;22.93 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ GLM-4.7-Flash Q8_0 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;99.59 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;82.94 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.13 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;14.93 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ GLM-4.7-Flash BF16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;62.00 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;55.15 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;10.59 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;10.50 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ gpt-oss-120B F16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;56.25 ¬± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;54.31 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.18 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;15.03 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Devstral-Small-2-24B-Instruct-2512 BF16 (NO GPU) - not MoE&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;18.99 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;18.69 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;1.95 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;1.94 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Qwen3-coder-30B-a3b BF16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;69.48 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;64.75 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;12.43 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;12.34 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Qwen3-coder-30B-a3b Q8 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;124.67 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;110.32 ¬± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.67 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q8_0&lt;/td&gt; &lt;td align="left"&gt;30.25 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;20.41 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Qwen3-coder-30B-a3b Q4 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;133.94 ¬± 0.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;116.23 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;28.35 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;27.87 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Qwen3 Next Q4 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;45.15 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;82.69 ¬± 0.13&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;45.15 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;78.64 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;45.15 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;10.99 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;45.15 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;10.97 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ gpt-oss 20B F16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td align="left"&gt;12.83 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;86.12 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td align="left"&gt;12.83 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;82.98 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td align="left"&gt;12.83 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.99 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss 20B F16&lt;/td&gt; &lt;td align="left"&gt;12.83 GiB&lt;/td&gt; &lt;td align="left"&gt;20.91 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;20.77 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üöÄ GPU Reference (for scale)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GLM-4.7-Flash Q4_K_M on GPU (5090)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;4638.85 ¬± 13.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;5927.16 ¬± 21.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;150.21 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;143.16 ¬± 0.39&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy_Bed3240"&gt; /u/Shoddy_Bed3240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T09:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm18qx</id>
    <title>Best use case for Ryzen 395+ (128gb variant)</title>
    <updated>2026-01-24T22:37:12+00:00</updated>
    <author>
      <name>/u/ironicstatistic</name>
      <uri>https://old.reddit.com/user/ironicstatistic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm aware that this question gets asked continually here. but everyone's use case is a little bit different and times are always changing... I figure it's okay to ask.&lt;/p&gt; &lt;p&gt;As an EE student with limited coding capabilities and a lot of tech related interests, I tend to use AI for:&lt;/p&gt; &lt;p&gt;- Personal question answer stuff (web searches, advice on certain things)&lt;br /&gt; - Coding help (I am not a CS student, my coding skills are limited but I have worked with AI to build some cool python projects a number of times.)&lt;br /&gt; - College help (posting screenshots of math problems, other physics and EE questions, etc.&lt;/p&gt; &lt;p&gt;I've also messed around on the hardware that I had access to - mixing an llm with text-to-speech models and with whisper to try to get a sort of personal AI Assistant for use on the desktop. I realized that if I wanted to get further with that and just for other use cases in my field of study I might need more Vram. I didn't want to break the bank, and I wanted a small computer that I could also do some light gaming on. In order to get into AI with more than 24gb (running vision/speech to text on the same system), It seemed my options were this or a full sized rig, which wasn't what I wanted - This seemed perfect.&lt;/p&gt; &lt;p&gt;That being said I am the poor. If I'm going to justify this purchase, I'm going to have to find use cases with AI that really make sense and models that make sense to run with this device for my purposes - otherwise any ancient desktop with a 7600xt in it would have been a better idea.&lt;/p&gt; &lt;p&gt;In the past I've really enjoyed Gemma because it seems to be a jack-of-all-trades type of model that you can rely on for a lot of different use cases. I used the 4B q4 and sometimes the 12B q4 model, but I was never able to run the 27B with any speed...&lt;/p&gt; &lt;p&gt;Now that I've essentially removed the need to worry about VRAM - If I'm looking for a good model that is good at conversation, help with homework, help with coding, but overall just works, what would be the best all-around-all-purpose model that fits in 128 gigabytes and runs ok?&lt;/p&gt; &lt;p&gt;And, bonus round: Am i stupid for buying this system? Part of the logic was that I really don't expect these chips to depreciate much in value in the next 3 years...&lt;/p&gt; &lt;p&gt;I also don't really care about token speed as long as it's over 10.&lt;/p&gt; &lt;p&gt;thankee&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironicstatistic"&gt; /u/ironicstatistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm18qx/best_use_case_for_ryzen_395_128gb_variant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm18qx/best_use_case_for_ryzen_395_128gb_variant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm18qx/best_use_case_for_ryzen_395_128gb_variant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T22:37:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkyex0</id>
    <title>Your post is getting popular and we just featured it on our Discord!</title>
    <updated>2026-01-23T18:16:47+00:00</updated>
    <author>
      <name>/u/roculus</name>
      <uri>https://old.reddit.com/user/roculus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your post is getting popular and we just featured it on our Discord! Come check it out!&lt;/p&gt; &lt;p&gt;You've also been given a special flair for your contribution. We appreciate your post!&lt;/p&gt; &lt;p&gt;I am a bot and this action was performed automatically.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Can you change this marketing bot to make these private messages to the OP of the post instead of pinning it to the top of all the threads? Are you making money off the discord or something? I don't know about anyone else but these bot spam posts are annoying. You make it appear you are talking to the OP so a private message would be better. You already have a pinned thread at the top of this reddit letting everyone know about the discord that's been there for the past 5 months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roculus"&gt; /u/roculus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T18:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltqfx</id>
    <title>The Eval problem for AI Agents</title>
    <updated>2026-01-24T17:54:16+00:00</updated>
    <author>
      <name>/u/AlpineContinus</name>
      <uri>https://old.reddit.com/user/AlpineContinus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I work at a company that develops AI agents for information retrieval, and I have observed some pretty important problems that are major bottlenecks for us.&lt;/p&gt; &lt;p&gt;I am very curious to hear from other people that work on AI agents companies to know if they face the same problems and how they handle it (approaches, tools, etc).&lt;/p&gt; &lt;p&gt;AI agents based on LLMs are essentially stochastic, and so it is very hard to affirm how well they behave. In order to evaluate it, you would need a relatively big, varied, realistic and bias-free dataset for your specific use case.&lt;/p&gt; &lt;p&gt;The problem is: Most specific use cases don‚Äôt have pre-made datasets available.&lt;/p&gt; &lt;p&gt;The option is to resort to synthetic data generation, but it is a pretty unreliable source of ground truth.&lt;/p&gt; &lt;p&gt;Writing a dataset by hand is not scalable at all.&lt;/p&gt; &lt;p&gt;The usual solution is some data augmentation on top of a curated hand-written dataset.&lt;/p&gt; &lt;p&gt;It feels like the entire AI agents industry is being built on very shaky grounds. It is very hard to affirm anything about these systems with precise metrics. Most of the evaluation is done by hand and based on very subjective metrics. And I believe this is really holding back the adoption of these systems.&lt;/p&gt; &lt;p&gt;I would love to know how other developers see these problems, and how they currently tackle them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlpineContinus"&gt; /u/AlpineContinus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltqfx/the_eval_problem_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltqfx/the_eval_problem_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltqfx/the_eval_problem_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T17:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql6cz7</id>
    <title>Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR</title>
    <updated>2026-01-23T23:20:23+00:00</updated>
    <author>
      <name>/u/Efficient-Proof-1824</name>
      <uri>https://old.reddit.com/user/Efficient-Proof-1824</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt; &lt;img alt="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" src="https://preview.redd.it/hlrhml65m6fg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=05c26e9e3a4c3182ca841254a0d81f18d6a5901f" title="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;The architecture on this thing is completely wonky, and it's a direct result of me changing ideas and scope midstream, but sharing because I think it's pretty neat&lt;/p&gt; &lt;p&gt;Ultimate goal for me here is to build an agent that can play Pokemon Red, ideally beat it! Plan is to use a mix of LLMs for action plan generation and then using a small neural network to score them. Set a auto-train and you can start stacking up data for training. I bundled everything here as a Svelte app and deployed it on github pages. &lt;/p&gt; &lt;p&gt;Live: &lt;a href="https://sidmohan0.github.io/tesserack/"&gt;https://sidmohan0.github.io/tesserack/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/sidmohan0/tesserack"&gt;https://github.com/sidmohan0/tesserack&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;LLM&lt;/strong&gt;: Qwen 2.5 1.5B running via WebLLM (WebGPU-accelerated) &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Policy network&lt;/strong&gt;: TensorFlow.js neural net that learns from gameplay &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Emulator&lt;/strong&gt;: binjgb compiled to WASM &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Game state&lt;/strong&gt;: Direct RAM reading for ground-truth (badges, party, location, items) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient-Proof-1824"&gt; /u/Efficient-Proof-1824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hlrhml65m6fg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T23:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlu6kh</id>
    <title>The mysterious price of Ada and and Ampere workstation GPUs</title>
    <updated>2026-01-24T18:10:28+00:00</updated>
    <author>
      <name>/u/insulaTropicalis</name>
      <uri>https://old.reddit.com/user/insulaTropicalis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's just something I can't wrap my head around.&lt;/p&gt; &lt;p&gt;An RTX Blackwell Pro 5000 has 48GB memory. Compute is less than an RTX 6000 Ada, but not so much less. If you use FP4 it is much more. QAT with 4-bit seems something that will become prevalent, so FP4 is a big deal. Memory bandwidth is 140% of Ada. Power draw is the same. PCIe is 5.0 vs 4.0.&lt;/p&gt; &lt;p&gt;Seems that Blackwell wins or ties in all important regards, and it costs &lt;em&gt;less&lt;/em&gt; than 6000 Ada. Even more bizzarre, RTX A6000 Ampere, which is inferior in every regard and very old, still costs as much as Pro 5000.&lt;/p&gt; &lt;p&gt;I understand that some people can have an Ada or Ampere multi-GPU set-up and wants to expend it or to change a broken one, but is it enough to explain this weird market? Do these sellers actually find buyers?&lt;/p&gt; &lt;p&gt;Even RTX 4090 costs more today than when I bought mine. Who buys at these prices? What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/insulaTropicalis"&gt; /u/insulaTropicalis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlanzn</id>
    <title>GLM-4.7-Flash-REAP on RTX 5060 Ti 16 GB - 200k context window!</title>
    <updated>2026-01-24T02:26:28+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my latest local coding setup, the params are mostly based on &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash"&gt;Unsloth's recommendation for tool calling&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"&gt;unsloth/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repeat penalty: disabled&lt;/li&gt; &lt;li&gt;Temperature: 0.7&lt;/li&gt; &lt;li&gt;Top P: 1&lt;/li&gt; &lt;li&gt;Min P: 0.01&lt;/li&gt; &lt;li&gt;Standard Microcenter PC setup: RTX 5060 Ti 16 GB, 32 GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running this in LM Studio for my own convenience, but it can be run in any setup you have.&lt;/p&gt; &lt;p&gt;With 16k context, everything fit within the GPU, so the speed was impressive:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;965.16 tok/s&lt;/td&gt; &lt;td&gt;26.27 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The tool calls were mostly accurate and the generated code was good, but the context window was too little, so the model ran into looping issue after exceeding that. It kept making the same tool call again and again because the conversation history was truncated.&lt;/p&gt; &lt;p&gt;With 64k context, everything still fit, but the speed started to slow down.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;671.48 tok/s&lt;/td&gt; &lt;td&gt;8.84 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm pushing my luck to see if 100k context still fits. It doesn't! Hahaha. The CPU fan started to scream, RAM usage spiked up, GPU copy chart (in Task Manager) started to dance. Completely unusable.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;172.02 tok/s&lt;/td&gt; &lt;td&gt;0.51 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LM Studio just got the new &amp;quot;Force Model Expert Weight onto CPU&amp;quot; feature (basically llama.cpp's &lt;code&gt;--n-cpu-moe&lt;/code&gt;), and yeah, why not? this is also an MoE model, so let's enable that. Still with 100k context. And wow! only half of the GPU memory was used (7 GB), but with 90% RAM now (29 GB), seems like flash attention also got disabled. The speed was impressive.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;485.64 tok/s&lt;/td&gt; &lt;td&gt;8.98 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's push our luck again, this time, 200k context!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;324.84 tok/s&lt;/td&gt; &lt;td&gt;7.70 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What a crazy time. Almost very month we're getting beefier models that somehow fit on even crappier hardware. Just this week I was thinking of selling my 5060 for an old 3090, but that definitely unnecessary now!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Turned out with CPU MoE offload, I can just run the non-REAP model it self. Here's the speed for UD Q5_K_XL on my card, at 100k token window:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;206.07 tok/s&lt;/td&gt; &lt;td&gt;5.06 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;With more tweak, reducing GPU offload count (36/47), keep KV cache in GPU memory, disable nmap,... the speed increased.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;267.23 tok/s&lt;/td&gt; &lt;td&gt;6.23 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And yes, I was running this without Flash Attention the whole time, since LM Studio didn't support it this model (at the time of writing).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update 2:&lt;/strong&gt; I decided to compile llama.cpp to get this running with FA, same UD Q5_K_XL model, it's now better!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;153.36 tok/s&lt;/td&gt; &lt;td&gt;11.49 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Update 3:&lt;/strong&gt; Alright, I think I'm gonna conclude the experiment here, llama.cpp is the way to go.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;423.77 tok/s&lt;/td&gt; &lt;td&gt;14.4 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here's the params to run:&lt;/p&gt; &lt;p&gt;&lt;code&gt; llama-server \ --model ./GLM-4.7-Flash-UD-Q5_K_XL.gguf \ --alias &amp;quot;glm-4.7-flash-q5&amp;quot; --seed 1234 \ --temp 0.7 --top-p 1 --min-p 0.01 \ --ctx-size 102400 --jinja \ --threads 7 --fit on --cpu-moe \ --batch-size 768 --ubatch-size 768 &lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T02:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qljf7o</id>
    <title>AI &amp; ML Weekly ‚Äî Hugging Face Highlights</title>
    <updated>2026-01-24T10:15:01+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the most notable &lt;strong&gt;AI models released or updated this week on Hugging Face&lt;/strong&gt;, categorized for easy scanning üëá&lt;/p&gt; &lt;h1&gt;Text &amp;amp; Reasoning Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7 (358B)&lt;/strong&gt; ‚Äî Large-scale multilingual reasoning model &lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash (31B)&lt;/strong&gt; ‚Äî Faster, optimized variant for text generation &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;https://huggingface.co/zai-org/GLM-4.7-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth GLM-4.7-Flash GGUF (30B)&lt;/strong&gt; ‚Äî Quantized version for local inference &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiquidAI LFM 2.5 Thinking (1.2B)&lt;/strong&gt; ‚Äî Lightweight reasoning-focused LLM &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alibaba DASD-4B-Thinking&lt;/strong&gt; ‚Äî Compact thinking-style language model &lt;a href="https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking"&gt;https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Agent &amp;amp; Workflow Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Report (8B)&lt;/strong&gt; ‚Äî Agent model optimized for report generation &lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;https://huggingface.co/openbmb/AgentCPM-Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Explore (4B)&lt;/strong&gt; ‚Äî Exploration-focused agent reasoning model &lt;a href="https://huggingface.co/openbmb/AgentCPM-Explore"&gt;https://huggingface.co/openbmb/AgentCPM-Explore&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sweep Next Edit (1.5B)&lt;/strong&gt; ‚Äî Code-editing and refactoring assistant &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"&gt;https://huggingface.co/sweepai/sweep-next-edit-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio: Speech, Voice &amp;amp; TTS&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VibeVoice-ASR (9B)&lt;/strong&gt; ‚Äî High-quality automatic speech recognition &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PersonaPlex 7B&lt;/strong&gt; ‚Äî Audio-to-audio personality-driven voice model &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 TTS (1.7B)&lt;/strong&gt; ‚Äî Custom &amp;amp; base voice text-to-speech models &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pocket-TTS&lt;/strong&gt; ‚Äî Lightweight open TTS model &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HeartMuLa OSS (3B)&lt;/strong&gt; ‚Äî Text-to-audio generation model &lt;a href="https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B"&gt;https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Vision: Image, OCR &amp;amp; Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Step3-VL (10B)&lt;/strong&gt; ‚Äî Vision-language multimodal model &lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;https://huggingface.co/stepfun-ai/Step3-VL-10B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LightOnOCR 2 (1B)&lt;/strong&gt; ‚Äî OCR-focused vision-language model &lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;https://huggingface.co/lightonai/LightOnOCR-2-1B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TranslateGemma (4B / 12B / 27B)&lt;/strong&gt; ‚Äî Multimodal translation models &lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;https://huggingface.co/google/translategemma-4b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-12b-it"&gt;https://huggingface.co/google/translategemma-12b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-27b-it"&gt;https://huggingface.co/google/translategemma-27b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MedGemma 1.5 (4B)&lt;/strong&gt; ‚Äî Medical-focused multimodal model &lt;a href="https://huggingface.co/google/medgemma-1.5-4b-it"&gt;https://huggingface.co/google/medgemma-1.5-4b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image Generation &amp;amp; Editing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-Image&lt;/strong&gt; ‚Äî Text-to-image generation model &lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;https://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FLUX.2 Klein (4B / 9B)&lt;/strong&gt; ‚Äî High-quality image-to-image models &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-4B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-4B&lt;/a&gt; &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-9B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-9B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen Image Edit (LoRA / AIO)&lt;/strong&gt; ‚Äî Advanced image editing &amp;amp; multi-angle edits &lt;a href="https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA"&gt;https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA&lt;/a&gt; &lt;a href="https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO"&gt;https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Z-Image-Turbo&lt;/strong&gt; ‚Äî Fast text-to-image generation &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Video Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LTX-2&lt;/strong&gt; ‚Äî Image-to-video generation model &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;https://huggingface.co/Lightricks/LTX-2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Any-to-Any / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chroma (6B)&lt;/strong&gt; ‚Äî Any-to-any multimodal generation &lt;a href="https://huggingface.co/FlashLabs/Chroma-4B"&gt;https://huggingface.co/FlashLabs/Chroma-4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T10:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlw8vl</id>
    <title>Loki-v2-70B: Narrative/DM-focused fine-tune (600M+ token custom dataset)</title>
    <updated>2026-01-24T19:25:15+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello from Crucible Labs!&lt;/p&gt; &lt;p&gt;We just finished the 1-epoch fine-tune for Loki-v2-70B, based on Llama-3.3-70B-Instruct.&lt;/p&gt; &lt;p&gt;The goal with this project wasn't to make another &amp;quot;helpful assistant,&amp;quot; but to build a model specifically for long-form narrative, TTRPG-style Dungeon Mastering, and consistent roleplay.&lt;/p&gt; &lt;p&gt;We‚Äôve spent around six months generating and curating a V2 version of our original Loki Dataset in what we believe is the largest custom-generated dataset for this specific niche:&lt;/p&gt; &lt;p&gt;Total Tokens: 600M+&lt;/p&gt; &lt;p&gt;Size: ~2.5 GB&lt;/p&gt; &lt;p&gt;Composition: 46k+ QA lines, 19k+ prose lines, and 12k+ lines focused on dark/high-stakes scenarios.&lt;/p&gt; &lt;p&gt;The model card has a very extensive guide on how to use the model and details on worlds and universes, so please make sure to read through it!&lt;/p&gt; &lt;p&gt;This is an independent project, so we‚Äôre looking for genuine feedback on how it handles long-context narrative and whether the DM bias feels right to you.&lt;/p&gt; &lt;p&gt;L3.3-70B-Loki-V2.0:&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-GGUF"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EXL3: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-EXL3"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-EXL3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lower quants seem to have an issue with how we trained in 256 rank, so please be aware of this. Higher rank training=more affected by quantization, and there doesn't seem to be a way to alleviate this.&lt;/p&gt; &lt;p&gt;- The Crucible Labs Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qloeu4</id>
    <title>MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations</title>
    <updated>2026-01-24T14:29:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt; &lt;img alt="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" src="https://a.thumbs.redditmedia.com/J6s266XP1Z7JRBZht5FAQGe2o36oOfOTvuiPLWD9El8.jpg" title="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/minimax/minimax-m2-her"&gt;https://openrouter.ai/minimax/minimax-m2-her&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations. Designed to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f"&gt;https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/api-reference/text-chat"&gt;https://platform.minimax.io/docs/api-reference/text-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/guides/models-intro"&gt;https://platform.minimax.io/docs/guides/models-intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwcoi</id>
    <title>My Strix Halo beholds itself but believes its in the cloud</title>
    <updated>2026-01-24T19:29:20+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"&gt; &lt;img alt="My Strix Halo beholds itself but believes its in the cloud" src="https://external-preview.redd.it/cnJ2N2V3eWxtY2ZnMQh20pvmbVT22ZdBE-sn9Tc8Ujs4oEH6LQUVqmOmu06-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bff3dbe0b1a561ec97bacc8d8a2a5c4290b4f772" title="My Strix Halo beholds itself but believes its in the cloud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This iPhone app sends photos to a VLM served by the Halo on the local network and gets the response back. &lt;/p&gt; &lt;p&gt;The singularity might require a new system prompt‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o88lli0mmcfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm0l2q</id>
    <title>I built a tool that learns your codebase's unwritten rules and conventions- no AI, just AST parsing</title>
    <updated>2026-01-24T22:11:05+00:00</updated>
    <author>
      <name>/u/Fluffy_Citron3547</name>
      <uri>https://old.reddit.com/user/Fluffy_Citron3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last six months teaching myself to orchestrate engineering codebases using AI agents. What I found is that the biggest bottleneck isn‚Äôt intelligence it‚Äôs the context window. Why have we not given agents the proper tooling to defeat this limitation? Agents constantly forget how I handle error structures or which specific components I use for the frontend. This forces mass auditing and refactoring, causing me to spend about 75% of my token budget on auditing versus writing.&lt;/p&gt; &lt;p&gt;That is why I built Drift. Drift is a first-in-class codebase intelligence tool that leverages semantic learning through AST parsing with Regex fallbacks. It scans your codebase and extracts 15 different categories with over 150 patterns. Everything is persisted and recallable via CLI or MCP in your IDE of choice.&lt;/p&gt; &lt;p&gt;What makes drift different?&lt;/p&gt; &lt;p&gt;It‚Äôs learning based not rule based. AI is capable of writing high quality code but the context limitation makes fitting conventions through a large code base extremely tedious and time consuming often leading to things silently failing or just straight up not working. &lt;/p&gt; &lt;p&gt;Drift_context is the real magic &lt;/p&gt; &lt;p&gt;Instead of an agent calling 10 tools and sytheneszing results it: &lt;/p&gt; &lt;p&gt;Takes intent &lt;/p&gt; &lt;p&gt;Takes focus area&lt;/p&gt; &lt;p&gt;Returned a curated package&lt;/p&gt; &lt;p&gt;This eliminates the audit loop, hallucination risk and gives the agent everything needed in one call.&lt;/p&gt; &lt;p&gt;Call graph analysis across 6 different languages&lt;/p&gt; &lt;p&gt;Not just ‚ÄúWhat functions exists‚Äù but..&lt;/p&gt; &lt;p&gt;Drift_reachability_forward &amp;gt; What data can this code access? (Massive for helping with security)&lt;/p&gt; &lt;p&gt;Drift_reachability_inverse &amp;gt; Who can access this field? &lt;/p&gt; &lt;p&gt;Drift_impact_analysis &amp;gt; what breaks if I change this with scoring.&lt;/p&gt; &lt;p&gt;Security-audit-grade analysis available to you or your agent through MCP or CLI&lt;/p&gt; &lt;p&gt;The MCP has been built out with frontier capabilities ensuring context is preserved and is a true tool for your agents&lt;/p&gt; &lt;p&gt;Currently support TS, PY, Java, C#, PHP, GO :&lt;/p&gt; &lt;p&gt;with‚Ä¶&lt;/p&gt; &lt;p&gt;Tree sitter parsing&lt;/p&gt; &lt;p&gt;Regex fallback&lt;/p&gt; &lt;p&gt;Framework aware detection&lt;/p&gt; &lt;p&gt;All data persist into a local file (/.drift) and you have the ability to approve, deny and ignore certain components, functions and features you don‚Äôt want the agent to be trained on.&lt;/p&gt; &lt;p&gt;check it out here: &lt;/p&gt; &lt;p&gt;IF you run into any edge cases or I don‚Äôt support the framework your code base is currently running on open a git issue feature request and ive been banging them out quick&lt;/p&gt; &lt;p&gt;Thank you for all the upvotes and stars on the project it means so much!&lt;/p&gt; &lt;p&gt;check it out here: &lt;a href="https://github.com/dadbodgeoff/drift"&gt;https://github.com/dadbodgeoff/drift&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Citron3547"&gt; /u/Fluffy_Citron3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T22:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwibf</id>
    <title>What is the best general-purpose model to run locally on 24GB of VRAM in 2026?</title>
    <updated>2026-01-24T19:35:11+00:00</updated>
    <author>
      <name>/u/Paganator</name>
      <uri>https://old.reddit.com/user/Paganator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Gemma 3 27b since its release nine months ago, which is an eternity in the AI field. Has anything better been released since then that can run well on a single 3090ti?&lt;/p&gt; &lt;p&gt;I'm not looking to code, to create agents, or to roleplay; I just want a good model to chat with and get reasonably smart answers to questions. If it can view images, that's even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paganator"&gt; /u/Paganator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlt3pw</id>
    <title>GLM 4.7 Flash uncensored - Balanced &amp; Aggressive variants (GGUF)</title>
    <updated>2026-01-24T17:30:56+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I made uncensored versions of the new GLM 4.7 Flash from Z.ai.&lt;/p&gt; &lt;p&gt;For those who don't know the model, it's 30B-A3B MoE, so only ~3B active params (will have fast inference!) and 200K context. Runs surprisingly well for what it is.&lt;/p&gt; &lt;p&gt;Two variants:&lt;/p&gt; &lt;p&gt;- Balanced - excellent for agentic coding stuff where you still want (uncensored) reliability&lt;/p&gt; &lt;p&gt;- Aggressive - great for every other uncensored topic&lt;/p&gt; &lt;p&gt;Quants available: FP16, Q8_0, Q6_K, Q4_K_M&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings from Z.ai:&lt;/p&gt; &lt;p&gt;- General: --temp 1.0 --top-p 0.95&lt;/p&gt; &lt;p&gt;- Agentic/tool use: --temp 0.7 --top-p 1.0&lt;/p&gt; &lt;p&gt;- Keep repeat penalty at 1.0 (or directly off)&lt;/p&gt; &lt;p&gt;- llama.cpp users: --min-p 0.01 and --jinja&lt;/p&gt; &lt;p&gt;Heads up, it currently doesn't play nice with Ollama (has some chat template issues). Works fine with llama.cpp, LM Studio, Jan, koboldcpp.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;Edit: P.S. For those looking for smaller models, I also did GPT-OSS 20B, MXFP4 - Lossless:&lt;br /&gt; - &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit2: To clarify, the aim of the abliteration versions I publish is that they are effectively lossless to their original (censored) counterparts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T17:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlr3wj</id>
    <title>I built an open-source audiobook converter using Qwen3 TTS - converts PDFs/EPUBs to high-quality audiobooks with voice cloning support</title>
    <updated>2026-01-24T16:16:15+00:00</updated>
    <author>
      <name>/u/TheyCallMeDozer</name>
      <uri>https://old.reddit.com/user/TheyCallMeDozer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Turn any book into an audiobook with AI voice synthesis!&lt;/strong&gt; I just released an open-source tool that converts PDFs, EPUBs, DOCX, and TXT files into high-quality audiobooks using &lt;strong&gt;Qwen3 TTS&lt;/strong&gt; - the amazing open-source voice model that just went public.&lt;/p&gt; &lt;h2&gt;What it does:&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Converts any document format&lt;/strong&gt; (PDF, EPUB, DOCX, DOC, TXT) into audiobooks &lt;strong&gt;Two voice modes&lt;/strong&gt;: Pre-built speakers (Ryan, Serena, etc.) or clone any voice from a reference audio &lt;strong&gt;Always uses 1.7B model&lt;/strong&gt; for best quality &lt;strong&gt;Smart chunking&lt;/strong&gt; with sentence boundary detection &lt;strong&gt;Intelligent caching&lt;/strong&gt; to avoid re-processing &lt;strong&gt;Auto cleanup&lt;/strong&gt; of temporary files &lt;/p&gt; &lt;h2&gt;Key Features:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Custom Voice Mode&lt;/strong&gt;: Professional narrators optimized for audiobook reading&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Clone Mode&lt;/strong&gt;: Automatically transcribes reference audio and clones the voice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-format support&lt;/strong&gt;: Works with PDFs, EPUBs, Word docs, and plain text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequential processing&lt;/strong&gt;: Ensures chunks are combined in correct order&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress tracking&lt;/strong&gt;: Real-time updates with time estimates ## Quick Start: Install Qwen3 TTS (one-click install with Pinokio) Install Python dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; Place your books in &lt;code&gt;book_to_convert/&lt;/code&gt; folder Run: &lt;code&gt;python audiobook_converter.py&lt;/code&gt; Get your audiobook from &lt;code&gt;audiobooks/&lt;/code&gt; folder! ## Voice Cloning Example: &lt;code&gt;bash python audiobook_converter.py --voice-clone --voice-sample reference.wav &lt;/code&gt; The tool automatically transcribes your reference audio - no manual text input needed! ## Why I built this: I was frustrated with expensive audiobook services and wanted a free, open-source solution. Qwen3 TTS going open-source was perfect timing - the voice quality is incredible and it handles both generic speech and voice cloning really well. ## Performance:&lt;/li&gt; &lt;li&gt;Processing speed: ~4-5 minutes per chunk (1.7B model) it is a little slow im working on it&lt;/li&gt; &lt;li&gt;Quality: High-quality audio suitable for audiobooks&lt;/li&gt; &lt;li&gt;Output: MP3 format, configurable bitrate ## GitHub: üîó &lt;strong&gt;&lt;a href="https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter"&gt;https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;What do you think?&lt;/strong&gt; Have you tried Qwen3 TTS? What would you use this for?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyCallMeDozer"&gt; /u/TheyCallMeDozer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T16:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and it‚Äôs a significant step forward for local speech synthesis. If you‚Äôve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAI‚Äôs TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;We‚Äôve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAI‚Äôs TTS endpoints.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ü§ó Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;üìÑ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;üíª Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlnruw</id>
    <title>Personal experience with GLM 4.7 Flash Q6 (unsloth) + Roo Code + RTX 5090</title>
    <updated>2026-01-24T14:02:56+00:00</updated>
    <author>
      <name>/u/Septerium</name>
      <uri>https://old.reddit.com/user/Septerium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am much more interested in how folks experience quantized versions of new models than just looking at bar graphs, so here is my humble contribution. &lt;/p&gt; &lt;p&gt;I have been using GLM 4.7 Flash to perform a few refactoring tasks in some personal web projects and have been quite impressed by how well the model handles Roo Code without breaking apart. For this agentic tool specifically, it has been much more reliable and precise than GPT-OSS 120b, GLM 4.5 Air, or Devstral 24b.&lt;/p&gt; &lt;p&gt;Here's the llama.cpp command I used to squeeze UD-Q6_K_XL + 48k tokens of context in my RTX 5090 VRAM and get about 150 tok/s (tg):&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server --model downloaded_models/GLM-4.7-Flash-UD-Q6_K_XL.gguf --port 11433 --host &amp;quot;0.0.0.0&amp;quot; -fa on --ctx-size 48000 --temp 0.7 --top-p 1.0 --min-p 0.01 --jinja -ngl 99&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septerium"&gt; /u/Septerium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltwza</id>
    <title>Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence.</title>
    <updated>2026-01-24T18:00:50+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt; &lt;img alt="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." src="https://preview.redd.it/66fd18ro6cfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f579cce389f709dbf297867095118be2027f04ea" title="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/2014786516153991339"&gt;https://x.com/ArtificialAnlys/status/2014786516153991339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A key driver of this momentum is the Korean National Sovereign AI Initiative, a government-backed, nationwide competition that incentivizes domestic model development through a multi-stage elimination process. The initiative shortlists national champions, with winners receiving direct government funding and guaranteed access to large-scale GPU capacity.&lt;/p&gt; &lt;p&gt;‚û§ In August 2025, five organizations were selected: Naver, SK Telecom, LG Group, Upstage, and NC AI&lt;/p&gt; &lt;p&gt;‚û§ In the most recent round announced last week, the field narrowed to three: LG, SK Telecom, and Upstage.&lt;/p&gt; &lt;p&gt;‚û§ A fourth finalist is expected to be selected in the coming months as the evaluation process continues&lt;/p&gt; &lt;p&gt;Generally, top Korean AI models tend to be open weights, and vary in size ranging from Motif‚Äòs 12.7B Thinking model to LG‚Äôs 236B K-EXAONE. Other models, such as Korea Telecom (KT)‚Äôs Mi:dm K 2.5 Pro, are proprietary and developed with a focus on business integration with existing KT clients.&lt;/p&gt; &lt;p&gt;Overview of major releases:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ LG | K-EXAONE -&lt;/strong&gt; The current leader in the Korean AI race and a shortlisted model in the Korean National Sovereign AI Initiative. K-EXAONE is a 236B open weights model and scores 32 on the Artificial Analysis Intelligence Index. K-EXAONE performs strongly across various intelligence evaluations from scientific reasoning, instruction following, to agentic coding. However, this model has high verbosity, using 100 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Upstage | Solar Open -&lt;/strong&gt; Another shortlisted model in the Korean National Sovereign AI Initiative. Solar Open is a 100B open-weights model and scores 21 on the Artificial Analysis Intelligence Index. Solar Open performs well in instruction following and has lower hallucination rate compared to peer Korean models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Naver | HyperCLOVA X SEED Think -&lt;/strong&gt; A 32B open weights reasoning model that scores 24 on the Artificial Analysis Intelligence Index. HyperCLOVA X SEED Think demonstrates strong performance on agentic tool-use workflows and scores highly in the Global MMLU Lite multilingual index for Korean, highlighting its potential usefulness in a primarily Korean language environment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Korea Telecom | Mi:dm K 2.5 Pro -&lt;/strong&gt; A proprietary reasoning model that scores 23 on the Artificial Analysis Intelligence Index. Mi:dm K 2.5 Pro sees strong performance in agentic tool-use. Mi:dm K 2.5 Pro currently has no publicly available endpoint. Instead, Korea Telecom primarily intends to package this model into product offerings and use this model to serve KT‚Äôs clients&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Motif | Motif-2-12.7B -&lt;/strong&gt; A small open weights model that scores 24 on the Artificial Analysis Intelligence Index. Motif-2-12.7B performs well in long-context reasoning and knowledge, but is highly token intensive - using 120 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66fd18ro6cfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
