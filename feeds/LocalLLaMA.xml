<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-06T15:48:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pfr7c2</id>
    <title>Facing 2 choice, 1 DGX spark or RTX 5090</title>
    <updated>2025-12-06T14:56:31+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can get both with fairly same amount of money (Gpu still need a cpu ram etc.) but Dgx offer 128 gb slow vram, another offer faster 32 gb vram, my main use case is inference, i wanna be able to local inference Qwen next 80B level model with not less than Q8 quantized but from time to time I wanna be able to fine tune some 8B-12B level model and training some TTS model too&lt;/p&gt; &lt;p&gt;What should I choose?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfr7c2/facing_2_choice_1_dgx_spark_or_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfr7c2/facing_2_choice_1_dgx_spark_or_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfr7c2/facing_2_choice_1_dgx_spark_or_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:56:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfrxwl</id>
    <title>[MCP Server] BA Workflow Tools - Sprint planning, velocity, MoSCoW prioritization</title>
    <updated>2025-12-06T15:29:10+00:00</updated>
    <author>
      <name>/u/cs97jjm3</name>
      <uri>https://old.reddit.com/user/cs97jjm3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Released an MCP server for Business Analyst workflows. Works with Claude Desktop (and anything else that supports MCP).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;17 tools including:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Working days calculator (UK bank holidays baked in)&lt;/li&gt; &lt;li&gt;Sprint date generator&lt;/li&gt; &lt;li&gt;Release date estimator from velocity/story points&lt;/li&gt; &lt;li&gt;MoSCoW priority calculator, capacity planner, dependency validator&lt;/li&gt; &lt;li&gt;User story formatter (proper As a/I want/So that structure)&lt;/li&gt; &lt;li&gt;Timezone converter + meeting time finder&lt;/li&gt; &lt;li&gt;Estimation converter (T-shirt ↔ story points ↔ hours)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Tech:&lt;/strong&gt; Node.js, uses &lt;a href="/u/modelcontextprotocol/sdk"&gt;u/modelcontextprotocol/sdk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/cs97jjm3/ba-workflow-tools"&gt;https://github.com/cs97jjm3/ba-workflow-tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Planning to add more tools - open to PRs and suggestions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cs97jjm3"&gt; /u/cs97jjm3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrxwl/mcp_server_ba_workflow_tools_sprint_planning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrxwl/mcp_server_ba_workflow_tools_sprint_planning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrxwl/mcp_server_ba_workflow_tools_sprint_planning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T15:29:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pflakt</id>
    <title>ARC Prize 2025 results and analysis</title>
    <updated>2025-12-06T09:31:34+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pflakt/arc_prize_2025_results_and_analysis/"&gt; &lt;img alt="ARC Prize 2025 results and analysis" src="https://external-preview.redd.it/g5_XbspyVoCUgoU87RpXGpJzxJV5r0xDHqeIzldwGzI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c661c404d070b6f2aee0a524819208649d2d9310" title="ARC Prize 2025 results and analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The ARC Prize 2025 concluded its second year, confirming &amp;quot;refinement loops&amp;quot; as the central theme driving progress in AI reasoning, although the Grand Prize remains unclaimed. The competition saw 1,455 teams and 90 papers submitted, with the top Kaggle score reaching a new state-of-the-art of 24% on the private ARC-AGI-2 dataset. Commercial AI systems also demonstrated significant advancement, with Anthropic's Opus 4.5 scoring 37.6% and a bespoke refinement solution on Gemini 3 Pro achieving 54%. ARC-AGI has cemented its role as a key industry benchmark, used by all four major AI labs to track frontier reasoning capabilities, which the report positions as a new technological paradigm on par with the invention of LLMs. All winning solutions and papers from the 2025 competition have been made open-source.&lt;/p&gt; &lt;p&gt;The core technical breakthrough highlighted is the &amp;quot;refinement loop,&amp;quot; an iterative process of generating candidate solutions (exploration) and analyzing them for feedback (verification) to incrementally optimize a program. This concept is manifesting in two major ways: through program synthesis approaches like Evolutionary Test-Time Compute, and in novel &amp;quot;zero-pretraining&amp;quot; deep learning methods. Examples of the latter include the Tiny Recursive Model (TRM) and CompressARC, which achieve impressive ARC-AGI performance with extremely small, test-time trained networks (7M and 76K parameters, respectively). Furthermore, commercial models are exhibiting refinement via extended, costly &amp;quot;chain-of-thought&amp;quot; reasoning, and application-layer refinement harnesses are proving highly effective, boosting Gemini 3 Pro's performance from 31% to 54% on ARC-AGI-2, demonstrating that task reliability can be meaningfully improved at the application layer.&lt;/p&gt; &lt;p&gt;Looking forward, the report notes that current AI reasoning systems can reliably automate tasks characterized by sufficient foundational model knowledge and a verifiable feedback signal, marking a profound upgrade in capability. However, this progress is leading to a new form of &amp;quot;overfitting&amp;quot; on benchmarks like ARC-AGI-1/2, where models are leveraging embedded knowledge of the ARC domain, necessitating a benchmark evolution. To continue driving progress toward AGI, the ARC Prize is preparing to release ARC-AGI-3 in early 2026. This new version will feature the first major format change since 2019, shifting from static reasoning to challenging interactive reasoning, requiring new capabilities like planning, memory, and goal acquisition, and will formally compare human versus AI action efficiency.&lt;/p&gt; &lt;h1&gt;High Scores&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Place&lt;/th&gt; &lt;th align="left"&gt;Prize&lt;/th&gt; &lt;th align="left"&gt;Team&lt;/th&gt; &lt;th align="left"&gt;ARC-AGI-2 Private Eval Score&lt;/th&gt; &lt;th align="left"&gt;Sources&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1st&lt;/td&gt; &lt;td align="left"&gt;$25k&lt;/td&gt; &lt;td align="left"&gt;NVARC&lt;/td&gt; &lt;td align="left"&gt;24.03%&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.kaggle.com/code/gregkamradt/arc2-qwen3-unsloth-flash-lora-batch8-queue-trm2/edit?fromFork=1"&gt;Code&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2nd&lt;/td&gt; &lt;td align="left"&gt;$10k&lt;/td&gt; &lt;td align="left"&gt;the ARChitects&lt;/td&gt; &lt;td align="left"&gt;16.53%&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.kaggle.com/code/gregkamradt/arc-2025-diffusion/edit?fromFork=1"&gt;Code&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3rd&lt;/td&gt; &lt;td align="left"&gt;$5k&lt;/td&gt; &lt;td align="left"&gt;MindsAI&lt;/td&gt; &lt;td align="left"&gt;12.64%&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.kaggle.com/code/gregkamradt/mindsai-tufa-2025-v4/edit?fromFork=1"&gt;Code&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4th&lt;/td&gt; &lt;td align="left"&gt;$5k&lt;/td&gt; &lt;td align="left"&gt;Lonnie&lt;/td&gt; &lt;td align="left"&gt;6.67%&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.kaggle.com/code/lonnieqin/lb-5-83-baseline-from-1st-place-of-2024"&gt;Code&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5th&lt;/td&gt; &lt;td align="left"&gt;$5k&lt;/td&gt; &lt;td align="left"&gt;G. Barbadillo&lt;/td&gt; &lt;td align="left"&gt;6.53%&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.kaggle.com/code/ironbar/the-architects-single-task-ttt"&gt;Code&lt;/a&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.kaggle.com/competitions/arc-prize-2025/leaderboard"&gt;View on Kaggle&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Paper Awards&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Place&lt;/th&gt; &lt;th align="left"&gt;Prize&lt;/th&gt; &lt;th align="left"&gt;Authors&lt;/th&gt; &lt;th align="left"&gt;Title&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1st&lt;/td&gt; &lt;td align="left"&gt;$50k&lt;/td&gt; &lt;td align="left"&gt;A. Jolicoeur-Martineau&lt;/td&gt; &lt;td align="left"&gt;Less is More: Recursive Reasoning with Tiny Networks (&lt;a href="https://arxiv.org/abs/2510.04871"&gt;paper&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=P9zzUM0PrBM"&gt;interview&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2nd&lt;/td&gt; &lt;td align="left"&gt;$20k&lt;/td&gt; &lt;td align="left"&gt;J. Pourcel, C. Colas &amp;amp; P. Oudeyer&lt;/td&gt; &lt;td align="left"&gt;Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI (&lt;a href="https://openreview.net/pdf?id=z4IG090qt2"&gt;paper&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=9lIuoslCHWI"&gt;video&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3rd&lt;/td&gt; &lt;td align="left"&gt;$5k&lt;/td&gt; &lt;td align="left"&gt;I. Liao &amp;amp; A. Gu&lt;/td&gt; &lt;td align="left"&gt;ARC-AGI Without Pretraining (&lt;a href="https://iliao2345.github.io/blog_posts/arc_agi_without_pretraining/ARC_AGI_Without_Pretraining.pdf"&gt;paper&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=N9GvFj0cE9s"&gt;video&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Runner Up&lt;/td&gt; &lt;td align="left"&gt;$2.5k&lt;/td&gt; &lt;td align="left"&gt;I. Joffe &amp;amp; C. Eliasmith&lt;/td&gt; &lt;td align="left"&gt;Vector Symbolic Algebras for the Abstraction and Reasoning Corpus (&lt;a href="https://github.com/ijoffe/ARC-VSA-2025/blob/main/paper/paper.pdf"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Runner Up&lt;/td&gt; &lt;td align="left"&gt;$2.5k&lt;/td&gt; &lt;td align="left"&gt;J. Berman&lt;/td&gt; &lt;td align="left"&gt;From Parrots to Von Neumanns: How Evolutionary Test-Time Compute Achieved State-of-the-Art on ARC-AGI (&lt;a href="https://github.com/jerber/arc-lang-public/blob/main/from_parrots_to_von_neumanns.pdf"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Runner Up&lt;/td&gt; &lt;td align="left"&gt;$2.5k&lt;/td&gt; &lt;td align="left"&gt;E. Pang&lt;/td&gt; &lt;td align="left"&gt;Efficient Evolutionary Program Synthesis (&lt;a href="https://open.substack.com/pub/ctpang/p/arc-agi-2-sota-efficient-evolutionary"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Runner Up&lt;/td&gt; &lt;td align="left"&gt;$2.5k&lt;/td&gt; &lt;td align="left"&gt;E. Guichard, F. Reimers, M. Kvalsund, M. Lepperød &amp;amp; S. Nichele&lt;/td&gt; &lt;td align="left"&gt;ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus (&lt;a href="https://etimush.github.io/ARC_NCA/"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Runner Up&lt;/td&gt; &lt;td align="left"&gt;$2.5k&lt;/td&gt; &lt;td align="left"&gt;M. Ho et al.&lt;/td&gt; &lt;td align="left"&gt;ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory (&lt;a href="https://arxiv.org/abs/2509.04439"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Honorable Mentions&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Authors&lt;/th&gt; &lt;th align="left"&gt;Title&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;K. Hu et al.&lt;/td&gt; &lt;td align="left"&gt;ARC-AGI is a Vision Problem! (&lt;a href="https://arxiv.org/abs/2511.14761"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;D. Franzen, J. Disselhoff &amp;amp; D. Hartmann&lt;/td&gt; &lt;td align="left"&gt;Product of Experts with LLMs: Boosting Performance on ARC Is a Matter of Perspective (&lt;a href="https://drive.google.com/file/d/1o1gGmlQTo6tsXzQ1T6NqTOqrP8hXNXnV/view?usp=sharing"&gt;paper&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=CcoGi47qD-w"&gt;interview&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;G. Barbadillo&lt;/td&gt; &lt;td align="left"&gt;Exploring the combination of search and learn for the ARC25 challenge (&lt;a href="https://ironbar.github.io/arc25/05_Solution_Summary/"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;A. Das, O. Ghugarkar, V. Bhat &amp;amp; J. McAuley&lt;/td&gt; &lt;td align="left"&gt;Beyond Brute Force: A Neuro-Symbolic Architecture for Compositional Reasoning in ARC-AGI-2 (&lt;a href="https://github.com/CoreThink-AI/Research-publications/blob/main/Preprints/Beyond_Brute_Force__A_Neuro_Symbolic_Architecture_for_Compositional_Reasoning_in_ARC_AGI_2.pdf"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;R. McGovern&lt;/td&gt; &lt;td align="left"&gt;Test-time Adaptation of Tiny Recursive Models (&lt;a href="https://trelis.com/wp-content/uploads/2025/11/mcgovern_test_time_adaptation_trm.pdf"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;P. Acuaviva et al.&lt;/td&gt; &lt;td align="left"&gt;Rethinking Visual Intelligence: Insights from Video Pretraining (&lt;a href="https://arxiv.org/abs/2510.24448"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;J. Cole &amp;amp; M. Osman&lt;/td&gt; &lt;td align="left"&gt;Don't throw the baby out with the bathwater: How and why deep learning for ARC (&lt;a href="https://arxiv.org/abs/2506.14276"&gt;paper&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=3lXXfNsWIgo"&gt;interview&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;I. Sorokin &amp;amp; Jean-François Puget&lt;/td&gt; &lt;td align="left"&gt;NVARC solution to ARC-AGI-2 2025 (&lt;a href="https://drive.google.com/file/d/1vkEluaaJTzaZiJL69TkZovJUkPSDH5Xc/view?usp=drive_link"&gt;paper&lt;/a&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Sources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arcprize.org/blog/arc-prize-2025-results-analysis"&gt;https://arcprize.org/blog/arc-prize-2025-results-analysis&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arcprize.org/competitions/2025/"&gt;https://arcprize.org/competitions/2025/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.kaggle.com/competitions/arc-prize-2025"&gt;https://www.kaggle.com/competitions/arc-prize-2025&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developer.nvidia.com/blog/nvidia-kaggle-grandmasters-win-artificial-general-intelligence-competition/"&gt;https://developer.nvidia.com/blog/nvidia-kaggle-grandmasters-win-artificial-general-intelligence-competition/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arcprize.org/blog/arc-prize-2025-results-analysis"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pflakt/arc_prize_2025_results_and_analysis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pflakt/arc_prize_2025_results_and_analysis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T09:31:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf92li</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T22:43:49+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your personal favourite (self hosted) model(s) in the &lt;strong&gt;8B&lt;/strong&gt; range for &lt;strong&gt;RAG&lt;/strong&gt; ?&lt;/p&gt; &lt;p&gt;I'm creating a &lt;strong&gt;RAG system&lt;/strong&gt; for a university project, and ideally i want a model that:&lt;br /&gt; * Hallucinates less and refuses to answer, if it doesn't find relevant information in it's context. If it finds partial info, then only answer with that partial piece of info found. won't fill in gaps with general knowledge. I want it strictly based on context.&lt;/p&gt; &lt;p&gt;* Follows instruction well, would do as asked.&lt;/p&gt; &lt;p&gt;* Can find info buried in chunks, and stitch info together to generate an answer. Not hallucinate stuff, but just put 2 and 2 together (instead of expecting a direct call out), make sense of the info, and answer the question.&lt;/p&gt; &lt;p&gt;* Fit in the &lt;strong&gt;&amp;lt;9B&lt;/strong&gt; range and run on a gpu with roughly &lt;strong&gt;8-10 gb vram&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'll also share what i've found so far:&lt;br /&gt; * I've found &lt;strong&gt;gemma3:12b-it-qat&lt;/strong&gt; as the best model, that fulfils my criteria well. But the problem it's not in my range, and i run out of memory issues. I'm pretty constrained here unfortunately.&lt;/p&gt; &lt;p&gt;* Reading lots of people speak highly of &lt;strong&gt;qwen3:4b-instruct-2507&lt;/strong&gt; here on reddit, i tried it, but didn't quite like it's ability to synthesise / stitch pieces of info together to answer. It's good at following instruction, and not making shit up generally but It would kinda expect a direct / callout . I tried lots of different prompts, but it was either the model refusing to answer, if it wasn't directly mentioned, or it would make shit up and use info from general knowledge, something that wasn't part of context. It was good instruction following.&lt;/p&gt; &lt;p&gt;* I also tried &lt;strong&gt;qwen3:8b&lt;/strong&gt; , it was good at stiching pieces of info together, but it would just make a lot of shit up instead of refusing to answer. fill in those missing gaps with either it's general knowledge or made up info.&lt;/p&gt; &lt;p&gt;* I also &lt;strong&gt;llama 3.2:8b&lt;/strong&gt; quantised, but it didn't follow instructions well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;qwen3:4b-instruct-2507-q4 for deciding whether to call the tool + rephrase the user queries. &lt;/p&gt; &lt;p&gt;gemma3:12b-it-qat for generating response (this is where i need recommendations)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I want ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you have developed a RAG solution with ollama models, please do write the model you found working well for your use case. I'm feel overwhelmed and kinda lost here, kinda feeling like an idiot, since i tried lots of models, and all of them seem to do some part of the job, not all. I know there are bigger models out there, that'd do the exact job pretty well, but i hope with all these developements, there must be some model in my range that would get the job done well.&lt;/p&gt; &lt;p&gt;It would be a huge help, to have your insights/recommendations if you've come across a similar problem. I'd highly welcome any comment, answer, suggestion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Big thanks&lt;/strong&gt; in advance ❤️.&lt;/p&gt; &lt;p&gt;Edit: If you dont know the answer, but would love to find out, please upvote so that it gets a better reach :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T22:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfk26q</id>
    <title>How can I make Gemma-3 4B better at generating a specific language?</title>
    <updated>2025-12-06T08:10:59+00:00</updated>
    <author>
      <name>/u/NoAdhesiveness7595</name>
      <uri>https://old.reddit.com/user/NoAdhesiveness7595</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m experimenting with the Gemma-3 4B model and I want it to be more fluent/accurate in a specific language (not English). What’s the best way to improve its output?&lt;br /&gt; Should I fine-tune it, use DPO, add prompts, or something else?&lt;br /&gt; Looking for practical steps, tools, or examples.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoAdhesiveness7595"&gt; /u/NoAdhesiveness7595 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk26q/how_can_i_make_gemma3_4b_better_at_generating_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk26q/how_can_i_make_gemma3_4b_better_at_generating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfk26q/how_can_i_make_gemma3_4b_better_at_generating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T08:10:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfjg34</id>
    <title>How is the agent system inside Cursor (or similar IDE agent workflows) actually designed?</title>
    <updated>2025-12-06T07:32:08+00:00</updated>
    <author>
      <name>/u/v0k3r</name>
      <uri>https://old.reddit.com/user/v0k3r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to understand how modern AI-powered IDEs like Cursor structure their internal agent systems.&lt;/p&gt; &lt;p&gt;From the outside, it looks like the tool is able to:&lt;br /&gt; – break a user request into multiple steps,&lt;br /&gt; – apply patches to the codebase,&lt;br /&gt; – run commands (install deps, start dev server),&lt;br /&gt; – detect errors,&lt;br /&gt; – and then automatically fix them in a loop.&lt;/p&gt; &lt;p&gt;is it?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a chain of multiple agents calling each other,&lt;/li&gt; &lt;li&gt;a single agent with tool-calling and a feedback loop,&lt;/li&gt; &lt;li&gt;or some kind of planner–executor architecture?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;How do they coordinate step-by-step tasks?&lt;br /&gt; Is there a public technical breakdown of how this “agentic IDE” architecture works?&lt;/p&gt; &lt;p&gt;I’d really appreciate a detailed explanation or any deep-dive resources.&lt;/p&gt; &lt;p&gt;Maybe links or explanation here&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/v0k3r"&gt; /u/v0k3r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjg34/how_is_the_agent_system_inside_cursor_or_similar/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T07:32:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pez5ch</id>
    <title>Why do LLM response formats often use &lt;| |&gt; (as in &lt;|message|&gt;) instead of &lt;message&gt;, and why do they use &lt;|end|&gt; instead of &lt;/message&gt;?</title>
    <updated>2025-12-05T16:14:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt; &lt;img alt="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" src="https://preview.redd.it/5e5ir2zlte5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b47f1bd3dabcdabf34fcf757aaea013f0a0c73" title="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I had to guess, I'd assume it's tokenization because &amp;quot;&amp;lt;|&amp;quot; is not a very commonly occurring pattern in pre-training, which allows devs to make &amp;quot;&amp;lt;|message|&amp;gt;&amp;quot; a single token.&lt;/p&gt; &lt;p&gt;That being said, the &amp;lt;|end|&amp;gt; is still a bit disorienting, at least to me reading as a human. You can see that the &amp;lt;|start|&amp;gt; block ends with another &amp;lt;|start|&amp;gt; block, but the &amp;lt;|message|&amp;gt; block ends in a &amp;lt;|end|&amp;gt; block.&lt;/p&gt; &lt;p&gt;This image is from &lt;a href="https://github.com/openai/harmony"&gt;openai's harmony response template&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5e5ir2zlte5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfpw7v</id>
    <title>Function calling Finetuners?</title>
    <updated>2025-12-06T13:56:14+00:00</updated>
    <author>
      <name>/u/zelkovamoon</name>
      <uri>https://old.reddit.com/user/zelkovamoon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Huggingface is full of finetunes, merges, etc; typically if you open a list of these for a given model - Qwen3, GPT-OSS, etc; you'll get a bunch of random models with a bunch of random names, it's not very searchable. I'm looking for finetunes / LoRas for tool calling / function performance improvement, and it just seems hard to find anything that unambiguously is trained for this and provides any sort of data about how much better it does.&lt;/p&gt; &lt;p&gt;I'm going to keep scrolling and eyeballing, but that *DOES* suck. So, I'm also going to ask the community - are there known good providers of tool / function calling LoRas? Finetunes? Who? ToolMaster69? Give names and specifics if you have them, please.&lt;/p&gt; &lt;p&gt;P.S. Dont tell me to train my own, that's not the question.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zelkovamoon"&gt; /u/zelkovamoon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpw7v/function_calling_finetuners/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpw7v/function_calling_finetuners/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpw7v/function_calling_finetuners/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfe5ou</id>
    <title>Open Unified TTS - Turn any TTS into an unlimited-length audio generator</title>
    <updated>2025-12-06T02:38:57+00:00</updated>
    <author>
      <name>/u/SouthernFriedAthiest</name>
      <uri>https://old.reddit.com/user/SouthernFriedAthiest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built an open-source TTS proxy that lets you generate unlimited-length audio from local backends without hitting their length limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Most local TTS models break after 50-100 words. Voice clones are especially bad - send a paragraph and you get gibberish, cutoffs, or errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The solution:&lt;/strong&gt; Smart chunking + crossfade stitching. Text splits at natural sentence boundaries, each chunk generates within model limits, then seamlessly joins with 50ms crossfades. No audible seams.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demos:&lt;/strong&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/intro.mp4"&gt;30-second intro&lt;/a&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/live_demo.mp4"&gt;4-minute live demo&lt;/a&gt; showing it in action&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; - OpenAI TTS-compatible API (drop-in for OpenWebUI, SillyTavern, etc.) - Per-voice backend routing (send &amp;quot;morgan&amp;quot; to VoxCPM, &amp;quot;narrator&amp;quot; to Kokoro) - Works with any TTS that has an API endpoint&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tested with:&lt;/strong&gt; Kokoro, VibeVoice, OpenAudio S1-mini, FishTTS, VoxCPM, MiniMax TTS, Chatterbox, Higgs Audio, Kyutai/Moshi&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/loserbcc/open-unified-tts"&gt;https://github.com/loserbcc/open-unified-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed with Claude and Z.ai (with me in the passenger seat).&lt;/p&gt; &lt;p&gt;Feedback welcome - what backends should I add adapters for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouthernFriedAthiest"&gt; /u/SouthernFriedAthiest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T02:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfpwum</id>
    <title>What can I run on my PC?</title>
    <updated>2025-12-06T13:57:02+00:00</updated>
    <author>
      <name>/u/Motor_Armadillo_7317</name>
      <uri>https://old.reddit.com/user/Motor_Armadillo_7317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;16g ddr4 , GTX 1660 ti, Ryzen 5500 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Armadillo_7317"&gt; /u/Motor_Armadillo_7317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpwum/what_can_i_run_on_my_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpwum/what_can_i_run_on_my_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfpwum/what_can_i_run_on_my_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T13:57:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexnfp</id>
    <title>LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering</title>
    <updated>2025-12-05T15:15:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt; &lt;img alt="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" src="https://external-preview.redd.it/wKVXYkAgQd2YCzTWH9wJHT9a9O4yMSOT8w5RQDj-cGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=266d079c79f26252dc4def3cc7e476d0209bb0af" title="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfibu4</id>
    <title>What agentic capabilities are you guys using llms for?</title>
    <updated>2025-12-06T06:23:46+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just curious&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfibu4/what_agentic_capabilities_are_you_guys_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pes3pu</id>
    <title>Basketball AI with RF-DETR, SAM2, and SmolVLM2</title>
    <updated>2025-12-05T10:53:12+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt; &lt;img alt="Basketball AI with RF-DETR, SAM2, and SmolVLM2" src="https://external-preview.redd.it/N2czYjlxanU4ZDVnMZ78lEX-DYraHupkrsvdafpxwsSm-SfqaN6z7l9OZr1B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aab29ff74cd044468cb8bd288eeaf647b5329d32" title="Basketball AI with RF-DETR, SAM2, and SmolVLM2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;resources: &lt;a href="https://www.youtube.com/watch?v=yGQb9KkvQ1Q"&gt;youtube&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;code&lt;/a&gt;, &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- player and number detection with RF-DETR&lt;/p&gt; &lt;p&gt;- player tracking with SAM2&lt;/p&gt; &lt;p&gt;- team clustering with SigLIP, UMAP and K-Means&lt;/p&gt; &lt;p&gt;- number recognition with SmolVLM2&lt;/p&gt; &lt;p&gt;- perspective conversion with homography&lt;/p&gt; &lt;p&gt;- player trajectory correction&lt;/p&gt; &lt;p&gt;- shot detection and classification&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k6kmogju8d5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T10:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfjvry</id>
    <title>Why so few benchmarks with the pcie p2p patches kernel module?</title>
    <updated>2025-12-06T07:59:49+00:00</updated>
    <author>
      <name>/u/unfortunate_jargon</name>
      <uri>https://old.reddit.com/user/unfortunate_jargon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of inference benchmarks on here, but I'm consistently baffled why it seems that nearly no one is using the various patched Nvidia kernel modules available which enabled pcie p2p.&lt;/p&gt; &lt;p&gt;It reduces the latency between RTX 30/40/50 cards by an order of magnitude, and makes tensor and export parallelism highly viable (leading to _drastically_ improved throughput)&lt;/p&gt; &lt;p&gt;Is this common knowledge around here? If not, then I highly encourage doing some testing with your multi-RTX GPU systems, because running without it is handicapping your performance by multiples.&lt;/p&gt; &lt;p&gt;edit: tinycorp was the first author I'm aware of that released a patch that was widely circulated, but others have forked and improved it, as well as rebasing against newer versions of the kernel module. here's an example I just pulled from chatgpt: &lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/unfortunate_jargon"&gt; /u/unfortunate_jargon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfjvry/why_so_few_benchmarks_with_the_pcie_p2p_patches/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T07:59:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfqmqu</id>
    <title>Trying to ship local RAG to both android and iOS and feeling disheartened</title>
    <updated>2025-12-06T14:30:00+00:00</updated>
    <author>
      <name>/u/chreezus</name>
      <uri>https://old.reddit.com/user/chreezus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a fullstack developer by experience, so forgive me if this is obvious. I've built a number of RAG applications for different industries (finance, government, etc). I recently got into trying to run these same RAG apps fully on-device (government agencies love privacy). I've been playing with Llama-3.2-3B with 4-bit quantization. I was able to get this running on IOS with CoreML after a ton of work (again, I'm not an AI or ML expert). Now I’m looking at Android and it feels pretty daunting: different hardware, multiple ABIs, different runtimes (TFLite / ExecuTorch / llama.cpp builds), and I’m worried I’ll end up with a totally separate pipeline just to get comparable behavior.&lt;/p&gt; &lt;p&gt;For folks who’ve shipped cross-platform on-device RAG:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Is there a sane way to target both iOS and Android without maintaining two totally separate build pipelines?&lt;/li&gt; &lt;li&gt;What are you using for the local vector database that works well on mobile? (SQLite-vec? Chroma? Custom C++?)&lt;/li&gt; &lt;li&gt;How do you handle updates to the source data. At some regular interval, I would need to rebuild the embeddings and ship them to device, essentially &amp;quot;deployments&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chreezus"&gt; /u/chreezus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqmqu/trying_to_ship_local_rag_to_both_android_and_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqmqu/trying_to_ship_local_rag_to_both_android_and_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqmqu/trying_to_ship_local_rag_to_both_android_and_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfrqvh</id>
    <title>Multi-directional ablation with self-organizing maps - anyone tried it yet?</title>
    <updated>2025-12-06T15:20:28+00:00</updated>
    <author>
      <name>/u/IllllIIlIllIllllIIIl</name>
      <uri>https://old.reddit.com/user/IllllIIlIllIllllIIIl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran across this preprint the other day: &lt;/p&gt; &lt;p&gt;Piras, Giorgio, et al. &amp;quot;&lt;a href="https://arxiv.org/abs/2511.08379"&gt;SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models.&lt;/a&gt;&amp;quot; arXiv preprint arXiv:2511.08379 (2025).&lt;/p&gt; &lt;p&gt;They have published their code here: &lt;a href="https://github.com/pralab/som-refusal-directions"&gt;https://github.com/pralab/som-refusal-directions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Basically rather than the usual difference of means method for ablating a single refusal direction, they train a SOM to learn a refusal manifold and use Bayesian Optimization to determine the best subset of k directions to ablate. They got some pretty impressive results. &lt;/p&gt; &lt;p&gt;They only implemented the method for a handful of smaller models (nothing bigger than 14B), probably because the BO step is rather expensive. But it shouldn't be that hard to extend their code to support new models. &lt;/p&gt; &lt;p&gt;I was able to run the full pipeline on Qwen2.5-3B and replicate the results on that. I started extending the code to support gpt-oss-20b, but the further I got, the more I realized I'm too GPU poor to succeed in running it on that. &lt;/p&gt; &lt;p&gt;Any of you GPU rich bastards try this out on a larger model yet, or want to give it a shot?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllllIIlIllIllllIIIl"&gt; /u/IllllIIlIllIllllIIIl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfrqvh/multidirectional_ablation_with_selforganizing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T15:20:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfq0kd</id>
    <title>PaperDebugger: the Best Overleaf Companion!</title>
    <updated>2025-12-06T14:01:42+00:00</updated>
    <author>
      <name>/u/NuoJohnChen</name>
      <uri>https://old.reddit.com/user/NuoJohnChen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt; &lt;img alt="PaperDebugger: the Best Overleaf Companion!" src="https://b.thumbs.redditmedia.com/LSzFW-bVRkmLrP-afZdLmy0DNmjvCz1MK2UnMO8aqLo.jpg" title="PaperDebugger: the Best Overleaf Companion!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chrome/APP Store: &lt;a href="https://www.paperdebugger.com/"&gt;https://www.paperdebugger.com/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.02589"&gt;https://arxiv.org/abs/2512.02589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/PaperDebugger/PaperDebugger"&gt;https://github.com/PaperDebugger/PaperDebugger&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enhancer: &lt;a href="https://huggingface.co/Xtra-Computing/XtraGPT-7B"&gt;https://huggingface.co/Xtra-Computing/XtraGPT-7B&lt;/a&gt; &lt;/p&gt; &lt;p&gt;An NUS team just released &amp;quot;PaperDebugger&amp;quot;: an in-editor system that uses multiple agents (Reviewer, Researcher, Scorer) to rewrite and critique papers in real-time within Overleaf. Just simply select a rough section, and it launches the full pipeline. &lt;/p&gt; &lt;p&gt;Direct Integration: No copy-pasting. It patches the document with Git-style before/after diffs.&lt;/p&gt; &lt;p&gt;Deep Research: Can pull arXiv papers, summarize them, and generate comparison tables inline.&lt;/p&gt; &lt;p&gt;Tech Stack: Uses an MCP toolchain and Kubernetes to scale the agent reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NuoJohnChen"&gt; /u/NuoJohnChen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pfq0kd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfq0kd/paperdebugger_the_best_overleaf_companion/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:01:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfcatm</id>
    <title>VoxCPM 1.5B just got released!</title>
    <updated>2025-12-06T01:07:54+00:00</updated>
    <author>
      <name>/u/Hefty_Wolverine_553</name>
      <uri>https://old.reddit.com/user/Hefty_Wolverine_553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt; &lt;img alt="VoxCPM 1.5B just got released!" src="https://external-preview.redd.it/MIb2iimHkfYqVDgmZztu-h5tz8yFqiAztGcy6umK7o8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d7fc02333ca119537bfd3af70a4c74b40c2e98" title="VoxCPM 1.5B just got released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just visiting the &lt;a href="https://github.com/OpenBMB/VoxCPM"&gt;GitHub page&lt;/a&gt; today (setting up a FastAPI TTS server) when I realized that they released a new version of the VoxCPM model. The original VoxCPM-0.5B was already very good in my testing, but this model looks like a straight improvement (it's still a 0.5B model, despite the rather confusing naming scheme).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;VoxCPM&lt;/th&gt; &lt;th align="left"&gt;VoxCPM1.5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Audio VAE Sampling Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16kHz&lt;/td&gt; &lt;td align="left"&gt;44.1kHz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LM Token Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12.5Hz&lt;/td&gt; &lt;td align="left"&gt;6.25Hz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Patch Size&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SFT Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LoRA Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;td align="left"&gt;✅&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They also added fine-tuning support as well as a guide &lt;a href="https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md"&gt;https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example output: &lt;a href="https://voca.ro/147qPjN98F6g"&gt;https://voca.ro/147qPjN98F6g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Wolverine_553"&gt; /u/Hefty_Wolverine_553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfbo6o</id>
    <title>Is there any model truly open, that you can train yourself from zero?</title>
    <updated>2025-12-06T00:38:47+00:00</updated>
    <author>
      <name>/u/puthre</name>
      <uri>https://old.reddit.com/user/puthre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, is there any open source LLM that comes with all the data it was trained on and all the instructions that you can replicate yourself assuming you have access to the necesary hardware? And if not why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puthre"&gt; /u/puthre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T00:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfqm0y</id>
    <title>Speed of DeepSeek with RAM offload</title>
    <updated>2025-12-06T14:29:05+00:00</updated>
    <author>
      <name>/u/vhthc</name>
      <uri>https://old.reddit.com/user/vhthc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have 96GB VRAM. By far not enough to run DeepSeek 3.x - bit I could upgrade my RAM so I can have the active layers on the GPU and the rest in system RAM. Yeah the RAM prices are a catastrophe but I need to run such a large model, and I don’t want to use cloud - this is locallama!&lt;/p&gt; &lt;p&gt;Has anyone tried this? What speed can I expect with a 64kb context length in prompt processing and tokens per second?&lt;/p&gt; &lt;p&gt;It would be quite the investment so if anyone has real world data that would be great!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vhthc"&gt; /u/vhthc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfqm0y/speed_of_deepseek_with_ram_offload/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T14:29:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfl0d8</id>
    <title>How big an open source model can I run on 128 GB unified memory?</title>
    <updated>2025-12-06T09:13:03+00:00</updated>
    <author>
      <name>/u/nameless_me</name>
      <uri>https://old.reddit.com/user/nameless_me</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just took delivery of a Minisforum MS-S1 with AMD Ryzen Ai Max+ 395 cpu, 128 GB unified memory architecture and AMD Radeon 8060S Graphics. In the BIOS the UDMA memory for the iGPU is set to 96 GB. Running a Debian Linux terminal in WSL 2, I downloaded and ran ollama which works fine.&lt;/p&gt; &lt;p&gt;Trying a Deepseek-r1:70b model, it refused to load in ollama. I checked a few sources which ended saying this &amp;quot;&lt;strong&gt;DeepSeek-R1-70B INT4 GGUF still requires ~55–60 GB VRAM equivalent&lt;/strong&gt;. &lt;strong&gt;You cannot run this model on a single consumer APU&lt;/strong&gt;, even with “128 GB unified memory”.&lt;/p&gt; &lt;p&gt;Is the above true? What is the largest LLM model I can run reasonably on this computer?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nameless_me"&gt; /u/nameless_me &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfl0d8/how_big_an_open_source_model_can_i_run_on_128_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T09:13:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfiar0</id>
    <title>Qwen3-TTS</title>
    <updated>2025-12-06T06:21:51+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-TTS-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfiar0/qwen3tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T06:21:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model’s context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
