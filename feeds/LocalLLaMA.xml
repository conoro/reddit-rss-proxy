<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-26T13:36:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ogjnsz</id>
    <title>Should I keep my GeForce RTX 5060 Ti?</title>
    <updated>2025-10-26T12:42:35+00:00</updated>
    <author>
      <name>/u/Accomplished_Ad4103</name>
      <uri>https://old.reddit.com/user/Accomplished_Ad4103</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;For the past 9-12 months I been thinking in getting into local AI + learning CUDA programming. I never expected to run very large models as I am on a very thight budget (~ 600$), so I have been postponing it foever. Anyway, I am more interested in the CUDA programming part. My idea is to take it as a hobby and mostly get in touch witth the local AI tools and models...&lt;/p&gt; &lt;p&gt;The thing is, that if I want to get into this I must have a NVIDIA GPU. I saw a discount for a GeForce RTX 5060 Ti 16 GB and went for it, as it is around my budget. However, I've been wondering if I did OK or not. &lt;/p&gt; &lt;p&gt;My first limitation is that had to go in my current (old) system. For my job I need a large core count + large RAM amount, so currently I have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Xeon E5-2698-v4: 20C/40T 2.2 GHZ - 3.5 Ghz &lt;/li&gt; &lt;li&gt;192 GB of DDR4 2400 MHz&lt;/li&gt; &lt;li&gt;x2 PCIe x16 3.0 and x1 PCIe x8 3.0 slots&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Therefore, I went for 5060 Ti the tought that I benefit from the RAM and do offloading to it. However, all my components are &amp;quot;slow&amp;quot; compared to state-of-the-art machines, so I don't know if it is a good idea or not. &lt;/p&gt; &lt;p&gt;So far, I haven't had time to test it, but I tested it in gaming and the performance has not been amazing, but I guess I am facing a strong CPU bottleneck. Anyway, gaming is not my thing and I don't care about it, it was just an easy benchmark test to do.&lt;/p&gt; &lt;p&gt;I also didn't care about PCIe version, as for gaming does not appear to matter, but I have read that PCIe version bandwith is much more important for local AI, specially for RAM off-loading. Since the RTX 5060 Ti is only PCIe x8 and my PCie is 3.0 I am limited to 8GB/s (I think). Will this make everything very slow?&lt;/p&gt; &lt;p&gt;Does anybody know what can I expect from my system? I can handle the system being slow, as I am not in any hurry, this would be only a hobby. Are all my other components too old? &lt;/p&gt; &lt;p&gt;I have been thinking about returning my RTX 5060Ti (I am thinking also that Black Friday is very close) and going for somethign older, like x2 RTX3060Ti (to have more VRAM). Is this a good idea? &lt;/p&gt; &lt;p&gt;However, I am worried about driver support (for the 3060), going into the future. &lt;/p&gt; &lt;p&gt;For me, there's a lot of money at stake, so I would really appreacity any help.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR: Is RTX 5060 Ti 16B in PCIe 3.0 + 192 GB DDR4 2400 MHz good for learning local AI or will it be extermly slow? Would it be better to go for dual RTX 3060 Ti (more VRAM)?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Accomplished_Ad4103"&gt; /u/Accomplished_Ad4103 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjnsz/should_i_keep_my_geforce_rtx_5060_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjnsz/should_i_keep_my_geforce_rtx_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjnsz/should_i_keep_my_geforce_rtx_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:42:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogk05b</id>
    <title>All thinking MOE models behave like this? "wait, but, wait, but, also, wait, but". In some ways checking thinking enjoyable and fun. Is this an army of experts arguing with each other? I give corrections that now 2025 and the 5060 ti exists, it's the same as the 4060ti but 20-30% more powerful.</title>
    <updated>2025-10-26T12:59:44+00:00</updated>
    <author>
      <name>/u/R_dva</name>
      <uri>https://old.reddit.com/user/R_dva</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogk05b/all_thinking_moe_models_behave_like_this_wait_but/"&gt; &lt;img alt="All thinking MOE models behave like this? &amp;quot;wait, but, wait, but, also, wait, but&amp;quot;. In some ways checking thinking enjoyable and fun. Is this an army of experts arguing with each other? I give corrections that now 2025 and the 5060 ti exists, it's the same as the 4060ti but 20-30% more powerful." src="https://b.thumbs.redditmedia.com/pysbZXMtIku2zlg2XefJz7lJ22U6LwE62MMNut507IU.jpg" title="All thinking MOE models behave like this? &amp;quot;wait, but, wait, but, also, wait, but&amp;quot;. In some ways checking thinking enjoyable and fun. Is this an army of experts arguing with each other? I give corrections that now 2025 and the 5060 ti exists, it's the same as the 4060ti but 20-30% more powerful." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Wait, but in their message, they wrote &amp;quot;5060ti&amp;quot; with lowercase 't' and no space? Maybe it's a typo for &amp;quot;4060ti&amp;quot;. So the correct term is RTX 4060 Ti, not 5060. Let me clarify that.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Also, they mentioned &amp;quot;16 gb&amp;quot; but the 4060 Ti typically has 8GB or 12GB of VRAM. Wait, maybe they're referring to a different variant with more memory? But standard 4060 Ti is 8GB (or sometimes 12GB), so maybe their GPU has 16GB? But that's not typical for 4060 Ti.&amp;quot;&lt;/p&gt; &lt;p&gt;&amp;quot;Wait, no: the RTX 4060 Ti usually comes in versions with 8GB or 12GB VRAM. The 16GB would be a different model like the 4070 Ti (which has 16GB), but not 4060 Ti. So maybe they're confusing it with another GPU.&lt;/p&gt; &lt;p&gt;&amp;quot;But regardless, I should clarify that &amp;quot;5060ti&amp;quot; is incorrect and likely a typo for &amp;quot;4060ti&amp;quot;. So in my response, I need to correct that by stating the actual GPU model they have is RTX 4060 Ti (or similar), not 5060.&amp;quot;&lt;/p&gt; &lt;p&gt;The temperature setting is not touched, the number of experts is also. I only adjust settings for improve performance. After think about 5060, he start check all other question and finished with repeating massage from 3 screenshot and it took 43 minutes and he didnt give me the answer. For other questions from time to time he just thinking, but also didn't answer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R_dva"&gt; /u/R_dva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ogk05b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogk05b/all_thinking_moe_models_behave_like_this_wait_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogk05b/all_thinking_moe_models_behave_like_this_wait_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:59:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogkjzs</id>
    <title>What AI voice / TTS model is used in these YouTube videos?</title>
    <updated>2025-10-26T13:25:16+00:00</updated>
    <author>
      <name>/u/Evening-Wolverine997</name>
      <uri>https://old.reddit.com/user/Evening-Wolverine997</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I came across these two YouTube videos and was wondering if anyone recognizes the AI voice or text-to-speech model being used in them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=yXbda83VERk"&gt;https://www.youtube.com/watch?v=yXbda83VERk&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=o-L-8AYMD2w"&gt;https://www.youtube.com/watch?v=o-L-8AYMD2w&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evening-Wolverine997"&gt; /u/Evening-Wolverine997 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogkjzs/what_ai_voice_tts_model_is_used_in_these_youtube/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogkjzs/what_ai_voice_tts_model_is_used_in_these_youtube/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogkjzs/what_ai_voice_tts_model_is_used_in_these_youtube/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T13:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofrbcy</id>
    <title>VSORA Launches Europe’s Most Powerful AI Inference Chip</title>
    <updated>2025-10-25T13:17:00+00:00</updated>
    <author>
      <name>/u/RG54415</name>
      <uri>https://old.reddit.com/user/RG54415</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"&gt; &lt;img alt="VSORA Launches Europe’s Most Powerful AI Inference Chip" src="https://external-preview.redd.it/yZMJwzqfo-1KodiPMX_oqs-WZlqWdJqQO_vA61_E2Yg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9bde9afeea2ecc09d17d3b9a496407c3e11f9c3" title="VSORA Launches Europe’s Most Powerful AI Inference Chip" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of its features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fully programmable&lt;/li&gt; &lt;li&gt;Algorithm agnostic&lt;/li&gt; &lt;li&gt;Host processor agnostic&lt;/li&gt; &lt;li&gt;RISC-V cores to offload host &amp;amp; run AI completely on-chip&lt;/li&gt; &lt;li&gt;Tensorcore (dense) &lt;ul&gt; &lt;li&gt;fp8: 3200 Tflops &lt;/li&gt; &lt;li&gt;fp16: 800 Tflops&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;General Purpose &lt;ul&gt; &lt;li&gt;fp8/int8: 100 Tflops&lt;/li&gt; &lt;li&gt;fp16/int16: 50 Tflops&lt;/li&gt; &lt;li&gt;fp32/int32: 25 Tflops&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Capacity HBM: 288GB&lt;/li&gt; &lt;li&gt;Throughput HBM: 8 TB/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Seems like a big win for local AI models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RG54415"&gt; /u/RG54415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://finance.yahoo.com/news/vsora-launches-europe-most-powerful-121700744.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofrbcy/vsora_launches_europes_most_powerful_ai_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T13:17:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1og6dis</id>
    <title>Qwen coder local is fabulous. Just a momentary lapse - we get on really well. I told it to take five and get a Monster or something.</title>
    <updated>2025-10-25T23:55:02+00:00</updated>
    <author>
      <name>/u/cromagnone</name>
      <uri>https://old.reddit.com/user/cromagnone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og6dis/qwen_coder_local_is_fabulous_just_a_momentary/"&gt; &lt;img alt="Qwen coder local is fabulous. Just a momentary lapse - we get on really well. I told it to take five and get a Monster or something." src="https://preview.redd.it/o0yzkkgoicxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=057b33b32462e1eddacc884320a5bf7761f5c984" title="Qwen coder local is fabulous. Just a momentary lapse - we get on really well. I told it to take five and get a Monster or something." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cromagnone"&gt; /u/cromagnone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/o0yzkkgoicxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og6dis/qwen_coder_local_is_fabulous_just_a_momentary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og6dis/qwen_coder_local_is_fabulous_just_a_momentary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T23:55:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogev2y</id>
    <title>Hey everyone! Positive update: I've successfully fine-tuned my model! I also have something to ask you all.</title>
    <updated>2025-10-26T07:56:23+00:00</updated>
    <author>
      <name>/u/Patience2277</name>
      <uri>https://old.reddit.com/user/Patience2277</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I successfully completed the first fine-tuning on my model! (It's a big model, so there were a lot of trials and errors, lol.)&lt;/p&gt; &lt;p&gt;I'm moving on to the second phase of tuning, which will include &lt;strong&gt;multi-turn dialogue&lt;/strong&gt;, &lt;strong&gt;persona&lt;/strong&gt;, a bit of &lt;strong&gt;technical Q&amp;amp;A&lt;/strong&gt;, and &lt;strong&gt;self-talk/monologues&lt;/strong&gt;! (The initial beta test was successful with the first phase—the base performance wasn't bad even before training!)&lt;/p&gt; &lt;p&gt;I set the learning rate and epochs aggressively to try and overwrite the core identity baked into the original layers, and now it seems like the &lt;strong&gt;model's general language ability has degraded&lt;/strong&gt; a bit.&lt;/p&gt; &lt;p&gt;So, I'm reaching out to ask for your help!&lt;/p&gt; &lt;p&gt;Please contact me on my Discord ID!&lt;br /&gt; 't_ricus'&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Conditions?&lt;/strong&gt; Um, nothing specific! I just need &lt;strong&gt;beta testers&lt;/strong&gt; and a &lt;strong&gt;little bit of Korean knowledge&lt;/strong&gt;? I'm Korean, haha.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Patience2277"&gt; /u/Patience2277 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogev2y/hey_everyone_positive_update_ive_successfully/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogev2y/hey_everyone_positive_update_ive_successfully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogev2y/hey_everyone_positive_update_ive_successfully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T07:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1og54b4</id>
    <title>Qwen3-VL-32B at text tasks - some thoughts after using yairpatch's fork and GGUF's</title>
    <updated>2025-10-25T22:55:30+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Setup&lt;/h2&gt; &lt;p&gt;Using &lt;a href="https://github.com/yairpatch/llama.cpp"&gt;YairPatch's&lt;/a&gt; fork and the &lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF/tree/main"&gt;Q5 GGUF&lt;/a&gt; from YairPatch's huggingface uploads.&lt;/p&gt; &lt;p&gt;Used a Lambda Labs gh200 instance, but I wasn't really testing for speed so that's less important aside from the fact that llama cpp was built with &lt;code&gt;-DLLAMA_CUDA on&lt;/code&gt; .&lt;/p&gt; &lt;h2&gt;Text Tests&lt;/h2&gt; &lt;p&gt;I did not test the vision functionality as I'm sure we'll be flooded with those in the coming weeks. I am more excited that this is the first dense-32B update/checkpoint we've had since Qwen3 first released.&lt;/p&gt; &lt;p&gt;Tests included a few one-shot coding tasks. A few multi-step (agentic) coding tasks. Some basic chatting and trivia.&lt;/p&gt; &lt;h2&gt;Vibes/Findings&lt;/h2&gt; &lt;p&gt;It's good, but as expected the benchmarks that approached Sonnet level are just silly. It's definitely smarter than the latest 30B-A3B models, but at the same time a worse coder than Qwen3-30b-flash-coder. It produces more 'correct' results but either takes uglier approaches or cuts corners in the design department (if the task is something visual) compared to Flash Coder. Still, its intelligence usually meant that it will always be the first to a working result. &lt;strong&gt;Its ability to design&lt;/strong&gt; - I am not kidding, is terrible. It seems to always succeed in the logic department compared to Qwen3-30b-flash-coder, but man no matter what settings or prompts I use, if it's a website, threejs game, pygame, or just ascii art.. VL-32B has zero visual flair to it.&lt;/p&gt; &lt;p&gt;Also, the recommended settings on Qwen's page for VL-32B in text mode are madness. It produces bad results or doesn't adhere to system prompts. I had a better time when I dropped the temperature down to 0.2-0.3 for coding and like 0.5 for everything else.&lt;/p&gt; &lt;p&gt;It's pretty smart and has good knowledge depth for a 32B model. Probably approaching Nemotron Super 49B in just raw trivia that I ask it.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;For a lot of folks this will be the new &lt;em&gt;&amp;quot;best model I can fit entirely in VRAM&amp;quot;&lt;/em&gt;. It's stronger than the top MoE's of similar sizing, but not strong enough that everyone will be willing to make the speed tradeoff. &lt;strong&gt;Also - none of this has been peer-reviewed and there are likely changes to come,&lt;/strong&gt; consider this a preview-review.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og54b4/qwen3vl32b_at_text_tasks_some_thoughts_after/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og54b4/qwen3vl32b_at_text_tasks_some_thoughts_after/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og54b4/qwen3vl32b_at_text_tasks_some_thoughts_after/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T22:55:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogatwe</id>
    <title>Behavior of agentic coding at the local level?</title>
    <updated>2025-10-26T03:47:20+00:00</updated>
    <author>
      <name>/u/SocietyTomorrow</name>
      <uri>https://old.reddit.com/user/SocietyTomorrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using my local Ollama instance with Continue in VSCode for a while as a second-opinion tool, and have wondered about some of the commercial code tools and how they differ. I've come to really appreciate Claude Code's workflow, to-do list management, and overall effectiveness. I've seen tools for connecting it to openrouter so it can use the models there as an endpoint provider, but I haven't found a way to use any local providers to do the same. I've got GPUs for days available to me for running GLM but wish I could get the kind of result I get from Claude Code CLI. If anyone knows of ways to do that I would appreciate it, or other agentic tools for local LLMs that function in a similar way I can try out that'd be awesome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SocietyTomorrow"&gt; /u/SocietyTomorrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogatwe/behavior_of_agentic_coding_at_the_local_level/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogatwe/behavior_of_agentic_coding_at_the_local_level/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogatwe/behavior_of_agentic_coding_at_the_local_level/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1og4j8x</id>
    <title>OpenArc 2.0: NPU, Multi-GPU Pipeline Parallell, CPU Tensor Parallell, kokoro, whisper, streaming tool use, openvino llama-bench and more. Apache 2.0</title>
    <updated>2025-10-25T22:28:34+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;Today I'm happy to announce &lt;a href="https://github.com/SearchSavior/OpenArc/tree/2.0"&gt;OpenArc 2.0&lt;/a&gt; is finally done!! 2.0 brings a full rewrite to support NPU, pipeline parallel for multi GPU, tensor parallel for dual socket CPU, tool use for LLM/VLM, and an &lt;strong&gt;OpenVINO version of llama-bench&lt;/strong&gt; and much more.&lt;/p&gt; &lt;p&gt;In the next few days I will post some benchmarks with A770 and CPU for models in the README. &lt;/p&gt; &lt;p&gt;Someone already shared &lt;a href="https://github.com/SearchSavior/OpenArc/issues/38#issuecomment-3446729812"&gt;NPU results&lt;/a&gt; for Qwen3-8B-int4.&lt;/p&gt; &lt;p&gt;2.0 solves every problem 1.0.5 had and more, garnering support from the community in two PRs which implement /v1/embeddings and /v1/rerank. Wow! For my first open source project, this change of pace has been exciting.&lt;/p&gt; &lt;p&gt;Anyway, I hope OpenArc ends up being useful to everyone :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og4j8x/openarc_20_npu_multigpu_pipeline_parallell_cpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og4j8x/openarc_20_npu_multigpu_pipeline_parallell_cpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og4j8x/openarc_20_npu_multigpu_pipeline_parallell_cpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T22:28:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofxt6s</id>
    <title>Optimizing gpt-oss-120B on AMD RX 6900 XT 16GB: Achieving 19 tokens/sec</title>
    <updated>2025-10-25T17:46:16+00:00</updated>
    <author>
      <name>/u/Bright_Resolution_61</name>
      <uri>https://old.reddit.com/user/Bright_Resolution_61</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;## Introduction OpenAI's gpt-oss-120B is a massive 117B parameter language model, with official recommendations calling for datacenter-grade GPUs like the H100 or MI300X (80GB VRAM). This article documents the optimization journey to run this model at practical speeds (19 tokens/sec) on a consumer AMD RX 6900 XT with only 16GB VRAM. ## Hardware Configuration ### Main Components - **GPU**: AMD Radeon RX 6900 XT 16GB VRAM - Architecture: RDNA2 (gfx1030) - Memory Bandwidth: 512 GB/s - Stream Processors: 5120 - Released: December 2020 - **CPU**: AMD Ryzen 9 7900 (12-core/24-thread) - Base Clock: 3.7 GHz - Boost Clock: 5.4 GHz - Instruction Sets: AVX, AVX2, AVX-512 capable - L3 Cache: 64MB - Architecture: Zen 4 - **Memory**: 64GB (32GB × 2) DDR5-5600MHz - Dual-channel configuration - Memory Bandwidth: 89.6 GB/s (theoretical) - CAS Latency: CL46 (typical) - **Storage**: NVMe SSD recommended (60GB model files) ### Software Environment - **OS**: Ubuntu 24.04 LTS - **ROCm**: 6.2.4 - **llama.cpp**: Latest build (ROCm backend, AVX-512 enabled) - **Drivers**: Mesa 24.x + AMDGPU kernel driver ## Why This Hardware Configuration Matters ### Ryzen 9 7900's Advantages The 12-core/24-thread design with AVX-512 support accelerates MoE layer CPU processing significantly. AVX-512 in particular provides 15-30% performance gains for matrix operations in the CPU processing path, making it ideal for handling the 28 MoE layers offloaded from GPU. ### DDR5-5600MHz Impact The gpt-oss-120B's MoE architecture processes 28 layers on CPU/RAM. DDR5's high bandwidth (89.6 GB/s) enables rapid transfer of model weight data, reducing memory bottlenecks. This is approximately 40% faster than DDR4-3200, directly improving token generation speed. ### 64GB RAM Necessity - Model weights (MoE portion): ~50-55GB - System usage: 6-8GB - KV cache: 2-4GB - **Total**: ~58-67GB 64GB is the minimum viable configuration. For longer contexts (32K+), 128GB is recommended. System was observed using only 6GB with 57GB available, but full context windows consume more. ## Initial Challenge: The Crash Wall The first attempt with default settings resulted in immediate crashes with `ggml_cuda_error` termination. ```bash # Initial attempt (failed) ./llama-server -m gpt-oss-120b.gguf --n-gpu-layers 999 # → Aborted (core dumped) ``` With only 16GB VRAM against a 120B model, this seemed impossible. However, gpt-oss-120B uses a Mixture of Experts (MoE) architecture, activating only 5.1B parameters per token. This characteristic became the key to success. ## Breakthrough 1: Environment Variables and MoE Offloading Running RX 6900 XT with ROCm requires specific environment variables: ```bash export HSA_OVERRIDE_GFX_VERSION=10.3.0 export ROCM_PATH=/opt/rocm export HIP_VISIBLE_DEVICES=0 export GPU_MAX_HEAP_SIZE=100 export GPU_MAX_ALLOC_PERCENT=95 ``` The `HSA_OVERRIDE_GFX_VERSION=10.3.0` is critical for gfx1030 (RX 6900 XT) architecture recognition. The breakthrough came with the `--n-cpu-moe` parameter, which offloads MoE layers to CPU: ```bash ./llama-server \ -m gpt-oss-120b.gguf \ --n-gpu-layers 5 \ --n-cpu-moe 36 \ --ctx-size 4096 ``` **Result**: First successful boot, but slow at **11.63 tokens/sec**. ## Breakthrough 2: Progressive GPU Layer Increase Monitoring VRAM usage with `rocm-smi`, I progressively increased GPU layers: | GPU Layers | MoE Layers (CPU) | Speed | VRAM Usage | |------------|------------------|-------|------------| | 5 layers | 36 layers | 11.6 t/s | 52% | | 20 layers | 32 layers | 15.2 t/s | 70% | | 30 layers | 29 layers | 17.8 t/s | 85% | | 38 layers | 28 layers | **19.1 t/s** | 95% | | 40 layers | 28 layers | 19.4 t/s | **99%** | | 42 layers | 27 layers | OOM | - | 38 layers proved to be the optimal balance. While 40 layers works, increasing context length causes KV cache to overflow VRAM. ## Breakthrough 3: Enabling AVX-512 The initial build had **all CPU AVX instructions disabled**: ```bash # Check configuration cat CMakeCache.txt | grep GGML_AVX # GGML_AVX:BOOL=OFF ← Problem! ``` This meant using only 10-30% of CPU capabilities. Rebuilding fixed this: ```bash cd llama.cpp rm -rf build &amp;amp;&amp;amp; mkdir build &amp;amp;&amp;amp; cd build cmake .. \ -DGGML_HIP=ON \ -DCMAKE_BUILD_TYPE=Release \ -DGGML_NATIVE=ON # ← Auto-detect optimizations cmake --build . --config Release -j$(nproc) ``` **Result**: AVX, AVX2, and AVX512 all enabled, significantly accelerating MoE layer CPU processing. ## Final Configuration The stable configuration: ```bash export HSA_OVERRIDE_GFX_VERSION=10.3.0 export ROCM_PATH=/opt/rocm export HIP_VISIBLE_DEVICES=0 export GPU_MAX_HEAP_SIZE=100 export GPU_MAX_ALLOC_PERCENT=95 ./llama-server \ -m gpt-oss-120b-mxfp4-00001-of-00003.gguf \ --n-gpu-layers 38 \ --n-cpu-moe 28 \ --ctx-size 24576 \ --batch-size 2048 \ --ubatch-size 512 \ --threads 12 \ --jinja \ --host 0.0.0.0 \ --port 8080 ``` ### Parameter Explanation - `--n-gpu-layers 38`: GPU processing layers (95% VRAM utilization) - `--n-cpu-moe 28`: Number of MoE layers processed on CPU - `--ctx-size 24576`: Context length (24K tokens) - `--batch-size 2048`: Batch size (processing efficiency) - `--threads 12`: Physical core count (12 cores) ## Performance Results ``` Prompt processing: 93-291 tokens/sec (with caching) Generation speed: 19.14 tokens/sec VRAM usage: 95% CPU usage: 47% ``` ## llama.cpp vs Ollama I used llama.cpp, but the differences with Ollama are clear: **llama.cpp**: - ✅ Fine-grained tuning possible - ✅ Extract maximum hardware performance - ❌ Complex configuration **Ollama**: - ✅ One-command startup - ✅ Beginner-friendly - ❌ Auto-settings achieve ~80% performance (10-12 t/s estimated) For specialized environments like AMD, llama.cpp's flexibility was essential. ## Troubleshooting ### Flash Attention Errors ```bash # Solution: Disable Flash Attention Remove --flash-attn parameter ``` ### OOM (Out of Memory) ```bash # Solution: Reduce GPU layers by 1-2 --n-gpu-layers 38 → 36 ``` ### Extremely Slow Performance ```bash # Check AVX instructions cat build/CMakeCache.txt | grep GGML_AVX # If all OFF, rebuild with optimizations ``` ## Key Learnings ### 1. AMD ROCm Challenges - Requires manual environment variable configuration - gfx architecture overrides necessary - Flash Attention often unstable - Less mature than CUDA ecosystem ### 2. MoE Architecture Advantages - 120B model activates only 5.1B parameters - Enables running on consumer hardware - CPU offloading is practical and effective ### 3. Progressive Optimization Works - Start conservative (low GPU layers) - Monitor VRAM with rocm-smi - Increment gradually - Find stability threshold ### 4. CPU Optimization Matters - AVX-512 provides 15-30% speedup for MoE - Physical core count optimal for threading - Memory bandwidth becomes bottleneck ## Theoretical Limits Reached At 19 tokens/sec with 95% VRAM usage, we've essentially hit the hardware ceiling. Further improvements would require: 1. **More VRAM**: Reduce MoE CPU offloading 2. **Faster Memory**: DDR5 (up to 6400MHz) 3. **Better GPU**: RDNA3 (RX 7900 series) or NVIDIA ## Conclusions Successfully running gpt-oss-120B at 19 t/s on AMD RX 6900 XT 16GB demonstrates that: 1. **Cost-Effectiveness**: $300-400 used GPU runs 120B models practically 2. **Learning Value**: Deep understanding of GPU architecture and memory management 3. **Practicality**: 19 t/s suffices for code completion and chat applications The greatest lesson: **Understand hardware limits and optimize progressively**. Perfect configuration doesn't appear instantly. Using monitoring tools (rocm-smi, htop) while adjusting parameters one-by-one requires patience. The fine‑tuning of this article was performed using gpt‑oss-120B. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bright_Resolution_61"&gt; /u/Bright_Resolution_61 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofxt6s/optimizing_gptoss120b_on_amd_rx_6900_xt_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofxt6s/optimizing_gptoss120b_on_amd_rx_6900_xt_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofxt6s/optimizing_gptoss120b_on_amd_rx_6900_xt_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T17:46:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1og8jil</id>
    <title>My LLM-powered text adventure needed a dynamic soundtrack, so I'm training a MIDI generation model to compose it on the fly. Here's a video of its progress so far.</title>
    <updated>2025-10-26T01:45:03+00:00</updated>
    <author>
      <name>/u/orblabs</name>
      <uri>https://old.reddit.com/user/orblabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og8jil/my_llmpowered_text_adventure_needed_a_dynamic/"&gt; &lt;img alt="My LLM-powered text adventure needed a dynamic soundtrack, so I'm training a MIDI generation model to compose it on the fly. Here's a video of its progress so far." src="https://external-preview.redd.it/dXp6ZXRxaTQxZHhmMSaFlwyBbCxWLebwmipVQ7r7-6zFGQDSVaPWFx0zsp7A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c97f76e0095051f80ca25d22b1c21f41a02c24e2" title="My LLM-powered text adventure needed a dynamic soundtrack, so I'm training a MIDI generation model to compose it on the fly. Here's a video of its progress so far." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a component of a larger project I'm working on called &lt;strong&gt;Synthasia&lt;/strong&gt;. It's a text adventure game, but the core idea is to have multiple LLMs working in synergy to create a deeply dynamic and open-ended world. During development, I hit a predictable wall: because the game can go in any direction, pre-made music is basically impossible, and I found that total silence gets boring fast. Sure, most users will play their own music if they really want to, but I felt like it needed something by default. So...&lt;/p&gt; &lt;p&gt;I decided to tackle this by training a MIDI generation model from scratch to act as the game's dynamic composer. Because... why not choose the most complex and interesting solution? :)&lt;/p&gt; &lt;p&gt;After a lot of research, failed attempts, walls hit, desperation, tears, punches against my poor desk (and... ehm... not proud of it, but some LLM verbal abuse, a lot of it...) I settled on using a 5-stage curriculum training approach. The idea is to build a strong, unconditional composer first before fine-tuning it to follow text prompts (hence why you will see &amp;quot;unconditional&amp;quot; in the video a lot).&lt;/p&gt; &lt;p&gt;The video I linked covers the first 3 of these 5 planned stages. I'm currently in the middle of training Stage 4, which is where I'm introducing an encoder to tie the generation to natural language prompts (that another LLM will generate in my game based on the situation). So this is very much a work-in-progress, and it could very well still fail spectacularly.&lt;/p&gt; &lt;p&gt;Be warned: a lot of what you will hear sucks... badly. In some cases, especially during Stage 3, the sucking is actually good, as the underlying musical structure shows progress even if it doesn't sound like it. &amp;quot;Trust the process&amp;quot; and all... I've had to learn to live by that motto.&lt;/p&gt; &lt;p&gt;You can literally watch its evolution:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stage 1:&lt;/strong&gt; It starts with classic mode collapse (just one repeating note) before eventually figuring out how to build simple melodies and harmonies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 2:&lt;/strong&gt; It learns the &amp;quot;full vocabulary,&amp;quot; discovering velocity (how hard a note is played) and rests. Its style gets way more expressive and splits into distinct &amp;quot;jazzy&amp;quot; and &amp;quot;lyrical&amp;quot; phases.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stage 3:&lt;/strong&gt; It gets introduced to a huge dataset with multiple instruments. The initial output is a chaotic but fascinating &amp;quot;instrument salad,&amp;quot; which slowly resolves as it starts to understand orchestration and counterpoint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To help me visualize all this, I put together a Python script to generate the video—and I have to give a huge shout-out to Gemini 2.5 Pro for doing most of the job on it. The music in the video is generated from the validation samples I create every few epochs to evaluate progress and keep an eye out for bugs and weirdness.&lt;/p&gt; &lt;p&gt;I have been overseeing every step of its learning, with dozens of custom loss functions tested and tweaked, so many hours i lost count of, tears and joy, so to me it is super interesting while I am sure to most of you it will be boring as fuck, but thought that maybe someone here will appreciate observing the learning steps and progress in such detail.&lt;/p&gt; &lt;p&gt;Btw, the model doesn't have a name yet. I've been kicking around a couple of cheesy puns: &lt;strong&gt;AI.da&lt;/strong&gt; (like the opera) or &lt;strong&gt;viv-AI-ldi&lt;/strong&gt;. Curious to hear which one lands better, or if you have any other ideas&lt;/p&gt; &lt;p&gt;Edit... forgot to mention that the goal is to have the smallest, working, model possible so that it can run locally within my game and together with other small models for other tasks (like TTS etc). The current design is at 20 mil total parameters and 140mb full precision (i hope to gain something by converting it to fp16 ONNX for actual use in game)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/orblabs"&gt; /u/orblabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fdy23li41dxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og8jil/my_llmpowered_text_adventure_needed_a_dynamic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og8jil/my_llmpowered_text_adventure_needed_a_dynamic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T01:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1og2k8e</id>
    <title>Who is using Granite 4? What's your use case?</title>
    <updated>2025-10-25T21:01:40+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been about 3 weeks since Granite 4 was released with base and instruct versions. If you're using it, what are you using it for? What made you choose it over (or alongside) others?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; this is great and extremely interesting. These use-cases are actually motivating me to consider Granite for a research-paper-parsing project I've been thinking about trying. &lt;/p&gt; &lt;p&gt;The basic idea: I read research papers, and increasingly I talk with LLMs about various bits of different papers. It's annoying to manually process chunks of a paper to pass into an LLM, so I've been thinking about making an agent or few to price a paper into markdown and summarize certain topics and parts automatically for me. &lt;em&gt;And&lt;/em&gt;, of course, I just recall that docling is already integrated with a granite model for basic processing..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og2k8e/who_is_using_granite_4_whats_your_use_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:01:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogezrx</id>
    <title>GPT-OSS DPO/RL fine-tuning, anyone?</title>
    <updated>2025-10-26T08:04:48+00:00</updated>
    <author>
      <name>/u/Few_Art_4147</name>
      <uri>https://old.reddit.com/user/Few_Art_4147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am quite surprised that I can't find a single example of GPT-OSS fine-tuning with DPO or RL. Anyone tried? I wanted to see some benchmarks before putting time into it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Art_4147"&gt; /u/Few_Art_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogezrx/gptoss_dporl_finetuning_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogezrx/gptoss_dporl_finetuning_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogezrx/gptoss_dporl_finetuning_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T08:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogfmt4</id>
    <title>GLM 4.5 air for coding</title>
    <updated>2025-10-26T08:46:12+00:00</updated>
    <author>
      <name>/u/Magnus114</name>
      <uri>https://old.reddit.com/user/Magnus114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You who use a local glm 4.5 air for coding, can you please share your software setup?&lt;/p&gt; &lt;p&gt;I have had some success with unsloth q4_k_m on llama.cpp with opencode. To get the tool usage to work I had to use a jinja template from a pull request, and still the tool calling fails occasionally. Tried unsloth jinja template from glm 4.6, but no success. Also experimented with claude code with open router with a similar result. Considering to trying to write my own template and also trying with vllm.&lt;/p&gt; &lt;p&gt;Would love to hear how others are using glm 4.5 air. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Magnus114"&gt; /u/Magnus114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T08:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1og9ije</id>
    <title>How good is Ling-1T?</title>
    <updated>2025-10-26T02:35:28+00:00</updated>
    <author>
      <name>/u/Aware_Magician7958</name>
      <uri>https://old.reddit.com/user/Aware_Magician7958</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"&gt; &lt;img alt="How good is Ling-1T?" src="https://preview.redd.it/cs7bb6igbdxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be874b0392daf9b87a138381fb588243291b71a3" title="How good is Ling-1T?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently there's been a new model by Ant Group (InclusionAI) that is an open-weight non-thinking model with 1000B parameters. According to their article their performance is better than paid models. Has anyone run this yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aware_Magician7958"&gt; /u/Aware_Magician7958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cs7bb6igbdxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T02:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogjkfj</id>
    <title>Cheaper &amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI</title>
    <updated>2025-10-26T12:37:45+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt; &lt;img alt="Cheaper &amp;amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI" src="https://b.thumbs.redditmedia.com/v4DjQ3ybawpfEVteS_F8SfFp6OReWcTch-9YAwPpLfs.jpg" title="Cheaper &amp;amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/v0ddm42g9gxf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f75f7809cead99b006dc49dc76a53f453f06a8f"&gt;Chamath Palihapitiya&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmbx1rcl9gxf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=154810c46f400e52c2ef4cef6c6a44c79fab9fef"&gt;https://preview.redd.it/dmbx1rcl9gxf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=154810c46f400e52c2ef4cef6c6a44c79fab9fef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The valley is built on open-source models?&lt;/p&gt; &lt;p&gt;On the All-In podcast, Chamath Palihapitiya says his team redirected a ton of workloads to &lt;strong&gt;Kimi K2&lt;/strong&gt; because it was “&lt;strong&gt;way more performant&lt;/strong&gt;” and “&lt;strong&gt;a ton cheaper&lt;/strong&gt;” than OpenAI and Anthropic.&lt;/p&gt; &lt;p&gt;Airbnb CEO Brian Chesky says they’re relying a lot on Alibaba’s &lt;strong&gt;Qwen&lt;/strong&gt; in production because it’s “&lt;strong&gt;fast and cheap.&lt;/strong&gt;” They still use OpenAI’s latest models, but “typically don’t use them that much in production” due to &lt;strong&gt;faster/cheaper&lt;/strong&gt; options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1og3cnt</id>
    <title>Llama.cpp model conversion guide</title>
    <updated>2025-10-25T21:35:28+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt; &lt;img alt="Llama.cpp model conversion guide" src="https://external-preview.redd.it/9H8ID2ho6Hmpcg_iEkUBl0dzrALfa1J8gFxkkoi6ojc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=960cfd58e78583a22cdad3922567cd461d36ac4b" title="Llama.cpp model conversion guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the open source community always benefits by having more people do stuff, I figured I would capitalize on my experiences with a few architectures I've done and add a guide for people who, like me, would like to gain practical experience by porting a model architecture.&lt;/p&gt; &lt;p&gt;Feel free to propose any topics / clarifications and ask any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1og87pd</id>
    <title>If you had $4k, would you invest in a DGX Spark?</title>
    <updated>2025-10-26T01:28:00+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys, I am very curious what everyone's opinion is regarding the DGX Spark. &lt;/p&gt; &lt;p&gt;If you had $4k and you needed to use that money to start building out your own personal AI data center, would you buy a DGX Spark... or go a different direction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T01:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogjtkn</id>
    <title>Poor GPU Club : Good Worthy Pruned models?</title>
    <updated>2025-10-26T12:50:38+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to explore more on this after seeing recent threads( &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;3&lt;/a&gt; , &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;2&lt;/a&gt; , &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;1&lt;/a&gt; ) from Cerebras. They already pruned few MOE models such as Qwen3-Coder-30B, Qwen3-Coder-480B, GLM-4.5-Air, GLM-4.6. I'm just waiting for few small MOE models from them, hope they do soon or later.&lt;/p&gt; &lt;p&gt;Meanwhile &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/"&gt;one other person pruned few other MOE models&lt;/a&gt;(Qwen3-30B, Qwen3-30B-Instruct, Qwen3-Coder-30B, GPT-OSS-20B, GPT-OSS-120B) using same Reap by Cerebras.&lt;/p&gt; &lt;p&gt;I'll be trying those small pruned models for sure since I have only 8GB VRAM(and 32GB RAM).&lt;/p&gt; &lt;p&gt;I'm sure some of you might have tried few pruned models before. HuggingFace has 100s of pruned models. Below are links to pruned models with different tags. Of course they must be some more pruned models without below tags. &lt;a href="https://huggingface.co/models?other=pruned"&gt;Pruned&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=prune"&gt;Prune&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=pruning"&gt;Pruning&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=pruned-model"&gt;pruned-model&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=expert-pruning"&gt;expert-pruning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;1] Please recommend good worthy pruned models particularly small ones under 50B&lt;/p&gt; &lt;p&gt;2] Cerebras Reap method is only for MOE models. Does anyone came across anything for Dense models? Recently I posted a thread about Q3/Q2 quants of Dense models since I couldn't run those models with high quants like Q4 &amp;amp; above. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;Anyone use Q3/Q2 quants of 20-40B Dense models? How's it?&lt;/a&gt; Unfortunately I couldn't run even Q3 with bearable t/s.&lt;/p&gt; &lt;p&gt;Currently I'm looking for Pruned models of below ones:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seed-OSS-36B-Instruct&lt;/li&gt; &lt;li&gt;Devstral-Small-2507&lt;/li&gt; &lt;li&gt;Magistral-Small-2509&lt;/li&gt; &lt;li&gt;Mistral-Small-3.2-24B-Instruct-2506&lt;/li&gt; &lt;li&gt;reka-flash-3.1&lt;/li&gt; &lt;li&gt;Gemma-3-27B-it&lt;/li&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;GLM-4-32B-0414&lt;/li&gt; &lt;li&gt;And lot of 20B+ finetunes from sources like TheDrummer, SicariusSicariiStuff, etc.,&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It would be great if someone shrink those dense models to 50%(at least 25-35%) so I could use Q4 with decent/bearable t/s with my 8GB VRAM(and 32GB RAM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogigyu</id>
    <title>Is SSM dead now?</title>
    <updated>2025-10-26T11:39:31+00:00</updated>
    <author>
      <name>/u/Spapoxl</name>
      <uri>https://old.reddit.com/user/Spapoxl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried researching about it and found almost all of the news and information is 1 years ago. Is it discontinued? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spapoxl"&gt; /u/Spapoxl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T11:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogaohi</id>
    <title>MiniMax: MiniMax M2 seems to VERY, VERY good</title>
    <updated>2025-10-26T03:39:02+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generally use GLM4.6 , been at a few problems most of the week, today threw these at MiniMax: MiniMax M2 and it sorted them with no fuss......Very impressed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oga7um</id>
    <title>All the models seem to love using the same names.</title>
    <updated>2025-10-26T03:13:40+00:00</updated>
    <author>
      <name>/u/AI_Renaissance</name>
      <uri>https://old.reddit.com/user/AI_Renaissance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In particular thorn and vance when doing horror or science fiction, for a woman its almost always elara vance, and if there is a male doctor or scientist, usually thomas thorn. Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;Right now I mostly use Cydonia which is a pretty good local model, but this even happens on the perchance ai website. It's funny, but annoying. I think maybe the training data eating itself with merges.&lt;/p&gt; &lt;p&gt;For example, try a prompt like &amp;quot;write a story about a mad scientist that creates a monster&amp;quot;. The name of the scientist will most likely be something like Dr. Aris or Thomas Thorne. Its not a that big of a deal if you come up with your own names for characters. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Renaissance"&gt; /u/AI_Renaissance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogh8ec</id>
    <title>Using GLM 4.6 to understand it's limitations</title>
    <updated>2025-10-26T10:27:40+00:00</updated>
    <author>
      <name>/u/Vozer_bros</name>
      <uri>https://old.reddit.com/user/Vozer_bros</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt; &lt;img alt="Using GLM 4.6 to understand it's limitations" src="https://a.thumbs.redditmedia.com/E-b1AYjpK-0wB4j8lY7BJQb9A8ucue0uF7crpypRrQ0.jpg" title="Using GLM 4.6 to understand it's limitations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gq9yiommnfxf1.png?width=1994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04be61d0c1fe988448c06878ea77b577ddd6aee1"&gt;https://preview.redd.it/gq9yiommnfxf1.png?width=1994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04be61d0c1fe988448c06878ea77b577ddd6aee1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The actual loosing point will start at 30% less than the number in the table. For example, tool calling actually starting to fail randomly at 70k context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vozer_bros"&gt; /u/Vozer_bros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T10:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofu15a</id>
    <title>I rebuilt DeepSeek’s OCR model in Rust so anyone can run it locally (no Python!)</title>
    <updated>2025-10-25T15:13:52+00:00</updated>
    <author>
      <name>/u/Outrageous-Voice</name>
      <uri>https://old.reddit.com/user/Outrageous-Voice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! After wrestling with the original DeepSeek-OCR release (Python + Transformers, tons of dependencies, zero UX), I decided to port the whole inference stack to Rust. The repo is deepseek-ocr.rs (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt;) and it ships both a CLI and an OpenAI-compatible server so you can drop it straight into existing clients like Open WebUI.&lt;/p&gt; &lt;h1&gt;Why bother?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No Python, no conda—just a single Rust binary.&lt;/li&gt; &lt;li&gt;Works offline and keeps documents private.&lt;/li&gt; &lt;li&gt;Fully OpenAI-compatible, so existing SDKs/ChatGPT-style UIs “just work”.&lt;/li&gt; &lt;li&gt;Apple Silicon support with optional Metal acceleration (FP16).&lt;/li&gt; &lt;li&gt;Built-in Hugging Face downloader: config/tokenizer/weights (≈6.3 GB) fetch automatically; needs about 13 GB RAM to run.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What’s inside the Rust port?&lt;/h1&gt; &lt;p&gt;- Candle-based reimplementation of the language model (DeepSeek-V2) with KV caches + optional FlashAttention.&lt;/p&gt; &lt;p&gt;- Full SAM + CLIP vision pipeline, image tiling, projector, and tokenizer alignment identical to the PyTorch release.&lt;/p&gt; &lt;p&gt;- Rocket server that exposes /v1/responses and /v1/chat/completions (OpenAI-compatible streaming included).&lt;/p&gt; &lt;p&gt;- Single-turn prompt compaction so OCR doesn’t get poisoned by multi-turn history.&lt;/p&gt; &lt;p&gt;- Debug hooks to compare intermediate tensors against the official model (parity is already very close).&lt;/p&gt; &lt;h1&gt;Getting started&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You can download prebuilt archives (macOS with Metal, Windows) from the latest successful run of the repo’s GitHub Actions “build-binaries (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml)”&lt;/a&gt;”) workflow—no local build required.&lt;/li&gt; &lt;li&gt;Prefer compiling? git clone &lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt; → cargo fetch&lt;/li&gt; &lt;li&gt;CLI: cargo run -p deepseek-ocr-cli -- --prompt &amp;quot;&amp;lt;image&amp;gt;...&amp;quot; --image mydoc.png&lt;/li&gt; &lt;li&gt;Server: cargo run -p deepseek-ocr-server -- --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --port 8000&lt;/li&gt; &lt;li&gt;On macOS, add --features metal plus --device metal --dtype f16 for GPU acceleration.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Batch document conversion (receipts → markdown, contracts → summaries, etc.).&lt;/li&gt; &lt;li&gt;Plugging into Open WebUI (looks/feels like ChatGPT but runs YOUR OCR model).&lt;/li&gt; &lt;li&gt;Building document QA bots that need faithful extraction.&lt;strong&gt;If you try it, I’d love to hear your feedback—feature requests, edge cases, performance reports, all welcome. And if it saves you from Python dependency hell, toss the repo a ⭐️.&lt;/strong&gt;Cheers!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Voice"&gt; /u/Outrageous-Voice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T15:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogg6sz</id>
    <title>Why didn't LoRA catch on with LLMs?</title>
    <updated>2025-10-26T09:22:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Explanation of LoRA for the folks at home&lt;/h2&gt; &lt;p&gt;(skip to next section if you already know what Lora is)&lt;/p&gt; &lt;p&gt;I only know it from the image generation Stable Diffusion world, and I only tried that briefly, so this won't be 100% exact.&lt;/p&gt; &lt;p&gt;Let's say your image generation model is Stable Diffusion 1.5, which came out a few years ago. It can't know the artstyle of a new artist that came up in the past year, let's say his name his Bobsolete.&lt;/p&gt; &lt;p&gt;What lora creators did is create a small dataset of Bobsolete's art, and use it to train SD 1.5 for like 1-2 days. This outputs a small lora file (the SD 1.5 model is 8GB, a lora is like 20MB). Users can download this lora, and when loading SD 1.5, say &amp;quot;also attach Bobsolete.lora to the model&amp;quot;. Now the user is interacting with SD 1.5 that has been augmented with knowledge of Bobsolete. The user can specify &amp;quot;drawn in the style of Bobsolete&amp;quot; and it will work.&lt;/p&gt; &lt;p&gt;Loras are used to add new styles to a model, new unique characters, and so on.&lt;/p&gt; &lt;h2&gt;Back to LLMs&lt;/h2&gt; &lt;p&gt;LLMs apparently support loras, but no one seems to use them. I've never ever seen them discussed on this sub in my 2 years of casual browsing, although I see they exist in the search results.&lt;/p&gt; &lt;p&gt;I was wondering why this hasn't caught on. People could add little bodies of knowledge to an already-released model. For example, you take a solid general model like Gemma 3 27B. Someone could release a lora trained on all scifi books, another based on all major movie scripts, etc. You could then &amp;quot;./llama.cpp -m models/gemma3.gguf --lora models/scifi-books-rev6.lora --lora models/movie-scripts.lora&amp;quot; and try to get Gemma 3 to help you write a modern scifi movie script. You could even focus even more on specific authors, cormac-mccarthy.lora etc.&lt;/p&gt; &lt;p&gt;A more useful/legal example would be attaching current-events-2025.lora to a model whose cutoff date was December 2024. &lt;/p&gt; &lt;p&gt;So why didn't this catch on the way it did in the image world? Is this technology inherently more limited on LLMs? Why does it seem like companies interested in integrating their doc with AI are more focused on RAG than training a Lora on their internal docs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T09:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
