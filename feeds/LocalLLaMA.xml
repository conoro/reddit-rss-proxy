<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-07T13:05:44+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q5ta4l</id>
    <title>llama-benchy - llama-bench style benchmarking for ANY LLM backend</title>
    <updated>2026-01-06T20:02:33+00:00</updated>
    <author>
      <name>/u/Eugr</name>
      <uri>https://old.reddit.com/user/Eugr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: I've built this tool primarily for myself as I couldn't easily compare model performance across different backends in the way that is easy to digest and useful for me. I decided to share this in case someone has the same need.&lt;/p&gt; &lt;h2&gt;Why I built this?&lt;/h2&gt; &lt;p&gt;As probably many of you here, I've been happily using llama-bench to benchmark local models performance running in llama.cpp. One great feature is that it can help to evaluate performance at different context lengths and present the output in a table format that is easy to digest.&lt;/p&gt; &lt;p&gt;However, llama.cpp is not the only inference engine I use, I also use SGLang and vLLM. But llama-bench can only work with llama.cpp, and other benchmarking tools that I found are more focused on concurrency and total throughput.&lt;/p&gt; &lt;p&gt;Also, llama-bench performs measurements using the C++ engine directly which is not representative of the end user experience which can be quite different in practice.&lt;/p&gt; &lt;p&gt;vLLM has its own powerful benchmarking tool, but while it can be used with other inference engines, there are a few issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You can't easily measure how prompt processing speed degrades as context grows. You can use &lt;code&gt;vllm bench sweep serve&lt;/code&gt;, but it only works well with vLLM with prefix caching disabled on the server. Even with random prompts it will reuse the same prompt between multiple runs which will hit the cache in &lt;code&gt;llama-server&lt;/code&gt; for instance. So you will get very low median TTFT times and very high prompt processing speeds. &lt;/li&gt; &lt;li&gt;The TTFT measurement it uses is not actually until the first usable token, it's until the very first data chunk from the server which may not contain any generated tokens in /v1/chat/completions mode.&lt;/li&gt; &lt;li&gt;Random dataset is the only ones that allows to specify an arbitrary number of tokens, but randomly generated token sequence doesn't let you adequately measure speculative decoding/MTP.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As of today, I haven't been able to find any existing benchmarking tool that brings llama-bench style measurements at different context lengths to any OpenAI-compatible endpoint.&lt;/p&gt; &lt;h2&gt;What is llama-benchy?&lt;/h2&gt; &lt;p&gt;It's a CLI benchmarking tool that measures:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt Processing (pp) and Token Generation (tg) speeds at different context lengths.&lt;/li&gt; &lt;li&gt;Allows to benchmark context prefill and follow up prompt separately.&lt;/li&gt; &lt;li&gt;Reports additional metrics, like time to first response, estimated prompt processing time and end-to-end time to first token.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It works with any OpenAI-compatible endpoint that exposes /v1/chat/completions and also:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports configurable prompt length (&lt;code&gt;--pp&lt;/code&gt;), generation length (&lt;code&gt;--tg&lt;/code&gt;), and context depth (&lt;code&gt;--depth&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Can run multiple iterations (&lt;code&gt;--runs&lt;/code&gt;) and report mean Â± std.&lt;/li&gt; &lt;li&gt;Uses HuggingFace tokenizers for accurate token counts.&lt;/li&gt; &lt;li&gt;Downloads a book from Project Gutenberg to use as source text for prompts to ensure better benchmarking of spec.decoding/MTP models.&lt;/li&gt; &lt;li&gt;Supports executing a command after each run (e.g., to clear cache).&lt;/li&gt; &lt;li&gt;Configurable latency measurement mode to estimate server/network overhead and provide more accurate prompt processing numbers.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Quick Demo&lt;/h2&gt; &lt;p&gt;Benchmarking MiniMax 2.1 AWQ running on my dual Spark cluster with up to 100000 context:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Run without installation&lt;/h1&gt; &lt;p&gt;uvx llama-benchy --base-url http://spark:8888/v1 --model cyankiwi/MiniMax-M2.1-AWQ-4bit --depth 0 4096 8192 16384 32768 65535 100000 --adapt-prompt --latency-mode generation --enable-prefix-caching ```&lt;/p&gt; &lt;p&gt;Output:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;th align="right"&gt;ttfr (ms)&lt;/th&gt; &lt;th align="right"&gt;est_ppt (ms)&lt;/th&gt; &lt;th align="right"&gt;e2e_ttft (ms)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;3544.10 Â± 37.29&lt;/td&gt; &lt;td align="right"&gt;688.41 Â± 6.09&lt;/td&gt; &lt;td align="right"&gt;577.93 Â± 6.09&lt;/td&gt; &lt;td align="right"&gt;688.45 Â± 6.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;36.11 Â± 0.06&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d4096&lt;/td&gt; &lt;td align="right"&gt;3150.63 Â± 7.84&lt;/td&gt; &lt;td align="right"&gt;1410.55 Â± 3.24&lt;/td&gt; &lt;td align="right"&gt;1300.06 Â± 3.24&lt;/td&gt; &lt;td align="right"&gt;1410.58 Â± 3.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d4096&lt;/td&gt; &lt;td align="right"&gt;34.36 Â± 0.08&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt;2562.47 Â± 21.71&lt;/td&gt; &lt;td align="right"&gt;909.77 Â± 6.75&lt;/td&gt; &lt;td align="right"&gt;799.29 Â± 6.75&lt;/td&gt; &lt;td align="right"&gt;909.81 Â± 6.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt;33.41 Â± 0.05&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d8192&lt;/td&gt; &lt;td align="right"&gt;2832.52 Â± 12.34&lt;/td&gt; &lt;td align="right"&gt;3002.66 Â± 12.57&lt;/td&gt; &lt;td align="right"&gt;2892.18 Â± 12.57&lt;/td&gt; &lt;td align="right"&gt;3002.70 Â± 12.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d8192&lt;/td&gt; &lt;td align="right"&gt;31.38 Â± 0.06&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt;2261.83 Â± 10.69&lt;/td&gt; &lt;td align="right"&gt;1015.96 Â± 4.29&lt;/td&gt; &lt;td align="right"&gt;905.48 Â± 4.29&lt;/td&gt; &lt;td align="right"&gt;1016.00 Â± 4.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt;30.55 Â± 0.08&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d16384&lt;/td&gt; &lt;td align="right"&gt;2473.70 Â± 2.15&lt;/td&gt; &lt;td align="right"&gt;6733.76 Â± 5.76&lt;/td&gt; &lt;td align="right"&gt;6623.28 Â± 5.76&lt;/td&gt; &lt;td align="right"&gt;6733.80 Â± 5.75&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d16384&lt;/td&gt; &lt;td align="right"&gt;27.89 Â± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt;1824.55 Â± 6.32&lt;/td&gt; &lt;td align="right"&gt;1232.96 Â± 3.89&lt;/td&gt; &lt;td align="right"&gt;1122.48 Â± 3.89&lt;/td&gt; &lt;td align="right"&gt;1233.00 Â± 3.89&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt;27.21 Â± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d32768&lt;/td&gt; &lt;td align="right"&gt;2011.11 Â± 2.40&lt;/td&gt; &lt;td align="right"&gt;16403.98 Â± 19.43&lt;/td&gt; &lt;td align="right"&gt;16293.50 Â± 19.43&lt;/td&gt; &lt;td align="right"&gt;16404.03 Â± 19.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d32768&lt;/td&gt; &lt;td align="right"&gt;22.09 Â± 0.07&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt;1323.21 Â± 4.62&lt;/td&gt; &lt;td align="right"&gt;1658.25 Â± 5.41&lt;/td&gt; &lt;td align="right"&gt;1547.77 Â± 5.41&lt;/td&gt; &lt;td align="right"&gt;1658.29 Â± 5.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt;21.81 Â± 0.07&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d65535&lt;/td&gt; &lt;td align="right"&gt;1457.71 Â± 0.26&lt;/td&gt; &lt;td align="right"&gt;45067.98 Â± 7.94&lt;/td&gt; &lt;td align="right"&gt;44957.50 Â± 7.94&lt;/td&gt; &lt;td align="right"&gt;45068.01 Â± 7.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d65535&lt;/td&gt; &lt;td align="right"&gt;15.72 Â± 0.04&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d65535&lt;/td&gt; &lt;td align="right"&gt;840.36 Â± 2.35&lt;/td&gt; &lt;td align="right"&gt;2547.54 Â± 6.79&lt;/td&gt; &lt;td align="right"&gt;2437.06 Â± 6.79&lt;/td&gt; &lt;td align="right"&gt;2547.60 Â± 6.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d65535&lt;/td&gt; &lt;td align="right"&gt;15.63 Â± 0.02&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_pp @ d100000&lt;/td&gt; &lt;td align="right"&gt;1130.05 Â± 1.89&lt;/td&gt; &lt;td align="right"&gt;88602.31 Â± 148.70&lt;/td&gt; &lt;td align="right"&gt;88491.83 Â± 148.70&lt;/td&gt; &lt;td align="right"&gt;88602.37 Â± 148.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;ctx_tg @ d100000&lt;/td&gt; &lt;td align="right"&gt;12.14 Â± 0.02&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d100000&lt;/td&gt; &lt;td align="right"&gt;611.01 Â± 2.50&lt;/td&gt; &lt;td align="right"&gt;3462.39 Â± 13.73&lt;/td&gt; &lt;td align="right"&gt;3351.90 Â± 13.73&lt;/td&gt; &lt;td align="right"&gt;3462.42 Â± 13.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d100000&lt;/td&gt; &lt;td align="right"&gt;12.05 Â± 0.03&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;llama-benchy (0.1.0) date: 2026-01-06 11:44:49 | latency mode: generation&lt;/p&gt; &lt;h2&gt;GitHub&lt;/h2&gt; &lt;p&gt;&lt;a href="https://github.com/eugr/llama-benchy"&gt;https://github.com/eugr/llama-benchy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eugr"&gt; /u/Eugr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5ta4l/llamabenchy_llamabench_style_benchmarking_for_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T20:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5t0hr</id>
    <title>Building opensource Zero Server Code Intelligence Engine</title>
    <updated>2026-01-06T19:53:26+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/"&gt; &lt;img alt="Building opensource Zero Server Code Intelligence Engine" src="https://external-preview.redd.it/d3piOG55d2Fhc2JnMRc4K08WCLBencfsxOajXaBEA9NZR-l8om0wN65iL7dR.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0e96e2b737fe1c01f0efd03220c91d85880ea64" title="Building opensource Zero Server Code Intelligence Engine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, guys, I m building GitNexus, an opensource Code Intelligence Engine which works fully client sided in-browser. What all features would be useful, any integrations, cool ideas, etc?&lt;/p&gt; &lt;p&gt;site: &lt;a href="https://gitnexus.vercel.app/"&gt;https://gitnexus.vercel.app/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the crux of how it works:&lt;br /&gt; Repo parsed into Graph using AST -&amp;gt; Embeddings model running in browser creates the embeddings -&amp;gt; Everything is stored in a graph DB ( this also runs in browser through webassembly ) -&amp;gt; user sees UI visualization -&amp;gt; AI gets tools to query graph (cyfer query tool), semantic search, grep and node highlight.&lt;/p&gt; &lt;p&gt;So therefore we get a quick code intelligence engine that works fully client sided 100% private. Except the LLM provider there is no external data outlet. ( working on ollama support )&lt;/p&gt; &lt;p&gt;Would really appreciate any cool ideas / inputs / etc.&lt;/p&gt; &lt;p&gt;This is what I m aiming for right now:&lt;/p&gt; &lt;p&gt;1&amp;gt; Case 1 is quick way to chat with a repo, but then deepwiki is already there. But gitnexus has graph tools+ui so should be more accurate on audits and UI can help in visualize.&lt;/p&gt; &lt;p&gt;2&amp;gt; Downstream potential usecase will be MCP server exposed from browser itself, windsurf / cursor, etc can use it to perform codebase wise audits, blast radius detection of code changes, etc.&lt;/p&gt; &lt;p&gt;3&amp;gt; Another case might be since its fully private, devs having severe restrictions can use it with ollama or their own inference&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6xrs78taasbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5t0hr/building_opensource_zero_server_code_intelligence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T19:53:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5xftv</id>
    <title>I built my own AMD based AI rig</title>
    <updated>2026-01-06T22:34:27+00:00</updated>
    <author>
      <name>/u/Clear_Lead4099</name>
      <uri>https://old.reddit.com/user/Clear_Lead4099</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"&gt; &lt;img alt="I built my own AMD based AI rig" src="https://a.thumbs.redditmedia.com/JRNumikzIO0w3wHXEDVPMYhStJyKIcMBuaBxnn0mKx4.jpg" title="I built my own AMD based AI rig" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As promised after some trial and error, here is my baby: 256gb/256gb vram/ram, 8 GPU AMD R9700, Epyc 7532 CPU, 4TB nvme storage (and planned 24GB ssd raid) AI rig. It runs on Debian 12. I didn't go Nvidia route because I hate ugly monopolies and fucking crooks extorting money from us - hobbists. AMD path was the only feasible way for me to move forward with this. I do HPC and AI inference via llama.cpp and vllm on it. I plan to use it for local training for SST and TTS models. Largest model I run so far is MiniMax 2.1 Q8 gguf. Below is the equipment list and cost. I built it over the course of last 12 month, so prices for MB, Memory, NVMe drives, PSUs are what they were back then. GPUs and SlimSAS hardware were bought in last two month as well as last PSU. The only issue I had is PCIe AER errors. The culprit seems to be either SlimSAS raisers, cables or two slot adapters. Downgrading PCIe bus speed to Gen3 seem fixed these. Happy to answer any questions.&lt;/p&gt; &lt;p&gt;my /etc/default/grub settings:&lt;/p&gt; &lt;p&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet nosmt amdgpu.runpm=0 irqpoll pci=noaer&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rnu7la9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1248802c0c6f3c03807b30320b1bf304e0661626"&gt;https://preview.redd.it/rnu7la9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1248802c0c6f3c03807b30320b1bf304e0661626&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mn2x7a9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f0d920dc8abed7ab2356c97cc0be6d281d0e5b76"&gt;https://preview.redd.it/mn2x7a9l2tbg1.jpg?width=3060&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f0d920dc8abed7ab2356c97cc0be6d281d0e5b76&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c8fyjbtl1tbg1.png?width=869&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=641b7c4d36ac5f58abcebceaa236aa6f3a9e9704"&gt;Cost before taxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cp7licb52tbg1.png?width=4080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1309390a34d447fdcc23402cb34563414b58bfff"&gt;PCIe4 errors&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Clear_Lead4099"&gt; /u/Clear_Lead4099 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5xftv/i_built_my_own_amd_based_ai_rig/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T22:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5qsvd</id>
    <title>The FinePDFs ðŸ“„ Book</title>
    <updated>2026-01-06T18:34:44+00:00</updated>
    <author>
      <name>/u/Other_Housing8453</name>
      <uri>https://old.reddit.com/user/Other_Housing8453</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"&gt; &lt;img alt="The FinePDFs ðŸ“„ Book" src="https://b.thumbs.redditmedia.com/-QMdTTGDscpwqAKvkKUZy4gqMc1FCD1LS2ACweeZBME.jpg" title="The FinePDFs ðŸ“„ Book" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends, Hynek from HuggingFace here. &lt;/p&gt; &lt;p&gt;We have released FinePDFs dataset of 3T tokens last year and we felt obliged to share the knowledge with there rest of OSS community. &lt;/p&gt; &lt;p&gt;The HuggingFace Press, has been pulling an extra hours through the Christmas, to put everything we know about PDFs inside:&lt;br /&gt; - How to make the SoTA PDFs dataset?&lt;br /&gt; - How much old internet is dead now?&lt;br /&gt; - Why we chose RolmOCR for OCR&lt;br /&gt; - What's the most Claude like OSS model?&lt;br /&gt; - Why is the horse racing site topping the FinePDFs URL list? &lt;/p&gt; &lt;p&gt;We hope you like it :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/z49knj5fwrbg1.png?width=1373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8"&gt;https://preview.redd.it/z49knj5fwrbg1.png?width=1373&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0f6b8ef4361692a270c9c3c388b31ef7c2b9ec8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Other_Housing8453"&gt; /u/Other_Housing8453 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5qsvd/the_finepdfs_book/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T18:34:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6e4sm</id>
    <title>Models for middle eastern languages?</title>
    <updated>2026-01-07T12:30:51+00:00</updated>
    <author>
      <name>/u/WeekLarge7607</name>
      <uri>https://old.reddit.com/user/WeekLarge7607</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm learning geopolitics, specifically about the middle east, and I'm wondering if anyone knows a good local model for translation and summarization for middle eastern languages (various types of Arabic, Hebrew, Persian)?&lt;/p&gt; &lt;p&gt;I've been using gemma3 and cohere command models, but some of them are old now, and new ones are too big for me (command a models are 100 something B and dense).&lt;/p&gt; &lt;p&gt;Something around 30b or 70b quantized would be perfect.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WeekLarge7607"&gt; /u/WeekLarge7607 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6e4sm/models_for_middle_eastern_languages/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6e4sm/models_for_middle_eastern_languages/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6e4sm/models_for_middle_eastern_languages/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T12:30:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6eg95</id>
    <title>This diagram shows everything you 'need' for LLM apps. I think 90% of it is overengineering. Change my mind.</title>
    <updated>2026-01-07T12:46:44+00:00</updated>
    <author>
      <name>/u/ImpressionTop1712</name>
      <uri>https://old.reddit.com/user/ImpressionTop1712</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6eg95/this_diagram_shows_everything_you_need_for_llm/"&gt; &lt;img alt="This diagram shows everything you 'need' for LLM apps. I think 90% of it is overengineering. Change my mind." src="https://preview.redd.it/kcfi6gjpaxbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3348c75d51f5e5285d35d3a377da2adaf885c131" title="This diagram shows everything you 'need' for LLM apps. I think 90% of it is overengineering. Change my mind." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ImpressionTop1712"&gt; /u/ImpressionTop1712 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kcfi6gjpaxbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6eg95/this_diagram_shows_everything_you_need_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6eg95/this_diagram_shows_everything_you_need_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T12:46:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6cbgy</id>
    <title>Released v0.1.6 of Owlex, an MCP server that integrates Codex CLI, Gemini CLI, and OpenCode into Claude Code.</title>
    <updated>2026-01-07T10:51:43+00:00</updated>
    <author>
      <name>/u/spokv</name>
      <uri>https://old.reddit.com/user/spokv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cbgy/released_v016_of_owlex_an_mcp_server_that/"&gt; &lt;img alt="Released v0.1.6 of Owlex, an MCP server that integrates Codex CLI, Gemini CLI, and OpenCode into Claude Code." src="https://external-preview.redd.it/ArYRt78-5RFb6CiMNI10gprlO6acxZMy0DlZ2xmn3uI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85292b673f6b38a2ab14ccd0da115b0f59ab17ec" title="Released v0.1.6 of Owlex, an MCP server that integrates Codex CLI, Gemini CLI, and OpenCode into Claude Code." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new async feature lets you:&lt;br /&gt; - Start a council deliberation that queries multiple AI models&lt;br /&gt; - Get a task ID immediately and continue working&lt;br /&gt; - Check back later for results with wait_for_task&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/agentic-mcp-tools/owlex"&gt;https://github.com/agentic-mcp-tools/owlex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's a &amp;quot;council&amp;quot;?&lt;br /&gt; Instead of relying on a single model's opinion, the council queries multiple agents (Codex/o3, Gemini, OpenCode) with your question and synthesizes their responses. Great for architecture decisions, code reviews, or when you want diverse perspectives.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1q6cbgy/video/hrj7rycqqwbg1/player"&gt;https://reddit.com/link/1q6cbgy/video/hrj7rycqqwbg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokv"&gt; /u/spokv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cbgy/released_v016_of_owlex_an_mcp_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cbgy/released_v016_of_owlex_an_mcp_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cbgy/released_v016_of_owlex_an_mcp_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T10:51:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q62pyh</id>
    <title>Why not Qwen3-30B Quantized over qwen3-14B or gemma-12B?</title>
    <updated>2026-01-07T02:12:35+00:00</updated>
    <author>
      <name>/u/arktik7</name>
      <uri>https://old.reddit.com/user/arktik7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I am learning :)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I have a 3080ti with 12GB of VRAM and 32GB of RAM and a 5900x. With this I can run qwen3-30b-a3b-thinking-2507 that does 3.3B activated parameters in LM studio 20 tok/sec which I believe is quantized right? It runs pretty well and has good answers. Why would I use the more recommended ones of qwen3-14b or gemma 12b over this that I see more often recommended for a computer of my specs?&lt;/p&gt; &lt;p&gt;My use case is primarily just a general AI that I can ask have search the web, clean up writing, troubleshoot IT issues on my homelab, and ask general questions. &lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arktik7"&gt; /u/arktik7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q62pyh/why_not_qwen330b_quantized_over_qwen314b_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T02:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5mh84</id>
    <title>Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)</title>
    <updated>2026-01-06T16:00:10+00:00</updated>
    <author>
      <name>/u/A-Rahim</name>
      <uri>https://old.reddit.com/user/A-Rahim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/"&gt; &lt;img alt="Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)" src="https://preview.redd.it/lf2sfats4rbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f7f9376d0c0fdcd670b38f3bb1ea143dc497573f" title="Unsloth-MLX - Fine-tune LLMs on your Mac (same API as Unsloth)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Everyone,&lt;/p&gt; &lt;p&gt;I've been working on something for Mac users in the ML space.&lt;/p&gt; &lt;p&gt;Unsloth-MLX - an MLX-powered library that brings the Unsloth fine-tuning experience to Apple Silicon.&lt;/p&gt; &lt;p&gt;The idea is simple:&lt;/p&gt; &lt;p&gt;â†’ Prototype your LLM fine-tuning locally on Mac&lt;br /&gt; â†’ Same code works on cloud GPUs with original Unsloth&lt;br /&gt; â†’ No API changes, just swap the import&lt;/p&gt; &lt;p&gt;Why? Cloud GPU costs add up fast during experimentation. Your Mac's unified memory (up to 512GB on Mac Studio) is sitting right there.&lt;/p&gt; &lt;p&gt;It's not a replacement for Unsloth - it's a bridge for local development before scaling up.&lt;/p&gt; &lt;p&gt;Still early days - would really appreciate feedback, bug reports, or feature requests.&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ARahim3/unsloth-mlx"&gt;https://github.com/ARahim3/unsloth-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: This is a personal fun project, &lt;strong&gt;not affiliated with Unsloth AI or Apple.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Personal Note:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I rely on Unsloth for my daily fine-tuning on cloud GPUsâ€”it's the gold standard for me. But recently, I started working on a MacBook M4 and hit a friction point: I wanted to prototype locally on my Mac, then scale up to the cloud without rewriting my entire training script.&lt;/p&gt; &lt;p&gt;Since Unsloth relies on Triton (which Macs don't have, yet), I couldn't use it locally. I built &lt;code&gt;unsloth-mlx&lt;/code&gt; to solve this specific &amp;quot;Context Switch&amp;quot; problem. It wraps Apple's native MLX framework in an Unsloth-compatible API.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The goal isn't to replace Unsloth or claim superior performance.&lt;/strong&gt; The goal is &lt;strong&gt;code portability&lt;/strong&gt;: allowing you to write &lt;code&gt;FastLanguageModel&lt;/code&gt; code once on your Mac, test it, and then push that &lt;em&gt;exact same script&lt;/em&gt; to a CUDA cluster. It solves a workflow problem, not just a hardware one.&lt;/p&gt; &lt;p&gt;This is an &amp;quot;unofficial&amp;quot; project built by a fan, for fans who happen to use Macs. It's helping me personally, and if it helps others like me, then I'll have my satisfaction.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A-Rahim"&gt; /u/A-Rahim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lf2sfats4rbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5mh84/unslothmlx_finetune_llms_on_your_mac_same_api_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T16:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6cihe</id>
    <title>I built a mobile game where a local Qwen3-VL acts as an "Oracle" that analyzes player photos</title>
    <updated>2026-01-07T11:03:08+00:00</updated>
    <author>
      <name>/u/franke777</name>
      <uri>https://old.reddit.com/user/franke777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been working on a solo project called Lenswalker a walking RPG where players physically walk to charge mana, then photograph real-world subjects. The interesting part: a locally-hosted vision model analyzes each photo and determines what they found.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;p&gt;- Ollama running Qwen3-VL on my home server (RTX 4090)&lt;/p&gt; &lt;p&gt;- FastAPI backend, PWA frontend&lt;/p&gt; &lt;p&gt;- Everything self-hosted, no cloud APIs, no data leaving my network&lt;/p&gt; &lt;p&gt;What the Oracle does:&lt;/p&gt; &lt;p&gt;- Analyzes the photo and identifies the subject&lt;/p&gt; &lt;p&gt;- Assigns a &amp;quot;rarity&amp;quot; (1-10) based on how interesting/unusual it is (a trash can = 1, a wild fox = 9)&lt;/p&gt; &lt;p&gt;- Determines capture quality (composition, lighting, focus)&lt;/p&gt; &lt;p&gt;- Extracts dominant color -&amp;gt; maps to game element (green -&amp;gt; Nature, white -&amp;gt; Light, etc.)&lt;/p&gt; &lt;p&gt;- Generates flavor text for the discovery&lt;/p&gt; &lt;p&gt;What surprised me:&lt;/p&gt; &lt;p&gt;- Qwen3-VL is remarkably consistent at judging &amp;quot;interestingness&amp;quot; - mundane objects score low, genuinely unusual finds score high&lt;/p&gt; &lt;p&gt;- Color extraction works well for element assignment&lt;/p&gt; &lt;p&gt;- ~15-45s per analysis on first load, ~5-10s when model is warm&lt;/p&gt; &lt;p&gt;- Running OLLAMA_MAX_CONCURRENT=4 handles multiple players fine&lt;/p&gt; &lt;p&gt;The whole thing started because I wanted a game where the AI couldn't be cheated by googling answers, you have to actually go outside and find something worth photographing.&lt;/p&gt; &lt;p&gt;Currently in pre-alpha with ~25 testers. Happy to answer questions about the vision model integration or the prompt engineering approach. &lt;/p&gt; &lt;p&gt;If anyone in Europe wants to try it out, DM me, server's hosted in Germany so latency is best for EU players.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/franke777"&gt; /u/franke777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cihe/i_built_a_mobile_game_where_a_local_qwen3vl_acts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cihe/i_built_a_mobile_game_where_a_local_qwen3vl_acts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cihe/i_built_a_mobile_game_where_a_local_qwen3vl_acts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T11:03:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5dnyw</id>
    <title>Performance improvements in llama.cpp over time</title>
    <updated>2026-01-06T09:03:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"&gt; &lt;img alt="Performance improvements in llama.cpp over time" src="https://preview.redd.it/lsqwma772pbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc5492d32ea47c504ec0399a9c2a02a046df6fc0" title="Performance improvements in llama.cpp over time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lsqwma772pbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5dnyw/performance_improvements_in_llamacpp_over_time/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T09:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6cuh5</id>
    <title>In NVIDIA's announcement of Rubin (successor to Blackwell) what do you think is meant by "adaptive compression"?</title>
    <updated>2026-01-07T11:22:07+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cuh5/in_nvidias_announcement_of_rubin_successor_to/"&gt; &lt;img alt="In NVIDIA's announcement of Rubin (successor to Blackwell) what do you think is meant by &amp;quot;adaptive compression&amp;quot;?" src="https://external-preview.redd.it/5t8jfpe67v909kH8kONfOcl8j7uy46XjufTzHSp5p-I.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81304f3376532fa1f1d936cef16bcd151043d65a" title="In NVIDIA's announcement of Rubin (successor to Blackwell) what do you think is meant by &amp;quot;adaptive compression&amp;quot;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developer.nvidia.com/blog/inside-the-nvidia-rubin-platform-six-new-chips-one-ai-supercomputer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cuh5/in_nvidias_announcement_of_rubin_successor_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6cuh5/in_nvidias_announcement_of_rubin_successor_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T11:22:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6524l</id>
    <title>[Research] I implemented a routed attention mechanism (R-GQA) for faster long-context models. Then wrote a paper on it.</title>
    <updated>2026-01-07T03:56:05+00:00</updated>
    <author>
      <name>/u/Snowyiu</name>
      <uri>https://old.reddit.com/user/Snowyiu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6524l/research_i_implemented_a_routed_attention/"&gt; &lt;img alt="[Research] I implemented a routed attention mechanism (R-GQA) for faster long-context models. Then wrote a paper on it." src="https://b.thumbs.redditmedia.com/EkCP1Em1k5Js1CdDuRArbzlFrgBqWX5uehIbXXodEuc.jpg" title="[Research] I implemented a routed attention mechanism (R-GQA) for faster long-context models. Then wrote a paper on it." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/v6vzstczmubg1.png?width=3347&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=249015d063395ee4381b6b7d56c2dd09cbe3e791"&gt;R-GQA diagram using pytorch operations&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So, a while ago I thought to myself: &amp;quot;Those query heads in grouped-query attention... what are the chances that at any given time they all do something different and useful?&amp;quot;&lt;/p&gt; &lt;p&gt;I hypothesized that for any given token, maybe only 1 or 2 query heads per KV group are actually relevant. Thus, I created &lt;strong&gt;R-GQA (Routed Grouped-Query Attention)&lt;/strong&gt;. Itâ€™s similar to regular GQA, but it uses a learned router to select the most relevant query heads and only computes attention for those.&lt;/p&gt; &lt;p&gt;I was honestly shocked that seemingly this hadn't been done before. So I implemented it, trained up a bunch of models at different scales on my RTX 3090, and looked at the results.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment:&lt;/strong&gt;&lt;br /&gt; I trained GQA baseline models on Wikipedia at 82M, 162M, and 940M parameters and compared them against R-GQA.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Head Specialization:&lt;/strong&gt; With regular GQA, heads in a group converge to extremely similar representations. With R-GQA, the router forces them to be orthogonal (highly diverse).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; I achieved up to a &lt;strong&gt;+40% training throughput improvement&lt;/strong&gt;, which is quite good.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;L&amp;quot;:&lt;/strong&gt; I compared performance against &lt;strong&gt;SwitchHead&lt;/strong&gt;, which is conceptually similar but routes Values instead of Queries. Unfortunately for me, SwitchHead outperformed my variant on perplexity.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Wall:&lt;/strong&gt; At the largest model scale (940M), my mechanism stopped being competitive and fell off against the GQA baseline. It seems aggressive sparsity hurts when you really need the capacity.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm providing the code and the current draft of the paper because I think the findings are valuable, even if the architecture isn't SOTA yet.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FSnowyiu%2Frgqa%2F"&gt;https://github.com/Snowyiu/rgqa/&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FSnowyiu%2Frgqa%2Fblob%2Fmain%2Frgqa_paper.pdf"&gt;https://github.com/Snowyiu/rgqa/blob/main/rgqa_paper.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;One last thing:&lt;/strong&gt; I would like to publish on ArXiv, but I am stuck needing an endorsement from a researcher in this field. If there's anyone here who could help with that, it would be much appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snowyiu"&gt; /u/Snowyiu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6524l/research_i_implemented_a_routed_attention/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6524l/research_i_implemented_a_routed_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6524l/research_i_implemented_a_routed_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T03:56:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5vk9m</id>
    <title>200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring</title>
    <updated>2026-01-06T21:24:42+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"&gt; &lt;img alt="200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring" src="https://external-preview.redd.it/hCm8D9e9AzrbuM1MK1zF3wZVeIaff34g_KhZMmvJyGM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2e9c0e4b8d5b4eb54a2abe96ddff2503d4b7f22f" title="200ms search over 40 million texts using just a CPU server + demo: binary search with int8 rescoring" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the inference strategy:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Embed your query using a dense embedding model into a 'standard' fp32 embedding&lt;/li&gt; &lt;li&gt;Quantize the fp32 embedding to binary: 32x smaller&lt;/li&gt; &lt;li&gt;Use an approximate (or exact) binary index to retrieve e.g. 40 documents (~20x faster than a fp32 index)&lt;/li&gt; &lt;li&gt;Load int8 embeddings for the 40 top binary documents from disk.&lt;/li&gt; &lt;li&gt;Rescore the top 40 documents using the fp32 query embedding and the 40 int8 embeddings&lt;/li&gt; &lt;li&gt;Sort the 40 documents based on the new scores, grab the top 10&lt;/li&gt; &lt;li&gt;Load the titles/texts of the top 10 documents&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This requires:&lt;br /&gt; - Embedding all of your documents once, and using those embeddings for:&lt;br /&gt; - A binary index, I used a IndexBinaryFlat for exact and IndexBinaryIVF for approximate&lt;br /&gt; - A int8 &amp;quot;view&amp;quot;, i.e. a way to load the int8 embeddings from disk efficiently given a document ID&lt;/p&gt; &lt;p&gt;Instead of having to store fp32 embeddings, you only store binary index (32x smaller) and int8 embeddings (4x smaller). Beyond that, you only keep the binary index in memory, so you're also saving 32x on memory compared to a fp32 search index.&lt;/p&gt; &lt;p&gt;By loading e.g. 4x as many documents with the binary index and rescoring those with int8, you restore ~99% of the performance of the fp32 search, compared to ~97% when using purely the binary index: &lt;a href="https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring"&gt;https://huggingface.co/blog/embedding-quantization#scalar-int8-rescoring&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Check out the demo that allows you to test this technique on 40 million texts from Wikipedia: &lt;a href="https://huggingface.co/spaces/sentence-transformers/quantized-retrieval"&gt;https://huggingface.co/spaces/sentence-transformers/quantized-retrieval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It would be simple to add a sparse component here as well: e.g. bm25s for a BM25 variant or an inference-free SparseEncoder with e.g. 'splade-index'.&lt;/p&gt; &lt;p&gt;In short: your retrieval doesn't need to be so expensive!&lt;/p&gt; &lt;p&gt;Sources:&lt;br /&gt; - &lt;a href="https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a"&gt;https://www.linkedin.com/posts/tomaarsen_quantized-retrieval-a-hugging-face-space-activity-7414325916635381760-Md8a&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://huggingface.co/blog/embedding-quantization"&gt;https://huggingface.co/blog/embedding-quantization&lt;/a&gt;&lt;br /&gt; - &lt;a href="https://cohere.com/blog/int8-binary-embeddings"&gt;https://cohere.com/blog/int8-binary-embeddings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/sentence-transformers/quantized-retrieval"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5vk9m/200ms_search_over_40_million_texts_using_just_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T21:24:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6edb2</id>
    <title>AI agents for searching and reasoning over internal documents</title>
    <updated>2026-01-07T12:42:45+00:00</updated>
    <author>
      <name>/u/Effective-Ad2060</name>
      <uri>https://old.reddit.com/user/Effective-Ad2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Iâ€™m excited to share something weâ€™ve been building for the past few months - &lt;strong&gt;PipesHub&lt;/strong&gt;, a &lt;strong&gt;fully open-source alternative to Glean,&lt;/strong&gt; designed to bring powerful Enterprise Search, Agent Builders to every team, without vendor lock-in. The platform brings all your business data together and makes it searchable. It connects with apps like Google Drive, Gmail, Slack, Notion, Confluence, Jira, OneDrive, Outlook, SharePoint Online, Dropbox, and even local file uploads. You can deploy it and run it with just one docker compose command.&lt;/p&gt; &lt;p&gt;The entire system is built on a &lt;strong&gt;fully event-streaming architecture powered by Kafka&lt;/strong&gt;, making indexing and retrieval scalable, fault-tolerant, and real-time across large volumes of data. PipesHub combines a vector database with a knowledge graph and uses Agentic RAG to deliver highly accurate results. We constrain the LLM to ground truth. Provides Visual citations, reasoning and confidence score. Our implementation says Information not found rather than hallucinating.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deep understanding of user, organization and teams with enterprise knowledge graph&lt;/li&gt; &lt;li&gt;Connect to any AI model of your choice including OpenAI, Gemini, Claude, or Ollama&lt;/li&gt; &lt;li&gt;Use any other provider that supports OpenAI compatible endpoints&lt;/li&gt; &lt;li&gt;Vision-Language Models and OCR for visual or scanned docs&lt;/li&gt; &lt;li&gt;Login with Google, Microsoft, OAuth, or SSO&lt;/li&gt; &lt;li&gt;Rich REST APIs for developers&lt;/li&gt; &lt;li&gt;All major file types support including pdfs with images, diagrams and charts&lt;/li&gt; &lt;li&gt;Agent Builder - Perform actions like Sending mails, Schedule Meetings, etc along with Search, Deep research, Internet search and more&lt;/li&gt; &lt;li&gt;Reasoning Agent that plans before executing tasks&lt;/li&gt; &lt;li&gt;40+ Connectors allowing you to connect to your entire business apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out and share your thoughts or feedback. Your feedback is immensely valuable and is much appreciated:&lt;br /&gt; &lt;a href="https://github.com/pipeshub-ai/pipeshub-ai"&gt;https://github.com/pipeshub-ai/pipeshub-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Video:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=xA9m3pwOgz8"&gt;https://www.youtube.com/watch?v=xA9m3pwOgz8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Effective-Ad2060"&gt; /u/Effective-Ad2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6edb2/ai_agents_for_searching_and_reasoning_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6edb2/ai_agents_for_searching_and_reasoning_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6edb2/ai_agents_for_searching_and_reasoning_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T12:42:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6a32c</id>
    <title>NousCoder-14B-GGUF is here!</title>
    <updated>2026-01-07T08:33:13+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6a32c/nouscoder14bgguf_is_here/"&gt; &lt;img alt="NousCoder-14B-GGUF is here!" src="https://external-preview.redd.it/DMiOkJBVtF0q2KjwqWnPlke-IPF1A5a0Z_XDB7tGLn8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b59c49d178c2f341a958665b9686d84962973bbe" title="NousCoder-14B-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RL post training on Qwen 3 14B&lt;/p&gt; &lt;p&gt;&amp;quot;On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/NousCoder-14B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6a32c/nouscoder14bgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6a32c/nouscoder14bgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T08:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q5m2n6</id>
    <title>A 30B Qwen Model Walks Into a Raspberry Piâ€¦ and Runs in Real Time</title>
    <updated>2026-01-06T15:45:12+00:00</updated>
    <author>
      <name>/u/ali_byteshape</name>
      <uri>https://old.reddit.com/user/ali_byteshape</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"&gt; &lt;img alt="A 30B Qwen Model Walks Into a Raspberry Piâ€¦ and Runs in Real Time" src="https://preview.redd.it/52juwyqq0rbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39e3a291db2422f84c16930831ae926a4cb20240" title="A 30B Qwen Model Walks Into a Raspberry Piâ€¦ and Runs in Real Time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Weâ€™re back with another &lt;strong&gt;ShapeLearn&lt;/strong&gt; GGUF release (&lt;a href="https://byteshape.com/blogs/Qwen3-30B-A3B-Instruct-2507/"&gt;Blog&lt;/a&gt;, &lt;a href="https://huggingface.co/byteshape/Qwen3-30B-A3B-Instruct-2507-GGUF"&gt;Models&lt;/a&gt;), this time for a model that &lt;em&gt;should not&lt;/em&gt; feel this usable on small hardwareâ€¦ and yet here we are:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Qwen3-30B-A3B-Instruct-2507&lt;/strong&gt; (device-optimized quant variants, llama.cpp-first).&lt;/p&gt; &lt;p&gt;Weâ€™re optimizing for TPS on a specific device without output quality falling off a cliff.&lt;/p&gt; &lt;p&gt;Instead of treating â€œsmallerâ€ as the goal, we treat memory as a budget: Fit first, then optimize TPS vs quality.&lt;/p&gt; &lt;p&gt;Why? Because llama.cpp has a quirk: â€œFewer bitsâ€ does &lt;em&gt;not&lt;/em&gt; automatically mean â€œmore speed.â€&lt;/p&gt; &lt;p&gt;Different quant formats trigger different kernels + decode overheads, and on GPUs you can absolutely end up with &lt;strong&gt;smaller and slower&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Yes, a 30B runs on a Raspberry Pi 5 (16GB). We achieve &lt;strong&gt;8.03 TPS&lt;/strong&gt; at 2.70 BPW, while retaining &lt;strong&gt;94.18% of BF16 quality&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Across devices, the pattern repeats: ShapeLearn tends to find better TPS/quality tradeoffs versus alternatives (we compare against Unsloth and MagicQuant as requested in our previous post).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Whatâ€™s new/interesting in this one&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1) CPU behavior isâ€¦ sane (mostly)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On CPUs, once youâ€™re past â€œit fits,â€ &lt;strong&gt;smaller tends to be faster&lt;/strong&gt; in a fairly monotonic way. The tradeoff curve behaves like youâ€™d expect.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2) GPU behavior isâ€¦ quirky (kernel edition)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On GPUs, performance depends as much on &lt;strong&gt;kernel choice&lt;/strong&gt; as on memory footprint. So you often get &lt;strong&gt;sweet spots&lt;/strong&gt; (especially around ~4b) where the kernels are â€œgolden path,â€ and pushing lower-bit can get weird.&lt;/p&gt; &lt;h1&gt;Request to the community ðŸ™&lt;/h1&gt; &lt;p&gt;Weâ€™d &lt;em&gt;love&lt;/em&gt; feedback and extra testing from folks here, especially if you can run:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;different llama.cpp builds / CUDA backends,&lt;/li&gt; &lt;li&gt;weird batch sizes / context lengths,&lt;/li&gt; &lt;li&gt;real workloads (coding assistants, long-form, tool-ish prompts),&lt;/li&gt; &lt;li&gt;or non-NVIDIA setups (weâ€™re aware this is where it gets spicy).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also: we heard you on the previous Reddit post and are actively working to improve our evaluation and reporting. Evaluation is currently our bottleneck, not quantization, so if you have strong opinions on what benchmarks best match real usage, weâ€™re all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ali_byteshape"&gt; /u/ali_byteshape &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/52juwyqq0rbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q5m2n6/a_30b_qwen_model_walks_into_a_raspberry_pi_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-06T15:45:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6b6u7</id>
    <title>Has anyone tested how the newest Rocm does in llms?</title>
    <updated>2026-01-07T09:43:40+00:00</updated>
    <author>
      <name>/u/Eden1506</name>
      <uri>https://old.reddit.com/user/Eden1506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6b6u7/has_anyone_tested_how_the_newest_rocm_does_in_llms/"&gt; &lt;img alt="Has anyone tested how the newest Rocm does in llms?" src="https://preview.redd.it/z3s6igomewbg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=56b025d3c97db74a9cc2754f7f890856d8f441b4" title="Has anyone tested how the newest Rocm does in llms?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using Vulkan but the newest rocm is supposed to be quite a Performance jump and wanted to know if its worth the headache to install?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eden1506"&gt; /u/Eden1506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z3s6igomewbg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6b6u7/has_anyone_tested_how_the_newest_rocm_does_in_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6b6u7/has_anyone_tested_how_the_newest_rocm_does_in_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T09:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q617ug</id>
    <title>Razer is demonstrating a â€œAI acceleratorâ€ box with a Wormhole n150 processor from Tenstorrent at CES</title>
    <updated>2026-01-07T01:07:27+00:00</updated>
    <author>
      <name>/u/Hasuto</name>
      <uri>https://old.reddit.com/user/Hasuto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"&gt; &lt;img alt="Razer is demonstrating a â€œAI acceleratorâ€ box with a Wormhole n150 processor from Tenstorrent at CES" src="https://external-preview.redd.it/55S8efCmWR_UNwVQwnrBhqr5tzC73SFwKgXTxGt7lKs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d858518876fe54114627febc2a60feabfd21a89" title="Razer is demonstrating a â€œAI acceleratorâ€ box with a Wormhole n150 processor from Tenstorrent at CES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is a press release from Tenstorrent as well, but I havenâ€™t seen anyone test it out.&lt;/p&gt; &lt;p&gt;From what Iâ€™ve seen before the hardware isnâ€™t super impressive. The n150 usually comes as a PCIe dev board with 12GB memory for $1000.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hasuto"&gt; /u/Hasuto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/razer-partners-tenstorrent-goes-into-full-ai-mode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q617ug/razer_is_demonstrating_a_ai_accelerator_box_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T01:07:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q64f26</id>
    <title>llama.cpp vs Ollama: ~70% higher code generation throughput on Qwen-3 Coder 32B (FP16)</title>
    <updated>2026-01-07T03:27:09+00:00</updated>
    <author>
      <name>/u/Shoddy_Bed3240</name>
      <uri>https://old.reddit.com/user/Shoddy_Bed3240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m seeing a significant throughput difference between &lt;strong&gt;llama.cpp&lt;/strong&gt; and &lt;strong&gt;Ollama&lt;/strong&gt; when running the same model locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;strong&gt;Qwen-3 Coder 32B&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Precision: &lt;strong&gt;FP16&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Hardware: &lt;strong&gt;RTX 5090 + RTX 3090 Ti&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Task: code generation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;llama.cpp:&lt;/strong&gt; ~52 tokens/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama:&lt;/strong&gt; ~30 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both runs use the same model weights and hardware. The gap is ~70% in favor of llama.cpp.&lt;/p&gt; &lt;p&gt;Has anyone dug into why this happens? Possibilities Iâ€™m considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;different CUDA kernels / attention implementations&lt;/li&gt; &lt;li&gt;default context or batching differences&lt;/li&gt; &lt;li&gt;scheduler or multi-GPU utilization differences&lt;/li&gt; &lt;li&gt;overhead from Ollamaâ€™s runtime / API layer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious if others have benchmarked this or know which knobs in Ollama might close the gap.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy_Bed3240"&gt; /u/Shoddy_Bed3240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q64f26/llamacpp_vs_ollama_70_higher_code_generation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q64f26/llamacpp_vs_ollama_70_higher_code_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q64f26/llamacpp_vs_ollama_70_higher_code_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T03:27:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1q61wpv</id>
    <title>NousResearch/NousCoder-14B Â· Hugging Face</title>
    <updated>2026-01-07T01:37:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"&gt; &lt;img alt="NousResearch/NousCoder-14B Â· Hugging Face" src="https://external-preview.redd.it/5B9NQ05vM8G6MB5IJCuanVCmVkz4L0DmuyDbK4fKfHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d0894b4bfc8c3fc81d8f7c59a878b6ad54d6614" title="NousResearch/NousCoder-14B Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;from NousResearch:&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce &lt;em&gt;NousCoder-14B&lt;/em&gt;, a competitive programming model post-trained on &lt;a href="https://huggingface.co/Qwen/Qwen3-14B"&gt;Qwen3-14B&lt;/a&gt; via reinforcement learning. On LiveCodeBench v6 (08/01/2024 - 05/01/2025), we achieve a Pass@1 accuracy of 67.87%, up 7.08% from the baseline Pass@1 accuracy of 60.79% of Qwen3-14B. We trained on 24k verifiable coding problems using 48 B200s over the course of four days.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/NousCoder-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q61wpv/nousresearchnouscoder14b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T01:37:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q694ic</id>
    <title>Don't put off hardware purchases: GPUs, SSDs, and RAM are going to skyrocket in price soon</title>
    <updated>2026-01-07T07:32:28+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you thought it was going to get better:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPU&lt;/strong&gt; prices are going up. &lt;a href="https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/"&gt;AMD and NVIDIA are planning to increase prices every month starting soon.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NAND flash&lt;/strong&gt; contract price &lt;a href="https://www.trendforce.com/price/flash/flash_contract"&gt;went up 20% in November&lt;/a&gt;, with &lt;a href="https://www.trendforce.com/research/download/RP251231KM"&gt;further increases in December&lt;/a&gt;. This means SSDs will be a lot more expensive soon.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DRAM&lt;/strong&gt; &lt;a href="https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/"&gt;prices are going to skyrocket&lt;/a&gt;, with no increase in production capacity and datacenters and OEMs competing for everything. &lt;/p&gt; &lt;p&gt;Even &lt;strong&gt;Consoles&lt;/strong&gt; are &lt;a href="https://insider-gaming.com/ram-prices-next-gen/"&gt;going to be delayed due to the shortages.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;According to TrendForce, conventional DRAM contract prices in 1Q26 are forecast to rise 55â€“60% quarter over quarter, while server DRAM prices are projected to surge by more than 60% QoQ. Meanwhile, NAND Flash prices are expected to increase 33â€“38% QoQ&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.trendforce.com/news/2026/01/07/news-memory-shortages-reportedly-spark-csp-buying-spree-2027-supply-contracts-eyed-as-early-as-q1/"&gt;Source.&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Industry sources cited by Kbench believe the latest price hikes will broadly affect NVIDIAâ€™s RTX 50 series and AMDâ€™s Radeon RX 9000 lineup. The outlet adds that NVIDIAâ€™s flagship GeForce RTX 5090 could see its price climb to as high as $5,000 later in 2026.&lt;/p&gt; &lt;p&gt;NVIDIA is also reportedly weighing a 30% to 40% reduction in output for parts of its midrange lineup, including the RTX 5070 and RTX 5060 Ti, according to Kbench.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;a href="https://www.trendforce.com/news/2026/01/05/news-nvidia-amd-reportedly-plan-price-hikes-starting-1q26-geforce-rtx-5090-may-reach-5000/"&gt;Source.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q694ic/dont_put_off_hardware_purchases_gpus_ssds_and_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T07:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q6c9wc</id>
    <title>DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail.</title>
    <updated>2026-01-07T10:49:12+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt; &lt;img alt="DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." src="https://b.thumbs.redditmedia.com/DUX7Mp3MJPzZfCIE1yl1lv9VDENeyLGC6ZkgyRLizOw.jpg" title="DeepSeek-R1â€™s paper was updated 2 days ago, expanding from 22 pages to 86 pages and adding a substantial amount of detail." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;arXiv:2501.12948 [cs.CL]: &lt;a href="https://arxiv.org/abs/2501.12948"&gt;https://arxiv.org/abs/2501.12948&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q6c9wc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q6c9wc/deepseekr1s_paper_was_updated_2_days_ago/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-07T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
