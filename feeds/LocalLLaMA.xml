<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-12T11:34:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o4gka5</id>
    <title>How to handle long running tools in realtime conversations.</title>
    <updated>2025-10-12T05:09:16+00:00</updated>
    <author>
      <name>/u/fajfas3</name>
      <uri>https://old.reddit.com/user/fajfas3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone. &lt;/p&gt; &lt;p&gt;I've been working on a realtime agent that has access to different tools for my client. Some of those tools might take a few seconds or even sometimes minutes to finish.&lt;/p&gt; &lt;p&gt;Because of the sequential behavior of models it just forces me to stop talking or cancels the tool call if I interrupt.&lt;/p&gt; &lt;p&gt;Did anyone here have this problem? How did you handle it?&lt;/p&gt; &lt;p&gt;I know pipecat has async tool calls done with some orchestration but I've tried this pattern and it's kinda working with gpt-5 but for any other model the replacement of tool result in the past just screws it up and it has no idea what just happened. Similarly with Claude. Gemini is the worst of them all.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fajfas3"&gt; /u/fajfas3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gka5/how_to_handle_long_running_tools_in_realtime/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gka5/how_to_handle_long_running_tools_in_realtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gka5/how_to_handle_long_running_tools_in_realtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T05:09:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mkve</id>
    <title>Local open source AI-sheets?</title>
    <updated>2025-10-12T11:24:50+00:00</updated>
    <author>
      <name>/u/SuddenWerewolf7041</name>
      <uri>https://old.reddit.com/user/SuddenWerewolf7041</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mkve/local_open_source_aisheets/"&gt; &lt;img alt="Local open source AI-sheets?" src="https://preview.redd.it/u9r363k61ouf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6801691f6a13702b411221cd785bd9fa2cb388da" title="Local open source AI-sheets?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any solution for local and open source AI that generates content based on an Excel sheet or preferably something web-based?&lt;/p&gt; &lt;p&gt;The use case is to generate content based on other column, try to fill gaps, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SuddenWerewolf7041"&gt; /u/SuddenWerewolf7041 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u9r363k61ouf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mkve/local_open_source_aisheets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mkve/local_open_source_aisheets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4dwj6</id>
    <title>Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp</title>
    <updated>2025-10-12T02:41:51+00:00</updated>
    <author>
      <name>/u/rtsov</name>
      <uri>https://old.reddit.com/user/rtsov</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dwj6/optimized_docker_image_for_unsloth_finetuning/"&gt; &lt;img alt="Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp" src="https://external-preview.redd.it/iRMYKPaKkEEDms46u7jhdX3Sb7NwqrocaVgHjkYHszM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87f388a35515f2a499f727fef68a4423768197b0" title="Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;üê≥ unsloth-docker&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Optimized Docker image for Unsloth fine-tuning + GGUF export via llama.cpp&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This Docker image seamlessly integrates &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; ‚Äî the ultra-fast LLM fine-tuning library ‚Äî with &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; to enable end-to-end training and quantized GGUF model export in a single, GPU-accelerated environment.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;‚ú® Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pre-installed Unsloth&lt;/strong&gt; with FlashAttention, xformers, and custom CUDA kernels for blazing-fast training&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full llama.cpp toolchain&lt;/strong&gt;, including &lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt; for easy GGUF conversion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jupyter Lab&lt;/strong&gt; pre-configured for interactive development&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU-accelerated&lt;/strong&gt; (CUDA 12.1 + cuDNN)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quantization-ready&lt;/strong&gt;: supports all standard GGUF quant types (&lt;code&gt;q4_k_m&lt;/code&gt;, &lt;code&gt;q5_k_m&lt;/code&gt;, &lt;code&gt;q8_0&lt;/code&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;üöÄ Quick Start&lt;/h2&gt; &lt;h3&gt;1. Build &amp;amp; Launch&lt;/h3&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Build the image&lt;/h1&gt; &lt;p&gt;docker compose build&lt;/p&gt; &lt;h1&gt;Start the container (Jupyter Lab runs on port 38888)&lt;/h1&gt; &lt;p&gt;docker compose up -d ```&lt;/p&gt; &lt;h3&gt;2. Access Jupyter Lab&lt;/h3&gt; &lt;p&gt;Open your browser at &lt;strong&gt;&lt;a href="http://127.0.0.1:38888"&gt;http://127.0.0.1:38888&lt;/a&gt;&lt;/strong&gt; and log in with your password.&lt;/p&gt; &lt;p&gt;Create a new notebook to fine-tune your model using Unsloth.&lt;/p&gt; &lt;p&gt;After training, save and convert your model directly inside the notebook:&lt;/p&gt; &lt;p&gt;```python&lt;/p&gt; &lt;h1&gt;Save merged model (Unsloth syntax)&lt;/h1&gt; &lt;p&gt;model.save_pretrained_merged(&amp;quot;your-new-model&amp;quot;, tokenizer)&lt;/p&gt; &lt;h1&gt;Convert to GGUF using pre-installed llama.cpp&lt;/h1&gt; &lt;p&gt;!python /workspace/llama.cpp/convert_hf_to_gguf.py \ --outfile your-new-model-gguf \ --outtype q8_0 \ your-new-model ``` &lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Train fast. Quantize smarter. Run anywhere. üöÄ&lt;/p&gt; &lt;p&gt;üëâ &lt;strong&gt;Star the repo if you find it useful!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/covrom/unsloth-docker"&gt;https://github.com/covrom/unsloth-docker&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rtsov"&gt; /u/rtsov &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/covrom/unsloth-docker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dwj6/optimized_docker_image_for_unsloth_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dwj6/optimized_docker_image_for_unsloth_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:41:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mmah</id>
    <title>embedding model which is good for short term tokens?</title>
    <updated>2025-10-12T11:27:06+00:00</updated>
    <author>
      <name>/u/emaayan</name>
      <uri>https://old.reddit.com/user/emaayan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi, i'm trying to rag on xml document inorder to be able to ask it several questions about various parts of the document which can be connected semanticaly.&lt;/p&gt; &lt;p&gt;the idea is that instead of actuallly taking the tags, flatten them so instead of xml&lt;br /&gt; you'll have&lt;br /&gt; a.b.c.=2328&lt;/p&gt; &lt;p&gt;the thing is some of those xml tags represents entities with odd names such ab-23-north and mk-28-clean &lt;/p&gt; &lt;p&gt;so i would like to pose a query such as what's the distacne between entity ab-23-north and mk-28-clean as they both have x,y coordinates, note that not all entities have such identifications, and not all entities are the same type either, you could have antoher entity which is a polygon) &lt;/p&gt; &lt;p&gt;i could ask for example how many entities inside polygon x, etc... do certain entities have properties configured properly based on some knowledge document someone wrote, etc.. &lt;/p&gt; &lt;p&gt;so i'm wondering what type of embedding is good for this. &lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;a&amp;gt; &amp;lt;b&amp;gt; &amp;lt;c&amp;gt;23ssd&amp;lt;/c&amp;gt; &amp;lt;/b&amp;gt; &amp;lt;/a&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emaayan"&gt; /u/emaayan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mmah/embedding_model_which_is_good_for_short_term/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mmah/embedding_model_which_is_good_for_short_term/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mmah/embedding_model_which_is_good_for_short_term/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4hqm7</id>
    <title>Good balance between RP and instructions</title>
    <updated>2025-10-12T06:20:22+00:00</updated>
    <author>
      <name>/u/GarmrNL</name>
      <uri>https://old.reddit.com/user/GarmrNL</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôve been playing for a while with several LLMs for a project I‚Äôm working on that requires the LLM to: - Follow instructions regarding text output (mainly things like adding BBCode that require opening/closing tags) - Ability to read JSON in messages correctly - Be decent at creating vivid descriptions of locations, engaging conversations while still respecting some form of scope boundaries.&lt;/p&gt; &lt;p&gt;Some context about the project; I‚Äôm aiming to create an interactive experience that puts the user in charge of running an alchemy shop. It‚Äôs basically inventory management with dynamic conversations :-)&lt;/p&gt; &lt;p&gt;I tried a few LLMs: - Qwen3 instruct: very good instruction wise, but I feel it lacks something - Shteno: Very good roleplaying, bad at instructions (when asking it, it told me it ‚Äúglances over‚Äù instructions like the ones I need) - Claude: Pretty good, but it started doing its own thing and disregarded my instructions.&lt;/p&gt; &lt;p&gt;This project started off as an experiment a few weeks ago but snowballed into something I‚Äôd like to finish; most parts are finished (player can talk to multiple unique characters running their own prompts, moving between locations works, characters can move between locations, drilling down items for exploring items). I‚Äôm using Qwen3-4B instruct right now and while that works pretty smooth, I‚Äôm missing the ‚Äúcozy‚Äù descriptions/details Shteno came up with. &lt;/p&gt; &lt;p&gt;As a newcomer in the world of LLMs there‚Äôs way too many and I was hoping someone here could guide me to some LLMs I could try that would fit my requirements?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GarmrNL"&gt; /u/GarmrNL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hqm7/good_balance_between_rp_and_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hqm7/good_balance_between_rp_and_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hqm7/good_balance_between_rp_and_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T06:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o42ch4</id>
    <title>Choosing a code completion (FIM) model</title>
    <updated>2025-10-11T18:03:11+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fill-in-the-middle (FIM) models don't necessarily get all of the attention that coder models get but they work great with llama.cpp and &lt;a href="https://github.com/ggml-org/llama.vim"&gt;llama.vim&lt;/a&gt; or &lt;a href="https://github.com/ggml-org/llama.vscode"&gt;llama.vscode&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Generally, when picking an FIM model, &lt;strong&gt;&lt;em&gt;speed&lt;/em&gt;&lt;/strong&gt; is absolute priority because no one wants to sit waiting for the completion to finish. Choosing models with few active parameters and running GPU only is key. Also, counterintuitively, &amp;quot;base&amp;quot; models work just as well as instruct models. Try to aim for &amp;gt;70 t/s.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Note that only some models support FIM.&lt;/em&gt;&lt;/strong&gt; Sometimes, it can be hard to tell from model cards whether they are supported or not.&lt;/p&gt; &lt;p&gt;Recent models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct"&gt;Qwen/Qwen3-Coder-30B-A3B-Instruct&lt;/a&gt; (the larger variant might also be FIM, I don't have the hardware to try it)&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-23B-A4B-v1"&gt;Kwaipilot/KwaiCoder-23B-A4B-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Kwaipilot/KwaiCoder-DS-V2-Lite-Base"&gt;Kwaipilot/KwaiCoder-DS-V2-Lite-Base&lt;/a&gt; (16b 2.4b active)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Slightly older but reliable small models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-3B"&gt;Qwen/Qwen2.5-Coder-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Qwen/Qwen2.5-Coder-1.5B"&gt;Qwen/Qwen2.5-Coder-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Untested, new models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/Salesforce/CoDA-v0-Instruct"&gt;Salesforce/CoDA-v0-Instruct&lt;/a&gt; (I'm unsure if this is FIM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What models am I missing? What models are you using?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o42ch4/choosing_a_code_completion_fim_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:03:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o48x07</id>
    <title>What is the most you can do to scale the inference of a model? Specifically looking for lesser known tricks and optimization you have found while tinkering with models</title>
    <updated>2025-10-11T22:38:20+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Scenario: Assuming I have the Phi 4 14b model hosted on a A100 40GB machine, and I can run it for a single data. If i have 1 million legal text documents, what is the best way to scale the inference such that I can process the 1 million text (4000 million words) and extract information out of it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o48x07/what_is_the_most_you_can_do_to_scale_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o48x07/what_is_the_most_you_can_do_to_scale_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o48x07/what_is_the_most_you_can_do_to_scale_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T22:38:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o47di4</id>
    <title>Running a large model overnight in RAM, use cases?</title>
    <updated>2025-10-11T21:30:21+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a 3945wx with 512gb of ddr4 2666mhz. Work is tossing out a few old servers so I am getting my hands on 1TB of ram for free. I have 2x3090 currently.&lt;/p&gt; &lt;p&gt;But was thinking of doing some scraping and analysis, particularly for stocks. My pricing goes to 7p per kw overnight and was thinking of using a night model in RAM that is slow, but fast and using the GPUs during the day. &lt;/p&gt; &lt;p&gt;Surely I‚Äôm not the only one who has thought about this? &lt;/p&gt; &lt;p&gt;Perplexity has started to throttle labs queries so this could be my replacement for deep research. It might be slow, but it will be cheaper than a GPU furnace!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o47di4/running_a_large_model_overnight_in_ram_use_cases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o47di4/running_a_large_model_overnight_in_ram_use_cases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o47di4/running_a_large_model_overnight_in_ram_use_cases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T21:30:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ahfi</id>
    <title>How do you discover &amp; choose right models for your agents? (genuinely curious)</title>
    <updated>2025-10-11T23:49:59+00:00</updated>
    <author>
      <name>/u/Curious-Engineer22</name>
      <uri>https://old.reddit.com/user/Curious-Engineer22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how people actually find the right model for their use case.&lt;/p&gt; &lt;p&gt;If you've recently picked a model for a project, how did you do it?&lt;/p&gt; &lt;p&gt;A few specific questions: 1. Where did you start your search? (HF search, Reddit, benchmarks, etc.) 2. How long did it take? (minutes, hours, days?) 3. What factors mattered most? (accuracy, speed, size?) 4. Did you test multiple models or commit to one? 5. How confident were you in your choice?&lt;/p&gt; &lt;p&gt;Also curious: what would make this process easier?&lt;/p&gt; &lt;p&gt;My hypothesis is that most of us are winging it more than we'd like to admit. Would love to hear if others feel the same way or if I'm just doing it wrong!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Curious-Engineer22"&gt; /u/Curious-Engineer22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ahfi/how_do_you_discover_choose_right_models_for_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ahfi/how_do_you_discover_choose_right_models_for_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ahfi/how_do_you_discover_choose_right_models_for_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T23:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4gfy0</id>
    <title>Recently started to dabble in LocalLLMs...</title>
    <updated>2025-10-12T05:02:03+00:00</updated>
    <author>
      <name>/u/Asbular</name>
      <uri>https://old.reddit.com/user/Asbular</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had an android powered ToughPad (3gb ram) that I had laying around so got it set up and running an uncensored Llama 3.2 1b as a off-grid mobile, albeit rather limited LLM option &lt;/p&gt; &lt;p&gt;But naturally I wanted more, so working with what I had spare, I set up a headless windows 11 box running Ollama and LM Studio, that I remote desktop into via RustDesk from my Android and Windows devices inorder to use the GUIs&lt;/p&gt; &lt;p&gt;System specs:&lt;/p&gt; &lt;p&gt;i7 4770K (Running at 3000mhz) 16gb DDR3 RAM (Running at 2200mhz) GTX 1070 8gb &lt;/p&gt; &lt;p&gt;I have got it up and running, managed to get the Wake on Lan working correctly, so that It sleeps when not being used, I just need to use an additional program to ping the PC prior to RD Connection&lt;/p&gt; &lt;p&gt;The current setup can run the following models at the speeds shown below: (Prompt &amp;quot;Hi&amp;quot;)&lt;/p&gt; &lt;p&gt;Gemma 4b 23.21 tok/sec (43 tokens) Gemma 12b 8.03 tok/sec (16tokens)&lt;/p&gt; &lt;p&gt;I have a couple of questions&lt;/p&gt; &lt;p&gt;I can perform a couple of upgrades to this systems for a low price in just wondering would they be worth it&lt;/p&gt; &lt;p&gt;I can double the ram to 32gb for around ¬£15 I can pick up an additional GTX 1070 8gb for around ¬£60. &lt;/p&gt; &lt;p&gt;If I doubled my RAM to 32gb and VRAM to 16gb and I can currently just about run a 12b model what can I likely expect to see? &lt;/p&gt; &lt;p&gt;Can Ollama and LM Studio (and Open WebUI) utilize and take advantage of 2 GPUs and if so would I need the SLI connector? &lt;/p&gt; &lt;p&gt;And finally does CPU speed or core count or even ram speed matter at all when offloading 100% of the model to the GPU?. This very old (2014) 4 core 8 thread CPU runs stable at 4.6ghz overclock, but is currently underclocked to 3.0 GHz (from 3.5ghz stock&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Asbular"&gt; /u/Asbular &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gfy0/recently_started_to_dabble_in_localllms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gfy0/recently_started_to_dabble_in_localllms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gfy0/recently_started_to_dabble_in_localllms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T05:02:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4m71e</id>
    <title>Help with RTX6000 Pros and vllm</title>
    <updated>2025-10-12T11:02:48+00:00</updated>
    <author>
      <name>/u/TaiMaiShu-71</name>
      <uri>https://old.reddit.com/user/TaiMaiShu-71</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So at work we were able to scrape together the funds to get a server with 6 x RTX 6000 Pro Blackwell server editions, and I want to setup vLLM running in a container. I know support for the card is still maturing, I've tried several different posts claiming someone got it working, but I'm struggling. Fresh Ubuntu 24.04 server, cuda 13 update 2, nightly build of pytorch for cuda 13, 580.95 driver. I'm compiling vLLM specifically for sm120. The cards show up running Nvidia-smi both in and out of the container, but vLLM doesn't see them when I try to load a model. I do see some trace evidence in the logs of a reference to sm100 for some components. Does anyone have a solid dockerfile or build process that has worked in a similar environment? I've spent two days on this so far so any hints would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TaiMaiShu-71"&gt; /u/TaiMaiShu-71 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m71e/help_with_rtx6000_pros_and_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m71e/help_with_rtx6000_pros_and_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m71e/help_with_rtx6000_pros_and_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4gi5j</id>
    <title>sm120 - is like everything gated? (Pre-training my own)</title>
    <updated>2025-10-12T05:05:42+00:00</updated>
    <author>
      <name>/u/exhorder72</name>
      <uri>https://old.reddit.com/user/exhorder72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me say that I‚Äôm new to this whole world of lm training and I‚Äôve pretty much learned as I go. For a couple weeks now I‚Äôve been working on a 1.8b param model just chugging along in pre training. I‚Äôve done many a search for a better, more effective strat. Things I read about such as FA2/3, MXFP8/4, some Hopper stuff all seems gated. I set up a nightly torchao build in another venv and getting blocked all around. I mean, sm120 been out for some time, right? Here‚Äôs the most stable I‚Äôve come up with to date. If anyone has any advice to share, I would love to hear it:&lt;/p&gt; &lt;p&gt;Ubuntu 22.04 (WSL2 on Win 11) PyTorch 2.8 + CUDA 12.8 / 13.0 drivers (5090 32gb) Transformer Engine 2.8 FP8 linears active cudaMallocAsync allocator enabled Doc-aware SDPA attention (efficient path, flash off) TE RMSNorm swap (+15 % throughput vs baseline) AdamW fused, D2Z LR schedule Training data ‚âà 20 B tokens Nemotron HQ mixed with some Nemo Math, The Stack V2 and 2025 Wikipedia.&lt;/p&gt; &lt;p&gt;15 k tokens/s steady @ batch 4 √ó grad-accum 6, ctx = 2048, loss ‚âà 0.7 ‚Üí 0.5 about 10b tokens chewed on. Had a bad 30k run because for whatever reason I had one or both embed.weight and lm_head.weight tensors blow up on me and since I had them tied, that was a bad day. Since then, smooth sailing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exhorder72"&gt; /u/exhorder72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gi5j/sm120_is_like_everything_gated_pretraining_my_own/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gi5j/sm120_is_like_everything_gated_pretraining_my_own/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4gi5j/sm120_is_like_everything_gated_pretraining_my_own/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T05:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4kyo9</id>
    <title>Deep Think with Confidence</title>
    <updated>2025-10-12T09:46:59+00:00</updated>
    <author>
      <name>/u/Temporary-Roof2867</name>
      <uri>https://old.reddit.com/user/Temporary-Roof2867</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, sorry if my English is terrible üôè&lt;/p&gt; &lt;p&gt;Do you know of any models that implement these functions?&lt;/p&gt; &lt;p&gt;üëá&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2508.15260"&gt;https://arxiv.org/abs/2508.15260&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I found this but I'm having a lot of trouble installing it.&lt;/p&gt; &lt;p&gt;üëá&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/deepconf"&gt;https://github.com/facebookresearch/deepconf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëá&lt;/p&gt; &lt;p&gt;&lt;a href="https://jiaweizzhao.github.io/deepconf/"&gt;https://jiaweizzhao.github.io/deepconf/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary-Roof2867"&gt; /u/Temporary-Roof2867 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4kyo9/deep_think_with_confidence/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4kyo9/deep_think_with_confidence/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4kyo9/deep_think_with_confidence/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T09:46:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4im2n</id>
    <title>I built an open-source repo to learn and apply AI Agentic Patterns</title>
    <updated>2025-10-12T07:15:09+00:00</updated>
    <author>
      <name>/u/learnwithparam</name>
      <uri>https://old.reddit.com/user/learnwithparam</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with how AI agents actually &lt;em&gt;work in production&lt;/em&gt; ‚Äî beyond simple prompt chaining. So I created an &lt;strong&gt;open-source project&lt;/strong&gt; that demonstrates &lt;strong&gt;30+ AI Agentic Patterns&lt;/strong&gt;, each in a single, focused file.&lt;/p&gt; &lt;p&gt;Each pattern covers a core concept like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt Chaining&lt;/li&gt; &lt;li&gt;Multi-Agent Coordination&lt;/li&gt; &lt;li&gt;Reflection &amp;amp; Self-Correction&lt;/li&gt; &lt;li&gt;Knowledge Retrieval&lt;/li&gt; &lt;li&gt;Workflow Orchestration&lt;/li&gt; &lt;li&gt;Exception Handling&lt;/li&gt; &lt;li&gt;Human-in-the-loop&lt;/li&gt; &lt;li&gt;And more advanced ones like Recursive Agents &amp;amp; Code Execution&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚úÖ Works with OpenAI, Gemini, Claude, Fireworks AI, Mistral, and even &lt;strong&gt;Ollama&lt;/strong&gt; for local runs.&lt;br /&gt; ‚úÖ Each file is self-contained ‚Äî perfect for learning or extending.&lt;br /&gt; ‚úÖ Open for contributions, feedback, and improvements!&lt;/p&gt; &lt;p&gt;You can check the full list and examples in the README here:&lt;br /&gt; üîó &lt;a href="https://github.com/learnwithparam/ai-agents-pattern"&gt;https://github.com/learnwithparam/ai-agents-pattern&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love your feedback ‚Äî especially on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Missing patterns worth adding&lt;/li&gt; &lt;li&gt;Ways to make it more beginner-friendly&lt;/li&gt; &lt;li&gt;Real-world examples to expand&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let‚Äôs make AI agent design patterns as clear and reusable as software design patterns once were.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/learnwithparam"&gt; /u/learnwithparam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4im2n/i_built_an_opensource_repo_to_learn_and_apply_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4im2n/i_built_an_opensource_repo_to_learn_and_apply_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4im2n/i_built_an_opensource_repo_to_learn_and_apply_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T07:15:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4ez13</id>
    <title>I made a plugin to run LLMs on phones</title>
    <updated>2025-10-12T03:39:04+00:00</updated>
    <author>
      <name>/u/Dragneel_passingby</name>
      <uri>https://old.reddit.com/user/Dragneel_passingby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I've been working on a side project to get LLMs (GGUF models) running locally on Android devices using Flutter.&lt;/p&gt; &lt;p&gt;The result is a plugin I'm calling Llama Flutter. It uses llama.cpp under the hood and lets you load any GGUF model from Hugging Face. I built a simple chat app as an example to test it.&lt;/p&gt; &lt;p&gt;I'm sharing this here because I'm looking for feedback from the community. Has anyone else tried building something similar? I'd be curious to know your thoughts on the approach, or any suggestions for improvement.&lt;/p&gt; &lt;p&gt;Video Demo: &lt;a href="https://files.catbox.moe/xrqsq2.mp4"&gt;https://files.catbox.moe/xrqsq2.mp4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example APK: &lt;a href="https://github.com/dragneel2074/Llama-Flutter/blob/master/example-app/app-release.apk"&gt;https://github.com/dragneel2074/Llama-Flutter/blob/master/example-app/app-release.apk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here are some of the technical details / features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt; Uses the latest llama.cpp (as of Oct 2025) with ARM64 optimizations.&lt;/li&gt; &lt;li&gt; Provides a simple Dart API with real-time token streaming.&lt;/li&gt; &lt;li&gt; Supports a good range of generation parameters and several built-in chat templates.&lt;/li&gt; &lt;li&gt; For now, it's Android-only and focused on text generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you're interested in checking it out to provide feedback or contribute, the links are below. If you find it useful, a star on GitHub would help me gauge interest.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;* GitHub Repo: &lt;a href="https://github.com/dragneel2074/Llama-Flutter"&gt;https://github.com/dragneel2074/Llama-Flutter&lt;/a&gt;&lt;/p&gt; &lt;p&gt;* Plugin on pub.dev: &lt;a href="https://pub.dev/packages/llama%5C_flutter%5C_android"&gt;https://pub.dev/packages/llama\_flutter\_android&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? Is local execution of LLMs on mobile something you see a future for in Flutter?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dragneel_passingby"&gt; /u/Dragneel_passingby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4ez13/i_made_a_plugin_to_run_llms_on_phones/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T03:39:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4av71</id>
    <title>LLM-JEPA: Large Language Models Meet Joint Embedding Predictive Architectures</title>
    <updated>2025-10-12T00:08:28+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Model (LLM) pretraining, finetuning, and evaluation rely on input-space reconstruction and generative capabilities. Yet, it has been observed in vision that embedding-space training objectives, e.g., with Joint Embedding Predictive Architectures (JEPAs), are far superior to their input-space counterpart. That mismatch in how training is achieved between language and vision opens up a natural question: {\em can language training methods learn a few tricks from the vision ones?} The lack of JEPA-style LLM is a testimony of the challenge in designing such objectives for language. In this work, we propose a first step in that direction where we develop LLM-JEPA, a JEPA based solution for LLMs applicable both to finetuning and pretraining. Thus far, LLM-JEPA is able to outperform the standard LLM training objectives by a significant margin across models, all while being robust to overfiting. Those findings are observed across numerous datasets (NL-RX, GSM8K, Spider, RottenTomatoes) and various models from the Llama3, OpenELM, Gemma2 and Olmo families. Code: &lt;a href="https://github.com/rbalestr-lab/llm-jepa"&gt;this https URL&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Limitations &lt;/p&gt; &lt;p&gt;Despite its strong accuracy gains, LLM-JEPA introduces two additional hyperparameters. As shown in fig. 7, the optimal configuration may occur at any point in a grid (Œª, k), which imposes a significant cost for hyperparameter tuning. While we have not identified an efficient method to explore this space, we empirically observe that adjacent grid points often yield similar accuracy, suggesting the potential for a more efficient tuning algorithm.&lt;/p&gt; &lt;p&gt;The primary bottleneck at present is the 2-fold increase in compute cost during training, which is mitigated by random loss dropout.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.14252"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4av71/llmjepa_large_language_models_meet_joint/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4av71/llmjepa_large_language_models_meet_joint/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T00:08:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o44u78</id>
    <title>We know the rule of thumb‚Ä¶ large quantized models outperform smaller less quantized models, but is there a level where that breaks down?</title>
    <updated>2025-10-11T19:43:54+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ask because I‚Äôve also heard quants below 4 bit are less effective, and that rule of thumb always seemed to compare 4bit large vs 8bit small.&lt;/p&gt; &lt;p&gt;As an example let‚Äôs take the large GLM 4.5 vs GLM 4.5 Air. You can have a much higher bitrate with GLM 4.5 Air‚Ä¶ but‚Ä¶ even with a 2bit quant made by unsloth, GLM 4.5 does quite well for me. &lt;/p&gt; &lt;p&gt;I haven‚Äôt figured out a great way to have complete confidence though so I thought I‚Äôd ask you all. What‚Äôs your rule of thumb when having to weigh a smaller model vs larger model at different quants? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o44u78/we_know_the_rule_of_thumb_large_quantized_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T19:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1o3opq5</id>
    <title>What the sub feels like lately</title>
    <updated>2025-10-11T06:47:33+00:00</updated>
    <author>
      <name>/u/marderbot13</name>
      <uri>https://old.reddit.com/user/marderbot13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt; &lt;img alt="What the sub feels like lately" src="https://preview.redd.it/92s8znbxifuf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4866bff0d572386ea47fc19d643a6b2261fbdb" title="What the sub feels like lately" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marderbot13"&gt; /u/marderbot13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/92s8znbxifuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o3opq5/what_the_sub_feels_like_lately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T06:47:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1o43qhn</id>
    <title>What rig are you running to fuel your LLM addiction?</title>
    <updated>2025-10-11T18:59:15+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post your shitboxes, H100's, nvidya 3080ti's, RAM-only setups, MI300X's, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o43qhn/what_rig_are_you_running_to_fuel_your_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-11T18:59:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4l7qk</id>
    <title>I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus</title>
    <updated>2025-10-12T10:02:47+00:00</updated>
    <author>
      <name>/u/alone_musk18</name>
      <uri>https://old.reddit.com/user/alone_musk18</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"&gt; &lt;img alt="I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus" src="https://preview.redd.it/1rpwzkgqmnuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afa5a5bad4ea1892710d6879e8e3673370dee73e" title="I have an interview scheduled after 2 days from now and I'm hoping to get a few suggestions on how to best prepare myself to crack it. These are the possible topics which will have higher focus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alone_musk18"&gt; /u/alone_musk18 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1rpwzkgqmnuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4l7qk/i_have_an_interview_scheduled_after_2_days_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T10:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4m7yt</id>
    <title>LM Studio no new runtimes since weeks..?</title>
    <updated>2025-10-12T11:04:23+00:00</updated>
    <author>
      <name>/u/therealAtten</name>
      <uri>https://old.reddit.com/user/therealAtten</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pardon the hyperbole and sorry to bother, but since the release of GLM-4.6 on Oct. 30 (that's fourteen days, or two weeks ago), I have been checking daily on LM Studio whether new Runtimes are provided to finally run the successsor to my favourite model, GLM-4.5. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw4sv6/unsloth_glm46_gguf_doesnt_work_in_lm_studio/"&gt;I was told&lt;/a&gt; their current runtime v1.52.1 is based on llama.cpp's b6651, with b6653 (just two releases later) adding support for GLM-4.6. Meanwhile as of writing, llama.cpp is on release b6739.&lt;/p&gt; &lt;p&gt;@ LM Studio, thank you so much for your &lt;em&gt;amazing&lt;/em&gt; platform, and sorry that we cannot contribute to your incessant efforts in proliferating Local LLMs. (obligatory &amp;quot;open-source when?&amp;quot;)&lt;br /&gt; I sincerely hope you are doing alright...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/therealAtten"&gt; /u/therealAtten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4m7yt/lm_studio_no_new_runtimes_since_weeks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4mdkh</id>
    <title>Why has Meta research failed to deliver foundational model at the level of Grok, Deepseek or GLM?</title>
    <updated>2025-10-12T11:13:22+00:00</updated>
    <author>
      <name>/u/External_Natural9590</name>
      <uri>https://old.reddit.com/user/External_Natural9590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They have been in the space for longer - could have atracted talent earlier, their means are comparable to ther big tech. So why have they been outcompeted so heavily? I get they are currently a one generation behind and the chinese did some really clever wizardry which allowed them to squeeze a lot more eke out of every iota. But what about xAI? They compete for the same talent and had to start from the scratch. Or was starting from the scratch actually an advantage here? Or is it just a matter of how many key ex OpenAI employees was each company capable of attracting - trafficking out the trade secrets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Natural9590"&gt; /u/External_Natural9590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4mdkh/why_has_meta_research_failed_to_deliver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T11:13:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4fxer</id>
    <title>PSA: Ollama no longer supports the Mi50 or Mi60</title>
    <updated>2025-10-12T04:32:11+00:00</updated>
    <author>
      <name>/u/TechEnthusiastx86</name>
      <uri>https://old.reddit.com/user/TechEnthusiastx86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ollama/ollama/pull/12481"&gt;https://github.com/ollama/ollama/pull/12481&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ollama recently upgraded its ROCM version and therefore no longer supports the Mi50 or Mi60.&lt;/p&gt; &lt;p&gt;Their most recent release notes states that &amp;quot;AMD gfx900 and gfx906 (MI50, MI60, etc) GPUs are no longer supported via ROCm. We're working to support these GPUs via Vulkan in a future release.&amp;quot; &lt;/p&gt; &lt;p&gt;This means if you pull the latest version of Ollama you won't be able to use the Mi50 even though Ollama docs still list it as being supported.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechEnthusiastx86"&gt; /u/TechEnthusiastx86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4fxer/psa_ollama_no_longer_supports_the_mi50_or_mi60/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T04:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4hxqe</id>
    <title>KoboldCpp now supports video generation</title>
    <updated>2025-10-12T06:32:40+00:00</updated>
    <author>
      <name>/u/fish312</name>
      <uri>https://old.reddit.com/user/fish312</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"&gt; &lt;img alt="KoboldCpp now supports video generation" src="https://external-preview.redd.it/n7QpKCCkcHUBLj4nC-Lh95amFG6mzdqatT5L5ZA_y1k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f4fc22015a57b49dec0643ef6f0d2a92f83c37" title="KoboldCpp now supports video generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fish312"&gt; /u/fish312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/LostRuins/koboldcpp/releases/latest"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4hxqe/koboldcpp_now_supports_video_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T06:32:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1o4dswr</id>
    <title>HuggingFace storage is no longer unlimited - 12TB public storage max</title>
    <updated>2025-10-12T02:36:39+00:00</updated>
    <author>
      <name>/u/Thireus</name>
      <uri>https://old.reddit.com/user/Thireus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In case you‚Äôve missed the memo like me, HuggingFace is no longer unlimited.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Type of account&lt;/th&gt; &lt;th&gt;Public storage&lt;/th&gt; &lt;th&gt;Private storage&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Free user or org&lt;/td&gt; &lt;td&gt;Best-effort* usually up to 5 TB for impactful work&lt;/td&gt; &lt;td&gt;100 GB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;PRO&lt;/td&gt; &lt;td&gt;Up to 10 TB included* ‚úÖ grants available for impactful work‚Ä†&lt;/td&gt; &lt;td&gt;1 TB + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Team Organizations&lt;/td&gt; &lt;td&gt;12 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Enterprise Organizations&lt;/td&gt; &lt;td&gt;500 TB base + 1 TB per seat&lt;/td&gt; &lt;td&gt;1 TB per seat + pay-as-you-go&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As seen on &lt;a href="https://huggingface.co/docs/hub/en/storage-limits"&gt;https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And yes, they started enforcing it.&lt;/p&gt; &lt;p&gt;‚Äî-&lt;/p&gt; &lt;p&gt;For ref. &lt;a href="https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits"&gt;https://web.archive.org/web/20250721230314/https://huggingface.co/docs/hub/en/storage-limits&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thireus"&gt; /u/Thireus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o4dswr/huggingface_storage_is_no_longer_unlimited_12tb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-12T02:36:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
