<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-21T12:13:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ralqm0</id>
    <title>I benchmarked PaddleOCR-VL 1.5 vs Marker vs PP-StructureV3 for PDF-to-Markdown on Modal (T4, A10G, L4) ‚Äî here's what I found</title>
    <updated>2026-02-21T08:16:31+00:00</updated>
    <author>
      <name>/u/Various_Hour_9857</name>
      <uri>https://old.reddit.com/user/Various_Hour_9857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Tested 3 PDF-to-Markdown tools on the same 15-page paper. PaddleOCR-VL: 7 min (slow, painful setup). Marker: 54s (best quality, easy setup). PP-StructureV3 lightweight: 26s (fastest, best math, but jumbles reading order). For most people: just use the Datalab API ($25/mo free credit).&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Spent a full day testing every PDF-to-markdown tool I could get running on Modal's serverless GPUs. Ran them all on the same document ‚Äî the &amp;quot;Attention Is All You Need&amp;quot; paper (15 pages, math-heavy, tables, figures, multi-column layout). Here are the real numbers, not cherry-picked benchmarks.&lt;/p&gt; &lt;h2&gt;The Contenders&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;PaddleOCR-VL 1.5&lt;/strong&gt; ‚Äî 0.9B VLM-based approach (autoregressive generation per element)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PP-StructureV3&lt;/strong&gt; ‚Äî Traditional multi-model pipeline from the same PaddleOCR project (layout det + OCR + table rec + formula rec)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PP-StructureV3 Lightweight&lt;/strong&gt; ‚Äî Same pipeline but with mobile OCR models + PP-FormulaNet_plus-M&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Marker&lt;/strong&gt; (datalab-to) ‚Äî PyTorch-based, built on Surya OCR&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Speed Results (same 15-page paper, warm container)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Tool&lt;/th&gt; &lt;th&gt;T4&lt;/th&gt; &lt;th&gt;A10G&lt;/th&gt; &lt;th&gt;L4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;PaddleOCR-VL 1.5&lt;/td&gt; &lt;td&gt;7 min&lt;/td&gt; &lt;td&gt;5.3 min&lt;/td&gt; &lt;td&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;PP-StructureV3 (default)&lt;/td&gt; &lt;td&gt;‚Äî&lt;/td&gt; &lt;td&gt;51.3s&lt;/td&gt; &lt;td&gt;‚Äî&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;PP-StructureV3 (lightweight)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;‚Äî&lt;/td&gt; &lt;td&gt;&lt;strong&gt;26.2s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;31.7s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Marker&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;3.2 min&lt;/td&gt; &lt;td&gt;&lt;strong&gt;54.0s&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;~70s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;PP-StructureV3 lightweight is the speed king at 1.7s/page on A10G. Marker is roughly 2x slower but still very good.&lt;/p&gt; &lt;h2&gt;Quality Comparison&lt;/h2&gt; &lt;p&gt;This is where it gets interesting. Speed doesn't matter if the output is garbage.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Math/LaTeX:&lt;/strong&gt; - StructureV3: Wraps everything in proper &lt;code&gt;$...$&lt;/code&gt; and &lt;code&gt;$$...$$&lt;/code&gt;. Even inline math like &lt;code&gt;W_i^Q ‚àà R^{d_model √ó d_k}&lt;/code&gt; comes out as proper LaTeX. Has a cosmetic issue with letter-spacing in &lt;code&gt;\operatorname&lt;/code&gt; but renders correctly. - Marker: Block equations are mostly fine, but inline math frequently degrades to plain text. &lt;code&gt;W Q i ‚àà R dmodel√ódk&lt;/code&gt; ‚Äî completely unreadable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tables:&lt;/strong&gt; - StructureV3: Outputs HTML &lt;code&gt;&amp;lt;table&amp;gt;&lt;/code&gt; tags. Works but ugly in raw markdown. Complex tables (like the model variations table) get messy. - Marker: Clean markdown pipe tables. Handles complex table structures better.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reading Order (THE BIG ONE):&lt;/strong&gt; - StructureV3: &lt;strong&gt;Jumbles the page order.&lt;/strong&gt; References and appendix figures appeared on pages 3-4 before the main body content. This is a dealbreaker for many use cases. - Marker: Perfect reading order throughout.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Completeness:&lt;/strong&gt; - StructureV3: Misses footnotes, author contribution notes, equation numbers. - Marker: Captures everything ‚Äî footnotes, equation numbers, clickable cross-references with anchor links.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Surprising finding:&lt;/strong&gt; The lightweight config produced BETTER OCR accuracy than the default. The default had errors like &lt;code&gt;&amp;quot;English-to-Grman&amp;quot;&lt;/code&gt;, &lt;code&gt;&amp;quot;self-atention&amp;quot;&lt;/code&gt;, and misread Figure 4 as a garbled HTML table. Lightweight had none of these issues. Heavier model ‚â† better output.&lt;/p&gt; &lt;h2&gt;Cost Breakdown&lt;/h2&gt; &lt;p&gt;Modal GPU pricing and what each run actually costs:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Tool + GPU&lt;/th&gt; &lt;th&gt;Warm time&lt;/th&gt; &lt;th&gt;GPU $/hr&lt;/th&gt; &lt;th&gt;Cost per run&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;SV3 Lightweight + L4&lt;/td&gt; &lt;td&gt;31.7s&lt;/td&gt; &lt;td&gt;$0.73&lt;/td&gt; &lt;td&gt;&lt;strong&gt;$0.006&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;SV3 Lightweight + A10G&lt;/td&gt; &lt;td&gt;26.2s&lt;/td&gt; &lt;td&gt;$1.10&lt;/td&gt; &lt;td&gt;$0.008&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Marker + A10G&lt;/td&gt; &lt;td&gt;54.0s&lt;/td&gt; &lt;td&gt;$1.10&lt;/td&gt; &lt;td&gt;$0.016&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;PaddleOCR-VL + A10G&lt;/td&gt; &lt;td&gt;5.3 min&lt;/td&gt; &lt;td&gt;$1.10&lt;/td&gt; &lt;td&gt;$0.097&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;vs. &lt;strong&gt;Datalab API&lt;/strong&gt; (Marker's hosted service): $4/1000 pages = $0.06 for 15 pages. They also give you $25 free credit/month (6,250 pages free).&lt;/p&gt; &lt;h2&gt;Setup Pain&lt;/h2&gt; &lt;p&gt;This matters. A lot.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PaddleOCR-VL / StructureV3:&lt;/strong&gt; - PaddlePaddle must be installed from a special Chinese mirror URL (not on PyPI properly) - &lt;code&gt;paddlepaddle-gpu&lt;/code&gt; segfaults on CPU during image build ‚Äî need GPU attached to build step - numpy 2.x breaks inference with cryptic &lt;code&gt;&amp;quot;only 0-dimensional arrays can be converted to Python scalars&amp;quot;&lt;/code&gt; ‚Äî must pin &lt;code&gt;numpy&amp;lt;2.0&lt;/code&gt; - &lt;code&gt;safetensors&lt;/code&gt; version conflicts - Silent crashes with unhelpful error messages - Hours of debugging&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Marker:&lt;/strong&gt; - &lt;code&gt;pip install marker-pdf torch&lt;/code&gt;. That's it. - Standard PyTorch, no special index URLs, no numpy hacks. - Worked on the first try.&lt;/p&gt; &lt;h2&gt;Modal-Specific Learnings&lt;/h2&gt; &lt;p&gt;Things I learned the hard way:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Use &lt;code&gt;@modal.cls()&lt;/code&gt; with &lt;code&gt;@modal.enter()&lt;/code&gt;&lt;/strong&gt; ‚Äî loads the model once, reuses across calls. Without this, you reload a 1GB+ model every single invocation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;scaledown_window=300&lt;/code&gt;&lt;/strong&gt; ‚Äî keeps the container warm for 5 min between calls. Second call to Marker on a warm container: 2.8s for a 1-page resume.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;Image.run_function(fn, gpu=&amp;quot;L4&amp;quot;)&lt;/code&gt;&lt;/strong&gt; ‚Äî lets you download/init models during image build with GPU attached. Models get baked into the image, zero download on cold start.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&lt;code&gt;modal deploy&lt;/code&gt; + separate caller script&lt;/strong&gt; ‚Äî build image once, call the function from any script without rebuilding.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;L4 is underrated&lt;/strong&gt; ‚Äî 34% cheaper than A10G, similar performance for PaddlePaddle workloads. But Marker specifically runs better on A10G.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Errors in &lt;code&gt;@modal.enter()&lt;/code&gt; are silent locally&lt;/strong&gt; ‚Äî they only show up in the Modal dashboard logs. Cost me 6 minutes staring at a hanging terminal.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;My Verdict&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Use case&lt;/th&gt; &lt;th&gt;Best choice&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Occasional PDF conversion&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Datalab API&lt;/strong&gt; ‚Äî $25/mo free credit, 15s processing, zero setup&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Math-heavy papers, speed matters&lt;/td&gt; &lt;td&gt;&lt;strong&gt;PP-StructureV3 lightweight&lt;/strong&gt; on L4 ‚Äî 26-32s, $0.006/run&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Best overall document quality&lt;/td&gt; &lt;td&gt;&lt;strong&gt;Marker&lt;/strong&gt; on A10G ‚Äî 54s, correct reading order, complete output&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Don't bother&lt;/td&gt; &lt;td&gt;PaddleOCR-VL ‚Äî slowest, worst quality, hardest to set up&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The &amp;quot;best&amp;quot; tool depends entirely on what you care about. If I could only pick one for general use: &lt;strong&gt;Marker&lt;/strong&gt;. The reading order and completeness issues with StructureV3 are hard to work around. If LaTeX formula accuracy is critical: &lt;strong&gt;StructureV3 lightweight&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Happy to share the Modal configs if anyone wants to reproduce this.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Various_Hour_9857"&gt; /u/Various_Hour_9857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ralqm0/i_benchmarked_paddleocrvl_15_vs_marker_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ralqm0/i_benchmarked_paddleocrvl_15_vs_marker_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ralqm0/i_benchmarked_paddleocrvl_15_vs_marker_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T08:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rakjyx</id>
    <title>implemented a pipeline by gepa that helps your ai agent perform way better</title>
    <updated>2026-02-21T07:05:26+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an open source project based on gskill, a pipeline from the team behind GEPA. It takes any github repository and generates a `.claude/skills/{repo-name}/SKILL.md` file with optimized, repo-specific instructions that significantly improve an agent‚Äôs task performance. You can easily use the resulting skill file with Claude Code, Codex and other ai agents. In the blog post, gskill improved resolve rate from 24% to 93% on some repositories and completed tasks up to 47% faster. In theory, with this strategy, smaller open weight models can perform much closer to the level of sota models.&lt;/p&gt; &lt;p&gt;Try it out and feel free to contribute!&lt;/p&gt; &lt;p&gt;blog post: &lt;a href="https://gepa-ai.github.io/gepa/blog/2026/02/18/automatically-learning-skills-for-coding-agents/"&gt;https://gepa-ai.github.io/gepa/blog/2026/02/18/automatically-learning-skills-for-coding-agents/&lt;/a&gt;&lt;br /&gt; repo: &lt;a href="https://github.com/itsmostafa/gskill"&gt;https://github.com/itsmostafa/gskill&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rakjyx/implemented_a_pipeline_by_gepa_that_helps_your_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rakjyx/implemented_a_pipeline_by_gepa_that_helps_your_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rakjyx/implemented_a_pipeline_by_gepa_that_helps_your_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T07:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uu5h</id>
    <title>Qwen3 Coder Next on 8GB VRAM</title>
    <updated>2026-02-20T13:05:21+00:00</updated>
    <author>
      <name>/u/Juan_Valadez</name>
      <uri>https://old.reddit.com/user/Juan_Valadez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a PC with 64 GB of RAM and an RTX 3060 12 GB, and I'm running Qwen3 Coder Next in MXFP4 with 131,072 context tokens.&lt;/p&gt; &lt;p&gt;I get a sustained speed of around 23 t/s throughout the entire conversation.&lt;/p&gt; &lt;p&gt;I mainly use it for front-end and back-end web development, and it works perfectly.&lt;/p&gt; &lt;p&gt;I've stopped paying for my Claude Max plan ($100 USD per month) to use only Claude Code with the following configuration:&lt;/p&gt; &lt;p&gt;&lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m ../GGUF/qwen3-coder-next-mxfp4.gguf -ngl 999 -sm none -mg 0 -t 12 -fa on -cmoe -c 131072 -b 512 -ub 512 -np 1 --jinja --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I promise you it works fast enough and with incredible quality to work with complete SaaS applications (I know how to program, obviously, but I'm delegating practically everything to AI).&lt;/p&gt; &lt;p&gt;If you have at least 64 GB of RAM and 8 GB of VRAM, I recommend giving it a try; you won't regret it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juan_Valadez"&gt; /u/Juan_Valadez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabo34</id>
    <title>Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)</title>
    <updated>2026-02-20T23:50:26+00:00</updated>
    <author>
      <name>/u/RIP26770</name>
      <uri>https://old.reddit.com/user/RIP26770</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"&gt; &lt;img alt="Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)" src="https://preview.redd.it/heh81bnslqkg1.png?width=140&amp;amp;height=69&amp;amp;auto=webp&amp;amp;s=e8760daf2610cb9721ded68d56da0aa9a90586c4" title="Local TTS server with voice cloning + near-realtime streaming replies (ElevenLabs alternative)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built a small local-first TTS server with voice cloning and streaming audio output so your LLM can reply back in a cloned voice almost in realtime.&lt;/p&gt; &lt;p&gt;Main reason: I wanted something that could replace ElevenLabs in a fully local stack without API costs or external dependencies.&lt;/p&gt; &lt;p&gt;Works well alongside llama.cpp / OpenAI-compatible endpoints and plugs cleanly into voice bots (I‚Äôm using it for Telegram voice replies).&lt;/p&gt; &lt;p&gt;Goals were simple:&lt;/p&gt; &lt;p&gt;-fully local -streaming audio output -voice cloning -lightweight + clean API -easy integration &lt;a href="https://github.com/ai-joe-git/pocket-tts-server"&gt;Pocket-TTS-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Already running it daily for voice-first bots.&lt;/p&gt; &lt;p&gt;Curious if anyone else here is building similar pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIP26770"&gt; /u/RIP26770 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rabo34"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabo34/local_tts_server_with_voice_cloning_nearrealtime/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rafggf</id>
    <title>Free open-source prompt compression engine ‚Äî pure text processing, no AI calls, works with any model</title>
    <updated>2026-02-21T02:40:41+00:00</updated>
    <author>
      <name>/u/bytesizei3</name>
      <uri>https://old.reddit.com/user/bytesizei3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built TokenShrink ‚Äî compresses prompts before you send them to any LLM. Pure text processing, no model calls in the loop. &lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Removes verbose filler (&amp;quot;in order to&amp;quot; ‚Üí &amp;quot;to&amp;quot;, &amp;quot;due to the fact that&amp;quot; ‚Üí &amp;quot;because&amp;quot;)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Abbreviates common words (&amp;quot;function&amp;quot; ‚Üí &amp;quot;fn&amp;quot;, &amp;quot;database&amp;quot; ‚Üí &amp;quot;db&amp;quot;)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Detects repeated phrases and collapses them&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Prepends a tiny [DECODE] header so the model understands&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Stress tested up to 10K words:&lt;/p&gt; &lt;p&gt;| Size | Ratio | Tokens Saved | Time |&lt;/p&gt; &lt;p&gt;|---|---|---|---|&lt;/p&gt; &lt;p&gt;| 500 words | 1.1x | 77 | 4ms |&lt;/p&gt; &lt;p&gt;| 1,000 words | 1.2x | 259 | 4ms |&lt;/p&gt; &lt;p&gt;| 5,000 words | 1.4x | 1,775 | 10ms |&lt;/p&gt; &lt;p&gt;| 10,000 words | 1.4x | 3,679 | 18ms |&lt;/p&gt; &lt;p&gt;Especially useful if you're running local models with limited context windows ‚Äî every token counts when you're on 4K or 8K ctx.&lt;/p&gt; &lt;p&gt;Has domain-specific dictionaries for code, medical, legal, and business prompts. Auto-detects which to use.&lt;/p&gt; &lt;p&gt;Web UI: &lt;a href="https://tokenshrink.com"&gt;https://tokenshrink.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/chatde/tokenshrink"&gt;https://github.com/chatde/tokenshrink&lt;/a&gt; (MIT, 29 unit tests)&lt;/p&gt; &lt;p&gt;API: POST &lt;a href="https://tokenshrink.com/api/compress"&gt;https://tokenshrink.com/api/compress&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Free forever. No tracking, no signup, client-side processing.&lt;/p&gt; &lt;p&gt;Curious if anyone has tested compression like this with smaller models ‚Äî does the [DECODE] header confuse 3B/7B models or do they handle it fine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bytesizei3"&gt; /u/bytesizei3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafggf/free_opensource_prompt_compression_engine_pure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafggf/free_opensource_prompt_compression_engine_pure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rafggf/free_opensource_prompt_compression_engine_pure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:40:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1raomh6</id>
    <title>What is the best way to deploy $1,300 (¬£1,000) to buy hardware to run a maximally powerful local LLM?</title>
    <updated>2026-02-21T11:12:40+00:00</updated>
    <author>
      <name>/u/philmethod</name>
      <uri>https://old.reddit.com/user/philmethod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I've never built a computer before and I want to spend ¬£1,000 to buy hardware to run the most powerful local LLM that this money can afford.&lt;/p&gt; &lt;p&gt;So I asked Google Gemini how to do this. It said I should buy: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Component&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Part Name&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Est. Price&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Where to Buy&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;NVIDIA RTX 3090 (24GB)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;¬£600&lt;/td&gt; &lt;td align="left"&gt;eBay / CeX (with 2yr warranty)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 5 7600&lt;/td&gt; &lt;td align="left"&gt;¬£140&lt;/td&gt; &lt;td align="left"&gt;Amazon / Scan / Ebuyer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mobo&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;B650M Micro-ATX&lt;/td&gt; &lt;td align="left"&gt;¬£110&lt;/td&gt; &lt;td align="left"&gt;Amazon / Overclockers UK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;32GB DDR5 6000MHz&lt;/td&gt; &lt;td align="left"&gt;¬£90&lt;/td&gt; &lt;td align="left"&gt;Any major UK retailer&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;850W 80+ Gold (Modular)&lt;/td&gt; &lt;td align="left"&gt;¬£100&lt;/td&gt; &lt;td align="left"&gt;Corsair or Seasonic&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SSD&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1TB NVMe Gen4&lt;/td&gt; &lt;td align="left"&gt;¬£60&lt;/td&gt; &lt;td align="left"&gt;Crucial or WD&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Any Mesh-front case&lt;/td&gt; &lt;td align="left"&gt;¬£50&lt;/td&gt; &lt;td align="left"&gt;Focus on airflow&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;It also told me that &lt;a href="http://PCPartPicker.com"&gt;PCPartPicker.com&lt;/a&gt; would flag any incompatabilities with hardware.&lt;/p&gt; &lt;p&gt;Since AIs can frequently hallucinate, I'd really appreciate a sanity check from a human community (i.e. you people) about whether I can put these parts together to build a computer that will actually work. &lt;/p&gt; &lt;p&gt;And whether this list of hardware truly is optimal for building the best localLLM that I can for ¬£1,000 ~$1,300.&lt;/p&gt; &lt;p&gt;So that I don't end up spend ¬£1,000 on something that doesn't work or delivers disappointing results.&lt;/p&gt; &lt;p&gt;Would really appreciate feedback on this. Is Gemini's advice on the what to buy to get the best LocalLLM possible for ¬£1,000 sensible? &lt;/p&gt; &lt;p&gt;What does everyone here think? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/philmethod"&gt; /u/philmethod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raomh6/what_is_the_best_way_to_deploy_1300_1000_to_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raomh6/what_is_the_best_way_to_deploy_1300_1000_to_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raomh6/what_is_the_best_way_to_deploy_1300_1000_to_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T11:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9y6s8</id>
    <title>TranscriptionSuite - A fully local, private &amp; open source audio transcription for Linux, Windows &amp; macOS</title>
    <updated>2026-02-20T15:22:24+00:00</updated>
    <author>
      <name>/u/TwilightEncoder</name>
      <uri>https://old.reddit.com/user/TwilightEncoder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt; &lt;img alt="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" src="https://external-preview.redd.it/ZjVodnR2dGoyb2tnMfrHn1-Z1IlbM1M-CdvVLf1S0fx3BvVT39BjZwD6xxr6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c01a41fb0c91487d97d9e6bbd7ba58c3750d09f" title="TranscriptionSuite - A fully local, private &amp;amp; open source audio transcription for Linux, Windows &amp;amp; macOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! This is a short presentation for my hobby project, &lt;a href="https://github.com/homelab-00/TranscriptionSuite"&gt;TranscriptionSuite&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; A fully local &amp;amp; private Speech-To-Text app for Linux, Windows &amp;amp; macOS. Python backend + Electron frontend, utilizing faster-whisper and CUDA acceleration.&lt;/p&gt; &lt;p&gt;If you're interested in the boring dev stuff, go to the bottom section.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I'm releasing a major UI upgrade today. Enjoy!&lt;/p&gt; &lt;p&gt;Short sales pitch:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;100% Local&lt;/strong&gt;: &lt;em&gt;Everything&lt;/em&gt; runs on your own computer, the app doesn't need internet beyond the initial setup&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Truly Multilingual&lt;/strong&gt;: Supports &lt;a href="https://github.com/openai/whisper/blob/main/whisper/tokenizer.py"&gt;90+ languages&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fully featured GUI&lt;/strong&gt;: Electron desktop app for Linux, Windows, and macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU + CPU Mode&lt;/strong&gt;: NVIDIA CUDA acceleration (recommended), or CPU-only mode for any platform including macOS&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Longform Transcription&lt;/strong&gt;: Record as long as you want and have it transcribed in seconds&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Mode&lt;/strong&gt;: Real-time sentence-by-sentence transcription for continuous dictation workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speaker Diarization&lt;/strong&gt;: PyAnnote-based speaker identification&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Static File Transcription&lt;/strong&gt;: Transcribe existing audio/video files with multi-file import queue, retry, and progress tracking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Remote Access&lt;/strong&gt;: Securely access your desktop at home running the model from anywhere (utilizing Tailscale)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio Notebook&lt;/strong&gt;: An Audio Notebook mode, with a calendar-based view, full-text search, and LM Studio integration (chat about your notes with the AI)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;System Tray Control&lt;/strong&gt;: Quickly start/stop a recording, plus a lot of other controls, available via the system tray.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üìå&lt;em&gt;Half an hour of audio transcribed in under a minute (RTX 3060)!&lt;/em&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;The seed of the project was my desire to quickly and reliably interface with AI chatbots using my voice. That was about a year ago. Though less prevalent back then, still plenty of AI services like GhatGPT offered voice transcription. However the issue is that, like every other AI-infused company, they &lt;em&gt;always&lt;/em&gt; do it shittily. Yes is works fine for 30s recordings, but what if I want to ramble on for 10 minutes? The AI is smart enough to decipher what I mean and I can speak to it like a smarter rubber ducky, helping me work through the problem.&lt;/p&gt; &lt;p&gt;Well, from my testing back then speak more than 5 minutes and they all start to crap out. And you feel doubly stupid because not only did you not get your transcription but you also wasted 10 minutes talking to the wall.&lt;/p&gt; &lt;p&gt;Moreover, there's the privacy issue. They already collect a ton of text data, giving them my voice feels like too much.&lt;/p&gt; &lt;p&gt;So I first looking at any existing solutions, but couldn't find any decent option that could run locally. Then I came across &lt;a href="https://github.com/KoljaB/RealtimeSTT"&gt;RealtimeSTT&lt;/a&gt;, an extremely impressive and efficient Python project that offered real-time transcription. It's more of a library or framework with only sample implementations.&lt;/p&gt; &lt;p&gt;So I started building around that package, stripping it down to its barest of bones in order to understand how it works so that I could modify it. This whole project grew out of that idea.&lt;/p&gt; &lt;p&gt;I built this project to satisfy my needs. I thought about releasing it only when it was decent enough where someone who doesn't know anything about it can just download a thing and run it. That's why I chose to Dockerize the server portion of the code.&lt;/p&gt; &lt;p&gt;The project was originally written in pure Python. Essentially it's a fancy wrapper around &lt;code&gt;faster-whisper&lt;/code&gt;. At some point I implemented a &lt;em&gt;server-client&lt;/em&gt; architecture and added a notebook mode (think of it like calendar for your audio notes).&lt;/p&gt; &lt;p&gt;And recently I decided to upgrade the frontend UI from Python to React + Typescript. Built all in Google AI Studio - App Builder mode for free believe it or not. No need to shell out the big bucks for Lovable, daddy Google's got you covered.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Don't hesitate to contact me here or open an issue on GitHub for any technical issues or other ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwilightEncoder"&gt; /u/TwilightEncoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/gxbrs1rj2okg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9y6s8/transcriptionsuite_a_fully_local_private_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T15:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1rad3hd</id>
    <title>I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python</title>
    <updated>2026-02-21T00:51:34+00:00</updated>
    <author>
      <name>/u/samaphp</name>
      <uri>https://old.reddit.com/user/samaphp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"&gt; &lt;img alt="I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python" src="https://preview.redd.it/jf8obilpwqkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8e68a6c305d0b33063478f23529d27aa4fddbb79" title="I evaluated LLaMA and 100+ LLMs on real engineering reasoning for Python" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I evaluated &lt;strong&gt;100+ LLMs&lt;/strong&gt; using a fixed set of questions covering &lt;strong&gt;7 software engineering categories&lt;/strong&gt; from the perspective of a Python developer. This was &lt;strong&gt;not coding tasks&lt;/strong&gt; and not traditional benchmarks, the questions focus on practical engineering reasoning and decision-making. All models were tested against the same prompts, and the results include both qualitative evaluation and &lt;strong&gt;token generation speed&lt;/strong&gt;, because usability over time matters as much as correctness.&lt;/p&gt; &lt;p&gt;Local models were evaluated on an NVIDIA RTX 4060 Ti 16GB using LM Studio, while most cloud models were tested via OpenRouter, with some Anthropic and OpenAI models evaluated directly through their official APIs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology:&lt;/strong&gt; the evaluation questions were collaboratively designed by &lt;strong&gt;ChatGPT 5.2&lt;/strong&gt; and &lt;strong&gt;Claude Opus 4.5&lt;/strong&gt;, including an agreed list of &lt;em&gt;good&lt;/em&gt; and &lt;em&gt;bad&lt;/em&gt; behaviors for each question. Model responses were then evaluated by &lt;strong&gt;gpt-4o-mini&lt;/strong&gt;, which checked each answer against that shared list. The evaluation categories were:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Problem Understanding &amp;amp; Reasoning&lt;/li&gt; &lt;li&gt;System Design &amp;amp; Architecture&lt;/li&gt; &lt;li&gt;API, Data &amp;amp; Domain Design&lt;/li&gt; &lt;li&gt;Code Quality &amp;amp; Implementation&lt;/li&gt; &lt;li&gt;Reliability, Security &amp;amp; Operations&lt;/li&gt; &lt;li&gt;LLM Behavior &amp;amp; Professional Discipline&lt;/li&gt; &lt;li&gt;Engineering Restraint &amp;amp; Practical Judgment&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;One thing that surprised me was that some of the &lt;strong&gt;highest-performing models&lt;/strong&gt; were also among the &lt;strong&gt;slowest and most token-heavy&lt;/strong&gt;. Once models pass roughly ~95%, quality differences shrink, and &lt;strong&gt;latency and efficiency become far more important&lt;/strong&gt;. My goal was to identify models I could realistically run &lt;strong&gt;24 hours a day&lt;/strong&gt;, either locally or via a cloud provider, without excessive cost or waiting time. The models I ended up favoriting for Python developer tasks weren't always the cheapest or the top scorers; they were the ones that finished quickly, used tokens efficiently, and still showed consistently good engineering judgment. For example, &lt;strong&gt;GPT 5.1 Codex&lt;/strong&gt; isn't very cheap, but it's very fast and highly token-efficient, which makes it practical for continuous use.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;Models I favored (efficient &amp;amp; suitable for my use case)&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Grok 4.1 Fast&lt;/strong&gt;: very fast, disciplined engineering responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 120B&lt;/strong&gt;: strong reasoning with excellent efficiency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt;: extremely fast and clean&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 20B (local)&lt;/strong&gt;: fast and practical on a consumer GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT 5.1 Codex Mini&lt;/strong&gt;: low verbosity, quick turnaround&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT 5.1 Codex&lt;/strong&gt;: not cheap, but very fast and token-efficient&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimax M2&lt;/strong&gt;:solid discipline with reasonable latency&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 4B (local)&lt;/strong&gt;: small, fast, and surprisingly capable&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The full list and the test results are available on this URL: &lt;a href="https://py.eval.draftroad.com"&gt;https://py.eval.draftroad.com&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Disclaimer:&lt;/strong&gt; these results reflect my personal experience and testing methodology. I may be wrong. Results can vary based on use cases, prompting styles, and evaluation criteria. This should be viewed as a transparent comparison, not a definitive benchmark for python with LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/samaphp"&gt; /u/samaphp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jf8obilpwqkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rad3hd/i_evaluated_llama_and_100_llms_on_real/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T00:51:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa7l</id>
    <title>Kimi has context window expansion ambitions</title>
    <updated>2026-02-20T08:54:10+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt; &lt;img alt="Kimi has context window expansion ambitions" src="https://preview.redd.it/3cvl2bdh5mkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e22f6604997ccccf6f6215ae239ab8f8b1dd09c3" title="Kimi has context window expansion ambitions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cvl2bdh5mkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:54:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wvg4</id>
    <title>GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title>
    <updated>2026-02-20T14:31:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt; &lt;img alt="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" src="https://external-preview.redd.it/tLGg2WMvFn2R5w7Nf2m6oJPphAYJILLSWaWPLPoW8i4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6bb0cd5000a00c0e28c8ae17203068e5acfb352" title="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;article by Georgi Gerganov, Xuan-Son Nguyen, Aleksander Grygier, Lysandre, Victor Mustar, Julien Chaumond&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-joins-hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1rafo5b</id>
    <title>[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS</title>
    <updated>2026-02-21T02:50:43+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"&gt; &lt;img alt="[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS" src="https://preview.redd.it/j509b0ozgrkg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=8c561d0403991ae31c82f4c63a33fcc76d123703" title="[Update] Vellium v0.3.5: Massive Writing Mode upgrade, Native KoboldCpp, and OpenAI TTS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just pushed a pretty big update for Vellium (v0.2.8 to v0.3.5). The main focus this time was overhauling the writing mode and making local providers work much smoother.&lt;/p&gt; &lt;p&gt;The writing mode got a huge rework. We finally added a proper book bible, direct DOCX import, and cached book summaries. The sidebar is way more compact now, and the character workspace is much better ‚Äî you can even use AI to patch-edit your characters directly. We also fixed a bunch of UX stuff, so project deletion and export/download (including inline scenes) are actually reliable now.&lt;/p&gt; &lt;p&gt;For local setups, KoboldCpp integration is fully native now. It supports the &lt;code&gt;provider:memory&lt;/code&gt; field, universal tags, and n-sigma. Payload fields are finally aligned with the official API, and we fixed those annoying model loading issues. Tool calling also properly disables in the UI when KoboldCpp is active.&lt;/p&gt; &lt;p&gt;A few other cool things: we added OpenAI-compatible TTS with a separate model just for translation. There's a new Zen Chat UI mode if you want zero visual distractions. Phrase bans are working properly now, and we turned off the default badwords by default. You also get more control in settings over API parameter forwarding, like sampler forwarding.&lt;/p&gt; &lt;p&gt;Under the hood, multi-character chat is way more stable (add at least one word from char name and he answer first than another). Squashed some runtime data leaks, sorted out the server bundle resolving inside&lt;code&gt;asar&lt;/code&gt;, and added some basic security hardening for local mode. Oh, and the project is now officially MIT licensed!&lt;/p&gt; &lt;p&gt;Grab the release on GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know if you hit any bugs or have ideas for the next updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rafo5b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rafo5b/update_vellium_v035_massive_writing_mode_upgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabg6o</id>
    <title>Qwen3 coder next oddly usable at aggressive quantization</title>
    <updated>2026-02-20T23:41:01+00:00</updated>
    <author>
      <name>/u/CoolestSlave</name>
      <uri>https://old.reddit.com/user/CoolestSlave</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys,&lt;/p&gt; &lt;p&gt;I've been testing the 30b range models but i've been a little disappointed by them (qwen 30b, devstral 2, nemotron etc) as they need a lot of guidance and almost all of them can't correct some mistake they made no matter what.&lt;/p&gt; &lt;p&gt;Then i tried to use qwen next coder at q2 because i don't have enough ram for q4. Oddly enough it does not say nonsense, even better, he one shot some html front page and can correct some mistake by himself when prompting back his mistake.&lt;/p&gt; &lt;p&gt;I've only made shallow testing but it really feel like at this quant, it already surpass all 30b models without sweating.&lt;/p&gt; &lt;p&gt;Do you have any experience with this model ? why is it that good ??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CoolestSlave"&gt; /u/CoolestSlave &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabg6o/qwen3_coder_next_oddly_usable_at_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rajez2</id>
    <title>what are your favorite lesser known models on huggingface</title>
    <updated>2026-02-21T06:01:33+00:00</updated>
    <author>
      <name>/u/EngineeringBright82</name>
      <uri>https://old.reddit.com/user/EngineeringBright82</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a professor, I want to expand my students minds by showing them models that are not chatGPT etc. Anyone have some unique / interesting / useful models hosted on huggingface?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EngineeringBright82"&gt; /u/EngineeringBright82 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rajez2/what_are_your_favorite_lesser_known_models_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T06:01:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rabcyp</id>
    <title>A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)</title>
    <updated>2026-02-20T23:37:12+00:00</updated>
    <author>
      <name>/u/spaceman_</name>
      <uri>https://old.reddit.com/user/spaceman_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"&gt; &lt;img alt="A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)" src="https://preview.redd.it/y3n05xxziqkg1.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=9c8cd5f0f499dbda0fd3ab30a338f120aa67e03d" title="A few Strix Halo benchmarks (Minimax M2.5, Step 3.5 Flash, Qwen3 Coder Next)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the release of Step 3.5 and MiniMax M2.5, we've got two new options for models that barely fit in memory.&lt;/p&gt; &lt;p&gt;To help people figure out which models run best on the platform, I decided to run some llama.cpp benchmarks for a few quants of these models.&lt;/p&gt; &lt;p&gt;I also included some benchmarks for Qwen3-coder-next (since we've been seeing lots of improvement lately), GLM 4.6V &amp;amp; GLM 4.7 Flash, and a few older models like gpt-oss-120b which compete in a similar size space.&lt;/p&gt; &lt;p&gt;My ROCm benchmarks are running against ROCm 7.2 as that is what my distro provides. My device has a Ryzen AI Max+ 395 @ 70W and 128GB of memory. All benchmarks are run at a context depth of 30,000 tokens.&lt;/p&gt; &lt;p&gt;If there's interest in other models or quants, feel free to ask for them in the comments, and I'll see if I can get some running.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spaceman_"&gt; /u/spaceman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rabcyp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rabcyp/a_few_strix_halo_benchmarks_minimax_m25_step_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:37:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ramir9</id>
    <title>[Release] Ouro-2.6B-Thinking ‚Äî first working inference (ByteDance's recurrent "thinking" model, fixed for transformers 4.55)</title>
    <updated>2026-02-21T09:04:04+00:00</updated>
    <author>
      <name>/u/PruneLanky3551</name>
      <uri>https://old.reddit.com/user/PruneLanky3551</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ByteDance released Ouro-2.6B-Thinking a few weeks ago and it's been tricky to run ‚Äî the architecture is genuinely unusual and existing GGUFs were producing garbage output because of it.&lt;/p&gt; &lt;p&gt;What makes Ouro different: It's a recurrent Universal Transformer ‚Äî it runs all 48 layers 4 times per token (192 effective passes). Standard llama.cpp just runs each layer once, so every existing GGUF was broken.&lt;/p&gt; &lt;p&gt;What I fixed:&lt;/p&gt; &lt;p&gt;The original modeling_ouro.py had two bugs incompatible with transformers 4.55:&lt;/p&gt; &lt;p&gt;UniversalTransformerCache inherits from Cache, which defines key_cache as a &lt;a href="/u/property"&gt;u/property&lt;/a&gt; ‚Äî so self.key_cache = [] in __init__ threw AttributeError: can't set attribute&lt;/p&gt; &lt;p&gt;Missing get_mask_sizes() method required by create_causal_mask() in transformers 4.55+&lt;/p&gt; &lt;p&gt;Patched both, tested output:&lt;/p&gt; &lt;p&gt;User: What is 2+2?&amp;lt;think&amp;gt;Okay, the user asked &amp;quot;What is 2+2?&amp;quot; It's a basic arithmetic problem...Adding 2 and 2 gives 4. That's a fundamental math fact...&amp;lt;/think&amp;gt;The sum of 2 and 2 is **4**.2 + 2 = 4&lt;/p&gt; &lt;p&gt;Performance (NVIDIA L4): ~3.8 t/s, 5.3 GB VRAM (float16)&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed"&gt;https://huggingface.co/scpalmetto/Ouro-2.6B-Thinking-Fixed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note: uses use_cache=False (full context recompute). KV cache pass-through doesn't work correctly with the 4-loop UT architecture ‚Äî this is the correct behavior matching early_exit_threshold: 1.0 in the config.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PruneLanky3551"&gt; /u/PruneLanky3551 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ramir9/release_ouro26bthinking_first_working_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T09:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vywq</id>
    <title>GGML.AI has got acquired by Huggingface</title>
    <updated>2026-02-20T13:54:26+00:00</updated>
    <author>
      <name>/u/Time_Reaper</name>
      <uri>https://old.reddit.com/user/Time_Reaper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt; &lt;img alt="GGML.AI has got acquired by Huggingface" src="https://external-preview.redd.it/l687iazpdDZhrDlIbQBxf8OTcfiJg6WGdsBpv03NqVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9e45ab199a5cdbdf8c5eb1968743c094b946e98" title="GGML.AI has got acquired by Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Reaper"&gt; /u/Time_Reaper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/19759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1raall0</id>
    <title>fixed parser for Qwen3-Coder-Next</title>
    <updated>2026-02-20T23:06:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"&gt; &lt;img alt="fixed parser for Qwen3-Coder-Next" src="https://external-preview.redd.it/Y3wE-GVXbELboPM9WQJZOtsZ_aPLgAL7jIOMvAV90UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6015d17bc47511d73e5fb3850ccf0851296f278f" title="fixed parser for Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;another fix for Qwen Next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19765"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raall0/fixed_parser_for_qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T23:06:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9zt8m</id>
    <title>The top 3 models on openrouter this week ( Chinese models are dominating!)</title>
    <updated>2026-02-20T16:21:50+00:00</updated>
    <author>
      <name>/u/keb_37</name>
      <uri>https://old.reddit.com/user/keb_37</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt; &lt;img alt="The top 3 models on openrouter this week ( Chinese models are dominating!)" src="https://preview.redd.it/h4l8zr4rdokg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1cb3201433eea5c7cd862fbc8c0f259e4e6b134" title="The top 3 models on openrouter this week ( Chinese models are dominating!)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the first time i see a model exceed 3 trillion tokens per week on openrouter!&lt;/p&gt; &lt;p&gt;the first time i see more than one model exceed a trillion token per week ( it was only grok 4 fast month ago)&lt;/p&gt; &lt;p&gt;the first time i see chinese models destroying US ones like this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keb_37"&gt; /u/keb_37 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4l8zr4rdokg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9zt8m/the_top_3_models_on_openrouter_this_week_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T16:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uuc6</id>
    <title>Deepseek and Gemma ??</title>
    <updated>2026-02-20T13:05:36+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt; &lt;img alt="Deepseek and Gemma ??" src="https://preview.redd.it/84ph0pirenkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d2b363b1900aae44bcfc12c0eeb9d8e2caa7d08" title="Deepseek and Gemma ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/84ph0pirenkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1raf3dm</id>
    <title>GLM 5 seems to have a "Claude" personality</title>
    <updated>2026-02-21T02:23:22+00:00</updated>
    <author>
      <name>/u/TinyApplet</name>
      <uri>https://old.reddit.com/user/TinyApplet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt; &lt;img alt="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" src="https://preview.redd.it/7nj17cwubrkg1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=e0025d5e56387cb178ff1a928dc4a19313407e90" title="GLM 5 seems to have a &amp;quot;Claude&amp;quot; personality" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've noticed that GLM 5 behaves significantly differently when told it is Claude, as with the following system prompt: &amp;quot;You are Claude, a large language model by Anthropic.&amp;quot; The writing style and personality changes significantly, and it even seems to bypass built-in censorship, as per my second image.&lt;/p&gt; &lt;p&gt;I've also tried a more nonsensical prompt: &amp;quot;You are Tiny, a large language model by Applet&amp;quot; (deliberately avoiding the names of any known models or companies), and, as expected, that didn't yield the same results nor bypassed the model's censorship.&lt;/p&gt; &lt;p&gt;Whether this was intentional on Zhipu's part or not, I can't say; it could be that they did, in fact, include a &amp;quot;Claude&amp;quot; personality in the training dataset, seeing as how they seem to have planned for GLM 5 to work well with Claude Code. It's also possible, of course, that this is emergent behavior, and that the personality changes are merely because GLM 5 has some information, however vague, on its dataset about what Claude is and how it's supposed to behave.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TinyApplet"&gt; /u/TinyApplet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1raf3dm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1raf3dm/glm_5_seems_to_have_a_claude_personality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T02:23:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ra8omf</id>
    <title>"Gemma, which we will be releasing a new version of soon"</title>
    <updated>2026-02-20T21:50:51+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"&gt; &lt;img alt="&amp;quot;Gemma, which we will be releasing a new version of soon&amp;quot;" src="https://external-preview.redd.it/9mfj1kMXjQ4Pove4Y8zbrEpz5ffGrhmDZ-YwmsdPJeE.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=66d6fe6182877c1d801afa8a61aec7616fb8a587" title="&amp;quot;Gemma, which we will be releasing a new version of soon&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;20:17&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/P0enFK4bzLE?si=2hfjhPrT4gbqsZwk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ra8omf/gemma_which_we_will_be_releasing_a_new_version_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T21:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ranako</id>
    <title>TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF ¬∑ Hugging Face</title>
    <updated>2026-02-21T09:52:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"&gt; &lt;img alt="TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/FYfNUuhT3WL90VoAzpzSy8fZEgRuGPVIPMxWk_wBrrg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0ae5ed8bdcc636a4a90b9972c253516a3b8e3bd" title="TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;featured yesterday (by Unsloth and on X) so let's check it out&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TeichAI/GLM-4.7-Flash-Claude-Opus-4.5-High-Reasoning-Distill-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ranako/teichaiglm47flashclaudeopus45highreasoningdistillg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T09:52:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ram2ov</id>
    <title>How I mapped every High Court of Australia case and their citations (1901-2025)</title>
    <updated>2026-02-21T08:36:59+00:00</updated>
    <author>
      <name>/u/Neon0asis</name>
      <uri>https://old.reddit.com/user/Neon0asis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt; &lt;img alt="How I mapped every High Court of Australia case and their citations (1901-2025)" src="https://preview.redd.it/2mntthxp7tkg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=2eb05b7ded68545504de00ea12ea1305b546acb8" title="How I mapped every High Court of Australia case and their citations (1901-2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve recently begun working on a project to convert entirety of Australian case law and legislation into a LexisNexis-style interlinked legal knowledge graph.&lt;/p&gt; &lt;p&gt;As I‚Äôve experimented with techniques to normalise case citations, I thought it would be cool to turn my work into a neat little visualisation, and explain how you could do the same with your own documents.&lt;/p&gt; &lt;p&gt;So the graph above is a visualisation of a cross-section of a legal knowledge graph I‚Äôve been developing of Australian case law.&lt;/p&gt; &lt;p&gt;Each node represents a High Court of Australia decision. The size of the node reflects how often that case has been cited by other High Court cases. The node's location and clustering comes from mapping each case‚Äôs semantic ‚Äúposition‚Äù into 3D space, based on its location in a higher-dimensional embedding space.&lt;/p&gt; &lt;h1&gt;How the dataset was built&lt;/h1&gt; &lt;p&gt;To assemble the graph, I downloaded the &lt;a href="https://huggingface.co/datasets/isaacus/open-australian-legal-corpus"&gt;Open Australian Legal Corpus &lt;/a&gt;and ran the &lt;a href="https://docs.isaacus.com/capabilities/enrichment"&gt;Kanon 2 Enricher&lt;/a&gt; to extract citations and additional metadata, such as decision dates and pinpoint references. I then used this additional metadata to repair and improve some of the dataset's missing features.&lt;/p&gt; &lt;p&gt;For roughly 90% of the corpus, I was able to recover and uniquely identify the party names, decision dates, and common aliases.&lt;/p&gt; &lt;p&gt;Using the party names and year as a composite key, I then normalised and deduplicated every citation appearing in High Court decisions. This produced ~20,000 High Court-to-High Court citations.&lt;/p&gt; &lt;p&gt;With the citations linked, I used the &lt;a href="https://docs.isaacus.com/capabilities/embedding"&gt;Kanon 2 Embedder&lt;/a&gt; to generate vector embeddings for each case, and then applied &lt;a href="https://github.com/YingfanWang/PaCMAP"&gt;PaCMAP&lt;/a&gt; (a dimensionality reduction library) to reduce those embeddings down to a 3D representation.&lt;/p&gt; &lt;p&gt;To infer clusters (i.e., broad topical groupings), I ran &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"&gt;K-means &lt;/a&gt;in the original embedding space. To make the clusters interpretable, I used &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"&gt;TF‚ÄìIDF&lt;/a&gt; to generate simple semantic labels based on the most characteristic terms in each cluster.&lt;/p&gt; &lt;p&gt;Finally, using the reception labels extracted by the Kanon 2 Enricher, I captured a sentiment-like signal for how cases treat the authorities they cite. Most citations are neutral (grey). Citations that overrule prior High Court authority are marked in red, while supportive citations are shown in green. Because the Enricher extracts these signals natively, that step was straightforward.&lt;/p&gt; &lt;p&gt;With the features extracted and linked, I then vibe coded a lightweight interface to render the network as an interactive node graph.&lt;/p&gt; &lt;h1&gt;What you can see in the result&lt;/h1&gt; &lt;p&gt;Even with around ~7,000 High Court cases, some patterns stand out immediately:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The semantic geometry works surprisingly well.&lt;/strong&gt; Closely related areas of law sit near one another in 3D space. Estate law and land law, for example, tend to cluster tightly (towards the bottom of the structure) while criminal law, which is not related to these fields, occupies the top end of the grap.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;You can explore fine-grained subregions interactively.&lt;/strong&gt; In the notebook (linked at the end of the post), there‚Äôs a region where several clusters intersect that corresponds strongly to constitutional cases involving Indigenous communities. &lt;em&gt;Mabo v Queensland (No 2)&lt;/em&gt; is one of the best-known cases in that neighbourhood.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The time dimension reflects legal history.&lt;/strong&gt; You can see a shift toward citing domestic authority more heavily after the &lt;a href="https://peo.gov.au/understand-our-parliament/history-of-parliament/history-milestones/australian-parliament-history-timeline/events/australia-act-1986"&gt;Australia Acts 1986&lt;/a&gt;, which helped establish Australia‚Äôs judicial independence. Earlier High Court decisions cite UK Privy Council rulings more often and are more visibly shaped by UK common law. This is one reason the earliest cases cite Australian authorities less than you might expect.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Reproducing it&lt;/h1&gt; &lt;p&gt;All code to reproduce the results is on &lt;a href="https://github.com/isaacus-dev/cookbooks/tree/main/cookbooks/semantic-legal-citation-graph"&gt;GitHub,&lt;/a&gt; and the interactive visualisation is embedded directly in the notebook, so you can explore it without running anything locally. If you‚Äôd like a guided walkthrough, there‚Äôs also a guided tour highlighting landmark cases in Australian constitutional law I have up on &lt;a href="https://youtu.be/in76S6P9xOw?si=hBaPpb0p6HVyjelv"&gt;YouTube&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon0asis"&gt; /u/Neon0asis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2mntthxp7tkg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ram2ov/how_i_mapped_every_high_court_of_australia_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-21T08:36:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
