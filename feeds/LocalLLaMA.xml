<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-28T03:36:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qouiy8</id>
    <title>One-shot Zelda Game Competition</title>
    <updated>2026-01-27T23:33:08+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am kicking off a competition - I'd like to see who can make the best one-shot HTML Zelda game with a local model&lt;/p&gt; &lt;p&gt;Rules:&lt;br /&gt; - You shall enter one prompt&lt;br /&gt; - The model must be an open-weights model&lt;br /&gt; - You may not use an agent, the model must output the entire HTML game in one shot, from one prompt. - If the game fails to run, you may copy the error message from the HTML console and give it to the model, once, in a follow up chat message, with a simple message: 'fix this', to allow it a chance to fix any minor bug it has, with no further instructions&lt;br /&gt; - You may not edit the code yourself or give the model any instructions on how to repair the game.&lt;br /&gt; - When posting your result, indicate the model, quant, prompt, and system prompt you used, along with whether the model was given a chance to fix the broken output. Us the format below.&lt;/p&gt; &lt;p&gt;That is all, let the competition begin!&lt;/p&gt; &lt;hr /&gt; &lt;ul&gt; &lt;li&gt;Model: GLM 4.7 Flash @ FP16&lt;/li&gt; &lt;li&gt;Prompt: &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Create a full featured beautiful 3d Zelda game in html that feels and plays beautifully, focusing on having a non-blocky visual asthetic of the characters. The map should be procedurally generated. it should have all the normal elements of a zelda game.&lt;/p&gt; &lt;p&gt;Use three.js r134 for the rendering&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Result: &lt;a href="https://slim-cube-3cf3.pagedrop.io"&gt;https://slim-cube-3cf3.pagedrop.io&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouiy8/oneshot_zelda_game_competition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouiy8/oneshot_zelda_game_competition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qouiy8/oneshot_zelda_game_competition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo349m</id>
    <title>deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face</title>
    <updated>2026-01-27T03:56:49+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face" src="https://external-preview.redd.it/c9LaruBvjfhr_AFkVVpu9jJ8NabAKdroEOMl2Akgn-0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a35ff741fcd21d9b3346fefa618503befa19d18" title="deepseek-ai/DeepSeek-OCR-2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo349m/deepseekaideepseekocr2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T03:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo3ri5</id>
    <title>Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement</title>
    <updated>2026-01-27T04:26:25+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt; &lt;img alt="Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement" src="https://preview.redd.it/0qp4pz0fbtfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=12b737e67556ca654785997ea815b78511476ed2" title="Jan v3 Instruct: a 4B coding Model with +40% Aider Improvement" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing Jan-v3-4B-base-instruct, a 4B-parameter model trained with &lt;strong&gt;continual pre-training&lt;/strong&gt; and &lt;strong&gt;RL&lt;/strong&gt;, to improve capabilities across common tasks while preserving other general capabilities.&lt;/p&gt; &lt;p&gt;What it‚Äôs for&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A good starting point for further fine-tuning&lt;/li&gt; &lt;li&gt;Improved math and coding performance for lightweight assistance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How to run it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Jan Desktop&lt;/p&gt; &lt;p&gt;Download Jan Desktop: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt; and then download Jan v3 via Jan Hub. &lt;/p&gt; &lt;p&gt;Model links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan-v3-4B: &lt;a href="https://huggingface.co/Menlo/Jan-v3-4B-base-instruct"&gt;https://huggingface.co/janhq/Jan-v3-4B-base-instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jan-v3-4B-GGUF: &lt;a href="https://huggingface.co/Menlo/Jan-v3-4B-base-instruct-gguf"&gt;https://huggingface.co/janhq/Jan-v3-4B-base-instruct-gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recommended parameters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;temperature: 0.7&lt;/li&gt; &lt;li&gt;top_p: 0.8&lt;/li&gt; &lt;li&gt;top_k: 20&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What‚Äôs coming next:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Jan-Code&lt;/strong&gt; (finetuned of Jan-v3-4B-base-instruct)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jan-v3-Seach-4B&lt;/strong&gt; (renewal of Jan-nano on Jan-v3-4B-base-instruct)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A 30B Jan-v3 family of models&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0qp4pz0fbtfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo3ri5/jan_v3_instruct_a_4b_coding_model_with_40_aider/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T04:26:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qofdc3</id>
    <title>tencent/Youtu-VL-4B-Instruct ¬∑ Hugging Face</title>
    <updated>2026-01-27T14:29:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qofdc3/tencentyoutuvl4binstruct_hugging_face/"&gt; &lt;img alt="tencent/Youtu-VL-4B-Instruct ¬∑ Hugging Face" src="https://external-preview.redd.it/ymzcxya4MbijdDp2b6xf6VUOGUPz9k8M8eOkIWZO9fk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bc2cda3e10198c3bb8f2e9efad0fe60e87895b4" title="tencent/Youtu-VL-4B-Instruct ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Youtu-VL&lt;/strong&gt; is a lightweight yet robust Vision-Language Model (VLM) built on the Youtu-LLM with 4B parameters. It pioneers Vision-Language Unified Autoregressive Supervision (VLUAS), which markedly strengthens visual perception and multimodal understanding. This enables a standard VLM to perform vision-centric tasks without task-specific additions. Across benchmarks, Youtu-VL stands out for its versatility, achieving competitive results on both vision-centric and general multimodal tasks.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF"&gt;https://huggingface.co/tencent/Youtu-VL-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/Youtu-VL-4B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qofdc3/tencentyoutuvl4binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qofdc3/tencentyoutuvl4binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T14:29:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qomra4</id>
    <title>Some initial benchmarks of Kimi-K2.5 on 4xB200</title>
    <updated>2026-01-27T18:50:51+00:00</updated>
    <author>
      <name>/u/benno_1237</name>
      <uri>https://old.reddit.com/user/benno_1237</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qomra4/some_initial_benchmarks_of_kimik25_on_4xb200/"&gt; &lt;img alt="Some initial benchmarks of Kimi-K2.5 on 4xB200" src="https://preview.redd.it/zyvu6wcjsxfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d94c746acf2b874a21b6387a83bb8da32100ae9" title="Some initial benchmarks of Kimi-K2.5 on 4xB200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just had some fun and ran a (very crude) benchmark script. Sadly, one GPU is busy so I can only run on 4 instead of 8 (thus limiting me to ~30k context without optimizations).&lt;/p&gt; &lt;p&gt;Command used (with random-input-len changing between sample points):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm bench serve \ --backend openai \ --base-url http://localhost:8000 \ --model /models/huggingface/moonshotai/Kimi-K2.5 \ --dataset-name random \ --random-input-len 24000 \ --random-output-len 512 \ --request-rate 2 \ --num-prompts 20 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;One full data point:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;============ Serving Benchmark Result ============ Successful requests: 20 Failed requests: 0 Request rate configured (RPS): 2.00 Benchmark duration (s): 61.48 Total input tokens: 480000 Total generated tokens: 10240 Request throughput (req/s): 0.33 Output token throughput (tok/s): 166.55 Peak output token throughput (tok/s): 420.00 Peak concurrent requests: 20.00 Total token throughput (tok/s): 7973.52 ---------------Time to First Token---------------- Mean TTFT (ms): 22088.76 Median TTFT (ms): 22193.34 P99 TTFT (ms): 42553.83 -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 34.37 Median TPOT (ms): 37.72 P99 TPOT (ms): 39.72 ---------------Inter-token Latency---------------- Mean ITL (ms): 34.37 Median ITL (ms): 17.37 P99 ITL (ms): 613.91 ================================================== &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, first token latency is terrible. This is probably due to an unoptimized tokenizer and inefficient chunk prefilling. I wanted to see the model perform with default vllm settings though.&lt;/p&gt; &lt;p&gt;Coding looks okay-ish at the moment but the context is limiting (this is a me problem, not the model).&lt;/p&gt; &lt;p&gt;Let me know if you want to see some benchmarks/have me try some settings.&lt;/p&gt; &lt;p&gt;Edit: &lt;/p&gt; &lt;p&gt;Maybe also interesting to know: first start took about 1.5h (with already downloaded safetensors). This is by far the longest time I ever had to wait for anything to start. Consecutive starts are much faster though&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benno_1237"&gt; /u/benno_1237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zyvu6wcjsxfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qomra4/some_initial_benchmarks_of_kimik25_on_4xb200/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qomra4/some_initial_benchmarks_of_kimik25_on_4xb200/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T18:50:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qov2lu</id>
    <title>EPYC, 1152GB RAM, RTX 6000, 5090, 2000</title>
    <updated>2026-01-27T23:55:34+00:00</updated>
    <author>
      <name>/u/Fit-Statistician8636</name>
      <uri>https://old.reddit.com/user/Fit-Statistician8636</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qov2lu/epyc_1152gb_ram_rtx_6000_5090_2000/"&gt; &lt;img alt="EPYC, 1152GB RAM, RTX 6000, 5090, 2000" src="https://b.thumbs.redditmedia.com/48qw3BjRynXOIe0glzacx9rzd1iP_2bv_3H_XMhYZqQ.jpg" title="EPYC, 1152GB RAM, RTX 6000, 5090, 2000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/a43y0zcdczfg1.jpg?width=1557&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=17cd5a28e9811760c5fd3d7c9d3ec7aaded2cdf6"&gt;https://preview.redd.it/a43y0zcdczfg1.jpg?width=1557&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=17cd5a28e9811760c5fd3d7c9d3ec7aaded2cdf6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I noticed people share their builds here, and it seems quite popular. I built one some time ago too, for LLMs. It can run Kimi in Q4, DeepSeek in Q8, and everything smaller. Full specs and some benchmarks links are here: &lt;a href="https://pcpartpicker.com/b/p8JMnQ"&gt;https://pcpartpicker.com/b/p8JMnQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions if you‚Äôre considering a similar setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fit-Statistician8636"&gt; /u/Fit-Statistician8636 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qov2lu/epyc_1152gb_ram_rtx_6000_5090_2000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qov2lu/epyc_1152gb_ram_rtx_6000_5090_2000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qov2lu/epyc_1152gb_ram_rtx_6000_5090_2000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:55:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo7wur</id>
    <title>OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances</title>
    <updated>2026-01-27T08:09:28+00:00</updated>
    <author>
      <name>/u/EchoOfOppenheimer</name>
      <uri>https://old.reddit.com/user/EchoOfOppenheimer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"&gt; &lt;img alt="OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances" src="https://external-preview.redd.it/v44P77PnYI5tRIZpnmaNJbBahgYNI024hkyMrYI6J24.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=147c3b3cde07c4eba8b8831f89c4a1dbc6ec6942" title="OpenAI could reportedly run out of cash by mid-2027 ‚Äî analyst paints grim picture after examining the company's finances" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new financial analysis predicts OpenAI could burn through its cash reserves by mid-2027. The report warns that Sam Altman‚Äôs '$100 billion Stargate' strategy is hitting a wall: training costs are exploding, but revenue isn't keeping up. With Chinese competitors like DeepSeek now offering GPT-5 level performance for 95% less cost, OpenAI‚Äôs 'moat' is evaporating faster than expected. If AGI doesn't arrive to save the economics, the model is unsustainable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EchoOfOppenheimer"&gt; /u/EchoOfOppenheimer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/big-tech/openai-could-reportedly-run-out-of-cash-by-mid-2027-nyt-analyst-paints-grim-picture-after-examining-companys-finances"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo7wur/openai_could_reportedly_run_out_of_cash_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T08:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoi1jc</id>
    <title>SERA 8B/32B</title>
    <updated>2026-01-27T16:08:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"&gt; &lt;img alt="SERA 8B/32B" src="https://a.thumbs.redditmedia.com/aiyHVH_H-noFKcxtqbQ2Vb5ZhuqTajFG0oLHTJx9LW8.jpg" title="SERA 8B/32B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/of9u5blh1xfg1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf11d0dc7016f0fadeee4eea761c68d7fed48098"&gt;https://preview.redd.it/of9u5blh1xfg1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf11d0dc7016f0fadeee4eea761c68d7fed48098&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allenai/SERA-32B"&gt;https://huggingface.co/allenai/SERA-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allenai/SERA-32B-GA"&gt;https://huggingface.co/allenai/SERA-32B-GA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allenai/SERA-8B-GA"&gt;https://huggingface.co/allenai/SERA-8B-GA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ykqidl1c1xfg1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b78c42146c0984889cd81cb6391cf3a03f061a5a"&gt;https://preview.redd.it/ykqidl1c1xfg1.png?width=779&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b78c42146c0984889cd81cb6391cf3a03f061a5a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoi1jc/sera_8b32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:08:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qol3s5</id>
    <title>[Preliminary] New subquadratic attention: ~20k tok/s prefill / ~100 tok/s decode @ 1M context (single GPU)</title>
    <updated>2026-01-27T17:54:19+00:00</updated>
    <author>
      <name>/u/Sad-Size2723</name>
      <uri>https://old.reddit.com/user/Sad-Size2723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, &lt;/p&gt; &lt;p&gt;Wanted to share some preliminary feasibility results from my work on a new attention mechanism (with custom kernels) on NVIDIA Nemotron Nano v3 30B. I am now able to run 1M context on a single GPU with this setup, and the early throughput numbers look promising. &lt;/p&gt; &lt;p&gt;TL;DR: 30B model + 1M context on a single GPU, with a jump-search-style attention mechanism. (Manuscript link: &lt;a href="https://arxiv.org/abs/2601.18401"&gt;https://arxiv.org/abs/2601.18401&lt;/a&gt;) &lt;/p&gt; &lt;p&gt;Numbers (single batch/sequence; single GPU: NVIDIA B200, similar results on RTX PRO 6000 Blackwell):&lt;br /&gt; - &lt;strong&gt;~20,000 tok/s&lt;/strong&gt; prefill&lt;br /&gt; - &lt;strong&gt;~100 tok/s&lt;/strong&gt; decode at &lt;strong&gt;1M&lt;/strong&gt; context&lt;br /&gt; - &lt;strong&gt;66 GB&lt;/strong&gt; GPU memory (6GB KV cache + 60GB FP16 model)&lt;br /&gt; - perfect NIAH (needle in a haystack) at 256K context (limited training so far) &lt;/p&gt; &lt;p&gt;I have completed an initial feasibility study, and I'm continuing to train the model toward real production use. The plan is to fully open-source the model for local inference, with a target of running a fully filled 1M context for a 30B model locally on ~24GB GPU memory. I'm cleaning up the codebase and plan to release the kernel implementations soon. For the model itself, I'll share it once we feel good about long-context performance/quality. &lt;/p&gt; &lt;p&gt;(Just to be clear: these are early numbers, and quality/evals are still in progress.) &lt;/p&gt; &lt;p&gt;1) What‚Äôs the main idea &lt;/p&gt; &lt;p&gt;You can think about the transformer attention mechanism as a search algorithm to find the relevant information to predict the next token. Standard attention is basically O(L) brute-force search. We‚Äôre doing an O(L^0.5) jump-search-style approach instead. For example, if you 10x the context length, a sqrt(L) search budget only grows by ~3.2x. &lt;/p&gt; &lt;p&gt;That subquadratic scaling really matters for long context, since the cost still grows with L. The main innovation is keeping that scaling while still making sure every token is reachable (i.e., not a fixed sliding window; think ‚Äò&lt;strong&gt;global random access&lt;/strong&gt;‚Äô). Most likely in long context inference, a large fraction of long-context computation is wasted by brute-force scanning, and that if we are smart about it, we can compute it much more efficiently. &lt;/p&gt; &lt;p&gt;2) What's the goal &lt;/p&gt; &lt;p&gt;Targeting high-quality and fast (~100 tok/s) open-source local models at long context: &lt;/p&gt; &lt;p&gt;- 1M context on a 24GB GPU: ~6GB KV cache + ~15GB 4-bit quantized model&lt;br /&gt; - 10M context on a 96GB GPU: ~60GB KV cache + ~30GB 8-bit quantized model &lt;/p&gt; &lt;p&gt;Our initial feasibility results suggest we‚Äôre already in the right ballpark on inference speed. The main work now is scaling training and doing broader quality evals on real long-context tasks. I‚Äôm sure we‚Äôll hit obstacles as we scale up, but overall we feel this direction is achievable. &lt;/p&gt; &lt;p&gt;3) Questions/feedback &lt;/p&gt; &lt;p&gt;I‚Äôm a big fan of running models locally (work + teaching + personal projects). Before COVID I bought 4√ó 1070 Ti GPUs for some non-LLM stuff, and these days I mostly use an A6000 at home. I‚Äôm excited about this because it could make really long-context workflows practical without needing a cluster.&lt;/p&gt; &lt;p&gt;Would love feedback / sanity checks on a few things:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What would you actually use 1M‚Äì10M context for locally? (offline search over docs, codebase-scale assistants, long-form editing, ‚Äúpersonal knowledge base‚Äù, etc.)&lt;/li&gt; &lt;li&gt;What evals would you trust most for long-context quality (beyond simple needle-in-a-haystack)?&lt;/li&gt; &lt;li&gt;What baselines should I compare against to make the speed/quality tradeoffs clear&lt;/li&gt; &lt;li&gt;What would make an open-source release most useful to you (kernels only vs full inference stack vs training code/configs)?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I kept this post high-level, but happy to go deeper if there‚Äôs interest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Size2723"&gt; /u/Sad-Size2723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T17:54:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qocvd4</id>
    <title>built an AI agent with shell access. found out the hard way why that's a bad idea.</title>
    <updated>2026-01-27T12:46:00+00:00</updated>
    <author>
      <name>/u/YogurtIll4336</name>
      <uri>https://old.reddit.com/user/YogurtIll4336</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;was building a tool to let claude/gpt4 navigate my codebase. gave it bash access, seemed fine.&lt;/p&gt; &lt;p&gt;then i tried asking it to &amp;quot;check imports and make ascii art from my env file&amp;quot;&lt;/p&gt; &lt;p&gt;it did both. printed my api keys as art.&lt;/p&gt; &lt;p&gt;went down a rabbit hole reading about this. turns out prompt injection is way worse than i thought:&lt;/p&gt; &lt;p&gt;anthropic has a whole page on it but it's pretty surface level&lt;/p&gt; &lt;p&gt;found this practical writeup from some YC startup that actually tested bypasses: &lt;a href="https://www.codeant.ai/blogs/agentic-rag-shell-sandboxing"&gt;https://www.codeant.ai/blogs/agentic-rag-shell-sandboxing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;simon willison has been screaming about this for months (&lt;a href="https://simonwillison.net/series/prompt-injection/"&gt;https://simonwillison.net/series/prompt-injection/&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;apparently docker shared kernel isn't enough. gvisor adds overhead. firecracker seems like overkill but it's what aws lambda uses so... maybe not? stuck between &amp;quot;ship it and hope&amp;quot; vs &amp;quot;burn 2 weeks adding proper isolation&amp;quot;&lt;/p&gt; &lt;p&gt;has anyone actually solved this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YogurtIll4336"&gt; /u/YogurtIll4336 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qocvd4/built_an_ai_agent_with_shell_access_found_out_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T12:46:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qos15u</id>
    <title>MiniMax-M2.1-REAP</title>
    <updated>2026-01-27T21:57:50+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2.1-REAP-139B-A10B"&gt;https://huggingface.co/cerebras/MiniMax-M2.1-REAP-139B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B"&gt;https://huggingface.co/cerebras/MiniMax-M2.1-REAP-172B-A10B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;so now you can run MiniMax on any potato ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qos15u/minimaxm21reap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qos15u/minimaxm21reap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qos15u/minimaxm21reap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T21:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qowfhi</id>
    <title>Arcee AI goes all-in on open models -- Interconnects interview</title>
    <updated>2026-01-28T00:51:04+00:00</updated>
    <author>
      <name>/u/RobotRobotWhatDoUSee</name>
      <uri>https://old.reddit.com/user/RobotRobotWhatDoUSee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arcee-AI has released their 400B-A13B model, as posted &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"&gt;elsewhere on LL&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This is an interview of the CEO, CTO and training lead of Arcee-AI, by Nathan Lambert of Allen Institute for AI (Ai2): &lt;/p&gt; &lt;p&gt;&amp;quot;&lt;a href="https://www.interconnects.ai/p/arcee-ai-goes-all-in-on-open-models"&gt;Arcee AI goes all-in on open models built in the U.S.&lt;/a&gt;,&amp;quot; Interconnects &lt;/p&gt; &lt;p&gt;Arcee-AI and Ai2 are two of the organizations that appear genuinely dedicated to developing LLMs in the open, releasing weights (and many checkpoints along the training arc; see both the Omlo 3 and Trinity collections), extensive reports on how they built models, and maintaining tools for open development of models. &lt;/p&gt; &lt;p&gt;Arcee-AI, for example, maintains &lt;a href="https://github.com/arcee-ai/mergekit"&gt;mergekit&lt;/a&gt;, which, among other things, allows one to build &amp;quot;clown-car MoEs&amp;quot; (though my impression is that the dense merge is used most often). &lt;/p&gt; &lt;p&gt;Hopefully will be able to try our their 400B-A13B preview model soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RobotRobotWhatDoUSee"&gt; /u/RobotRobotWhatDoUSee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qowfhi/arcee_ai_goes_allin_on_open_models_interconnects/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qowfhi/arcee_ai_goes_allin_on_open_models_interconnects/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qowfhi/arcee_ai_goes_allin_on_open_models_interconnects/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T00:51:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qob8de</id>
    <title>Honest question: what do you all do for a living to afford these beasts?</title>
    <updated>2026-01-27T11:23:48+00:00</updated>
    <author>
      <name>/u/ready_to_fuck_yeahh</name>
      <uri>https://old.reddit.com/user/ready_to_fuck_yeahh</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically I am from India, a medium high end job here pays Rs. 1 lakh($ 1100) per month and there are deductions on top of it.&lt;/p&gt; &lt;p&gt;An RTX Pro 6000 starts from 8 lakh and goes upto 10 lakh($ 10989), 5090 costs 3.5 lakhs($ 3800), threadripper costs 7-8 lakhs($ 8800), ram prices have soared and corsair vengeance costs 52,000 ($ 571) for 32GB, motherboard, cabinet, and other accessories makes it look like a dream to own in a lifetime. And people here are using multi gpu setup, recently saw 4xrtx 6000 pro setup here.&lt;/p&gt; &lt;p&gt;Been seeing a lot of beautiful multi-GPU setups here and I'm genuinely curious about the community makeup.&lt;/p&gt; &lt;p&gt;Are most of you:&lt;/p&gt; &lt;p&gt;Software engineers / AI researchers (expensing to employer or side business)?&lt;/p&gt; &lt;p&gt;Serious hobbyists with high-paying day jobs?&lt;/p&gt; &lt;p&gt;Consultants/freelancers writing off hardware?&lt;/p&gt; &lt;p&gt;Something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ready_to_fuck_yeahh"&gt; /u/ready_to_fuck_yeahh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qob8de/honest_question_what_do_you_all_do_for_a_living/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T11:23:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qo595n</id>
    <title>Introducing Kimi K2.5, Open-Source Visual Agentic Intelligence</title>
    <updated>2026-01-27T05:39:09+00:00</updated>
    <author>
      <name>/u/Kimi_Moonshot</name>
      <uri>https://old.reddit.com/user/Kimi_Moonshot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üîπ&lt;strong&gt;Global SOTA on Agentic Benchmarks&lt;/strong&gt;: HLE full set (50.2%), BrowseComp (74.9%) &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Open-source SOTA on Vision and Coding&lt;/strong&gt;: MMMU Pro (78.5%), VideoMMMU (86.6%), SWE-bench Verified (76.8%) &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Code with Taste&lt;/strong&gt;: turn chats, images &amp;amp; videos into aesthetic websites with expressive motion. &lt;/p&gt; &lt;p&gt;üîπ&lt;strong&gt;Agent Swarm (Beta)&lt;/strong&gt;: self-directed agents working in parallel, at scale. Up to &lt;strong&gt;100&lt;/strong&gt; sub-agents, &lt;strong&gt;1,500&lt;/strong&gt; tool calls, &lt;strong&gt;4.5√ó&lt;/strong&gt; faster compared with single-agent setup. &lt;/p&gt; &lt;p&gt;ü•ù&lt;strong&gt;K2.5&lt;/strong&gt; is now live on &lt;a href="https://t.co/YutVbwktG0"&gt;http://kimi.com&lt;/a&gt; in &lt;strong&gt;chat mod&lt;/strong&gt;e and &lt;strong&gt;agent mode&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;ü•ù&lt;strong&gt;K2.5 Agent Swarm&lt;/strong&gt; in beta for high-tier users. &lt;/p&gt; &lt;p&gt;ü•ùFor production-grade coding, you can pair K2.5 with &lt;strong&gt;Kim&lt;/strong&gt;i Code: &lt;a href="https://t.co/A5WQozJF3s"&gt;https://kimi.com/code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóAPI: &lt;a href="https://t.co/EOZkbOwCN4"&gt;https://platform.moonshot.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîóTech blog: &lt;a href="https://www.kimi.com/blog/kimi-k2-5.html"&gt;https://www.kimi.com/blog/kimi-k2-5.html&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üîóWeights &amp;amp; code: &lt;a href="https://huggingface.co/moonshotai/Kimi-K2.5"&gt;https://huggingface.co/moonshotai/Kimi-K2.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412"&gt;https://preview.redd.it/b3lldwzvwtfg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ffa7bb89f8a91ef050af44cc3fa6090c9e1a7412&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kimi_Moonshot"&gt; /u/Kimi_Moonshot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qo595n/introducing_kimi_k25_opensource_visual_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T05:39:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoikji</id>
    <title>Drummer's Rocinante X 12B v1 - It's back and it's stronger than ever! A funtastic creative Claude-like RP model at home!</title>
    <updated>2026-01-27T16:27:12+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoikji/drummers_rocinante_x_12b_v1_its_back_and_its/"&gt; &lt;img alt="Drummer's Rocinante X 12B v1 - It's back and it's stronger than ever! A funtastic creative Claude-like RP model at home!" src="https://external-preview.redd.it/vMt_thwlfOcsDVODCeD_E13hrou5XcQcrAzk35oTLtU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ad8c88d8902c9e03aa9ee1c38b6f2620bca2b91" title="Drummer's Rocinante X 12B v1 - It's back and it's stronger than ever! A funtastic creative Claude-like RP model at home!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rocinante-X-12B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoikji/drummers_rocinante_x_12b_v1_its_back_and_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoikji/drummers_rocinante_x_12b_v1_its_back_and_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoocgn</id>
    <title>allenai released new open coding models</title>
    <updated>2026-01-27T19:45:55+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"&gt; &lt;img alt="allenai released new open coding models" src="https://b.thumbs.redditmedia.com/40jQPf7fLYGIV-9GrNxTp-Fu-rEYdD8Bc_t8B0upc1U.jpg" title="allenai released new open coding models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/allenai/open-coding-agents"&gt;https://huggingface.co/collections/allenai/open-coding-agents&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3wanlr674yfg1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c31d64089433fd350f3aaa72d94242e9326b7ab"&gt;https://preview.redd.it/3wanlr674yfg1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c31d64089433fd350f3aaa72d94242e9326b7ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://allenai.org/papers/opencodingagents"&gt;https://allenai.org/papers/opencodingagents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoocgn/allenai_released_new_open_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T19:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoa8rp</id>
    <title>The Qwen Devs Are Teasing Something</title>
    <updated>2026-01-27T10:28:56+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"&gt; &lt;img alt="The Qwen Devs Are Teasing Something" src="https://preview.redd.it/umvks92vcvfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=297a92382cbb71a347dd9192a2d8ae1054cf9fb2" title="The Qwen Devs Are Teasing Something" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm going to assume a new VL model&lt;/p&gt; &lt;p&gt;Edit: It's likely to be Z-Image&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/umvks92vcvfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoa8rp/the_qwen_devs_are_teasing_something/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T10:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qou799</id>
    <title>Stanford Proves Parallel Coding Agents are a Scam</title>
    <updated>2026-01-27T23:20:18+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"&gt; &lt;img alt="Stanford Proves Parallel Coding Agents are a Scam" src="https://a.thumbs.redditmedia.com/bSPz9zEUo0BXRhW42ijPdUhwVHlYft5MVuPWRmgiAK0.jpg" title="Stanford Proves Parallel Coding Agents are a Scam" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/coxs8w3z3zfg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0875df6bf260ca3af0f9fe7eef7bbd3697a0c73"&gt;https://preview.redd.it/coxs8w3z3zfg1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0875df6bf260ca3af0f9fe7eef7bbd3697a0c73&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;A fascinating new &lt;a href="https://cooperbench.com/static/pdfs/main.pdf"&gt;preprint&lt;/a&gt; from Stanford and SAP drops a truth bomb that completely upends the &amp;quot;parallel coordinated coding&amp;quot; &amp;quot;productivity boost&amp;quot; assumption for AI coding agents.&lt;/p&gt; &lt;p&gt;Their &amp;quot;CooperBench&amp;quot; reveals what they call the &amp;quot;curse of coordination.&amp;quot; When you add a second coding agent, performance doesn't just fail to improve - it plummets. On average, two agents working together have a 30% lower success rate. For top models like GPT-5 and Claude 4.5 Sonnet, the success rate is a staggering 50% lower than just using one agent to do the whole job.&lt;/p&gt; &lt;p&gt;Why? The agents are terrible teammates. They fail to model what their partner is doing (42% of failures), don't follow through on commitments (32%), and have communication breakdowns (26%). They hallucinate shared states and silently overwrite each other's work.&lt;/p&gt; &lt;p&gt;This brings me to the elephant in the room. Platforms like Cursor, Antigravity, and others are increasingly marketing &amp;quot;parallel agent&amp;quot; features as a productivity revolution. But if foundational research shows this approach is fundamentally broken and makes you less productive, what are they actually selling? It feels like they're monetizing a feature they might know is a scam, &amp;quot;persuading&amp;quot; users into thinking they're getting a 10x team when they're really getting a mess of conflicting code.&lt;/p&gt; &lt;p&gt;As the Stanford authors put it, it's &amp;quot;hard to imagine how an agent incapable of coordination would contribute to such a future however strong the individual capabilities.&amp;quot; Food for thought next time you see a &amp;quot;parallel-agent&amp;quot; feature advertised.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qou799/stanford_proves_parallel_coding_agents_are_a_scam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:20:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoiep6</id>
    <title>The z-image base is here!</title>
    <updated>2026-01-27T16:21:29+00:00</updated>
    <author>
      <name>/u/bobeeeeeeeee8964</name>
      <uri>https://old.reddit.com/user/bobeeeeeeeee8964</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image"&gt;https://huggingface.co/Tongyi-MAI/Z-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobeeeeeeeee8964"&gt; /u/bobeeeeeeeee8964 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoiep6/the_zimage_base_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T16:21:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qouf0x</id>
    <title>Arcee AI releases Trinity Large : OpenWeight 400B-A13B</title>
    <updated>2026-01-27T23:28:47+00:00</updated>
    <author>
      <name>/u/abkibaarnsit</name>
      <uri>https://old.reddit.com/user/abkibaarnsit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"&gt; &lt;img alt="Arcee AI releases Trinity Large : OpenWeight 400B-A13B" src="https://external-preview.redd.it/kpKiKke1xSzMMszPwBcvRHFEWu1KcRJDoXwrfinT_zM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=409d9b6883693b08a64db17e15a4f65b6ef32a87" title="Arcee AI releases Trinity Large : OpenWeight 400B-A13B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abkibaarnsit"&gt; /u/abkibaarnsit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arcee.ai/blog/trinity-large"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qouf0x/arcee_ai_releases_trinity_large_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:28:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoml1n</id>
    <title>[LEAKED] Kimi K2.5‚Äôs full system prompt + tools (released &lt;24h ago)</title>
    <updated>2026-01-27T18:44:50+00:00</updated>
    <author>
      <name>/u/Pretty_Mountain2714</name>
      <uri>https://old.reddit.com/user/Pretty_Mountain2714</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was messing around with Moonshot's new Kimi K2.5 and pulled the whole system prompt + tools. (~5k tk) &lt;/p&gt; &lt;p&gt;Got hyped I grabbed this so fast cause usually someone posts this stuff way before I get to it&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/dnnyngyen/kimi-k2.5-prompts-tools"&gt; https://github.com/dnnyngyen/kimi-k2.5-prompts-tools &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Contents:&lt;br /&gt; -full system prompt&lt;br /&gt; -all tool schemas + instructions&lt;br /&gt; -memory CRUD protocols&lt;br /&gt; -context engineering + assembling user profile&lt;br /&gt; -basic guardrails/rules&lt;br /&gt; -external datasources (finance, arxiv, etc)&lt;/p&gt; &lt;p&gt;After running a couple attempts/verification across 2 different accounts: &lt;a href="https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b"&gt; https://www.kimi.com/share/19c003f5-acb2-838b-8000-00006aa45d9b &lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to be able to contribute sum to this community&lt;/p&gt; &lt;p&gt;[EDIT 1]: independent verification of the same prompt posted in CN earlier today: &lt;a href="https://linux.do/t/topic/1523104"&gt;https://linux.do/t/topic/1523104 &lt;/a&gt;&lt;br /&gt; [EDIT 2]: another independent verification just posted:&lt;br /&gt; &lt;a href="https://linux.do/t/topic/1518643"&gt;https://linux.do/t/topic/1518643&lt;/a&gt;&lt;br /&gt; [EDIT 3]: independent verification just posted on &lt;a href="/u/Spiritual_Spell_9469"&gt;u/Spiritual_Spell_9469&lt;/a&gt;'s thread on&lt;a href="https://www.reddit.com/r/ClaudeAIJailbreak/comments/1qoeos7/kimi_k25_jailbroken/"&gt; jailbreaking Kimi K2.5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pretty_Mountain2714"&gt; /u/Pretty_Mountain2714 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoml1n/leaked_kimi_k25s_full_system_prompt_tools/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T18:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qorbdk</id>
    <title>Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp; GPU inference. Surprising results.</title>
    <updated>2026-01-27T21:31:27+00:00</updated>
    <author>
      <name>/u/Icy-Measurement8245</name>
      <uri>https://old.reddit.com/user/Icy-Measurement8245</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/"&gt; &lt;img alt="Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp;amp; GPU inference. Surprising results." src="https://b.thumbs.redditmedia.com/eWRvxwIb2VPDDptePsLNamF8MJiBRPWZaLITl4Q7jPs.jpg" title="Dual RTX PRO 6000 Workstation with 1.15TB RAM. Finally multi-users and long contexts benchmarks. GPU only vs. CPU &amp;amp; GPU inference. Surprising results." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Me and my team have been building AI workstations for enterprise use and wanted to share some real benchmark data on a dual RTX PRO 6000 Blackwell Max-Q setup (192GB VRAM total) with over 1.15TB of DDR5 RAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Can a $30K-$50K workstation serve a team of 4-50 people or run multiple agents? Tested MiniMax M2.1 native fp8 (GPU+CPU via KTransformers) vs int4 quantized (GPU-only via SGLang). &lt;strong&gt;Key finding: int4 on GPU only is 2-4x faster on prefill but maxes out at ~3 concurrent requests due to KV-cache constraints. Native fp8 scales much better to 10+ users on large contexts but remains slower E2E.&lt;/strong&gt; Full configs and data below. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2x NVIDIA RTX PRO 6000 Max-Q (192GB VRAM total))&lt;/li&gt; &lt;li&gt;AMD EPYC9645 96-core/192-thread &lt;/li&gt; &lt;li&gt;12x DDR5 ECC RDIMM 96GB 5600 Mt/s (1152GB total)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Model tested so far:&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native fp8 version: MiniMax-M2.1 (&lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Quantized version: MiniMax-M2.1-BF16-INT4-AWQ (&lt;a href="https://huggingface.co/mratsim/MiniMax-M2.1-BF16-INT4-AWQ"&gt;link&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to compare two approaches: fp8 precision with CPU offloading vs quantized weights fitting entirely in VRAM.&lt;/p&gt; &lt;h1&gt;Why I‚Äôm sharing this&lt;/h1&gt; &lt;p&gt;Most workstation benchmarks show single-user performance with limited context sizes. Given the investment here, I wanted to test if one plug-and-play workstation could actually serve an entire team or multiple simultaneous agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I want to know how many people or agents can use this setup before it degrades too much.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Key metrics: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prefill speed per user (tokens/s/user): Request processing speed&lt;/li&gt; &lt;li&gt;TTFT (Time To First Tokens) (s/request): Time until first output generated&lt;/li&gt; &lt;li&gt;Decode speed per user (tokens/s/request): Generation speed&lt;/li&gt; &lt;li&gt;E2E request time (s/request): Total time from request to completion&lt;/li&gt; &lt;li&gt;Queue time (s/request): Time waiting before processing starts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The priority use case is a coding agent as we would like to run a vibecoding platform 100% locally, hence the choice of MiniMax-M2.1 (more in follow-up posts).&lt;/p&gt; &lt;h1&gt;Methodology&lt;/h1&gt; &lt;p&gt;There are two types of tests for now:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Simple chat&lt;/strong&gt; (~140 tokens input, 300 tokens max output)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large context&lt;/strong&gt; (~64K tokens input, 300 tokens max output)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Key details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Used sglang‚Äôs per request metrics logs, in order to properly measure TTFT, prefill and decode speed. &lt;/li&gt; &lt;li&gt;Measured queueing time separately, as it is a good indicator to see if the server starts to be overloaded.&lt;/li&gt; &lt;li&gt;No prefix caching &lt;/li&gt; &lt;li&gt;Tested with 1, 2, 4, 6, 8 and 10 simultaneous users (threads calling the API over and over again)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results: short context (~140 tokens input)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;[see graphs attached]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaway:&lt;/strong&gt; The quantized model is running on GPU alone far better than the fp8 model running on CPU and GPU, which was expected.&lt;/p&gt; &lt;p&gt;However the use of the fp8 model is still usable, for up to 2 or 4 simultaneous users (less than 30s processing time). And while the prefill speed with the fp8 model is very low (260 to 110 tokens/s) on short context, it‚Äôs important to note the speed increase over larger contexts.&lt;/p&gt; &lt;p&gt;Over a certain input size threshold (about 4k tokens) KTransformer processes the prefill layer-wise, which adds a constant overhead but greatly increases the computation speed by doing all the computation on the GPU, loading and processing one layer at a time, leading to the following results on large contexts.&lt;/p&gt; &lt;h1&gt;Results: Large context (64K tokens)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;[see graphs attached]&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Processing 64K tokens with one user takes ~15s for MiniMax-M2.1-INT4 on GPU-only and double that with MiniMax-M2.1 with GPU and CPU offloading.&lt;/p&gt; &lt;p&gt;But here's the thing: INT4 has way less KV-cache available since the model must fit entirely in VRAM. It maxes out at 3 parallel requests. Beyond that, processing speed per request stays flat - requests just pile up in the queue. Queue time explodes and becomes the dominant factor in TTFT and E2E processing.&lt;/p&gt; &lt;p&gt;The results on large contexts are more favorable to the GPU+CPU setup. It's not significantly slower, and the massive KV-cache means real-world usage would see a lot of cache-hit in real usage, furthermore improving processing speed. However, the decode rate remains low (8 to 3 tokens/s for 4 to 10 simultaneous users), so for long generation tasks it may be of limited use.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key message. Do not underestimate queue time, it becomes an essential element of bottleneck. Moreover, recompute of prefill can be costly and grow over time.&lt;/strong&gt; &lt;/p&gt; &lt;h1&gt;SGLang and KTransformers were used for GPU and CPU offloading with MiniMax-M2.1&lt;/h1&gt; &lt;p&gt;At first, I started experimenting with llama.cpp, which worked okay with CPU offloading but didn‚Äôt scale well with several simultaneous users. In addition, no optimisation is done for long inputs. I then switched to KTransformers, which supports layer-wise prefill with CPU offloading, which works great for long inputs. It‚Äôs based on SGLang and also runs great for simultaneous users. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;KTransformers configuration, highly biased toward kv-cache size:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kt run --enable-shared-experts-fusion \ --cpu-threads 96 \ --chunked-prefill-size 60000 \ --model-path /fast-data/ktransformer/MinimaxM2.1/ \ --max-total-tokens 600000 \ --gpu-experts 20 \ -p 8000 MiniMax-M2.1 \ --mem-fraction-static 0.85 \ --max-running-requests 12 \ --max-prefill-tokens 80000 \ --export-metrics-to-file \ --enable-metrics \ --export-metrics-to-file-dir ./metrics/ \ --enable-request-time-stats-logging \ --enable-cache-report &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;SGLang config:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 -m sglang.launch_server \ --host 127.0.0.1 \ --port &amp;quot;8000&amp;quot; \ --sleep-on-idle \ --disable-custom-all-reduce \ --max-running-requests 16 \ --cuda-graph-max-bs 16 \ --attention-backend flashinfer \ --served-model-name &amp;quot;MiniMax-M2.1&amp;quot; \ --model-path &amp;quot;mratsim/MiniMax-M2.1-BF16-INT4-AWQ&amp;quot; \ --tool-call-parser minimax-m2 \ --reasoning-parser minimax \ --trust-remote-code \ --export-metrics-to-file \ --enable-metrics \ --export-metrics-to-file-dir ./metrics/ \ --enable-request-time-stats-logging \ --enable-cache-report \ --tp 2 \ --mem-fraction-static 0.93 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What's next&lt;/h1&gt; &lt;p&gt;I want to extend the tests to larger workloads and context. My next test is to run coding agents using Claude Code in parallel on real coding tasks in ‚ÄúRalph‚Äù mode. I will continue comparing MiniMax-M2.1 and MiniMax-M2.1-INT4. I am also in the process of testing other models: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-235B-A22B&lt;/li&gt; &lt;li&gt;GPT-OSS 120B &lt;/li&gt; &lt;li&gt;DeepSeek V3.2&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to run specific tests if there's interest. Also curious if anyone else has multi-user scaling data on similar hardware. &lt;/p&gt; &lt;p&gt;&lt;em&gt;We're a small team deploying local AI agents and setting up private infrastructures. If you have questions about the setup or want us to test something specific, drop a comment.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy-Measurement8245"&gt; /u/Icy-Measurement8245 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qorbdk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qorbdk/dual_rtx_pro_6000_workstation_with_115tb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T21:31:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qoty38</id>
    <title>Kimi K2.5 costs almost 10% of what Opus costs at a similar performance</title>
    <updated>2026-01-27T23:10:16+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"&gt; &lt;img alt="Kimi K2.5 costs almost 10% of what Opus costs at a similar performance" src="https://preview.redd.it/xz7okply3zfg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74631e6bf621619eca977768f8c7287dc6164e45" title="Kimi K2.5 costs almost 10% of what Opus costs at a similar performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been trying out Kimi k2.5 and this is the first time that I feel an open model is truly competitive with SOTA closed models.&lt;/p&gt; &lt;p&gt;Compared to GLM, Kimi is a bit better, specially when it comes to non-website tasks.&lt;/p&gt; &lt;p&gt;Have you tried it? What's your take?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xz7okply3zfg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qoty38/kimi_k25_costs_almost_10_of_what_opus_costs_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T23:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qos25i</id>
    <title>Kimi K2 Artificial Analysis Score</title>
    <updated>2026-01-27T21:58:50+00:00</updated>
    <author>
      <name>/u/Virenz</name>
      <uri>https://old.reddit.com/user/Virenz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"&gt; &lt;img alt="Kimi K2 Artificial Analysis Score" src="https://preview.redd.it/0xqbgnt0syfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e64564291959092586bed5a52ce15a55c2fad64c" title="Kimi K2 Artificial Analysis Score" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/i/status/2016250137115557953"&gt;https://x.com/i/status/2016250137115557953&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Virenz"&gt; /u/Virenz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0xqbgnt0syfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qos25i/kimi_k2_artificial_analysis_score/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-27T21:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
