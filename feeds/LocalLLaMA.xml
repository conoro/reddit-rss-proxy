<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-14T11:23:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pmbmt1</id>
    <title>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</title>
    <updated>2025-12-14T11:10:29+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;new anthropic paper on &lt;strong&gt;localizing dangerous knowledge&lt;/strong&gt; instead of letting it leak everywhere in the model: &lt;a href="http://arxiv.org/abs/2512.05648"&gt;arxiv.org/abs/2512.05648&lt;/a&gt;&lt;/p&gt; &lt;p&gt;where we started: llms pick up risky stuff from messy datasets. filters miss things, bad content gets trained in, and once it‚Äôs there it‚Äôs almost impossible to fully remove. worse, that knowledge usually spreads across the whole network&lt;/p&gt; &lt;p&gt;sooo, they pre-designate a tiny part of the model - a small set of neurons + attention heads - as a &lt;strong&gt;‚Äúrisky zone.‚Äù&lt;/strong&gt; any dangerous knowledge is forced to live there&lt;/p&gt; &lt;p&gt;how it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;risky examples only update the risky zone. gradients to the rest of the model are zeroed;&lt;/li&gt; &lt;li&gt;normal examples train with the risky zone turned off;&lt;/li&gt; &lt;li&gt;after training, they just zero out that zone, deleting the bad stuff while keeping most general abilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;why this actually works lol:&lt;/p&gt; &lt;p&gt;early labeled risky data ‚Äúsets the path.‚Äù later leaks from unlabeled or mislabeled data get routed into the same area instead of spreading everywhere.&lt;/p&gt; &lt;p&gt;what they showed:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;much better removal than plain data filtering on bilingual stories and bio/ military wikipedia topics;&lt;/li&gt; &lt;li&gt;far more resistant to adversarial fine-tuning that usually brings banned skills back;&lt;/li&gt; &lt;li&gt;downside: more compute&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this feels like an early but real step toward &lt;strong&gt;controlled capability removal&lt;/strong&gt; in llms, imho - not endless dataset cleaning, not post-hoc patching, deliberately localizing where certain knowledge lives is dope&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbmt1/beyond_data_filtering_knowledge_localization_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbmt1/beyond_data_filtering_knowledge_localization_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbmt1/beyond_data_filtering_knowledge_localization_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbsnr</id>
    <title>How evaluating ai models should be done</title>
    <updated>2025-12-14T11:20:43+00:00</updated>
    <author>
      <name>/u/bob_the_scob</name>
      <uri>https://old.reddit.com/user/bob_the_scob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The orginal Article was created by Nexus at &lt;a href="http://theenexus.com"&gt;theenexus.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Crisis of Trust in AI Benchmarks&lt;/p&gt; &lt;p&gt;The AI industry has a dirty secret: the benchmarks everyone uses to compare models are fundamentally broken. Not because the tests themselves are poorly designed, but because they've become targets for optimization rather than measures of true capability.&lt;/p&gt; &lt;p&gt;When benchmark questions and tasks are publicly available, they stop measuring generalization and start measuring memorization. Models are deliberately overfitted to these specific test sets, inflating scores while real-world performance remains mediocre. The result is a marketplace flooded with misleading claims where benchmark scores have become marketing tools rather than meaningful metrics.&lt;/p&gt; &lt;p&gt;At Nexus, we've taken a different approach. We don't trust current industry benchmarks, and we've built an independent evaluation system specifically designed to combat the benchmark gaming that has corrupted AI model assessment.&lt;/p&gt; &lt;h1&gt;The Overfitting Problem: How Benchmarks Became Meaningless&lt;/h1&gt; &lt;h1&gt;The Public Benchmark Trap&lt;/h1&gt; &lt;p&gt;Every major AI benchmark used today‚ÄîMMLU, HumanEval, GSM8K, and others‚Äîshares a fatal flaw: their test sets are public. This creates an irresistible incentive for model developers to optimize specifically for these known questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The process is straightforward:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Benchmark questions are publicly available&lt;/li&gt; &lt;li&gt;Companies include these exact question types in training data&lt;/li&gt; &lt;li&gt;Models learn to pattern-match against known test cases&lt;/li&gt; &lt;li&gt;Benchmark scores increase dramatically&lt;/li&gt; &lt;li&gt;Real-world performance remains stagnant&lt;br /&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This isn't speculation. We've observed this pattern repeatedly in our independent testing. Models claiming massive benchmark advantages consistently fail to demonstrate those capabilities when tested on novel questions they haven't been optimized for.&lt;/p&gt; &lt;h1&gt;Evidence of Benchmark Gaming&lt;/h1&gt; &lt;p&gt;Consider a recent example: Grok Code was benchmarked as beating Gemini by enormous margins in coding tasks. The marketing materials showed impressive graphs with substantial performance gaps. Yet when we tested both models using our independent evaluation, Grok Code barely outperformed our own model‚Äîand we don't even focus on coding in our training datasets.&lt;/p&gt; &lt;p&gt;This discrepancy is not an anomaly. It's evidence of systematic overfitting. If a model truly possessed superior coding intelligence, that advantage should manifest across all coding tasks, not just public benchmark questions.&lt;/p&gt; &lt;p&gt;We've observed similar patterns across multiple model comparisons:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models claiming state-of-the-art reasoning often produce illogical outputs on novel problems&lt;/li&gt; &lt;li&gt;Top-tier models with impressive MMLU scores frequently fail basic comprehension tasks&lt;/li&gt; &lt;li&gt;&amp;quot;Superior&amp;quot; models consistently underperform their benchmark predictions in our testing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The disconnect between claimed capability and actual performance has become so severe &lt;/p&gt; &lt;p&gt;that public benchmarks have lost their value as evaluation tools.&lt;/p&gt; &lt;h1&gt;Our Solution: Closed, Comprehensive Evaluation&lt;/h1&gt; &lt;h1&gt;The Core Evaluation Framework&lt;/h1&gt; &lt;p&gt;Our primary evaluation system consists of &lt;strong&gt;5,000 carefully designed questions and tasks&lt;/strong&gt; that vary systematically across multiple dimensions:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Difficulty Distribution:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;K-12 level content across all major subjects&lt;/li&gt; &lt;li&gt;Professional-level questions requiring domain expertise&lt;/li&gt; &lt;li&gt;Graduate-level reasoning tasks&lt;/li&gt; &lt;li&gt;Edge cases designed to test true understanding&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Category Distribution:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Math and computational tasks&lt;/li&gt; &lt;li&gt;Science questions across physics, chemistry, biology&lt;/li&gt; &lt;li&gt;Writing assessments for coherence and accuracy&lt;/li&gt; &lt;li&gt;Factual knowledge retrieval&lt;/li&gt; &lt;li&gt;Reasoning and logic problems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Subject Balance:&lt;/strong&gt; Approximately 20% of questions fall into each of five major disciplines, ensuring no single domain dominates the evaluation. This balance prevents models from achieving high scores through narrow specialization.&lt;/p&gt; &lt;h1&gt;Transitional Questions: Testing Context Coherence&lt;/h1&gt; &lt;p&gt;The final 250 questions in each discipline are &lt;strong&gt;Transitional Questions (TQs)&lt;/strong&gt;‚Äîspecially designed to bridge subject areas and test contextual understanding.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key characteristics of TQs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-subject integration:&lt;/strong&gt; Require knowledge from two or more disciplines simultaneously&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context dependency:&lt;/strong&gt; Rely on information from previous questions in the conversation history&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coherence testing:&lt;/strong&gt; Evaluate whether models maintain logical consistency across topic shifts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-world simulation:&lt;/strong&gt; Mirror how humans actually use AI‚Äîjumping between topics within a single conversation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For example, a TQ might start with physics concepts, incorporate mathematical calculations, and require writing a technical explanation‚Äîall while referencing specific details from earlier questions in the evaluation session. This design catches models that perform well on isolated questions but struggle with sustained, contextual reasoning‚Äîa critical capability for real-world applications.&lt;/p&gt; &lt;h1&gt;The Closed System Advantage&lt;/h1&gt; &lt;p&gt;Here's the crucial element that makes our evaluation system resistant to gaming: &lt;strong&gt;it's completely closed and anonymous.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;No one knows:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What questions are in our evaluation set&lt;/li&gt; &lt;li&gt;How those questions are structured or phrased&lt;/li&gt; &lt;li&gt;What the distribution of difficulty levels is&lt;/li&gt; &lt;li&gt;Who we are or what organization is conducting these evaluations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Since we maintain complete control over our training data and know exactly what was fed into our model during development, we can guarantee that our evaluation questions were never part of the training set. This ensures we're testing true generalization rather than memorization.&lt;/p&gt; &lt;p&gt;Competing models have no ability to overfit to our evaluation because they don't know it exists. They can't optimize for questions they've never seen. This asymmetry allows us to measure actual intelligence rather than benchmark-specific pattern matching.&lt;/p&gt; &lt;h1&gt;The Accuracy Evaluation System: Beyond Simple Scripts&lt;/h1&gt; &lt;h1&gt;Why Traditional Scripts Don't Work&lt;/h1&gt; &lt;p&gt;Initially, many assume accuracy evaluation could be handled with a simple programmatic script‚Äîcheck if the output matches the expected answer, mark it correct or incorrect, move &lt;/p&gt; &lt;p&gt;on.&lt;/p&gt; &lt;p&gt;This approach fails immediately because of output variability. Even when models produce correct answers, they structure those answers differently every time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Different sentence structures&lt;/li&gt; &lt;li&gt;Varied explanations&lt;/li&gt; &lt;li&gt;Additional context or reasoning&lt;/li&gt; &lt;li&gt;Alternative but equivalent phrasings&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A rigid script would mark many correct answers as failures simply because they don't match a predefined string exactly. This approach is fundamentally incompatible with evaluating natural language.&lt;/p&gt; &lt;h1&gt;The Accuracy LLM: Intelligent Evaluation&lt;/h1&gt; &lt;p&gt;Our solution is an &lt;strong&gt;Accuracy LLM&lt;/strong&gt;‚Äîa specialized language model dedicated entirely to evaluating other models' outputs. This is not a general-purpose model; it's been designed and configured specifically for rigorous, consistent evaluation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Accuracy LLM operates with:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Predefined evaluation parameters:&lt;/strong&gt; Specific criteria it must check for each question type&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured review steps:&lt;/strong&gt; A systematic process it follows to validate outputs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Search API integration:&lt;/strong&gt; Access to specialized search APIs to verify factual claims&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Known answer references:&lt;/strong&gt; For certain question types, predefined correct answers to compare against&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How the Accuracy LLM Works&lt;/h1&gt; &lt;p&gt;When evaluating a model's response, the Accuracy LLM follows a multi-step process:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Step 1: Parse the Question and Response&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Identifies the question type (math, factual, reasoning, etc.)&lt;/li&gt; &lt;li&gt;Extracts the core claim or answer from the model's output&lt;/li&gt; &lt;li&gt;Determines what criteria must be satisfied for correctness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 2: Fact Verification (when applicable)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses specialized search APIs to verify factual claims&lt;/li&gt; &lt;li&gt;Cross-references information against authoritative sources&lt;/li&gt; &lt;li&gt;Checks for internal consistency in the response&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 3: Answer Comparison (when applicable)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For questions with predetermined answers (e.g., math problems), compares the model's answer against the known correct answer&lt;/li&gt; &lt;li&gt;Accounts for equivalent formulations (e.g., &amp;quot;105&amp;quot; = &amp;quot;one hundred five&amp;quot; = &amp;quot;1.05 √ó 10¬≤&amp;quot;)&lt;/li&gt; &lt;li&gt;Identifies if the correct answer is present even if embedded in additional explanation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 4: Quality Assessment&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Evaluates whether the response actually addresses the question&lt;/li&gt; &lt;li&gt;Checks for logical coherence&lt;/li&gt; &lt;li&gt;Assesses completeness of the answer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Step 5: Generate Structured Output&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Accuracy LLM produces output in a specific format with the following structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model identifier:&lt;/strong&gt; Model Name in double parentheses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Question number:&lt;/strong&gt; Question ID in double curly braces&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Original output:&lt;/strong&gt; The model's actual response in triple square brackets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expected answer:&lt;/strong&gt; The correct answer in double square brackets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Evaluation result:&lt;/strong&gt; Either &amp;quot;Correct&amp;quot; or &amp;quot;Incorrect&amp;quot; in double angle brackets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example of structured output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: Gemini-Pro | Question: 0847&lt;/p&gt; &lt;p&gt;Original Model Output: &amp;quot;Ohm's Law states that voltage equals current times resistance, or V = IR. This means that if you increase the resistance in a circuit while keeping voltage constant, current will decrease proportionally.&amp;quot;&lt;/p&gt; &lt;p&gt;Proper Answer: &amp;quot;Ohm's Law: V = IR, where voltage is directly proportional to current and resistance&amp;quot;&lt;/p&gt; &lt;p&gt;Evaluation: Correct&lt;/p&gt; &lt;p&gt;This structured format enables automated processing while preserving the full context for manual review if needed.&lt;/p&gt; &lt;h1&gt;The Extraction and Analysis Pipeline&lt;/h1&gt; &lt;h1&gt;Automated Processing&lt;/h1&gt; &lt;p&gt;The structured output from the Accuracy LLM flows into an &lt;strong&gt;extraction script&lt;/strong&gt; that processes evaluation results at scale.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The extraction script:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Parses each evaluation output&lt;/strong&gt; into its component parts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extracts correctness indicators&lt;/strong&gt; to determine if responses were correct or incorrect&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Calculates accuracy percentages&lt;/strong&gt; by dividing correct responses by total questions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Generates performance reports&lt;/strong&gt; broken down by category, difficulty, and question type&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creates JSON files&lt;/strong&gt; containing all model outputs for archival and review&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;JSON Output Structure&lt;/h1&gt; &lt;p&gt;All evaluated outputs are preserved in JSON format with the following fields:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;model:&lt;/strong&gt; The name of the model being evaluated (e.g., &amp;quot;CompetitorX&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;question_id:&lt;/strong&gt; Unique identifier for each question (e.g., 847)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;category:&lt;/strong&gt; Subject area (e.g., &amp;quot;Physics&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;difficulty:&lt;/strong&gt; Difficulty level (e.g., &amp;quot;K12&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;question:&lt;/strong&gt; The actual question text (e.g., &amp;quot;Explain Ohm's Law&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;model_output:&lt;/strong&gt; The model's complete response (e.g., &amp;quot;Ohm's Law states that...&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;expected_answer:&lt;/strong&gt; The correct answer (e.g., &amp;quot;V = IR...&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;accuracy_evaluation:&lt;/strong&gt; Result marking (e.g., &amp;quot;Correct&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;accuracy_llm_reasoning:&lt;/strong&gt; Explanation of why it was marked correct/incorrect (e.g., &amp;quot;Response correctly identifies...&amp;quot;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;timestamp:&lt;/strong&gt; When the evaluation occurred (e.g., &amp;quot;2025-10-15T14:23:11Z&amp;quot;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This structured data enables:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Long-term performance tracking&lt;/li&gt; &lt;li&gt;Anomaly detection&lt;/li&gt; &lt;li&gt;Category-specific analysis&lt;/li&gt; &lt;li&gt;Manual spot-checking for quality assurance&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Manual Review for Anomalies&lt;/h1&gt; &lt;p&gt;While the system is largely automated, we maintain manual review capabilities for quality assurance. The JSON outputs are regularly sampled to identify:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Potential bugs in the Accuracy LLM's evaluation logic&lt;/li&gt; &lt;li&gt;Edge cases that might require refinement&lt;/li&gt; &lt;li&gt;Patterns of systemic errors&lt;/li&gt; &lt;li&gt;Unexpected model behaviors&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Pre-Evaluation Validation: Ensuring System Accuracy&lt;/h1&gt; &lt;p&gt;Before deploying this evaluation system at scale, we conducted extensive validation testing &lt;/p&gt; &lt;p&gt;to ensure the Accuracy LLM itself was performing correctly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Validation process included:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Known answer testing:&lt;/strong&gt; Running questions with objectively correct answers through the system&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-validation:&lt;/strong&gt; Having multiple evaluators (human and AI) assess the same outputs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Edge case testing:&lt;/strong&gt; Deliberately submitting ambiguous or borderline responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistency checks:&lt;/strong&gt; Running identical responses through evaluation multiple times to ensure deterministic results&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Human audits:&lt;/strong&gt; Manual review of thousands of evaluation decisions to identify systematic biases&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The goal was to confirm that the Accuracy LLM could reliably distinguish correct from incorrect responses across diverse question types. After extensive testing and refinement, we're confident the system achieves extremely high evaluation accuracy.&lt;/p&gt; &lt;h1&gt;Category-Specific Challenges: The Subjectivity Problem&lt;/h1&gt; &lt;h1&gt;The Writing Evaluation Challenge&lt;/h1&gt; &lt;p&gt;While our system performs exceptionally well on objective questions, we've encountered challenges with subjective evaluation‚Äîparticularly in assessing writing quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Initial approach (too lenient):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Early versions of the accuracy evaluation would assess writing tasks like this:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt:&lt;/strong&gt; &amp;quot;Write a 3 paragraph, 12 sentence long paper on Ohm's Law&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Evaluation criteria:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Does it discuss Ohm's Law? ‚úì&lt;/li&gt; &lt;li&gt;Is it 3 paragraphs? ‚úì&lt;/li&gt; &lt;li&gt;Is it 12 sentences? ‚úì&lt;/li&gt; &lt;li&gt;Is the grammar correct? ‚úì&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Result:&lt;/strong&gt; Nearly 100% scores across all models, even when the content quality was poor.&lt;/p&gt; &lt;p&gt;The problem was clear: structural requirements and grammar checking are insufficient for evaluating writing quality. A response could be technically correct while being repetitive, superficial, or poorly organized.&lt;/p&gt; &lt;h1&gt;Refined Writing Evaluation&lt;/h1&gt; &lt;p&gt;We've since implemented more sophisticated evaluation criteria for writing tasks:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Content quality metrics:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Depth of explanation:&lt;/strong&gt; Does the writing demonstrate genuine understanding?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Clarity:&lt;/strong&gt; Is the explanation accessible to the intended audience?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Organization:&lt;/strong&gt; Is information presented in a logical sequence?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Completeness:&lt;/strong&gt; Are all relevant aspects of the topic covered?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Originality:&lt;/strong&gt; Does the writing avoid repetitive or formulaic patterns?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Implementation approach:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The Accuracy LLM now uses a rubric-based evaluation for writing tasks, scoring multiple dimensions independently and combining them into an overall assessment. This provides more granular feedback and better differentiates between adequate and excellent writing.&lt;/p&gt; &lt;p&gt;However, we acknowledge this remains an imperfect science. Writing quality contains irreducible subjective elements, and we continue refining these evaluation methods.&lt;/p&gt; &lt;h1&gt;Genre-Based Testing: Focused Evaluation&lt;/h1&gt; &lt;p&gt;In addition to our comprehensive 5,000-question core evaluation, we maintain &lt;strong&gt;Genre-Based Tests&lt;/strong&gt;‚Äîsmaller, focused assessment sets ranging from 500 to 1,500 questions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Purpose of Genre-Based Tests:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Edge case exploration:&lt;/strong&gt; Testing unusual or boundary conditions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety validation:&lt;/strong&gt; Ensuring models don't produce harmful outputs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized capability testing:&lt;/strong&gt; Deep dives into specific capabilities like coding, math, or reasoning&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Third-party integration testing:&lt;/strong&gt; Evaluating performance when models have access to external tools like search APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rapid iteration:&lt;/strong&gt; Smaller test sets enable faster experimentation and refinement&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Manual review advantage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smaller scope of Genre-Based Tests makes comprehensive manual review feasible. We can examine every response in detail, catching nuances that automated evaluation might miss.&lt;/p&gt; &lt;p&gt;These focused tests complement the broad coverage of our core evaluation, providing both breadth and depth in our assessment methodology.&lt;/p&gt; &lt;h1&gt;Real-World Findings: What We've Discovered&lt;/h1&gt; &lt;h1&gt;Benchmark Claims vs. Measured Performance&lt;/h1&gt; &lt;p&gt;Our independent testing has revealed a consistent pattern: &lt;strong&gt;the claimed performance of top-tier models rarely materializes in our evaluation framework.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specific observations:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Inflated benchmark scores don't translate&lt;/strong&gt; Models scoring at the top of public benchmarks often perform at the middle of the pack in our testing. The correlation between public &lt;/p&gt; &lt;p&gt;benchmark performance and our evaluation results is surprisingly weak.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. &amp;quot;Inferior&amp;quot; models outperform &amp;quot;superior&amp;quot; ones&lt;/strong&gt; We routinely observe models that score lower on public benchmarks outperforming their supposedly superior competitors in our tests. This suggests public benchmarks are measuring something other than general intelligence.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Claimed capabilities don't manifest&lt;/strong&gt; Models marketed with specific capabilities‚Äî&amp;quot;best in class reasoning,&amp;quot; &amp;quot;superior coding,&amp;quot; &amp;quot;state-of-the-art math&amp;quot;‚Äîfrequently fail to demonstrate those advantages when rigorously tested on novel problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Private benchmarks are also gamed&lt;/strong&gt; Some organizations have developed &amp;quot;private&amp;quot; benchmarks as alternatives to public ones. However, we've found that claimed performance on these private benchmarks also fails to materialize in our testing, suggesting gaming occurs even with supposedly closed evaluation sets.&lt;/p&gt; &lt;h1&gt;Chain-of-Thought Models: A Double-Edged Sword&lt;/h1&gt; &lt;p&gt;We've made particularly interesting observations about models that use explicit Chain-of-Thought (CoT) reasoning:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance degradation with context length:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;As context windows grow larger‚Äîas conversations become longer and more complex‚ÄîCoT models begin generating increasingly nonsensical reasoning chains. This degradation then propagates into their final outputs, producing incorrect answers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We've reviewed the &amp;quot;thinking&amp;quot; processes from third-party CoT models and found that the reasoning becomes circular, contradictory, or completely unmoored from the original question as context accumulates. The very mechanism intended to improve performance‚Äîexplicit reasoning‚Äîbecomes a liability.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Implication:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This suggests that current CoT implementations lack robust mechanisms for maintaining coherence over long contexts. The &amp;quot;thinking&amp;quot; process requires as much intelligence as the answering process, and current approaches haven't solved this effectively.&lt;/p&gt; &lt;h1&gt;The Search Dependency Problem&lt;/h1&gt; &lt;p&gt;One of our most revealing findings concerns model performance with and without web search access.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The experiment:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We evaluated models in two conditions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;With access to web search APIs&lt;/li&gt; &lt;li&gt;Without any external search capabilities&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When stripped of search access, we observed &lt;strong&gt;significantly degraded performance across all tasks&lt;/strong&gt;‚Äîincluding those that don't require access to current information.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tasks that shouldn't require search:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Mathematical calculations&lt;/li&gt; &lt;li&gt;Logical reasoning puzzles&lt;/li&gt; &lt;li&gt;Coding problems&lt;/li&gt; &lt;li&gt;Explaining established scientific concepts&lt;/li&gt; &lt;li&gt;Language translation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Yet performance dropped substantially even on these tasks when search was disabled.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What this means:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe this is compelling evidence that these models are not as &amp;quot;intelligent&amp;quot; as their developers claim. True intelligence should not depend on external search for tasks that require only reasoning and knowledge synthesis.&lt;/p&gt; &lt;p&gt;Models appear to be using search as a crutch‚Äîcompensating for gaps in genuine reasoning capability by retrieving information even when that information should already be encoded in their parameters.&lt;/p&gt; &lt;h1&gt;Small Models, Big Performance&lt;/h1&gt; &lt;p&gt;Perhaps our most surprising finding challenges the industry's &amp;quot;bigger is better&amp;quot; paradigm:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our model specifications:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Approximately 3 billion parameters&lt;/li&gt; &lt;li&gt;Roughly 425 times smaller than competing models on average&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Performance results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Similar to much larger models across most categories&lt;/li&gt; &lt;li&gt;Superior to larger models in several specific domains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why smaller models can compete:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Our hypothesis, supported by our testing data, is that &lt;strong&gt;massive models suffer from internal conflicts and hallucinations caused by having access to vast amounts of irrelevant data.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The noise problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When a model is trained on everything, it has difficulty determining what information is relevant to a given task. Contradictory training data creates internal conflicts. Irrelevant information introduces noise into reasoning processes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The focus advantage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A smaller, more focused model with a carefully curated training set:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has less internal contradiction&lt;/li&gt; &lt;li&gt;Experiences less noise in its reasoning processes&lt;/li&gt; &lt;li&gt;Can achieve higher accuracy on the tasks it's designed for&lt;/li&gt; &lt;li&gt;Requires less computational resources for inference&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Industry implications:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This suggests the race to build ever-larger models may be misguided. The future of AI performance may lie not in raw parameter count but in intelligent architecture design and high-quality, focused training data.&lt;/p&gt; &lt;h1&gt;The Cost of Independent Evaluation&lt;/h1&gt; &lt;p&gt;One significant challenge we face is the &lt;strong&gt;financial cost of evaluating competing models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How costs accumulate:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;To compare our model against competitors fairly, we must run their models through our entire evaluation suite. Since most competing models are only available through paid APIs, this means:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5,000+ API calls per full evaluation&lt;/li&gt; &lt;li&gt;Multiple evaluations as models update&lt;/li&gt; &lt;li&gt;Testing multiple competing models&lt;/li&gt; &lt;li&gt;Genre-based test evaluations&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cost breakdown example:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For a single comprehensive evaluation of one competing model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5,000 questions √ó $0.01 per API call (average) = $50&lt;/li&gt; &lt;li&gt;Accuracy LLM evaluation of those outputs = additional processing costs&lt;/li&gt; &lt;li&gt;Multiple evaluation runs for consistency checking = multiply by 3-5x&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When testing 10 different competing models with periodic re-evaluation as they update, costs can easily reach thousands of dollars monthly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why we bear this cost:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Despite the expense, we consider this investment essential. The only way to make honest claims about relative performance is to actually test those models rigorously. We refuse to rely on public benchmarks or marketing claims that we know to be misleading.&lt;/p&gt; &lt;p&gt;This commitment to honest evaluation differentiates us from competitors who make performance claims based solely on cherry-picked or gamed benchmarks.&lt;/p&gt; &lt;h1&gt;Limitations and Ongoing Refinement&lt;/h1&gt; &lt;p&gt;We acknowledge our evaluation system, while significantly more reliable than public benchmarks, is not perfect.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current limitations:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Subjective evaluation challenges&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Writing quality assessment remains partially subjective&lt;/li&gt; &lt;li&gt;Creative tasks are difficult to evaluate objectively&lt;/li&gt; &lt;li&gt;Style preferences vary across use cases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Coverage limitations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5,000 questions, while comprehensive, cannot cover every possible task&lt;/li&gt; &lt;li&gt;Edge cases continually emerge as models evolve&lt;/li&gt; &lt;li&gt;New capabilities require new evaluation questions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Accuracy LLM dependency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Our evaluation quality depends on the Accuracy LLM's performance&lt;/li&gt; &lt;li&gt;We must continually validate that the Accuracy LLM remains unbiased&lt;/li&gt; &lt;li&gt;As evaluated models improve, evaluation criteria must evolve&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cost constraints&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Comprehensive evaluation of many models is expensive&lt;/li&gt; &lt;li&gt;We must prioritize which models to evaluate most thoroughly&lt;/li&gt; &lt;li&gt;API costs limit evaluation frequency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Ongoing refinement:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We treat our evaluation system as a living framework requiring continuous improvement:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Regular manual audits of evaluation decisions&lt;/li&gt; &lt;li&gt;Addition of new question types as capabilities expand&lt;/li&gt; &lt;li&gt;Refinement of subjective evaluation criteria&lt;/li&gt; &lt;li&gt;Validation testing of the Accuracy LLM itself&lt;/li&gt; &lt;li&gt;Community feedback on evaluation methodology&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Future of AI Evaluation&lt;/h1&gt; &lt;h1&gt;What Needs to Change&lt;/h1&gt; &lt;p&gt;The AI industry must move beyond benchmark gaming toward genuine, honest evaluation. This requires:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Closed evaluation sets&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test questions must not be public&lt;/li&gt; &lt;li&gt;Evaluation methodologies should be transparent but test content should remain private&lt;/li&gt; &lt;li&gt;Multiple independent evaluation organizations to prevent single points of gaming&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Comprehensive assessment&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Evaluations must test diverse capabilities across many domains&lt;/li&gt; &lt;li&gt;Context coherence and long-form reasoning must be assessed&lt;/li&gt; &lt;li&gt;Edge cases and failure modes must be explored systematically&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Real-world task simulation&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Evaluation should mirror actual use cases&lt;/li&gt; &lt;li&gt;Multi-turn conversations and context maintenance matter&lt;/li&gt; &lt;li&gt;Integration with tools and external resources should be tested&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Transparency in limitations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models should be evaluated for what they cannot do, not just successes&lt;/li&gt; &lt;li&gt;Failure modes should be documented and published&lt;/li&gt; &lt;li&gt;Confidence intervals and error bars should accompany all performance claims&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Independent verification&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Third-party evaluation should be the norm&lt;/li&gt; &lt;li&gt;Model developers' benchmark claims should be treated skeptically&lt;/li&gt; &lt;li&gt;Community-driven evaluation efforts should be supported&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Our Commitment&lt;/h1&gt; &lt;p&gt;At Nexus, we're committed to honest evaluation that reflects genuine model capabilities. We will:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Continue maintaining our closed, comprehensive evaluation framework&lt;/li&gt; &lt;li&gt;Publish evaluation results transparently (within the constraints of protecting our evaluation set)&lt;/li&gt; &lt;li&gt;Refine our methodology based on feedback and new findings&lt;/li&gt; &lt;li&gt;Bear the cost of independent testing rather than relying on marketing claims&lt;/li&gt; &lt;li&gt;Advocate for industry-wide adoption of more rigorous evaluation standards&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;The AI industry's reliance on gameable public benchmarks has created a crisis of trust. Performance claims have become disconnected from real-world capabilities. Models are optimized for test scores rather than genuine intelligence.&lt;/p&gt; &lt;p&gt;At Nexus, we've built an independent evaluation system specifically designed to combat benchmark gaming. Through closed, comprehensive testing with an intelligent Accuracy LLM and careful methodology, we can measure true model performance rather than memorization of known test sets.&lt;/p&gt; &lt;p&gt;Our findings challenge many industry assumptions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Public benchmark scores are poor predictors of real-world performance&lt;/li&gt; &lt;li&gt;Smaller, focused models can compete with massive general-purpose ones&lt;/li&gt; &lt;li&gt;Search dependency reveals gaps in genuine reasoning capability&lt;/li&gt; &lt;li&gt;Chain-of-thought reasoning can degrade in long contexts&lt;/li&gt; &lt;li&gt;Claimed capabilities frequently fail to materialize under rigorous testing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The path forward requires the AI community to embrace honest evaluation, acknowledge the limitations of current benchmarks, and invest in rigorous, independent testing methodologies.&lt;/p&gt; &lt;p&gt;Because in the end, the goal is not to achieve high benchmark scores. The goal is to build AI systems that actually work‚Äîthat genuinely understand, reason, and assist in the complex, nuanced ways that real-world applications demand. And that requires knowing the truth about model performance, even when that truth is uncomfortable.&lt;/p&gt; &lt;p&gt;&lt;em&gt;This evaluation methodology represents our current approach as of October 2025. We welcome feedback, criticism, and suggestions for improvement. Contact us at&lt;/em&gt; [&lt;em&gt;&lt;a href="mailto:nexusdevolpercontact@gmail.com"&gt;nexusdevolpercontact@gmail.com&lt;/a&gt;&lt;/em&gt;](mailto:&lt;a href="mailto:nexusdevolpercontact@gmail.com"&gt;nexusdevolpercontact@gmail.com&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bob_the_scob"&gt; /u/bob_the_scob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbsnr/how_evaluating_ai_models_should_be_done/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbsnr/how_evaluating_ai_models_should_be_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbsnr/how_evaluating_ai_models_should_be_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:20:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbt62</id>
    <title>Journaling with LLMs</title>
    <updated>2025-12-14T11:21:37+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The main benefit of local LLMs is the privacy and I personally feel like my emotions and deep thoughts are the thing I‚Äôm least willing to send through the interwebs.&lt;/p&gt; &lt;p&gt;I‚Äôve been thinking about using local LLMs (gpt-oss-120b most likely as that runs superbly on my Mac) to help me dive deeper, spot patterns, and give guidance when journaling. &lt;/p&gt; &lt;p&gt;Are you using LLMs for things like this? Are there any applications / LLMs / tips and tricks that you‚Äôd recommend? What worked well for you?&lt;/p&gt; &lt;p&gt;(Any workflows or advice about establishing this as a regular habit are also welcome, though not quite the topic of this sub üòÖ)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbt62/journaling_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbt62/journaling_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbt62/journaling_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:21:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1plv12w</id>
    <title>[Idea] Given the leak that was made public before quickly being removed again - CAN a service be built that instantly downloads any upload to HF and seeds it? SHOULD this be done?</title>
    <updated>2025-12-13T20:22:32+00:00</updated>
    <author>
      <name>/u/Competitive_Wait_267</name>
      <uri>https://old.reddit.com/user/Competitive_Wait_267</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;See title ;) Further points:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Context:&lt;/strong&gt; Models from NVIDIA were uploaded to HF yesterday that very likely were not intended to be made public yet (more precisely: The parent folder was uploaded to hf instead of the model itself, it seems). More context here: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1pkpxss/someone_from_nvidia_made_a_big_mistake_and/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;IANAL, so if in doubt, this is all hypothetical and respecting the law in each relevant country of course. (Although I think you can hardly blame users to download publicly available data. Otherwise, taking it to its logical conclusion, we might not be permitted to store &lt;em&gt;anything&lt;/em&gt; being made public, because &lt;em&gt;every&lt;/em&gt; source might change, get taken down, whatever at &lt;em&gt;some&lt;/em&gt; point in the future...)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I understand and sympathize with the decision of the person who took the model down themselves. At the end of the day, there is at least one human behind every mouse slip. What I want to bring up is more along the lines of establishing automatisms for events like this.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;Further points (will edit this section once as long as discussion is ongoing. &lt;strong&gt;Current Edit: 1.&lt;/strong&gt; Grabbing some food after making this edit)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;The legal situation of making models available to other for unlicensed models might be a problem, as was pointed in &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/ntvfdzq/"&gt;this comment&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I think the technical question &amp;quot;How can a community of hobbyists store a big amount of LLMs (most of the LLMs being somewhat familiar to each other, i.e. finetunes, newer versions, ...)?&amp;quot; can be viewed independently from &amp;quot;would it be a good idea to mirror models from HF? (if even legal?)&amp;quot;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Wait_267"&gt; /u/Competitive_Wait_267 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plv12w/idea_given_the_leak_that_was_made_public_before/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T20:22:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1plewrk</id>
    <title>NVIDIA gpt-oss-120b Eagle Throughput model</title>
    <updated>2025-12-13T06:42:30+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"&gt; &lt;img alt="NVIDIA gpt-oss-120b Eagle Throughput model" src="https://external-preview.redd.it/qdVRXmmV8Rf9W9JXBulc6Wu3niNm-zSxeaMwUkpajKs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a65c4d10380bd77d1bb995e698c64823e1fb437" title="NVIDIA gpt-oss-120b Eagle Throughput model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GPT-OSS-120B-Eagle3-throughput is an &lt;strong&gt;optimized speculative decoding module&lt;/strong&gt; built on top of the &lt;em&gt;OpenAI gpt-oss-120b&lt;/em&gt; base model, designed to improve throughput during text generation. &lt;/li&gt; &lt;li&gt;It uses NVIDIA‚Äôs &lt;strong&gt;Eagle3 speculative decoding&lt;/strong&gt; approach with the Model Optimizer to predict a single draft token efficiently, making it useful for high-concurrency inference scenarios where fast token generation is a priority. &lt;/li&gt; &lt;li&gt;The model is licensed under the &lt;strong&gt;nvidia-open-model-license&lt;/strong&gt; and is intended for commercial and non-commercial use in applications like AI agents, chatbots, retrieval-augmented generation (RAG) systems, and other instruction-following tasks. &lt;a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3-throughput"&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/gpt-oss-120b-Eagle3-throughput"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plewrk/nvidia_gptoss120b_eagle_throughput_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T06:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm9col</id>
    <title>I built an OS style web based Ollama manager GUI that manages a remote or local Ollama Server</title>
    <updated>2025-12-14T08:39:56+00:00</updated>
    <author>
      <name>/u/g023dev</name>
      <uri>https://old.reddit.com/user/g023dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9col/i_built_an_os_style_web_based_ollama_manager_gui/"&gt; &lt;img alt="I built an OS style web based Ollama manager GUI that manages a remote or local Ollama Server" src="https://preview.redd.it/0g0c2jrys47g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d20d6f5163992bcfaf4403d0538bd7d7c77d5364" title="I built an OS style web based Ollama manager GUI that manages a remote or local Ollama Server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an OS style web based GUI Ollama manager that handles model management (pull/delete/view), chat, model listings, terminal, shows a dashboard, lets you compare single prompt against multiple models, conversation export as md or json, and some other things. Sure some menus still have to be hooked up on the main &amp;quot;desktop&amp;quot; and in the settings, but one step at a time. Done in PHP and uses sqlite. Runs as a web app on a server. I call it g023's OllamaMan. Feel free to checkout. It is open source. You probably want to protect the directory it is run in from the public. &lt;a href="https://github.com/g023/g023-OllamaMan"&gt;https://github.com/g023/g023-OllamaMan&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/g023dev"&gt; /u/g023dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0g0c2jrys47g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9col/i_built_an_os_style_web_based_ollama_manager_gui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9col/i_built_an_os_style_web_based_ollama_manager_gui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T08:39:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm7rtw</id>
    <title>Which company makes your favorite local models?</title>
    <updated>2025-12-14T06:57:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(Only 6 options are allowed in a poll! sorry DeepSeek, Kimi, and others.)&lt;/p&gt; &lt;p&gt;Please note I am not asking which open model has highest benchmarks, I am asking what you use locally. On your local setup.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1pm7rtw"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm7rtw/which_company_makes_your_favorite_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm7rtw/which_company_makes_your_favorite_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm7rtw/which_company_makes_your_favorite_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T06:57:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1plv07e</id>
    <title>Optical Context Compression Is Just (Bad) Autoencoding</title>
    <updated>2025-12-13T20:21:25+00:00</updated>
    <author>
      <name>/u/simulated-souls</name>
      <uri>https://old.reddit.com/user/simulated-souls</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There was some recent excitement here regarding Optical Context Compression models like DeepSeek-OCR. The idea is that rendering text to an image and passing into a vision model uses fewer tokens than regular LLM pipelines, saving compute and potentially increasing context length.&lt;/p&gt; &lt;p&gt;This research shows that optical compression actually lags behind old-school autoencoders. Basically, training a model to directly compress text into fewer tokens significantly outperforms the roundabout image-based method.&lt;/p&gt; &lt;p&gt;The optical compression hype might have been premature.&lt;/p&gt; &lt;p&gt;Abstract:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at this https URL&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simulated-souls"&gt; /u/simulated-souls &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2512.03643"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plv07e/optical_context_compression_is_just_bad/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plv07e/optical_context_compression_is_just_bad/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T20:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmb335</id>
    <title>[Project] I built a fully local autonomous QA Agent that writes &amp; fixes unit tests using Ollama (Llama 3 / DeepSeek) or any Cloud APIs</title>
    <updated>2025-12-14T10:35:22+00:00</updated>
    <author>
      <name>/u/Swarnim1312</name>
      <uri>https://old.reddit.com/user/Swarnim1312</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmb335/project_i_built_a_fully_local_autonomous_qa_agent/"&gt; &lt;img alt="[Project] I built a fully local autonomous QA Agent that writes &amp;amp; fixes unit tests using Ollama (Llama 3 / DeepSeek) or any Cloud APIs" src="https://b.thumbs.redditmedia.com/HhLRKuCkPV2eKtVPFr3Pu9Biquk3pNVuD82vsdTLY2s.jpg" title="[Project] I built a fully local autonomous QA Agent that writes &amp;amp; fixes unit tests using Ollama (Llama 3 / DeepSeek) or any Cloud APIs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2Ftripathiji1312%2Fghost"&gt;https://github.com/tripathiji1312/ghost&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Pip:&lt;/strong&gt; &lt;code&gt;pip install ghosttest&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Please give your reviews and give your insights and contributions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Swarnim1312"&gt; /u/Swarnim1312 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pmb335"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmb335/project_i_built_a_fully_local_autonomous_qa_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmb335/project_i_built_a_fully_local_autonomous_qa_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T10:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pluy08</id>
    <title>Those who've deployed a successful self hosted RAG system, what are your hardware specs?</title>
    <updated>2025-12-13T20:18:47+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm working on a &lt;strong&gt;self hosted rag system&lt;/strong&gt; and having a difficult time figuring out the &lt;strong&gt;hardware specs&lt;/strong&gt; for the server. Feeling overwhelmed that i'll either choose a setup that won't be enough or i'll end up choosing something that's an overkill.&lt;/p&gt; &lt;p&gt;So decided it's best to ask others who've been through the same situation, those of you who've deployed a successful self hosted system, &lt;strong&gt;what are your hardware specs ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current setup and intended use:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The idea is simple, letting the user talk to their files. They'll have the option to upload to upload a bunch of files, and then they could chat with the model about these files (documents and images).&lt;/p&gt; &lt;p&gt;I'm using docling with rapidocr for parsing documents, moondream 2for describing images., bge large embeddings v1.5 for embeddings, weaviate for vector db, and ollama qwen2.5-7b-instruct-q6 for response generation.&lt;/p&gt; &lt;p&gt;Rn i'm using Nvidia A16 (16Gb vram with 64 Gb ram) and 6 cpu cores.&lt;/p&gt; &lt;p&gt;I Would really love to hear what kind of setups others (who've successfully deployed a rag setup) are running , and what sort of latency/token speeds they're getting.&lt;/p&gt; &lt;p&gt;If you don't have an answer but you are just as interested as me to find out more about those hardware specs, please upvote, so that it would get the attention and reach out to more people.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Big&lt;/strong&gt; &lt;strong&gt;thanks&lt;/strong&gt; in advance for your help ‚ù§Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pluy08/those_whove_deployed_a_successful_self_hosted_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pluy08/those_whove_deployed_a_successful_self_hosted_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pluy08/those_whove_deployed_a_successful_self_hosted_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T20:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm8x8e</id>
    <title>AI assisted coding with open weight models</title>
    <updated>2025-12-14T08:11:33+00:00</updated>
    <author>
      <name>/u/nonerequired_</name>
      <uri>https://old.reddit.com/user/nonerequired_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, &lt;/p&gt; &lt;p&gt;TLDR: I need good tool and good model for coding &lt;/p&gt; &lt;p&gt;I was using Cursor extensively. I bought 20$ and Auto can do lots of good things, and it was free. So I didn‚Äôt think too much about other coding tools and models. Recently, Cursor made Auto paid. I did use all my limits after 15 days. I am looking for a good coding agent, but I have a hard time finding a good one. I used Zed with these models:&lt;/p&gt; &lt;p&gt;GLM 4.6 via coding plan: &lt;/p&gt; &lt;p&gt;That was $3, so it was a very good deal. While it was not as good as Cursor, it was okay. But speed is a real problem. I don‚Äôt know how Cursor is lightning fast. I am not waiting for a long time to iterate.&lt;/p&gt; &lt;p&gt;Qwen from qwen cli. I used the auth token and their OpenAI endpoint in Zed. &lt;/p&gt; &lt;p&gt;Qwen is good to create a project from scratch, but it has a very hard time editing specific lines. Mostly, it deletes all the code in file and just writes a function that needed to be edited. I somehow solved it after prompting for a while, but the new problem was speed. It was hell slow, especially after 128k context. Most of the time, I had to end the chat and open a new one just for the unbearable speeds. &lt;/p&gt; &lt;p&gt;At this point, speed was very slow, and models were not intelligent enough. I think maybe the problem is the tool (in that case, Zed). I switched to the Cursor and added custom models. It felt better, but I still have problems.&lt;/p&gt; &lt;p&gt;Glm 4.6 via coding plan:&lt;/p&gt; &lt;p&gt;I get the best results from it, but it is still not as good as Cursor Auto and very, very slow. I wouldn‚Äôt mind solving a problem in one shot or 3-4 shots, but spending time became unbearable. &lt;/p&gt; &lt;p&gt;Qwen and most free models from openrouter:&lt;/p&gt; &lt;p&gt;There were problems with tool calling, especially Amazon Nova 2 Lite reading a file over and over and without changing anything. I had to terminate tasks multiple times because of that. Qwen had tool calling problems too, but it was less severe, but speed‚Ä¶ not good, even not okay-ish. &lt;/p&gt; &lt;p&gt;Sorry for grammar mistakes. English is not my native language &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nonerequired_"&gt; /u/nonerequired_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8x8e/ai_assisted_coding_with_open_weight_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8x8e/ai_assisted_coding_with_open_weight_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8x8e/ai_assisted_coding_with_open_weight_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T08:11:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm0ccl</id>
    <title>I built an open-source MCP server for uv so your agents can self-repair their Python environments (and install their own packages)</title>
    <updated>2025-12-14T00:22:36+00:00</updated>
    <author>
      <name>/u/saadmanrafat</name>
      <uri>https://old.reddit.com/user/saadmanrafat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a tool to give local agents better control over their runtime environments. We all know the pain of an agent writing perfect code, only to fail because a library is missing or the virtual environment is messed up.&lt;/p&gt; &lt;p&gt;I built &lt;code&gt;uv-mcp&lt;/code&gt;, a Model Context Protocol (MCP) server that bridges your agent (Claude Desktop, Gemini CLI, or any MCP-compliant client) with &lt;a href="https://github.com/astral-sh/uv"&gt;&lt;strong&gt;uv&lt;/strong&gt;&lt;/a&gt;, the blazing-fast Python package manager.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt; Instead of just &lt;em&gt;telling&lt;/em&gt; you to &lt;code&gt;pip install pandas&lt;/code&gt;, your agent can now:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Diagnose issues:&lt;/strong&gt; Check if the venv exists, if &lt;code&gt;pyproject.toml&lt;/code&gt; is valid, and if dependencies are out of sync.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-Repair:&lt;/strong&gt; Automatically create virtual environments and sync lockfiles if they are missing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Install Packages:&lt;/strong&gt; Instantly add dependencies using &lt;code&gt;uv&lt;/code&gt;'s cache (which is significantly faster than pip).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why&lt;/strong&gt; &lt;code&gt;uv&lt;/code&gt;? &lt;/p&gt; &lt;p&gt;Speed is critical for agents. Waiting for &lt;code&gt;pip&lt;/code&gt; to resolve dependencies breaks the flow. &lt;code&gt;uv&lt;/code&gt; is almost instant, meaning your agent doesn't time out or lose context while waiting for an install to finish.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt; Here is a quick video showing the agent diagnosing a broken environment and fixing it itself:&lt;br /&gt; &lt;a href="https://www.youtube.com/watch?v=Tv2dUt73mM8"&gt;Demo&lt;/a&gt; &lt;strong&gt;|&lt;/strong&gt; &lt;a href="https://www.youtube.com/watch?v=Tv2dUt73mM8"&gt;https://www.youtube.com/watch?v=Tv2dUt73mM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/saadmanrafat/uv-mcp"&gt;https://github.com/saadmanrafat/uv-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's fully open source. I‚Äôd love to hear if this fits into your local agent workflows or if there are other &lt;code&gt;uv&lt;/code&gt; features you'd want exposed to the model!&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Your feedbacks are appreciated!&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/saadmanrafat"&gt; /u/saadmanrafat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm0ccl/i_built_an_opensource_mcp_server_for_uv_so_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm0ccl/i_built_an_opensource_mcp_server_for_uv_so_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm0ccl/i_built_an_opensource_mcp_server_for_uv_so_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T00:22:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm9yzy</id>
    <title>Show: A deterministic agent runtime that works with small models (GPT-5-mini, GPT-4o-mini)</title>
    <updated>2025-12-14T09:21:19+00:00</updated>
    <author>
      <name>/u/TraditionalListen994</name>
      <uri>https://old.reddit.com/user/TraditionalListen994</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9yzy/show_a_deterministic_agent_runtime_that_works/"&gt; &lt;img alt="Show: A deterministic agent runtime that works with small models (GPT-5-mini, GPT-4o-mini)" src="https://external-preview.redd.it/NjRjOG9saWkwNTdnMQ-B8_9qWYO_RAPmf4gIEnQjDPfr2OOW9e1-Pv7DoGUY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae5852149251d8d9baa1f4901aee04e87eb11e99" title="Show: A deterministic agent runtime that works with small models (GPT-5-mini, GPT-4o-mini)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I wanted to share a small demo I‚Äôve been working on around an agent runtime design that stays simple enough to work with small, cheap models.&lt;/p&gt; &lt;p&gt;TL;DR&lt;br /&gt; This is a demo web app where the LLM never mutates UI or application state directly.&lt;br /&gt; It only emits validated Intents, which are then executed deterministically by a runtime layer.&lt;/p&gt; &lt;p&gt;Right now the demo runs on GPT-5-mini, using 1‚Äì2 calls per user interaction.&lt;br /&gt; I‚Äôve also tested the same setup with GPT-4o-mini, and it behaves essentially the same.&lt;br /&gt; Based on that, I suspect this pattern could work with even smaller models, as long as the intent space stays well-bounded.&lt;/p&gt; &lt;h1&gt;Why I built this&lt;/h1&gt; &lt;p&gt;A lot of agent demos I see today assume things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;large models&lt;/li&gt; &lt;li&gt;planner loops&lt;/li&gt; &lt;li&gt;retries / reflection&lt;/li&gt; &lt;li&gt;long tool-call chains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That can work, but it also gets expensive very quickly and becomes hard to reason about.&lt;/p&gt; &lt;p&gt;I was curious what would happen if the model‚Äôs role was much narrower:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM ‚Üí figure out what the user wants (intent selection)&lt;/li&gt; &lt;li&gt;Runtime ‚Üí decide whether it‚Äôs valid and apply state changes&lt;/li&gt; &lt;li&gt;UI ‚Üí just render state&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What the demo shows&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;A simple task management UI (Kanban / Table / Todo views)&lt;/li&gt; &lt;li&gt;Natural language input&lt;/li&gt; &lt;li&gt;An LLM generates a structured Intent JSON&lt;/li&gt; &lt;li&gt;The intent is schema-validated&lt;/li&gt; &lt;li&gt;A deterministic runtime converts Intent ‚Üí Effects&lt;/li&gt; &lt;li&gt;Effects are applied to a snapshot (Zustand store)&lt;/li&gt; &lt;li&gt;The UI re-renders purely from state&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;There‚Äôs no planner, no multi-agent setup, and no retry loop.&lt;br /&gt; Just Intent ‚Üí Effect ‚Üí Snapshot.&lt;/p&gt; &lt;p&gt;Internally, the demo uses two very small LLM roles:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;one to parse user input into intents&lt;/li&gt; &lt;li&gt;one (optional) to generate a user-facing response based on what actually happened&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Neither of them directly changes state.&lt;/p&gt; &lt;h1&gt;Why this seems to work with small models&lt;/h1&gt; &lt;p&gt;What surprised me is that once the decision space is explicit:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model doesn‚Äôt need to plan or reason about execution&lt;/li&gt; &lt;li&gt;It only needs to choose which intent fits the input&lt;/li&gt; &lt;li&gt;Invalid or ambiguous cases are handled by the system, not the model&lt;/li&gt; &lt;li&gt;The same prompt structure works across different model sizes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In practice, GPT-5-mini is more than enough, and GPT-4o-mini behaves similarly.&lt;br /&gt; At that point, model size matters less than how constrained the interaction space is.&lt;/p&gt; &lt;h1&gt;What this is not&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Not a multi-agent framework&lt;/li&gt; &lt;li&gt;Not RPA or browser automation&lt;/li&gt; &lt;li&gt;Not production-ready ‚Äî it‚Äôs intentionally a small, understandable demo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Demo + code:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/manifesto-ai/taskflow"&gt;https://github.com/manifesto-ai/taskflow&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://taskflow.manifesto-ai.dev/"&gt;https://taskflow.manifesto-ai.dev&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd love to hear thoughts from people here, especially around:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;how small a model you think this kind of intent-selection approach could go&lt;/li&gt; &lt;li&gt;whether you‚Äôve tried avoiding planners altogether&lt;/li&gt; &lt;li&gt;tradeoffs between model autonomy vs deterministic runtimes&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions or clarify details.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TraditionalListen994"&gt; /u/TraditionalListen994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ln1b16ii057g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9yzy/show_a_deterministic_agent_runtime_that_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9yzy/show_a_deterministic_agent_runtime_that_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T09:21:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm36fl</id>
    <title>Local AI: Managing VRAM by dynamically swapping models via API</title>
    <updated>2025-12-14T02:42:56+00:00</updated>
    <author>
      <name>/u/PersianDeity</name>
      <uri>https://old.reddit.com/user/PersianDeity</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kept wanting automation pipelines that could call different models for different purposes, sometimes even across different runtimes or servers (Ollama, LM Studio, Faster-Whisper, TTS servers, etc.).&lt;/p&gt; &lt;p&gt;The problem is I only have 16 GB of VRAM, so I can‚Äôt keep everything loaded at once. I didn‚Äôt want to hard-code one model per pipeline, manually start and stop runtimes just to avoid OOM, or limit myself to only running one pipeline at a time.&lt;/p&gt; &lt;p&gt;So I built a lightweight, easy-to-implement control plane that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dynamically loads and unloads models on demand (easy to add additional runtimes)&lt;/li&gt; &lt;li&gt;Routes requests to different models based on task&lt;/li&gt; &lt;li&gt;Runs one request at a time using a queue to avoid VRAM contention, and groups requests for the same model together to reduce reload overhead&lt;/li&gt; &lt;li&gt;Exposes a single API for all runtimes, so you only configure one endpoint to access all models&lt;/li&gt; &lt;li&gt;Spins models up and down automatically and queues tasks based on what‚Äôs already loaded&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The next step is intelligently running more than one model concurrently when VRAM allows.&lt;/p&gt; &lt;p&gt;The core idea is treating models as &lt;strong&gt;on-demand workloads&lt;/strong&gt; rather than long-running processes.&lt;/p&gt; &lt;p&gt;It‚Äôs open source (MIT). Mostly curious:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are others handling multi-model local setups with limited VRAM?&lt;/li&gt; &lt;li&gt;Any scheduling or eviction strategies you‚Äôve found work well?&lt;/li&gt; &lt;li&gt;Anything obvious I‚Äôm missing or overthinking?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; &lt;a href="https://github.com/Dominic-Shirazi/ConductorAPI.git"&gt;https://github.com/Dominic-Shirazi/ConductorAPI.git&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PersianDeity"&gt; /u/PersianDeity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm36fl/local_ai_managing_vram_by_dynamically_swapping/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm36fl/local_ai_managing_vram_by_dynamically_swapping/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm36fl/local_ai_managing_vram_by_dynamically_swapping/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T02:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm8va8</id>
    <title>Which models to try as a beginner? I got a 3090ti</title>
    <updated>2025-12-14T08:08:03+00:00</updated>
    <author>
      <name>/u/salary_pending</name>
      <uri>https://old.reddit.com/user/salary_pending</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title. I am a beginner and trying to understand how the models work. Different architectures, LoRas, uncensored models, coding models, etc.&lt;/p&gt; &lt;p&gt;I've tried GPT OSS 20b and it's cool but it doesn't do anything the free GPT 5 version would do.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/salary_pending"&gt; /u/salary_pending &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8va8/which_models_to_try_as_a_beginner_i_got_a_3090ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8va8/which_models_to_try_as_a_beginner_i_got_a_3090ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm8va8/which_models_to_try_as_a_beginner_i_got_a_3090ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T08:08:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1plpc6h</id>
    <title>Mistral 3 Large is DeepSeek V3!?</title>
    <updated>2025-12-13T16:24:39+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt; &lt;img alt="Mistral 3 Large is DeepSeek V3!?" src="https://b.thumbs.redditmedia.com/yYDI9PVijp_cl2RjrFgG1_MnObuOyb3iYTmzt8uL1_I.jpg" title="Mistral 3 Large is DeepSeek V3!?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Mistral 3 and DeepSeek V3.2, we got two major open-weight LLMs this month already. I looked into DeepSeek V3.2 last week and just caught up with reading through the config of the Mistral 3 architecture in more detail. &lt;/p&gt; &lt;p&gt;Interestingly, based on &lt;a href="https://mistral.ai/news/mistral-3"&gt;their official announcement post&lt;/a&gt;, Mistral 3 and DeepSeek V3.2 have an almost identical size, 671B and 673B, which makes for an interesting comparison, I thought! &lt;/p&gt; &lt;p&gt;Unfortunately, there is no technical report on Mistral 3 that contains more information about the model development. However, since it‚Äôs an open-weight model, we do have the &lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-NVFP4/blob/main/params.json"&gt;model weights on the HuggingFace Model Hub&lt;/a&gt;, though. So, l was taking a closer look at Mistral 3 Large yesterday, and it turns out to be exactly the same architecture as DeepSeek V3/V3.1. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/70lznwrbzz6g1.png?width=2846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aca49968a91f54b80594024ab98b9cd968be8bdf"&gt;https://preview.redd.it/70lznwrbzz6g1.png?width=2846&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aca49968a91f54b80594024ab98b9cd968be8bdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The only difference is that they increased the size of the experts by a factor of 2 while decreasing the number of experts by the same factor. This keeps the number of expert parameters constant, but it should help a bit with latency (1 big expert is faster than 2 smaller experts since there are fewer operations to deal with). &lt;/p&gt; &lt;p&gt;I think that Mistral 3 reusing the DeepSeek V3 architecture is totally fair in the spirit of open source. I am just surprised by it, because I haven't seen anyone mentioning that yet. &lt;/p&gt; &lt;p&gt;However, while it‚Äôs effectively the same architecture, it is likely the Mistral team trained Mistral 3 from scratch rather than initializing it from DeepSeek V3 and further training it, because Mistral uses its own tokenizer. &lt;/p&gt; &lt;p&gt;Next to Kimi K2, Mistral 3 Large is now the second major model to use the DeepSeek V3 architecture. However, where the Kimi K2 team scaled up the model size from 673B to 1 trillion, the Mistral 3 team only changed the expert size ratio and added a vision encoder for multimodal support. But yes, why not? I think DeepSeek V3 is a pretty solid architecture design, plus it has these nice MoE and MLA efficiency aspects to it. So, why change what ain‚Äôt broke? A lot of the secret sauce these days is in the training pipeline as well as the inference scaling strategies.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plpc6h/mistral_3_large_is_deepseek_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T16:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1plxdeg</id>
    <title>Mistral 3 llama.cpp benchmarks</title>
    <updated>2025-12-13T22:05:08+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are some benchmarks using a few different GPUs. I'm using unsloth models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ministral 3 14B Instruct 2512 on Hugging Face&lt;/p&gt; &lt;p&gt;HF list &amp;quot; The largest model in the Ministral 3 family, Ministral 3 14B offers frontier capabilities and performance comparable to its larger Mistral Small 3.2 24B counterpart. A powerful and efficient language model with vision capabilities.&amp;quot;&lt;/p&gt; &lt;p&gt;System is Kubuntu OS&lt;/p&gt; &lt;p&gt;All benchmarks done using &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;llama.cpp&lt;/a&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/10879"&gt;Vulkan&lt;/a&gt; backend &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b7273/llama-b7273-bin-ubuntu-vulkan-x64.tar.gz"&gt;build: c4c10bfb8 (7273)&lt;/a&gt; &lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF/resolve/main/Ministral-3-14B-Instruct-2512-UD-Q6_K_XL.gguf?download=true"&gt;Q6_K_XL&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model &lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B Q6_K&lt;/td&gt; &lt;td align="left"&gt; 10.62 GiB&lt;/td&gt; &lt;td align="left"&gt;13.51 B&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Ministral-3-14B-Instruct-2512-UD-Q6_K_XL.gguf or Ministral-3-14B-Reasoning-2512-Q6_K_L.gguf&lt;/p&gt; &lt;p&gt;AMD Radeon &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt;RX 7900 GRE&lt;/a&gt; 16GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;766.85 ¬± 0.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;43.51 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Ryzen &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;6800H with 680M &lt;/a&gt;on 64GB DDR5&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;117.81 ¬± 1.60&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;3.84 ¬± 0.30&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX-1080 Ti &lt;/a&gt;11GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;194.15 ¬± 0.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;26.64 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX1080 Ti&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/p102-101.c3284"&gt;P102-100&lt;/a&gt; 21GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;175.58 ¬± 0.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;25.11 ¬± 0.11&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX-1080 Ti&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; 19GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;147.12 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;22.00 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nvidia &lt;a href="https://www.techpowerup.com/gpu-specs/p102-101.c3284"&gt;P102-100&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; 18GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;139.66 ¬± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.84 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080.c2839"&gt;GTX-1080&lt;/a&gt; and &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; 16GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;132.84 ¬± 2.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.54 ¬± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070&lt;/a&gt; x 3 total 24GB Vram&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;114.89 ¬± 1.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;17.06 ¬± 0.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Combined sorted by tg128 t/s speed&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AMD Radeon RX 7900 GRE (16GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;766.85&lt;/td&gt; &lt;td align="left"&gt;43.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 Ti (11GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;194.15&lt;/td&gt; &lt;td align="left"&gt;26.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 Ti + P102-100 (21GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;175.58&lt;/td&gt; &lt;td align="left"&gt;25.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 Ti + GTX 1070 (19GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;147.12&lt;/td&gt; &lt;td align="left"&gt;22.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nvidia P102-100 + GTX 1070 (18GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;139.66&lt;/td&gt; &lt;td align="left"&gt;20.84&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1070 √ó 3 (24GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;114.89&lt;/td&gt; &lt;td align="left"&gt;17.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GTX 1080 + GTX 1070 (16GB VRAM)&lt;/td&gt; &lt;td align="left"&gt;132.84&lt;/td&gt; &lt;td align="left"&gt;15.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Ryzen 6800H with 680M iGPU&lt;/td&gt; &lt;td align="left"&gt;117.81&lt;/td&gt; &lt;td align="left"&gt;3.84&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Nvidia P102-100 unable to run without using &lt;code&gt;-ngl 39&lt;/code&gt; offload flag&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Nvidia P102-100&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;127.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nvidia P102-100&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.14&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plxdeg/mistral_3_llamacpp_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plxdeg/mistral_3_llamacpp_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plxdeg/mistral_3_llamacpp_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm0oge</id>
    <title>Success on running a large, useful LLM fast on NVIDIA Thor!</title>
    <updated>2025-12-14T00:38:36+00:00</updated>
    <author>
      <name>/u/catplusplusok</name>
      <uri>https://old.reddit.com/user/catplusplusok</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It took me weeks to figure this out, so want to share!&lt;/p&gt; &lt;p&gt;A good base model choice is MOE with low activated experts, quantized to NVFP4, such as Qwen3-Next-80B-A3B-Instruct-NVFP4 from huggingface. Thor has a lot of memory but it's not very fast, so you don't want to hit all of it for each token, MOE+NVFP4 is the sweet spot. This used to be broken in NVIDIA containers and other vllm builds, but I just got it to work today.&lt;/p&gt; &lt;p&gt;- Unpack and bind my pre-built python venv from &lt;a href="https://huggingface.co/datasets/catplusplus/working-thor-vllm/tree/main"&gt;https://huggingface.co/datasets/catplusplus/working-thor-vllm/tree/main&lt;/a&gt;&lt;br /&gt; - It's basically building vllm and flashinfer from the latest GIT, but there is enough elbow grease that I wanted to share the prebuild. Hope later NVIDIA containers fix MOE support&lt;br /&gt; - Spin up &lt;a href="http://nvcr.io/nvidia/vllm:25.11-py3"&gt;nvcr.io/nvidia/vllm:25.11-py3&lt;/a&gt; docker container, bind my venv and model into it and give command like:&lt;br /&gt; /path/to/bound/venv/bin/python -m vllm.entrypoints.openai.api_server --model /path/to/model ‚Äìserved-model-name MyModelName ‚Äìenable-auto-tool-choice --tool-call-parser hermes.&lt;br /&gt; - Point Onyx AI to the model (&lt;a href="https://github.com/onyx-dot-app/onyx"&gt;https://github.com/onyx-dot-app/onyx&lt;/a&gt;, you need the tool options for that to work), enable web search. You now have capable AI that has access to latest online information.&lt;/p&gt; &lt;p&gt;If you want image gen / editing, QWEN Image / Image Edit with nunchaku lightning checkpoints is a good place to start for similar reasons. Also these understand composition rather than hallucinating extra limbs like better know diffusion models.&lt;/p&gt; &lt;p&gt;All of this should also apply to DGX Spark and it's variations.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/catplusplusok"&gt; /u/catplusplusok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm0oge/success_on_running_a_large_useful_llm_fast_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm0oge/success_on_running_a_large_useful_llm_fast_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm0oge/success_on_running_a_large_useful_llm_fast_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T00:38:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm5ik5</id>
    <title>So.. slightly off topic, but does anyone else here see that the emperor has no clothes?</title>
    <updated>2025-12-14T04:46:43+00:00</updated>
    <author>
      <name>/u/RedParaglider</name>
      <uri>https://old.reddit.com/user/RedParaglider</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished an 18 stage SDD on a very complex code system in a dialectical auto coding structure using a staggered qwen 80b locally first, then rolling over 5 stages in to deepseek as my coding team and GLM 4.6 as my quality team, then deepseek as my security and bug testing team. My total usage to implement the SDD with awesome code quality was &amp;lt;10 cents with the caveat that I did use my m365 corporate subscription to copilot me hone my SDD. &lt;/p&gt; &lt;p&gt;How does the math here make sense on any of this with this stock market? I mean, I do get that having a base subscription to anthropic/gemini/openai/etc to get a deep thinking type model and better yet a research model is super helpful, but it just doesn't seem like on an enterprise level there is a good reason to spend much money on this stuff. It seems like a giant scam at this point. I do understand that I have the ability to run big models from my strix halo 128gb vram system, and that there will always be a premium for enterprise tools, security, etc, etc. But it still seems like this whole market is a giant bullshit bubble. &lt;/p&gt; &lt;p&gt;Am I crazy for thinking that if the world knew how good open source and open weight models were that the market would erupt into flames?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RedParaglider"&gt; /u/RedParaglider &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm5ik5/so_slightly_off_topic_but_does_anyone_else_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm5ik5/so_slightly_off_topic_but_does_anyone_else_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm5ik5/so_slightly_off_topic_but_does_anyone_else_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T04:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm9xzg</id>
    <title>vibe + devstral2 small</title>
    <updated>2025-12-14T09:19:25+00:00</updated>
    <author>
      <name>/u/megadonkeyx</name>
      <uri>https://old.reddit.com/user/megadonkeyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else using this combo?&lt;/p&gt; &lt;p&gt;i think its fairly amazing, rtx3090 with q4 and q4 for kv fits well with 110k context. &lt;/p&gt; &lt;p&gt;these two are little miracle, the first local coding that ive used that can actually do stuff that i would consider useful for production work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/megadonkeyx"&gt; /u/megadonkeyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T09:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1plng6f</id>
    <title>Qwen3 Next generation optimization</title>
    <updated>2025-12-13T15:04:40+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt; &lt;img alt="Qwen3 Next generation optimization" src="https://external-preview.redd.it/lCjR6IsnTKIxBcWFwTVHflX0Ssz6EaHSVZWuf8jfKVA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb67476532fe0a6cc1d75e633320c04f4f773566" title="Qwen3 Next generation optimization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A lot of people were requesting dedicated optimizations, so here they are.&lt;/p&gt; &lt;p&gt;I added an optimized autoregressive delta net computation that short-circuits all the recurrect decay calculation because for `n_seq_tokens = 1` it all collapses. I also made sure to specifically optimize out all unneeded reshapes / conts in that version.&lt;/p&gt; &lt;p&gt;The end result is a 40% generation speed upgrade on my box. If you want, you can try it out and tell me how it works on your end.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plng6f/qwen3_next_generation_optimization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1plnuqu</id>
    <title>OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark.</title>
    <updated>2025-12-13T15:22:27+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt; &lt;img alt="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." src="https://preview.redd.it/l93slaq9oz6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bab51a5045543fef0a9b8a60e5d6a113bc1f0cef" title="OpenAI's flagship model, ChatGPT-5.2 Thinking, ranks most censored AI on Sansa benchmark." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l93slaq9oz6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plnuqu/openais_flagship_model_chatgpt52_thinking_ranks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T15:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm7ab8</id>
    <title>Download before its gone</title>
    <updated>2025-12-14T06:27:11+00:00</updated>
    <author>
      <name>/u/SovietWarBear17</name>
      <uri>https://old.reddit.com/user/SovietWarBear17</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/datasets/DavidBrowne17/epstein-files-20k"&gt;https://huggingface.co/datasets/DavidBrowne17/epstein-files-20k&lt;/a&gt;. Does anyone want an 8b model trained on these files?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SovietWarBear17"&gt; /u/SovietWarBear17 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm7ab8/download_before_its_gone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm7ab8/download_before_its_gone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm7ab8/download_before_its_gone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T06:27:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1plwgun</id>
    <title>8x RTX Pro 6000 server complete</title>
    <updated>2025-12-13T21:25:43+00:00</updated>
    <author>
      <name>/u/koushd</name>
      <uri>https://old.reddit.com/user/koushd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt; &lt;img alt="8x RTX Pro 6000 server complete" src="https://b.thumbs.redditmedia.com/W2GEGsEUMbENTMcyKk5kW114aYR7rHtS0S5yetky0Lc.jpg" title="8x RTX Pro 6000 server complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: 768 GB VRAM via 8x RTX Pro 6000 (4 Workstation, 4 Max-Q) + Threadripper PRO 9955WX + 384 GB RAM&lt;/p&gt; &lt;p&gt;Longer:&lt;/p&gt; &lt;p&gt;I've been slowly upgrading my GPU server over the past few years. I initially started out using it to train vision models for another project, and then stumbled into my current local LLM obsession.&lt;/p&gt; &lt;p&gt;In reverse order:&lt;/p&gt; &lt;p&gt;Pic 5: Initially was using only a single 3080, which I upgraded to a 4090 + 3080. Running on an older 10900k Intel system.&lt;/p&gt; &lt;p&gt;Pic 4: But the mismatched sizes for training batches and compute was problematic, so I upgraded to double 4090s and sold off the 3080. They were packed in there, and during a training run I ended up actually overheating my entire server closet, and all the equipment in there crashed. When I noticed something was wrong and opened the door, it was like being hit by the heat of an industrial oven.&lt;/p&gt; &lt;p&gt;Pic 3: 2x 4090 in their new home. Due to the heat issue, I decided to get a larger case and a new host that supported PCIe 5.0 and faster CPU RAM, the AMD 9950x. I ended up upgrading this system to dual RTX Pro 6000 Workstation edition (not pictured).&lt;/p&gt; &lt;p&gt;Pic 2: I upgraded to 4x RTX Pro 6000. This is where problems started happening. I first tried to connect them using M.2 risers and it would not POST. The AM5 motherboard I had couldn't allocate enough IOMMU addressing and would not post with the 4th GPU, 3 worked fine. There are consumer motherboards out there that could likely have handled it, but I didn't want to roll the dice on another AM5 motherboard as I'd rather get a proper server platform.&lt;/p&gt; &lt;p&gt;In the meantime, my workaround was to use 2 systems (brought the 10900k out of retirement) with 2 GPUs each in pipeline parallel. This worked, but the latency between systems chokes up token generation (prompt processing was still fast). I tried using 10Gb DAC SFP and also Mellanox cards for RDMA to reduce latency, but gains were minimal. Furthermore, powering all 4 means they needed to be on separate breakers (2400w total) since in the US the max load you can put through 120v 15a is ~1600w.&lt;/p&gt; &lt;p&gt;Pic 1: 8x RTX Pro 6000. I put a lot more thought into this before building this system. There were more considerations, and it became a many months long obsession planning the various components: motherboard, cooling, power, GPU connectivity, and the physical rig.&lt;/p&gt; &lt;p&gt;GPUs: I considered getting 4 more RTX Pro 6000 Workstation Editions, but powering those would, by my math, require a third PSU. I wanted to keep it 2, so I got Max Q editions. In retrospect I should have gotten the Workstation editions as they run much quieter and cooler, as I could have always power limited them.&lt;/p&gt; &lt;p&gt;Rig: I wanted something fairly compact and stackable that I could directly connect 2 cards on the motherboard and use 3 bifurcating risers for the other 6. Most rigs don't support taller PCIe cards on the motherboard directly and assume risers will be used. Options were limited, but I did find some generic &amp;quot;EO3&amp;quot; stackable frames on Aliexpress. The stackable case also has plenty of room for taller air coolers.&lt;/p&gt; &lt;p&gt;Power: I needed to install a 240V outlet; switching from 120V to 240V was the only way to get ~4000W necessary out of a single outlet without a fire. Finding 240V high-wattage PSUs was a bit challenging as there are only really two: the Super Flower Leadex 2800W and the Silverstone Hela 2500W. I bought the Super Flower, and its specs indicated it supports 240V split phase (US). It blew up on first boot. I was worried that it took out my entire system, but luckily all the components were fine. After that, I got the Silverstone, tested it with a PSU tester (I learned my lesson), and it powered on fine. The second PSU is the Corsair HX1500i that I already had.&lt;/p&gt; &lt;p&gt;Motherboard: I kept going back and forth between using a Zen5 EPYC or Threadripper PRO (non-PRO does not have enough PCI lanes). Ultimately, the Threadripper PRO seemed like more of a known quantity (can return to Amazon if there were compatibility issues) and it offered better air cooling options. I ruled out water cooling, because the small chance of a leak would be catastrophic in terms of potential equipment damage. The Asus WRX90 had a lot of concerning reviews, so the Asrock WRX90 was purchased, and it has been great. Zero issues on POST or RAM detection on all 8 RDIMMs, running with the expo profile.&lt;/p&gt; &lt;p&gt;CPU/Memory: The cheapest Pro Threadripper, the 9955wx with 384GB RAM. I won't be doing any CPU based inference or offload on this.&lt;/p&gt; &lt;p&gt;Connectivity: The board has 7 PCIe 5.0 x16 cards. At least 1 bifurcation adapter would be necessary. Reading up on the passive riser situation had me worried there would be signal loss at PCIe 5.0 and possibly even 4.0. So I ended up going the MCIO route and bifurcated 3 5.0 lanes. A PCIe switch was also an option, but compatibility seemed sketchy and it's costs $3000 by itself. The first MCIO adapters I purchased were from ADT Link; however, they had two significant design flaws: The risers are powered via the SATA peripheral power, which is a fire hazard as those cable connectors/pins are only rated for 50W or so safely. Secondly, the PCIe card itself does not have enough clearance for the heat pipe that runs along the back of most EPYC and Threadripper boards just behind the PCI slots on the back of the case. Only 2 slots were usable. I ended up returning the ADT Link risers and buying several Shinreal MCIO risers instead. They worked no problem.&lt;/p&gt; &lt;p&gt;Anyhow, the system runs great (though loud due to the Max-Q cards which I kind of regret). I typically use Qwen3 Coder 480b fp8, but play around with GLM 4.6, Kimi K2 Thinking, and Minimax M2 at times. Personally I find Coder and M2 the best for my workflow in Cline/Roo. Prompt processing is crazy fast, I've seen VLLM hit around ~24000 t/s at times. Generation is still good for these large models, despite it not being HBM, around 45-100 t/s depending on model.&lt;/p&gt; &lt;p&gt;Happy to answer questions in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koushd"&gt; /u/koushd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1plwgun"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T21:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
