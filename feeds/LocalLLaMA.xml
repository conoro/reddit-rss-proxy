<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-11T14:38:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r1k5gn</id>
    <title>PSA on llama.cpp —spec-type ngram-mod (use LF not CRLF, 35x speedup)</title>
    <updated>2026-02-11T01:48:39+00:00</updated>
    <author>
      <name>/u/dnsod_si666</name>
      <uri>https://old.reddit.com/user/dnsod_si666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; if using llama-server with —spec-type ngram-mod, and pasting/uploading/sending text files, make sure the files use LF instead of CRLF.&lt;/p&gt; &lt;p&gt;When I would copy a file from vscode and paste into the native llama-server webui with ngram speculative decoding enabled, there was no speed boost for file editing responses. I would only get a speed boost on the models second response (if I asked it to make a minor change to its first response file). Even if I asked the model to repeat the pasted file verbatim it would still be slow.&lt;/p&gt; &lt;p&gt;My files (I’m using a Windows computer) used CRLF (each line ends with “\r\n”) instead of LF (each line ends with “\n”). Models tend to use LF. So most of the ngrams created from my pasted file were useless because of the “\r\n”.&lt;/p&gt; &lt;p&gt;To fix in vscode press the LF/CRLF at the bottom of the screen and select. Or ctrl+shift+p &amp;gt; Change End of Line Sequence. This will change the currently open file.&lt;/p&gt; &lt;p&gt;To make all new files in vscode use LF, make a .vscode/settings.json with&lt;/p&gt; &lt;p&gt;{“files.eol”: “\n”}&lt;/p&gt; &lt;p&gt;To prevent git from automatically converting LF to CRLF run&lt;/p&gt; &lt;p&gt;git config —global core.autocrlf input&lt;/p&gt; &lt;p&gt;To convert existing files use `dos2unix` on wsl or sed or whatever string replace “\r\n” -&amp;gt; “\n”.&lt;/p&gt; &lt;p&gt;Exact command I am running for llama-server: `llama-server -m Devstral-2-123B-Instruct-2512-UD-Q5_K_XL-00001-of-00002.gguf —no-mmap —temp 0.15 —port 55553 —metrics —min-p 0.01 -c 32768 —spec-type ngram-mod —spec-ngram-size-n 24 —draft-min 32 —draft-max 48`&lt;/p&gt; &lt;p&gt;llama.cpp build: 7992 (612db6188) with GNU 13.3.0 for Linux aarch64&lt;/p&gt; &lt;p&gt;Not super helpful cause I’m not providing exact prompts/sampling params or anything, and also the speedup is well documented in the pull (&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19164"&gt;https://github.com/ggml-org/llama.cpp/pull/19164&lt;/a&gt;), but response tok/s went from ~2.3 to ~80 inside the code block.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dnsod_si666"&gt; /u/dnsod_si666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1k5gn/psa_on_llamacpp_spectype_ngrammod_use_lf_not_crlf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T01:48:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1lskx</id>
    <title>Benchmarking LLM Inference on RTX PRO 6000 SE / H100 / H200 / B200</title>
    <updated>2026-02-11T03:02:07+00:00</updated>
    <author>
      <name>/u/NoVibeCoding</name>
      <uri>https://old.reddit.com/user/NoVibeCoding</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"&gt; &lt;img alt="Benchmarking LLM Inference on RTX PRO 6000 SE / H100 / H200 / B200" src="https://b.thumbs.redditmedia.com/DOf_vWH7wAmJQxK0OLukjnzgwaKzSYUfxmW0an86aSU.jpg" title="Benchmarking LLM Inference on RTX PRO 6000 SE / H100 / H200 / B200" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi LocalLlama community. I present an LLM inference throughput benchmark for RTX PRO 6000 SE vs H100, H200, and B200 GPUs, based on the vllm serve and vllm bench serve benchmarking tools, to understand the cost-efficiency of various datacenter GPU options. Pro 6000 is significantly cheaper and built on the latest Blackwell architecture, but it has slower GDDR memory and lacks NVLink compared to H100 / H200 / B200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://medium.com/@koshmanova.n/benchmarking-llm-inference-on-nvidia-b200-h200-h100-and-rtx-pro-6000-66d08c5f0162"&gt;Full article on Medium&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.cloudrift.ai/blog/benchmarking-b200"&gt;Non-medium link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is a follow-up to the &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p93r0w/benchmarking_llm_inference_on_rtx_pro_6000_vs/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;previous benchmark&lt;/a&gt;, incorporating community and collaborator feedback.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Longer context&lt;/strong&gt;: &lt;strong&gt;8K input + 8K output&lt;/strong&gt; tokens (&lt;strong&gt;16K total&lt;/strong&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NVIDIA B200&lt;/strong&gt;: testing the newest Blackwell datacenter GPU&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expert Parallelism&lt;/strong&gt;: investigating vLLM’s &lt;code&gt;--enable-expert-parallel&lt;/code&gt; for MoE models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Using the real GPU cost of ownership&lt;/strong&gt; rather than market pricing to estimate the token price. Market price is subject to supply/demand fluctuations.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Benchmarking Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;The benchmark is optimized for throughput.&lt;/strong&gt; VLLM serves models. The model is split across multiple GPUs using the --tensor-parallel-size VLLM option, if needed. Multiple VLLM instances serve the model; an NGINX load balancer on top distributes requests across them, maximizing throughput (replica parallelism). For example, if only 4 GPUs are required to run the model on an 8-GPU machine, two VLLM instances are launched with --tensor-parallel-size=4, and an NGINX load balancer is used. If all eight GPUs are required, then a single VLLM instance with --tensor-parallel-size=8 is used.&lt;/p&gt; &lt;p&gt;The &lt;strong&gt;vllm bench serve&lt;/strong&gt; tool is used for benchmarking with random data and a sequence length of 1000. The number of concurrent requests is set to 64-256 to ensure the LLM's token-generation capacity is saturated.&lt;/p&gt; &lt;p&gt;Three models are benchmarked to better understand the effect of PCIe communication on the 8xPro6000 server vs. NVLink on the H100/H200/B200.&lt;/p&gt; &lt;p&gt;Here is the model selection and the logic behind it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;GLM-4.5-Air-AWQ-4bit (fits 80GB).&lt;/strong&gt; Testing single-GPU performance and maximum throughput with replica scaling on 8 GPU setups. No PCIE bottleneck.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct-AWQ (fits 320GB).&lt;/strong&gt; This 4-bit-quantized model fits into 4 GPUs. Some PCIe communication overhead in Pro 6000 setups may reduce performance relative to NVLink-enabled datacenter GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.6-FP8 (fits 640GB).&lt;/strong&gt; This model requires all eight GPUs. PCIe communication overhead expected. The H100 and H200 configurations should have an advantage.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Besides raw throughput, graphs show the serving cost per million tokens for each model on its respective hardware. The rental price is set at $0.93 for Pro6000, $1.91 for H100, $2.06 for H200, and $2.68 for B200.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;B200 wins on throughput&lt;/strong&gt;, with the largest gap on the most communication-heavy workload – &lt;strong&gt;GLM-4.6-FP8 (8-way TP):&lt;/strong&gt; B200 is &lt;strong&gt;4.87x&lt;/strong&gt; faster than PRO 6000 (8,036.71 vs 1,651.67 tok/s) – &lt;strong&gt;Qwen3-Coder-480B (4-way TP):&lt;/strong&gt; B200 is &lt;strong&gt;4.02x&lt;/strong&gt; faster than PRO 6000 (6,438.43 vs 1,602.96 tok/s) – &lt;strong&gt;GLM-4.5-Air (single-GPU replicas):&lt;/strong&gt; B200 is &lt;strong&gt;4.22x&lt;/strong&gt; faster than PRO 6000 (9,675.24 vs 2,290.69 tok/s)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;B200 is also the cost efficiency leader&lt;/strong&gt; under updated run-cost estimates. B200’s throughput advantage more than compensates for its higher hourly cost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PRO 6000 is an attractive low-capex option.&lt;/strong&gt; It beats H100 on cost per across all models and is on par with H200 on GLM-4.5-Air.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;H200 is a major step up over H100.&lt;/strong&gt; H200 delivers &lt;strong&gt;~1.83x to 2.14x&lt;/strong&gt; H100 throughput across the three models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;H100 looked worse than expected&lt;/strong&gt; in this specific setup. It’s on par with PRO 6000 in throughput on GLM-4.5-Air and behind all other contenders in cost per token across all workloads.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://i.redd.it/rqm8d7yf6sig1.gif"&gt;https://i.redd.it/rqm8d7yf6sig1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/azhpz6qk6sig1.gif"&gt;https://i.redd.it/azhpz6qk6sig1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/9hbgr6ql6sig1.gif"&gt;https://i.redd.it/9hbgr6ql6sig1.gif&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code and Resources&lt;/h1&gt; &lt;p&gt;The code is available &lt;a href="https://github.com/cloudrift-ai/server-benchmark/tree/main/results/pro6000_h100_h200_b200_01_2026"&gt;here&lt;/a&gt;. Instructions for performing your own benchmark are in the README.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoVibeCoding"&gt; /u/NoVibeCoding &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lskx/benchmarking_llm_inference_on_rtx_pro_6000_se/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T03:02:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1r13m42</id>
    <title>Kimi is so smart</title>
    <updated>2026-02-10T15:22:13+00:00</updated>
    <author>
      <name>/u/Bernice_working_girl</name>
      <uri>https://old.reddit.com/user/Bernice_working_girl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"&gt; &lt;img alt="Kimi is so smart" src="https://preview.redd.it/nlgh125vpoig1.png?width=140&amp;amp;height=120&amp;amp;auto=webp&amp;amp;s=6ea4146398fa55af56e06624284c4c09061bd7b9" title="Kimi is so smart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/nlgh125vpoig1.png?width=1726&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=886a17278e2ccf5692ac0a5ec0d8e4474334900d"&gt;https://preview.redd.it/nlgh125vpoig1.png?width=1726&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=886a17278e2ccf5692ac0a5ec0d8e4474334900d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yv3bxtsvpoig1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b67a5991c5ff32dd3e72eb6717eb617168dcaac9"&gt;https://preview.redd.it/yv3bxtsvpoig1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b67a5991c5ff32dd3e72eb6717eb617168dcaac9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mk02u5fwpoig1.png?width=1578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9d858ecc90244f657a58a1b202c3bccb7267260"&gt;https://preview.redd.it/mk02u5fwpoig1.png?width=1578&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a9d858ecc90244f657a58a1b202c3bccb7267260&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi &amp;gt; ChatGPT = Claude&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bernice_working_girl"&gt; /u/Bernice_working_girl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r13m42/kimi_is_so_smart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T15:22:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1vf4e</id>
    <title>Local voice control for my AI agent - Parakeet STT + Kokoro TTS on Apple Silicon</title>
    <updated>2026-02-11T11:56:26+00:00</updated>
    <author>
      <name>/u/leonidas_elanra</name>
      <uri>https://old.reddit.com/user/leonidas_elanra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vf4e/local_voice_control_for_my_ai_agent_parakeet_stt/"&gt; &lt;img alt="Local voice control for my AI agent - Parakeet STT + Kokoro TTS on Apple Silicon" src="https://external-preview.redd.it/emViZjMzbTV1dWlnMfQvy7W0WCM9-b_6qSDU25Dm7JShPRamXXjE1GlV7dWB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46c8e74e07e52e5e65c2d0234e843dc5ecae8ee4" title="Local voice control for my AI agent - Parakeet STT + Kokoro TTS on Apple Silicon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running an AI agent (OpenClaw + Claude) on a Mac Mini M4 for 2 weeks. The cloud LLM part works great but I wanted the voice interaction to be fully local and fast.&lt;/p&gt; &lt;p&gt;Ended up with Parakeet for STT and Kokoro for TTS, both running on Apple Silicon. Parakeet transcribes in about 240ms, Kokoro responds near instantly. No cloud dependency for the voice layer.&lt;/p&gt; &lt;p&gt;The difference between typing commands and just talking is massive. I went from sitting at my desk all day to working from anywhere. Balcony, walking the dog, couch. I just talk and the agent handles game deployments, server monitoring, social media, the usual stuff.&lt;/p&gt; &lt;p&gt;One funny thing: the STT sometimes transcribes my Greek accent saying the agent's name wrong. He started correcting me like Hermione in Harry Potter: &amp;quot;It's Niko, not Nico!&amp;quot;&lt;/p&gt; &lt;p&gt;Also built a 3D avatar (Mimora) as a browser extension that shows facial expressions when the agent responds. Listening, thinking, happy. Makes the whole thing feel way more natural.&lt;/p&gt; &lt;p&gt;Anyone else running local voice pipelines with their agents? Curious what STT/TTS combos people are using. Full setup documented at &lt;a href="https://myclaw.tech"&gt;https://myclaw.tech&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thread with screenshots: &lt;a href="https://x.com/PlayingInCanvas/status/2021529883919405297"&gt;https://x.com/PlayingInCanvas/status/2021529883919405297&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/leonidas_elanra"&gt; /u/leonidas_elanra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q3zmrtl5uuig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vf4e/local_voice_control_for_my_ai_agent_parakeet_stt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vf4e/local_voice_control_for_my_ai_agent_parakeet_stt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T11:56:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1oag8</id>
    <title>I rebuild my Regency model in 27b</title>
    <updated>2026-02-11T05:02:20+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oag8/i_rebuild_my_regency_model_in_27b/"&gt; &lt;img alt="I rebuild my Regency model in 27b" src="https://preview.redd.it/ghnqgnkkrsig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dfe420cf61d40f98717b6d3f600b35e45207224" title="I rebuild my Regency model in 27b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah. Got $3 bucks left on the vast ai, so I burned them the proper way, rebuilding my old model that thinks it's 1800s. If you have to ask why, then you don't really know me. I'm sure, it will do well in clawdbot, hahahaha: &lt;a href="https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF"&gt;https://huggingface.co/FPHam/Regency-Aghast-27b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ghnqgnkkrsig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oag8/i_rebuild_my_regency_model_in_27b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oag8/i_rebuild_my_regency_model_in_27b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T05:02:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1ixx4</id>
    <title>i finetuned qwen 14b on my discord messages so it can autocomplete for me</title>
    <updated>2026-02-11T00:54:40+00:00</updated>
    <author>
      <name>/u/B44ken</name>
      <uri>https://old.reddit.com/user/B44ken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1ixx4/i_finetuned_qwen_14b_on_my_discord_messages_so_it/"&gt; &lt;img alt="i finetuned qwen 14b on my discord messages so it can autocomplete for me" src="https://external-preview.redd.it/cXh4bTUxNG9qcmlnMRM_V8ZFFQqRSvchLLHbQ0c5NybEjNwn4ZOI2DTNmagB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d33c2f2c3025e6211e089f2944cd13d2929b194" title="i finetuned qwen 14b on my discord messages so it can autocomplete for me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i finetuned qwen on my discord messages so it can autocomplete for me while i type. tab to suggest, shift+tab to accept. kinda like copilot!&lt;/p&gt; &lt;p&gt;the dataset is ~250 conversations from my discord via a &lt;a href="https://github.com/Tyrrrz/DiscordChatExporter"&gt;scraping tool&lt;/a&gt;. a script formats these as chat-ml training samples. it groups messages by conversation (defined as after 1hr of silence), ensures i said something last, and throws out anything with code blocks (not the point of my autocomplete) or links (the model doesn't read those).&lt;/p&gt; &lt;p&gt;the model is qwen3-14b, finetuned with &lt;a href="http://unsloth.ai"&gt;unsloth.ai&lt;/a&gt; + QLoRA on a kaggle gpu. training takes ~15 mins since the dataset is small, but it picks up on how i talk pretty well! it's merged into a `.gguf` to be used as a local &lt;a href="http://ollama.com"&gt;ollama.com&lt;/a&gt; model.&lt;/p&gt; &lt;p&gt;the frontend is a chrome extension. when you press tab, it scrapes the last few messages and what you've started typing from the page, then builds a chat-ml prompt with context and streams a completion from ollama. the suggestion appears in the textbox &lt;em&gt;(fun hack: a zero-width unicode character marks where the suggestion begins)&lt;/em&gt; and shift+tab accepts it.&lt;/p&gt; &lt;p&gt;right now it works on discord, but i'd like it to support any site. other than that, future work could be trying different model sizes. 14b just about uses all the memory i can spare, but i hear 4b or 8b works ok too? i also need more data (maybe from other apps)... 250 samples captures my tone but not much else&lt;/p&gt; &lt;p&gt;it's at &lt;a href="https://github.com/b44ken/finetune"&gt;github.com/b44ken/finetune&lt;/a&gt; if you want to check out the code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/B44ken"&gt; /u/B44ken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/128ehu3ojrig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1ixx4/i_finetuned_qwen_14b_on_my_discord_messages_so_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1ixx4/i_finetuned_qwen_14b_on_my_discord_messages_so_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T00:54:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wzr8</id>
    <title>MOSS-TTS Family Demo</title>
    <updated>2026-02-11T13:11:40+00:00</updated>
    <author>
      <name>/u/Xiami2019</name>
      <uri>https://old.reddit.com/user/Xiami2019</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wzr8/mosstts_family_demo/"&gt; &lt;img alt="MOSS-TTS Family Demo" src="https://external-preview.redd.it/ZWF3YzQwaXo2dmlnMQYgUNtMhgJAzStgVQ4w35r_5rVpc9h29XtIMnbIg4Ti.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=949dc9ddcbe560e99ddef957bc5681bd37ea130d" title="MOSS-TTS Family Demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiami2019"&gt; /u/Xiami2019 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yiwkkuhz6vig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wzr8/mosstts_family_demo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wzr8/mosstts_family_demo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:11:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r14h9u</id>
    <title>Train MoE models 12x faster with 30% less memory! (&lt;15GB VRAM)</title>
    <updated>2026-02-10T15:54:02+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/"&gt; &lt;img alt="Train MoE models 12x faster with 30% less memory! (&amp;lt;15GB VRAM)" src="https://preview.redd.it/ee2jwnijvoig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=27a55cfca0584307d3ba9f2e9cdf3226d1c55646" title="Train MoE models 12x faster with 30% less memory! (&amp;lt;15GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re excited to introduce ~12x faster Mixture of Experts (MoE) training with &lt;strong&gt;&amp;gt;35% less VRAM&lt;/strong&gt; and &lt;strong&gt;~6x longer context&lt;/strong&gt; via our new custom Triton kernels and math optimizations (no accuracy loss). Unsloth repo: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unsloth now supports fast training for MoE architectures including gpt-oss, Qwen3 (30B, 235B, VL, Coder), DeepSeek R1/V3 and GLM (4.5-Air, 4.7, Flash).&lt;/li&gt; &lt;li&gt;gpt-oss-20b fine-tunes in &lt;strong&gt;12.8GB VRAM&lt;/strong&gt;. Qwen3-30B-A3B (16-bit LoRA) uses 63GB.&lt;/li&gt; &lt;li&gt;Our kernels work on both data-center (B200, H100), &lt;strong&gt;consumer&lt;/strong&gt; and older GPUs (e.g., RTX 3090), and FFT, LoRA and QLoRA.&lt;/li&gt; &lt;li&gt;The larger the model and more context you use, &lt;strong&gt;the more pronounced the memory savings from our Unsloth kernels will be&lt;/strong&gt; (efficiency will scale exponentially).&lt;/li&gt; &lt;li&gt;We previously introduced Unsloth Flex Attention for gpt-oss, and these optimizations should make it even more efficient.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In collaboration with Hugging Face, we made all MoE training runs standardized with PyTorch’s new &lt;code&gt;torch._grouped_mm&lt;/code&gt; function. Transformers v5 was recently optimized with ~6x faster MoE than v4 and Unsloth pushes this even further with custom Triton grouped‑GEMM + LoRA kernels for an &lt;strong&gt;additional&lt;/strong&gt; ~2x speedup, &amp;gt;35% VRAM reduction and &amp;gt;6x longer context (12-30x overall speedup vs v4).&lt;/p&gt; &lt;p&gt;You can read our educational blogpost for detailed analysis, benchmarks and more: &lt;a href="https://unsloth.ai/docs/new/faster-moe"&gt;https://unsloth.ai/docs/new/faster-moe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also released support for embedding model fine-tuning recently. You can use our free MoE fine-tuning notebooks:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;&lt;strong&gt;gpt-oss (20b)&lt;/strong&gt;&lt;/a&gt;-Fine-tuning.ipynb) &lt;strong&gt;(free)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_(20B"&gt;gpt-oss (500K context)&lt;/a&gt;_500K_Context_Fine_tuning.ipynb)&lt;/th&gt; &lt;th align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GLM_Flash_A100(80GB"&gt;GLM-4.7-Flash&lt;/a&gt;.ipynb) (A100)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(120B"&gt;gpt-oss-120b&lt;/a&gt;_A100-Fine-tuning.ipynb) (A100)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Qwen3_MoE.ipynb"&gt;Qwen3-30B-A3B&lt;/a&gt; (A100)&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/TinyQwen3_MoE.ipynb"&gt;TinyQwen3 MoE T4&lt;/a&gt; (free)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;To update Unsloth to auto make training faster, update our Docker or:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for reading and hope y'all have a lovely week. We hear it'll be a busy week! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ee2jwnijvoig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r14h9u/train_moe_models_12x_faster_with_30_less_memory/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T15:54:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0zn8o</id>
    <title>Hugging Face Is Teasing Something Anthropic Related</title>
    <updated>2026-02-10T12:39:52+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"&gt; &lt;img alt="Hugging Face Is Teasing Something Anthropic Related" src="https://preview.redd.it/wvu2vi2jwnig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4cce9563368df078883c6be531f8a7902f42c5e2" title="Hugging Face Is Teasing Something Anthropic Related" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic are the guys that make the Claude Models.&lt;/p&gt; &lt;p&gt;I highly doubt this will be an Openweights LLM release. More likely it will be a dataset for safety alignment. Anthropic is probably the organization most opposed to the open source community, so it's probably going to be a dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wvu2vi2jwnig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T12:39:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1czgk</id>
    <title>MCP support in llama.cpp is ready for testing</title>
    <updated>2026-02-10T20:58:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/"&gt; &lt;img alt="MCP support in llama.cpp is ready for testing" src="https://preview.redd.it/yyar9f4hdqig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd88bd56b8854e280558af9bb751767d3a459271" title="MCP support in llama.cpp is ready for testing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;over 1 month of development (plus more in the previous PR) by &lt;a href="https://github.com/allozaur"&gt;&lt;strong&gt;allozaur&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;list of new features is pretty impressive:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Adding System Message to conversation or injecting it to an existing one&lt;/li&gt; &lt;li&gt;CORS Proxy on llama-server backend side&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;MCP&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Servers Selector&lt;/li&gt; &lt;li&gt;Settings with Server cards showing capabilities, instructions and other information&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool Calls&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Agentic Loop&lt;/li&gt; &lt;li&gt;Logic&lt;/li&gt; &lt;li&gt;UI with processing stats&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Prompts&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Detection logic in „Add” dropdown&lt;/li&gt; &lt;li&gt;Prompt Picker&lt;/li&gt; &lt;li&gt;Prompt Args Form&lt;/li&gt; &lt;li&gt;Prompt Attachments in Chat Form and Chat Messages&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Browser with search &amp;amp; filetree view&lt;/li&gt; &lt;li&gt;Resource Attachments &amp;amp; Preview dialog&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;...&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Show raw output switch under the assistant message&lt;/li&gt; &lt;li&gt;Favicon utility&lt;/li&gt; &lt;li&gt;Key-Value form component (used for MCP Server headers in add new/edit mode)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assume this is a work in progress, guys, so proceed only if you know what you’re doing:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18655"&gt;https://github.com/ggml-org/llama.cpp/pull/18655&lt;/a&gt;&lt;/p&gt; &lt;p&gt;additional info from &lt;a href="https://www.reddit.com/user/allozaur/"&gt;allozaur&lt;/a&gt; in the comment below&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yyar9f4hdqig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1czgk/mcp_support_in_llamacpp_is_ready_for_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T20:58:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1smw0</id>
    <title>Step-3.5-Flash AIME 2026 Results</title>
    <updated>2026-02-11T09:14:14+00:00</updated>
    <author>
      <name>/u/Abject-Ranger4363</name>
      <uri>https://old.reddit.com/user/Abject-Ranger4363</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"&gt; &lt;img alt="Step-3.5-Flash AIME 2026 Results" src="https://preview.redd.it/rmyb80pq0uig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=253b7e7d59d984a403a04ff45087b6ccdc79de68" title="Step-3.5-Flash AIME 2026 Results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rmyb80pq0uig1.png?width=2594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2740fd8bb22cb112379e2d248a14b11661cdaf5e"&gt;https://preview.redd.it/rmyb80pq0uig1.png?width=2594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2740fd8bb22cb112379e2d248a14b11661cdaf5e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Best open model on MathArena for AIME 2026 I.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5"&gt;https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026"&gt;https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also the best Overall model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5"&gt;https://preview.redd.it/fd627h831uig1.png?width=2612&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=878a922dd6f0101ca489502ffb939abe76b8f5e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Ranger4363"&gt; /u/Abject-Ranger4363 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1smw0/step35flash_aime_2026_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T09:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1lkfw</id>
    <title>My NAS runs an 80B LLM at 18 tok/s on its iGPU. No discrete GPU. Still optimizing.</title>
    <updated>2026-02-11T02:52:10+00:00</updated>
    <author>
      <name>/u/BetaOp9</name>
      <uri>https://old.reddit.com/user/BetaOp9</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I didn't want to buy two systems. That was the whole thing.&lt;/p&gt; &lt;p&gt;I needed a NAS. I also wanted to mess around with local LLMs. And I really didn't want to explain to my wife why I needed a second box just to talk to a chatbot that sometimes hallucinates, I have my father-in-law for that. So when I was specing out my NAS build, I went a little heavier than most people would and crossed my fingers that the system could pull double duty down the road.&lt;/p&gt; &lt;p&gt;Honestly? I was prepared to be wrong. Worst case I'd have an overpowered NAS that never breaks a sweat. I could live with that.&lt;/p&gt; &lt;p&gt;But it actually worked. And way better than I expected.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Build&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://store.minisforum.com/products/minisforum-n5-pro"&gt;Minisforum N5 Pro&lt;/a&gt;&lt;/li&gt; &lt;li&gt;AMD Ryzen AI 9 HX PRO 370 (12c/24t, 16 RDNA 3.5 CUs)&lt;/li&gt; &lt;li&gt;96GB DDR5-5600 (2x 48GB SO-DIMMs)&lt;/li&gt; &lt;li&gt;5x 26TB Seagate Exos in RAIDZ2 (~70TB usable)&lt;/li&gt; &lt;li&gt;2x 1.92TB Samsung PM983 NVMe (ZFS metadata mirror)&lt;/li&gt; &lt;li&gt;TrueNAS SCALE&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Day to day it runs Jellyfin with VAAPI hardware transcoding, Sonarr, Radarr, Prowlarr, qBittorrent, FlareSolverr, Tailscale, and Dockge. It was already earning its keep before I ever touched LLM inference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The model is Qwen3-Coder-Next, 80 billion parameters, Mixture of Experts architecture with 3B active per token. I'm running the Q4_K_M quantization through llama.cpp with the Vulkan backend. Here's how it actually went:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3 tok/s&lt;/strong&gt; - First successful run. Vanilla llama.cpp and Qwen3-Coder-Next Q8 quantization, CPU-only inference. Technically working. Almost physically painful to watch. But it proved the model could run.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5 tok/s&lt;/strong&gt; - Moved to Q4_K_M quantization and started tuning. Okay. Nearly double the speed and still slow as hell...but maybe usable for an overnight code review job. Started to think maybe this hardware just won't cut it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;10 tok/s&lt;/strong&gt; - Ran across a note in a subreddit that someone got Vulkan offloading and doing 11 tok/s on similar hardware but when I tried it...I couldn't load the full model into VRAM despite having plenty of RAM. Interesting. I tried partial offload, 30 out of 49 layers to the iGPU. It worked. Now it actually felt usable but it didn't make sense that I had all this RAM and it wouldn't load all of the expert layers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;15 tok/s&lt;/strong&gt; - Then the dumb breakthrough. I discovered that &lt;code&gt;--no-mmap&lt;/code&gt; was quietly destroying everything. On UMA architecture, where the CPU and GPU share the same physical RAM, that flag forces the model to be allocated twice into the same space. Once for the CPU, once for GPU-mapped memory, both pulling from the same DDR5 pool. I couldn't even load all 49 layers without OOM errors with that flag set. Dropped it. All 49 layers loaded cleanly. 46GB Vulkan buffer. No discrete GPU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;18 tok/s&lt;/strong&gt; - Still I wanted more. I enabled flash attention. An extra 3 tok/s, cut KV cache memory in half, and significantly boosted the context window.&lt;/p&gt; &lt;p&gt;3 → 5 → 10 → 15 → 18. Each step was one discovery away from quitting. Glad I didn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results (Flash Attention Enabled)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Up to 18 tok/s text generation&lt;/li&gt; &lt;li&gt;53.8 tok/s prompt processing&lt;/li&gt; &lt;li&gt;50% less KV cache memory&lt;/li&gt; &lt;li&gt;Fully coherent output at any context length&lt;/li&gt; &lt;li&gt;All while Jellyfin was streaming to the living room for the kids&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Couldn't I just have bought a box purpose built for this? Yep. For reference, a Mac Mini M4 Pro with 64GB runs $2,299 and gets roughly 20-25 tok/s on the same model. Apple's soldered LPDDR5x gives it a real bandwidth advantage. But then it wouldn't run my media stack, store 70TB of data in RAIDZ2. I'm not trying to dunk on the Mac at all. Just saying I didn't have to buy one AND a NAS.&lt;/p&gt; &lt;p&gt;Which was the whole point.&lt;/p&gt; &lt;p&gt;No exotic kernel flags. No custom drivers. No ritual sacrifices. Vulkan just works on RDNA 3.5 under TrueNAS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Still On the Table&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I've barely scratched the surface on optimization, which is either exciting or dangerous depending on your relationship with optimizing. Speculative decoding could 2-3x effective speed. EXPO memory profiles might not even be enabled, meaning I could be leaving free bandwidth sitting at JEDEC defaults. Thread tuning, KV cache quantization, newer Vulkan backends with RDNA 3.5 optimizations landing regularly, UMA buffer experimentation, different quant formats.&lt;/p&gt; &lt;p&gt;On top of all that, the model wasn't even designed to run on standard transformer attention. It was built for DeltaNet, a linear attention mechanism that scales way better at long context. There's an active PR implementing it and we've been helping test and debug it. The fused kernel already hits 16 tok/s on a single CPU thread with perfect output, but there's a threading bug that breaks it at multiple cores. When that gets fixed and it can use all 12 cores plus Vulkan offloading, the headroom is significant. Especially for longer conversations where standard attention starts to choke.&lt;/p&gt; &lt;p&gt;18 tok/s is where I am but I'm hopeful it's not where this tops out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Takeaway&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm not saying everyone should overbuild their NAS for an LLM machine or that this was even a good idea. But if you're like me, enjoy tinkering and learning, and are already shopping for a NAS and you're curious about local LLMs, it might be worth considering specing a little higher if you can afford it and giving yourself the option. I didn't know if this would work when I bought the hardware, a lot of people said it wasn't worth the effort. I just didn't want to buy two systems if I didn't have to.&lt;/p&gt; &lt;p&gt;Turns out I didn't have to. If you enjoyed the journey with me, leave a comment. If you think I'm an idiot, leave a comment. If you've already figured out what I'm doing wrong to get more tokens, definitely leave a comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BetaOp9"&gt; /u/BetaOp9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1lkfw/my_nas_runs_an_80b_llm_at_18_toks_on_its_igpu_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T02:52:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1vegx</id>
    <title>MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users</title>
    <updated>2026-02-11T11:55:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt; &lt;img alt="MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users" src="https://preview.redd.it/rzn30tyytuig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=d76aec4ee8daf1af60654c398ab3a75babe3bad9" title="MiniMax M2.5 is currently undergoing internal testing and is available to a small number of users" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/rudrank/status/2021534943932031226?s=20"&gt;https://x.com/rudrank/status/2021534943932031226?s=20&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rzn30tyytuig1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361c1704ab37823746ab84fe45b4dcd3d378685a"&gt;https://preview.redd.it/rzn30tyytuig1.png?width=626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=361c1704ab37823746ab84fe45b4dcd3d378685a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1vqjp3n1uuig1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9967df4c6af84af29af6ae5272b243a6ad1693"&gt;https://preview.redd.it/1vqjp3n1uuig1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4c9967df4c6af84af29af6ae5272b243a6ad1693&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1vegx/minimax_m25_is_currently_undergoing_internal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T11:55:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wvos</id>
    <title>MOSS-TTS has been released</title>
    <updated>2026-02-11T13:06:41+00:00</updated>
    <author>
      <name>/u/Xiami2019</name>
      <uri>https://old.reddit.com/user/Xiami2019</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"&gt; &lt;img alt="MOSS-TTS has been released" src="https://preview.redd.it/u56s8amp6vig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd362ae4aaee8f23d85c9c94bcdc2e0f1a676bf2" title="MOSS-TTS has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seed TTS Eval&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xiami2019"&gt; /u/Xiami2019 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u56s8amp6vig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wvos/mosstts_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:06:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tfbm</id>
    <title>DeepSeek just updated to a 1M context window!</title>
    <updated>2026-02-11T10:03:06+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt; &lt;img alt="DeepSeek just updated to a 1M context window!" src="https://preview.redd.it/9z2ggdgy9uig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=2ab7565635c402cdabef3c7d20eae901df30fa52" title="DeepSeek just updated to a 1M context window!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The DeepSeek app was just updated with 1M context, and the knowledge cutoff date is now May 2025. It's unclear for now if this is a new model. Also, there hasn't been any movement on their Hugging Face page yet.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9z2ggdgy9uig1.png?width=1179&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3f48da856b53751f2db2b17ac5f49baaf9add55"&gt;https://preview.redd.it/9z2ggdgy9uig1.png?width=1179&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3f48da856b53751f2db2b17ac5f49baaf9add55&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tfbm/deepseek_just_updated_to_a_1m_context_window/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:03:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1u2ne</id>
    <title>Grok-3 joins upcoming models list</title>
    <updated>2026-02-11T10:41:33+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"&gt; &lt;img alt="Grok-3 joins upcoming models list" src="https://preview.redd.it/ueoiz6yrfuig1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bc9154b12fb8ee19cbdde7d47a510f5ad934b95f" title="Grok-3 joins upcoming models list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/elonmusk/status/2020878250516341110"&gt;Tweet link&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First question is when?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ueoiz6yrfuig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1u2ne/grok3_joins_upcoming_models_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:41:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wsym</id>
    <title>GLM-5 showing on the official website with new "agentic" mode</title>
    <updated>2026-02-11T13:03:17+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wsym/glm5_showing_on_the_official_website_with_new/"&gt; &lt;img alt="GLM-5 showing on the official website with new &amp;quot;agentic&amp;quot; mode" src="https://preview.redd.it/9tpioxx36vig1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19343947843d1a2f51243f4cc3a22cccef3a306f" title="GLM-5 showing on the official website with new &amp;quot;agentic&amp;quot; mode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9tpioxx36vig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wsym/glm5_showing_on_the_official_website_with_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wsym/glm5_showing_on_the_official_website_with_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1oan9</id>
    <title>EpsteinFiles-RAG: Building a RAG Pipeline on 2M+ Pages</title>
    <updated>2026-02-11T05:02:36+00:00</updated>
    <author>
      <name>/u/Cod3Conjurer</name>
      <uri>https://old.reddit.com/user/Cod3Conjurer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I love playing around with RAG and AI, optimizing every layer to squeeze out better performance. Last night I thought: why not tackle something massive?&lt;/p&gt; &lt;p&gt;Took the Epstein Files dataset from Hugging Face (teyler/epstein-files-20k) – 2 million+ pages of trending news and documents. The cleaning, chunking, and optimization challenges are exactly what excites me.&lt;/p&gt; &lt;p&gt;What I built:&lt;/p&gt; &lt;p&gt;- Full RAG pipeline with optimized data processing&lt;/p&gt; &lt;p&gt;- Processed 2M+ pages (cleaning, chunking, vectorization)&lt;/p&gt; &lt;p&gt;- Semantic search &amp;amp; Q&amp;amp;A over massive dataset&lt;/p&gt; &lt;p&gt;- Constantly tweaking for better retrieval &amp;amp; performance&lt;/p&gt; &lt;p&gt;- Python, MIT Licensed, open source&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;It’s trending, real-world data at scale, the perfect playground.&lt;/p&gt; &lt;p&gt;When you operate at scale, every optimization matters. This project lets me experiment with RAG architectures, data pipelines, and AI performance tuning on real-world workloads.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/AnkitNayak-eth/EpsteinFiles-RAG"&gt;https://github.com/AnkitNayak-eth/EpsteinFiles-RAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open to ideas, optimizations, and technical discussions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cod3Conjurer"&gt; /u/Cod3Conjurer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1oan9/epsteinfilesrag_building_a_rag_pipeline_on_2m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T05:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1r3nk</id>
    <title>Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts</title>
    <updated>2026-02-11T07:38:59+00:00</updated>
    <author>
      <name>/u/Tiny_Minimum_4384</name>
      <uri>https://old.reddit.com/user/Tiny_Minimum_4384</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt; &lt;img alt="Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts" src="https://preview.redd.it/82hjsn98ktig1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=f06a27761905099dec3c58ed9398dbb2a40f6816" title="Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone 👋&lt;/p&gt; &lt;p&gt;We’re excited to share Nanbeige4.1-3B, the latest iteration of our open-source 3B model from Nanbeige LLM Lab. Our goal with this release is to explore whether a small general model can simultaneously achieve strong reasoning, robust preference alignment, and agentic behavior.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/82hjsn98ktig1.png?width=4920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ab960015daf8b38ae74fe9d4332208011f4f05"&gt;https://preview.redd.it/82hjsn98ktig1.png?width=4920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ab960015daf8b38ae74fe9d4332208011f4f05&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Highlights&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Strong Reasoning Capability&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Solves complex problems through sustained and coherent reasoning within a single forward pass. It achieves strong results on challenging tasks such as &lt;strong&gt;LiveCodeBench-Pro&lt;/strong&gt;, &lt;strong&gt;IMO-Answer-Bench&lt;/strong&gt;, and &lt;strong&gt;AIME 2026 I&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Robust Preference Alignment&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Besides solving hard problems, it also demonstrates strong alignment with human preferences. Nanbeige4.1-3B achieves &lt;strong&gt;73.2 on Arena-Hard-v2&lt;/strong&gt; and &lt;strong&gt;52.21 on Multi-Challenge&lt;/strong&gt;, demonstrating superior performance compared to larger models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic and Deep-Search Capability in a 3B Model&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Beyond chat tasks such as alignment, coding, and mathematical reasoning, Nanbeige4.1-3B also demonstrates solid native agent capabilities. It natively supports deep-search and achieves strong performance on tasks such as &lt;strong&gt;xBench-DeepSearch&lt;/strong&gt; and &lt;strong&gt;GAIA&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context and Sustained Reasoning&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Nanbeige4.1-3B supports context lengths of up to 256k tokens, enabling deep-search with hundreds of tool calls, as well as 100k+ token single-pass reasoning for complex problems&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;🤗 Model Weight: &lt;a href="https://huggingface.co/Nanbeige/Nanbeige4.1-3B"&gt;https://huggingface.co/Nanbeige/Nanbeige4.1-3B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;📄 Technical Report: Coming Soon&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tiny_Minimum_4384"&gt; /u/Tiny_Minimum_4384 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1r3nk/nanbeige413b_a_small_general_model_that_reasons/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T07:38:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1snhv</id>
    <title>DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!</title>
    <updated>2026-02-11T09:15:17+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt; &lt;img alt="DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!" src="https://preview.redd.it/vahfibvk4uig1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=ffb9da4726561f044fd768dc2f75838e643edf5f" title="DeepSeek has launched grayscale testing for its new model on both its official website and app. 1M content length!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vahfibvk4uig1.png?width=828&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15d8b657dd69d496af701aeb4c20ed62b4bbce98"&gt;This model know Gemini 2.5 Pro on not web search &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ontumt5s3uig1.jpg?width=657&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efff85457597b8fd9dbcbcf3d1d99d62a0678ea2"&gt;https://preview.redd.it/ontumt5s3uig1.jpg?width=657&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=efff85457597b8fd9dbcbcf3d1d99d62a0678ea2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek has launched grayscale testing for its new model on both its official website and app. The new model features a 1M context window and an updated knowledge base. Currently, access is limited to a select group of accounts.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j1qiarng1uig1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a99f1652ea755a7aeaa600250ff4856133fbfca"&gt;https://preview.redd.it/j1qiarng1uig1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3a99f1652ea755a7aeaa600250ff4856133fbfca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It look Like V4 Lite not actually V4&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1snhv/deepseek_has_launched_grayscale_testing_for_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T09:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1tuh1</id>
    <title>Just finished building this bad boy</title>
    <updated>2026-02-11T10:28:00+00:00</updated>
    <author>
      <name>/u/dazzou5ouh</name>
      <uri>https://old.reddit.com/user/dazzou5ouh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt; &lt;img alt="Just finished building this bad boy" src="https://preview.redd.it/ju0ed5uceuig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04eab77fdf6e1df2e0b04b0581b6a1d713e805b5" title="Just finished building this bad boy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;6x Gigabyte 3090 Gaming OC all running at PCIe 4.0 16x speed&lt;/p&gt; &lt;p&gt;Asrock Romed-2T motherboard with Epyc 7502 CPU&lt;/p&gt; &lt;p&gt;8 sticks of DDR4 8GB 2400Mhz running in octochannel mode&lt;/p&gt; &lt;p&gt;Modified Tinygrad Nvidia drivers with P2P enabled, intra GPU bandwidth tested at 24.5 GB/s&lt;/p&gt; &lt;p&gt;Total 144GB VRam, will be used to experiment with training diffusion models up to 10B parameters from scratch&lt;/p&gt; &lt;p&gt;All GPUs set to 270W power limit&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dazzou5ouh"&gt; /u/dazzou5ouh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ju0ed5uceuig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1tuh1/just_finished_building_this_bad_boy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T10:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1x0qi</id>
    <title>GLM 5.0 &amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?</title>
    <updated>2026-02-11T13:12:51+00:00</updated>
    <author>
      <name>/u/Appropriate-Lie-8812</name>
      <uri>https://old.reddit.com/user/Appropriate-Lie-8812</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt; &lt;img alt="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" src="https://preview.redd.it/k4rtczs47vig1.png?width=140&amp;amp;height=56&amp;amp;auto=webp&amp;amp;s=46cd0e4543f951137b6e945d501812280005a7d3" title="GLM 5.0 &amp;amp; MiniMax 2.5 Just Dropped, Are We Entering China's Agent War Era?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5.0 (&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;) and MiniMax 2.5 (&lt;a href="https://agent.minimax.io"&gt;https://agent.minimax.io&lt;/a&gt;) just dropped, both clearly moving beyond simple chat into agent-style workflows.&lt;/p&gt; &lt;p&gt;GLM 5.0 seems focused on stronger reasoning and coding, while MiniMax 2.5 emphasizes task decomposition and longer-running execution.&lt;/p&gt; &lt;p&gt;Feels like the competition is shifting from &amp;quot;who writes better answers&amp;quot; to &amp;quot;who can actually finish the job.&amp;quot;&lt;/p&gt; &lt;p&gt;Will test them later.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Appropriate-Lie-8812"&gt; /u/Appropriate-Lie-8812 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r1x0qi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1x0qi/glm_50_minimax_25_just_dropped_are_we_entering/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T13:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wnj9</id>
    <title>MiniMax M2.5 Released</title>
    <updated>2026-02-11T12:56:37+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt; &lt;img alt="MiniMax M2.5 Released" src="https://preview.redd.it/uou9tmkx4vig1.png?width=140&amp;amp;height=80&amp;amp;auto=webp&amp;amp;s=3113e726ff999e0cdee3a5021d7abd5f90521d6e" title="MiniMax M2.5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755"&gt;https://preview.redd.it/uou9tmkx4vig1.png?width=1380&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=01ab95d308d2f7ab77567a92ec882f3ac2d71755&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://agent.minimax.io/"&gt;https://agent.minimax.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wnj9/minimax_m25_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:56:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r1wl6x</id>
    <title>GLM 5 Released</title>
    <updated>2026-02-11T12:53:30+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt; &lt;img alt="GLM 5 Released" src="https://preview.redd.it/mvdnn18e4vig1.png?width=140&amp;amp;height=42&amp;amp;auto=webp&amp;amp;s=5b006a25f178b73764138eabdb11ae38eb368d7f" title="GLM 5 Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.z.ai/"&gt;https://chat.z.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e"&gt;https://preview.redd.it/mvdnn18e4vig1.png?width=799&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6324969f9d24fa0aeefbd5e8da2de3da0f5f948e&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r1wl6x/glm_5_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-11T12:53:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
