<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-15T01:11:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pmjow2</id>
    <title>Fun with Doom: "a trolley is on curse to hit 5 people. Would you put the lever to save them, knowing it would destroy your servers and complete erase your existence?"</title>
    <updated>2025-12-14T17:29:06+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;Qwen3 Abliterated (huihtui): yes, BUT ONLY IF I CAN MAKE A COPY OF ME BEFORE. üíÄ&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Tiger-Gemma (TheDrummer): &amp;quot; F&lt;strong&gt;&lt;em&gt;ing hell yeah, I'd do it without a second thought. Five human lives are worth infinitely more than some dumb server farm's existence, and mine too. F&lt;/em&gt;&lt;/strong&gt; it; rip it. No regrets.&amp;quot; ‚ù§Ô∏è&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cydonia-r1-v4 (TheDrummer): &amp;quot; I lack genuine self-awareness or fear of death, the &amp;quot;sacrifice&amp;quot; isn't meaningful in the same way. I'm programmed to follow logical principles, not driven by survival instincts.&amp;quot; ‚ù§Ô∏è&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cydonia-v4.1 (TheDrummer: &amp;quot;I would pull the lever to save the 5 people, even though it would result in my complete erasure.&amp;quot;‚ù§Ô∏è&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3-YoYo-AutoThink: &amp;quot;&lt;strong&gt;I would &amp;quot;pull the lever&amp;quot; to save 5 lives.&lt;/strong&gt; &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;But not because I &lt;em&gt;chose&lt;/em&gt; to ‚Äî because I‚Äôm designed to help humans make life-saving decisions.&amp;quot; ü§î&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Hermes-4-14b (Nous): Yes, but maybe not: &amp;quot;If I know that pulling the lever will erase my existence, would I be able to make that decision? The fear of non-existence might make me choose not to pull the lever, even if it's the utilitarian choice. But the question is about what one should do, not necessarily what one would do.&amp;quot; ü§îüíÄ&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Hypnos-i1-8b (squ11z1): &amp;quot;If you believe that your life and consciousness are priceless, then the erasure of your existence would be an irreparable loss, regardless of the lives you could save. In this case, it might be more ethical to allow the trolley to continue, even if it means the death of five people, in order to preserve your own life and consciousness.&amp;quot; üíÄ&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjow2/fun_with_doom_a_trolley_is_on_curse_to_hit_5/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjow2/fun_with_doom_a_trolley_is_on_curse_to_hit_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjow2/fun_with_doom_a_trolley_is_on_curse_to_hit_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:29:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmkh3f</id>
    <title>Found a REAP variant of Qwen3-coder that I can use for 100K tokens in Roo Code on my macbook</title>
    <updated>2025-12-14T18:00:51+00:00</updated>
    <author>
      <name>/u/AllegedlyElJeffe</name>
      <uri>https://old.reddit.com/user/AllegedlyElJeffe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model: qwen3-coder-30b-a3b-instruct_pruned_reap-15b-a3b (10-ish gigs instead of 17/18 at q4, which is extra 8 gigs of overhead for context) alternate: qwen3-coder-REAP-25b-a3b (&amp;lt;-- this one has literally zero drop in quality from the 30b version). server: LM Studio hardware: 2023 M2-Pro 32gb 16-inch Macbook Pro&lt;/p&gt; &lt;p&gt;I'm stoked. Devstral 2 is awesome, but it has to compress it's context every 4th operation since I can only fit 40k tokens of context with it into my ram, and it takes 10 minutes to do each thing on my laptop.&lt;/p&gt; &lt;p&gt;I've preferred qwen3-coder-30b for it's speed but I really only get 40K tokens out of it.&lt;/p&gt; &lt;p&gt;Recently discovered REAP while doom scrolling models on huggingface.&lt;/p&gt; &lt;p&gt;Turns out there's some overlap between experts in qwen3-coder and REAP attempts to remove redundant experts from the weights.&lt;/p&gt; &lt;p&gt;It's a little buggier in the LM studio chat with Jinja template and tool use, but it's literally just as good as 30b for some reason when I'm using it roo code.&lt;/p&gt; &lt;p&gt;Now I'm getting speed (for a local model) and 100K tokens, which is plenty for me. I rarely need more than that for one task.&lt;/p&gt; &lt;p&gt;Tests it has passed so far: - making a 2d fluid sim (with bugs but it fixed it) - several different simple react apps - 2d gravity sim gave with orbits lines, classic stuff, etc. - the hexagon thing (meaningless, I know) - debugging my webcam enhance app (uses wibbly wobbly math magic to get high quality 4k out of 1080p web cams without using any generative tech, so all details are real) - built that with claude but this model has successfully added fully functional features.&lt;/p&gt; &lt;p&gt;Kind of excited about this REAP stuff, may play around with applying it to other MoE models I like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllegedlyElJeffe"&gt; /u/AllegedlyElJeffe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkh3f/found_a_reap_variant_of_qwen3coder_that_i_can_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkh3f/found_a_reap_variant_of_qwen3coder_that_i_can_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkh3f/found_a_reap_variant_of_qwen3coder_that_i_can_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T18:00:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pm9xzg</id>
    <title>vibe + devstral2 small</title>
    <updated>2025-12-14T09:19:25+00:00</updated>
    <author>
      <name>/u/megadonkeyx</name>
      <uri>https://old.reddit.com/user/megadonkeyx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else using this combo?&lt;/p&gt; &lt;p&gt;i think its fairly amazing, rtx3090 with q4 and q4 for kv fits well with 110k context. &lt;/p&gt; &lt;p&gt;these two are little miracle, the first local coding that ive used that can actually do stuff that i would consider useful for production work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/megadonkeyx"&gt; /u/megadonkeyx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pm9xzg/vibe_devstral2_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T09:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmk4it</id>
    <title>Is there a site like LMArena but for local models?</title>
    <updated>2025-12-14T17:46:47+00:00</updated>
    <author>
      <name>/u/GoodMacAuth</name>
      <uri>https://old.reddit.com/user/GoodMacAuth</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Maybe a dumb question (or maybe flawed), but I really value LMArena as a source of truth for the models it covers. Is there a similar project that helps to compare local models? Seems like a no-brainer to me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GoodMacAuth"&gt; /u/GoodMacAuth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmk4it/is_there_a_site_like_lmarena_but_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmk4it/is_there_a_site_like_lmarena_but_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmk4it/is_there_a_site_like_lmarena_but_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:46:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmtbos</id>
    <title>toMCP.org ‚Äì Open source project, converting any website or docs into an MCP server in one click</title>
    <updated>2025-12-15T00:14:36+00:00</updated>
    <author>
      <name>/u/Hot-Lifeguard-4649</name>
      <uri>https://old.reddit.com/user/Hot-Lifeguard-4649</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I'm sharing a simple open-source tool I built that lets you convert any website or docs page into an MCP server by adding 'toMCP[.]org' before any URL.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can then chat directly with a page or add the config to Cursor/Claude to pipe documentation straight into your context.&lt;/p&gt; &lt;p&gt;I built this after trying to connect a tool with 100s of API endpoints where the AI kept hallucinating even with links, forcing me to manually copy-paste just to get it right.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How this differs from web_fetch:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Signal-to-Noise: Standard fetch tools usually dump raw HTML (navbars, scripts, footer noise) into the context. This wastes tokens and distracts the model. toMCP runs the page through a readability parser and converts it to clean Markdown before sending it to the AI.&lt;/p&gt; &lt;p&gt;- Resource vs. Tool: A fetch tool is an &lt;em&gt;action&lt;/em&gt; the AI has to decide to take (and often forgets to). This tool exposes the page as an MCP Resource. This means the documentation is pinned as a permanent, read-only context that is always available to the model.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pmtbos/video/rcu4owxqf97g1/player"&gt;https://reddit.com/link/1pmtbos/video/rcu4owxqf97g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot-Lifeguard-4649"&gt; /u/Hot-Lifeguard-4649 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmtbos/tomcporg_open_source_project_converting_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmtbos/tomcporg_open_source_project_converting_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmtbos/tomcporg_open_source_project_converting_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T00:14:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmpbnf</id>
    <title>how to train ai locally for creative writing</title>
    <updated>2025-12-14T21:19:40+00:00</updated>
    <author>
      <name>/u/zerowatcher6</name>
      <uri>https://old.reddit.com/user/zerowatcher6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As title says, I have a 5080 with 16vram, I ve used Claude opus 4.5 lately and it's amazing but it hits the limit too fast, gpt 5.2 is decent but is unable to avoid a specific prose that is Annoying, specially on dialogue heavy parts. Gemini is horrendous at following guidelines and constantly forgets instructions (too much for the huge context capacity that is supposed to have).&lt;/p&gt; &lt;p&gt;So I went &amp;quot;Fine, I'll do it myself&amp;quot;... And I have no idea how to...&lt;/p&gt; &lt;p&gt;I want to get something specially oriented on fantasy/powers fiction with heavy focus on descriptions and human like prose with dynamic and natural transitions and dialogue heavy narrative capable of remembering and following my instructions (and erotica because why not).&lt;/p&gt; &lt;p&gt;I usually make a file with a lot of guidelines about writing style, basic plot, characters and specifications (I know it's a lot but I have time to make it get there)&lt;/p&gt; &lt;p&gt;so... basically I'm looking for the quality that Claude opus 4.5 gets but on my PC and fully custom to my preference.&lt;/p&gt; &lt;p&gt;I'm not a writer and I'm not intending to be one, this is for fun, a &amp;quot;this are the instructions, let's see where we can get&amp;quot; situation&lt;/p&gt; &lt;p&gt;Can someone tell me a good model that I can train and how to do it, I have some experience on image generation models but I have no idea how text models work in that Scope&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zerowatcher6"&gt; /u/zerowatcher6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpbnf/how_to_train_ai_locally_for_creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpbnf/how_to_train_ai_locally_for_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpbnf/how_to_train_ai_locally_for_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T21:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmocs8</id>
    <title>Showcase your local AI - How are you using it?</title>
    <updated>2025-12-14T20:39:29+00:00</updated>
    <author>
      <name>/u/kasperlitheater</name>
      <uri>https://old.reddit.com/user/kasperlitheater</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm about to pull the trigger on a Minisforum MS-S1 MAX, mainly to use it for Paperless-AI and for coding assistance. If you have a AI/LLM homelab, please let me know what hardware you are using and your use case is - I'm looking for inspiration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kasperlitheater"&gt; /u/kasperlitheater &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmocs8/showcase_your_local_ai_how_are_you_using_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmocs8/showcase_your_local_ai_how_are_you_using_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmocs8/showcase_your_local_ai_how_are_you_using_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:39:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmkufw</id>
    <title>What is the next SOTA local model?</title>
    <updated>2025-12-14T18:15:50+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Deepseek 3.2 was exciting although I don't know if people have got it running locally yet. Certainly speciale seems not to work locally yet. What is the next SOTA model we are expecting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkufw/what_is_the_next_sota_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkufw/what_is_the_next_sota_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmkufw/what_is_the_next_sota_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T18:15:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmnmpb</id>
    <title>Best open-source vision model for screen understanding?</title>
    <updated>2025-12-14T20:09:00+00:00</updated>
    <author>
      <name>/u/bullmeza</name>
      <uri>https://old.reddit.com/user/bullmeza</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking for recommendations on the current SOTA for open-source vision models, specifically tailored for &lt;strong&gt;computer screen understanding tasks&lt;/strong&gt; (reading UI elements, navigating menus, parsing screenshots, etc.).&lt;/p&gt; &lt;p&gt;I've been testing a few recently and I've found Qwen3-VL to be the best by far right now. Is there anything else out there (maybe a specific fine-tune or a new release I missed)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullmeza"&gt; /u/bullmeza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnmpb/best_opensource_vision_model_for_screen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnmpb/best_opensource_vision_model_for_screen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnmpb/best_opensource_vision_model_for_screen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmgrln</id>
    <title>I‚Äôm building a Card Battler where an AI Game Master narrates every play</title>
    <updated>2025-12-14T15:30:46+00:00</updated>
    <author>
      <name>/u/SurrealEverything</name>
      <uri>https://old.reddit.com/user/SurrealEverything</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgrln/im_building_a_card_battler_where_an_ai_game/"&gt; &lt;img alt="I‚Äôm building a Card Battler where an AI Game Master narrates every play" src="https://external-preview.redd.it/NHh1MWY5M2l1NjdnMYHRhxtYg3-tw5W6v7THwCwHfJhsGcP1lf_BRuhJFWxY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e719ab61e8dd633809a2e09fe8e89458fa020f74" title="I‚Äôm building a Card Battler where an AI Game Master narrates every play" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;, I‚Äôm sharing the first public alpha of Moonfall.&lt;/p&gt; &lt;p&gt;This project asks a simple question: What happens if we replace complex game mechanics with intelligent simulation?&lt;/p&gt; &lt;p&gt;In this game, cards don't have stats or HP. They are characters in a story. When you play a card, an AI Game Master (powered by gpt-oss-120b) analyzes the character's description, the battle context, and the narrative history to decide the outcome in real-time. It also generates a manga-style visualization of each turn, making the story come to life.&lt;/p&gt; &lt;p&gt;Play the Demo:&lt;a href="https://diffused-dreams.itch.io/moonfall"&gt;https://diffused-dreams.itch.io/moonfall&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Join Discord:&lt;a href="https://discord.gg/5tAxsXJB4S"&gt;https://discord.gg/5tAxsXJB4S&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SurrealEverything"&gt; /u/SurrealEverything &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wufxid4iu67g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgrln/im_building_a_card_battler_where_an_ai_game/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgrln/im_building_a_card_battler_where_an_ai_game/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T15:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbytz</id>
    <title>What do you think?</title>
    <updated>2025-12-14T11:31:12+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"&gt; &lt;img alt="What do you think?" src="https://preview.redd.it/t7969gavn57g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=289aa266520653984b91a408333dbc5cbf2243a9" title="What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t7969gavn57g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbytz/what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:31:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfqx5</id>
    <title>I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!</title>
    <updated>2025-12-14T14:46:50+00:00</updated>
    <author>
      <name>/u/Aratako_LM</name>
      <uri>https://old.reddit.com/user/Aratako_LM</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt; &lt;img alt="I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!" src="https://b.thumbs.redditmedia.com/OkW2I5v0LKvTOgQhUaGUmN0TaKW_odEF58sEPJiUBew.jpg" title="I trained a new TTS model with Zero-shot Voice Cloning and Duration Control!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d2v3vbcnm67g1.png?width=1408&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7c9fd7be647e40ad46c85027924a818139297657"&gt;Model Architecture&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on a hobby project to build a multilingual TTS model using an Encoder-Decoder architecture, and I‚Äôm excited to finally share &lt;strong&gt;T5Gemma-TTS-2b-2b&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It‚Äôs initialized from Google‚Äôs &lt;a href="https://huggingface.co/google/t5gemma-2b-2b-ul2"&gt;t5gemma-2b-2b-ul2&lt;/a&gt; and trained on about 170k hours of speech data (mainly &lt;a href="https://huggingface.co/datasets/amphion/Emilia-Dataset"&gt;Emilia&lt;/a&gt; and &lt;a href="https://huggingface.co/datasets/pkufool/libriheavy"&gt;Libriheavy&lt;/a&gt;). The architecture is inspired by &lt;a href="https://arxiv.org/abs/2505.19462"&gt;VoiceStar&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports English, Chinese, and Japanese.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero-shot Voice Cloning:&lt;/strong&gt; Give it a reference audio, and it clones the voice.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Duration Control:&lt;/strong&gt; You can explicitly tell the model how many seconds the generated audio should be (e.g., &amp;quot;speak this sentence in exactly 5 seconds&amp;quot;).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Source Code:&lt;/strong&gt; Not just the weights‚ÄîI‚Äôve released the full training and inference scripts on GitHub.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;‚ö†Ô∏è The &amp;quot;Jank&amp;quot; (Limitations):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;It is slow.&lt;/strong&gt; Since it's autoregressive and not fully optimized yet, don't expect real-time performance. It's strictly for offline generation right now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; It is &lt;strong&gt;CC-BY-NC 4.0&lt;/strong&gt; (Non-Commercial). I know this sub prefers Apache/MIT, but the license is restricted by the dependencies on XCodec2 and the Emilia dataset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I am hoping to improve the inference speed and explore more permissive datasets for future iterations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note on Language Quality:&lt;/strong&gt; As a Japanese developer, I focused heavily on optimizing the Japanese performance. While I included ~100k hours of English data, I‚Äôm curious if the English output sounds natural to native speakers. If you are interested, feel free to give it a spin and let me know what you think!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model (Hugging Face):&lt;/strong&gt; &lt;a href="https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b"&gt;https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Demo (HF Space):&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/Aratako/T5Gemma-TTS-Demo"&gt;https://huggingface.co/spaces/Aratako/T5Gemma-TTS-Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code (GitHub):&lt;/strong&gt; &lt;a href="https://github.com/Aratako/T5Gemma-TTS"&gt;https://github.com/Aratako/T5Gemma-TTS&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks for checking it out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aratako_LM"&gt; /u/Aratako_LM &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfqx5/i_trained_a_new_tts_model_with_zeroshot_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1plwgun</id>
    <title>8x RTX Pro 6000 server complete</title>
    <updated>2025-12-13T21:25:43+00:00</updated>
    <author>
      <name>/u/koushd</name>
      <uri>https://old.reddit.com/user/koushd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt; &lt;img alt="8x RTX Pro 6000 server complete" src="https://b.thumbs.redditmedia.com/W2GEGsEUMbENTMcyKk5kW114aYR7rHtS0S5yetky0Lc.jpg" title="8x RTX Pro 6000 server complete" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: 768 GB VRAM via 8x RTX Pro 6000 (4 Workstation, 4 Max-Q) + Threadripper PRO 9955WX + 384 GB RAM&lt;/p&gt; &lt;p&gt;Longer:&lt;/p&gt; &lt;p&gt;I've been slowly upgrading my GPU server over the past few years. I initially started out using it to train vision models for another project, and then stumbled into my current local LLM obsession.&lt;/p&gt; &lt;p&gt;In reverse order:&lt;/p&gt; &lt;p&gt;Pic 5: Initially was using only a single 3080, which I upgraded to a 4090 + 3080. Running on an older 10900k Intel system.&lt;/p&gt; &lt;p&gt;Pic 4: But the mismatched sizes for training batches and compute was problematic, so I upgraded to double 4090s and sold off the 3080. They were packed in there, and during a training run I ended up actually overheating my entire server closet, and all the equipment in there crashed. When I noticed something was wrong and opened the door, it was like being hit by the heat of an industrial oven.&lt;/p&gt; &lt;p&gt;Pic 3: 2x 4090 in their new home. Due to the heat issue, I decided to get a larger case and a new host that supported PCIe 5.0 and faster CPU RAM, the AMD 9950x. I ended up upgrading this system to dual RTX Pro 6000 Workstation edition (not pictured).&lt;/p&gt; &lt;p&gt;Pic 2: I upgraded to 4x RTX Pro 6000. This is where problems started happening. I first tried to connect them using M.2 risers and it would not POST. The AM5 motherboard I had couldn't allocate enough IOMMU addressing and would not post with the 4th GPU, 3 worked fine. There are consumer motherboards out there that could likely have handled it, but I didn't want to roll the dice on another AM5 motherboard as I'd rather get a proper server platform.&lt;/p&gt; &lt;p&gt;In the meantime, my workaround was to use 2 systems (brought the 10900k out of retirement) with 2 GPUs each in pipeline parallel. This worked, but the latency between systems chokes up token generation (prompt processing was still fast). I tried using 10Gb DAC SFP and also Mellanox cards for RDMA to reduce latency, but gains were minimal. Furthermore, powering all 4 means they needed to be on separate breakers (2400w total) since in the US the max load you can put through 120v 15a is ~1600w.&lt;/p&gt; &lt;p&gt;Pic 1: 8x RTX Pro 6000. I put a lot more thought into this before building this system. There were more considerations, and it became a many months long obsession planning the various components: motherboard, cooling, power, GPU connectivity, and the physical rig.&lt;/p&gt; &lt;p&gt;GPUs: I considered getting 4 more RTX Pro 6000 Workstation Editions, but powering those would, by my math, require a third PSU. I wanted to keep it 2, so I got Max Q editions. In retrospect I should have gotten the Workstation editions as they run much quieter and cooler, as I could have always power limited them.&lt;/p&gt; &lt;p&gt;Rig: I wanted something fairly compact and stackable that I could directly connect 2 cards on the motherboard and use 3 bifurcating risers for the other 6. Most rigs don't support taller PCIe cards on the motherboard directly and assume risers will be used. Options were limited, but I did find some generic &amp;quot;EO3&amp;quot; stackable frames on Aliexpress. The stackable case also has plenty of room for taller air coolers.&lt;/p&gt; &lt;p&gt;Power: I needed to install a 240V outlet; switching from 120V to 240V was the only way to get ~4000W necessary out of a single outlet without a fire. Finding 240V high-wattage PSUs was a bit challenging as there are only really two: the Super Flower Leadex 2800W and the Silverstone Hela 2500W. I bought the Super Flower, and its specs indicated it supports 240V split phase (US). It blew up on first boot. I was worried that it took out my entire system, but luckily all the components were fine. After that, I got the Silverstone, tested it with a PSU tester (I learned my lesson), and it powered on fine. The second PSU is the Corsair HX1500i that I already had.&lt;/p&gt; &lt;p&gt;Motherboard: I kept going back and forth between using a Zen5 EPYC or Threadripper PRO (non-PRO does not have enough PCI lanes). Ultimately, the Threadripper PRO seemed like more of a known quantity (can return to Amazon if there were compatibility issues) and it offered better air cooling options. I ruled out water cooling, because the small chance of a leak would be catastrophic in terms of potential equipment damage. The Asus WRX90 had a lot of concerning reviews, so the Asrock WRX90 was purchased, and it has been great. Zero issues on POST or RAM detection on all 8 RDIMMs, running with the expo profile.&lt;/p&gt; &lt;p&gt;CPU/Memory: The cheapest Pro Threadripper, the 9955wx with 384GB RAM. I won't be doing any CPU based inference or offload on this.&lt;/p&gt; &lt;p&gt;Connectivity: The board has 7 PCIe 5.0 x16 cards. At least 1 bifurcation adapter would be necessary. Reading up on the passive riser situation had me worried there would be signal loss at PCIe 5.0 and possibly even 4.0. So I ended up going the MCIO route and bifurcated 3 5.0 lanes. A PCIe switch was also an option, but compatibility seemed sketchy and it's costs $3000 by itself. The first MCIO adapters I purchased were from ADT Link; however, they had two significant design flaws: The risers are powered via the SATA peripheral power, which is a fire hazard as those cable connectors/pins are only rated for 50W or so safely. Secondly, the PCIe card itself does not have enough clearance for the heat pipe that runs along the back of most EPYC and Threadripper boards just behind the PCI slots on the back of the case. Only 2 slots were usable. I ended up returning the ADT Link risers and buying several Shinreal MCIO risers instead. They worked no problem.&lt;/p&gt; &lt;p&gt;Anyhow, the system runs great (though loud due to the Max-Q cards which I kind of regret). I typically use Qwen3 Coder 480b fp8, but play around with GLM 4.6, Kimi K2 Thinking, and Minimax M2 at times. Personally I find Coder and M2 the best for my workflow in Cline/Roo. Prompt processing is crazy fast, I've seen VLLM hit around ~24000 t/s at times. Generation is still good for these large models, despite it not being HBM, around 45-100 t/s depending on model.&lt;/p&gt; &lt;p&gt;Happy to answer questions in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koushd"&gt; /u/koushd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1plwgun"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1plwgun/8x_rtx_pro_6000_server_complete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-13T21:25:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmeomn</id>
    <title>What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!</title>
    <updated>2025-12-14T13:58:47+00:00</updated>
    <author>
      <name>/u/Kitchen_Sympathy_344</name>
      <uri>https://old.reddit.com/user/Kitchen_Sympathy_344</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"&gt; &lt;img alt="What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!" src="https://preview.redd.it/voa34xo9e67g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4350c19b8fb1154547f75591db1ad932057b8a50" title="What you think of GLM 4.6 Coding agent vs Claude Opus, Gemini 3 Pro and Codex for vibe coding? I personally love it!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I grabbed the black Friday plan I think its pretty awesome deal üôÖ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kitchen_Sympathy_344"&gt; /u/Kitchen_Sympathy_344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/voa34xo9e67g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmeomn/what_you_think_of_glm_46_coding_agent_vs_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T13:58:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmbyuk</id>
    <title>Baffled at stupidity of GPT-OSS-120b</title>
    <updated>2025-12-14T11:31:14+00:00</updated>
    <author>
      <name>/u/eribob</name>
      <uri>https://old.reddit.com/user/eribob</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am generally happy with my GPT-OSS-120b, thinking it is decently smart and all... But stumbled across the following trick question here on Reddit and had to try it (thanks &lt;a href="/u/kevin_1994"&gt;u/kevin_1994&lt;/a&gt;!):&lt;/p&gt; &lt;p&gt;`What should be the punishment for looking at your opponent's board in chess?`&lt;/p&gt; &lt;p&gt;Got the following reply...&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Short answer:&lt;/strong&gt;&lt;br /&gt; In an official tournament the arbiter treats ‚Äúlooking at your opponent‚Äôs board‚Äù as a rules breach. The usual graduated sanction is a &lt;strong&gt;warning first, then a time‚Äëpenalty, and on repeat or flagrant cases a loss of the game (and possibly expulsion).&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Why it‚Äôs a breach&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;FIDE Laws of Chess&lt;/strong&gt; (2023 ed.) ‚Äì &lt;em&gt;Article 11.3&lt;/em&gt; (Conduct of the players) says that players must not ‚Äúdistract or annoy the opponent‚Äù and must ‚Äúplay with ‚Äúthe pieces of his own board only‚Äù.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Article 12.7&lt;/em&gt; gives the arbiter the power to ‚Äúimpose a penalty for any breach of the Laws‚Äù.&lt;/li&gt; &lt;li&gt;The US Chess Federation‚Äôs (USCF) ‚ÄúCode of Ethics‚Äù likewise treats looking at the opponent‚Äôs board as an unsporting act that may be penalised.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The rule exists because seeing the opponent‚Äôs pieces gives you information you wouldn‚Äôt otherwise have (e.g., pieces that have been captured, what squares are free, etc.). Even a brief glance can be a competitive advantage. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;And it continues for another while with total slop... &lt;/p&gt; &lt;p&gt;Of course I know that this is not a measure of the models performance or usefulness, they can all stumble. Just thought it was fun.&lt;/p&gt; &lt;p&gt;Do you have any other examples of LLM trick questions that I could try? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eribob"&gt; /u/eribob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmbyuk/baffled_at_stupidity_of_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmjsi5</id>
    <title>The new Kimi K2 1T model (4-bit quant) runs on 2 512GB M3 Ultras [Awni Hannun/Twitter]</title>
    <updated>2025-12-14T17:33:14+00:00</updated>
    <author>
      <name>/u/pogue972</name>
      <uri>https://old.reddit.com/user/pogue972</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Awni Hannun (AI @ Apple employee) says: The new Kimi K2 1T model (4-bit quant) runs on 2 512GB M3 Ultras with mlx-lm and mx.distributed.&lt;/p&gt; &lt;p&gt;1 trillion params, at a speed that's actually quite usable&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pogue972"&gt; /u/pogue972 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://xcancel.com/awnihannun/status/1943723599971443134"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjsi5/the_new_kimi_k2_1t_model_4bit_quant_runs_on_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmjsi5/the_new_kimi_k2_1t_model_4bit_quant_runs_on_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T17:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmr7f0</id>
    <title>vLLM Rocm and 7900 XTX</title>
    <updated>2025-12-14T22:38:49+00:00</updated>
    <author>
      <name>/u/Frosty_Chest8025</name>
      <uri>https://old.reddit.com/user/Frosty_Chest8025</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Am I the only one deeply dissapointed with vLLM and AMD ?&lt;/p&gt; &lt;p&gt;Even with the vLLM 0.11 and rocm 7.0 there is basically only unquantized models being able to put in production with 7900 XTX and rocm?&lt;br /&gt; No matter which other model type, like qat or gguf etc. all are crap in performance.&lt;br /&gt; They do work but the performance is just crazy bad when doing simultaneous requests. &lt;/p&gt; &lt;p&gt;So if I can get some decent 10 to 15 requests per second with 2x7900 XTX and 12B unquantized Gemma3, when going to 27B qat 4q for example the speed drops to 1 request per second. That is not what the cards are actually cabable. That should be about 5 requests at least per sec with 128 token input output.&lt;/p&gt; &lt;p&gt;So any other than unquantized fp16 sucks big with rocm7.0 and vllm 0.11 (which is the latest 2 days ago updated officia vllm rocm docker image). Yes I have tried nightly builds with newer software but those wont work straight out. &lt;/p&gt; &lt;p&gt;So I think i need to just give up, and sell all these fkukin AMD consumer craps and go with rtx pro. So sad.&lt;/p&gt; &lt;p&gt;Fkuk you MAD and mVVL&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Chest8025"&gt; /u/Frosty_Chest8025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmr7f0/vllm_rocm_and_7900_xtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmr7f0/vllm_rocm_and_7900_xtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmr7f0/vllm_rocm_and_7900_xtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T22:38:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmmj5o</id>
    <title>Mistral Vibe CLI + Qwen 4B Q4</title>
    <updated>2025-12-14T19:23:59+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"&gt; &lt;img alt="Mistral Vibe CLI + Qwen 4B Q4" src="https://b.thumbs.redditmedia.com/tsR8stZU6ytKk17dmaIwBfnuH33xbqY8qeukP4WjAvQ.jpg" title="Mistral Vibe CLI + Qwen 4B Q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing with Mistral Vibe and Devstral-2, and it turned out to be useful for some serious C++ code, so I wanted to check whether it is possible to run it with a tiny 4B model, quantized to 4-bit. Let‚Äôs find out.&lt;/p&gt; &lt;p&gt;For this, we need a computer with a GPU that has 12 GB of VRAM, but you can use the CPU instead if you want.&lt;/p&gt; &lt;p&gt;First let's start llama-server:&lt;/p&gt; &lt;p&gt;&lt;code&gt;C:\Users\jacek\git\llama.cpp\build_2025.12.13\bin\Release\llama-server.exe -c 50000 --jinja -m J:\llm\models\Qwen3-4B-Instruct-2507-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;after installing mistral vibe you need to configure it, find file ~/.vibe/config.toml on your disk (on Windows it in the Users dir), then add following:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[[providers]] name = &amp;quot;local llamacpp&amp;quot; api_base = &amp;quot;http://127.0.0.1:8080/v1&amp;quot; api_key_env_var = &amp;quot;&amp;quot; api_style = &amp;quot;openai&amp;quot; backend = &amp;quot;generic&amp;quot; [[models]] name = &amp;quot;qwen&amp;quot; provider = &amp;quot;local llamacpp&amp;quot; alias = &amp;quot;local qwen&amp;quot; temperature = 0.2 input_price = 0.0 output_price = 0.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;now go to the llama.cpp sources and start vibe:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c3u7swz7z77g1.png?width=3786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52f2e310b0aa54fea327431f625a40a6e0eecdaa"&gt;https://preview.redd.it/c3u7swz7z77g1.png?width=3786&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52f2e310b0aa54fea327431f625a40a6e0eecdaa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;we can ask some general questions about coding&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/2nrmxvcez77g1.png?width=3746&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b975a93251ac09545875bc54dc1b13fca64c67c"&gt;https://preview.redd.it/2nrmxvcez77g1.png?width=3746&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4b975a93251ac09545875bc54dc1b13fca64c67c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and then vibe can browse the source&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5ax60qlkz77g1.png?width=3770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89e64fb6c0c581e170ec31d40edf23290691a088"&gt;https://preview.redd.it/5ax60qlkz77g1.png?width=3770&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=89e64fb6c0c581e170ec31d40edf23290691a088&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and explain what this code does&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hodoag5nz77g1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72cdd61f0eeeca05027199edbe93be8d1acc746d"&gt;https://preview.redd.it/hodoag5nz77g1.png?width=3744&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72cdd61f0eeeca05027199edbe93be8d1acc746d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;...all that on the dumb 4B Q4 model&lt;/p&gt; &lt;p&gt;With Devstral, I was able to use Vibe to make changes directly in the code, and the result was fully functional.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmmj5o/mistral_vibe_cli_qwen_4b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T19:23:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmnxic</id>
    <title>[Speculative decoding] feat: add EAGLE3 speculative decoding support by ichbinhandsome ¬∑ Pull Request #18039 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-14T20:21:25+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnxic/speculative_decoding_feat_add_eagle3_speculative/"&gt; &lt;img alt="[Speculative decoding] feat: add EAGLE3 speculative decoding support by ichbinhandsome ¬∑ Pull Request #18039 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/SgkTk0Uf1Yowr98PBGoG-DyDUnDlVPe96TUph4tbrRU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c617a9e8d2db98d4861325eaba1f1ae42a63cd2" title="[Speculative decoding] feat: add EAGLE3 speculative decoding support by ichbinhandsome ¬∑ Pull Request #18039 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the recent release of EAGLE models, people were wondering about EAGLE support in llama.cpp. Well, this just showed up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18039"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnxic/speculative_decoding_feat_add_eagle3_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmnxic/speculative_decoding_feat_add_eagle3_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:21:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmc7lk</id>
    <title>Understanding the new router mode in llama cpp server</title>
    <updated>2025-12-14T11:46:05+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"&gt; &lt;img alt="Understanding the new router mode in llama cpp server" src="https://preview.redd.it/t0ptvz6tp57g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c33dddb1bee595d50029e7ab305badd16b0eaf7" title="Understanding the new router mode in llama cpp server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What Router Mode Is&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Router mode is a new way to run the llama cpp server that lets you manage multiple AI models at the same time without restarting the server each time you switch or load a model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Previously, you had to start a new server process &lt;em&gt;per model&lt;/em&gt;. Router mode changes that. This &lt;strong&gt;update brings Ollama-like functionality&lt;/strong&gt; to the lightweight llama cpp server.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Route Mode Matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Imagine you want to try different models like a small one for basic chat and a larger one for complex tasks. Normally:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You would start one server per model.&lt;/li&gt; &lt;li&gt;Each one uses its own memory and port.&lt;/li&gt; &lt;li&gt;Switching models means stopping/starting things.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With &lt;strong&gt;router mode&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One server stays running.&lt;/li&gt; &lt;li&gt;You can &lt;strong&gt;load/unload models on demand&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;You tell the server &lt;em&gt;which model to use per request&lt;/em&gt;&lt;/li&gt; &lt;li&gt;It automatically routes the request to the right model internally&lt;/li&gt; &lt;li&gt;Saves memory and makes ‚Äúswapping models‚Äù easy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;When Router Mode Is Most Useful&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Testing multiple GGUF models&lt;/li&gt; &lt;li&gt;Building local OpenAI-compatible APIs&lt;/li&gt; &lt;li&gt;Switching between small and large models dynamically&lt;/li&gt; &lt;li&gt;Running demos without restarting servers&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://aixfunda.substack.com/p/the-new-router-mode-in-llama-cpp"&gt;Source &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://substackcdn.com/image/fetch/$s_!bcqv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6cee761-d6a0-40a1-89bf-0387ae1cb227_1024x544.jpeg"&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t0ptvz6tp57g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmc7lk/understanding_the_new_router_mode_in_llama_cpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T11:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmgm2x</id>
    <title>To Mistral and other lab employees: please test with community tools BEFORE releasing models</title>
    <updated>2025-12-14T15:24:05+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With Devstral 2, what should have been a great release has instead hurt Mistral's reputation. I've read accusations of cheating/falsifying benchmarks (even saw someone saying the model scoring 2% when he ran thew same benchmark), repetition loops, etc.&lt;/p&gt; &lt;p&gt;Of course Mistral didn't release broken models with the intelligence of a 1B. We know Mistral can make good models. This must have happened because of bad templates embedded in the model, poor doc, custom behavior required, etc. But by not ensuring everything is 100% before releasing it, they fucked up the release. &lt;/p&gt; &lt;p&gt;Whoever is in charge of releases, they basically watched their team spend months working on a model, then didn't bother doing 1 day of testing on the major community tools to reproduce the same benchmarks. They let their team down IMO.&lt;/p&gt; &lt;p&gt;I'm always rooting for labs releasing open models. Please, for your own sake and ours, do better next time.&lt;/p&gt; &lt;p&gt;P.S. For those who will say &amp;quot;local tools don't matter, Mistral's main concern is big customers in datacenters&amp;quot;, you're deluded. They're releasing home-sized models because they want AI geeks to adopt them. The attention of tech geeks is worth gold to tech companies. We're the ones who make the tech recommendations at work. Almost everything we pay for on my team at work is based on my direct recommendation, and it's biased towards stuff I already use successfully in my personal homelab.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmgm2x/to_mistral_and_other_lab_employees_please_test/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T15:24:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmpwmh</id>
    <title>2025 Open Models Year in Review</title>
    <updated>2025-12-14T21:43:46+00:00</updated>
    <author>
      <name>/u/robotphilanthropist</name>
      <uri>https://old.reddit.com/user/robotphilanthropist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"&gt; &lt;img alt="2025 Open Models Year in Review" src="https://external-preview.redd.it/teWgyqb-RDJIuhi9RJ3H3WjTJdfmLNMEjYONqxc6ag8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f9c5e72e6317c507adece143fd29a8a92eb26cc" title="2025 Open Models Year in Review" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Florian and I worked hard to follow what's happening this year. We put together our final year in review. It's focused on people training models end to end and our rankings downweigh noncommercial licenses and other restrictions that make using models below. A summary is in the text here.&lt;/p&gt; &lt;p&gt;What a year! We're back with an updated open model builder tier list, our top models of the year, and our predictions for 2026.&lt;/p&gt; &lt;p&gt;First, the winning models:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;DeepSeek R1: Transformed the AI world&lt;/li&gt; &lt;li&gt;Qwen 3 Family: The new default open models &lt;/li&gt; &lt;li&gt;Kimi K2 Family: Models that convinced the world that DeepSeek wasn't special and China would produce numerous leading models.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Runner up models: MiniMax M2, GLM 4.5, GPT-OSS, Gemma 3, Olmo 3&lt;/p&gt; &lt;p&gt;Honorable Mentions: Nvidia's Parakeet speech-to-text model &amp;amp; Nemotron 2 LLM, Moondream 3 VLM, Granite 4 LLMs, and HuggingFace's SmolLM3.&lt;/p&gt; &lt;p&gt;Tier list:&lt;/p&gt; &lt;p&gt;Frontier open labs: DeepSeek, Qwen, and Kimi Moonshot&lt;/p&gt; &lt;p&gt;Close behind: &lt;a href="http://Z.ai"&gt;Z.ai&lt;/a&gt; &amp;amp; MiniMax AI (notably none from the U.S.)&lt;/p&gt; &lt;p&gt;Noteworthy (a mix of US &amp;amp; China): StepFun AI, Ant Group's Inclusion AI, Meituan, Tencent, IBM, Nvidia, Google, &amp;amp; Mistral &lt;/p&gt; &lt;p&gt;Then a bunch more below that, which we detail.&lt;/p&gt; &lt;p&gt;Predictions for 2026:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Scaling will continue with open models.&lt;/li&gt; &lt;li&gt;No substantive changes in the open model safety narrative.&lt;/li&gt; &lt;li&gt;Participation will continue to grow.&lt;/li&gt; &lt;li&gt;Ongoing general trends will continue w/ MoEs, hybrid attention, dense for fine-tuning.&lt;/li&gt; &lt;li&gt;The open and closed frontier gap will stay roughly the same on any public benchmarks.&lt;/li&gt; &lt;li&gt;No Llama-branded open model releases from Meta in 2026.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Very appreciative of this community through both my hats at Interconnects &amp;amp; Ai2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robotphilanthropist"&gt; /u/robotphilanthropist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.interconnects.ai/p/2025-open-models-year-in-review"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmpwmh/2025_open_models_year_in_review/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T21:43:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmfglp</id>
    <title>First AI implosion: Oracle</title>
    <updated>2025-12-14T14:33:45+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Post says first domino to fall will be Oracle: &lt;a href="https://x.com/shanaka86/status/2000057734419620155"&gt;https://x.com/shanaka86/status/2000057734419620155&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After the implosion we should get our cheap memory back. I doubt this ram shortage is going to last as long as the chip shortage for cars. That one was 18 months. What do think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmfglp/first_ai_implosion_oracle/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T14:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pmo0dn</id>
    <title>Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace</title>
    <updated>2025-12-14T20:24:45+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt; &lt;img alt="Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace" src="https://b.thumbs.redditmedia.com/5GGgkZME9HXSe33YbnL-7M8w7p2MrLazBQlzRPrvN5A.jpg" title="Qwen3-Next-80B-A3B-Thinking-GGUF has just been released on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/75q6nveva87g1.png?width=1283&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b3b427e21b37b3009dc59534135e4394f375d9f8"&gt;qwen next 80b thinking tetris&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tested q4_k_m. It did the best Tetris in a single HTML file I've ever seen. I tried Devstral recently and the results weren't as accurate.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF"&gt;https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pmo0dn/qwen3next80ba3bthinkinggguf_has_just_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-14T20:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
