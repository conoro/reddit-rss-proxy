<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-03T22:49:46+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pdgudg</id>
    <title>What are the minimum viable LLMs to test "thinking" techniques?</title>
    <updated>2025-12-03T21:19:39+00:00</updated>
    <author>
      <name>/u/SometimesObsessed</name>
      <uri>https://old.reddit.com/user/SometimesObsessed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'd like to test various &amp;quot;thinking&amp;quot; techniques like chain-of-thought, tree-of-thought, etc. I'm wondering what you think the minimum viable language models are to get reasonable results back. And where the results would probably generalize to larger LMs. &lt;/p&gt; &lt;p&gt;The truly tiny LMs in huggingface are nice for speed, memory, and budget, but they tend to produce nonsense. I'm wondering if there's an LM I could run locally or call fairly cheaply via API to experiment with. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SometimesObsessed"&gt; /u/SometimesObsessed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgudg/what_are_the_minimum_viable_llms_to_test_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgudg/what_are_the_minimum_viable_llms_to_test_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgudg/what_are_the_minimum_viable_llms_to_test_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:19:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdgz3k</id>
    <title>Is it THE Mistral Large 3 benchmark?</title>
    <updated>2025-12-03T21:24:50+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgz3k/is_it_the_mistral_large_3_benchmark/"&gt; &lt;img alt="Is it THE Mistral Large 3 benchmark?" src="https://b.thumbs.redditmedia.com/-Trh_iaNUj9wzoPdBQVXIfwdL52zYXr4LF_bJwa0TQA.jpg" title="Is it THE Mistral Large 3 benchmark?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/qywlf0nm125g1.png?width=1482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c571833e59b57c86339467c0066938a771a6cfa"&gt;https://preview.redd.it/qywlf0nm125g1.png?width=1482&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c571833e59b57c86339467c0066938a771a6cfa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/s22jxtym125g1.png?width=1474&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3738287420522481be6f5444fdf0e98085fdf38"&gt;https://preview.redd.it/s22jxtym125g1.png?width=1474&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3738287420522481be6f5444fdf0e98085fdf38&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Not surprised about Ministral 3 performance, but is this THE Mistral Large 3 released yesterday, just below Qwen3 30B? Or it's some older model with the same name.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgz3k/is_it_the_mistral_large_3_benchmark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgz3k/is_it_the_mistral_large_3_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdgz3k/is_it_the_mistral_large_3_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcayfs</id>
    <title>Mistral 3 Blog post</title>
    <updated>2025-12-02T15:13:14+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt; &lt;img alt="Mistral 3 Blog post" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 Blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdhkhx</id>
    <title>Fine Tuning Project LLM. Specialized in Home use with IoT audio commands, audio relay of video analysis, etc..</title>
    <updated>2025-12-03T21:47:51+00:00</updated>
    <author>
      <name>/u/RefrigeratorPlus8700</name>
      <uri>https://old.reddit.com/user/RefrigeratorPlus8700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I'm wanting to develop a home assistant that can receive human like commands for IoT devices, be connected to schedules, give audio reminders, etc. I was wondering if any one had any experience doing that and could give some insight on the challenges that would come along with it. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RefrigeratorPlus8700"&gt; /u/RefrigeratorPlus8700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhkhx/fine_tuning_project_llm_specialized_in_home_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhkhx/fine_tuning_project_llm_specialized_in_home_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhkhx/fine_tuning_project_llm_specialized_in_home_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdhuyy</id>
    <title>Why doesn't deepseek release a smaller air model? Because they are focused at research?</title>
    <updated>2025-12-03T21:58:57+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why doesn't deepseek release a smaller air model like a 120b A10b MoE model or a 32b dense model? It seems like they are mainly focused in research and doesn't frequently release small models unlike GLM and qwen &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd8i1u</id>
    <title>Cheapest and best way to host a GGUF model with an API (like OpenAI) for production?</title>
    <updated>2025-12-03T16:15:32+00:00</updated>
    <author>
      <name>/u/New-Worry6487</name>
      <uri>https://old.reddit.com/user/New-Worry6487</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt; &lt;p&gt;I'm trying to host a &lt;code&gt;.gguf&lt;/code&gt; LLM in a way that lets me access it using an API ‚Äî similar to how we call the OpenAI API (&lt;code&gt;/v1/chat/completions&lt;/code&gt;, etc).&lt;br /&gt; I want to expose my own hosted GGUF model through a clean HTTP API that any app can use.&lt;/p&gt; &lt;h3&gt;What I need:&lt;/h3&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Host a GGUF model&lt;/strong&gt; (7B / 13B / possibly 30B later)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Access it over a REST API&lt;/strong&gt; (Ollama-style, OpenAI-style, or custom)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Production-ready setup&lt;/strong&gt; (stable, scalable enough, not hobby-only)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cheapest possible hosting options&lt;/strong&gt; (VPS or GPU cloud)&lt;/li&gt; &lt;li&gt;Advice on &lt;strong&gt;which server/runtime is best&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Ollama API server&lt;/li&gt; &lt;li&gt;llama.cpp server mode&lt;/li&gt; &lt;li&gt;LocalAI&lt;/li&gt; &lt;li&gt;vLLM (if GGUF isn‚Äôt ideal for it)&lt;/li&gt; &lt;li&gt;or anything else that works well&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Budget Focus&lt;/h3&gt; &lt;p&gt;Trying to find the &lt;strong&gt;best price-to-performance platform&lt;/strong&gt;.&lt;br /&gt; Options I'm considering but unsure about: - Hetzner - RunPod - Vast.ai - Vultr - Lambda Labs - Any cheap GPU rental providers?&lt;/p&gt; &lt;h3&gt;My goals:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Host the model once&lt;/li&gt; &lt;li&gt;Call it from my mobile or backend app through an API&lt;/li&gt; &lt;li&gt;Avoid OpenAI-style monthly costs&lt;/li&gt; &lt;li&gt;Keep latency reasonable&lt;/li&gt; &lt;li&gt;Ensure it runs reliably even with multiple requests&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Questions:&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;What‚Äôs the &lt;strong&gt;cheapest&lt;/strong&gt; but still &lt;strong&gt;practical&lt;/strong&gt; setup for production?&lt;/li&gt; &lt;li&gt;Is &lt;strong&gt;Ollama on a VPS&lt;/strong&gt; good enough?&lt;/li&gt; &lt;li&gt;Should I use &lt;strong&gt;llama.cpp server&lt;/strong&gt; instead?&lt;/li&gt; &lt;li&gt;Does anyone run GGUF models in production at scale?&lt;/li&gt; &lt;li&gt;Any recommended architectures or pitfalls?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would really appreciate hearing what setups have worked for you ‚Äî especially from people who have deployed GGUF models behind an API for real apps!&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Worry6487"&gt; /u/New-Worry6487 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd8i1u/cheapest_and_best_way_to_host_a_gguf_model_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd8i1u/cheapest_and_best_way_to_host_a_gguf_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd8i1u/cheapest_and_best_way_to_host_a_gguf_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T16:15:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1zeu</id>
    <title>apple/CLaRa-7B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-12-03T11:43:05+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-Instruct"&gt;https://huggingface.co/apple/CLaRa-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-Base"&gt;https://huggingface.co/apple/CLaRa-7B-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-E2E"&gt;https://huggingface.co/apple/CLaRa-7B-E2E&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3uvy</id>
    <title>Can we expect better LLM hardware in 2026?</title>
    <updated>2025-12-03T13:16:07+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean with a lot of fast(!) VRAM.&lt;/p&gt; &lt;p&gt;DGX spark and AMD AI Max have really low memory speeds.&lt;/p&gt; &lt;p&gt;China is releasing so many open source models, when will they come with cheap hardware that we can run them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pceipb</id>
    <title>Mistral just released Mistral 3 ‚Äî a full open-weight model family from 3B all the way up to 675B parameters.</title>
    <updated>2025-12-02T17:26:06+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are Apache 2.0 and fully usable for research + commercial work.&lt;/p&gt; &lt;p&gt;Quick breakdown:&lt;/p&gt; &lt;p&gt;‚Ä¢ Ministral 3 (3B / 8B / 14B) ‚Äì compact, multimodal, and available in base, instruct, and reasoning variants. Surprisingly strong for their size.&lt;/p&gt; &lt;p&gt;‚Ä¢ Mistral Large 3 (675B MoE) ‚Äì their new flagship. Strong multilingual performance, high efficiency, and one of the most capable open-weight instruct models released so far.&lt;/p&gt; &lt;p&gt;Why it matters: You now get a full spectrum of open models that cover everything from on-device reasoning to large enterprise-scale intelligence. The release pushes the ecosystem further toward distributed, open AI instead of closed black-box APIs.&lt;/p&gt; &lt;p&gt;Full announcement: &lt;a href="https://mistral.ai/news/mistral-3"&gt;https://mistral.ai/news/mistral-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcomhi</id>
    <title>I'm surprised how simple Qwen3 VL's architecture is.</title>
    <updated>2025-12-02T23:50:06+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt; &lt;img alt="I'm surprised how simple Qwen3 VL's architecture is." src="https://preview.redd.it/bfrh4xf5nv4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2643d2d6457fb6e3adfc09a5cf9e18b995e4219f" title="I'm surprised how simple Qwen3 VL's architecture is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the new 3D position id logic really got a lot more intuitive compared to qwen2.5 vl. it basically index image patches on width and height dimension in addition to the regular token sequence / temporal dimension (while treating text as one same number across all 3 dimensions). &lt;/p&gt; &lt;p&gt;in addition to this, they added deepstack, which essentially is just some residual connections between vision encoder blocks and downstream LLM blocks.&lt;/p&gt; &lt;p&gt;here's the full repo if you want to read more: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfrh4xf5nv4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T23:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcp8z3</id>
    <title>Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?</title>
    <updated>2025-12-03T00:16:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt; &lt;img alt="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" src="https://preview.redd.it/buxyht7ltv4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed225e778fb3ebb1d3e4ff9ac401e09c3aced65e" title="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking at you, Unsloth üòÅ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/buxyht7ltv4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1yqc</id>
    <title>Hot take: We‚Äôre overselling 'semantic search' in RAG.</title>
    <updated>2025-12-03T11:42:02+00:00</updated>
    <author>
      <name>/u/Raisin_False</name>
      <uri>https://old.reddit.com/user/Raisin_False</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building some RAG stuff and 'semantic search' feels way more magical in marketing than in reality.&lt;/p&gt; &lt;p&gt;Embeddings are great &lt;strong&gt;fuzzy matchers in meaning space&lt;/strong&gt; - they shine on paraphrases, synonyms, 'something like this' queries. But whenever I need sharper behavior (logic, constraints, dates, 'papers using X on Y after 2019'), plain bi-encoder vector search starts to fall over unless I add extra machinery.&lt;/p&gt; &lt;p&gt;In practice my setups end up looking more like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;BM25 or dense (or hybrid) &lt;/li&gt; &lt;li&gt;Reranker and/or LLM query rewrite &lt;/li&gt; &lt;li&gt;LLM reasoning also maybe graphs/filters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At that point, calling just the first stage 'semantic search' feels a bit misleading, cause it's more like 'dense/vector retrieval' plus a bunch of stuff on top that actually does the reasoning.&lt;/p&gt; &lt;p&gt;So i have 2 questions for you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is 'semantic search' a fair name for plain vector similarity, or do you avoid that term?&lt;/li&gt; &lt;li&gt;How far did you get with just embeddings before needing reranking / query rewriting / graphs / filters?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raisin_False"&gt; /u/Raisin_False &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3mdw</id>
    <title>Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI</title>
    <updated>2025-12-03T13:05:36+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt; &lt;img alt="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" src="https://external-preview.redd.it/0mZ7_HvOTkdLgtq4s_qT3vry9cE_RWRALKiuljZ3Fl8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01be5fde96c8bded5f16d12f17d20ed686c5e29" title="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b60-battlematrix-preview-192gb-of-vram-for-on-premise-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdi8o7</id>
    <title>DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp; CPU Support</title>
    <updated>2025-12-03T22:13:40+00:00</updated>
    <author>
      <name>/u/Dogacel</name>
      <uri>https://old.reddit.com/user/Dogacel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt; &lt;img alt="DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" src="https://external-preview.redd.it/9aiVAPD5fnnzElgE74nQoRx4aoPflI7CwyCPycl2BLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f593757a501d49b05111d714ec689f7acf643d1" title="DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently updated DeepSeek-OCR to support Apple Metal (MPS) and CPU acceleration. I wanted to share this in case anyone else has been looking to run it efficiently on macOS.&lt;/p&gt; &lt;p&gt;To make it easier to use, I also forked an existing desktop client and applied the patch. You can check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Dogacel/deepseek-ocr-client-macos"&gt;https://github.com/Dogacel/deepseek-ocr-client-macos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogacel"&gt; /u/Dogacel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Dogacel/DeepSeek-OCR-Metal-MPS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdin3b</id>
    <title>The Best Open Weights Coding Models of 2025</title>
    <updated>2025-12-03T22:29:44+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt; &lt;img alt="The Best Open Weights Coding Models of 2025" src="https://external-preview.redd.it/wxWktbYwfvy3j0Y1BhifZGPwnHUnY7JGZEyykx4N_HM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8af816e33acba1ab83ce8086f21346740c37d" title="The Best Open Weights Coding Models of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm back with uncontaminated evals for DeepSeek-V3.2, Kimi K2 Thinking, and MiniMax M2. (We caught GLM 4.6 last time around.) &lt;/p&gt; &lt;p&gt;If you just want the numbers, you can find them for the finalists &lt;a href="https://brokk.ai/power-ranking?models=dsv3.2%2Cglm4.6-fp8%2Ck2-thinking%2Cm2"&gt;here&lt;/a&gt; and for everyone else &lt;a href="https://brokk.ai/power-ranking?dataset=openround"&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.brokk.ai/the-best-open-weights-coding-models-of-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9tgj</id>
    <title>A Technical Tour of the DeepSeek Models from V3 to V3.2</title>
    <updated>2025-12-03T17:03:17+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt; &lt;img alt="A Technical Tour of the DeepSeek Models from V3 to V3.2" src="https://external-preview.redd.it/Oy9W7OYOeVO8Z6Sl3EWWZR-9AbREkAwoyEei1XJ7yeY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a5effd7f132f71b2efdd47cc12daa448023c0bf" title="A Technical Tour of the DeepSeek Models from V3 to V3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T17:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd5yxy</id>
    <title>My experiences with the new Ministral 3 14B Reasoning 2512 Q8</title>
    <updated>2025-12-03T14:41:03+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt; &lt;img alt="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" src="https://b.thumbs.redditmedia.com/YQNWxn03P5a0Q35GBj3cSIS0Oa0a8pdRn0Pkkl0sUGM.jpg" title="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;45 minutes and 33K tokens of thinking about making html tetris (1 line prompt):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819"&gt;https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tool calling breaks all the time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184"&gt;https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also at some point it stopped using the [think] tags altogether and just started thinking out loud. I'll leave it running for a couple of hours and see if it eventually manages to build the HTML Tetris.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T14:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9f4x</id>
    <title>I trained a 7B to learn a niche language and reaching 86% code accuracy</title>
    <updated>2025-12-03T16:49:14+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"&gt; &lt;img alt="I trained a 7B to learn a niche language and reaching 86% code accuracy" src="https://b.thumbs.redditmedia.com/y1P83lnQKXF0ESpmUcRyqN8DslMG6uG2cjIoC8294mY.jpg" title="I trained a 7B to learn a niche language and reaching 86% code accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I just wanted to share a project I did over the last weekend.&lt;/p&gt; &lt;p&gt;I‚Äôm no ML engineer or having any relevant background in AI, just have been toying with the idea of training an LLM myself for a while.&lt;/p&gt; &lt;p&gt;Most of my previous training attempts did not yield and meaningful result, but I‚Äôm still managed to learned a thing or two. And this time, I decided to give it a try again.&lt;/p&gt; &lt;p&gt;The niche language I picked to train the LLM (Qwen2.5-coder-7b) was a less popular text-to-diagram language called Pintora. Since most open source models did not have any knowledge about this language, it‚Äôs a fun project to try.&lt;/p&gt; &lt;p&gt;Long story short, I planned to train this for free on Google Colab, but ended up renting a 48GB A40 for a naive mistake, and doing a lot of the training pipeline myself (in a much smaller scale), from creating the dataset, cleaning them up, to do two phases training: Continued Pretraining and then Instruction Finetune, to teach the model how to either generate diagrams from scratch and editing existing diagrams. &lt;/p&gt; &lt;p&gt;In the end, I‚Äôm quite happy with the result, although it‚Äôs not great, the model was able to generate syntactically correct code, the diagrams are showing up. I did a quick evaluation to confirm how accurate (in terms of of compile-able diagrams) that the model can generate, out of 1000 examples, only about 140 are failing, that‚Äôs about 86% accuracy.&lt;/p&gt; &lt;p&gt;Both the model (safetensors, gguf, full and quantized) are available on HF if you are interested. I also did a write up to document the process, I think it might be helpful to share so I can learn from all of your feedback! &lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language"&gt;https://huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/huytd189/pintora-coder-7b"&gt;https://huggingface.co/huytd189/pintora-coder-7b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/huytd189/pintora-coder-7b-gguf"&gt;https://huggingface.co/huytd189/pintora-coder-7b-gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dataset:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/huytd189/pintora-instruct"&gt;https://huggingface.co/datasets/huytd189/pintora-instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/huytd189/pintora-edit-instruct"&gt;https://huggingface.co/datasets/huytd189/pintora-edit-instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pd9f4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3xyp</id>
    <title>Why don't Google and Openai release their old models?</title>
    <updated>2025-12-03T13:19:51+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPt 4 and gemini 2 pro are dated, they should release it... Are they afraid of releasing their data and architecture? They released gemma and gpt oss already. Gemini 2 has a large context window, but the quality degrades when it gets large though and it is replicable.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
