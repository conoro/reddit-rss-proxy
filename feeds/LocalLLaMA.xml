<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-04T23:06:40+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q40qec</id>
    <title>Real-time visibility into PyTorch training (dataloader stalls, memory leaks, step time drift)</title>
    <updated>2026-01-04T20:31:11+00:00</updated>
    <author>
      <name>/u/traceml-ai</name>
      <uri>https://old.reddit.com/user/traceml-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, &lt;/p&gt; &lt;p&gt;Quick share, I have been working on TraceML, a live observability tool for PyTorch training that shows you what's happening in real-time while your job runs. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it tracks live:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dataloader fetch time (catches input pipeline stalls)&lt;/li&gt; &lt;li&gt;GPU step time (non-blocking CUDA events, no sync overhead)&lt;/li&gt; &lt;li&gt;GPU CUDA memory (spots leaks before OOM)&lt;/li&gt; &lt;li&gt;Layerwise memory and compute time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has two modes: lightweight &lt;strong&gt;essential&lt;/strong&gt; mode that runs with minimal overhead, and a deeper &lt;strong&gt;diagnostic&lt;/strong&gt; mode for layerwise breakdowns when you need it.&lt;/p&gt; &lt;p&gt;Works with any PyTorch model. I have tested on LLM fine-tuning (TinyLLaMA + QLoRA), but it's model-agnostic. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Read the full breakdown:&lt;/strong&gt; &lt;a href="https://medium.com/p/af8fbd899928"&gt;https://medium.com/p/af8fbd899928&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/traceopt-ai/traceml"&gt;https://github.com/traceopt-ai/traceml&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently supports single GPU, multi-GPU coming soon. If anyone tries it and has feedback or feature requests, I am actively responding to issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/traceml-ai"&gt; /u/traceml-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q40qec/realtime_visibility_into_pytorch_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q40qec/realtime_visibility_into_pytorch_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q40qec/realtime_visibility_into_pytorch_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:31:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3w7pw</id>
    <title>Gen-AI Security</title>
    <updated>2026-01-04T17:42:43+00:00</updated>
    <author>
      <name>/u/meetrais</name>
      <uri>https://old.reddit.com/user/meetrais</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;My this GitHub repo has comprehensive guide and sample code for gen-ai security topics.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/meetrais/genai-security"&gt;https://github.com/meetrais/genai-security&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meetrais"&gt; /u/meetrais &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3w7pw/genai_security/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3w7pw/genai_security/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3w7pw/genai_security/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T17:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3z89v</id>
    <title>Good article on training vs inference architectures for data center compute (and why Groq for Nvidia)</title>
    <updated>2026-01-04T19:34:53+00:00</updated>
    <author>
      <name>/u/Mental-At-ThirtyFive</name>
      <uri>https://old.reddit.com/user/Mental-At-ThirtyFive</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mental-At-ThirtyFive"&gt; /u/Mental-At-ThirtyFive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/infrastructure/inference-is-splitting-in-two-nvidias-usd20b-groq-bet-explains-its-next-act"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3z89v/good_article_on_training_vs_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3z89v/good_article_on_training_vs_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T19:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3ni9a</id>
    <title>Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works</title>
    <updated>2026-01-04T11:16:46+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt; &lt;img alt="Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works" src="https://b.thumbs.redditmedia.com/NqogaSPaaD_FMIyK39hO5FVNMpj8KVCo4B1fbHqRmNk.jpg" title="Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ajwolqh4gbbg1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00d58cca824906c8b394d7ef2c1276fb3a1861a9"&gt;Testing coding.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SWE-Bench Style Prompt: &amp;quot;The Database Connection Leak&amp;quot;&lt;br /&gt; Project Context: You are working on a backend service called fast-api-sync. The system handles database sessions. You have two files:&lt;/p&gt; &lt;p&gt;infrastructure/db_manager.py: Handles the low-level connection logic.&lt;/p&gt; &lt;p&gt;services/data_processor.py: Uses the manager to save processed data.&lt;/p&gt; &lt;p&gt;Current Code:&lt;/p&gt; &lt;p&gt;infrastructure/db_manager.py:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;p&gt;class DatabaseConnection:&lt;br /&gt; def &lt;strong&gt;init&lt;/strong&gt;(self):&lt;br /&gt; self.is_connected = False&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def connect(self): print(&amp;quot;Connecting to DB...&amp;quot;) self.is_connected = True def disconnect(self): print(&amp;quot;Closing connection...&amp;quot;) self.is_connected = False def execute_query(self, query): if not self.is_connected: raise ConnectionError(&amp;quot;Database not connected!&amp;quot;) return f&amp;quot;Result for {query}&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;services/data_processor.py:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;p&gt;from infrastructure.db_manager import DatabaseConnection&lt;/p&gt; &lt;p&gt;def process_and_save(data_list):&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; Processes a list of items and saves them to the DB.&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; db = DatabaseConnection()&lt;br /&gt; db.connect()&lt;/p&gt; &lt;pre&gt;&lt;code&gt;results = [] for item in data_list: # Business logic: if item is None, we skip it if item is None: continue result = db.execute_query(f&amp;quot;INSERT {item}&amp;quot;) results.append(result) db.disconnect() return results &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Bug: Users are reporting Connection Leaks. If an error occurs during the execute_query call (e.g., a syntax error or timeout), the db.disconnect() method is never called, leaving the database connection open.&lt;/p&gt; &lt;p&gt;Your Task: Refactor services/data_processor.py to ensure the connection is always closed, even if an exception is raised during processing.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;p&gt;Use a try...finally block to guarantee the disconnection.&lt;/p&gt; &lt;p&gt;Refactoring Goal: Instead of creating a new DatabaseConnection inside the function (which is hard to test), modify the function signature to accept a db_connection instance as an optional argument (Dependency Injection). If no instance is provided, then create a new one.&lt;/p&gt; &lt;p&gt;If the function creates its own connection, it must close it. If it receives an external connection, it should not close it (as the caller might want to use it again).&lt;/p&gt; &lt;p&gt;Output: Provide the updated services/data_processor.py.&lt;/p&gt; &lt;p&gt;Result: I asked Gemini 3 to evaluate the result. &lt;/p&gt; &lt;p&gt;Here is the evaluation of the solution in English. &lt;/p&gt; &lt;p&gt;This response indicates that the LLM is operating at a &lt;strong&gt;Senior Software Engineer&lt;/strong&gt; level.&lt;/p&gt; &lt;h1&gt;Evaluation: Senior / Expert Level&lt;/h1&gt; &lt;p&gt;The model passed all the critical logic tests, demonstrating a deep understanding of &lt;strong&gt;software architecture&lt;/strong&gt;, &lt;strong&gt;resource ownership&lt;/strong&gt;, and &lt;strong&gt;robustness&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Key Strengths of the Solution&lt;/h1&gt; &lt;h1&gt;1. Sophisticated Resource Ownership (The &amp;quot;Expert&amp;quot; Touch)&lt;/h1&gt; &lt;p&gt;The model correctly identified the most complex part of the requirement: &lt;strong&gt;&amp;quot;Who opens the connection must be the one to close it.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It introduced the &lt;code&gt;should_close&lt;/code&gt; flag. This is crucial because if an external connection is injected, the function should &lt;strong&gt;not&lt;/strong&gt; disconnect it, as the caller likely needs it for subsequent tasks.&lt;/li&gt; &lt;li&gt;Most standard LLMs fail here by putting &lt;code&gt;db.disconnect()&lt;/code&gt; in the &lt;code&gt;finally&lt;/code&gt; block without checking where the connection originated, which would break the caller's workflow.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Proper Dependency Injection (DI)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It correctly modified the signature: &lt;code&gt;def process_and_save(data_list, db_connection=None)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It maintained &lt;strong&gt;backward compatibility&lt;/strong&gt;. Existing code calling &lt;code&gt;process_and_save(my_list)&lt;/code&gt; will still work perfectly because the parameter is optional.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Guaranteed Cleanup (Exception Safety)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;By using the &lt;code&gt;try...finally&lt;/code&gt; block, it ensures that there are no &amp;quot;connection leaks.&amp;quot; Even if &lt;code&gt;db.execute_query&lt;/code&gt; raises an exception (e.g., a timeout or syntax error), the resource is released if it was created locally.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Logical Integrity&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The model preserved the existing business logic (&lt;code&gt;if item is None: continue&lt;/code&gt;) while wrapping it in the new safety structure.&lt;/li&gt; &lt;li&gt;The comments are professional and explain the &lt;em&gt;why&lt;/em&gt; (the logic of the lifecycle) rather than just the &lt;em&gt;what&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Verdict&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Score: 10/10&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The LLM being tested is highly capable of handling &lt;strong&gt;real-world refactoring tasks&lt;/strong&gt;. It doesn't just &amp;quot;write code that runs&amp;quot;; it writes code that respects the &lt;strong&gt;contracts&lt;/strong&gt; between different parts of a system. It understands side effects and state management.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q31ltd</id>
    <title>Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched</title>
    <updated>2026-01-03T18:11:26+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt; &lt;img alt="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" src="https://b.thumbs.redditmedia.com/q10c8h1nk9ulWsUdfgzAsIflFbSIgTra6fA9fSqXqqQ.jpg" title="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife&lt;/p&gt; &lt;p&gt;It started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1"&gt;https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It got to the information but I had questions about why it thought for 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so &amp;quot;outlandish&amp;quot; that the model was actively choosing to ignore the proof that it had pulled. &lt;/p&gt; &lt;p&gt;I added:&lt;/p&gt; &lt;p&gt;Evidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way. &lt;/p&gt; &lt;p&gt;So then I thought lets go talk to Spark, my trusty default model that never lets me down. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca"&gt;https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Spark 4.0 is GPT-OSS:20B that is always loaded for the family and runs on a dedicated 4080 Super. &lt;/p&gt; &lt;p&gt;Spark just flat out said, nope cant help you and then said it didnt have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT etc that I gave Qwen that it finally acknowledged that the event was real.&lt;/p&gt; &lt;p&gt;I'm testing with GPT-OSS:120B now and its working thru the process of &amp;quot;skeptical but verify&amp;quot; much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b"&gt;https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But he powered thru and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say &amp;quot;Nah, that CAN'T be real, that's too ridiculous&amp;quot;, the event must be really bad. But it does shine a light on knowledge cut offs, &amp;quot;fake news&amp;quot; threshold, how models handle global/international events and the smaller models we daily drive. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q442q1</id>
    <title>Built an API to index videos into embeddings—optimized for running RAG locally</title>
    <updated>2026-01-04T22:44:05+00:00</updated>
    <author>
      <name>/u/soroushamdg</name>
      <uri>https://old.reddit.com/user/soroushamdg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMA folks, I'm working on something that might be useful if you're running RAG setups locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Video indexing for RAG is a pain. If you want to index your own videos (recordings, lectures, internal content) for local LLM querying, you either:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manually run Whisper + OCR + embedding code&lt;/li&gt; &lt;li&gt;Rely on cloud APIs (defeats the purpose of local)&lt;/li&gt; &lt;li&gt;Give up and just use transcripts (miss all visual context)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt;&lt;br /&gt; An API that handles the messy preprocessing: transcript extraction, frame sampling, OCR, and embedding. You get back clean, chunked JSON that's ready to feed into your local vector store (Milvus, Weaviate, whatever).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Transcript + OCR:&lt;/strong&gt; Captures both speech and visual content (slides, UI, diagrams)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Timestamped chunks:&lt;/strong&gt; So you can jump back to the source video&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Embeddings included:&lt;/strong&gt; Ready for local semantic search&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Minimal dependencies:&lt;/strong&gt; I keep processing lightweight (CPU-friendly frame sampling, local OCR option)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Use cases for local builders:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Index internal/private videos without uploading to cloud&lt;/li&gt; &lt;li&gt;Run semantic search over your own video archives using local LLMs&lt;/li&gt; &lt;li&gt;Build local RAG agents that reference video content&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt;&lt;br /&gt; Live demo on the site shows what the output looks like. You can search inside sample videos and see the exact JSON chunks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The ask:&lt;/strong&gt;&lt;br /&gt; If you're building local RAG stuff and this solves a pain point, I'd love feedback. Also curious if you'd want self-hosted/on-prem options.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;URL:&lt;/strong&gt; &lt;a href="https://www.vector-vid.com/"&gt;https://www.vector-vid.com/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soroushamdg"&gt; /u/soroushamdg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q442q1/built_an_api_to_index_videos_into/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q442q1/built_an_api_to_index_videos_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q442q1/built_an_api_to_index_videos_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T22:44:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3uliz</id>
    <title>766ms voice assistant on DGX Spark - VibeVoice + Whisper + Ollama streaming pipeline</title>
    <updated>2026-01-04T16:42:27+00:00</updated>
    <author>
      <name>/u/logos_flux</name>
      <uri>https://old.reddit.com/user/logos_flux</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Microsoft's new VibeVoice-Realtime TTS running on DGX Spark with full GPU acceleration. Sharing the setup since I couldn't find any guides for this. I know the issues about running interference on Spark, not the point of this post. &lt;/p&gt; &lt;h3&gt;The Numbers&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Before&lt;/th&gt; &lt;th&gt;After&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Time to first audio&lt;/td&gt; &lt;td&gt;2-3 seconds&lt;/td&gt; &lt;td&gt;&lt;strong&gt;766ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;TTS speed&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;RTF 0.48x (2x faster than real-time)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Architecture&lt;/h3&gt; &lt;p&gt;Mic → Whisper STT → Ollama LLM → VibeVoice TTS → Speaker&lt;/p&gt; &lt;p&gt;The key insight: &lt;strong&gt;sentence-level streaming&lt;/strong&gt;. Buffer LLM tokens until you hit a sentence boundary (. ! ?), then immediately stream that sentence to TTS while the LLM keeps generating. Combined with continuous audio playback (OutputStream with callback instead of discrete play() calls), it feels responsive.&lt;/p&gt; &lt;h3&gt;The Fix for Spark&lt;/h3&gt; &lt;p&gt;If you're seeing &lt;code&gt;CUDA available: False&lt;/code&gt; on DGX Spark, your PyTorch may not have CUDA enabled. This is a common issue - &lt;a href="https://simonwillison.net/2025/Oct/14/nvidia-dgx-spark/"&gt;Simon Willison wrote about struggling with PyTorch on Spark&lt;/a&gt;, and there are multiple NVIDIA forum threads about it.&lt;/p&gt; &lt;p&gt;Fix:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash pip uninstall torch torchaudio torchvision -y pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;NVIDIA has ARM64 + CUDA 13 wheels on PyPI - this installs the GPU-enabled version.&lt;/p&gt; &lt;h3&gt;VibeVoice Notes&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;0.5B Realtime model: ~300ms to first audio, but only 7 preset voices (Emma, Mike, Carter, Davis, Frank, Grace, Samuel)&lt;/li&gt; &lt;li&gt;1.5B model: Voice cloning from 10s audio sample, but higher latency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full code: &lt;a href="https://github.com/Logos-Flux/spark-voice-pipeline"&gt;GitHub link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logos_flux"&gt; /u/logos_flux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3uliz/766ms_voice_assistant_on_dgx_spark_vibevoice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3uliz/766ms_voice_assistant_on_dgx_spark_vibevoice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3uliz/766ms_voice_assistant_on_dgx_spark_vibevoice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T16:42:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3kdb0</id>
    <title>Is Kimi K2 Vision about to be released?</title>
    <updated>2026-01-04T08:06:52+00:00</updated>
    <author>
      <name>/u/Sherrydelectable7</name>
      <uri>https://old.reddit.com/user/Sherrydelectable7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt; &lt;img alt="Is Kimi K2 Vision about to be released?" src="https://b.thumbs.redditmedia.com/4TCUZ05vDXOcmTNM_AXACK65pwIbjfiIAkQnyjJ60Rg.jpg" title="Is Kimi K2 Vision about to be released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/70168651iabg1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6681761c4ba97b45ffa31d7ff8ed95d70ee8a7f"&gt;https://preview.redd.it/70168651iabg1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6681761c4ba97b45ffa31d7ff8ed95d70ee8a7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new model called &lt;strong&gt;Kiwi do&lt;/strong&gt; has appeared on Lmarena.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherrydelectable7"&gt; /u/Sherrydelectable7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T08:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3m9oc</id>
    <title>is there any reason why Qwen has been really quiet about llms recently?</title>
    <updated>2026-01-04T10:02:49+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T10:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q40r6e</id>
    <title>5070 Ti slower than 4070 Ti when ram spills?</title>
    <updated>2026-01-04T20:32:03+00:00</updated>
    <author>
      <name>/u/AllTey</name>
      <uri>https://old.reddit.com/user/AllTey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I recently upgraded my GPU from a 4070 Ti (12GB) to an 5070 Ti (16GB). When I load a model with a context that's larger than the VRAM and it spills to system memory, the 5070 Ti is way slower.&lt;/p&gt; &lt;p&gt;E. g. with ministral 3 14b (Q4_K_M) with 64k ctx I get 23 t/s with the 4070 Ti, but only 11 t/s with the newer 5070 Ti. When there is no ram spill the 5070 Ti is faster, which is to be expected.&lt;/p&gt; &lt;p&gt;Why can that be the case? Surely the older card can not be this much faster when offloading to system ram?&lt;/p&gt; &lt;p&gt;Loading this model with 262144 ctx and q4 kv cache quant will result in 33 t/s on 4070 Ti and 9 t/s on 5070 Ti. This is weird, isn't it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllTey"&gt; /u/AllTey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q40r6e/5070_ti_slower_than_4070_ti_when_ram_spills/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q40r6e/5070_ti_slower_than_4070_ti_when_ram_spills/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q40r6e/5070_ti_slower_than_4070_ti_when_ram_spills/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:32:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q414vj</id>
    <title>gsh - play with any local model directly in your shell REPL or scripts</title>
    <updated>2026-01-04T20:46:34+00:00</updated>
    <author>
      <name>/u/atinylittleshell</name>
      <uri>https://old.reddit.com/user/atinylittleshell</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q414vj/gsh_play_with_any_local_model_directly_in_your/"&gt; &lt;img alt="gsh - play with any local model directly in your shell REPL or scripts" src="https://preview.redd.it/yh1dwt9j8ebg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=383dc60edeb928b15b21feb2777015966be76471" title="gsh - play with any local model directly in your shell REPL or scripts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing a holiday side project i just built: gsh - a new shell, like bash, zsh, fish, but fully agentic. I find it really useful for playing with local models both interactively and in automation scripts. &lt;a href="https://github.com/atinylittleshell/gsh"&gt;https://github.com/atinylittleshell/gsh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Key features:&lt;br /&gt; - It can predict the next shell command you may want to run, or help you write one when you forgot how to&lt;br /&gt; - It can act as a coding agent itself, or delegate to other agents via ACP&lt;br /&gt; - It comes with an agentic scripting language which you can use to build agentic workflows, or to customize gsh (almost the entire repl can be customized, like neovim)&lt;br /&gt; - Use whatever LLM you like - a lot can be done with local models&lt;br /&gt; - Battery included - syntax highlighting, tab completion, history, auto suggestion, starship integration all work out of the box&lt;/p&gt; &lt;p&gt;Super early of course, but i've been daily driving for a while and replaced zsh with it. If you think it's time to try a new shell or new ways to play with local models, give it a try and let me know how it goes! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atinylittleshell"&gt; /u/atinylittleshell &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yh1dwt9j8ebg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q414vj/gsh_play_with_any_local_model_directly_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q414vj/gsh_play_with_any_local_model_directly_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:46:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3t7go</id>
    <title>LLM memory systems</title>
    <updated>2026-01-04T15:48:40+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is good in LLM memory systems these days?&lt;/p&gt; &lt;p&gt;I don’t mean RAG&lt;/p&gt; &lt;p&gt;I mean like memory storage that an LLM can read or write to, or long-term memory that persists across generations &lt;/p&gt; &lt;p&gt;Has anyone seen any interesting design patterns or github repos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T15:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3oqvl</id>
    <title>MiniMax-M2.1 REAP models from 0xSero</title>
    <updated>2026-01-04T12:27:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now you can run MiniMax on everything :)&lt;/p&gt; &lt;p&gt;(waiting for GGUFs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-40"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-30"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-25"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;looks like there will be more: Intellect3 25 / 30 / 40 / 50&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3nvcn</id>
    <title>Can you connect a GPU with 12V rail coming from a second PSU?</title>
    <updated>2026-01-04T11:38:04+00:00</updated>
    <author>
      <name>/u/Rock_and_Rolf</name>
      <uri>https://old.reddit.com/user/Rock_and_Rolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt; &lt;img alt="Can you connect a GPU with 12V rail coming from a second PSU?" src="https://preview.redd.it/vyb6lxutibbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ed9009e4b7eaa72d7fc916ae20f8ff60475ec3" title="Can you connect a GPU with 12V rail coming from a second PSU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;; Can you connect a GPU with the 12V rail coming from a second PSU? &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update1:&lt;/strong&gt; I have already made a connector to connect both GND's, i forgot to mention this.&lt;br /&gt; &lt;strong&gt;Update2:&lt;/strong&gt; I have found another way to test this without breaking needed hardware. Somebody on a local marketplace sells a GTX770 for €20 that appears to have a 6 + 8 pin power connector, i can pick this up in a few hours. If this doesn't work i'll look in to splitting 12V or bifurcation. Thanks for your replies!!&lt;br /&gt; &lt;strong&gt;Update3:&lt;/strong&gt; I nearly have my scrap test setup ready to test, but I have other thing to do now and will continue tomorrow, i'll keep you all posted. Thanks for all the replies, much appreciated!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full story&lt;/strong&gt;; I currently have a Dell T7910 with two AMD Radeon VII's (GFX906, Pmax set=190W) to play with LLMs/Roo Code. Last week, i managed to buy two more of these GPU's for an absurdly low price. I knew i had enough PCI-E slots, but i would need to use PCI-E extender cables to actually connect them (i already bought a pair). But i hadn't fully thought about the power supply, because despite the 1300W PSU, it doesn't have enough 8 or 6-pin 12V connectors. Now i have a second 950W PSU from a deceased Dell T5820 that i could use to power these extra GPUs.&lt;/p&gt; &lt;p&gt;As i am an electrical engineer myself, i had an idea of how this should work, but i also see a problem. Switching on synchronized works fine and i split the on/off button to both PSU breakout boards via a relay. However, since the PCI-E slot it self also supplies 12V to the GPU (25 or 75W depending on the slot), this is likely to cause problems with balancing the difference in 12V voltages on the GPU or motherboard, since these currents are huge and these are quite low resistance paths, even 100 to 200mV difference can cause huge balancing currents in places that are not meant for this.&lt;/p&gt; &lt;p&gt;On the other hand, other PSU's commonly have different 12V rails that can cause similar problems. So since i didn't measure a direct contact i got the feeling the solution/isolation to my problem is already designed in for these kind of PSU's.&lt;/p&gt; &lt;p&gt;Since i am surely not the first person to encounter this problem, i started looking for information about it. Most of the time, you end up on forums about crypto mining, and they often use a PCI-E extender via USB, which makes their situation completely different. I have read in several places that the PCI-E slot power is not directly connected to the 6 and/or 8-pin connectors and that this should be possible. I also verified this by measuring resistance between the 6/8 pins to the PCI-E connector, these are not directly connected. However, i think this is a huge risk and i would like to know from you, whether my information/assumptions are correct and how others have solved similar problems.&lt;/p&gt; &lt;p&gt;Since the PSU in this PC is &lt;strong&gt;not a standard ATX PSU&lt;/strong&gt;, replacing it with a high-power version with enough power/connections is not possible. Otherwise, i would have done so, because i don't want to risk my system to save a (tiny) bit of money. Also the standard multi PSU turn on cables are not compatible because the architecture is somewhat different, because this machine need so much (peak) power, they feed everything with 12V and convert down to the low voltages locally, to reduce the impedance/loses of the path. So most of the plugs from the PSU &amp;lt;&amp;gt; Motherboard are different.&lt;/p&gt; &lt;p&gt;I'm also thinking about using my old workstation (Dell T5600) and an old GPU as a first test. But my old GPU (Nvidia 1060) i need to drive my old dual DVI 2k monitor on my bench PC, so it would be shame to lose that system as well. Another option would be to remove the 12V pins on the PCI-E extender, but if that fails i've ruined another €100. If this test setup works i can check with a sensitive thermal camera (Flir E8) if no new hotspots appear.&lt;/p&gt; &lt;p&gt;Does anyone have information or experience with this? or have good ideas on how to test it more safely, i have all the measurement tools i might ever need so exotic suggestions/solutions/tests are also welcome. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rock_and_Rolf"&gt; /u/Rock_and_Rolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyb6lxutibbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3zl67</id>
    <title>Best memory strategy for long-form NSFW/Erotic RP: Raw context vs. Summarization vs. MemGPT?</title>
    <updated>2026-01-04T19:48:18+00:00</updated>
    <author>
      <name>/u/FollowingFresh6411</name>
      <uri>https://old.reddit.com/user/FollowingFresh6411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I’m experimenting with a dedicated LLM bot for writing long-form erotic stories and roleplay, and I’m hitting the classic context wall. I’m curious about what the community finds most effective for maintaining &amp;quot;the heat&amp;quot; and prose quality over long sessions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Which approach yields better results in your experience?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Full Raw Context (Sliding Window): Sending the entire recent history. It keeps the vibe and prose style consistent, but obviously, I lose the beginning of the story once the token limit is reached.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. LLM-based Summarization: Using a secondary (or the same) model to summarize previous events. My concern here is that summaries often feel too &amp;quot;clinical&amp;quot; or dry, which tends to kill the tension and descriptive nuances that are crucial for erotic writing.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Persistent Memory (MemGPT / Letta / Mem0): Using a memory engine to store facts and character traits. Does this actually work for keeping the narrative &amp;quot;flow,&amp;quot; or is it better suited only for static lore facts?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I’m currently looking at SillyTavern’s hybrid approach (Lorebooks + Summarize extension), but I’m wondering if anyone has found a way to use MemGPT-style memory without making the AI sound like a robot reciting a Wikipedia entry mid-scene.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What’s your setup for keeping the story consistent without losing the stylistic &amp;quot;soul&amp;quot; of the writing?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FollowingFresh6411"&gt; /u/FollowingFresh6411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3zl67/best_memory_strategy_for_longform_nsfwerotic_rp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3zl67/best_memory_strategy_for_longform_nsfwerotic_rp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3zl67/best_memory_strategy_for_longform_nsfwerotic_rp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T19:48:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3u89f</id>
    <title>HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)</title>
    <updated>2026-01-04T16:28:19+00:00</updated>
    <author>
      <name>/u/genielabs</name>
      <uri>https://old.reddit.com/user/genielabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"&gt; &lt;img alt="HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)" src="https://external-preview.redd.it/aXVic3Z5NTEwZGJnMX-wuN5UqDYSq_G1PvG8gD6oltW7ZDgAnY8CDzv70t9I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39b052b57497a280e1ac5403e6f63479f2615fd4" title="HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I’ve been working on HomeGenie 2.0, focusing on bringing &amp;quot;Agentic AI&amp;quot; to the edge.&lt;/p&gt; &lt;p&gt;Unlike standard dashboards, it integrates a local neural core (Lailama) that uses LLamaSharp to run GGUF models (Qwen 3, Llama 3.2, etc.) entirely offline.&lt;/p&gt; &lt;p&gt;Key technical bits: - &lt;strong&gt;Autonomous Reasoning:&lt;/strong&gt; It's not just a chatbot. It gets a real-time briefing of the home state (sensors, weather, energy) and decides which API commands to trigger. - &lt;strong&gt;Sub-5s Latency:&lt;/strong&gt; Optimized KV Cache management and history pruning to keep it fast on standard CPUs. - &lt;strong&gt;Programmable UI:&lt;/strong&gt; Built with zuix.js, allowing real-time widget editing directly in the browser. - &lt;strong&gt;Privacy First:&lt;/strong&gt; 100% cloud-independent.&lt;/p&gt; &lt;p&gt;I’m looking for feedback from the self-hosted community! Happy to answer any technical questions about the C# implementation or the agentic logic.&lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://homegenie.it"&gt;https://homegenie.it&lt;/a&gt; Source: &lt;a href="https://github.com/genielabs/HomeGenie"&gt;https://github.com/genielabs/HomeGenie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/genielabs"&gt; /u/genielabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m40jjx610dbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T16:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3ryd7</id>
    <title>Will the prices of GPUs go up even more?</title>
    <updated>2026-01-04T14:57:45+00:00</updated>
    <author>
      <name>/u/NotSoCleverAlternate</name>
      <uri>https://old.reddit.com/user/NotSoCleverAlternate</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hear discussions about this so I wanted to hear your guys take on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NotSoCleverAlternate"&gt; /u/NotSoCleverAlternate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T14:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q401ka</id>
    <title>Ratios of Active Parameters to Total Parameters on major MoE models</title>
    <updated>2026-01-04T20:05:05+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Params&lt;/th&gt; &lt;th align="left"&gt;Active Params&lt;/th&gt; &lt;th align="left"&gt;% Active&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.5 Air&lt;/td&gt; &lt;td align="left"&gt;106&lt;/td&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;11.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM 4.6 and 4.7&lt;/td&gt; &lt;td align="left"&gt;355&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 20B&lt;/td&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;3.6&lt;/td&gt; &lt;td align="left"&gt;17.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT OSS 120B&lt;/td&gt; &lt;td align="left"&gt;117&lt;/td&gt; &lt;td align="left"&gt;5.1&lt;/td&gt; &lt;td align="left"&gt;4.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 30B A3B&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;10%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 Next 80B A3B&lt;/td&gt; &lt;td align="left"&gt;80&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;3.8%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 235B A22B&lt;/td&gt; &lt;td align="left"&gt;235&lt;/td&gt; &lt;td align="left"&gt;22&lt;/td&gt; &lt;td align="left"&gt;9.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek 3.2&lt;/td&gt; &lt;td align="left"&gt;685&lt;/td&gt; &lt;td align="left"&gt;37&lt;/td&gt; &lt;td align="left"&gt;5.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MiniMax M2.1&lt;/td&gt; &lt;td align="left"&gt;230&lt;/td&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;4.3%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Kimi K2&lt;/td&gt; &lt;td align="left"&gt;1000&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;3.2%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And for fun, some oldies:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Params&lt;/th&gt; &lt;th align="left"&gt;Active Params&lt;/th&gt; &lt;th align="left"&gt;% Active&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral 8x7B&lt;/td&gt; &lt;td align="left"&gt;47&lt;/td&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mixtral 8x22B&lt;/td&gt; &lt;td align="left"&gt;141&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;27.7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Deepseek V2&lt;/td&gt; &lt;td align="left"&gt;236&lt;/td&gt; &lt;td align="left"&gt;21&lt;/td&gt; &lt;td align="left"&gt;8.9%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Grok 2&lt;/td&gt; &lt;td align="left"&gt;270&lt;/td&gt; &lt;td align="left"&gt;115&lt;/td&gt; &lt;td align="left"&gt;42.6% (record highest?)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;(Disclaimer: I'm just a casual user, and I know very little about the science of LLMs. My opinion is entirely based on osmosis and vibes.)&lt;/p&gt; &lt;p&gt;Total Parameters tends to represent the variety of knowledge available to the LLM, while Active Parameters is the intelligence. We've been trending towards lower percentage of Active params, probably because of the focus on benchmarks. Models have to know all sorts of trivia to pass all those multiple-choice tests, and know various programming languages to pass coding benchmarks.&lt;/p&gt; &lt;p&gt;I personally prefer high Active (sometimes preferring dense models for this reason), because I mainly use local LLMs for creative writing or one-off local tasks where I want it to read between the lines instead of me having to be extremely clear.&lt;/p&gt; &lt;p&gt;Fun thought: how would some popular models have changed with a different parameter count? What if GLM-4.5-Air was 5B active and GPT-OSS-120B was 12B? What if Qwen3 80B was 10B active?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q401ka/ratios_of_active_parameters_to_total_parameters/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:05:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3sfr1</id>
    <title>Propagate: Train thinking models using evolutionary strategies!</title>
    <updated>2026-01-04T15:17:42+00:00</updated>
    <author>
      <name>/u/Good-Assumption5582</name>
      <uri>https://old.reddit.com/user/Good-Assumption5582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"&gt; &lt;img alt="Propagate: Train thinking models using evolutionary strategies!" src="https://b.thumbs.redditmedia.com/jKCq9SIWUx4OFIfHcnbQ57A60zT3kWy5DTpg1qMi3XA.jpg" title="Propagate: Train thinking models using evolutionary strategies!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, this paper released:&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2509.24372"&gt;https://arxiv.org/abs/2509.24372&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And showed that with only 30 random gaussian perturbations, you can accurately approximate a gradient and outperform GRPO on RLVR tasks. They found zero overfitting, and training was significantly faster because you didn't have to perform any backward passes.&lt;/p&gt; &lt;p&gt;I thought that this was ridiculous, so I took their repo, cleaned up the codebase, and it replicates!&lt;/p&gt; &lt;p&gt;A couple weeks later, and I've implemented LoRA and pass@k training, with more features to come.&lt;/p&gt; &lt;p&gt;I hope you'll give ES a try!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Green0-0/propagate"&gt;https://github.com/Green0-0/propagate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Assumption5582"&gt; /u/Good-Assumption5582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q3sfr1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T15:17:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3p9oz</id>
    <title>MultiverseComputingCAI/HyperNova-60B · Hugging Face</title>
    <updated>2026-01-04T12:55:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"&gt; &lt;img alt="MultiverseComputingCAI/HyperNova-60B · Hugging Face" src="https://external-preview.redd.it/4y3rB_X0xi7PC07OhAWlbpJK6pkTGA-GxUmQGu5l2u4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9d4e40f5211cdcc28dd6fde8fa1da920bd51a8" title="MultiverseComputingCAI/HyperNova-60B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HyperNova 60B&lt;/strong&gt; base architecture is &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;code&gt;gpt-oss-120b&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;59B parameters with 4.8B active parameters&lt;/li&gt; &lt;li&gt;MXFP4 quantization&lt;/li&gt; &lt;li&gt;Configurable reasoning effort (low, medium, high)&lt;/li&gt; &lt;li&gt;GPU usage of less than 40GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/HyperNova-60B-GGUF"&gt;https://huggingface.co/mradermacher/HyperNova-60B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/HyperNova-60B-i1-GGUF"&gt;https://huggingface.co/mradermacher/HyperNova-60B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MultiverseComputingCAI/HyperNova-60B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3yug4</id>
    <title>FLUX.2-dev-Turbo is surprisingly good at image editing</title>
    <updated>2026-01-04T19:20:47+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3yug4/flux2devturbo_is_surprisingly_good_at_image/"&gt; &lt;img alt="FLUX.2-dev-Turbo is surprisingly good at image editing" src="https://external-preview.redd.it/dmN0aGFnMHN1ZGJnMWhQRIGbuygHvibzarhf8EVUxtFTiMplyPAlWNfH6-Zg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e2f3950c64b4732685a054c1ec22a163f1d83f4e" title="FLUX.2-dev-Turbo is surprisingly good at image editing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Getting excellent results, FAL did a great job with this FLUX.2 [dev] LoRA: &lt;a href="https://huggingface.co/fal/FLUX.2-dev-Turbo"&gt;https://huggingface.co/fal/FLUX.2-dev-Turbo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The speed and cost (&lt;strong&gt;only 8 inference steps!&lt;/strong&gt;) of it makes it very competitive with closed models. Perfect for daily creative workflow and local use.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/os8k650sudbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3yug4/flux2devturbo_is_surprisingly_good_at_image/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3yug4/flux2devturbo_is_surprisingly_good_at_image/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T19:20:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q42wtt</id>
    <title>Introducing Adaptive-P: A New Sampler for Creative Text Generation (llama.cpp PR)</title>
    <updated>2026-01-04T21:58:39+00:00</updated>
    <author>
      <name>/u/DragPretend7554</name>
      <uri>https://old.reddit.com/user/DragPretend7554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a sampling method we've been working on called Adaptive-P. Before I get into it, I should mention that due to a visual impairment, I used AI assistance in writing both the documentation and this post. I want to be upfront about that. The algorithm itself and the underlying idea are human created, however.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What is it?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Adaptive-P is a different approach to token sampling that tries to address models getting stuck in predictable patterns. When generating creative content, models often fall back on the same phrasing, sentence structures, and narrative beats. The model has more interesting options available, but standard sampling methods don't give you a way to encourage it toward those alternatives.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How does it work?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of uniformly scaling probabilities like temperature does, or making binary keep/discard decisions like truncation methods, Adaptive-P lets you specify a probability range you want to target. It applies a transformation that creates a preference curve centered on your target probability—tokens near the target get boosted, tokens far from it get suppressed.&lt;/p&gt; &lt;p&gt;The transformation uses unbounded negative logits for distant tokens rather than a floor value. This prevents probability from accumulating in the tail of the distribution, which is a problem that affects some other approaches to forced alternative selection.&lt;/p&gt; &lt;p&gt;The sampler maintains an exponential moving average of the original probabilities of selected tokens. It uses this history to compute an adjusted target at each step. If recent selections have been running above your configured target, the sampler compensates by aiming lower on the next step, and vice versa. This feedback loop keeps the average selection probability tracking toward your target over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chain breaking&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The adaptive mechanism is what breaks repetitive high-confidence chains. When the model keeps selecting dominant tokens, the history shifts upward, which pushes the calculated target downward, which makes alternatives more attractive. The sampler naturally resists getting stuck in a rut without requiring external repetition penalties.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's it good for?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is designed for creative work—fiction, roleplay, brainstorming. It's not meant for tasks where accuracy matters more than variety.&lt;/p&gt; &lt;p&gt;It pairs well with Min-P, which handles removing genuinely bad options while Adaptive-P handles selection among the remaining quality candidates. Adaptive-P needs to be the final sampler in the chain since it performs the actual token selection.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://github.com/MrJackSpade/adaptive-p-docs/blob/main/Documentation.md"&gt;https://github.com/MrJackSpade/adaptive-p-docs/blob/main/Documentation.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp PR: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17927"&gt;https://github.com/ggml-org/llama.cpp/pull/17927&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Discord discussion: &lt;a href="https://discord.com/channels/1238219753324281886/1447392417769721926"&gt;https://discord.com/channels/1238219753324281886/1447392417769721926&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any and all questions will likely be answered by the documentation, or the discord server.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DragPretend7554"&gt; /u/DragPretend7554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q42wtt/introducing_adaptivep_a_new_sampler_for_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T21:58:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1q41bw1</id>
    <title>GLM-Image model from Z.ai is coming</title>
    <updated>2026-01-04T20:54:04+00:00</updated>
    <author>
      <name>/u/Ravencloud007</name>
      <uri>https://old.reddit.com/user/Ravencloud007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt; &lt;img alt="GLM-Image model from Z.ai is coming" src="https://preview.redd.it/sm31vizebebg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae576450ba7112c06760ff8cddee6f5bdd7b672" title="GLM-Image model from Z.ai is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43100/files"&gt;https://github.com/huggingface/transformers/pull/43100/files&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ravencloud007"&gt; /u/Ravencloud007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sm31vizebebg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q41bw1/glmimage_model_from_zai_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T20:54:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
