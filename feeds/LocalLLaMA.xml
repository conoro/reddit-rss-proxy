<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-11T14:06:47+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ou7juw</id>
    <title>Pls tell me I shouldn't spend $3k on 5090 32gb vram desktop PC nor Strix Halo 128Gb</title>
    <updated>2025-11-11T11:56:58+00:00</updated>
    <author>
      <name>/u/IntroductionSouth513</name>
      <uri>https://old.reddit.com/user/IntroductionSouth513</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run local LLMs that are good for frequent coding tasks but I also want a powerful gaming machine.. but both of these are good to haves.. help!! &lt;/p&gt; &lt;p&gt;understand that it may be impulse purchase but I feel like fomo at this time &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntroductionSouth513"&gt; /u/IntroductionSouth513 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou7juw/pls_tell_me_i_shouldnt_spend_3k_on_5090_32gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou7juw/pls_tell_me_i_shouldnt_spend_3k_on_5090_32gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou7juw/pls_tell_me_i_shouldnt_spend_3k_on_5090_32gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T11:56:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1otpql6</id>
    <title>LLM-driven puzzle sandbox: anything you try becomes an action (Cosmic Egg)</title>
    <updated>2025-11-10T20:56:23+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otpql6/llmdriven_puzzle_sandbox_anything_you_try_becomes/"&gt; &lt;img alt="LLM-driven puzzle sandbox: anything you try becomes an action (Cosmic Egg)" src="https://external-preview.redd.it/dHZxczAzbTN0aDBnMTBAWoGHzmzPlCXmWH6RtU6SjIImLDmcCL43zhjlQgdI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb4fbdff352a105ed4a910befbf1d1449947eb83" title="LLM-driven puzzle sandbox: anything you try becomes an action (Cosmic Egg)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre using LLMs to generate actions in our upcoming puzzle game Cosmic Egg‚Äîso ‚Äúanything you can think of‚Äù becomes a validated, in-world interaction.&lt;/p&gt; &lt;p&gt;The system works with local LLMs + smart caching + a bit of game-dev smoke &amp;amp; mirrors‚Äîwhile keeping the game deterministic so everyone shares a common action pool and outcomes are reproducible.&lt;/p&gt; &lt;p&gt;Still lots to do, right now we‚Äôre improving sprite generation and adding player inventory &amp;amp; items.&lt;/p&gt; &lt;p&gt;Feedback very welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6i40e2m3th0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otpql6/llmdriven_puzzle_sandbox_anything_you_try_becomes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otpql6/llmdriven_puzzle_sandbox_anything_you_try_becomes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T20:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1otmamz</id>
    <title>When does RTX 6000 Pro make sense over a 5090?</title>
    <updated>2025-11-10T18:49:40+00:00</updated>
    <author>
      <name>/u/Herald_Of_Rivia</name>
      <uri>https://old.reddit.com/user/Herald_Of_Rivia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;pre&gt;&lt;code&gt;Hey all‚Äîtrying to sanity-check an upgrade. Current GPU: RTX 5090 Use cases: training mid-size LLMs, Stable Diffusion/ComfyUI, inferencing GPT-OSS-120B / GLM 4.5 Air Rig: 9950X3D / 96GB DDR5 / 1500W Corsair H1500i ‚Ä¢ OS: Win11 / Ubuntu 24.04 I‚Äôm eyeing the RTX 6000 Pro (Blackwell) mainly for: * More VRAM/ECC * Potential tensor/FP improvements for AI workloads Questions for folks who‚Äôve used the 6000 Pro vs the RXT 5090: * In real projects, what speed/throughput gains did you see for general AI workload? * Did ECC + pro drivers measurably reduce crashes/corruption vs 5090? * Any gotchas (thermals, power, coil whine, chassis fit, Linux/Windows quirks, NVLink/virtualization)? * If you switched back, why? If my workloads are mainly for LLM inference / small training and SD, is the upgrade worth it, or is 5090 still the best value? Benchmarks and anecdotes welcome! Thanks. &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Herald_Of_Rivia"&gt; /u/Herald_Of_Rivia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otmamz/when_does_rtx_6000_pro_make_sense_over_a_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otmamz/when_does_rtx_6000_pro_make_sense_over_a_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otmamz/when_does_rtx_6000_pro_make_sense_over_a_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T18:49:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwcg0</id>
    <title>Hello I‚Äôm planning to open-source my Sesame alternative. It‚Äôs kinda rough, but not too bad!</title>
    <updated>2025-11-11T01:26:50+00:00</updated>
    <author>
      <name>/u/Danny-1257</name>
      <uri>https://old.reddit.com/user/Danny-1257</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"&gt; &lt;img alt="Hello I‚Äôm planning to open-source my Sesame alternative. It‚Äôs kinda rough, but not too bad!" src="https://external-preview.redd.it/dNMvETR3TAgky9xFPLP6B7g5tW9HVTS23HKHMQaioOs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17e8eafbceac1243a6cb9d4832551a0bb15664c7" title="Hello I‚Äôm planning to open-source my Sesame alternative. It‚Äôs kinda rough, but not too bad!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1otwcg0/video/bzrf0ety5j0g1/player"&gt;https://reddit.com/link/1otwcg0/video/bzrf0ety5j0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey guys,&lt;/p&gt; &lt;p&gt;I wanted to share a project I‚Äôve been working on. I‚Äôm a founder currently building a new product, but until last month I was making a conversational AI. After pivoting, I thought I should share my codes.&lt;/p&gt; &lt;p&gt;The project is a voice AI that can have real-time conversations. The client side runs on the web, and the backend runs models in the cloud with gpu.&lt;/p&gt; &lt;p&gt;In detail : for STT, I used whisper-large-v3-turbo, and for TTS, I modified chatterbox for real-time streaming. LLM is gpt api or gpt-oss-20b by ollama.&lt;/p&gt; &lt;p&gt;One advantage of local llm is that all data can remain local on your machine. In terms of speed and performance, I also recommend using the api. and the pricing is not expensive anymore. (costs $0.1 for 30 minutes? I guess)&lt;/p&gt; &lt;p&gt;In numbers: TTFT is around 1000 ms, and even with the llm api cost included, it‚Äôs roughly $0.50 per hour on a runpod A40 instance.&lt;/p&gt; &lt;p&gt;There are a few small details I built to make conversations feel more natural (though they might not be obvious in the demo video):&lt;/p&gt; &lt;ol&gt; &lt;li&gt;When the user is silent, it occasionally generates small self-talk.&lt;/li&gt; &lt;li&gt;The llm is always prompted to start with a pre-set ‚Äúfirst word,‚Äù and that word‚Äôs audio is pre-generated to reduce TTFT.&lt;/li&gt; &lt;li&gt;It can insert short silences mid sentence for more natural pacing.&lt;/li&gt; &lt;li&gt;You can interrupt mid-speech, and only what‚Äôs spoken before interruption gets logged in the conversation history.&lt;/li&gt; &lt;li&gt;Thanks to multilingual Chatterbox, it can talk in any language and voice (English works best so far).&lt;/li&gt; &lt;li&gt;Audio is encoded and decoded with Opus.&lt;/li&gt; &lt;li&gt;Smart turn detection.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;This is the repo! It includes both client and server codes. &lt;a href="https://github.com/thxxx/harper"&gt;https://github.com/thxxx/harper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear what the community thinks. what do you think matters most for truly natural voice conversations?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Danny-1257"&gt; /u/Danny-1257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwcg0/hello_im_planning_to_opensource_my_sesame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:26:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou92ga</id>
    <title>Best Opensource OCR Models Support Arabic + English</title>
    <updated>2025-11-11T13:11:24+00:00</updated>
    <author>
      <name>/u/Ai_Peep</name>
      <uri>https://old.reddit.com/user/Ai_Peep</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to find a good open source OCR solution that works well with Arabic and English.Most of my documents are receipts, contracts, and invoices&lt;/p&gt; &lt;p&gt;If anyone has experience with Arabic OCR. could you pls let me know which model you have tried?&lt;/p&gt; &lt;p&gt;Thanks in advance&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ai_Peep"&gt; /u/Ai_Peep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou92ga/best_opensource_ocr_models_support_arabic_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou92ga/best_opensource_ocr_models_support_arabic_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou92ga/best_opensource_ocr_models_support_arabic_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T13:11:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ot95gj</id>
    <title>Qwen3-VL's perceptiveness is incredible.</title>
    <updated>2025-11-10T09:12:28+00:00</updated>
    <author>
      <name>/u/Trypocopris</name>
      <uri>https://old.reddit.com/user/Trypocopris</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.imgur.com/liqVUJd.jpeg"&gt;I took a 4k image and scattered around 6 medium-length words.&lt;/a&gt; &lt;/p&gt; &lt;p&gt;With &lt;code&gt;Qwen3-VL-8B-Instruct-GGUF&lt;/code&gt; and a temperature of &lt;code&gt;0&lt;/code&gt;, an image token count of &lt;code&gt;2300&lt;/code&gt; (seems to be the sweet spot), and the prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Provide transcriptions and bounding boxes for the words in the image. Use JSON format.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is the output:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[ {&amp;quot;bbox_2d&amp;quot;: [160, 867, 181, 879], &amp;quot;text_content&amp;quot;: &amp;quot;steam&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [146, 515, 168, 527], &amp;quot;text_content&amp;quot;: &amp;quot;queen&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [565, 731, 589, 743], &amp;quot;text_content&amp;quot;: &amp;quot;satisfied&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [760, 615, 784, 627], &amp;quot;text_content&amp;quot;: &amp;quot;feather&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [335, 368, 364, 379], &amp;quot;text_content&amp;quot;: &amp;quot;mention&amp;quot;}, {&amp;quot;bbox_2d&amp;quot;: [515, 381, 538, 392], &amp;quot;text_content&amp;quot;: &amp;quot;cabinet&amp;quot;} ]&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Flawless. No notes. &lt;a href="https://i.imgur.com/5bejqK9.jpeg"&gt;It even got the bounding boxes correct.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How do other models compare?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gemini 2.5 pro: Hallucinates an answer.&lt;/li&gt; &lt;li&gt;Claude Opus 4: Correctly identifies 3/6 words.&lt;/li&gt; &lt;li&gt;ChatGPT 5: After 5 minutes (!!) of thinking, it finds all 6 words. The bounding boxes are wrong.&lt;/li&gt; &lt;li&gt;DeepSeekOCR: Produces garbage (possible PEBCAK)&lt;/li&gt; &lt;li&gt;PaddleOCR-VL-0.9B: Finds 3 words, hallucinates 2. Doesn't output bounding boxes.&lt;/li&gt; &lt;li&gt;GLM-4.5V: Also perfect results.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Very impressive that such as small model can get such good results, especially considering it's not tuned for OCR.&lt;/p&gt; &lt;p&gt;edit:&lt;/p&gt; &lt;p&gt;&lt;a href="https://gist.github.com/vapetrov/f5597628e77f4238ce25bd9a63e14af1"&gt;Here's the script I used to run it.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://files.catbox.moe/byex4n.jpg"&gt;The exact image I used.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-8B-Instruct-GGUF"&gt;The model.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trypocopris"&gt; /u/Trypocopris &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ot95gj/qwen3vls_perceptiveness_is_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T09:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1otu6ez</id>
    <title>Hi reddit, I rebuilt Karpathy's Nanochat in pure Rust [nanochat-rs]</title>
    <updated>2025-11-10T23:51:18+00:00</updated>
    <author>
      <name>/u/Exciting-Camera3226</name>
      <uri>https://old.reddit.com/user/Exciting-Camera3226</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The repo is at: &lt;a href="https://github.com/AntigmaLabs/nanochat-rs"&gt;https://github.com/AntigmaLabs/nanochat-rs&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The goal to provide the community with a reference implementation in a different language and possibly a clean nice little hackable cognitive core that is easier to understand and deploy(without the python weak types and heavy pytorch dependencies) &lt;/p&gt; &lt;p&gt;Main features &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Native rust&lt;/li&gt; &lt;li&gt;Integration with HuggingFace&lt;/li&gt; &lt;li&gt;Centralized model loader resilient to tensor name changes&lt;/li&gt; &lt;li&gt;Minimal surface area to keep cognitive load low (not product-grade)&lt;/li&gt; &lt;li&gt;Compatible with tiktoken &lt;code&gt;.pkl&lt;/code&gt; tokenizer configs&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exciting-Camera3226"&gt; /u/Exciting-Camera3226 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otu6ez/hi_reddit_i_rebuilt_karpathys_nanochat_in_pure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otu6ez/hi_reddit_i_rebuilt_karpathys_nanochat_in_pure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otu6ez/hi_reddit_i_rebuilt_karpathys_nanochat_in_pure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T23:51:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1otihl1</id>
    <title>Open-dLLM: Open Diffusion Large Language Models</title>
    <updated>2025-11-10T16:33:06+00:00</updated>
    <author>
      <name>/u/pengzhangzhi</name>
      <uri>https://old.reddit.com/user/pengzhangzhi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"&gt; &lt;img alt="Open-dLLM: Open Diffusion Large Language Models" src="https://external-preview.redd.it/eHlpNXJmc3BpZzBnMbC2Q-rs9CfDNgw85akHP4ZCgTS81bEyqZb3k8CkqU2r.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fedc17befcb0ad76c15f3434bb58981727894c5" title="Open-dLLM: Open Diffusion Large Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the most open release of a diffusion-based large language model to date ‚Äî&lt;br /&gt; including &lt;strong&gt;pretraining, evaluation, inference, and checkpoints&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/pengzhangzhi/Open-dLLM"&gt;https://github.com/pengzhangzhi/Open-dLLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a"&gt;https://oval-shell-31c.notion.site/Open-dLLM-Open-Diffusion-Large-Language-Model-25e03bf6136480b7a4ebe3d53be9f68a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pengzhangzhi"&gt; /u/pengzhangzhi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qb62efspig0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otihl1/opendllm_open_diffusion_large_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T16:33:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oty5a9</id>
    <title>Realtime video analysis with Moondream</title>
    <updated>2025-11-11T02:50:19+00:00</updated>
    <author>
      <name>/u/radiiquark</name>
      <uri>https://old.reddit.com/user/radiiquark</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oty5a9/realtime_video_analysis_with_moondream/"&gt; &lt;img alt="Realtime video analysis with Moondream" src="https://external-preview.redd.it/ZGZqb20yZHBrajBnMRQH5Ip-LXWUY-NVe752F9-1VFqZvu8plOUVm68qhzC0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f2b9159f6d23bd4df70a756251c6b0a9849903" title="Realtime video analysis with Moondream" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Live demo (no login required): &lt;a href="https://moondream.ai/solutions/analyze-live-video"&gt;https://moondream.ai/solutions/analyze-live-video&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/m87-labs/Analyze-Live-Video-Solution"&gt;https://github.com/m87-labs/Analyze-Live-Video-Solution&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/radiiquark"&gt; /u/radiiquark &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/norsa3dpkj0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oty5a9/realtime_video_analysis_with_moondream/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oty5a9/realtime_video_analysis_with_moondream/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:50:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou787r</id>
    <title>Kani TTS Vie ‚Äî Fast &amp; Natural Vietnamese Text-to-Speech üòª</title>
    <updated>2025-11-11T11:38:43+00:00</updated>
    <author>
      <name>/u/DrCrab97</name>
      <uri>https://old.reddit.com/user/DrCrab97</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou787r/kani_tts_vie_fast_natural_vietnamese_texttospeech/"&gt; &lt;img alt="Kani TTS Vie ‚Äî Fast &amp;amp; Natural Vietnamese Text-to-Speech üòª" src="https://external-preview.redd.it/n6cwpUf5ye-oo9T-ubvOmKJ1rcHOcczkg6rjd9sasNs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=056408ba1f5131625f673ff5226c4060dcbee4f2" title="Kani TTS Vie ‚Äî Fast &amp;amp; Natural Vietnamese Text-to-Speech üòª" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ou787r/video/ri61g9qx6m0g1/player"&gt;https://reddit.com/link/1ou787r/video/ri61g9qx6m0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We just finished fine-tuning Kani TTS Vie, a high-quality Vietnamese Text-to-Speech model based on Kani-370M.&lt;/p&gt; &lt;p&gt;This release focuses on speed, clarity, and natural prosody ‚Äî aiming to be one of the fastest and most expressive Vietnamese TTS models available right now.&lt;/p&gt; &lt;p&gt;If you're working with voice apps, narration systems, chatbots, VTubers, or dubbing, feel free to try it out!&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/pnnbao-ump/kani-tts-370m-vie"&gt;https://huggingface.co/pnnbao-ump/kani-tts-370m-vie&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source Code: &lt;a href="https://github.com/pnnbao97/Kani-TTS-VieDemo"&gt;https://github.com/pnnbao97/Kani-TTS-VieDemo&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try demo: &lt;a href="https://huggingface.co/spaces/pnnbao-ump/Kani-TTS-Vie"&gt;https://huggingface.co/spaces/pnnbao-ump/Kani-TTS-Vie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrCrab97"&gt; /u/DrCrab97 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou787r/kani_tts_vie_fast_natural_vietnamese_texttospeech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou787r/kani_tts_vie_fast_natural_vietnamese_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou787r/kani_tts_vie_fast_natural_vietnamese_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T11:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1otnj2k</id>
    <title>Are any of you using local llms for "real" work?</title>
    <updated>2025-11-10T19:34:39+00:00</updated>
    <author>
      <name>/u/hmsenterprise</name>
      <uri>https://old.reddit.com/user/hmsenterprise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am having fun personally tinkering with local models and workflows and such, but sometimes it feels like we're all still stuck in the &amp;quot;fun experimentation&amp;quot; phase with local LLMs and not actually producing any &amp;quot;production grade&amp;quot; outputs or using it in real workflows. &lt;/p&gt; &lt;p&gt;Idk if it's just the gap between what &amp;quot;personal&amp;quot; LLM-capable rigs can handle vs the compute needs of current best-in-class models or what.&lt;/p&gt; &lt;p&gt;Am I wrong here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hmsenterprise"&gt; /u/hmsenterprise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otnj2k/are_any_of_you_using_local_llms_for_real_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T19:34:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1otl8q8</id>
    <title>Omnilingual ASR: Advancing Automatic Speech Recognition for 1,600+ Languages</title>
    <updated>2025-11-10T18:12:13+00:00</updated>
    <author>
      <name>/u/jean-</name>
      <uri>https://old.reddit.com/user/jean-</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jean-"&gt; /u/jean- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otl8q8/omnilingual_asr_advancing_automatic_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otl8q8/omnilingual_asr_advancing_automatic_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T18:12:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ottmjb</id>
    <title>Meta drops new ASR models (up to 7B)</title>
    <updated>2025-11-10T23:28:10+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just released a new kind of ASR models that are particularly useful to transcribe languages for which little training data is available.&lt;/p&gt; &lt;p&gt;Most interestingly, they seem to have implemented something like audio context, where you can provide some audio and the correct transcriptions and use that to improve ASR without needing a full fine-tune. It appears that the audio needed for this is very much doable without large scale transcription efforts you would normally have to do to run a fine-tune.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/facebookresearch/omnilingual-asr"&gt;https://github.com/facebookresearch/omnilingual-asr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ottmjb/meta_drops_new_asr_models_up_to_7b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ottmjb/meta_drops_new_asr_models_up_to_7b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ottmjb/meta_drops_new_asr_models_up_to_7b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T23:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou4qvh</id>
    <title>RAG Paper 25.11.11</title>
    <updated>2025-11-11T09:06:14+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07328v1"&gt;Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.07262v1"&gt;AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06973v1"&gt;Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06738v1"&gt;Rethinking Retrieval-Augmented Generation for Medicine: A Large-Scale, Systematic Expert Evaluation and Practical Insights&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06668v1"&gt;When Evidence Contradicts: Toward Safer Retrieval-Augmented Generation in Healthcare&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="http://arxiv.org/abs/2511.06582v1"&gt;TabRAG: Tabular Document Retrieval via Structured Language Representations&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4qvh/rag_paper_251111/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4qvh/rag_paper_251111/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4qvh/rag_paper_251111/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T09:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou8t7z</id>
    <title>Kimi K2 Thinking is a Better Agentic AI than I thought</title>
    <updated>2025-11-11T13:00:09+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1ou8t7z/video/9dtnlbhhlm0g1/player"&gt;https://reddit.com/link/1ou8t7z/video/9dtnlbhhlm0g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;just ran a quick eval on a deep agent built for customer support. It‚Äòs on par with GPT-5 in agentic capabilities.&lt;br /&gt; It's a bigger deal than I thought!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8t7z/kimi_k2_thinking_is_a_better_agentic_ai_than_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T13:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1otx50l</id>
    <title>Is open-webui vibe coded? Why else is the documentation littered with emoji?</title>
    <updated>2025-11-11T02:03:13+00:00</updated>
    <author>
      <name>/u/ksoops</name>
      <uri>https://old.reddit.com/user/ksoops</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's like every other 5 words: an emoji.&lt;/p&gt; &lt;p&gt;God damn, the future is bleak&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ksoops"&gt; /u/ksoops &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otx50l/is_openwebui_vibe_coded_why_else_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otx50l/is_openwebui_vibe_coded_why_else_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otx50l/is_openwebui_vibe_coded_why_else_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:03:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1otwek3</id>
    <title>Full Replication of Google's Nested Learning Paper in PyTorch ‚Äì code now live</title>
    <updated>2025-11-11T01:29:37+00:00</updated>
    <author>
      <name>/u/complains_constantly</name>
      <uri>https://old.reddit.com/user/complains_constantly</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some of you may have seen Google Research‚Äôs &lt;a href="https://research.google/blog/introducing-nested-learning-a-new-ml-paradigm-for-continual-learning/"&gt;&lt;strong&gt;Nested Learning paper&lt;/strong&gt;&lt;/a&gt;. They introduced HOPE, a self-modifying TITAN variant with a Continuum Memory System (multi-frequency FFN chain) + deep optimizer stack. They published the research but no code (like always), so I rebuilt the architecture and infra in PyTorch over the weekend.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/kmccleary3301/nested_learning"&gt;https://github.com/kmccleary3301/nested_learning&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Highlights&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Level clock + CMS implementation (update-period gating, associative-memory optimizers).&lt;/li&gt; &lt;li&gt;HOPE block w/ attention, TITAN memory, self-modifier pathway.&lt;/li&gt; &lt;li&gt;Hydra configs for pilot/mid/target scales, uv-managed env, Deepspeed/FSDP launchers.&lt;/li&gt; &lt;li&gt;Data pipeline: filtered RefinedWeb + supplements (C4, RedPajama, code) with tokenizer/sharding scripts.&lt;/li&gt; &lt;li&gt;Evaluation: zero-shot harness covering PIQA, HellaSwag, WinoGrande, ARC-E/C, BoolQ, SIQA, CommonsenseQA, OpenBookQA + NIAH long-context script.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What I need help with:&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Running larger training configs (760M+, 4‚Äì8k context) and reporting W&amp;amp;B benchmarks.&lt;/li&gt; &lt;li&gt;Stress-testing CMS/self-modifier stability + alternative attention backbones.&lt;/li&gt; &lt;li&gt;Continual-learning evaluation (streaming domains) &amp;amp; regression tests.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you try it, please file issues/PRs‚Äîespecially around stability tricks, data pipelines, or eval scripts. Would love to see how it stacks up against these Qwen, DeepSeek, Minimax, and Kimi architectures.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/complains_constantly"&gt; /u/complains_constantly &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otwek3/full_replication_of_googles_nested_learning_paper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T01:29:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1otscki</id>
    <title>Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it‚Äôs open source.</title>
    <updated>2025-11-10T22:37:40+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"&gt; &lt;img alt="Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it‚Äôs open source." src="https://external-preview.redd.it/ARR7y9mlLeCC9oWmE5UREkOw8RADA8XOccGD021Q5lw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4be25bea8245c672919ac843febfe66e1da8de0" title="Reflection AI reached human-level performance (85%) on ARC-AGI v1 for under $10k and within 12 hours. You can run this code yourself, it‚Äôs open source." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/jerber/arc-lang-public"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otscki/reflection_ai_reached_humanlevel_performance_85/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T22:37:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou4ubn</id>
    <title>Building LLM inference from scratch - clean, minimal and (sort of) fast</title>
    <updated>2025-11-11T09:12:29+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4ubn/building_llm_inference_from_scratch_clean_minimal/"&gt; &lt;img alt="Building LLM inference from scratch - clean, minimal and (sort of) fast" src="https://preview.redd.it/sozysc8wgl0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5fd1ce0ff5b4fb76f2aa3daa6eb7e75a6ad13138" title="Building LLM inference from scratch - clean, minimal and (sort of) fast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote my own LLM inference script for gpt-2 models from scratch following first principles with the motto of &lt;strong&gt;learning by building&lt;/strong&gt;. I built it incrementally starting from a very naive greedy decoding-based inference all the way to latency optimized (kv-cache/speculative decoding) inference using pytorch.&lt;/p&gt; &lt;p&gt;My implementation includes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference &amp;amp; Sampling:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;greedy decoding, EOS handling, context window management using sliding window&lt;/li&gt; &lt;li&gt;temperature scaling, multinomial sampling&lt;/li&gt; &lt;li&gt;top-k and top-p (nucleus) sampling&lt;/li&gt; &lt;li&gt;presence, frequency, and repetition penalties controls&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Latency Optimizations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;fp16/bf16 optimized inference&lt;/li&gt; &lt;li&gt;kv-cache (dynamic -&amp;gt; static + overflow fix) integration&lt;/li&gt; &lt;li&gt;variable-length batching with right-padding (allows for samples with different lengths)&lt;/li&gt; &lt;li&gt;draft-verify speculative decoding based on the &lt;a href="https://arxiv.org/abs/2302.01318"&gt;DeepMind paper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also benchmarked my kv-cache and speculative decoding implementations on GPT-2 models to see what kind of speedups are achievable using my implementations.&lt;/p&gt; &lt;p&gt;Here are the best speedups I was able to get:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;config:&lt;/strong&gt; RTX 4090, cuda 12.8, torch 2.9.0&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Optimization&lt;/th&gt; &lt;th align="left"&gt;Best Speedup (float32)&lt;/th&gt; &lt;th align="left"&gt;Best Speedup (float16)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;kv-cache&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2.76√ó&lt;/strong&gt; (gpt2-large, 800 tokens)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.48√ó&lt;/strong&gt; (gpt2-xl, 800 tokens)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;speculative decoding&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.63√ó&lt;/strong&gt; (draft: gpt2 -&amp;gt; target: gpt2-xl, gamma=5)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1.31√ó&lt;/strong&gt; (draft: gpt2 -&amp;gt; target: gpt2-xl, gamma=3)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The speedups are quite encouraging given the relatively small model sizes and my basic implementations without fancy tricks. :)&lt;/p&gt; &lt;p&gt;Like always, I've documented everything from the code, implementations and notes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/garg-aayush/building-from-scratch/tree/main/llm-inference"&gt;https://github.com/garg-aayush/building-from-scratch/tree/main/llm-inference&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Readme and benchmarks:&lt;/strong&gt; &lt;a href="https://github.com/garg-aayush/building-from-scratch/blob/main/llm-inference/Readme.md"&gt;https://github.com/garg-aayush/building-from-scratch/blob/main/llm-inference/Readme.md&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Commit-by-commit development&lt;/strong&gt;: Each implementation and optimization is a separate commit for easy understanding&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sozysc8wgl0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4ubn/building_llm_inference_from_scratch_clean_minimal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou4ubn/building_llm_inference_from_scratch_clean_minimal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T09:12:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou8b89</id>
    <title>Why is MiniMax M2 a Full Attention model?</title>
    <updated>2025-11-11T12:36:28+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The CEO of MiniMax addresses frequent community questions about why MiniMax M2 sticks with Full Attention instead of adopting more efficient alternatives like Linear or Sparse Attention. After many repeated private explanations, they decided to publicly share the reasoning and lessons behind this decision.&lt;/p&gt; &lt;h1&gt;Theory vs. Reality: The Efficient Attention Dilemma&lt;/h1&gt; &lt;p&gt;While the benefits of Linear/Sparse Attention are widely discussed, real-world implementation in large-scale, industrial LLM systems is much more complex. Full Attention still holds practical advantages across various scenarios (code/math, agents, multimodal tasks, long chain-of-thought, RL, low-precision compute, speculative decoding, etc.). To justify switching to efficient attention, many technical and evaluation challenges need to be overcome.&lt;/p&gt; &lt;h1&gt;Motivation: Why Even Try Efficient Attention?&lt;/h1&gt; &lt;p&gt;If compute were unlimited, most wouldn‚Äôt bother with Linear/Sparse Attention. Today, all efforts to develop efficient attention are fundamentally about saving compute, not necessarily about reducing token counts or hitting scaling limits. The goal is to build a model structure that delivers the best performance under fixed compute budgets for both training and inference.&lt;/p&gt; &lt;h1&gt;Core Problems: Effectiveness, Speed, and Price&lt;/h1&gt; &lt;p&gt;To make efficient attention viable in production, three key factors must be balanced: effectiveness (the model‚Äôs floor), speed (throughput), and cost. The biggest hurdle is not the structure itself, but the limitations of current evaluation methodologies. Comprehensive benchmarks and real-world metrics are both necessary and difficult to build.&lt;/p&gt; &lt;h1&gt;1. Limitations of Evaluation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Observability&lt;/strong&gt;: Benchmarks rapidly improve as models are optimized for them, but creating a truly comprehensive evaluation pipeline to expose real capability gaps remains unsolved‚Äîespecially for new attention mechanisms.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No Free Lunch&lt;/strong&gt;: Reducing attention complexity isn‚Äôt without trade-offs. Earlier, hybrid models combining Lightning Attention and Full Attention seemed to perform well on standard benchmarks, but larger models exposed clear weaknesses in complex, multi-step reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proxy Metrics and Scaling&lt;/strong&gt;: Proxy metrics can match or beat MHA on benchmarks after several iterations, but may not generalize as models scale up. Many issues only emerge at scale.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Observation Cost&lt;/strong&gt;: Early proxy indicators for complex tasks are hard to measure during pretraining, and as task complexity grows, so does the compute needed to reach statistical confidence, slowing iteration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Other Variables&lt;/strong&gt;: There are many confounding factors‚Äîmodel structure, data distribution, optimizer choice‚Äîall can sway outcomes, and conclusions may flip as the data pipeline evolves.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Infrastructure Gaps for Efficient Attention&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Training&lt;/strong&gt;: Linear/Sparse Attention often becomes memory-bound rather than compute-bound. Without deep IO optimization, GPU utilization suffers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Inference&lt;/strong&gt;: Delivering truly faster, cheaper inference is difficult. Theoretical memory/computation savings only kick in for long enough sequences (several thousand tokens), which is still short for modern LLMs. &lt;ul&gt; &lt;li&gt;Challenges include: &lt;ul&gt; &lt;li&gt;Low-precision state storage (more sensitive for linear attention)&lt;/li&gt; &lt;li&gt;Efficient prefix caching (critical for practical workloads)&lt;/li&gt; &lt;li&gt;Speculative decoding optimizations&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Fortunately, these are solvable, but require engineering effort.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Next Steps: What Needs to Happen&lt;/h1&gt; &lt;p&gt;Scaling remains a central theme. As context lengths increase faster than GPU compute, the payoff from efficient attention will become more pronounced. To prepare, the team needs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;More diverse and information-rich long-form data&lt;/li&gt; &lt;li&gt;Better evaluation systems and experimental paradigms for rapid iteration&lt;/li&gt; &lt;li&gt;Improved training/inference infrastructure to fully exploit available hardware&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Appendix: Lessons from Open-Source and Failed Experiments&lt;/h1&gt; &lt;p&gt;They briefly discusses the (now-removed) SWA inference code and why it didn‚Äôt make the cut‚Äîit simply didn‚Äôt work well enough. Hybrid approaches (mixing CPT and SWA, inter/intra-layer hybridization) were explored, but all exhibited significant performance drops with longer contexts, especially in agent scenarios. Analysis revealed entrenched attention patterns (like retrieval and induction heads) are established early and hard to adapt via hybridization, and probing to selectively retain full attention wasn‚Äôt practically successful. This issue isn‚Äôt related to ‚Äúattention sink.‚Äù Readers interested in this line of thinking are encouraged to analyze performance in models like GPT-OSS, CWM, and Gemma, especially for long-context tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8b89/why_is_minimax_m2_a_full_attention_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8b89/why_is_minimax_m2_a_full_attention_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou8b89/why_is_minimax_m2_a_full_attention_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T12:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1otxs37</id>
    <title>Our sub got a shout-out from the Corridor Crew</title>
    <updated>2025-11-11T02:33:11+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt; &lt;img alt="Our sub got a shout-out from the Corridor Crew" src="https://external-preview.redd.it/MG5qbWE3OXZoajBnMfJFc8SM8imSZJpbD6BkmsMZ2u1jbLaP-XMJEPc_yiXX.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=269b48a90a4a2de8bb79dd262a86c54e29a97ed9" title="Our sub got a shout-out from the Corridor Crew" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From their recent video &lt;a href="https://youtu.be/6hI9T4jnrSI?si=h7An0736C93hs7YO"&gt;AI Experts Debunk The Latest SLOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/10yfbe8vhj0g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otxs37/our_sub_got_a_shoutout_from_the_corridor_crew/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T02:33:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou14ry</id>
    <title>baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case..</title>
    <updated>2025-11-11T05:21:46+00:00</updated>
    <author>
      <name>/u/PaceZealousideal6091</name>
      <uri>https://old.reddit.com/user/PaceZealousideal6091</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"&gt; &lt;img alt="baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case.." src="https://external-preview.redd.it/81GI5f2SH41ji6Aiuro1sKkxz-x19lfHg7ZgCRL6MOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4217ff485db0b42df0be643ea3fe3e8636aa4480" title="baidu/ERNIE-4.5-VL-28B-A3B-Thinking released. Curious case.." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems Baidu has released the &amp;quot;thinking&amp;quot; variant if their vl model silently. The earlier model was supposedly hybrid, supporting both &amp;quot;thinking&amp;quot; and &amp;quot;non-thinking&amp;quot;. The model card says that they have introduced something called &amp;quot;thinking with images&amp;quot; without explaining what it is. They have one put a small hardly visible graph comparing it with gemini 2.5 pro and gpt-5 high in various benchmarks . If you squint your eye enough, then you'll see they claim using the graph that this model keeps up or beat them good in many of the benchmarks. Surely benchmaxxed. Its too good to believe. Has anyone tried it? The previous ernie versions have been decent. It might be worth testing it. Does anyone have any idea how is this &amp;quot;thinking&amp;quot; variant different?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceZealousideal6091"&gt; /u/PaceZealousideal6091 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-VL-28B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou14ry/baiduernie45vl28ba3bthinking_released_curious_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:21:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1otveug</id>
    <title>A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K</title>
    <updated>2025-11-11T00:44:49+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt; &lt;img alt="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" src="https://external-preview.redd.it/j6x6Pm9GXcBDejuI8fZ_JaGjEF5FKmyowYdHbKM_k34.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f2207a39f85b0be48a03566c4c904bcc528405b" title="A startup Olares is attempting to launch a small 3.5L MiniPC dedicated to local AI, with RTX 5090 Mobile (24GB VRAM) and 96GB of DDR5 RAM for $3K" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techpowerup.com/342779/olares-to-launch-a-personal-ai-device-bringing-cloud-level-performance-home"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1otveug/a_startup_olares_is_attempting_to_launch_a_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T00:44:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1j3e</id>
    <title>Seems like the new K2 benchmarks are not too representative of real-world performance</title>
    <updated>2025-11-11T05:45:03+00:00</updated>
    <author>
      <name>/u/cobalt1137</name>
      <uri>https://old.reddit.com/user/cobalt1137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"&gt; &lt;img alt="Seems like the new K2 benchmarks are not too representative of real-world performance" src="https://preview.redd.it/awzjyvo3gk0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7f2dd0a4c362653c960b794e0943c0b3784b17b" title="Seems like the new K2 benchmarks are not too representative of real-world performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cobalt1137"&gt; /u/cobalt1137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/awzjyvo3gk0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1j3e/seems_like_the_new_k2_benchmarks_are_not_too/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ou1emx</id>
    <title>We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp; coding benchmarks</title>
    <updated>2025-11-11T05:37:41+00:00</updated>
    <author>
      <name>/u/innocent2powerful</name>
      <uri>https://old.reddit.com/user/innocent2powerful</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"&gt; &lt;img alt="We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp;amp; coding benchmarks" src="https://preview.redd.it/fnpk5t7kbk0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c57d729e6fb3ff57f9b7d7ba1d9d6be31f27588" title="We put a lot of work into a 1.5B reasoning model ‚Äî now it beats bigger ones on math &amp;amp; coding benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;We put a lot of care into making sure the &lt;strong&gt;training data is fully decontaminated&lt;/strong&gt; ‚Äî every stage (SFT and RL) went through strict filtering to avoid any overlap with evaluation benchmarks.&lt;/li&gt; &lt;li&gt;It achieves state-of-the-art performance among small (&amp;lt;4B) models, both in competitive math and competitive coding tasks. Even &lt;strong&gt;surpass the DeepSeek R1 0120 in competitive math benchmarks&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;It‚Äôs not designed as a general chatbot&lt;/strong&gt; (though it can handle basic conversation and factual QA). Our main goal was to &lt;strong&gt;prove that small models can achieve strong reasoning&lt;/strong&gt; ability, and we‚Äôve put a lot of work and iteration into achieving that, starting from a base like Qwen2.5-Math-1.5B (which originally had weak math and almost no coding ability) to reach this point.&lt;/li&gt; &lt;li&gt;We‚Äôd love for the community to &lt;strong&gt;test it on your own competitive math/coding benchmarks&lt;/strong&gt; and share results or feedback here. Any insights will help us keep improving.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;HuggingFace Paper: &lt;a href="https://huggingface.co/papers/2511.06221"&gt;paper&lt;/a&gt;&lt;br /&gt; X Post: &lt;a href="https://x.com/WeiboLLM/status/1988109435902832896?s=20"&gt;X&lt;/a&gt;&lt;br /&gt; Model: &lt;a href="https://huggingface.co/WeiboAI/VibeThinker-1.5B"&gt;Download Model&lt;/a&gt; Ôºàset resp_len=40k, temp=0.6 / 1.0, top_p=0.95, top_k=-1 for better performance.Ôºâ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/innocent2powerful"&gt; /u/innocent2powerful &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fnpk5t7kbk0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ou1emx/we_put_a_lot_of_work_into_a_15b_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-11T05:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oth5pw</id>
    <title>AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model</title>
    <updated>2025-11-10T15:44:10+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt; &lt;img alt="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" src="https://b.thumbs.redditmedia.com/yz9_FpdLcHNiCkaH5fLEIoXS2f5u5twNBr7SQ9Go3AI.jpg" title="AMA With Moonshot AI, The Open-source Frontier Lab Behind Kimi K2 Thinking Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Moonshot AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="/u/ComfortableAsk4494"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/zxytim"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="/u/ppwwyyxx"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87"&gt;https://preview.redd.it/5yg0ncsn7g0g1.png?width=3525&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5318680204ef7502ad349aec148147d9e3398f87&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oth5pw/ama_with_moonshot_ai_the_opensource_frontier_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-10T15:44:10+00:00</published>
  </entry>
</feed>
