<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-28T15:07:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oi16jj</id>
    <title>vLLM MoE Benchmark Configs for Qwen3 Coder REAP 25B &amp; RTX Pro 6000</title>
    <updated>2025-10-28T05:23:46+00:00</updated>
    <author>
      <name>/u/j4ys0nj</name>
      <uri>https://old.reddit.com/user/j4ys0nj</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi16jj/vllm_moe_benchmark_configs_for_qwen3_coder_reap/"&gt; &lt;img alt="vLLM MoE Benchmark Configs for Qwen3 Coder REAP 25B &amp;amp; RTX Pro 6000" src="https://external-preview.redd.it/uKKImkX9nhrtp0LIvGXTQD2Xl65r6cYkFlt89RXy4k4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51a720fb2a0814150cb703882892cbe1a2524dcd" title="vLLM MoE Benchmark Configs for Qwen3 Coder REAP 25B &amp;amp; RTX Pro 6000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1oi16jj/video/53rpmw42fsxf1/player"&gt;https://reddit.com/link/1oi16jj/video/53rpmw42fsxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Took me a while to figure this out and I couldn't find the configs online anywhere, so I thought I'd share in case anyone else has been looking. If you see a message like this in your vLLM logs: &lt;code&gt;Using default MoE config. Performance might be sub-optimal!&lt;/code&gt; you'll want one of these configs. This combined with a few other params took Qwen3 Coder REAP 25b from often randomly taking 10+ minutes to complete a request, to being able to handle multiple requests at once (of around 25k tokens each in this example) and responding to all requests at the same time at a rate of around 45 tokens/sec. (see video)&lt;/p&gt; &lt;p&gt;For fused mixture of expert models vLLM needs a config that's specific to the &amp;quot;shape&amp;quot; of the MoE and the device. &lt;code&gt;E=&amp;lt;experts&amp;gt;,N=&amp;lt;moe_intermediate/2&amp;gt;,device_name=&amp;lt;GPU&amp;gt;.json&lt;/code&gt; like: &lt;code&gt;E=103,N=768,device_name=NVIDIA_RTX_PRO_6000_Blackwell_Server_Edition.json&lt;/code&gt;&lt;/p&gt; &lt;p&gt;vLLM has a bunch of common combos, but doesn't have one for Qwen3 coder or any Blackwell GPUs. And on top of that (at least in vLLM v0.10.1.1), the benchmark script to produce the configs runs way more combinations than are needed, so I modified the script to pair that down, and thus take less time, and also made it save the files incrementally incase thats helpful as the original script doesn't save them incrementally.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/MissionSquad/vllm-moe-configs"&gt;https://github.com/MissionSquad/vllm-moe-configs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/j4ys0nj"&gt; /u/j4ys0nj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi16jj/vllm_moe_benchmark_configs_for_qwen3_coder_reap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi16jj/vllm_moe_benchmark_configs_for_qwen3_coder_reap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi16jj/vllm_moe_benchmark_configs_for_qwen3_coder_reap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T05:23:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi9zj0</id>
    <title>Wanted to ask a question about models that can be used to convert my Figma designs into html + css</title>
    <updated>2025-10-28T13:53:33+00:00</updated>
    <author>
      <name>/u/TheWeebSamurai</name>
      <uri>https://old.reddit.com/user/TheWeebSamurai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So hey there, I'm a Backend developer and an GameDev student, I wanted to ask which mid-low end model can be used to convert my figma designs into html +css. I don't really want to write html + css (I want to save time) and since most of the &amp;quot;frontend coding is almost dead&amp;quot;(or so I think) , I wanted to ask this question! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheWeebSamurai"&gt; /u/TheWeebSamurai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9zj0/wanted_to_ask_a_question_about_models_that_can_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9zj0/wanted_to_ask_a_question_about_models_that_can_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9zj0/wanted_to_ask_a_question_about_models_that_can_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi5i90</id>
    <title>What live profiling features would actually help you train or fine-tune models more efficiently?</title>
    <updated>2025-10-28T10:16:01+00:00</updated>
    <author>
      <name>/u/traceml-ai</name>
      <uri>https://old.reddit.com/user/traceml-ai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on TraceML a lightweight profiler that shows memory and timing live during PyTorch training. &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/traceopt-ai/traceml"&gt;https://github.com/traceopt-ai/traceml&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My goal is not to replace Nsight or the PyTorch Profiler, but to make live observability lightweight and useful, something you can keep running every day without slowing training down.&lt;/p&gt; &lt;p&gt;I am exploring what to build next and would love to know what matters most to you (and whatâ€™s missing from current tools):&lt;/p&gt; &lt;p&gt;â€¢ Multi-GPU / multi-process view, see utilization, memory, and sync overheads across devices&lt;/p&gt; &lt;p&gt;â€¢ Throughput metrics, tokens/sec, batches/sec, or FLOPs efficiency â€¢ Gradient stability tracking, detect spikes, vanishing gradients, or divergence early&lt;/p&gt; &lt;p&gt;â€¢ Memory evolution curves, see how activation/grad memory grows over steps&lt;/p&gt; &lt;p&gt;â€¢ Energy or cost metrics, wattage, $ per run, or energy per token&lt;/p&gt; &lt;p&gt;â€¢ Simple alerts such as OOM risk or performance drop detection&lt;/p&gt; &lt;p&gt;The focus is to keep it lightweight and easy to use, no heavy trace dumps or configs, just real-time insights you can actually use mid-training.&lt;/p&gt; &lt;p&gt;What do you think would be most useful (or hardest to get today)? Are there any live metrics or signals you wish existed but can not get easily right now?&lt;/p&gt; &lt;p&gt;Any feedback or feature votes would really help shape where I take this next. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/traceml-ai"&gt; /u/traceml-ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5i90/what_live_profiling_features_would_actually_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5i90/what_live_profiling_features_would_actually_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi5i90/what_live_profiling_features_would_actually_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:16:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohpqvy</id>
    <title>Radeon R9700 Dual GPU First Look â€” AI/vLLM plus creative tests with Nuke &amp; the Adobe Suite</title>
    <updated>2025-10-27T20:34:58+00:00</updated>
    <author>
      <name>/u/atape_1</name>
      <uri>https://old.reddit.com/user/atape_1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt; &lt;img alt="Radeon R9700 Dual GPU First Look â€” AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" src="https://external-preview.redd.it/nujocFBKcx2R2jDqpB8QfazaXJG6_pPL0crrme1qkLM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=166ad90b700b767ba9a03c0fc586c47d3289ab0d" title="Radeon R9700 Dual GPU First Look â€” AI/vLLM plus creative tests with Nuke &amp;amp; the Adobe Suite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atape_1"&gt; /u/atape_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=efQPFhZmhAo&amp;amp;embeds_referring_euri=https%3A%2F%2Fwww.reddit.com%2F"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohpqvy/radeon_r9700_dual_gpu_first_look_aivllm_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:34:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohq5bc</id>
    <title>GLM-4.6 vs Minimax-M2</title>
    <updated>2025-10-27T20:50:13+00:00</updated>
    <author>
      <name>/u/baykarmehmet</name>
      <uri>https://old.reddit.com/user/baykarmehmet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I've been using the GLM Coding Plan and it works well&lt;/strong&gt; â€” not quite Sonnet 4.5 performance, but with clear prompts it gets the job done.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;However, everyone's hyping Minimax M2&lt;/strong&gt;, claiming it crushes every benchmark. The problem? I haven't seen any real-world coding examples or projects using it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Has anyone here actually used Minimax M2 for development work?&lt;/strong&gt; If so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How does it compare to other models in practice?&lt;/li&gt; &lt;li&gt;Is it worth switching to?&lt;/li&gt; &lt;li&gt;Any specific use cases where it excels or falls short?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear some hands-on experiences beyond the benchmark numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/baykarmehmet"&gt; /u/baykarmehmet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohq5bc/glm46_vs_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T20:50:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohlhdx</id>
    <title>Phoronix benchmarks single and dual AMD R9700 GPUs against a single NVIDIA RTX 6000 Ada GPU</title>
    <updated>2025-10-27T17:56:13+00:00</updated>
    <author>
      <name>/u/Brian-Puccio</name>
      <uri>https://old.reddit.com/user/Brian-Puccio</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brian-Puccio"&gt; /u/Brian-Puccio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohlhdx/phoronix_benchmarks_single_and_dual_amd_r9700/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T17:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oibaz2</id>
    <title>Waiting for an UnSloth GUFF for MiniMax-M2!</title>
    <updated>2025-10-28T14:45:00+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth has already put MiniMax-M2 on Hugging Face! That means a guff version could arrive very soon. In other words, we might not be far from truly accessible local use. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/MiniMax-M2"&gt;https://huggingface.co/unsloth/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oibuio</id>
    <title>Will the AMD Ryzenâ„¢ AI Max+ 395 --EVO-X2 AI Mini PC -- 128 GB Ram hold its value of around 1.8k in two years time?</title>
    <updated>2025-10-28T15:05:35+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I am looking into purchasing this Strix Halo. Do you guys think the value of this will significantly depreciate? Or remain relatively stable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibuio/will_the_amd_ryzen_ai_max_395_evox2_ai_mini_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibuio/will_the_amd_ryzen_ai_max_395_evox2_ai_mini_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oibuio/will_the_amd_ryzen_ai_max_395_evox2_ai_mini_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T15:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohh1l2</id>
    <title>86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent.</title>
    <updated>2025-10-27T15:12:09+00:00</updated>
    <author>
      <name>/u/Ok-Attention1022</name>
      <uri>https://old.reddit.com/user/Ok-Attention1022</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt; &lt;img alt="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." src="https://external-preview.redd.it/Mhekv1CVcCnqQ6OsfNa_xd5RwOhyacefoOODajDng28.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=513369b6dc41c3172d89ddffae9bf50cb41bb901" title="86% accuracy on SimpleQA with gpt-4.1-mini. Open-source deep research agent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built SGR Deep Research: a lightweight framework for structured reasoning agents using small LLMs&lt;/p&gt; &lt;p&gt;No LangChain/CrewAI bloat&lt;/p&gt; &lt;p&gt;~500 LOC core logic&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark&lt;/strong&gt;: 86.1% on SimpleQA (4,326 questions)&lt;/p&gt; &lt;p&gt;Model: gpt-4.1-mini&lt;br /&gt; Tavily Search: basic&lt;/p&gt; &lt;p&gt;Cost: $0.03 per query&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/akowocw57oxf1.png?width=2460&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d332497cfe0686bedb5b11f58bbb7e6de61f0a3"&gt;Performance Metrics on gpt-4.1-mini and Tavily basic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SGR understanding&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bocirpd67oxf1.png?width=1176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2c1adc29c14fc211311efbf31063e3d449ffcbd0"&gt;SGR Deep Research: open-source framework for building intelligent research agents using Schema-Guided Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Explicitly control reasoning flow instead of hoping model figures it out ReAct&amp;amp;PlanAct-style but with structured steps Running in production at telecom and banking right now&lt;/p&gt; &lt;p&gt;Testing local models next (Qwen, Llama) for $0 API costs&lt;br /&gt; Everything public: logs, configs, code GitHub MIT: &lt;a href="https://github.com/vamplabAI/sgr-deep-research"&gt;https://github.com/vamplabAI/sgr-deep-research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Attention1022"&gt; /u/Ok-Attention1022 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohh1l2/86_accuracy_on_simpleqa_with_gpt41mini_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T15:12:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi92ux</id>
    <title>What are some hyperparameters to tune for QwenVL models?</title>
    <updated>2025-10-28T13:16:29+00:00</updated>
    <author>
      <name>/u/Ok_Television_9000</name>
      <uri>https://old.reddit.com/user/Ok_Television_9000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If my input files are pdf, what are some hyperparameters i can play with. To come up with the best set of hyperparameters. Eg, dpi value, temperature etc. Would beam_search be good?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Television_9000"&gt; /u/Ok_Television_9000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi92ux/what_are_some_hyperparameters_to_tune_for_qwenvl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi92ux/what_are_some_hyperparameters_to_tune_for_qwenvl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi92ux/what_are_some_hyperparameters_to_tune_for_qwenvl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohzfdu</id>
    <title>VellumForge2 - A high performance, very configurable and really easy to use DPO dataset generation tool, create high quality datasets for completely free</title>
    <updated>2025-10-28T03:41:47+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally releasing my new dataset generation tool, and some Fantasy writing datasets to go with it (soon). &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/lemon07r/VellumForge2"&gt;https://github.com/lemon07r/VellumForge2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sample Dataset: &lt;a href="https://huggingface.co/collections/lemon07r/vellumforge2-datasets"&gt;https://huggingface.co/collections/lemon07r/vellumforge2-datasets&lt;/a&gt; (large datasets coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Functionality&lt;/strong&gt; (all you need for a tl;dr)&lt;/p&gt; &lt;p&gt;This tool creates DPO-style datasets using a main topic and LLMs to generate subtopics, prompts, and chosen/rejected response pairs through a hierarchical pipeline. What sets it apart is the optional LLM-as-a-judge rubric scoring system, inspired by how Kimi K2 was trained using rubric-based evaluation to generate higher quality writing samples. The output uses a flexible &amp;quot;one-to-many&amp;quot; hybrid schema that works seamlessly with DPOTrainer, RewardTrainer, and MORL training, no data transformation needed. You can also skip the judge entirely for DPO training or just use the prompt and chosen responses for SFT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Overview &amp;amp; Features&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My original python script that I was using for making datasets worked mostly fine, but I broke it, many many times trying to refactor it and add features to it. It did get to a good place at some point, with working async, rate limiting, etc, before I broke it again with some experimental stuff that turned out to not be a good idea even if it did work. Some good lessons learned here.&lt;/p&gt; &lt;p&gt;What I did learn, I used in my complete re-write of the tool. This time I wrote it in Go, and kept it very simple and easy to use. I also kept it very modular and highly configurable from the very start. This tool works with any OpenAI-compatible API including local servers like llama.cpp, kobold.cpp, LM studio, vLLM or Ollama. Handles rate limiting automatically, supports concurrent workers, and can upload directly to Hugging Face Hub in one command, which was implemented without needing any external tools/dependencies like the HF cli. Generation templates are fully customizable via TOML config, meaning you can make any type of dataset. The example configs come with a strong default template for fantasy writing to help give an idea of what a good template would look like. The documentation includes a thorough quick start guide, and examples. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset Generation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This thing works fast. Had a much bigger impact than I expected in dataset generation speed compared to the old tool. Even using the completely free (and unlimited) Nvidia NIM api with it's 40 RPM rate limit and slow 20-30 tps Kimi K2 0905 model, plus any small local model for rejected responses, you can create a very high quality (possibly only topped by using Sonnet 4.5) DPO datasets, with about 1000 rows of high quality data in under a few hours, for completely free. No expensive hardware or API provider required (which of course you can use with this tool too). The sample dataset I linked completed under these conditions in only a 36-minute run, which would have been only half as long without a judge. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohzfdu/vellumforge2_a_high_performance_very_configurable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T03:41:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi3w68</id>
    <title>Flex Attention vs Flash Attention 3</title>
    <updated>2025-10-28T08:27:54+00:00</updated>
    <author>
      <name>/u/Extra-Designer9333</name>
      <uri>https://old.reddit.com/user/Extra-Designer9333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm pretty new to accelerated framework APIs like FlexAttn from PyTorch team and FlashAttn from Tri Dao out of Princeton. Unsloth itself uses Flex Attn as I know and reports: &amp;quot;10x faster on a single GPU and up to 30x faster on multiple GPU systems compared to Flash Attention 2 (FA2).&amp;quot; However, FlashAttn 3 turns out to be 1.5-2x faster than FlashAttn 2. &lt;/p&gt; &lt;p&gt;I'm trying to decide which one to use for training my LLM whether it's FlexAttn (Unsloth) or FlashAttn 3. What's your personal suggestion and experience you had from these 2. Which one is more error prone, which turns out to be more memory heavy or computationally less expensive and etc.&lt;/p&gt; &lt;p&gt;Thank you all in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Designer9333"&gt; /u/Extra-Designer9333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi3w68/flex_attention_vs_flash_attention_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T08:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohihvo</id>
    <title>Another Banger from Inclusion AI: Ming-flash-omni-Preview</title>
    <updated>2025-10-27T16:06:14+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt; &lt;img alt="Another Banger from Inclusion AI: Ming-flash-omni-Preview" src="https://external-preview.redd.it/PFGMqHZG1FenJLDckxpcToXwao333pejl_fZNW4bWqk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=04ce1f6abd38ec8e123f358b2f5b41d3b28a30d6" title="Another Banger from Inclusion AI: Ming-flash-omni-Preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ming-flash-omni-Preview"&gt;https://huggingface.co/inclusionAI/Ming-flash-omni-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Based on &lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;Ling-Flash-2.0&lt;/a&gt; this model has 100b total parameters and 6b active ones and supports context aware asr, text to speech, image generation and editing, segmentation etc (well its an omni modal model so you know the drill). Since its fairly sparse it is very efficient and while I couldn't test it myself the benchmarks seem promising, and it also supports voice cloning (;&lt;/p&gt; &lt;p&gt;It says it can do dialect-aware ASR, though im not sure if that will only work with Chinese ðŸ¤”&lt;/p&gt; &lt;p&gt;Anyways, if im not mistaken this is the biggest open sourced omni modal model yet so thanks to the mad lads at inclusion ai!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556"&gt;https://preview.redd.it/qml9ai33goxf1.png?width=2972&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29df775ad390dc4e6cb4306e302540f231bdf556&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/oh86jahegoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player"&gt;https://reddit.com/link/1ohihvo/video/zbxb11vnhoxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohihvo/another_banger_from_inclusion_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T16:06:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohm80t</id>
    <title>Newegg has 32gb AMD r9700 for $1,300</title>
    <updated>2025-10-27T18:23:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu"&gt;https://videocardz.com/newz/amd-radeon-pro-ai-r9700-is-now-available-32gb-memory-and-full-navi-48-gpu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Phoronix did a poor job of benchmarking it. Would prefer benchmarking a 30gb model like qwen3 coder, but instead focuses on 8gb model: &lt;a href="https://www.phoronix.com/review/amd-radeon-ai-pro-r9700"&gt;https://www.phoronix.com/review/amd-radeon-ai-pro-r9700&lt;/a&gt; Doesn't bother to compare it to 4090/5090. This video does gaming benchmarks: &lt;a href="https://www.youtube.com/watch?v=x0YJ32Q0mNw"&gt;https://www.youtube.com/watch?v=x0YJ32Q0mNw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guessing 30 tokens per second (TPS) for qwen3 coder.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohm80t/newegg_has_32gb_amd_r9700_for_1300/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T18:23:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdl9q</id>
    <title>Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives</title>
    <updated>2025-10-27T12:53:12+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt; &lt;img alt="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" src="https://external-preview.redd.it/cDlpaWtncThobnhmMYyuPjWRTezxeRfqB3upVJ5ATISaueUIVVjdl6ikWaxE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49803f057b43fcece9390b3b966fe6ba4de209b3" title="Silicon Valley is migrating from expensive closed-source models to cheaper open-source alternatives" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chamath Palihapitiya said his team migrated a large number of workloads to Kimi K2 because it was significantly more performant and much cheaper than both OpenAI and Anthropic.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/avwpphq8hnxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdl9q/silicon_valley_is_migrating_from_expensive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T12:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi2tky</id>
    <title>I built a small Python tool to track how your directories get messy (and clean again)</title>
    <updated>2025-10-28T07:12:21+00:00</updated>
    <author>
      <name>/u/VegetableSense</name>
      <uri>https://old.reddit.com/user/VegetableSense</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, much as we hate to admit, almost every project or downloads folder gets out of control over time (yep).&lt;/p&gt; &lt;p&gt;I got curious â€” not just about which files change, but &lt;strong&gt;how the structure itself evolves.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;So I built &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;&lt;strong&gt;Directory Monitor&lt;/strong&gt;&lt;/a&gt; â€” a lightweight Python script that keeps tabs on &lt;strong&gt;directory organization&lt;/strong&gt;, not just file edits. This tool uses local LLMs (Qwen, Llama, choose your own) to analyze project structure and give cleanup recommendations. Everything runs locally - no cloud APIs.&lt;/p&gt; &lt;p&gt;**The interesting technical bits:**&lt;/p&gt; &lt;p&gt;- Uses RAG with local sentence-transformers to compare current state against historical scans&lt;/p&gt; &lt;p&gt;- LLM analyzes trends and gives specific, actionable recommendations &lt;/p&gt; &lt;p&gt;- Terminal UI with Rich showing real-time metrics and sparklines&lt;/p&gt; &lt;p&gt;- All stored in SQLite locally&lt;/p&gt; &lt;p&gt;**Example output:**&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Messiness Score: 6.2/10&lt;/p&gt; &lt;p&gt;Top 3 Issues:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Too many files (28) in src/components - split into ui/, forms/, layouts/&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;8 files contain 'temp' - move to .archive/ or use proper version control&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Directory depth exceeds 7 levels - flatten structure&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Trend: ðŸ“‰ Improving (was 7.8, now 6.2)&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;**Stack:**&lt;/p&gt; &lt;p&gt;- Ollama (Qwen/Llama) for LLM&lt;/p&gt; &lt;p&gt;- sentence-transformers for embeddings&lt;/p&gt; &lt;p&gt;- SQLite for history&lt;/p&gt; &lt;p&gt;- Python with Rich/Flask&lt;/p&gt; &lt;p&gt;Works completely offline after setup. Tested with Qwen3:8b and Llama3.2.&lt;/p&gt; &lt;p&gt;Would love feedback â€” what features would &lt;em&gt;you&lt;/em&gt; add for keeping folders sane?&lt;/p&gt; &lt;p&gt;**GitHub:** &lt;a href="https://github.com/sukanto-m/directory-monitor"&gt;https://github.com/sukanto-m/directory-monitor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VegetableSense"&gt; /u/VegetableSense &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi2tky/i_built_a_small_python_tool_to_track_how_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T07:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohvcwt</id>
    <title>Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?</title>
    <updated>2025-10-28T00:26:07+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt; &lt;img alt="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" src="https://a.thumbs.redditmedia.com/UBwX6pI157vt7gMYDChu_R8QUux04jne3QgjRhJMLr0.jpg" title="Is an NVIDIA A40 48GB for 1500USD a bad idea because it's age?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hope you're fine.&lt;/p&gt; &lt;p&gt;Short question, I managed to find, working and testing on my PC right now, an A40 48GB. It is passively cooled and it gets quite hot.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8az1kqsdyqxf1.png?width=764&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=301fff8d7b8d78a3f33c97765bb96ebdeaa03e2d"&gt;Local testing on my PC&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The seller (a friend) is asking me 1500USD for it. I'm not from USA, but a 3rd world country.&lt;/p&gt; &lt;p&gt;But I have read here on Local llama that such old cards and such aren't very worth it, also no FP8 support, etc.&lt;/p&gt; &lt;p&gt;So I'm really torn and indecisive about it. For reference, 5090 new goes for about 2700-3300USD (so 32GB, but fp8/fp4 support, like 4x times the bandwidth, etc). Used 4090s are 1600USD. 4090 48GB modded when importing they're about 4200-4400USD. 3090s are 550-600USD.&lt;/p&gt; &lt;p&gt;What would you guys do? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohvcwt/is_an_nvidia_a40_48gb_for_1500usd_a_bad_idea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T00:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohyeee</id>
    <title>Minimax-M2 support added in MLX</title>
    <updated>2025-10-28T02:49:15+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt; &lt;img alt="Minimax-M2 support added in MLX" src="https://preview.redd.it/4yqqtqzynrxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85e601e94e902746685decb25bfc69d58f508850" title="Minimax-M2 support added in MLX" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4yqqtqzynrxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohyeee/minimaxm2_support_added_in_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T02:49:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi9d43</id>
    <title>50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp; Deep Learning class</title>
    <updated>2025-10-28T13:28:38+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt; &lt;img alt="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" src="https://external-preview.redd.it/rTJa8Nhli5TnEtPcCyezMrozQD4vuVxVpOAoemUSml4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85dd88668fea8b054bcbd26215365a38c320197b" title="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ze0Xq5QMvmA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi7k25</id>
    <title>HF Space to help create the -ot flags in llama.cpp</title>
    <updated>2025-10-28T12:07:11+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt; &lt;img alt="HF Space to help create the -ot flags in llama.cpp" src="https://external-preview.redd.it/g9TtSkJliQx_HIwoIut_ECyFVGHzqlshzt1AT9ZxIjA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=711e4dcc38b2e4a206b6b1d8e5ce67eae349c3be" title="HF Space to help create the -ot flags in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Mainly as I was frustrated when manually assigning the layers with the -of flag in llama.cpp and ik_llama.cpp and when increasing maybe just 1 layer in a previous gpu i had to increase the number in all the rest of the gpu, I created a Hugging face space to help with that.&lt;/p&gt; &lt;p&gt;It lets you select the number of GPUs, the size of the model weights and the number of layers and it automatically tries to assign how many layers would fit in your gpu size &lt;strong&gt;on an empty context.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then if you want to fit more context either switch to manual and reduce 1-2 layers per gpu, or increase the size in GB of the model a bit.&lt;/p&gt; &lt;p&gt;Example:&lt;br /&gt; I want to load &lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF"&gt;Bartowski GLM-4.6&lt;/a&gt; in Q6 in my rig (rtx6000, 2x5090, 4x3090) and I have 256GB VRAM and the quant takes 294 GB in Q6 as you can see now in HF if you go to the folder:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837"&gt;https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And GLM-4.6 has 92 layers as you can see here: &lt;a href="https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31"&gt;https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So fill the settings as such:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8"&gt;https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And that actually loads using 2048 context and the GPU are all almost at a 100% vram usage which is what we want.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442"&gt;https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I reduce one layer per GPU to quickly allow more vram for ctx, I can now load 32K context. But checking the GPU usage I might be able to assign one more layer to the rtx6000.&lt;/p&gt; &lt;p&gt;So the final command would be:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=2,0,6,1,3,4,5 ./build/bin/llama-server \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model /mnt/llms/models/bartowski/zai-org_GLM-4.6-GGUF/zai-org_GLM-4.6-Q6_K/zai-org_GLM-4.6-Q6_K-00001-of-00008.gguf \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--alias glm-4.6 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 32768 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 99 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--port 5000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn_.*=CUDA0&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(31|32|33|34|35|36|37|38)\.ffn_.*=CUDA1&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(39|40|41|42|43|44|45|46)\.ffn_.*=CUDA2&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(47|48|49|50|51)\.ffn_.*=CUDA3&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(52|53|54|55|56)\.ffn_.*=CUDA4&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(57|58|59|60|61)\.ffn_.*=CUDA5&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(62|63|64|65|66)\.ffn_.*=CUDA6&amp;quot; --cpu-moe&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Link to the HF space: &lt;a href="https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool"&gt;https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T12:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oht9pw</id>
    <title>Z.ai release Glyph weight</title>
    <updated>2025-10-27T22:55:19+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt; &lt;img alt="Z.ai release Glyph weight" src="https://a.thumbs.redditmedia.com/69TaqDj6bS-Vs7O_YYHmJygT9-976J0J5KJcduszmX8.jpg" title="Z.ai release Glyph weight" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Glyph: Scaling Context Windows via Visual-Text Compression&lt;/p&gt; &lt;p&gt;Paper: arxiv.org/abs/2510.17800&lt;/p&gt; &lt;p&gt;Weights: huggingface.co/zai-org/Glyph&lt;/p&gt; &lt;p&gt;Repo: github.com/thu-coai/Glyph&lt;/p&gt; &lt;p&gt;Glyph is a framework for scaling the context length through visual-text compression. It renders long textual sequences into images and processes them using visionâ€“language models.&lt;/p&gt; &lt;p&gt;This design transforms the challenge of long-context modeling into a multimodal problem, substantially reducing computational and memory costs while preserving semantic information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oht9pw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oht9pw/zai_release_glyph_weight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T22:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia7pp</id>
    <title>GLM-4.6 on fresh SWE-benchâ€“style tasks collected in September 2025</title>
    <updated>2025-10-28T14:02:30+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm Anton from Nebius.&lt;/p&gt; &lt;p&gt;Weâ€™ve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with model evaluations of GLM-4.6 on 49 fresh tasks.&lt;/p&gt; &lt;p&gt;Key takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; joins the leaderboard and is now the &lt;strong&gt;best open-source performer&lt;/strong&gt;, achieving &lt;strong&gt;37.0 % resolved rate&lt;/strong&gt; and &lt;strong&gt;42.9 % pass@5&lt;/strong&gt;, surpassing &lt;strong&gt;GLM 4.5&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the full leaderboard and insights here, and feel free to reach out if youâ€™d like to see other models evaluated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=sep_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi63n6</id>
    <title>OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI</title>
    <updated>2025-10-28T10:50:29+00:00</updated>
    <author>
      <name>/u/mythz</name>
      <uri>https://old.reddit.com/user/mythz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt; &lt;img alt="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" src="https://external-preview.redd.it/NbBv8AZ8_FKSnCg3gr7veZ2x9ORPuKnCYj3fZNiyQ4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa75645d12e8474d1f573ac3f747ada63b172124" title="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythz"&gt; /u/mythz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ServiceStack/llms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industryâ€”John Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)â€”have shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia8fi</id>
    <title>The vLLM team's daily life be like:</title>
    <updated>2025-10-28T14:03:17+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt; &lt;img alt="The vLLM team's daily life be like:" src="https://external-preview.redd.it/ZDF3MmtiYW16dXhmMWptouG6uHo-mrPzGurb2qCOnKrlpr9yhnl7mMdksMxF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0aa03383224463057de137e1db2d57ee7e56cd5" title="The vLLM team's daily life be like:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A massive shout-out to the vLLM team for being the heroes holding it all together so we can actually run all these amazing new models.&lt;/p&gt; &lt;p&gt;And, of course, a huge thank you to all the open-source teams like DeepSeek, Qwen, Kimi, and so many others. You are all pushing the entire field forward. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lw255camzuxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 â€¢ 10 AM â€“ 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM â€“ 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur BÃ¶Ã¶k (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;â†’ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; â†’ &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
