<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-01T18:26:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q13040</id>
    <title>Does anyone know good email clients with local LLM?</title>
    <updated>2026-01-01T12:20:29+00:00</updated>
    <author>
      <name>/u/TurthHurtsDoesntIt</name>
      <uri>https://old.reddit.com/user/TurthHurtsDoesntIt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to find some good email client for Linux/Windows/Android without success. I do not even have unreasonable requirements but not even one of currently accessible projects (for example: inbox-zero, eppie) that I found meet them:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;finished application&lt;/li&gt; &lt;li&gt;imap login (no api key mumbo jumbos)&lt;/li&gt; &lt;li&gt;Local AI model usage only&lt;/li&gt; &lt;li&gt;Local AI needs to sort emails, automatically unsubscribe junk, remove spam, add events to calendar and set reminders.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Does anyone know anything that would fit above requirements?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TurthHurtsDoesntIt"&gt; /u/TurthHurtsDoesntIt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q13040/does_anyone_know_good_email_clients_with_local_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q13040/does_anyone_know_good_email_clients_with_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q13040/does_anyone_know_good_email_clients_with_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T12:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q17y0d</id>
    <title>Llama 3.2 3B fMRI LOAD BEARING DIMS FOUND</title>
    <updated>2026-01-01T16:20:17+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q17y0d/llama_32_3b_fmri_load_bearing_dims_found/"&gt; &lt;img alt="Llama 3.2 3B fMRI LOAD BEARING DIMS FOUND" src="https://b.thumbs.redditmedia.com/vuNZPVEZ2xVWerJWw8WHyzrs7J73AyOR9OcOfJGMmuA.jpg" title="Llama 3.2 3B fMRI LOAD BEARING DIMS FOUND" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a local interpretability toolchain to explore &lt;strong&gt;hidden-dimension coupling&lt;/strong&gt; in small LLMs (Llama-3.2-3B-Instruct). This started as visualization (‚Äúconstellations‚Äù of co-activating dims), but the visuals alone were too noisy to move beyond theory.&lt;/p&gt; &lt;p&gt;So I rebuilt the pipeline to answer a more specific question:&lt;/p&gt; &lt;p&gt;Are there a small number of hidden dimensions that &lt;em&gt;consistently&lt;/em&gt; move with a given ‚Äúhero‚Äù dimension, regardless of prompt, magnitude, or polarity?&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Yes.&lt;br /&gt; And perturbing the top one causes &lt;strong&gt;catastrophic loss of semantic commitment&lt;/strong&gt; while leaving fluency intact.&lt;/p&gt; &lt;h1&gt;Step 1 ‚Äî Reducing noise upstream (not in the renderer)&lt;/h1&gt; &lt;p&gt;Instead of rendering everything, I tightened the experiment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Deterministic decoding&lt;/strong&gt; (no sampling)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Stratified prompt suite&lt;/strong&gt; (baseline, constraints, reasoning, commitment, transitions, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Event-based logging&lt;/strong&gt;, not frame-based&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I only logged events where:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;the hero dim was &lt;strong&gt;active&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;the hero dim was &lt;strong&gt;moving&lt;/strong&gt; (std gate)&lt;/li&gt; &lt;li&gt;Pearson correlation with another dim was &lt;strong&gt;strong&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;polarity relationship was consistent&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Metrics logged per event:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pearson correlation (centered)&lt;/li&gt; &lt;li&gt;Cosine similarity (raw geometry)&lt;/li&gt; &lt;li&gt;Dot/energy&lt;/li&gt; &lt;li&gt;Polarity agreement&lt;/li&gt; &lt;li&gt;Classification: &lt;code&gt;FEATURE&lt;/code&gt; (structural) vs &lt;code&gt;TRIGGER&lt;/code&gt; (functional)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This produced a &lt;em&gt;hostile filter&lt;/em&gt;: most dims disappear unless they matter repeatedly.&lt;/p&gt; &lt;h1&gt;Step 2 ‚Äî Persistence analysis across runs&lt;/h1&gt; &lt;p&gt;Instead of asking ‚Äúwhat lights up,‚Äù I counted:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;The result was a sharp hierarchy, not a cloud.&lt;/p&gt; &lt;p&gt;Top hits (example):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DIM 1731 ‚Äî ~14k hits&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DIM 221 ‚Äî ~10k hits&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;then a steep drop-off into the long tail&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This strongly suggests a &lt;strong&gt;small structural core&lt;/strong&gt; + many conditional ‚Äúguest‚Äù dims.&lt;/p&gt; &lt;h1&gt;Step 3 ‚Äî Causal test (this is the key part)&lt;/h1&gt; &lt;p&gt;I then built a small UI to &lt;strong&gt;intervene on individual hidden dimensions&lt;/strong&gt; during generation:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;choose layer&lt;/li&gt; &lt;li&gt;choose dim&lt;/li&gt; &lt;li&gt;apply epsilon bias (not hard zero)&lt;/li&gt; &lt;li&gt;apply to attention output + MLP output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When I biased &lt;strong&gt;DIM 1731&lt;/strong&gt; (layer ~20) with Œµ ‚âà +3:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;grammar stayed intact&lt;/li&gt; &lt;li&gt;tokens kept flowing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;semantic commitment collapsed&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;reasoning failed completely&lt;/li&gt; &lt;li&gt;output devolved into repetitive, affect-heavy, indecisive text&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This was &lt;em&gt;not&lt;/em&gt; random noise or total model failure.&lt;br /&gt; It looks like the model can still ‚Äútalk‚Äù but &lt;strong&gt;cannot commit to a trajectory&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;That failure mode was consistent with what the persistence analysis predicted.&lt;/p&gt; &lt;h1&gt;Interpretation (carefully stated)&lt;/h1&gt; &lt;p&gt;DIM 1731 does &lt;em&gt;not&lt;/em&gt; appear to be:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a topic neuron&lt;/li&gt; &lt;li&gt;a style feature&lt;/li&gt; &lt;li&gt;a lexical unit&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It behaves like part of a &lt;strong&gt;decision-stability / constraint / routing spine&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;present whenever the hero dim is doing real work&lt;/li&gt; &lt;li&gt;polarity-stable&lt;/li&gt; &lt;li&gt;survives across prompt classes&lt;/li&gt; &lt;li&gt;causally load-bearing when perturbed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm calling it ‚ÄúThe King‚Äù internally because removing or overdriving it destabilizes everything downstream ‚Äî but that‚Äôs just a nickname, not a claim.&lt;/p&gt; &lt;h1&gt;Why I think this matters&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;This is a concrete example of &lt;strong&gt;persistent, high-centrality hidden dimensions&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;It suggests a path toward: &lt;ul&gt; &lt;li&gt;targeted pruning&lt;/li&gt; &lt;li&gt;hallucination detection (hero activation without core engagement looks suspect)&lt;/li&gt; &lt;li&gt;mechanistic comparison across models&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;It bridges visualization ‚Üí aggregation ‚Üí &lt;strong&gt;causal confirmation&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm not claiming universality or that this generalizes yet.&lt;br /&gt; Next steps are sign-flip tests, ablations on the next-ranked dim (‚Äúthe Queen‚Äù), and cross-model replication.&lt;/p&gt; &lt;p&gt;Happy to hear critiques, alternative explanations, or suggestions for better controls.&lt;/p&gt; &lt;p&gt;&lt;em&gt;(Screenshot attached below: hit distribution, and causal intervention output.)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;DIM 1731: 13,952 hits (The King)&lt;/p&gt; &lt;p&gt;DIM 221: 10,841 hits (The Queen)&lt;/p&gt; &lt;p&gt;DIM 769: 4,941 hits&lt;/p&gt; &lt;p&gt;DIM 1935: 2,300 hits&lt;/p&gt; &lt;p&gt;DIM 2015: 2,071 hits&lt;/p&gt; &lt;p&gt;DIM 1659: 1,900 hits&lt;/p&gt; &lt;p&gt;DIM 571: 1,542 hits&lt;/p&gt; &lt;p&gt;DIM 1043: 1,536 hits&lt;/p&gt; &lt;p&gt;DIM 1283: 1,388 hits&lt;/p&gt; &lt;p&gt;DIM 642: 1,280 hits&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/acsagvzqjrag1.png?width=1542&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=29560087a6c9c8f2a752957863b78700b89016b5"&gt;Perturbation of the load bearing dim directly affecting output&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q17y0d/llama_32_3b_fmri_load_bearing_dims_found/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q17y0d/llama_32_3b_fmri_load_bearing_dims_found/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q17y0d/llama_32_3b_fmri_load_bearing_dims_found/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T16:20:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q110g5</id>
    <title>My third and final derivation post: Understanding GRPO step by step</title>
    <updated>2026-01-01T10:14:17+00:00</updated>
    <author>
      <name>/u/garg-aayush</name>
      <uri>https://old.reddit.com/user/garg-aayush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q110g5/my_third_and_final_derivation_post_understanding/"&gt; &lt;img alt="My third and final derivation post: Understanding GRPO step by step" src="https://external-preview.redd.it/E--8HNmTu_NoPUyVcISVd4hoLpw2KZ2Gi_gnYk8f1WI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=455941393ba91ffd55472ff18fdbd18999911820" title="My third and final derivation post: Understanding GRPO step by step" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy New Year everyone!&lt;/p&gt; &lt;p&gt;I am starting my 2026 by finishing what I started a few days ago. This is the third and final post in my &lt;strong&gt;derive the RL loss(es) from first principles&lt;/strong&gt; series, following &lt;a href="https://huggingface.co/blog/garg-aayush/ppo-from-first-principle"&gt;PPO&lt;/a&gt; and &lt;a href="https://huggingface.co/blog/garg-aayush/derive-dpo-loss"&gt;DPO&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This time I focused on GRPO (Group Relative Policy Optimization), the algorithm introduced in the DeepSeekMath paper that has become one of the most widely used approaches for training reasoning models using RLVR throughout 2025.&lt;/p&gt; &lt;p&gt;In simple terms, GRPO tries to mitigate the memory and compute overhead associated with PPO due to training a critic (value function) model of similar size as the policy alongside the policy model.&lt;/p&gt; &lt;p&gt;The key insight is that the PPO value function is fundamentally just a baseline for variance reduction. Instead of training a separate critic model to estimate this baseline, we can sample multiple completions (&lt;strong&gt;group&lt;/strong&gt;) for each prompt and use their rewards to form a baseline for advantage computation.&lt;/p&gt; &lt;p&gt;This helps us eliminate the need to train a separate critic model and lowers training compute and memory footprint while still preserving PPO‚Äôs core stability mechanisms, including the clipped surrogate objective and KL regularization.&lt;/p&gt; &lt;p&gt;You can find the blog post here: &lt;a href="https://huggingface.co/blog/garg-aayush/derive-grpo-loss"&gt;https://huggingface.co/blog/garg-aayush/derive-grpo-loss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is probably my last mathematical derivation post for a while. Working through PPO, DPO, and GRPO derivations was both hectic and frustrating at times. However, it has been a great way to build intuition around the most popular RL algorithms. Moreover, it helped me understand the key differences and commonalities between all three and how they relate to each other.&lt;/p&gt; &lt;p&gt;As always, happy to discuss or get corrections if I have messed something up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/garg-aayush"&gt; /u/garg-aayush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/garg-aayush/derive-grpo-loss"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q110g5/my_third_and_final_derivation_post_understanding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q110g5/my_third_and_final_derivation_post_understanding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T10:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1q16w0k</id>
    <title>The State Of LLMs 2025: Progress, Problems, and Predictions</title>
    <updated>2026-01-01T15:35:11+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q16w0k/the_state_of_llms_2025_progress_problems_and/"&gt; &lt;img alt="The State Of LLMs 2025: Progress, Problems, and Predictions" src="https://external-preview.redd.it/ip3t1phQ469yOBa2kbOD__RHIhAqj8C7dU-KA_Pn0lI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=859212246454f6bb28f9d882875d0a6ae9694d87" title="The State Of LLMs 2025: Progress, Problems, and Predictions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://magazine.sebastianraschka.com/p/state-of-llms-2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q16w0k/the_state_of_llms_2025_progress_problems_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q16w0k/the_state_of_llms_2025_progress_problems_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T15:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1aif6</id>
    <title>Running an unsupported DeepSeek V3.2 in llama.cpp for some New Year's fun</title>
    <updated>2026-01-01T18:03:59+00:00</updated>
    <author>
      <name>/u/fairydreaming</name>
      <uri>https://old.reddit.com/user/fairydreaming</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday was a New Year's eve and somewhere between a fourth beer and a glass of bubbly I had this weird idea to see how DeepSeek V3.2 runs with a dense (non-sparse) attention. I know that sparse attention is the main highlight of this release, but people were trying to vibe-code it for months so maybe we can just skip that for now and use the model as it is...&lt;/p&gt; &lt;p&gt;So here's the patch allowing to convert and use the model with the current llama.cpp:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;diff --git a/convert_hf_to_gguf.py b/convert_hf_to_gguf.py index edc0ed539..dd6dbe8d5 100755 --- a/convert_hf_to_gguf.py +++ b/convert_hf_to_gguf.py @@ -952,6 +952,9 @@ class TextModel(ModelBase): return seems_special + def override_tokenizer_settings(self, tokenizer): + return tokenizer + # used for GPT-2 BPE and WordPiece vocabs def get_vocab_base(self) -&amp;gt; tuple[list[str], list[int], str]: tokens: list[str] = [] @@ -959,6 +962,7 @@ class TextModel(ModelBase): from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(self.dir_model) + tokenizer = self.override_tokenizer_settings(tokenizer) vocab_size = self.hparams.get(&amp;quot;vocab_size&amp;quot;, len(tokenizer.vocab)) assert max(tokenizer.vocab.values()) &amp;lt; vocab_size @@ -7180,14 +7184,26 @@ class DeepseekModel(TextModel): u/ModelBase.register( &amp;quot;DeepseekV2ForCausalLM&amp;quot;, &amp;quot;DeepseekV3ForCausalLM&amp;quot;, + &amp;quot;DeepseekV32ForCausalLM&amp;quot;, &amp;quot;KimiVLForConditionalGeneration&amp;quot;, ) class DeepseekV2Model(TextModel): model_arch = gguf.MODEL_ARCH.DEEPSEEK2 + def override_tokenizer_settings(self, tokenizer): + # override add_bos_token setting to get pre-tokenizer recognized + if self.hparams.get(&amp;quot;model_type&amp;quot;) == &amp;quot;deepseek_v32&amp;quot;: + tokenizer.add_bos_token = True + return tokenizer + def set_vocab(self): try: self._set_vocab_gpt2() + # in DeepSeek V3.2 adding BOS token is disabled in tokenizer configuration + # instead the BOS token is added in encode_messages() Python code + # therefore we have to override this setting + if self.hparams.get(&amp;quot;model_type&amp;quot;) == &amp;quot;deepseek_v32&amp;quot;: + self.gguf_writer.add_add_bos_token(True) return except Exception: pass @@ -7277,7 +7293,7 @@ class DeepseekV2Model(TextModel): def modify_tensors(self, data_torch: Tensor, name: str, bid: int | None) -&amp;gt; Iterable[tuple[str, Tensor]]: # skip vision tensors and remove &amp;quot;language_model.&amp;quot; for Kimi-VL - if &amp;quot;vision_tower&amp;quot; in name or &amp;quot;multi_modal_projector&amp;quot; in name: + if &amp;quot;vision_tower&amp;quot; in name or &amp;quot;multi_modal_projector&amp;quot; in name or &amp;quot;self_attn.indexer&amp;quot; in name: return [] if name.startswith(&amp;quot;language_model.&amp;quot;): &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It's nothing fancy, just:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; an override for add_bos_token that is now false in 3.2 and it prevents the (unchanged) pre-tokenizer from being recognized,&lt;/li&gt; &lt;li&gt;skipping lightning indexer tensors since there's no support for that yet.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;With this patch I converted the model:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python convert_hf_to_gguf.py /mnt/md0/huggingface/hub/models--deepseek-ai--DeepSeek-V3.2/snapshots/a7e62ac04ecb2c0a54d736dc46601c5606cf10a6/ --outfile /mnt/md0/models/DeepSeek-V3.2-nolight.gguf --outtype q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that DeepSeek V3.2 has no jinja chat template at all. AFAIK the overall structure of the template did not change, but they use some new tool calls that require some Python code magic.&lt;/p&gt; &lt;p&gt;I tried to run the model with &lt;code&gt;--chat-template deepseek3&lt;/code&gt; but for some reason it didn't work correctly (bug in llama.cpp?). So instead I saved jinja template from DeepSeek V3 to a file and used this command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ./bin/llama-cli -m /mnt/md0/models/DeepSeek-V3.2-nolight.gguf -nr --temp 0.0 -cnv --jinja --chat-template-file ../deepseek3.jinja -p &amp;quot;who are you?&amp;quot; Loading model... ‚ñÑ‚ñÑ ‚ñÑ‚ñÑ ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñÄ‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñÑ‚ñà‚ñà‚ñà‚ñÑ ‚ñÄ‚ñÄ‚ñà‚ñÑ ‚ñÑ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà‚ñà‚ñà‚ñÑ ‚ñà‚ñà ‚ñà‚ñà ‚ñÑ‚ñà‚ñÄ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÑ‚ñà‚ñÄ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñà‚ñÑ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñà‚ñÑ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñà‚ñà‚ñà‚ñà ‚ñà‚ñà‚ñà‚ñà‚ñÄ ‚ñà‚ñà‚ñà‚ñà‚ñÄ ‚ñà‚ñà ‚ñà‚ñà ‚ñÄ‚ñÄ ‚ñÄ‚ñÄ build : b7597-13814eb37 model : DeepSeek-V3.2-nolight.gguf modalities : text available commands: /exit or Ctrl+C stop or exit /regen regenerate the last response /clear clear the chat history /read add a text file &amp;gt; who are you? Hello! I'm DeepSeek, an AI assistant created by DeepSeek Company. I'm here to help you with a wide variety of tasks - whether you need answers to questions, help with writing, analysis, problem-solving, coding assistance, or just want to have a conversation! I'm a text-based AI model with some special features: - I can process uploaded files (images, PDFs, Word docs, Excel sheets, PowerPoint presentations, and text files) - I have a large 128K context window for handling lengthy conversations and documents - I support web search functionality (though you need to manually enable it in the Web/App interface) - I'm completely free to use with no charges I'm designed to be helpful, detailed, and warm in my responses. How can I assist you today? üòä [ Prompt: 14.3 t/s | Generation: 6.9 t/s ] &amp;gt; how many r's are in the phrase &amp;quot;crrrunchy crrrabby crrrabs&amp;quot;? Let‚Äôs count the letter **‚Äúr‚Äù** in the phrase: **&amp;quot;crrrunchy crrrabby crrrabs&amp;quot;** Break it down word by word: 1. **crrrunchy** ‚Üí c r r r u n c h y ‚Üí **3 r‚Äôs** 2. **crrrabby** ‚Üí c r r r a b b y ‚Üí **3 r‚Äôs** 3. **crrrabs** ‚Üí c r r r a b s ‚Üí **3 r‚Äôs** Total = 3 + 3 + 3 = **9 r‚Äôs**. So the answer is **9**. [ Prompt: 18.7 t/s | Generation: 6.7 t/s ] &amp;gt; what is 21345*1234? solve this like a high-schooler with a pen and paper would do Alright, let‚Äôs multiply **21345 √ó 1234** step-by-step like in school. We‚Äôll break 1234 into: 1234 = 1000 + 200 + 30 + 4 --- **Step 1: Multiply 21345 by 1000** 21345 √ó 1000 = 21,345,000 --- **Step 2: Multiply 21345 by 200** 21345 √ó 2 = 42,690 Then √ó 100 = 4,269,000? Wait, careful: 21345 √ó 200 = 21345 √ó 2 √ó 100 = 42,690 √ó 100 = 4,269,000 --- **Step 3: Multiply 21345 by 30** 21345 √ó 3 = 64,035 Then √ó 10 = 640,350 --- **Step 4: Multiply 21345 by 4** 21345 √ó 4 = 85,380 --- **Step 5: Add them all up** First, write them aligned by place value: ``` 21,345,000 (√ó1000) + 4,269,000 (√ó200) + 640,350 (√ó30) + 85,380 (√ó4) ``` --- Add step-by-step: 21,345,000 + 4,269,000 = 25,614,000 25,614,000 + 640,350 = 26,254,350 26,254,350 + 85,380 = 26,339,730 --- **Final answer:** **26,339,730** ‚úÖ [ Prompt: 17.6 t/s | Generation: 6.5 t/s ] &amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Looks good so far, now I'm going to try my lineage-bench benchmark to see if the model is dumber with dense attention. Hopefully not!&lt;/p&gt; &lt;p&gt;Happy New Year!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fairydreaming"&gt; /u/fairydreaming &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1aif6/running_an_unsupported_deepseek_v32_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T18:03:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1akch</id>
    <title>Unpopular opinion: if your product only works on GPT-4, you don‚Äôt have a model problem, you have a systems problem</title>
    <updated>2026-01-01T18:06:07+00:00</updated>
    <author>
      <name>/u/aieatstheworld</name>
      <uri>https://old.reddit.com/user/aieatstheworld</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been stress-testing workflows across Llama, Mistral, Qwen, etc. (fully local, no safety net), and one thing became obvious fast:&lt;/p&gt; &lt;p&gt;Most ‚ÄúAI products‚Äù are pipeline-dependent and not model-dependent.&lt;/p&gt; &lt;p&gt;When you remove frontier models from the equation:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Bad RAG designs stop working immediately ‚Ä¢ Prompt spaghetti gets exposed ‚Ä¢ Implicit assumptions break ‚Ä¢ Latency, batching, memory, and context suddenly matter &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Cloud models feel magical because they hide architectural debt.&lt;/p&gt; &lt;p&gt;Local models don‚Äôt hide anything.&lt;/p&gt; &lt;p&gt;Ironically, once you fix the system:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Open-source models become predictable ‚Ä¢ Costs drop by an order of magnitude ‚Ä¢ You regain control over data, latency, and failure modes &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Basically deploying your own models is like planting a seed which keeps paying you forever in the form of fruits&lt;/p&gt; &lt;p&gt;Frontier models still win on zero-shot reasoning and generality.&lt;/p&gt; &lt;p&gt;But for real deployments? The gap is smaller than people think if the infra is done right.&lt;/p&gt; &lt;p&gt;Curious how others here see it:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;‚Ä¢ Which OSS models have held up best in production? ‚Ä¢ What was the hardest thing to get right (RAG, inference, evals, serving)? ‚Ä¢ Do you see local deployments replacing APIs, or coexisting long-term? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No hype, genuinely interested in war stories.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aieatstheworld"&gt; /u/aieatstheworld &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1akch/unpopular_opinion_if_your_product_only_works_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1akch/unpopular_opinion_if_your_product_only_works_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1akch/unpopular_opinion_if_your_product_only_works_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T18:06:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0wemx</id>
    <title>Happy New Years everyone!</title>
    <updated>2026-01-01T05:12:40+00:00</updated>
    <author>
      <name>/u/LocoMod</name>
      <uri>https://old.reddit.com/user/LocoMod</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2026 will feel like a decade. Onward!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LocoMod"&gt; /u/LocoMod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wemx/happy_new_years_everyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wemx/happy_new_years_everyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0wemx/happy_new_years_everyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T05:12:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q17qej</id>
    <title>MCP Chat Studio v2: Workspace mode, workflows, contracts, mocks, and more</title>
    <updated>2026-01-01T16:11:16+00:00</updated>
    <author>
      <name>/u/Some-Put8242</name>
      <uri>https://old.reddit.com/user/Some-Put8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q17qej/mcp_chat_studio_v2_workspace_mode_workflows/"&gt; &lt;img alt="MCP Chat Studio v2: Workspace mode, workflows, contracts, mocks, and more" src="https://external-preview.redd.it/bM3wmh0mQCXkN-9fJxrnXP4wYWEqJcOslg5Xd3ZXyOI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3e09675c96693763724226fa03738d214c14eea" title="MCP Chat Studio v2: Workspace mode, workflows, contracts, mocks, and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building MCP Chat Studio as a ‚ÄúPostman for MCP servers,‚Äù and v2 is now live.&lt;/p&gt; &lt;p&gt;What‚Äôs new in v2:&lt;/p&gt; &lt;p&gt;- Workspace mode: infinite canvas with draggable panels, radial menu, quick bar, command palette, sessions + export/&lt;/p&gt; &lt;p&gt;import.&lt;/p&gt; &lt;p&gt;- Inspector: tool runner, protocol timeline, bulk test, diff view.&lt;/p&gt; &lt;p&gt;- Workflows: visual builder + AI Builder + debugger (breakpoints/step mode).&lt;/p&gt; &lt;p&gt;- Collections: scenario runner + run reports.&lt;/p&gt; &lt;p&gt;- Contracts: schema validation + breaking change checks.&lt;/p&gt; &lt;p&gt;- Mocks: generate/connect mock servers, call via Inspector.&lt;/p&gt; &lt;p&gt;- Docs generator (Markdown/HTML/JSON).&lt;/p&gt; &lt;p&gt;- Workflow export to Python + Node scripts.&lt;/p&gt; &lt;p&gt;- Analytics/Performance + Monitors + Brain view.&lt;/p&gt; &lt;p&gt;Repo + demo GIFs: &lt;a href="https://github.com/JoeCastrom/mcp-chat-studio"&gt;https://github.com/JoeCastrom/mcp-chat-studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you build MCP servers, I‚Äôd love feedback on missing capabilities or workflow improvements.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Some-Put8242"&gt; /u/Some-Put8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/JoeCastrom/mcp-chat-studio"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q17qej/mcp_chat_studio_v2_workspace_mode_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q17qej/mcp_chat_studio_v2_workspace_mode_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T16:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1q163um</id>
    <title>GLM 4.7 on 8x3090</title>
    <updated>2026-01-01T15:00:38+00:00</updated>
    <author>
      <name>/u/DeltaSqueezer</name>
      <uri>https://old.reddit.com/user/DeltaSqueezer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is anyone running GLM 4.7 (or 4.5-4.6) on eight 3090s? I was wondering what kind of speeds you were getting as I was considering this set up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeltaSqueezer"&gt; /u/DeltaSqueezer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q163um/glm_47_on_8x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T15:00:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0x19t</id>
    <title>Anyone tried IQuest-Coder-V1 yet? The 40B numbers look wild</title>
    <updated>2026-01-01T05:51:19+00:00</updated>
    <author>
      <name>/u/Agile-Salamander1667</name>
      <uri>https://old.reddit.com/user/Agile-Salamander1667</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"&gt; &lt;img alt="Anyone tried IQuest-Coder-V1 yet? The 40B numbers look wild" src="https://a.thumbs.redditmedia.com/ORV5UJcDg_JMf-s_fhhD2rkOvb4H-H9k_D7F3CobGM8.jpg" title="Anyone tried IQuest-Coder-V1 yet? The 40B numbers look wild" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new IQuest-Coder-V1 family just dropped on GitHub and Hugging Face, and the benchmark numbers are honestly looking a bit wild for a 40B model. It‚Äôs claiming &lt;strong&gt;81.4% on SWE-Bench Verified&lt;/strong&gt; and over &lt;strong&gt;81% on LiveCodeBench v6&lt;/strong&gt;, which puts it right up there with (or ahead of) much larger proprietary models like GPT-5.1 and Claude 4.5 Sonnet. What's interesting is their &amp;quot;Code-Flow&amp;quot; training approach‚Äîinstead of just learning from static files, they trained it on repository evolution and commit transitions to better capture how logic actually changes over time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vek0sb18foag1.png?width=3022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=560bd32d14cdc982931196028beafea8dc97d3a1"&gt;https://preview.redd.it/vek0sb18foag1.png?width=3022&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=560bd32d14cdc982931196028beafea8dc97d3a1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They've released both &amp;quot;Instruct&amp;quot; and &amp;quot;Thinking&amp;quot; versions, with the latter using reasoning-driven RL to trigger better autonomous error recovery in long-horizon tasks. There's also a &amp;quot;Loop&amp;quot; variant that uses a recurrent transformer design to save on deployment footprint while keeping the capacity high. Since it supports a native &lt;strong&gt;128k context&lt;/strong&gt;, I‚Äôm curious if anyone has hooked this up to Aider or Cline yet.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1"&gt;https://github.com/IQuestLab/IQuest-Coder-V1&lt;/a&gt;&lt;br /&gt; &lt;a href="https://iquestlab.github.io/"&gt;https://iquestlab.github.io/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/IQuestLab"&gt;https://huggingface.co/IQuestLab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agile-Salamander1667"&gt; /u/Agile-Salamander1667 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x19t/anyone_tried_iquestcoderv1_yet_the_40b_numbers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T05:51:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0vph6</id>
    <title>OpenForecaster Release</title>
    <updated>2026-01-01T04:30:45+00:00</updated>
    <author>
      <name>/u/logisbase2</name>
      <uri>https://old.reddit.com/user/logisbase2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"&gt; &lt;img alt="OpenForecaster Release" src="https://preview.redd.it/iuw1u1y61oag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39850abc1dea70f66b62d9a4cafb3424faf01f53" title="OpenForecaster Release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/papers/2512.25070"&gt;https://huggingface.co/papers/2512.25070&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/logisbase2"&gt; /u/logisbase2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuw1u1y61oag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vph6/openforecaster_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T04:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q15vr6</id>
    <title>Any clues as to what Gemma 3's training data consisted of?</title>
    <updated>2026-01-01T14:50:22+00:00</updated>
    <author>
      <name>/u/EducationalCicada</name>
      <uri>https://old.reddit.com/user/EducationalCicada</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know Google would never release this information, but has anyone been able to extract parts of the training data from Gemma 3? I'm really curious about what they used.&lt;/p&gt; &lt;p&gt;I'm guessing it was trained on public domain (and lower quality, compared to what they fed Gemini) data due to the existence of such attacks on open-weight models.&lt;/p&gt; &lt;p&gt;It's a bit frustrating because Google is sitting on some of the most valuable data on the planet , but Gemma will never see any of it in training.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EducationalCicada"&gt; /u/EducationalCicada &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q15vr6/any_clues_as_to_what_gemma_3s_training_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T14:50:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0uoys</id>
    <title>Top 10 Open Models by Providers on LMArena</title>
    <updated>2026-01-01T03:32:13+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"&gt; &lt;img alt="Top 10 Open Models by Providers on LMArena" src="https://preview.redd.it/xo7h0asvqnag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1f90fd4135eb4647968b39123e68d3e463e24269" title="Top 10 Open Models by Providers on LMArena" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xo7h0asvqnag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uoys/top_10_open_models_by_providers_on_lmarena/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T03:32:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1q11bpg</id>
    <title>Upstage released an official response regarding the Solar 102B controversy</title>
    <updated>2026-01-01T10:34:37+00:00</updated>
    <author>
      <name>/u/Lucidstyle</name>
      <uri>https://old.reddit.com/user/Lucidstyle</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"&gt; &lt;img alt="Upstage released an official response regarding the Solar 102B controversy" src="https://b.thumbs.redditmedia.com/UVpAFlyRpF-TXsFk90RKqLGFFfU7kttl0lfAk_EgShQ.jpg" title="Upstage released an official response regarding the Solar 102B controversy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;From Upstage CEO Sung Kim's Facebook:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;[Solar-Open-100B is not derived from GLM-4.5-Air]&lt;/p&gt; &lt;p&gt;Kevin Ko, who leads the open-source LLM development, has clearly addressed the issue.&lt;a href="https://github.com/hyunwoongko/solar-vs-glm-vs-phi"&gt;https://github.com/hyunwoongko/solar-vs-glm-vs-phi&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's really great to see the ecosystem's self-correcting mechanism in action‚Äîwhere the community raises doubts and verifies them independently. Thank you.&lt;/p&gt; &lt;p&gt;Translated by Gemini&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ihjgtupfupag1.png?width=492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ac7f1a4b5675b48a4f6cc951ba09a29ac66b0cb"&gt;https://preview.redd.it/ihjgtupfupag1.png?width=492&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3ac7f1a4b5675b48a4f6cc951ba09a29ac66b0cb&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lucidstyle"&gt; /u/Lucidstyle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q11bpg/upstage_released_an_official_response_regarding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T10:34:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1q19nel</id>
    <title>ISON: 70% fewer tokens than JSON. Built for LLM context stuffing.</title>
    <updated>2026-01-01T17:29:56+00:00</updated>
    <author>
      <name>/u/Immediate-Cake6519</name>
      <uri>https://old.reddit.com/user/Immediate-Cake6519</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stop burning tokens on JSON syntax.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;This JSON&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;{&lt;/p&gt; &lt;p&gt;&amp;quot;users&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&amp;quot;id&amp;quot;: 1, &amp;quot;name&amp;quot;: &amp;quot;Alice&amp;quot;, &amp;quot;email&amp;quot;: &amp;quot;[&lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt;](mailto:&lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt;)&amp;quot;, &amp;quot;active&amp;quot;: true},&lt;/p&gt; &lt;p&gt;{&amp;quot;id&amp;quot;: 2, &amp;quot;name&amp;quot;: &amp;quot;Bob&amp;quot;, &amp;quot;email&amp;quot;: &amp;quot;[&lt;a href="mailto:bob@example.com"&gt;bob@example.com&lt;/a&gt;](mailto:&lt;a href="mailto:bob@example.com"&gt;bob@example.com&lt;/a&gt;)&amp;quot;, &amp;quot;active&amp;quot;: false},&lt;/p&gt; &lt;p&gt;{&amp;quot;id&amp;quot;: 3, &amp;quot;name&amp;quot;: &amp;quot;Charlie&amp;quot;, &amp;quot;email&amp;quot;: &amp;quot;[&lt;a href="mailto:charlie@test.com"&gt;charlie@test.com&lt;/a&gt;](mailto:&lt;a href="mailto:charlie@test.com"&gt;charlie@test.com&lt;/a&gt;)&amp;quot;, &amp;quot;active&amp;quot;: true}&lt;/p&gt; &lt;p&gt;],&lt;/p&gt; &lt;p&gt;&amp;quot;config&amp;quot;: {&lt;/p&gt; &lt;p&gt;&amp;quot;timeout&amp;quot;: 30,&lt;/p&gt; &lt;p&gt;&amp;quot;debug&amp;quot;: true,&lt;/p&gt; &lt;p&gt;&amp;quot;api_key&amp;quot;: &amp;quot;sk-xxx-secret&amp;quot;,&lt;/p&gt; &lt;p&gt;&amp;quot;max_retries&amp;quot;: 3&lt;/p&gt; &lt;p&gt;},&lt;/p&gt; &lt;p&gt;&amp;quot;orders&amp;quot;: [&lt;/p&gt; &lt;p&gt;{&amp;quot;id&amp;quot;: &amp;quot;O1&amp;quot;, &amp;quot;user_id&amp;quot;: 1, &amp;quot;product&amp;quot;: &amp;quot;Widget Pro&amp;quot;, &amp;quot;total&amp;quot;: 99.99},&lt;/p&gt; &lt;p&gt;{&amp;quot;id&amp;quot;: &amp;quot;O2&amp;quot;, &amp;quot;user_id&amp;quot;: 2, &amp;quot;product&amp;quot;: &amp;quot;Gadget Plus&amp;quot;, &amp;quot;total&amp;quot;: 149.50},&lt;/p&gt; &lt;p&gt;{&amp;quot;id&amp;quot;: &amp;quot;O3&amp;quot;, &amp;quot;user_id&amp;quot;: 1, &amp;quot;product&amp;quot;: &amp;quot;Super Tool&amp;quot;, &amp;quot;total&amp;quot;: 299.00}&lt;/p&gt; &lt;p&gt;]&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;~180 tokens. Brackets, quotes, colons everywhere.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Same data in ISON:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;table.users&lt;/p&gt; &lt;p&gt;id name email active&lt;/p&gt; &lt;p&gt;1 Alice [&lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt;](mailto:&lt;a href="mailto:alice@example.com"&gt;alice@example.com&lt;/a&gt;) true&lt;/p&gt; &lt;p&gt;2 Bob [&lt;a href="mailto:bob@example.com"&gt;bob@example.com&lt;/a&gt;](mailto:&lt;a href="mailto:bob@example.com"&gt;bob@example.com&lt;/a&gt;) false&lt;/p&gt; &lt;p&gt;3 Charlie [&lt;a href="mailto:charlie@test.com"&gt;charlie@test.com&lt;/a&gt;](mailto:&lt;a href="mailto:charlie@test.com"&gt;charlie@test.com&lt;/a&gt;) true&lt;/p&gt; &lt;p&gt;object.config&lt;/p&gt; &lt;p&gt;timeout 30&lt;/p&gt; &lt;p&gt;debug true&lt;/p&gt; &lt;p&gt;api_key &amp;quot;sk-xxx-secret&amp;quot;&lt;/p&gt; &lt;p&gt;max_retries 3&lt;/p&gt; &lt;p&gt;table.orders&lt;/p&gt; &lt;p&gt;id user_id product total&lt;/p&gt; &lt;p&gt;O1 :1 &amp;quot;Widget Pro&amp;quot; 99.99&lt;/p&gt; &lt;p&gt;O2 :2 &amp;quot;Gadget Plus&amp;quot; 149.50&lt;/p&gt; &lt;p&gt;O3 :1 &amp;quot;Super Tool&amp;quot; 299.00&lt;/p&gt; &lt;p&gt;~60 tokens. Clean. Readable. LLMs parse it without instructions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://table.name/"&gt;table.name&lt;/a&gt; for arrays of objects&lt;/p&gt; &lt;p&gt;&lt;a href="http://object.name/"&gt;object.name&lt;/a&gt; for key-value configs&lt;/p&gt; &lt;p&gt;:1 references row with id=1 (cross-table relationships)&lt;/p&gt; &lt;p&gt;No escaping hell&lt;/p&gt; &lt;p&gt;TSV-like structure (LLMs already know this from training)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;| Format | Tokens | LLM Accuracy |&lt;/p&gt; &lt;p&gt;|---------|---------|-----------------|&lt;/p&gt; &lt;p&gt;| JSON | 2,039 | 84.0% |&lt;/p&gt; &lt;p&gt;| ISON | 685 | 88.0% |&lt;/p&gt; &lt;p&gt;Fewer tokens. Better accuracy. Tested on GPT-4, Claude, DeepSeek, Llama 3.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Available everywhere:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Python | pip install ison-py&lt;/p&gt; &lt;p&gt;TypeScript | npm install ison-ts&lt;/p&gt; &lt;p&gt;Rust | cargo add ison-rs&lt;/p&gt; &lt;p&gt;Go | &lt;a href="http://github.com/maheshvaikri/ison-go"&gt;github.com/maheshvaikri/ison-go&lt;/a&gt;&lt;/p&gt; &lt;p&gt;VS Code | ison-lang extension&lt;/p&gt; &lt;p&gt;n8n | n8n-nodes-ison&lt;/p&gt; &lt;p&gt;vscode extension | &lt;a href="mailto:ison-lang@1.0.1"&gt;ison-lang@1.0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Ecosystem Includes&lt;/strong&gt;&lt;br /&gt; &lt;strong&gt;ISON&lt;/strong&gt; - Data Format&lt;br /&gt; &lt;strong&gt;ISONL&lt;/strong&gt; - DataFormat for Large Datasets - similar to JSONL&lt;br /&gt; &lt;strong&gt;ISONantic&lt;/strong&gt; for Validation - Similar to Pydantic for JSON&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/maheshvaikri-code/ison"&gt;https://github.com/maheshvaikri-code/ison&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this for my agentic memory system where every token counts and where context window matters. Now open source.&lt;/p&gt; &lt;p&gt;Feedback welcome. Give a Star if you like it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Immediate-Cake6519"&gt; /u/Immediate-Cake6519 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q19nel/ison_70_fewer_tokens_than_json_built_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q19nel/ison_70_fewer_tokens_than_json_built_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q19nel/ison_70_fewer_tokens_than_json_built_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T17:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1q170wt</id>
    <title>reap is near lossless btw /s</title>
    <updated>2026-01-01T15:40:54+00:00</updated>
    <author>
      <name>/u/runawaychicken</name>
      <uri>https://old.reddit.com/user/runawaychicken</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q170wt/reap_is_near_lossless_btw_s/"&gt; &lt;img alt="reap is near lossless btw /s" src="https://b.thumbs.redditmedia.com/6ZBP8lcHOLdM1KmIfUmE5BTGauzGQ4snzIamrgBVO6I.jpg" title="reap is near lossless btw /s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xjsst0xvbrag1.png?width=995&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46adcb08a79db53d0397cda4f212139138e28ac0"&gt;.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;every reap model i tried has been much worse than just the smaller original quantized one. The full weight ones wont get a single mistake. q2 would get 1 or 2 mistakes. reap ones makes 10000 mistakes, i dont understand how benchmarks wont reflect this degradation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/runawaychicken"&gt; /u/runawaychicken &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q170wt/reap_is_near_lossless_btw_s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q170wt/reap_is_near_lossless_btw_s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q170wt/reap_is_near_lossless_btw_s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T15:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0vom4</id>
    <title>IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)</title>
    <updated>2026-01-01T04:29:26+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"&gt; &lt;img alt="IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)" src="https://external-preview.redd.it/BV6FpKtNQWUgU3wdfJKH5UR3dlogn4uR0Fs4eIn5vSk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3873330c45da75d454dd4483d37c39a58e5c6810" title="IQuestLab/IQuest-Coder-V1 ‚Äî 40B parameter coding LLM ‚Äî Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0vom4/iquestlabiquestcoderv1_40b_parameter_coding_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T04:29:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1986x</id>
    <title>IQuestCoder - new 40B dense coding model</title>
    <updated>2026-01-01T17:12:51+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"&gt; &lt;img alt="IQuestCoder - new 40B dense coding model" src="https://external-preview.redd.it/puEaI60nzHUbVlmNCXfE1sl9fmhvVgJHgKO3FYQHywY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7035b6af6efe8e8b4c47de0ff3945cb68bc2973c" title="IQuestCoder - new 40B dense coding model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As usual, benchmarks claim it's absolutely SOTA and crushes the competition. Since I'm willing to verify it, I've adapted it to GGUF. It's basically Llama arch (reportedly was supposed to be using SWA, but it didn't get used in the final version), so works out of the box with Llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ilintar/IQuest-Coder-V1-40B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T17:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q189os</id>
    <title>7900 XTX + ROCm: A Year Later. Llama.cpp vs vLLM Benchmarks (TB3 eGPU)</title>
    <updated>2026-01-01T16:33:45+00:00</updated>
    <author>
      <name>/u/reujea0</name>
      <uri>https://old.reddit.com/user/reujea0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've had the 7900 XTX for over a year now. While the situation with ROCm has definitely gotten better, it is still a frustrating experience compared to just plugging in an NVIDIA card.&lt;/p&gt; &lt;p&gt;I was curious to see if we could at least run newer models reliably now, so I decided to compare the maturity of &lt;strong&gt;llama.cpp&lt;/strong&gt; vs &lt;strong&gt;vLLM&lt;/strong&gt; on this hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Important Context:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The Setup:&lt;/strong&gt; 7900 XTX connected via &lt;strong&gt;Thunderbolt 3 (eGPU)&lt;/strong&gt;. This might introduce some bandwidth limitations, so I specifically chose models that fit entirely in VRAM to minimize penalty.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Disclaimer:&lt;/strong&gt; This is &lt;em&gt;not&lt;/em&gt; scientific. These are just some quick numbers I ran to check the current state of things.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Environment:&lt;/strong&gt; Huge thanks to &lt;a href="https://github.com/kyuz0"&gt;kyuz0 on GitHub&lt;/a&gt; whose repo allowed me to actually build working images for both llama.cpp and vLLM on this platform.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Here are the results&lt;/p&gt; &lt;h1&gt;Llama.cpp (ROCm)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Running&lt;/em&gt; &lt;code&gt;llama-bench&lt;/code&gt; &lt;em&gt;on local GGUF files.&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;GGUF Filename&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;PP (512)&lt;/th&gt; &lt;th align="left"&gt;Gen (tg512)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth_Llama-3.1-8B-Instruct-GGUF_Llama-3.1-8B-Instruct-BF16.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;14.96 GB&lt;/td&gt; &lt;td align="left"&gt;2226 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;42.51 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;Mistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;15.63 GB&lt;/td&gt; &lt;td align="left"&gt;861 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32.20 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth_Qwen2.5-VL-32B-Instruct-GGUF_Qwen2.5-VL-32B-Instruct-Q4_K_M.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;18.48 GB&lt;/td&gt; &lt;td align="left"&gt;626 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;22.95 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;DeepSeek-R1-Distill-Qwen-32B-Q3_K_M.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;14.84 GB&lt;/td&gt; &lt;td align="left"&gt;669 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;24.12 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;gpt-oss-20b-F16.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;12.83 GB&lt;/td&gt; &lt;td align="left"&gt;2620 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;87.09 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;Qwen3-VL-30B-A3B-Thinking-UD-Q4_K_XL.gguf&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;16.49 GB&lt;/td&gt; &lt;td align="left"&gt;1793 t/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.86 t/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;vLLM (ROCm)&lt;/h1&gt; &lt;p&gt;&lt;em&gt;Running&lt;/em&gt; &lt;code&gt;vllm bench serve&lt;/code&gt; &lt;em&gt;directly from HF repos.&lt;/em&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Repo (HF)&lt;/th&gt; &lt;th align="left"&gt;Format&lt;/th&gt; &lt;th align="left"&gt;Gen Speed&lt;/th&gt; &lt;th align="left"&gt;Latency (TTFT)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth/Meta-Llama-3.1-8B-Instruct&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Native BF16&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;94.19 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;282 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth/gpt-oss-20b&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;F16 (MoE)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;48.33 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1044 ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;code&gt;unsloth/Mistral-Small-3.2-24B-Instruct-2506-bnb-4bit&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;bnb-4bit&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.99 t/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1063 ms&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Just wanted to share some data for anyone else suffering through the AMD local LLM journey.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reujea0"&gt; /u/reujea0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q189os/7900_xtx_rocm_a_year_later_llamacpp_vs_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T16:33:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0uuqt</id>
    <title>Happy New Year: Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning - Fine Tune. (based on recent find of L3.3 8b in the wild)</title>
    <updated>2026-01-01T03:41:30+00:00</updated>
    <author>
      <name>/u/Dangerous_Fix_5526</name>
      <uri>https://old.reddit.com/user/Dangerous_Fix_5526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Special thanks to :&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/user/jacek2023/"&gt;jacek2023&lt;/a&gt; [posting about this model]&lt;/p&gt; &lt;p&gt;and extra special thanks for &amp;quot;&lt;strong&gt;allura-forge&lt;/strong&gt; &amp;quot; for finding this model:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct"&gt;https://huggingface.co/allura-forge/Llama-3.3-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;( For an incredible find of Llama 3.3 8B &amp;quot;in the wild&amp;quot; !!)&lt;/p&gt; &lt;p&gt;I fine tuned it using Unsloth and Claude 4.5 Opus High Reasoning Dataset:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning"&gt;https://huggingface.co/DavidAU/Llama3.3-8B-Instruct-Thinking-Claude-4.5-Opus-High-Reasoning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This has created a reasoning/instruct hybrid.&lt;br /&gt; Details at the repo, along with credits and links.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ADDED:&lt;/strong&gt;&lt;br /&gt; - 1 example generation at repo&lt;br /&gt; - special instructions on how to control &amp;quot;instruct&amp;quot; or &amp;quot;thinking&amp;quot; modes.&lt;/p&gt; &lt;p&gt;GGUF quants are now available.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt;&lt;br /&gt; Working on a Heretic (&amp;quot;uncensored&amp;quot;) tune of this next.&lt;/p&gt; &lt;p&gt;DavidAU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous_Fix_5526"&gt; /u/Dangerous_Fix_5526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0uuqt/happy_new_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T03:41:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0x8ci</id>
    <title>Software FP8 for GPUs without hardware support - 3x speedup on memory-bound operations</title>
    <updated>2026-01-01T06:03:27+00:00</updated>
    <author>
      <name>/u/Venom1806</name>
      <uri>https://old.reddit.com/user/Venom1806</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got tired of my RTX 3050 not supporting FP8, so I built a workaround. Packs lower-precision values into FP32 using bitwise operations + Triton kernels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;: 3x faster on memory-bound operations (GEMV, FlashAttention)&lt;/p&gt; &lt;p&gt;Works on any GPU - RTX 30/20 series, older cards without native FP8 support. Early stage but functional. Open to feedback.&lt;/p&gt; &lt;p&gt;&lt;a href="https://towardsdatascience.com/breaking-the-hardware-barrier-software-fp8-for-older-gpus/"&gt;Article Link&lt;/a&gt; | &lt;a href="https://github.com/SuriyaaMM/feather"&gt;Github Link&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Venom1806"&gt; /u/Venom1806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0x8ci/software_fp8_for_gpus_without_hardware_support_3x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T06:03:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0zk1u</id>
    <title>DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections</title>
    <updated>2026-01-01T08:35:29+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt; &lt;img alt="DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections" src="https://b.thumbs.redditmedia.com/lonIaVTlKZO_iYxmJPzoCCxeHd37wnAoXeHPfGhOUqA.jpg" title="DeepSeek new paper: mHC: Manifold-Constrained Hyper-Connections" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.24880"&gt;https://arxiv.org/abs/2512.24880&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bovsed0x8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe"&gt;https://preview.redd.it/bovsed0x8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=e292dc415f7fda8b1211ffe34864bb25ed4f32fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g9986afz8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b"&gt;https://preview.redd.it/g9986afz8pag1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fe031ea160ebff21a0dc46196d3dcf3b1b58548b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zk1u/deepseek_new_paper_mhc_manifoldconstrained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T08:35:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q0zst6</id>
    <title>Upstage Solar-Open-100B Public Validation</title>
    <updated>2026-01-01T08:52:25+00:00</updated>
    <author>
      <name>/u/PerPartes</name>
      <uri>https://old.reddit.com/user/PerPartes</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"&gt; &lt;img alt="Upstage Solar-Open-100B Public Validation" src="https://preview.redd.it/w789uyo0cpag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b12cc1f1a7783b1d9a40f9851206fbcdbdbf782" title="Upstage Solar-Open-100B Public Validation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Official company counterstrike to the claim that Solar 100B Open is just finetuned GLM-Air-4.5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerPartes"&gt; /u/PerPartes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/w789uyo0cpag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q0zst6/upstage_solaropen100b_public_validation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T08:52:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
