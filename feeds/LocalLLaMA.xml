<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-01T08:37:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1olany1</id>
    <title>What should I do with my old pc?</title>
    <updated>2025-11-01T00:28:51+00:00</updated>
    <author>
      <name>/u/__Jes__</name>
      <uri>https://old.reddit.com/user/__Jes__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Upgraded my pc, the old one is an HP Omen with an RTX 3070 8GB in it. Unfortunately it is an HP branded card and only worth $200 resale. Thoughts on what to do with it? Any fun projects I could dedicate this machine to run?&lt;/p&gt; &lt;p&gt;My first thought was mining but according to calculators I would be loosing money on power costs. Second thought was a Silly Tavern vector provider instance, but seems overkill. Third thought was just say f#ck it and run Folding@Home for fun. Or tear out the GPU and put it in my new pc and run a multi-gpu setup.&lt;/p&gt; &lt;p&gt;Just spitballing ideas here, what would you do with a spare 8gb gpu &amp;amp; 48gb ram.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Jes__"&gt; /u/__Jes__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olany1/what_should_i_do_with_my_old_pc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olany1/what_should_i_do_with_my_old_pc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olany1/what_should_i_do_with_my_old_pc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T00:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9a0j</id>
    <title>The wiki plugin should come pre-install for LM-studio</title>
    <updated>2025-10-31T23:21:34+00:00</updated>
    <author>
      <name>/u/OldEffective9726</name>
      <uri>https://old.reddit.com/user/OldEffective9726</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"&gt; &lt;img alt="The wiki plugin should come pre-install for LM-studio" src="https://b.thumbs.redditmedia.com/QSFjhpfepCS36ds4CnQEAwJzByr0jfThPsaIAms2mOk.jpg" title="The wiki plugin should come pre-install for LM-studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's so helpful. The command line is:&lt;/p&gt; &lt;p&gt;&lt;em&gt;lms get lmstudio/wikipedia&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bsuhcw846jyf1.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a1c7b6269d354920963a9fd92baa8d44081f3cc"&gt;https://preview.redd.it/bsuhcw846jyf1.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a1c7b6269d354920963a9fd92baa8d44081f3cc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldEffective9726"&gt; /u/OldEffective9726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9ajs</id>
    <title>A Mobile Strix Halo!!!</title>
    <updated>2025-10-31T23:22:16+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/onexplayer-onexfly-apex-ryzen-ai-max-395-handheld-announced-costs-1200-2200-features-85wh-external-battery-and-liquid-cooling"&gt;https://videocardz.com/newz/onexplayer-onexfly-apex-ryzen-ai-max-395-handheld-announced-costs-1200-2200-features-85wh-external-battery-and-liquid-cooling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All you need is a keyboard!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9ajs/a_mobile_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9ajs/a_mobile_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9ajs/a_mobile_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1olijqq</id>
    <title>Training with RTX6000 Pro</title>
    <updated>2025-11-01T08:21:41+00:00</updated>
    <author>
      <name>/u/HerrHruby</name>
      <uri>https://old.reddit.com/user/HerrHruby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone here have experience doing single- or multi-node training with the RTX6000 Pro? The Blackwell one with 96GB VRAM. How does it compare to the usual A100/H100/H200 cards? &lt;/p&gt; &lt;p&gt;I care mostly about RL using something like verl, but also interested to know how these GPUs perform for inference and SFT.&lt;/p&gt; &lt;p&gt;The nice thing about these cards are that you can buy three or four nodes for the cost of a single H200 node…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HerrHruby"&gt; /u/HerrHruby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olijqq/training_with_rtx6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olijqq/training_with_rtx6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olijqq/training_with_rtx6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:21:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol3o4l</id>
    <title>Upcoming Coding Models?</title>
    <updated>2025-10-31T19:19:17+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything coming soon or later? Speculations/rumors?&lt;/p&gt; &lt;p&gt;Nothing from Llama for now. I think same on Microsoft too(or Phi new version coming?).&lt;/p&gt; &lt;p&gt;Would be great to have Coder (Both MOE &amp;amp; Dense) models like below.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM Coder - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok0i7q/comment/nm82eq2/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;We're currently exploring the possibility of small coding models...&lt;/a&gt; &amp;amp; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok0i7q/comment/nm984bv/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Thanks for the feedback on the demand for the Coding models and FIM models. We are constantly thinking about what makes the most sense to release next.&lt;/a&gt; - LFM @ AMA&lt;/li&gt; &lt;li&gt;Granite Coder 30B - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oichb7/comment/nm7n2lc/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;It is not currently on the roadmap, but we will pass this request along to the Research team! - IBM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GPT OSS 2.0 Coder 30B - MXFP4 quant would be 17GB size without quantization(As their 20B model is just 12GB)&lt;/li&gt; &lt;li&gt;Seed OSS Coder 30B - Unfortunately I can't even touch their Seed-OSS-36B model with my 8GB VRAM :(&lt;/li&gt; &lt;li&gt;Gemma Coder 20-30B - It seems many from this sub waiting for Gemma4 release as I found multiple threads in last 2 months from my search.&lt;/li&gt; &lt;li&gt;GLM Coder 30B - So many fans for GLM &amp;amp; GLM Air. Great to have small MOE in 30B size.&lt;/li&gt; &lt;li&gt;Mistral Coder - Their recent Magistral &amp;amp; Devstral using by people on coding/FIM stuff. But not suitable for Poor GPU club as those are Dense models. It's been long time that they released a small model in 12B size. Mistral-Nemo-Instruct-2407 is more than a year old.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recent models related to Coding we got through this sub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;internlm/JanusCoder-8B - 8B text model based on Qwen3-8B&lt;/li&gt; &lt;li&gt;internlm/JanusCoder-14B - 14B text model based on Qwen3-14B&lt;/li&gt; &lt;li&gt;internlm/JanusCoderV-7B - 7B multimodal model based on Qwen2.5-VL-7B&lt;/li&gt; &lt;li&gt;internlm/JanusCoderV-8B - 8B multimodal model based on InternVL3.5-8B&lt;/li&gt; &lt;li&gt;nvidia/Qwen3-Nemotron-32B-RLBFF&lt;/li&gt; &lt;li&gt;inference-net/Schematron-3B&lt;/li&gt; &lt;li&gt;Tesslate/UIGEN-FX-Agentic-32B - Trained on Qwen3 32B&lt;/li&gt; &lt;li&gt;Tesslate/WEBGEN-Devstral-24B - Trained on Devstral 24B&lt;/li&gt; &lt;li&gt;Kwaipilot/KAT-Dev&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3o4l/upcoming_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3o4l/upcoming_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3o4l/upcoming_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T19:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok3xie</id>
    <title>200+ pages of Hugging Face secrets on how to train an LLM</title>
    <updated>2025-10-30T16:11:22+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt; &lt;img alt="200+ pages of Hugging Face secrets on how to train an LLM" src="https://preview.redd.it/s12qz4k3w9yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c78fbb2faf8b6857633466eb7cf34609898a57" title="200+ pages of Hugging Face secrets on how to train an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's elie from the hugging face pre-training team! We're very excited to share our new blog (book?) that cover the full pipeline: pre-training, post-training and infra. 200+ pages of what worked, what didn’t, and how to make it run reliably :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook"&gt;https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope yall will enjoy it, don't hesitate to make feedback on the community tab :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s12qz4k3w9yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T16:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ole210</id>
    <title>Case for 4 3090s?</title>
    <updated>2025-11-01T03:31:18+00:00</updated>
    <author>
      <name>/u/WyattTheSkid</name>
      <uri>https://old.reddit.com/user/WyattTheSkid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all. I have 2 3090 TI (founders edition) a gigabyte 3090, and a evga 3090. I was thinking about getting the phanteks enthoo pro 2 server edition but I’m worried they won’t all fit. I don’t want to deal with liquid cooling and I don’t want a mining frame. I converted my “normie” machine into a workstation and I would like to keep it in a box under my desk. Please give me suggestions. Can’t afford anything ridiculous but like $300~ USD is okay &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WyattTheSkid"&gt; /u/WyattTheSkid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ole210/case_for_4_3090s/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ole210/case_for_4_3090s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ole210/case_for_4_3090s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T03:31:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol1vdm</id>
    <title>Mergekit has been re-licensed under GNU LGPL v3</title>
    <updated>2025-10-31T18:08:57+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kinda self-promo ? But also feel it's worth shouting out anyways, mergekit is back to LGPL license!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/arcee-ai/mergekit"&gt;https://github.com/arcee-ai/mergekit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.arcee.ai/blog/mergekit-returns-to-its-roots"&gt;https://www.arcee.ai/blog/mergekit-returns-to-its-roots&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oku9og</id>
    <title>Why the hype around ultra small models like Granite4_350m? What are the actual use cases for these models?</title>
    <updated>2025-10-31T13:14:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get that small models can run on edge devices, but what are people actually planning on using a 350m parameter model for in the real world? I’m just really curious as to what use cases developers see these fitting into vs. using 1b, 4b, or 8b? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T13:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol5zx5</id>
    <title>Adding a RTX 5080 into a 2U server with OcuLink</title>
    <updated>2025-10-31T20:54:28+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"&gt; &lt;img alt="Adding a RTX 5080 into a 2U server with OcuLink" src="https://a.thumbs.redditmedia.com/Vdh4_ZPajSdyshlNSjO227Vq2w1bY7lOkx2ZQxIWik8.jpg" title="Adding a RTX 5080 into a 2U server with OcuLink" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As my P40 was no more up to the task, I needed a better card in my main server. The main issues were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It does not fit (NVidia makes sure of that)&lt;/li&gt; &lt;li&gt;It is really hard to get a correct power cable for these new cards. I was afraid to damage my server motherboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the alternative I found was to setup a OcuLink dock with its own power supply. I used the MINIS FORUM DEG1 (because it was the one I could get overnight at Amazon). I put a 4 port OcuLink card in the server (I can use bifurcation later for more GPU).&lt;/p&gt; &lt;p&gt;Performance are great: 140+ token/s with Mistral.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol5zx5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T20:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oliavy</id>
    <title>Finding GPUs for LLaMA training was painful — so I built Market01 (demo live now 🔥)</title>
    <updated>2025-11-01T08:04:48+00:00</updated>
    <author>
      <name>/u/Pleasant_Ear3991</name>
      <uri>https://old.reddit.com/user/Pleasant_Ear3991</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks 👋&lt;br /&gt; I’ve been training small LLaMA models lately, and got tired of hopping between &lt;strong&gt;RunPod, LambdaLabs, and&lt;/strong&gt; &lt;a href="http://Vast.ai"&gt;&lt;strong&gt;Vast.ai&lt;/strong&gt;&lt;/a&gt; just to find an available GPU that fits my budget.&lt;/p&gt; &lt;p&gt;So I built [&lt;strong&gt;Market01&lt;/strong&gt;]() — a &lt;strong&gt;chat assistant&lt;/strong&gt; that helps you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Find and compare GPUs across multiple providers&lt;/li&gt; &lt;li&gt; See real-time prices and availability&lt;/li&gt; &lt;li&gt;(Soon) Deploy or raise quotas directly from chat&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can &lt;strong&gt;try the demo version right now&lt;/strong&gt; — it’s live and working.&lt;br /&gt; When you use the demo, you can &lt;strong&gt;join the waitlist directly inside&lt;/strong&gt; to unlock full access.&lt;/p&gt; &lt;p&gt;🎁 Early users get free credits and early deployment access.&lt;/p&gt; &lt;p&gt;👉 Try it here: [market01.tech/chat-assistant]()&lt;/p&gt; &lt;p&gt;Curious — what GPU setups are you all using for your LLaMA runs right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pleasant_Ear3991"&gt; /u/Pleasant_Ear3991 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oliavy/finding_gpus_for_llama_training_was_painful_so_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oliavy/finding_gpus_for_llama_training_was_painful_so_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oliavy/finding_gpus_for_llama_training_was_painful_so_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:04:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1olfys6</id>
    <title>[Open Source] We deployed numerous agents in production and ended up building our own GenAI framework</title>
    <updated>2025-11-01T05:26:15+00:00</updated>
    <author>
      <name>/u/Traditional-Let-856</name>
      <uri>https://old.reddit.com/user/Traditional-Let-856</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After building and deploying GenAI solutions in production, we got tired of fighting with bloated frameworks, debugging black boxes, and dealing with vendor lock-in. Often the support for open source LLM inference frameworks like Ollama, or vLLM is missing.&lt;/p&gt; &lt;p&gt;So we built Flo AI - a Python framework that actually respects your time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem We Solved&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM frameworks give you two bad options:&lt;/p&gt; &lt;p&gt;Too much abstraction → You have no idea why your agent did what it did&lt;/p&gt; &lt;p&gt;Too little structure → You're rebuilding the same patterns over and over.&lt;/p&gt; &lt;p&gt;We wanted something that's predictable, debuggable, customizable, composable and production-ready from day one.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What Makes FloAI Different&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;OpenSource LLMs are first class citizens, we support vLLM, Ollama out of the box&lt;/strong&gt; &lt;strong&gt;Built-in&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observability&lt;/strong&gt;: OpenTelemetry tracing out of the box. See exactly what your agents are doing, track token usage, and debug performance issues without adding extra libraries. (pre-release)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-Agent Collaboration (Arium)&lt;/strong&gt;: Agents can call other specialized agents. Build a trip planner that coordinates weather experts and web researchers - it just works. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Composable by Design&lt;/strong&gt;: Ability to build larger and larger agentic workflows, by composable smaller units &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Customizable via YAML&lt;/strong&gt;: Design your agents using for YAMLs for easy customizations and prompt changes, as well as flo changes &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vendor Agnostic&lt;/strong&gt;: Start with OpenAI, switch to Claude, add Gemini - same code. We support OpenAI, Anthropic, Google, Ollama, vLLM and VertextAI. (more coming soon)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why We're Sharing This&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We believe in less abstraction, more control.&lt;/p&gt; &lt;p&gt;If you’ve ever been frustrated by frameworks that hide too much or make you reinvent the wheel, Flo AI might be exactly what you’re looking for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🐙 GitHub: &lt;a href="https://github.com/rootflo/flo-ai"&gt;https://github.com/rootflo/flo-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Documentation: &lt;a href="https://flo-ai.rootflo.ai"&gt;https://flo-ai.rootflo.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We Need Your Feedback&lt;/p&gt; &lt;p&gt;We’re actively building and would love your input: What features would make this useful for your use case?&amp;amp; What pain points do you face with current LLM frameworks?&lt;/p&gt; &lt;p&gt;Found a bug? We respond fast!&lt;/p&gt; &lt;p&gt;⭐ Star us on GitHub if this resonates — it really helps us know we’re solving real problems.&lt;/p&gt; &lt;p&gt;Happy to chat or answer questions in the comments! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Traditional-Let-856"&gt; /u/Traditional-Let-856 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olfys6/open_source_we_deployed_numerous_agents_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olfys6/open_source_we_deployed_numerous_agents_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olfys6/open_source_we_deployed_numerous_agents_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T05:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1olf2lx</id>
    <title>Built a Structured Prompt Builder for Local LLMs — Design, Save &amp; Export Prompts Visually (Open-Source + Browser-Only)</title>
    <updated>2025-11-01T04:30:12+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olf2lx/built_a_structured_prompt_builder_for_local_llms/"&gt; &lt;img alt="Built a Structured Prompt Builder for Local LLMs — Design, Save &amp;amp; Export Prompts Visually (Open-Source + Browser-Only)" src="https://a.thumbs.redditmedia.com/0RgzgsvwCjNecHJ0oNnAcz3oXyQ2WIXm6vfnBD4Zzy8.jpg" title="Built a Structured Prompt Builder for Local LLMs — Design, Save &amp;amp; Export Prompts Visually (Open-Source + Browser-Only)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;br /&gt; I made a small open-source tool called &lt;strong&gt;Structured Prompt Builder&lt;/strong&gt; — a simple web app to design, save, and export prompts in a clean, structured format.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lets you build prompts using fields like &lt;em&gt;role&lt;/em&gt;, &lt;em&gt;task&lt;/em&gt;, &lt;em&gt;tone&lt;/em&gt;, &lt;em&gt;steps&lt;/em&gt;, &lt;em&gt;constraints&lt;/em&gt;, etc.&lt;/li&gt; &lt;li&gt;Live preview in &lt;strong&gt;Markdown, JSON, or YAML&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Save prompts locally in your browser (no backend, full privacy).&lt;/li&gt; &lt;li&gt;Copy or download prompts with one click.&lt;/li&gt; &lt;li&gt;Optional Gemini API support for polishing your prompt text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it’s useful:&lt;/strong&gt;&lt;br /&gt; If you work with local LLMs, this helps you stay organized and consistent. Instead of messy free-form prompts, you can build clear reusable templates that integrate easily with your scripts or configs.&lt;/p&gt; &lt;p&gt;Try it here: &lt;a href="https://structured-prompt-builder.vercel.app/?utm_source=chatgpt.com"&gt;structured-prompt-builder.vercel.app&lt;/a&gt;&lt;br /&gt; Source: &lt;a href="https://github.com/Siddhesh2377/structured-prompt-builder?utm_source=chatgpt.com"&gt;github.com/Siddhesh2377/structured-prompt-builder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1olf2lx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olf2lx/built_a_structured_prompt_builder_for_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olf2lx/built_a_structured_prompt_builder_for_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T04:30:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1olildc</id>
    <title>How much VRAM do you have?</title>
    <updated>2025-11-01T08:24:47+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/poll/1olildc"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olildc/how_much_vram_do_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T08:24:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol2oxw</id>
    <title>Unbound In-Character Reasoning Model - Apollo-V0.1-4B-Thinking</title>
    <updated>2025-10-31T18:40:35+00:00</updated>
    <author>
      <name>/u/AllThingsIntel</name>
      <uri>https://old.reddit.com/user/AllThingsIntel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An experimental model with many of its creative inhibitions lifted. Its internal reasoning process adapts to the persona you assign (via the system prompt), allowing it to explore a wider spectrum of themes. This is a V0.1 preview for testing. More refined versions (non-reasoning variants as well) are planned. Follow for updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllThingsIntel"&gt; /u/AllThingsIntel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AllThingsIntel/Apollo-V0.1-4B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol006s</id>
    <title>Drummer's Rivermind™ 24B v1 - A spooky future for LLMs, Happy Halloween!</title>
    <updated>2025-10-31T16:58:28+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt; &lt;img alt="Drummer's Rivermind™ 24B v1 - A spooky future for LLMs, Happy Halloween!" src="https://external-preview.redd.it/cJYTAXorc18tRHqzBZaT1gjlzMY9WdV4LW05HQehJqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fbf968d35d15b5cb941693631cde3f0e872cd42" title="Drummer's Rivermind™ 24B v1 - A spooky future for LLMs, Happy Halloween!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The older brother of &lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1"&gt;https://huggingface.co/TheDrummer/Rivermind-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-24B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:58:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9cai</id>
    <title>Gerbil: An open source desktop app for running LLMs locally</title>
    <updated>2025-10-31T23:24:38+00:00</updated>
    <author>
      <name>/u/i_got_the_tools_baby</name>
      <uri>https://old.reddit.com/user/i_got_the_tools_baby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt; &lt;img alt="Gerbil: An open source desktop app for running LLMs locally" src="https://external-preview.redd.it/eno0eDlyNTQ3anlmMYROUy6ynC042_ngFZye_M2RHtEEKYtBWVHWeI_XUuyY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4340ab3f6ed93d4f4c1334de594505f6406eaea" title="Gerbil: An open source desktop app for running LLMs locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_got_the_tools_baby"&gt; /u/i_got_the_tools_baby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/096u8qj06jyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol6qlk</id>
    <title>MiniMax M2 Llama.cpp support merged</title>
    <updated>2025-10-31T21:25:47+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"&gt; &lt;img alt="MiniMax M2 Llama.cpp support merged" src="https://external-preview.redd.it/zIJQO0qHVJjeSmNiAhS1pkkPDhsRxWN-vwvuIbrWY3s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=190f42b7e585927328c4767cedf1cc2831910bf7" title="MiniMax M2 Llama.cpp support merged" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aight, the MiniMax M2 support is officially in. &lt;/p&gt; &lt;p&gt;Remember that there is no support for the chat format yet, and for a good reason - there is currently no easy way to deal with the &amp;quot;interleaved&amp;quot; thinking format of the model. &lt;/p&gt; &lt;p&gt;I'm currently considering the intermediate solution - since the model makers recommend passing the thinking blocks back to the model, I'm thinking of leaving all the thinking tags inside the normal content and letting clients parse it (so no `reasoning_content`), but add parsing for tool calls (and possibly reinject the starting `&amp;lt;think&amp;gt;` tag).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/0de0a01576772032008a689afc4d7c80685074c4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T21:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1okppxs</id>
    <title>Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?</title>
    <updated>2025-10-31T09:13:34+00:00</updated>
    <author>
      <name>/u/Successful-Newt1517</name>
      <uri>https://old.reddit.com/user/Successful-Newt1517</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt; &lt;img alt="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" src="https://preview.redd.it/ej8yokr9zeyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd9469bd25e003a0703a4ea86d079690b75d94f" title="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, what's going on? Are Chinese models saving American startups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Newt1517"&gt; /u/Successful-Newt1517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ej8yokr9zeyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T09:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oky3un</id>
    <title>Qwen3-VL GGUF!</title>
    <updated>2025-10-31T15:46:07+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have not tried any yet, multiple other Veterans have uploaded GGUF Quants, linking to unsloth for their guide and all available models from 2B-32B.&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth"&gt;Hugging Face Unsloth&lt;/a&gt;&lt;br /&gt; &lt;a href="https://docs.unsloth.ai/models/qwen3-vl-run-and-fine-tune"&gt;Unsloth Guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7ri8</id>
    <title>support for Minimax M2 has been merged into llama.cpp</title>
    <updated>2025-10-31T22:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt; &lt;img alt="support for Minimax M2 has been merged into llama.cpp" src="https://external-preview.redd.it/I_x1QIcREivfRZWw6RyYObzeaj8mdE6DXTQR3kx1F5I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47afb9f21e4fd695dd9279346c35a27194d0369b" title="support for Minimax M2 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16831"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7vwv</id>
    <title>For any LLM enthusiast in Finland you have decommission Super Computer equipped with 96 Nvidia A100 40Gb Pcie , if you live nearby Kajaani try contact company maybe you get them on discount ;)</title>
    <updated>2025-10-31T22:16:56+00:00</updated>
    <author>
      <name>/u/DeathRabit86</name>
      <uri>https://old.reddit.com/user/DeathRabit86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/"&gt;https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;“CSC is preparing the end-of-life plans for Mahti and Puhti in line with scientific needs and sustainability principles. In practice, we’ll donate the systems to suitable recipients for continued use or spare parts”, says&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Sebastian von Alfthan&lt;/em&gt;&lt;/strong&gt;*, Development Manager at CSC.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathRabit86"&gt; /u/DeathRabit86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:16:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol8bfx</id>
    <title>New AI workstation</title>
    <updated>2025-10-31T22:36:32+00:00</updated>
    <author>
      <name>/u/faileon</name>
      <uri>https://old.reddit.com/user/faileon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt; &lt;img alt="New AI workstation" src="https://b.thumbs.redditmedia.com/lEmf1RVi5jrp-eyNGLmi5QJsRATbD0Vj35rKsMda9_Q.jpg" title="New AI workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to fit in 4x RTX 3090 to a Phantek Server/Workstation case. Scores each card for roughly 800$. The PCIE riser on picture was too short (30cm) and had to be replaced with a 60cm one. The vertical mount is for Lian LI case, but manages to hook it up in the Phantek too. Mobo is ASRock romed8-2t, CPU is EPYC 7282 from eBay for 75$. So far it's a decent machine especially considering the cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faileon"&gt; /u/faileon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol8bfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol30e5</id>
    <title>qwen2.5vl:32b is saving me $1400 from my HOA</title>
    <updated>2025-10-31T18:53:04+00:00</updated>
    <author>
      <name>/u/jedsk</name>
      <uri>https://old.reddit.com/user/jedsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over this year I finished putting together my local LLM machine with a quad 3090 setup. Built a few workflows with it but like most of you, just wanted to experiment with local models and for the sake of burning tokens lol.&lt;/p&gt; &lt;p&gt;Then in July, my ceiling got damaged from an upstairs leak. HOA says &amp;quot;not our problem.&amp;quot; I'm pretty sure they're wrong, but proving it means reading their governing docs (20 PDFs, +1,000 pages total).&lt;/p&gt; &lt;p&gt;Thought this was the perfect opportunity to create an actual useful app and do bulk PDF processing with vision models. Spun up qwen2.5vl:32b on Ollama and built a pipeline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF → image conversion → markdown&lt;/li&gt; &lt;li&gt;Vision model extraction&lt;/li&gt; &lt;li&gt;Keyword search across everything&lt;/li&gt; &lt;li&gt;Found 6 different sections proving HOA was responsible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took about 3-4 hours to process everything locally. Found the proof I needed on page 287 of their Declaration. Sent them the evidence, but ofc still waiting to hear back.&lt;/p&gt; &lt;p&gt;Finally justified the purpose of this rig lol.&lt;/p&gt; &lt;p&gt;Anyone else stumble into unexpectedly practical uses for their local LLM setup? Built mine for experimentation, but turns out it's perfect for sensitive document processing you can't send to cloud services.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jedsk"&gt; /u/jedsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz8qz</id>
    <title>pewdiepie dropped a video about running local ai</title>
    <updated>2025-10-31T16:28:56+00:00</updated>
    <author>
      <name>/u/topfpflanze187</name>
      <uri>https://old.reddit.com/user/topfpflanze187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt; &lt;img alt="pewdiepie dropped a video about running local ai" src="https://external-preview.redd.it/WddxiFHLc3dMB9LBPGHmNWXXrzglB78uxpSOk1Y4d6E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d41e205151bfdcec37d1be377abc09d05a02773e" title="pewdiepie dropped a video about running local ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topfpflanze187"&gt; /u/topfpflanze187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qw4fDU18RcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0i7q</id>
    <title>AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo</title>
    <updated>2025-10-30T14:00:16+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" src="https://b.thumbs.redditmedia.com/hb9XoRhxPRhv8ljYkjnVbJWgnClXeLGMxbG1TEDCwos.jpg" title="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We’re super excited to host this week’s AMA! &lt;/p&gt; &lt;p&gt;Join us and ask your questions directly to the human minds behind all things Liquid: Liquid Foundational Models, the Liquid Edge AI Platform (LEAP) for model customization and deployment, and Apollo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our participants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks &lt;a href="https://www.reddit.com/user/jamarks13/"&gt;u/jamarks13&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith &lt;a href="https://www.reddit.com/user/jimmysmith1919/"&gt;u/jimmysmith1919&lt;/a&gt; (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne &lt;a href="https://www.reddit.com/user/mlabonne/"&gt;u/mlabonne&lt;/a&gt; (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes &lt;a href="https://www.reddit.com/user/Wide-Half-7982/"&gt;u/Wide-Half-7982&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak &lt;a href="https://www.reddit.com/user/ankebananke/"&gt;u/ankebananke&lt;/a&gt; (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur Böök &lt;a href="https://www.reddit.com/user/ManWithARedFace/"&gt;u/ManWithARedFace&lt;/a&gt; (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev &lt;a href="https://www.reddit.com/user/ykhrustalev/"&gt;u/ykhrustalev&lt;/a&gt; (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena &lt;a href="https://www.reddit.com/user/humble_pi_314/"&gt;u/humble_pi_314&lt;/a&gt; (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca &lt;a href="https://www.reddit.com/user/Ok-Safe-5316/"&gt;u/Ok-Safe-5316&lt;/a&gt; (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale &lt;a href="https://www.reddit.com/user/anthony-liquidai/"&gt;u/anthony-liquidai&lt;/a&gt; (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo &lt;a href="https://www.reddit.com/user/PauLabartaBajo/"&gt;u/PauLabartaBajo&lt;/a&gt; (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from &lt;strong&gt;10 AM - 1 PM PST&lt;/strong&gt;. The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&amp;gt; &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198"&gt;https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks to everyone who participated in this AMA. It was a pleasure.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://discord.gg/DFU3WQeaYD"&gt;Join the Liquid AI Discord Community&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:00:16+00:00</published>
  </entry>
</feed>
