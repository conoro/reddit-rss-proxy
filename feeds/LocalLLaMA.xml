<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-10T11:35:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1piasv8</id>
    <title>MagicQuant - Hybrid Evolution GGUF (TPS boosts, precision gains, full transparency)</title>
    <updated>2025-12-09T15:47:38+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a system that evolves &lt;strong&gt;hybrid GGUF quantizations&lt;/strong&gt; to automatically find the best tensor level mix for any model. It‚Äôs called &lt;strong&gt;MagicQuant&lt;/strong&gt;, and the whole idea is simple:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stop guessing quant types. Let the math decide the optimal configuration.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MagicQuant runs survival rounds, epsilon-greedy exploration, precision-loss scoring, TPS benchmarking, and a ton of tensor-group heuristics to evolve better (and sometimes &lt;em&gt;way&lt;/em&gt; better) GGUFs than standard baselines.&lt;/p&gt; &lt;p&gt;And the results so far have been amazing.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Example: Seed-OSS 36B&lt;/h2&gt; &lt;p&gt;This is one of the crazier results I‚Äôve gotten so far.&lt;/p&gt; &lt;p&gt;The best Q4-range baseline was &lt;strong&gt;IQ4_NL&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;19.31 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;27.70 TPS&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.1076% precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MagicQuant evolved a hybrid at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;18.95 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32.00 TPS&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.2709% precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slightly smaller&lt;/li&gt; &lt;li&gt;&lt;strong&gt;+15.5% faster&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~75% LESS precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This hybrid: &lt;a href="https://huggingface.co/magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF"&gt;mxfp4_moe-EHQKOUD-IQ4NL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the kind of thing MagicQuant keeps finding.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;MagicQuant Hybrids for Seed OSS 36B&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model_name&lt;/th&gt; &lt;th&gt;file_size_gb&lt;/th&gt; &lt;th&gt;bench_tps&lt;/th&gt; &lt;th&gt;avg_prec_loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-HK-B16-EO-Q5K-QUD-Q8_0&lt;/td&gt; &lt;td&gt;39.71&lt;/td&gt; &lt;td&gt;17.73&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.0213%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-O-MXFP4-EHQKUD-Q8_0&lt;/td&gt; &lt;td&gt;35.78&lt;/td&gt; &lt;td&gt;18.72&lt;/td&gt; &lt;td&gt;0.0272%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-E-B16-D-IQ4NL-KOU-Q6K-HQ-Q8_0&lt;/td&gt; &lt;td&gt;28.02&lt;/td&gt; &lt;td&gt;24.27&lt;/td&gt; &lt;td&gt;0.1768%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-EHQKOUD-Q6K&lt;/td&gt; &lt;td&gt;27.63&lt;/td&gt; &lt;td&gt;23.34&lt;/td&gt; &lt;td&gt;0.2037%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;mxfp4_moe-EHQKOUD-IQ4NL&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;18.95&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;32.00&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.2709%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-HQKU-IQ4NL-EOD-MXFP4&lt;/td&gt; &lt;td&gt;18.66&lt;/td&gt; &lt;td&gt;26.90&lt;/td&gt; &lt;td&gt;0.7098%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MXFP4_MOE&lt;/td&gt; &lt;td&gt;17.90&lt;/td&gt; &lt;td&gt;20.46&lt;/td&gt; &lt;td&gt;2.7338%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Baseline Reference (for comparison)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model_name&lt;/th&gt; &lt;th&gt;file_size_gb&lt;/th&gt; &lt;th&gt;bench_tps&lt;/th&gt; &lt;th&gt;avg_prec_loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;BF16&lt;/td&gt; &lt;td&gt;67.35&lt;/td&gt; &lt;td&gt;11.48&lt;/td&gt; &lt;td&gt;0.0000%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q8_0&lt;/td&gt; &lt;td&gt;35.78&lt;/td&gt; &lt;td&gt;17.77&lt;/td&gt; &lt;td&gt;0.0272%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q6_K&lt;/td&gt; &lt;td&gt;27.63&lt;/td&gt; &lt;td&gt;22.95&lt;/td&gt; &lt;td&gt;0.2037%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q5_K&lt;/td&gt; &lt;td&gt;23.84&lt;/td&gt; &lt;td&gt;22.04&lt;/td&gt; &lt;td&gt;0.2923%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ4_NL&lt;/td&gt; &lt;td&gt;19.31&lt;/td&gt; &lt;td&gt;27.70&lt;/td&gt; &lt;td&gt;1.1076%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MXFP4_MOE&lt;/td&gt; &lt;td&gt;17.90&lt;/td&gt; &lt;td&gt;20.46&lt;/td&gt; &lt;td&gt;2.7338%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q4_K_M&lt;/td&gt; &lt;td&gt;20.27&lt;/td&gt; &lt;td&gt;26.65&lt;/td&gt; &lt;td&gt;2.9161%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;MagicQuant compares everything against these to determine the ‚Äúwinner.‚Äù&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What MagicQuant keeps discovering&lt;/h2&gt; &lt;p&gt;Different architectures respond to quantization very differently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some &lt;em&gt;love&lt;/em&gt; MXFP4.&lt;/li&gt; &lt;li&gt;Some prefer IQ4_NL.&lt;/li&gt; &lt;li&gt;Some models randomly explode in quality on Q5_K.&lt;/li&gt; &lt;li&gt;Seed-OSS ditched most baselines entirely.&lt;/li&gt; &lt;li&gt;Apriel 1.5-15B? That model is a complete gremlin, it loves &lt;strong&gt;Q5_K&lt;/strong&gt; more than anything else I‚Äôve thrown at it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MagicQuant isn‚Äôt about producing hybrids for the sake of hybrids. &lt;strong&gt;MagicQuant is the verdict, whatever wins stays.&lt;/strong&gt; Sometimes that‚Äôs a hybrid. Sometimes the baseline reigns king. Sometimes Q6_K beats Q8_0 in both TPS and precision. Sometimes Q4_K_M outperforms IQ4_NL on &lt;em&gt;certain models.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Everything depends on the architecture.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Philosophically&lt;/h2&gt; &lt;p&gt;I‚Äôm honestly tired of downloading Q8/Q6/Q5/Q4 files with &lt;strong&gt;no benchmarks&lt;/strong&gt;. If a quant is bigger, slower, &lt;em&gt;and&lt;/em&gt; more precision loss, why use it? If a smaller quant loses 5% precision, I want to &lt;strong&gt;see that number&lt;/strong&gt; before downloading.&lt;/p&gt; &lt;p&gt;MagicQuant is my attempt at making quantization:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;empirical&lt;/li&gt; &lt;li&gt;transparent&lt;/li&gt; &lt;li&gt;repeatable&lt;/li&gt; &lt;li&gt;and actually &lt;em&gt;useful&lt;/em&gt; for the community&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Every model will always include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmark TPS&lt;/li&gt; &lt;li&gt;precision loss scoring&lt;/li&gt; &lt;li&gt;file size&lt;/li&gt; &lt;li&gt;the full hybrid naming breakdown&lt;/li&gt; &lt;li&gt;data sets&lt;/li&gt; &lt;li&gt;methodology&lt;/li&gt; &lt;li&gt;raw results&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything is open and reproducible.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;HuggingFace Collection&lt;/h2&gt; &lt;p&gt;All MagicQuant releases live here: &lt;a href="https://huggingface.co/collections/magiccodingman/magic-quant"&gt;https://huggingface.co/collections/magiccodingman/magic-quant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More hybrids are already in the pipeline.&lt;/p&gt; &lt;p&gt;Right now a dense 4B model takes ~2-3 hours to run. A 30B MOE takes ~24 hours (MOE takes ~double as long due to sensitivity). My prediction engine has to build sample data until confidence is high enough that it can properly predict hybrids. Some models are easier than others. Sine dense models need only 46-55 samples, while others need 120 samples, while some need more or less. The engine figures that out.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Documentation / Wiki&lt;/h2&gt; &lt;p&gt;Full documentation, philosophy, naming scheme, methodology, and technical breakdown: &lt;a href="https://github.com/magiccodingman/MagicQuant-Wiki"&gt;https://github.com/magiccodingman/MagicQuant-Wiki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MagicQuant is still evolving, but the results so far have been extremely promising and the more models I run, the weirder and more interesting the quantization patterns become.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;But if you have any suggestions, requests for MagicQuant models, holes to poke, I'm all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1piz6vx</id>
    <title>Devstral-Small-2-24B q6k entering loop (both Unsloth and Bartowski) (llama.cpp)</title>
    <updated>2025-12-10T10:24:12+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying both:&lt;/p&gt; &lt;p&gt;Unsloth: Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL.gguf&lt;br /&gt; and&lt;br /&gt; Bartowki: mistralai_Devstral-Small-2-24B-Instruct-2512-Q6_K_L.gguf&lt;/p&gt; &lt;p&gt;and with a context of 24k (still have enough VRAM available) for a 462 tokens prompt, it enters a loop after a few tokens.&lt;/p&gt; &lt;p&gt;I tried different options with llama-server (llama.cpp), which I started with the Unsloth's recommended one and then I started making some changes, leaving it as clean as possible, but I still get a loop.&lt;/p&gt; &lt;p&gt;I managed to get an answer, once, with Bartowski one with the very basic settings (flags) but although it didn't enter a loop, it did repeated the same line 3 times.&lt;/p&gt; &lt;p&gt;The cleaner one was (also tried temp: 0.15):&lt;/p&gt; &lt;p&gt;--threads -1 --cache-type-k q8_0 --n-gpu-layers 99 --temp 0.2 -c 24786&lt;/p&gt; &lt;p&gt;Is Q6 broken? or are there any new flags that need to be added?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piz6vx/devstralsmall224b_q6k_entering_loop_both_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piz6vx/devstralsmall224b_q6k_entering_loop_both_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piz6vx/devstralsmall224b_q6k_entering_loop_both_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T10:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pix9yv</id>
    <title>Voice-AI Game for MCP-looking for feedback &amp; Support!</title>
    <updated>2025-12-10T08:17:08+00:00</updated>
    <author>
      <name>/u/Economy_Situation_41</name>
      <uri>https://old.reddit.com/user/Economy_Situation_41</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://youtu.be/7VWELEUr-wE"&gt;https://youtu.be/7VWELEUr-wE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey everyone! For the MCP hackathon, our team built Voice Sementle ‚Äî a voice-only guessing game where AI scores two things:&lt;/p&gt; &lt;p&gt;1Ô∏è‚É£ Did you say the &lt;strong&gt;correct line&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;2Ô∏è‚É£ Did you &lt;strong&gt;deliver it like the original&lt;/strong&gt; (tone, timing, vibe)?&lt;/p&gt; &lt;p&gt;It uses our acoustic embeddings model to combine &lt;strong&gt;semantic&lt;/strong&gt; + &lt;strong&gt;performance&lt;/strong&gt; similarity.&lt;/p&gt; &lt;p&gt;The online demo is temporarily &lt;strong&gt;video-only&lt;/strong&gt; due to hackathon submission freeze ‚Äî but we would &lt;strong&gt;love genuine feedback&lt;/strong&gt; on the idea and the scoring approach.&lt;/p&gt; &lt;p&gt;And if you like the direction ‚Üí ‚≠ê like means a lot for our team üôè&lt;/p&gt; &lt;p&gt;Feedback and Support on our linkedin or X post would be much appreciated!&lt;br /&gt; üëâ &lt;a href="https://www.linkedin.com/posts/traceychoi911_mcpinaction-buildwithmcp-gradio-activity-7400151841759494145-lA8U?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAC-3H-cBXdaYHCxd_4zJDXUFtvmruQDZw78"&gt;https://www.linkedin.com/posts/traceychoi911_mcpinaction-buildwithmcp-gradio-activity-7400151841759494145-lA8U?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAC-3H-cBXdaYHCxd_4zJDXUFtvmruQDZw78&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ&lt;a href="https://x.com/ChoiTracey24876/status/1994388486699245591?s=20"&gt;https://x.com/ChoiTracey24876/status/1994388486699245591?s=20&lt;/a&gt;&lt;br /&gt; üëâ&lt;a href="https://huggingface.co/spaces/MCP-1st-Birthday/VoiceSementle"&gt;https://huggingface.co/spaces/MCP-1st-Birthday/VoiceSementle&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Situation_41"&gt; /u/Economy_Situation_41 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pix9yv/voiceai_game_for_mcplooking_for_feedback_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pix9yv/voiceai_game_for_mcplooking_for_feedback_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pix9yv/voiceai_game_for_mcplooking_for_feedback_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T08:17:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1phujwo</id>
    <title>Check on lil bro</title>
    <updated>2025-12-09T01:25:42+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt; &lt;img alt="Check on lil bro" src="https://preview.redd.it/s8rfm29bz26g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99684b39e5571f190bf37b141c34049e9f79cc1" title="Check on lil bro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s8rfm29bz26g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T01:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1piklt8</id>
    <title>Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md</title>
    <updated>2025-12-09T21:59:11+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piklt8/linux_foundation_announces_the_formation_of_the/"&gt; &lt;img alt="Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md" src="https://external-preview.redd.it/LOt13m09jkanBZNSJY_12A-wBrJw_RiimpI3OBp-Oqo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2624b56d80a1d938eb72c11fc7efad59cb3ee98" title="Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piklt8/linux_foundation_announces_the_formation_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piklt8/linux_foundation_announces_the_formation_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T21:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pib8z9</id>
    <title>New ways to roast people in the AI era</title>
    <updated>2025-12-09T16:04:20+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the AI era, we can update the way we roast people.&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;nerd,&amp;quot; try saying &amp;quot;benchmaxxed.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;brain-dead,&amp;quot; try saying &amp;quot;pruned/quantized.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;no brain,&amp;quot; try saying &amp;quot;low params count.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;didn't study,&amp;quot; try saying &amp;quot;undertrained.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;only knows book knowledge,&amp;quot; try saying &amp;quot;overfitted.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;boring and dull,&amp;quot; try saying &amp;quot;safetymaxxed.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;slow to react,&amp;quot; try saying &amp;quot;slow prompt processing/token generation.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;clumsy,&amp;quot; try saying &amp;quot;poor tool use performance.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;talks nonsense endlessly,&amp;quot; try saying &amp;quot;temperature too high/missing EOS.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;speaks gibberish,&amp;quot; try saying &amp;quot;template config error/topK sampling error.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;disobedient,&amp;quot; try saying &amp;quot;non-instruct base model.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;doesn't think with the brain,&amp;quot; try saying &amp;quot;non-thinking instruct model.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;poor memory,&amp;quot; try saying &amp;quot;low context window.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;easily fooled,&amp;quot; try saying &amp;quot;vulnerable to prompt injection.&amp;quot;&lt;/p&gt; &lt;p&gt;It's normal if you don't understand any of this. If you understand all of these, go outside and touch some grass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T16:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1piduwm</id>
    <title>I wanted audiobooks of stories that don't exist - so I built an app to read them to me</title>
    <updated>2025-12-09T17:40:47+00:00</updated>
    <author>
      <name>/u/DigiJoe79</name>
      <uri>https://old.reddit.com/user/DigiJoe79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt; &lt;img alt="I wanted audiobooks of stories that don't exist - so I built an app to read them to me" src="https://b.thumbs.redditmedia.com/522Sef2MfP-ZR7uASytJh9L2_m1rO3FnZwVv7ALL4eE.jpg" title="I wanted audiobooks of stories that don't exist - so I built an app to read them to me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After multiple weeks of work, I'm excited to share my passion project: an open-source desktop app for creating audiobooks using AI text-to-speech with voice cloning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The story behind it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I wanted to listen to fan fiction and web novels that don't have audiobook versions. Commercial TTS services are expensive and therer workflos is not focused on audiobook generation. So I built my own solution that runs completely locally on your machine - no subscriptions, no cloud, your data stays private.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean drag &amp;amp; drop interface for organizing chapters and segments&lt;/li&gt; &lt;li&gt;Supports multiple TTS engines (XTTS, Chatterbox) - swap them as you like&lt;/li&gt; &lt;li&gt;Built-in quality check using Whisper to catch mispronunciations and Silero-VAD for audio issues&lt;/li&gt; &lt;li&gt;Import full books in .md Format and use spaCy for autosegmentation&lt;/li&gt; &lt;li&gt;Pronunciation rules to fix words the AI struggles with&lt;/li&gt; &lt;li&gt;Engine template for hassle-free adding of new engines as they get released&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The tech (for those interested):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tauri 2 desktop app with React frontend and Python backend. Each AI engine runs in isolation, so you can mix and match without dependency hell. Works on Windows, Linux, and macOS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current state:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just released v1.0.1. It's stable and I use it daily for my own audiobooks. Still a solo project, but fully functional.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/DigiJoe79/AudioBook-Maker"&gt;https://github.com/DigiJoe79/AudioBook-Maker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community. What features would you find most useful?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkfvltynt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c87dff564a0357f01b044949c234d7a5236afe"&gt;https://preview.redd.it/bkfvltynt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c87dff564a0357f01b044949c234d7a5236afe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckbtf3mpt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45701f8412e10717377fafcc8d57a2e41ef51cbf"&gt;https://preview.redd.it/ckbtf3mpt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45701f8412e10717377fafcc8d57a2e41ef51cbf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiJoe79"&gt; /u/DigiJoe79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T17:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1piw1sw</id>
    <title>Best small LLM for general advice?</title>
    <updated>2025-12-10T06:59:14+00:00</updated>
    <author>
      <name>/u/Qxz3</name>
      <uri>https://old.reddit.com/user/Qxz3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not as a coding assistant or puzzle solver, but for general discussions about life, health, relationships etc. &lt;/p&gt; &lt;p&gt;So far my best bet has been Gemma 3. Have fiddled a bit with Ministral 3 but it tends to produce answers that are long, lack focus, rely too much on bullet points and speaks the dreaded AI slop language. Perhaps better prompting would help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qxz3"&gt; /u/Qxz3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw1sw/best_small_llm_for_general_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw1sw/best_small_llm_for_general_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piw1sw/best_small_llm_for_general_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T06:59:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi8z74</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:35:50+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/h9d1fvb7w66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd2f0eef8f43dfd10528e31cb2b9efd46b87bfb" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9d1fvb7w66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pil53r</id>
    <title>AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision</title>
    <updated>2025-12-09T22:19:37+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"&gt; &lt;img alt="AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision" src="https://b.thumbs.redditmedia.com/3AFqcVzBcIqkShsFZeKC1B1EUCe3mUgvy1_R1nSDzVc.jpg" title="AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twice as fast at running 8-bit transformers than the previous generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pil53r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T22:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1piykjp</id>
    <title>Built a visual debugger for my local agents because I was lost in JSON, would you use this?</title>
    <updated>2025-12-10T09:44:56+00:00</updated>
    <author>
      <name>/u/AdVivid5763</name>
      <uri>https://old.reddit.com/user/AdVivid5763</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"&gt; &lt;img alt="Built a visual debugger for my local agents because I was lost in JSON, would you use this?" src="https://preview.redd.it/ymvtn22clc6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75a1585a27e4b916d4d1714f215f2080267441e6" title="Built a visual debugger for my local agents because I was lost in JSON, would you use this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run local LLM agents with tools / RAG. When a run broke, my workflow was basically: &lt;/p&gt; &lt;p&gt;rerun with more logging, diff JSON, and guess which step actually screwed things up. Slow and easy to miss.&lt;/p&gt; &lt;p&gt;So I hacked a small tool for myself: it takes a JSON trace and shows the run as a graph + timeline. &lt;/p&gt; &lt;p&gt;Each step is a node with the prompt / tool / result, and there‚Äôs a basic check that highlights obvious logic issues (like using empty tool results as if they were valid). &lt;/p&gt; &lt;p&gt;It‚Äôs already way faster for me than scrolling logs.&lt;/p&gt; &lt;p&gt;Long-term, I‚Äôd like this to become a proper ‚Äúcognition debugger‚Äù layer on top of whatever logs/traces you already have, especially for non-deterministic agents where ‚Äúwhat happened?‚Äù is not obvious.&lt;/p&gt; &lt;p&gt;It‚Äôs model-agnostic as long as the agent can dump a trace.&lt;/p&gt; &lt;p&gt;I‚Äôm mostly curious if anyone else here hits the same pain. &lt;/p&gt; &lt;p&gt;If this sounds useful, tell me what a debugger like this must show for you to actually use it. &lt;/p&gt; &lt;p&gt;I‚Äôll drop a demo link in the comments üîó.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdVivid5763"&gt; /u/AdVivid5763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymvtn22clc6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T09:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1piw3mp</id>
    <title>VSCode Copilot Autocomplete with local / custom models</title>
    <updated>2025-12-10T07:02:05+00:00</updated>
    <author>
      <name>/u/mter24</name>
      <uri>https://old.reddit.com/user/mter24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;I am the creator of this issue: &lt;a href="https://github.com/microsoft/vscode/issues/263535"&gt;https://github.com/microsoft/vscode/issues/263535&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is basically a feature request that allows developers to use their own LLMs for autocomplete.&lt;/p&gt; &lt;p&gt;Now I need now &lt;strong&gt;your help&lt;/strong&gt;. If you think this could be a useful feature please &lt;strong&gt;upvote this issue&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mter24"&gt; /u/mter24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw3mp/vscode_copilot_autocomplete_with_local_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw3mp/vscode_copilot_autocomplete_with_local_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piw3mp/vscode_copilot_autocomplete_with_local_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T07:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir03u</id>
    <title>New ASR modelÔºöGLM-ASR-Nano-2512 1.5B Supports Mandarin/English/Cantonese and more</title>
    <updated>2025-12-10T02:34:29+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"&gt; &lt;img alt="New ASR modelÔºöGLM-ASR-Nano-2512 1.5B Supports Mandarin/English/Cantonese and more" src="https://b.thumbs.redditmedia.com/cXXf5ug_0MFMT80R9gQEIam1ZEahJsu9lpAYtVW8efI.jpg" title="New ASR modelÔºöGLM-ASR-Nano-2512 1.5B Supports Mandarin/English/Cantonese and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j5os8wcdga6g1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c976a30e633fb6c7dfcdc9bf2cdceaaf3798438"&gt;https://preview.redd.it/j5os8wcdga6g1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c976a30e633fb6c7dfcdc9bf2cdceaaf3798438&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-ASR-Nano-2512"&gt;https://huggingface.co/zai-org/GLM-ASR-Nano-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-ASR-Nano-2512&lt;/strong&gt;&lt;br /&gt; 1.5B&lt;br /&gt; Supports Mandarin/English/Cantonese and more&lt;br /&gt; Clearly recognizes whisper/quiet speech&lt;br /&gt; Excels in noisy, overlapping environments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1piabn8</id>
    <title>Devstral-Small-2-24B-Instruct-2512 on Hugging Face</title>
    <updated>2025-12-09T15:29:19+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt; &lt;img alt="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" src="https://external-preview.redd.it/9AtiZkI9TGGX4HUjb1yXt2_pTwLgjJScmnM7q3ZVgpw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98ba88a60b440a866a778f583a15081cf6838a4" title="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pizl8t</id>
    <title>Built a GGUF memory &amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL</title>
    <updated>2025-12-10T10:49:12+00:00</updated>
    <author>
      <name>/u/ittaboba</name>
      <uri>https://old.reddit.com/user/ittaboba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"&gt; &lt;img alt="Built a GGUF memory &amp;amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL" src="https://external-preview.redd.it/cnpqZXU4dXV0YzZnMYh73P_j0pnSesQyyRb8l_QLx5gX0RNmxMe-sw-YRlmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc6af8aed5100a84c198cee502219f22065dba7" title="Built a GGUF memory &amp;amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Built a small utility that estimates how much memory you need to run GGUF models locally, plus an approximate tok/sec based on your machine (Apple Silicon only atm, more hardware soon) and task (e.g. ask a generic question, write a draft, etc.).&lt;/p&gt; &lt;p&gt;You can select a model from a dropdown or paste any direct GGUF URL from HF. The tool parses the model metadata (size, layers, hidden dimensions, KV cache, etc.) and uses that to estimate:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total memory needed for weights + KV cache + activations + overhead&lt;/li&gt; &lt;li&gt;Expected latency and generation speed (tok/sec)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Demo: &lt;a href="https://manzoni.app/llm_calculator"&gt;https://manzoni.app/llm_calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code + formulas: &lt;a href="https://github.com/gems-platforms/gguf-memory-calculator"&gt;https://github.com/gems-platforms/gguf-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, edge cases, or bug reports (e.g. comparisons against your actual tokens/sec to tighten the estimates). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ittaboba"&gt; /u/ittaboba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qahbzltutc6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1piq11p</id>
    <title>Mac with 64GB? Try Qwen3-Next!</title>
    <updated>2025-12-10T01:50:12+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried qwen3-next-80b-a3b-thinking-4bit using mlx-lm on my M3 Max with 64GB, and the quality is excellent with very reasonable speed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing: 7122 tokens at 295.24 tokens per second&lt;/li&gt; &lt;li&gt;Text generation: 1222 tokens at 10.99 tokens per second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also tried qwen3-next-80b-a3b-thinking-q4_K_M.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing: 7123 tokens at 1015.80 tokens per second&lt;/li&gt; &lt;li&gt;Text generation: 1253 tokens at 65.84 tokens per second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;People mentioned that Qwen3-next is not optimized for speed with gguf yet.&lt;/p&gt; &lt;p&gt;I can also load a full 32K context to the GPU using 57 GB, and I can push and allocate up to 58 of 64 GB without any freezing.&lt;/p&gt; &lt;p&gt;I think this model really pushes a 64 GB Mac to its limits in the best way!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piq11p/mac_with_64gb_try_qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piq11p/mac_with_64gb_try_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piq11p/mac_with_64gb_try_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T01:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pigb3i</id>
    <title>DeepSeek-V3.2-REAP: 508B and 345B checkpoints</title>
    <updated>2025-12-09T19:14:58+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, to get us all in the holiday mood we're continuing to REAP models, this time we got DeepSeek-V3.2 for you at 25% and 50% compression: &lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're pretty excited about this one and are working to get some agentic evals for coding and beyond on these checkpoints soon. Enjoy and stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T19:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pihu16</id>
    <title>bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF</title>
    <updated>2025-12-09T20:10:40+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt; &lt;img alt="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" src="https://external-preview.redd.it/Y9-VSUeByMali_oSJcuRXft1g3dj7X6u-O2vcI7YtII.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9d7830dcda85560752ed0db90867edc36dddee1" title="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T20:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir8jc</id>
    <title>3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)</title>
    <updated>2025-12-10T02:45:16+00:00</updated>
    <author>
      <name>/u/Electronic-Fly-6465</name>
      <uri>https://old.reddit.com/user/Electronic-Fly-6465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"&gt; &lt;img alt="3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)" src="https://preview.redd.it/nzlqosj6ia6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c781c9e64faa5592f78eccce290897f05ec44256" title="3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a visualisation tool that displays the internal layer dynamics of GPT-2 Small during a single forward pass.&lt;/p&gt; &lt;p&gt;It renders:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;per-head vector deltas&lt;/li&gt; &lt;li&gt;PCA-3 residual stream projections&lt;/li&gt; &lt;li&gt;angle + magnitude differences between heads&lt;/li&gt; &lt;li&gt;stabilisation behaviour in early layers&lt;/li&gt; &lt;li&gt;the sharp directional transition around layers 9‚Äì10&lt;/li&gt; &lt;li&gt;the consistent ‚Äúanchoring / braking‚Äù effect in layer 11&lt;/li&gt; &lt;li&gt;two-prompt comparison mode (‚ÄúI like X‚Äù vs ‚ÄúI like Y‚Äù)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything in the video is generated from real measurements ‚Äî no mock data or animation shortcuts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video (22 min raw walkthrough):&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://youtu.be/dnWikqNAQbE"&gt;https://youtu.be/dnWikqNAQbE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just sharing the prototype.&lt;br /&gt; If anyone working on interpretability or visualisation wants to discuss it, I‚Äôm around.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Fly-6465"&gt; /u/Electronic-Fly-6465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nzlqosj6ia6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1piumvw</id>
    <title>bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face</title>
    <updated>2025-12-10T05:37:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"&gt; &lt;img alt="bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/0jH917Owmr7iKrMXvyA0r05fWobE4kYASAkKFjbuamg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8285c76b1c0f5aa8e695fdb89fac6a270b922a8" title="bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it was gated before, finally it's available&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T05:37:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9q3t</id>
    <title>Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI</title>
    <updated>2025-12-09T15:05:54+00:00</updated>
    <author>
      <name>/u/YanderMan</name>
      <uri>https://old.reddit.com/user/YanderMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt; &lt;img alt="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YanderMan"&gt; /u/YanderMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1piux9z</id>
    <title>Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters</title>
    <updated>2025-12-10T05:54:13+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt; &lt;img alt="Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters" src="https://b.thumbs.redditmedia.com/02J_w_E-jjdBogT1atigLYYoW24wILytNHbOE75U0FI.jpg" title="Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4hs2rkx0gb6g1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1005ca9567e6c31bb0b23f8a3e9473959507757"&gt;Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed for real-world complexity, it outperforms OpenAI Whisper V3 on multiple benchmarks while maintaining a compact size.&lt;/p&gt; &lt;p&gt;Key capabilities include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Exceptional Dialect Support:&lt;/strong&gt; Beyond standard Mandarin and English, the model is highly optimized for &lt;strong&gt;Cantonese&lt;/strong&gt; and other dialects, effectively bridging the gap in dialectal speech recognition.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Volume Speech Robustness:&lt;/strong&gt; Specifically trained for &lt;strong&gt;&amp;quot;Whisper/Quiet Speech&amp;quot;&lt;/strong&gt; scenarios. It captures and accurately transcribes extremely low-volume audio that traditional models often miss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA Performance:&lt;/strong&gt; Achieves the &lt;strong&gt;lowest average error rate (4.10)&lt;/strong&gt; among comparable open-source models, showing significant advantages in Chinese benchmarks (Wenet Meeting, Aishell-1, etc..)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/zai-org/GLM-ASR-Nano-2512"&gt;https://huggingface.co/zai-org/GLM-ASR-Nano-2512&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T05:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1piwx9u</id>
    <title>Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores</title>
    <updated>2025-12-10T07:54:13+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt; &lt;img alt="Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores" src="https://external-preview.redd.it/G7Gcft3BKg57j9czqWCQwa5R5JjWhPW-BbTK-PcJb1k.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0aad896057c96dc1a1c9470d0d19ea461ad37b1" title="Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arcee AI quietly dropped a pretty interesting model last week: Trinity Mini, a 26B-parameter sparse MoE with only 3B active parameters&lt;/p&gt; &lt;p&gt;A few things that actually stand out beyond the headline numbers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;128 experts, 8 active + 1 shared expert&lt;/strong&gt;. Routing is noticeably more stable than typical 2/4-expert MoEs, especially on math and tool-calling tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10T curated tokens&lt;/strong&gt;, built on top of the Datology dataset stack. The math/code additions seem to actually matter, the model holds state across multi-step reasoning better than most mid-size MoEs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;128k context&lt;/strong&gt; without the ‚Äúfalls apart after 20k tokens‚Äù behavior a lot of open models still suffer from.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong zero-shot scores&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;84.95% MMLU (ZS)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;92.10% Math-500&lt;/strong&gt; These would be impressive even for a 70B dense model. For a 3B-active MoE, it‚Äôs kind of wild.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to experiment with it, it‚Äôs available via &lt;a href="https://clarifai.com/arcee_ai/AFM/models/trinity-mini"&gt;Clarifai&lt;/a&gt; and also &lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Curious what you all think after trying it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1m97sj3f0c6g1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddc01b2fd25dddd2c9f1e45965cbff3e58cccdf"&gt;https://preview.redd.it/1m97sj3f0c6g1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddc01b2fd25dddd2c9f1e45965cbff3e58cccdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T07:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir555</id>
    <title>So what's the closest open-source thing to claude code?</title>
    <updated>2025-12-10T02:40:56+00:00</updated>
    <author>
      <name>/u/According-Ebb917</name>
      <uri>https://old.reddit.com/user/According-Ebb917</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just wondering which coding agent/multi-agent system out there is the closest to claude code? Particularly in terms of good scaffolding (subagents, skills, proper context engineering, etc...) and works well with a set of models? I feel like there's a new one everyday but I can't seem to figure out which work and which don't&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Ebb917"&gt; /u/According-Ebb917 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
