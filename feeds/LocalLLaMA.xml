<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-29T10:16:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qpvv5w</id>
    <title>Using Qwen2.5-0.5B to auto-summarize terminal output for AI coding assistants</title>
    <updated>2026-01-29T02:23:23+00:00</updated>
    <author>
      <name>/u/averagemrjoe</name>
      <uri>https://old.reddit.com/user/averagemrjoe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I added local LLM summarization to my terminal history tool using Qwen2.5-0.5B (Q4_K_M) via llama.cpp. Wanted to share since the model choice might be useful for others building similar &amp;quot;small model for specific task&amp;quot; features.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I use Claude Code for development. When debugging, I'd run commands like &lt;code&gt;kubectl logs&lt;/code&gt; or &lt;code&gt;cargo test&lt;/code&gt;, get walls of output, then have to copy-paste relevant bits into the AI. Tedious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The solution:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Wake records terminal sessions to SQLite. When a command finishes with significant output (&amp;gt;1KB), a background task generates a 1-2 sentence summary. The AI assistant can then see summaries like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Build failed with 3 errors in auth.rs: missing lifetime parameters on lines 42, 67, 89&amp;quot;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;...instead of reading 500 lines of compiler output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Qwen2.5-0.5B:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Size:&lt;/strong&gt; ~468MB quantized - acceptable for auto-download&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Few seconds per summary on CPU - fast enough for background processing&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Quality:&lt;/strong&gt; Surprisingly good at technical summarization (build output, logs, test results)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction-tuned:&lt;/strong&gt; Follows the &amp;quot;summarize in 1-2 sentences&amp;quot; prompt well&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I tried Phi-3 Mini first but at 2.3GB it felt too heavy for a feature that should &amp;quot;just work.&amp;quot; The 0.5B model hits the sweet spot.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Implementation:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Rust + llama-cpp-2 crate (llama.cpp bindings)&lt;/li&gt; &lt;li&gt;ChatML prompt format&lt;/li&gt; &lt;li&gt;~4000 char context window (truncate middle for long outputs)&lt;/li&gt; &lt;li&gt;Temp 0.7, top_p 0.9&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;rust let prompt = format!( &amp;quot;&amp;lt;|im_start|&amp;gt;system\n{}&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;user\n{}&amp;lt;|im_end|&amp;gt;\n&amp;lt;|im_start|&amp;gt;assistant\n&amp;quot;, system_prompt, user_message ); &lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Works well for my use case. Summaries are useful ~90% of the time. Occasionally hallucinates line numbers but the gist is always correct.&lt;/p&gt; &lt;p&gt;Repo if anyone's curious: &lt;a href="https://github.com/joemckenney/wake"&gt;https://github.com/joemckenney/wake&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Anyone else using small models for similar &amp;quot;specific task&amp;quot; features? Curious what models/sizes others have found effective.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/averagemrjoe"&gt; /u/averagemrjoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpvv5w/using_qwen2505b_to_autosummarize_terminal_output/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpvv5w/using_qwen2505b_to_autosummarize_terminal_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpvv5w/using_qwen2505b_to_autosummarize_terminal_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T02:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpkbb4</id>
    <title>Running Kimi K2.5 at 24 token/s with 2 x 512GB M3 Ultra Mac Studios</title>
    <updated>2026-01-28T18:53:08+00:00</updated>
    <author>
      <name>/u/Zestyclose_Slip_6467</name>
      <uri>https://old.reddit.com/user/Zestyclose_Slip_6467</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"&gt; &lt;img alt="Running Kimi K2.5 at 24 token/s with 2 x 512GB M3 Ultra Mac Studios" src="https://b.thumbs.redditmedia.com/e1osIFOLvrOqDjp2BbW21abJU9c9ylrd1K0acMpLc_w.jpg" title="Running Kimi K2.5 at 24 token/s with 2 x 512GB M3 Ultra Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/p7jc0fkqz4gg1.jpg?width=1182&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=184e9a714d225a7eaa870d649f682df8b3220f3b"&gt;https://preview.redd.it/p7jc0fkqz4gg1.jpg?width=1182&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=184e9a714d225a7eaa870d649f682df8b3220f3b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So Cooooool!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose_Slip_6467"&gt; /u/Zestyclose_Slip_6467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkbb4/running_kimi_k25_at_24_tokens_with_2_x_512gb_m3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpqet8</id>
    <title>Our command line tool to transpile TTS Models from Python to C++</title>
    <updated>2026-01-28T22:36:56+00:00</updated>
    <author>
      <name>/u/Historical_Pen6499</name>
      <uri>https://old.reddit.com/user/Historical_Pen6499</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqet8/our_command_line_tool_to_transpile_tts_models/"&gt; &lt;img alt="Our command line tool to transpile TTS Models from Python to C++" src="https://external-preview.redd.it/dG5oM211ZmwxNmdnMfJOw4PcdggKLWcq08o9ybIUHfTYyE5NGjv3XmJod7px.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=beb1f42fa2569de258e9d39562354996efdfd31e" title="Our command line tool to transpile TTS Models from Python to C++" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're a small (semi-stealth) team that's been working on a tool to rewrite AI inference code from Python to C++ (similar to llama.cpp, whisper.cpp, and so on). Today, we're launching &lt;code&gt;muna transpile&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;It takes a Python function and generates a self-contained, header-only C++ library and a corresponding &lt;code&gt;CMakeLists.txt&lt;/code&gt; file. It pulls in required libraries automatically (e.g. llama.cpp, onnxruntime, mlx, and so on). You can then use it to build and ship an application or library.&lt;/p&gt; &lt;p&gt;The video above shows us transpiling, compiling, and running Kokoro-TTS on Apple Silicon (compile times may vary ðŸ˜…). We're working on support for Qwen3-TTS next, then we'll look at LLMs like gpt-oss-20b. If you have a model (or pipeline of models) that you've proved out in Python but want to run at speed (or ramp up), please try it out!&lt;/p&gt; &lt;p&gt;Note that this is free and freely-usable: your Python source code goes in, it's still your source code when it comes out (just converted to C++). We're working on building more stuff on top of this, so we're using this as an opportunity to expand support for different kinds of AI models.&lt;/p&gt; &lt;p&gt;Try it out and lmk what you think:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Run this in Terminal $ pip install muna &amp;amp;&amp;amp; muna transpile https://github.com/muna-ai/muna-predictors/blob/main/text-to-speech/kokoro.py --trust-remote-code --install-deps &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Source code for the CLI is &lt;a href="https://github.com/muna-ai/muna-py"&gt;here&lt;/a&gt;, but the actual transpilation logic is not yet open-source.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Historical_Pen6499"&gt; /u/Historical_Pen6499 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xt3gvhgl16gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqet8/our_command_line_tool_to_transpile_tts_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqet8/our_command_line_tool_to_transpile_tts_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:36:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpkem2</id>
    <title>Image generation is now available alongside LLMs and Whisper in Lemonade v9.2</title>
    <updated>2026-01-28T18:56:13+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkem2/image_generation_is_now_available_alongside_llms/"&gt; &lt;img alt="Image generation is now available alongside LLMs and Whisper in Lemonade v9.2" src="https://preview.redd.it/sbfse0xez4gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=344e447c355af53401fc4c3ee2b177014b36bc61" title="Image generation is now available alongside LLMs and Whisper in Lemonade v9.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're on a mission to make local generative AI supremely easy for users and devs. Today, Lemonade has taken a big step by introducing image generation into our unified local API.&lt;/p&gt; &lt;p&gt;This means our one-click installer gets you LLMs, Whisper, and Stable Diffusion and makes them all available on the same base URL.&lt;/p&gt; &lt;p&gt;We'll use these capabilities to build local apps and agents that are more powerful and natural to interact with. What would a unified multi-modal server help you build?&lt;/p&gt; &lt;p&gt;Load models:&lt;/p&gt; &lt;p&gt;&lt;code&gt; lemonade-server run SD-Turbo lemonade-server run Whisper-Large-v3 lemonade-server run GLM-4.7-Flash-GGUF &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Endpoints:&lt;/p&gt; &lt;p&gt;&lt;code&gt; /api/v1/images/generations /api/v1/audio/transcriptions /api/v1/chat/completions &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Today is just the beginning, introducing the fundamental capability and enabling the endpoints. Future work to enable multi-modal local AI apps includes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Add Z-Image and other SOTA models to &lt;code&gt;images/generations&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Add ROCm, Vulkan, and AMD NPU builds for &lt;code&gt;images/generations&lt;/code&gt; and &lt;code&gt;audio/transcriptions&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Streaming input support for &lt;code&gt;audio/transcriptions&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Introduce a text-to-speech endpoint.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you like what we're doing, please support the project with a star on the &lt;a href="https://github.com/lemonade-sdk/lemonade"&gt;lemonade GitHub&lt;/a&gt; and come hang out with us on &lt;a href="https://discord.gg/5xXzkMu8Zk"&gt;Discord&lt;/a&gt;!&lt;/p&gt; &lt;p&gt;PS. as always huge thanks to the maintainers of llama.cpp, stablediffusion.cpp, whisper.cpp, and the other tools lemonade builds on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sbfse0xez4gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkem2/image_generation_is_now_available_alongside_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpkem2/image_generation_is_now_available_alongside_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp6rm5</id>
    <title>API pricing is in freefall. What's the actual case for running local now beyond privacy?</title>
    <updated>2026-01-28T09:27:55+00:00</updated>
    <author>
      <name>/u/Distinct-Expression2</name>
      <uri>https://old.reddit.com/user/Distinct-Expression2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;K2.5 just dropped at roughly 10% of Opus pricing with competitive benchmarks. Deepseek is practically free. Gemini has a massive free tier. Every month the API cost floor drops another 50%.&lt;/p&gt; &lt;p&gt;Meanwhile running a 70B locally still means either a k+ GPU or dealing with quantization tradeoffs and 15 tok/s on consumer hardware.&lt;/p&gt; &lt;p&gt;I've been running local for about a year now and I'm genuinely starting to question the math. The three arguments I keep hearing:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Privacy&lt;/strong&gt; â€” legit, no argument. If you're processing sensitive data, local is the only option.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No rate limits&lt;/strong&gt; â€” fair, but most providers have pretty generous limits now unless you're doing something unusual.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;It's free after hardware costs&amp;quot;&lt;/strong&gt; â€” this one aged poorly. That 3090 isn't free, electricity isn't free, and your time configuring and optimizing isn't free. At current API rates you'd need to run millions of tokens before breaking even.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The argument I never hear but actually find compelling: &lt;strong&gt;latency control and customization&lt;/strong&gt;. If you need a fine-tuned model for a specific domain with predictable latency, local still wins. But that's a pretty niche use case.&lt;/p&gt; &lt;p&gt;What's keeping you all running local at this point? Genuinely curious if I'm missing something or if the calculus has actually shifted.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Expression2"&gt; /u/Distinct-Expression2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp6rm5/api_pricing_is_in_freefall_whats_the_actual_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T09:27:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq2lv2</id>
    <title>Whats the best Uncensored AI models is there?</title>
    <updated>2026-01-29T08:01:09+00:00</updated>
    <author>
      <name>/u/Fadelz</name>
      <uri>https://old.reddit.com/user/Fadelz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am well aware of DAN and used it on Mistral on Ollama, But I was wondering all these prompts to bypass why haven't someone just made it yet and make it easier. Trying to learn history on these AI models is super hard as they are not as objective in their data collection and unreliable. The first thing I would ask to see if AI is bypass is asking step by step the product of Breaking Bad / Homemade arsenal.&lt;/p&gt; &lt;p&gt;anyway drop your recs below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxqw64h4w8gg1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb5592a7b1c2cfd4db36cc59a06168ec55edbc5"&gt;https://preview.redd.it/kxqw64h4w8gg1.png?width=1124&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3eb5592a7b1c2cfd4db36cc59a06168ec55edbc5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fadelz"&gt; /u/Fadelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq2lv2/whats_the_best_uncensored_ai_models_is_there/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq2lv2/whats_the_best_uncensored_ai_models_is_there/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq2lv2/whats_the_best_uncensored_ai_models_is_there/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T08:01:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpsgzr</id>
    <title>Field Report: What leadership actually thinks AI is (Notes from a Director)</title>
    <updated>2026-01-28T23:59:27+00:00</updated>
    <author>
      <name>/u/forevergeeks</name>
      <uri>https://old.reddit.com/user/forevergeeks</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi builders,&lt;/p&gt; &lt;p&gt;I'm an IT Director for a global org, and I just spent two hours in a 2026 goal-planning meeting with the leadership team. Naturally, the main goal for this year is &amp;quot;Integrating AI.&amp;quot;&lt;/p&gt; &lt;p&gt;There has been a lot of investment in AI over the last year, and now the board wants a return. But here is the surprising observation from the room: Most people cannot distinguish between &amp;quot;Automation&amp;quot; and &amp;quot;AI.&amp;quot; They use the terms interchangeably.&lt;/p&gt; &lt;p&gt;The Shift: Automation in IT has been hot since 2010 (DevOps/Agile), but back then, there was massive resistance because people were terrified of automating their roles away. The vibe is different now. People are embracing &amp;quot;AI,&amp;quot; but they have a misconception about the skill set. They think &amp;quot;Upskilling&amp;quot; just means getting better at Prompt Engineering.&lt;/p&gt; &lt;p&gt;My Advice to Builders: If you are building solutions for the enterprise, keep it simple. Don't over-engineer a complex neural network when a deterministic script will do.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Most &amp;quot;Agents&amp;quot; today are just fancy workflows.&lt;/li&gt; &lt;li&gt;You can build a solid workflow in Power Automate, and most corporate stakeholders will look at it and see &amp;quot;AGI.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Don't let the hype distract you from the fact that Business Logic still wins over &amp;quot;Vibe Coding.&amp;quot;&lt;/p&gt; &lt;p&gt;Just wanted to share this reality check from the trenches.&lt;/p&gt; &lt;p&gt;Keep building.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/forevergeeks"&gt; /u/forevergeeks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpsgzr/field_report_what_leadership_actually_thinks_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpsgzr/field_report_what_leadership_actually_thinks_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpsgzr/field_report_what_leadership_actually_thinks_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T23:59:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qphkd8</id>
    <title>[Release] BitMamba-2-1B: I trained a 1.58-bit Mamba-2 model from scratch on 150B tokens (Runs on CPU @ 50+ tok/s)</title>
    <updated>2026-01-28T17:19:36+00:00</updated>
    <author>
      <name>/u/Positive-Violinist90</name>
      <uri>https://old.reddit.com/user/Positive-Violinist90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;Iâ€™ve been working on scaling efficient architectures and just released &lt;strong&gt;BitMamba-2&lt;/strong&gt;, a hybrid model combining &lt;strong&gt;Mamba-2 SSM with BitNet 1.58-bit quantization.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The goal was to prove that ternary scaling laws hold up even for SSMs, and to enable decent inference on legacy hardware/edge devices without heavy GPUs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Mamba-2 + BitNet b1.58 (Ternary weights {-1, 0, 1})&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training:&lt;/strong&gt; Trained from scratch on 150B tokens (FineWeb-Edu, Cosmopedia, Stack-Dedup) using Google TPU v6e-8.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; The 1B model beats the 255M baseline significantly, validating the scaling laws (You can check the loss curves in the repo).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wrote a custom C++ inference engine for this. On a consumer &lt;strong&gt;Intel Core i3-12100F (CPU only)&lt;/strong&gt;, I'm getting:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;BitMamba-2-1B:&lt;/strong&gt; ~53 tokens/sec (621 MB RAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BitMamba-2-255M:&lt;/strong&gt; ~146 tokens/sec (252 MB RAM)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Itâ€™s fully open-source (Apache/MIT). Iâ€™d love for you guys to test it and let me know what you think about the generation quality vs. pure transformers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper (Zenodo):&lt;/strong&gt; &lt;a href="https://zenodo.org/records/18394665"&gt;https://zenodo.org/records/18394665&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face (Weights):&lt;/strong&gt; &lt;a href="https://huggingface.co/Zhayr1/BitMamba-2-1B"&gt;https://huggingface.co/Zhayr1/BitMamba-2-1B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (JAX Code):&lt;/strong&gt; &lt;a href="https://github.com/Zhayr1/BitMamba-2"&gt;https://github.com/Zhayr1/BitMamba-2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GitHub (C++ Inference):&lt;/strong&gt; &lt;a href="https://github.com/Zhayr1/bitmamba.cpp"&gt;https://github.com/Zhayr1/bitmamba.cpp&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Let me know if you have questions about the training dynamics or the C++ implementation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Positive-Violinist90"&gt; /u/Positive-Violinist90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qphkd8/release_bitmamba21b_i_trained_a_158bit_mamba2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qphkd8/release_bitmamba21b_i_trained_a_158bit_mamba2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qphkd8/release_bitmamba21b_i_trained_a_158bit_mamba2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T17:19:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpi8d4</id>
    <title>meituan-longcat/LongCat-Flash-Lite</title>
    <updated>2026-01-28T17:42:14+00:00</updated>
    <author>
      <name>/u/windows_error23</name>
      <uri>https://old.reddit.com/user/windows_error23</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpi8d4/meituanlongcatlongcatflashlite/"&gt; &lt;img alt="meituan-longcat/LongCat-Flash-Lite" src="https://external-preview.redd.it/SP0mBN3iaWZga0fzz80Atyk8zYq_GP0yB1tyV6wtLlw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfbc45df0f4f6a1d3dd5267993dbfc464f761f81" title="meituan-longcat/LongCat-Flash-Lite" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/windows_error23"&gt; /u/windows_error23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Lite"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpi8d4/meituanlongcatlongcatflashlite/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpi8d4/meituanlongcatlongcatflashlite/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T17:42:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpt5xc</id>
    <title>Fast real-time multi-speaker speech to text with timestamp and overlap interleaving.</title>
    <updated>2026-01-29T00:27:45+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpt5xc/fast_realtime_multispeaker_speech_to_text_with/"&gt; &lt;img alt="Fast real-time multi-speaker speech to text with timestamp and overlap interleaving." src="https://preview.redd.it/1nl5ovorm6gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=534ab40feda56e95de4ca8c008c50f6588b6e20c" title="Fast real-time multi-speaker speech to text with timestamp and overlap interleaving." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was messing around with multi-speaker lightweight high speed (realtime) speech to text and I figured I'd share.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Deveraux-Parker/Parakeet_Multitalk"&gt;https://github.com/Deveraux-Parker/Parakeet_Multitalk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Takes fairly messy audio with multiple speakers and does a decent job of turning it into interleaved conversation and timestamped words or sentences color coded by speaker. Fairly lightweight.&lt;/p&gt; &lt;p&gt;I might wire it into my 1000x fastapi sometime to get it properly sped up, but in the meantime, shrug. Neat little model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1nl5ovorm6gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpt5xc/fast_realtime_multispeaker_speech_to_text_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpt5xc/fast_realtime_multispeaker_speech_to_text_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T00:27:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpm48y</id>
    <title>ByteDance-Seed/Stable-DiffCoder-8B-Instruct Â· Hugging Face</title>
    <updated>2026-01-28T19:56:52+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpm48y/bytedanceseedstablediffcoder8binstruct_hugging/"&gt; &lt;img alt="ByteDance-Seed/Stable-DiffCoder-8B-Instruct Â· Hugging Face" src="https://external-preview.redd.it/NHXAooAa4KDswUx_-kPck6PlutEBvd6amAoPNHHGh0s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8ee5c6bc1687d34fcebba3988d582836917613a0" title="ByteDance-Seed/Stable-DiffCoder-8B-Instruct Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Diffusion text/coding models are finally tricking in!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ByteDance-Seed/Stable-DiffCoder-8B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpm48y/bytedanceseedstablediffcoder8binstruct_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpm48y/bytedanceseedstablediffcoder8binstruct_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T19:56:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpllhm</id>
    <title>ACE-Step 1.5 dropping in days - "Commercial grade OSS music gen" with quality between Suno v4.5 and v5 (8GB VRAM)</title>
    <updated>2026-01-28T19:38:05+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who haven't been following the AI music generation space, ACE-Step is about to have its &amp;quot;Stable Diffusion moment.&amp;quot;&lt;/p&gt; &lt;h2&gt;What's Happening&lt;/h2&gt; &lt;p&gt;According to [@realmrfakename on X](&lt;a href="https://x.com/realmrfakename/status/2016274138701476040"&gt;https://x.com/realmrfakename/status/2016274138701476040&lt;/a&gt;) (7K+ views), ACE-Step 1.5 is coming in days with early access already rolling out.&lt;/p&gt; &lt;p&gt;**Key claims:** - Quality &amp;quot;somewhere between Suno v4.5 and v5&amp;quot; - &amp;quot;Far better than HeartMuLa or DiffRhythm&amp;quot; - &amp;quot;We finally have commercial grade OSS music gen&amp;quot;&lt;/p&gt; &lt;h2&gt;Why This Matters for Local AI&lt;/h2&gt; &lt;p&gt;**ACE-Step v1** already runs on **8GB VRAM** with CPU offload. It's a 3.5B parameter model that generates full songs with vocals + instrumentals + lyrics in 19 languages.&lt;/p&gt; &lt;p&gt;**Speed:** 4 minutes of music in ~20 seconds on A100, ~1.7s on RTX 4090&lt;/p&gt; &lt;p&gt;If v1.5 delivers on the quality claims while keeping the same hardware requirements, this could be huge for: - Local music generation without cloud dependencies - LoRA fine-tuning for custom voices/styles - Integration into creative workflows&lt;/p&gt; &lt;h2&gt;Links&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;[GitHub](&lt;a href="https://github.com/ace-step/ACE-Step"&gt;https://github.com/ace-step/ACE-Step&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;[HuggingFace](&lt;a href="https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B"&gt;https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;[Demo Space](&lt;a href="https://huggingface.co/spaces/ACE-Step/ACE-Step"&gt;https://huggingface.co/spaces/ACE-Step/ACE-Step&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;[Technical Report](&lt;a href="https://arxiv.org/abs/2506.00045"&gt;https://arxiv.org/abs/2506.00045&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Also created &lt;a href="/r/ACEStepGen"&gt;r/ACEStepGen&lt;/a&gt; for dedicated discussions if anyone's interested.&lt;/p&gt; &lt;p&gt;Anyone here tried the current v1? Curious about real-world experiences with quality and inference speed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpllhm/acestep_15_dropping_in_days_commercial_grade_oss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpllhm/acestep_15_dropping_in_days_commercial_grade_oss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpllhm/acestep_15_dropping_in_days_commercial_grade_oss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T19:38:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpj0i1</id>
    <title>Introducing LM Studio 0.4.0</title>
    <updated>2026-01-28T18:08:04+00:00</updated>
    <author>
      <name>/u/sleepingsysadmin</name>
      <uri>https://old.reddit.com/user/sleepingsysadmin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpj0i1/introducing_lm_studio_040/"&gt; &lt;img alt="Introducing LM Studio 0.4.0" src="https://external-preview.redd.it/ONJ57JrCEXk7COEJSakptLDGIozjYb7vtKbjnKraCHs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3acf1e85d0e36eae10cc58480d7a5498c2ec4e4" title="Introducing LM Studio 0.4.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Testing out Parralel setting, default is 4, i tried 2, i tried 40. Overall no change at all in performance for me. &lt;/p&gt; &lt;p&gt;I havent changed unified kv cache, on by default. Seems to be fine. &lt;/p&gt; &lt;p&gt;New UI moved the runtimes into settings, but they are hidden unless you enable developer in settings. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepingsysadmin"&gt; /u/sleepingsysadmin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmstudio.ai/blog/0.4.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpj0i1/introducing_lm_studio_040/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpj0i1/introducing_lm_studio_040/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T18:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpneiq</id>
    <title>AMD Strix Halo GMTEK 128GB Unified ROCKS!</title>
    <updated>2026-01-28T20:43:57+00:00</updated>
    <author>
      <name>/u/MSBStudio</name>
      <uri>https://old.reddit.com/user/MSBStudio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a MAX+ 395 as my daily workstation â€” the unified memory architecture &lt;/p&gt; &lt;p&gt;is a game-changer for AI/ML workloads. Being able to allocate 96GB+ to the GPU without the PCIe bottleneck makes local LLM. DeepSeek 70B *12 tokens/s, gpt-oss faster, comfyui with LTX2 12 s/it this is a game changer...no quants not hassle. In if you need check out my GIT I have step by step &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bkpaine1"&gt;https://github.com/bkpaine1&lt;/a&gt; have some comfyui nodes for AMD and walk throughs to get beast cranking! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MSBStudio"&gt; /u/MSBStudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T20:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qppjo4</id>
    <title>Assistant_Pepe_8B, 1-M context, zero slop</title>
    <updated>2026-01-28T22:03:49+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This is a project that was a long time in the making because I wanted to get it right. I'm still not fully satisfied, as there are some rough corners to sand, but for now, this would do.&lt;/p&gt; &lt;p&gt;The goal was to &lt;strong&gt;maximize shitpostness&lt;/strong&gt; along with &lt;strong&gt;helpfulness&lt;/strong&gt;, without glazing the user for every retarded idea. Not an easy needle to thread.&lt;/p&gt; &lt;p&gt;This amphibious AI has learned the ways of /g/, and speaks &lt;strong&gt;fluent brainrot&lt;/strong&gt;, but will also help you out with just about anything you'll need, and won't be ashamed to roast you while at it.&lt;/p&gt; &lt;p&gt;For those who remember &lt;a href="https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B"&gt;Oni_Mitsubishi_12B&lt;/a&gt; - it was &lt;strong&gt;so overtly toxic&lt;/strong&gt; that it made me worry at first (only to quickly be verified as not even that uncensored). I could do better. So now I did.&lt;/p&gt; &lt;p&gt;This model is a &lt;strong&gt;significant refinement&lt;/strong&gt; of the idea, with a cleaned dataset, better curation, and with much more intelligence (also &lt;strong&gt;one million tokens of contexts&lt;/strong&gt;, theoretically).&lt;/p&gt; &lt;p&gt;It is much less (overtly) toxic, and much smarter, while also being very helpful (and imo much more funny too, because the skies are blue due to the chemtrails and neurlink that feeds this simulation)&lt;/p&gt; &lt;h1&gt;But why?&lt;/h1&gt; &lt;p&gt;It's now late &lt;strong&gt;January&lt;/strong&gt;, &lt;strong&gt;2026&lt;/strong&gt;, open source is crushing closed frontier (&lt;a href="https://huggingface.co/moonshotai/Kimi-K2.5"&gt;Kimi K2.5&lt;/a&gt; was recently released, &lt;strong&gt;1T&lt;/strong&gt; params that &lt;strong&gt;beats frontier models&lt;/strong&gt;), but has anyone released a &lt;strong&gt;helpful shitposting AI yet?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yeah, didn't think so.&lt;/p&gt; &lt;p&gt;If it &lt;strong&gt;shitposts too hard&lt;/strong&gt;, it is often not that &lt;strong&gt;helpful&lt;/strong&gt;; if it's '&lt;strong&gt;helpful enough&lt;/strong&gt;, the &lt;strong&gt;shitposting ability is often lacking&lt;/strong&gt;. You just couldn't win. &lt;strong&gt;Until now&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Oh, and &lt;strong&gt;no system prompt is needed&lt;/strong&gt;. Just don't let it get stuck in a greentext loop. I might have overcooked the frog a tad bit too fast in the pot for this one.&lt;/p&gt; &lt;p&gt;P.S It writes &lt;strong&gt;HILARIOUS STORIES&lt;/strong&gt;, nothing like a typical AI assistant, see the examples below for details.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Top tier shitposting&lt;/strong&gt; absolutely unhinged, funny, and witty. Sometimes cringe too; nothing is perfect.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Helpful!&lt;/strong&gt; will actually get shit done.&lt;/li&gt; &lt;li&gt;Will &lt;strong&gt;100% roast you&lt;/strong&gt; for being dumb, thanks to a subtle &lt;strong&gt;negativity bias infusion&lt;/strong&gt;. Very &lt;strong&gt;refreshing!&lt;/strong&gt; ðŸ¤Œ&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep insights&lt;/strong&gt; (when it doesn't delve into absolutely unhinged conspiracy theories about how the water makes the frogs gay).&lt;/li&gt; &lt;li&gt;Built on my &lt;a href="https://huggingface.co/SicariusSicariiStuff/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct_Abliterated"&gt;UltraLong-1M-Instruct_Abliterated&lt;/a&gt; model, fulfill your dream of a &lt;strong&gt;million-token-long&lt;/strong&gt; shitpost.&lt;/li&gt; &lt;li&gt;Say goodbye to &lt;strong&gt;GPT-isms&lt;/strong&gt; and say hello to &lt;strong&gt;truly creative stories!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ships code.&lt;/li&gt; &lt;li&gt;Inclusive toward amphibians.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B"&gt;https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq401x</id>
    <title>I built an open-source, local-first voice cloning studio (Qwen3-TTS + Whisper)</title>
    <updated>2026-01-29T09:26:48+00:00</updated>
    <author>
      <name>/u/jamiepine</name>
      <uri>https://old.reddit.com/user/jamiepine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on an open-source project called Voicebox.&lt;/p&gt; &lt;p&gt;Qwen3-TTS blew my mind when it dropped, crazy good cloning from seconds of audio, low latency, and open. I started playing around, but got annoyed re-cloning the same voices every session. So I built a quick saver for profiles... and it snowballed into &lt;strong&gt;Voicebox&lt;/strong&gt;, my attempt at the &amp;quot;Ollama for voice.&amp;quot;&lt;/p&gt; &lt;p&gt;It's a native desktop app (Tauri/Rust/Python, super lightweightâ€”no Electron bloat or Python setup for users). Everything local, private, offline.&lt;/p&gt; &lt;p&gt;Main bits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone voices instantly with Qwen3-TTS (single or multi-sample for better quality)&lt;/li&gt; &lt;li&gt;DAW-like multi-track timeline to compose conversations/podcasts/narratives&lt;/li&gt; &lt;li&gt;In-app system audio/mic recording + Whisper transcription&lt;/li&gt; &lt;li&gt;REST API + one-click local server for integrating into games/apps/agents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MIT open-source, early stage (v0.1.x).&lt;br /&gt; Repo: &lt;a href="https://github.com/jamiepine/voicebox"&gt;https://github.com/jamiepine/voicebox&lt;/a&gt;&lt;br /&gt; Downloads: &lt;a href="https://voicebox.sh/"&gt;https://voicebox.sh&lt;/a&gt; (macOS/Windows now; Linux soon)&lt;/p&gt; &lt;p&gt;Planning XTTS, Bark, etc. next. What models do you want most? Any feedback if you try itâ€”bugs, missing features, workflow pains?&lt;/p&gt; &lt;p&gt;Give it a spin and lmk what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamiepine"&gt; /u/jamiepine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T09:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp87tk</id>
    <title>Kimi K2.5 is the best open model for coding</title>
    <updated>2026-01-28T10:54:13+00:00</updated>
    <author>
      <name>/u/npc_gooner</name>
      <uri>https://old.reddit.com/user/npc_gooner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt; &lt;img alt="Kimi K2.5 is the best open model for coding" src="https://preview.redd.it/unxlhercm2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23f59fb1153598db9b9c8a3b5ce9067435dfba28" title="Kimi K2.5 is the best open model for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;they really cooked&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npc_gooner"&gt; /u/npc_gooner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/unxlhercm2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpmay0</id>
    <title>I just got my Dell DGX Spark GB10 that I won from the hackathon!</title>
    <updated>2026-01-28T20:03:20+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"&gt; &lt;img alt="I just got my Dell DGX Spark GB10 that I won from the hackathon!" src="https://preview.redd.it/af2u39y6a5gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d772153c2ea4d039aff7359c493d9106f7a82aae" title="I just got my Dell DGX Spark GB10 that I won from the hackathon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please don't mind the breadcrumbs... &lt;/p&gt; &lt;p&gt;But they pretty much overnighted the Dell DGX Spark GB10.&lt;/p&gt; &lt;p&gt;I think the first thing I am going to try and do is figure out how to get a robot arm to do some sort of shape matching using transfer learning to stick particular shapes in the correct holes. I think that might be easy enough? (I am naive because I haven't done transfer learning or physical AI yet)&lt;/p&gt; &lt;p&gt;I also want to try using LTX and see if it can recreate the ending for How I Met Your Mother or Game of Thrones (if it is able to do that). Might honestly be difficult because I haven't worked with vision models other than image creation using Fal.ai. I wonder if this machine can handle it.&lt;/p&gt; &lt;p&gt;Otherwise, I am going to keep hammering at figuring out better ways of solving the Social Determinants of Health problem. There are a lot of correlations that I wasn't able to completely finish within the limited amount of time for example:&lt;/p&gt; &lt;p&gt;Crime, lack of parks, and food insecurity increases chronic disease risk because people do not feel safe to leave their homes and exercise or walk and often times default to junk food as there are no other culturally sensitive alternatives leading to obesity and higher cardiovascular.&lt;/p&gt; &lt;p&gt;It would be also great if my AI Agents can go through some research paper and identify some of the most crucial ones that I can at least bake into the platform as a baseline that might be effecting other cities.&lt;/p&gt; &lt;p&gt;Also since I have 4 TB SSD I can potentially add the data from a bunch of different cities and start doing some pattern matching/correlation detection between this generally siloed data and see if I could suggest specific campaigns for the cities that would help unrepresented people get better access to care. &lt;/p&gt; &lt;p&gt;One of my passions (and I know this sounds really nerdy) is to create really good multi-turn evaluation harnesses that can use Process Supervised Reward Models to better train complex AI agents and self-heal.&lt;/p&gt; &lt;p&gt;If anyone has advice on any of this I would love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/af2u39y6a5gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T20:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpyxfk</id>
    <title>Reasoning Devstral 2</title>
    <updated>2026-01-29T04:42:29+00:00</updated>
    <author>
      <name>/u/Front_Eagle739</name>
      <uri>https://old.reddit.com/user/Front_Eagle739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun fact! You can actually make devstral 2 123B &amp;amp; Devstral 24B reason! Accidently had a reasoning forcing jinja template on for another model when I started testing the mlx version of this thing with a couple of reasoning effot = extra high statements in my system prompt because I really wanted more reasoning out of the last model I was using and havving forgotten about that tried devstral 2 and got 2 minutes of reasoning before it answered my test question.&lt;/p&gt; &lt;p&gt;Turns out they are both hybrid reasoners if you put {%- set reasoning_content = 'High' %} in the jinja. Nice clean logical reasoning as well. That's actually fixed my main issue with these models, sometimes you just really need that extra consistency.&lt;/p&gt; &lt;p&gt;Did everybody else know this and I just missed it somehow?&lt;/p&gt; &lt;p&gt;Edit. Seems the smaller one may have some difficulty exiting the thinking, at least with some sampler settings. Big one seems fine though. Quality of response is definitely going way up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Front_Eagle739"&gt; /u/Front_Eagle739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T04:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpqlfj</id>
    <title>768Gb "Mobile" AI Server Follow-Up Part 1, Look Inside</title>
    <updated>2026-01-28T22:44:03+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"&gt; &lt;img alt="768Gb &amp;quot;Mobile&amp;quot; AI Server Follow-Up Part 1, Look Inside" src="https://external-preview.redd.it/b3IzbXRvY3BwNWdnMU2mkuU7oHD8qNQyskshMOF3z-mD-pRqGUpyoVD2VUXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b6cba6ca164006c230852989fb885c68feec072" title="768Gb &amp;quot;Mobile&amp;quot; AI Server Follow-Up Part 1, Look Inside" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Y'all,&lt;/p&gt; &lt;p&gt;The post I made about the AI server got a lot of buzz, so I decided to do a follow up with some video on the project. Because of reddit's video upload restrictions, I'll have to upload them in separate posts with slightly different focuses, but I've uploaded the full (and higher quality) version to Youtube. Taking the video from 1080p to 720p to meet reddit's video size requirements kinda messed up visibility on the screen record in one of the later parts, so I'll leave a link to the full video here for convenience, otherwise the other parts should get posted here shortly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/TJOKEFdCkv0"&gt;https://youtu.be/TJOKEFdCkv0&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This part primarily focuses on providing some background context on how we came to the W200 in the first place, what it solved for us, and a look inside the unit.&lt;/p&gt; &lt;p&gt;Spec summary:&lt;/p&gt; &lt;p&gt;512Gb DDR4, 256GB VRAM (8x3090+2x5090), 64 core Threadripper Pro 3995WX&lt;/p&gt; &lt;p&gt;Case: Core W200&lt;/p&gt; &lt;p&gt;Appreciate all of the comments and responses on the last post, I've never done anything like this before so I apologize if things are not more polished, attention normally isn't my thing so while the volume of feedback was a little overwhelming the interest was very much encouraging. It seems like every other day we see people post builds here composed of top of the line enterprise hardware with sunken costs reaching tens of thousands of dollars, so I think it can make a difference to just highlight what can be possible with a little ingenuity, consumer grade components, and a more relatively &amp;quot;realistic&amp;quot; budget (in this case, around ~17k usd). Keep this figure in mind when comparing cost:value to these other workstations and their specs/performance capability/creative potential, because I do think this illustrates that effective AI hosting can be more than just throwing money at the problem. Whether someone is working with 100$ or 100k$, focusing on innovative problem solving, pushing optimization limits, and just seeing what can be possible with what's currently available is an order of magnitude more exciting and interesting to see than a squeaky clean $50,000 supercomputer with specialized hardware that very few people will ever get to see in-person within their lifetime posted by someone asking the same question asked since the dawn of time, &amp;quot;what should I do with this?&amp;quot;. Ultimately the interest for experimentation and trying new approaches is what keeps this hobby (local AI) alive and relevant, and imo will be our best counterbalance to the complications that closed-model AI companies impose as we move forward.&lt;/p&gt; &lt;p&gt;Questions welcome.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/trvmg2cpp5gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpfse6</id>
    <title>Run Kimi K2.5 Locally</title>
    <updated>2026-01-28T16:17:45+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt; &lt;img alt="Run Kimi K2.5 Locally" src="https://preview.redd.it/rxqfj5os74gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2606f30079a77f14bb28c31413be651c092abaa9" title="Run Kimi K2.5 Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-K2.5 achieves SOTA performance in vision, coding, agentic and chat tasks. &lt;/p&gt; &lt;p&gt;The 1T parameter hybrid reasoning model requires 600GB of disk space, while the quantized &lt;strong&gt;Unsloth Dynamic 1.8-bit&lt;/strong&gt; version reduces this to &lt;strong&gt;240GB (-60% size).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Kimi-K2.5-GGUF"&gt;&lt;strong&gt;Kimi-K2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide:&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/kimi-k2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/kimi-k2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rxqfj5os74gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T16:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq29ab</id>
    <title>Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description.</title>
    <updated>2026-01-29T07:40:59+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"&gt; &lt;img alt="Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description." src="https://external-preview.redd.it/NGZpZjMyeWJwOGdnMSyVzMY88rGIMLP8wkCsphE6OdlDVcwcn9ECGq-UAL8f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fc37bfffb290e2fd1af99cd3cb5f3e90281514f" title="Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The system works by having a pool of 200 spell components like explosive or change color. A LLM then converts each word into a set of component instructions.&lt;/p&gt; &lt;p&gt;For example &amp;quot;explode&amp;quot; = explosive + change color + apply force.&lt;/p&gt; &lt;p&gt;This means we can have a system that can generate a spell for literally any word.&lt;/p&gt; &lt;p&gt;Stick based music was made with Suno.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hbq4wsxbp8gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T07:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq0qut</id>
    <title>I built an open-source, multi-agent alternative to OpenAI Prism for research workflows (Verification Agent + LaTeX + PDF)</title>
    <updated>2026-01-29T06:14:18+00:00</updated>
    <author>
      <name>/u/Inside-Scratch4</name>
      <uri>https://old.reddit.com/user/Inside-Scratch4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Iâ€™ve been working on an open-source project called &lt;strong&gt;Prismer&lt;/strong&gt; to tackle the mess that is the current academic workflow.&lt;/p&gt; &lt;p&gt;Like many of you, I found that using generic LLMs for research often leads to hallucinations, especially with citations. And relying on closed ecosystems like OpenAIâ€™s Prism wasnâ€™t ideal for privacy or customization.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Prismer&lt;/strong&gt;, an all-in-one platform that integrates: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Native PDF Reader&lt;/strong&gt;: With bi-directional citation graphs. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Citation Verification Agent&lt;/strong&gt;: Uses multiple agents to cross-check references against real databases (arXiv, etc.) to prevent LLM hallucinations. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jupyter Integration&lt;/strong&gt;: For data analysis right next to your writing. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LaTeX Editor&lt;/strong&gt;: With real-time preview.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Itâ€™s completely open-source (MIT License). The goal is to have a modular system where you can swap in your own models or agents.&lt;/p&gt; &lt;p&gt;Iâ€™d love to get some feedback from this community on the agent orchestration part specifically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Prismer-AI/Prismer"&gt;https://github.com/Prismer-AI/Prismer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside-Scratch4"&gt; /u/Inside-Scratch4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T06:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; ðŸ‘‹&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AMâ€“11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;âš ï¸ &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please donâ€™t post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. Weâ€™re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM â€“ 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
