<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-07T20:25:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qxqpdz</id>
    <title>GLM 5 Is Being Tested On OpenRouter</title>
    <updated>2026-02-06T19:05:23+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxqpdz/glm_5_is_being_tested_on_openrouter/"&gt; &lt;img alt="GLM 5 Is Being Tested On OpenRouter" src="https://preview.redd.it/6cbhnbxe9xhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de924647f1b7c78ba0a6f6653bc5bd25c424af9e" title="GLM 5 Is Being Tested On OpenRouter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6cbhnbxe9xhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxqpdz/glm_5_is_being_tested_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxqpdz/glm_5_is_being_tested_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T19:05:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyncj1</id>
    <title>Ltx 2 video finetuning</title>
    <updated>2026-02-07T19:44:12+00:00</updated>
    <author>
      <name>/u/miteshyadav</name>
      <uri>https://old.reddit.com/user/miteshyadav</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone played around with finetuning Ltx 2 and achieved good results? How does it compare with Kling / Veo3 based models? Trying to understand if it's worth finetuning these open source video models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/miteshyadav"&gt; /u/miteshyadav &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyncj1/ltx_2_video_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyncj1/ltx_2_video_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyncj1/ltx_2_video_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T19:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxpf86</id>
    <title>[Release] Experimental Model with Subquadratic Attention: 100 tok/s @ 1M context, 76 tok/s @ 10M context (30B model, single GPU)</title>
    <updated>2026-02-06T18:19:46+00:00</updated>
    <author>
      <name>/u/Sad-Size2723</name>
      <uri>https://old.reddit.com/user/Sad-Size2723</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Last week I shared preliminary results on a new subquadratic attention mechanism (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qol3s5/preliminary_new_subquadratic_attention_20k_toks&lt;/a&gt;). Following up with the full release: model + inference code are now available.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: 30B model achieving O(L^(3/2)) scaling instead of O(L^2). Enables 1Mâ€“10M context on a single GPU with decode speeds that stay practical even at extreme context lengths. Ships with an OpenAI-compatible server and CLI to try out.&lt;/p&gt; &lt;p&gt;- ðŸ¤— &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/concavity-ai/superlinear-exp-v0.1"&gt;https://huggingface.co/concavity-ai/superlinear-exp-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- ðŸ’» &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/concavity-ai/superlinear"&gt;https://github.com/concavity-ai/superlinear&lt;/a&gt; (`pip install superlinear`)&lt;/p&gt; &lt;p&gt;- ðŸ“„ &lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2601.18401"&gt;https://arxiv.org/abs/2601.18401&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main Idea&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can think of attention as a search algorithm to find relevant information for next-token prediction. Standard attention is basically O(L) brute-force search. We're doing O(L^0.5) jump-search with learned routing: score O(L^0.5) candidate spans, select top-k, then do token-level attention within the selected spans.&lt;/p&gt; &lt;p&gt;This gives &lt;strong&gt;O(L^(3/2)) total complexity&lt;/strong&gt; while preserving &lt;strong&gt;random context access&lt;/strong&gt; â€” any token can be selected by content-dependent routing, unlike fixed sliding windows. When you 10x the context length, the search budget only grows by ~3.2x. That subquadratic scaling really matters for long context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance (Single B200 GPU)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| Context Length | Prefill (tok/s) | Decode (tok/s) | Memory | |----------------|-----------------|----------------|---------| | 1M tokens | ~20,202 | ~109 | 66 GB | | 10M tokens | ~5,576 | ~76 | ~120 GB | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Key point: 1M â†’ 10M context (10x increase) only drops decode speed by ~30%, not the 10x slowdown with dense attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why This Matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When you have fast long-context inference, usage patterns change. The key is &lt;strong&gt;maintaining the cache&lt;/strong&gt; instead of reprocessing everything:&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&lt;em&gt;Almost-infinite chat&lt;/em&gt;&lt;/strong&gt;: KV cache in memory for instant responses, save/restore sessions to disk for persistence&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&lt;em&gt;Document Q&amp;amp;A&lt;/em&gt;&lt;/strong&gt;: Load documents once, ask cross-document questions without reprocessing (our GitHub example: 8 Wikipedia articles with cross-document reasoning)&lt;/p&gt; &lt;p&gt;- &lt;strong&gt;&lt;em&gt;Long-form generation&lt;/em&gt;&lt;/strong&gt;: 20k+ token reasoning on difficult math problems and coherent long article writing, all with maintained context&lt;/p&gt; &lt;p&gt;Early results: perfect NIAH at 512K context (up from 256K last week), cross-document reasoning working, subquadratic scaling working in practice.&lt;/p&gt; &lt;p&gt;Since no existing inference engine is going to support our custom kernels, we built the full stack ourselves: Triton kernels, OpenAI-compatible server, session snapshots, chunked prefill, CLI with BM25 RAG.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Limitations &amp;amp; Next Steps&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Current limitations:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- This is an **architecture + systems feasibility release**, not production-quality&lt;/p&gt; &lt;p&gt;- Limited training data (initial SFT only)&lt;/p&gt; &lt;p&gt;- Comprehensive evals beyond NIAH still needed&lt;/p&gt; &lt;p&gt;- FP16 only (66GB for 1M context) â€” quantization coming soon&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Quantization&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;(coming soon):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- 4-bit/8-bit quantization to run 1M context on 24GB consumer GPUs&lt;/p&gt; &lt;p&gt;- Target: RTX 4090 / RTX 5090 with full 1M context&lt;/p&gt; &lt;p&gt;- 2M context on 48GB cards (e.g., RTX 6000 Ada)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Hardware support:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Currently CUDA only (B200, RTX 6000 Blackwell tested)&lt;/p&gt; &lt;p&gt;- AMD ROCm port coming (Triton kernels should make this straightforward)&lt;/p&gt; &lt;p&gt;- Eventually Apple Silicon (harder but not impossible)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Training &amp;amp; Quality improvements:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Scaling up SFT data with more long-context examples&lt;/p&gt; &lt;p&gt;- Potentially doing continued pretraining on long documents&lt;/p&gt; &lt;p&gt;- Expanding perfect NIAH range beyond 512K&lt;/p&gt; &lt;p&gt;- Real-world long-context benchmarks (book QA, codebase analysis, multi-document reasoning)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;New end-user applications&lt;/em&gt;&lt;/strong&gt;: We are planning to develop local-first end-user applications based on this. What would you actually use long context for? Would love to hear specific use cases to help us prioritize.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Trying something new is extremely hard. Everyone likes existing transformer architectures â€” optimizations at every level, predictable scaling laws. But to make truly long-context models practical on local hardware, I think we need new ideas. It doesn't hurt to try, right?&lt;/p&gt; &lt;p&gt;I'm trying not to spam this sub, so the GitHub repo is the best place to follow progress. Happy to answer questions here though! If you try it and hit issues, open a GitHub issue. And if you have thoughts on long-context use cases, I'd love to hear them.&lt;/p&gt; &lt;p&gt;Thanks for all the encouragement on the last post!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;- ðŸ¤— &lt;strong&gt;Model&lt;/strong&gt;: &lt;a href="https://huggingface.co/concavity-ai/superlinear-exp-v0.1"&gt;https://huggingface.co/concavity-ai/superlinear-exp-v0.1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- ðŸ’» &lt;strong&gt;Code&lt;/strong&gt;: &lt;a href="https://github.com/concavity-ai/superlinear"&gt;https://github.com/concavity-ai/superlinear&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- ðŸ“„ &lt;strong&gt;Paper&lt;/strong&gt;: &lt;a href="https://arxiv.org/abs/2601.18401"&gt;https://arxiv.org/abs/2601.18401&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad-Size2723"&gt; /u/Sad-Size2723 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxpf86/release_experimental_model_with_subquadratic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T18:19:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy3tr8</id>
    <title>An ode to Minimax m2.1</title>
    <updated>2026-02-07T04:11:39+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just wanted to share my experience with Minimax m2.1 Specifically the Minimax m2.1 4-bit DWQ MLX quant. &lt;/p&gt; &lt;p&gt;I do alot of research, analysis, and synthesis of various papers and architectural components. To date, no other model has been able to touch this model and quant on my hardware (running on an M2 Ultra Mac Studio).&lt;/p&gt; &lt;p&gt;From depth of knowledge, directness, lack of sycophancy, intelligence, tone, and speed this model and quant is a godsend for my work.&lt;/p&gt; &lt;p&gt;The reasoning is concise - it doesn't ramble for thousands of tokens. It's quick, on point, and logical.&lt;/p&gt; &lt;p&gt;For agentic coding it's very good. It follows instructions well, has a 196k context window, and is proficient with every coding language I've tried.&lt;/p&gt; &lt;p&gt;I've used hundreds of local models of many different sizes, and this is the one I keep coming back to. For academic and LLM-centric research it's smart as hell. It doesn't glaze me, and it doesn't ramble.&lt;/p&gt; &lt;p&gt;I don't know if any other quants are this good, but I feel like I stumbled upon a hidden gem here and wanted to share.&lt;/p&gt; &lt;p&gt;Edit: I'm using Temp = 1.0, top_p = 0.95, top_k = 40 as per the &lt;a href="https://huggingface.co/MiniMaxAI/MiniMax-M2.1"&gt;HF page.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy3tr8/an_ode_to_minimax_m21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy3tr8/an_ode_to_minimax_m21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy3tr8/an_ode_to_minimax_m21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T04:11:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qygaup</id>
    <title>What are possible use cases for going full BF16?</title>
    <updated>2026-02-07T15:14:02+00:00</updated>
    <author>
      <name>/u/phwlarxoc</name>
      <uri>https://old.reddit.com/user/phwlarxoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering when it would make sense to use the BF16 version of certain (smaller!) LLMs.&lt;/p&gt; &lt;p&gt;What might be use cases where BF16 really generates additional value?&lt;/p&gt; &lt;p&gt;Are those mainly coding-related or, on the contrary, do they best cover fields not related to coding, I'd be most interested in multilingual (comprehension of non-English complicated texts) for example.&lt;/p&gt; &lt;p&gt;I tried a couple of BF16 version (Nemotron-3-Nano-30B-A3B-BF16, GLM-4.7 Flash, Qwen3-Coder-30B-A3B-Instruct-GGUF, Qwen3-Coder-30B-A3B-Instruct-1M-GGUF, Qwen3-Next-80B-A3B-Instruct-GGUF and Qwen3-Coder-Next-GGUF) and while all of those ran very well and at impressive speeds, their benefit is less clear.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phwlarxoc"&gt; /u/phwlarxoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qygaup/what_are_possible_use_cases_for_going_full_bf16/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qygaup/what_are_possible_use_cases_for_going_full_bf16/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qygaup/what_are_possible_use_cases_for_going_full_bf16/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T15:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyivbk</id>
    <title>QAT + LoRa giving me better results that QLora?</title>
    <updated>2026-02-07T16:54:25+00:00</updated>
    <author>
      <name>/u/OperationHaunting687</name>
      <uri>https://old.reddit.com/user/OperationHaunting687</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Playing with some models, and when fine tuning them (usually bf16 or fp16 models that get quantized into int4), and measuring benchmarks, QAT + LoRa (so doing QAT but with adapters), seems to be working much better for me than some other strategies. Researching it a bit, I see that's not a standard method compared to full QAT. But full QAT is too slow for me, do you think spending $$$ for full QAT might be worth it if QAT + LoRa is promising?&lt;/p&gt; &lt;p&gt;Anyone else with same experience?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OperationHaunting687"&gt; /u/OperationHaunting687 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyivbk/qat_lora_giving_me_better_results_that_qlora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyivbk/qat_lora_giving_me_better_results_that_qlora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyivbk/qat_lora_giving_me_better_results_that_qlora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T16:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyizo9</id>
    <title>Best lightweight local TTS model?</title>
    <updated>2026-02-07T16:59:12+00:00</updated>
    <author>
      <name>/u/Bartholomheow</name>
      <uri>https://old.reddit.com/user/Bartholomheow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using KokoroTTS and it's still very good and lightweight, I can run it very fast on my 3060 geforce rtx gpu. The problem is only few of the voices are good, and even then, sometimes they make mistakes, especially with foreign or uncommon words, or sound robotic, also the voices with less training data (most of them) are much more prone to mistakes. They are decent, but with how fast better models are created, are there any better lightweight models? I heard of Qwen, but I'm creating many hours of audio, I don't think it's as fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bartholomheow"&gt; /u/Bartholomheow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyizo9/best_lightweight_local_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyizo9/best_lightweight_local_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyizo9/best_lightweight_local_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T16:59:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qykauz</id>
    <title>Local ACE-Step v1.5 RADIO</title>
    <updated>2026-02-07T17:48:31+00:00</updated>
    <author>
      <name>/u/MustBeSomethingThere</name>
      <uri>https://old.reddit.com/user/MustBeSomethingThere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykauz/local_acestep_v15_radio/"&gt; &lt;img alt="Local ACE-Step v1.5 RADIO" src="https://external-preview.redd.it/MDZ2ZWEwcmV6M2lnMS9qzklExUOna9Qpg2s2zaVFgrhC5fDMbQaQypoYRMTw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=deca0c32e2b5b4d53147ae46e603834f528b1654" title="Local ACE-Step v1.5 RADIO" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/PasiKoodaa/ACE-Step-1.5-RADIO"&gt;https://github.com/PasiKoodaa/ACE-Step-1.5-RADIO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mostly vibe coded with Kimi 2.5 (because why not). Uses LM Studio for automatic lyrics generation. Only 2 added files (RADIO.html and proxy-server.py), so it does not ruin current official installations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MustBeSomethingThere"&gt; /u/MustBeSomethingThere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/j7633yqez3ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykauz/local_acestep_v15_radio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qykauz/local_acestep_v15_radio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T17:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qymiis</id>
    <title>Plano reaches 5K GH stars as I continue to help devs build agents locally</title>
    <updated>2026-02-07T19:11:41+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qymiis/plano_reaches_5k_gh_stars_as_i_continue_to_help/"&gt; &lt;img alt="Plano reaches 5K GH stars as I continue to help devs build agents locally" src="https://preview.redd.it/9ow4328ze4ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374ee80e949d5affe2bcad868cc3d979a3f5418f" title="Plano reaches 5K GH stars as I continue to help devs build agents locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey peeps! Super happy today. Big thank you to all the contribution, users and the community members that have helped the project reach this milestone! &lt;/p&gt; &lt;p&gt;My early bet on small LLMs (for &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;routing&lt;/a&gt; and &lt;a href="https://huggingface.co/katanemo/Plano-Orchestrator-30B-A3B"&gt;orchestration&lt;/a&gt;) that offload a lot of the rote decision making in agentic systems seems to be the striking a chord. Plus our framework-agnostic approach seems to be resonating as well. Btw, for those who might be hearing about us the first time, Plano is a models-integrated proxy server and data plane for agentic AI. &lt;/p&gt; &lt;p&gt;Check it out and if you like our work please continue supporting the cause &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9ow4328ze4ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qymiis/plano_reaches_5k_gh_stars_as_i_continue_to_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qymiis/plano_reaches_5k_gh_stars_as_i_continue_to_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T19:11:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydwox</id>
    <title>DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)</title>
    <updated>2026-02-07T13:32:57+00:00</updated>
    <author>
      <name>/u/poppear</name>
      <uri>https://old.reddit.com/user/poppear</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydwox/doomsdayos_running_on_my_thinkpad_t14s_live_from/"&gt; &lt;img alt="DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)" src="https://external-preview.redd.it/amhoMTMxeGttMmlnMX8NGHmEIKR1Shq8PrhwLMOPZOE4F_KOxFoLbBMbU6CW.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ac0e57675428604f2828fe09bc5da475e14962c" title="DoomsdayOS running on my Thinkpad T14s live from a USB stick! (all-in-one ISO: LLMs, Wikipedia, Runtime, etc...)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am ready for the apocalypse.&lt;/p&gt; &lt;p&gt;Repo here: &lt;a href="https://github.com/cartesia-one/doomsday-os"&gt;https://github.com/cartesia-one/doomsday-os&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poppear"&gt; /u/poppear &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lhz2yavkm2ig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydwox/doomsdayos_running_on_my_thinkpad_t14s_live_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydwox/doomsdayos_running_on_my_thinkpad_t14s_live_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:32:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qykuxd</id>
    <title>GLM-4.7-Flash reasoning is amazing</title>
    <updated>2026-02-07T18:09:12+00:00</updated>
    <author>
      <name>/u/perfect-finetune</name>
      <uri>https://old.reddit.com/user/perfect-finetune</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model is very aware when to start using structured points and when to talk directly and use minimal tokens.&lt;/p&gt; &lt;p&gt;For example I asked it a maths problem and asked it to do web search,when he saw the math problem he started to put the problem into different pieces and analyze each and then achieved conclusion.&lt;/p&gt; &lt;p&gt;where when it was operating in agentic environment it's like &amp;quot;user told me ..,I should...&amp;quot; Then it calls the tool directly without Yapping inside the Chain-Of-Thought.&lt;/p&gt; &lt;p&gt;Another good thing that it uses MLA instead of GQA which makes it's memory usage significantly lower and allows it to fit directly on some GPUs without offload.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/perfect-finetune"&gt; /u/perfect-finetune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qykuxd/glm47flash_reasoning_is_amazing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyhppc</id>
    <title>Built comprehensive Grafana monitoring for my LLM home server</title>
    <updated>2026-02-07T16:09:27+00:00</updated>
    <author>
      <name>/u/pfn0</name>
      <uri>https://old.reddit.com/user/pfn0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyhppc/built_comprehensive_grafana_monitoring_for_my_llm/"&gt; &lt;img alt="Built comprehensive Grafana monitoring for my LLM home server" src="https://a.thumbs.redditmedia.com/ll3zFE54xcfIpCMbLRHpGZHWMeuf5bY2qrdJO2uBGH4.jpg" title="Built comprehensive Grafana monitoring for my LLM home server" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted better visibility into my LLMs running on llama-server, particularly since it tends to crash silently during model loading when allocation failures occur. Instead of manually checking logs and CLI each time, I built this dashboard.&lt;/p&gt; &lt;p&gt;All components run in docker containers: - grafana - prometheus&lt;br /&gt; - dcgm-exporter - llama-server - go-tapo-exporter (wall power monitoring) - custom docker image&lt;/p&gt; &lt;p&gt;The custom image provides HTTP service discovery for Prometheus, exposes model load states (visible at bottom), and scrapes nvidia-smi processes for per-compute-process statistics.&lt;/p&gt; &lt;p&gt;Dashboarding isn't just passive - I can click the green status bar (color-coded over time) or any model in the list to load/unload them directly.&lt;/p&gt; &lt;p&gt;The dashboard tracks: - Prompt and token processing rates - GPU utilization and memory paging - Power consumption breakdowns - VRAM/RAM usage per compute process&lt;br /&gt; - Network and disk throughput&lt;/p&gt; &lt;p&gt;I'm satisfied with how it functions and looks at this point.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pfn0"&gt; /u/pfn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qyhppc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyhppc/built_comprehensive_grafana_monitoring_for_my_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyhppc/built_comprehensive_grafana_monitoring_for_my_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T16:09:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyns06</id>
    <title>AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test.</title>
    <updated>2026-02-07T20:01:22+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt; &lt;img alt="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." src="https://preview.redd.it/7euavxiwo4ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31891ab1e02bef6fcc1b33374b8b479e2fec1051" title="AIME 2026 Results are out and both closed and open models score above 90%. DeepSeek V3.2 only costs $0.09 to run the entire test." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026"&gt;https://matharena.ai/?view=problem&amp;amp;comp=aime--aime_2026&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7euavxiwo4ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyns06/aime_2026_results_are_out_and_both_closed_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T20:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyeje2</id>
    <title>The M5 max and possibly the m5 ultra macs are coming soon!</title>
    <updated>2026-02-07T14:01:01+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mac os 26.3 should be coming out next week since the rc version is already out . They might release the m5 max with it since the os leak has the m5 max and ultra codenames in it. Crazy deepseek 4 and glm 5 and non codex gpt 5.3 are coming out soon too. Minimax 2.2 shouldnt be far either . If they release a macbook with the m5 ultra , I think people will go crazy over it, but the cooling is not good enough. A mac studio is more likely But since fhe design is different, u might be able to choose your gpu separately from your cpu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyeje2/the_m5_max_and_possibly_the_m5_ultra_macs_are/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyeje2/the_m5_max_and_possibly_the_m5_ultra_macs_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyeje2/the_m5_max_and_possibly_the_m5_ultra_macs_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T14:01:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qycn5s</id>
    <title>DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison.</title>
    <updated>2026-02-07T12:32:28+00:00</updated>
    <author>
      <name>/u/RelativeOperation483</name>
      <uri>https://old.reddit.com/user/RelativeOperation483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"&gt; &lt;img alt="DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison." src="https://b.thumbs.redditmedia.com/HDuzkmv1h50aT7P7iyAduUT2TYW4g84iH4RYq4kv6Hg.jpg" title="DeepSeek-V2-Lite vs GPT-OSS-20B on my 2018 potato i3-8145U + UHD 620, OpenVINO Comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Same potato, new test. If you saw my last post, you will catch this up. I run LLMs on a &lt;strong&gt;2018 HP ProBook 8th Gen i3 with no Nvidia, no dedicated GPU&lt;/strong&gt;, just hope and an OpenVINO backend. This time I wanted to see how two MoE models compare head to head on the exact same hardware, same questions, same settings, same everything.&lt;/p&gt; &lt;p&gt;Same 10 questions for both models. Logic, health, history, coding, creative writing, factual biography, math, tech explainer, ethics, food science. Wide spread of topics to stress test general capability.&lt;/p&gt; &lt;p&gt;Each model was tested 3 times, each time running all 10 questions on CPU first then on iGPU with 1 layer offloaded. So that is 10 questions x 3 runs = 30 samples per device per model. 120 total inference runs. Same context (4096), same max output (256 tokens), same temperature (0.2), same top_p (0.9). Identical conditions.&lt;/p&gt; &lt;p&gt;&lt;em&gt;THE SPEED&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-V2-Lite absolutely smoked GPT-OSS. Almost 2x faster across the board.&lt;/li&gt; &lt;li&gt;DeepSeek on CPU: 7.93 tok/s average, TTFT 2.36s&lt;/li&gt; &lt;li&gt;DeepSeek on iGPU: 8.08 tok/s average, TTFT 1.86s&lt;/li&gt; &lt;li&gt;Peak decode: 8.28 tok/s (iGPU) â€” Lowest: 5.50 tok/s (CPU, cold start Q1)&lt;/li&gt; &lt;li&gt;GPT-OSS on CPU: 4.20 tok/s average, TTFT 3.13s&lt;/li&gt; &lt;li&gt;GPT-OSS on iGPU: 4.36 tok/s average, TTFT 3.07s&lt;/li&gt; &lt;li&gt;Peak decode: 4.46 tok/s (CPU) â€” Lowest: 3.18 tok/s (CPU, two questions got stuck slow)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In real time, DeepSeek finishes a 256-token response in about 32 seconds. GPT-OSS takes over a minute. That is the difference between usable and painful on a slow machine. The iGPU helped DeepSeek more than GPT-OSS. DeepSeek's time to first token dropped 21% on iGPU (from 2.36s to 1.86s). GPT-OSS barely changed. So if you are on iGPU, the smaller active parameter count benefits more from that little offload. (Just my opinion) &lt;/p&gt; &lt;p&gt;&lt;em&gt;THE QUALITY (I read every single response)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;I went through all the outputs manually. Not vibes, actually reading them.&lt;/p&gt; &lt;p&gt;DeepSeek-V2-Lite: 7.5 out of 10&lt;/p&gt; &lt;p&gt;Very consistent. Clean structured answers. Good at health, history, math, tech explainers, ethics, food science. Wrote a complete cyberpunk poem. Solid Magna Carta summary. Nailed the Golden Ratio with three nature examples. Good VPN envelope analogy. Maillard reaction explanation was textbook quality.&lt;/p&gt; &lt;p&gt;Weaknesses&lt;br /&gt; But for today, it got the logic question wrong. The classic &amp;quot;All A are B, some B are C, therefore some A are C&amp;quot;. DeepSeek confidently said it is valid. It is not. That is a well-known syllogistic fallacy. Also on the coding question (Tower of Hanoi), &lt;strong&gt;it spent all its tokens explaining the problem and left the actual function as &amp;quot;# Your code here&amp;quot; without writing the implementation. Small factual error in Marie Curie bio (described her heritage incorrectly)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;GPT-OSS-20B: &lt;strong&gt;2 out of 10&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;When it worked, it was impressive. It correctly identified the logic question as invalid and gave a concrete counterexample with sets to prove it. That was genuinely good reasoning. It also produced a complete working Tower of Hanoi implementation with proper recursion, base case, and example usage. The ethics response on the trolley problem was decent too.&lt;/p&gt; &lt;p&gt;Weaknesses &lt;/p&gt; &lt;p&gt;Hallucinated or broke down on 8 out of 10 questions. And I do not mean subtle errors, I mean full collapse. The health question turned into a loop of &amp;quot;Sure! Here is a revised version of the prompt&amp;quot; repeated over and over without ever answering. The history question started ok then degenerated into repeated &amp;quot;Answer:&amp;quot; blocks and &amp;quot;**...**&amp;quot; until the token limit. The VPN question was the worst â€” it looped &amp;quot;The user is a 3rd person perspective. The user is a 3. The user is a 3.&amp;quot; endlessly. Marie Curie question confused itself trying to summarize events from 2018-2023 for a woman who died in 1934. Golden Ratio collapsed into the same looping pattern. The poem spent all its tokens reasoning about what to write and only managed 4 lines.&lt;/p&gt; &lt;p&gt;This was not random. The same questions broke the same way across all 3 runs. It is a problem, GPT-OSS seems to be a reasoning/thinking model that burns its output budget on internal chain-of-thought and then either never reaches the answer or gets trapped in repetition loops. &lt;strong&gt;With only 256 tokens of output, it simply cannot think AND answer. Caution, I'm not saying Gpt-oss is bad, It can probably be the effect of Q4_K_M.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;DeepSeek-Coder-V2-Lite is the better model for budget hardware if we compare these 2 only. It is faster, more coherent, and way more reliable. &lt;strong&gt;GPT-OSS has flashes of real intelligence (that logic answer was better than what most small models produce)&lt;/strong&gt; but a model that loops on 8 out of 10 questions is not usable for anything practical at Q4_K_M. &lt;strong&gt;GPT-OSS might do better with higher max_tokens, and higher quantization.&lt;/strong&gt; I only tested Q4_K_M at 256 max output. If someone with better hardware wants to test it with more ram, more higher specs, Go for it. &lt;/p&gt; &lt;p&gt;I attached some screenshots in this post. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelativeOperation483"&gt; /u/RelativeOperation483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qycn5s"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qycn5s/deepseekv2lite_vs_gptoss20b_on_my_2018_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T12:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy5xnn</id>
    <title>Kimi-Linear-48B-A3B &amp; Step3.5-Flash are ready - llama.cpp</title>
    <updated>2026-02-07T05:59:11+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Below are actual releases for both models. Anyway get &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;latest version&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Step3.5-Flash&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7964"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7964&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi-Linear-48B-A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7957"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7957&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I don't see any new GGUFs( &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;other=base_model:quantized:moonshotai%2FKimi-Linear-48B-A3B-Instruct&amp;amp;sort=created"&gt;Kimi&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/models?library=gguf&amp;amp;other=base_model:quantized:stepfun-ai%2FStep-3.5-Flash&amp;amp;sort=trending"&gt;Step-3.5&lt;/a&gt; ) from our favorite sources yet. Probably today or tomorrow. &lt;/p&gt; &lt;p&gt;But ik_llama folks got GGUF for &lt;a href="https://huggingface.co/ubergarm/Step-3.5-Flash-GGUF"&gt;Step-3.5-Flash&lt;/a&gt; by ubergarm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy5xnn/kimilinear48ba3b_step35flash_are_ready_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T05:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qy0l26</id>
    <title>Nemo 30B is insane. 1M+ token CTX on one 3090</title>
    <updated>2026-02-07T01:39:58+00:00</updated>
    <author>
      <name>/u/Dismal-Effect-1914</name>
      <uri>https://old.reddit.com/user/Dismal-Effect-1914</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been playing around with llama.cpp and some 30-80B parameter models with CPU offloading. Currently have one 3090 and 32 GB of RAM. Im very impressed by Nemo 30B. 1M+ Token Context cache, runs on one 3090, CPU offloading for experts. Does 35 t/s which is faster than I can read at least. Usually slow as fuck at this large a context window. Feed it a whole book or research paper and its done summarizing in like a few mins. This really makes long context windows on local hardware possible. The only other contender I have tried is Seed OSS 36b and it was much slower by about 20 tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Effect-1914"&gt; /u/Dismal-Effect-1914 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qy0l26/nemo_30b_is_insane_1m_token_ctx_on_one_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T01:39:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydx7z</id>
    <title>Successfully built an Autonomous Research Agent to handle 10k PDFs locally (32GB RAM / AnythingLLM)</title>
    <updated>2026-02-07T13:33:36+00:00</updated>
    <author>
      <name>/u/NGU-FREEFIRE</name>
      <uri>https://old.reddit.com/user/NGU-FREEFIRE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to share a quick win. Iâ€™ve been experimenting with Agentic RAG to handle a massive local dataset (10,000+ PDFs).&lt;/p&gt; &lt;p&gt;Most standard RAG setups were failing or hallucinating at this scale, so I moved to an &lt;strong&gt;Autonomous Agent&lt;/strong&gt; workflow using AnythingLLM and Llama 3.2. The agent now performs recursive searches and cross-references data points before giving me a final report.&lt;/p&gt; &lt;p&gt;Running it on 32GB RAM was the sweet spot for handling the context window without crashing.&lt;/p&gt; &lt;p&gt;If you're looking for a way to turn a &amp;quot;dumb&amp;quot; archive into a searchable, intelligent local database without sending data to the cloud, this is definitely the way to go.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NGU-FREEFIRE"&gt; /u/NGU-FREEFIRE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydx7z/successfully_built_an_autonomous_research_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydx7z/successfully_built_an_autonomous_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydx7z/successfully_built_an_autonomous_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:33:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyljr0</id>
    <title>Prompt injection is killing our self-hosted LLM deployment</title>
    <updated>2026-02-07T18:34:55+00:00</updated>
    <author>
      <name>/u/mike34113</name>
      <uri>https://old.reddit.com/user/mike34113</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We moved to self-hosted models specifically to avoid sending customer data to external APIs. Everything was working fine until last week when someone from QA tried injecting prompts during testing and our entire system prompt got dumped in the response.&lt;/p&gt; &lt;p&gt;Now I'm realizing we have zero protection against this. Traditional web application firewalls don't understand LLM-specific attacks. The model just treats malicious prompts like normal user input and happily complies.&lt;/p&gt; &lt;p&gt;Has anyone actually solved prompt injection for production LLM apps? Not talking about basic input sanitization because adversarial prompts can be crafted to look completely normal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mike34113"&gt; /u/mike34113 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyljr0/prompt_injection_is_killing_our_selfhosted_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyjm0l</id>
    <title>Benchmarking total wait time instead of pp/tg</title>
    <updated>2026-02-07T17:22:35+00:00</updated>
    <author>
      <name>/u/batsba</name>
      <uri>https://old.reddit.com/user/batsba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"&gt; &lt;img alt="Benchmarking total wait time instead of pp/tg" src="https://preview.redd.it/dmf3ykavv3ig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb531575915521581cd6f6e05acf9e09b011c7f3" title="Benchmarking total wait time instead of pp/tg" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I find pp512/tg128 numbers not very useful for judging real-world performance. I've had setups that looked acceptable on paper but turned out to be too slow in real use.&lt;/p&gt; &lt;p&gt;So I started benchmarking total time to process realistic context sizes (1k to 64k tokens) + generation (always 500 tokens), which I think better represents what actually matters: how long do I need to wait?&lt;/p&gt; &lt;p&gt;Automated the whole process and put results on a website. Attached a screenshot showing some results for the Strix Halo 128 GB. Link if anyone's curious: &lt;a href="https://llocalhost.com/speed-bench/best-per-system/"&gt;https://llocalhost.com/speed-bench/best-per-system/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think is the best way to express how fast a local setup actually is?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/batsba"&gt; /u/batsba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dmf3ykavv3ig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyjm0l/benchmarking_total_wait_time_instead_of_pptg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T17:22:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyl6rd</id>
    <title>Gemini System Prompt - Google decided to remove "PRO" option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription.</title>
    <updated>2026-02-07T18:21:34+00:00</updated>
    <author>
      <name>/u/Educational_Rent1059</name>
      <uri>https://old.reddit.com/user/Educational_Rent1059</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt; &lt;img alt="Gemini System Prompt - Google decided to remove &amp;quot;PRO&amp;quot; option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription." src="https://b.thumbs.redditmedia.com/GQChSaPbMeljTuOrlvLzH1SfN18Sj71SBClWPpwoU_M.jpg" title="Gemini System Prompt - Google decided to remove &amp;quot;PRO&amp;quot; option for paid subscribers mostly in EU due to their A/B testing, so I extracted their system prompt and cancelled the subscription." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8fcauhhx64ig1.png?width=601&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b7a38b522ce96958f3d5df022bd77d140090255"&gt;https://preview.redd.it/8fcauhhx64ig1.png?width=601&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3b7a38b522ce96958f3d5df022bd77d140090255&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As the title says! Enjoy&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Rent1059"&gt; /u/Educational_Rent1059 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyl6rd/gemini_system_prompt_google_decided_to_remove_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qydlwi</id>
    <title>Potential new Qwen and ByteDance Seed models are being tested on the Arena. The â€œKarp-001â€ and â€œKarp-002â€ models claim to be Qwen-3.5 models. The â€œPisces-llm-0206aâ€ and â€œPisces-llm-0206bâ€ models claim to be ByteDance models.</title>
    <updated>2026-02-07T13:19:21+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt; &lt;img alt="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The â€œKarp-001â€ and â€œKarp-002â€ models claim to be Qwen-3.5 models. The â€œPisces-llm-0206aâ€ and â€œPisces-llm-0206bâ€ models claim to be ByteDance models." src="https://preview.redd.it/rtrygqo1p2ig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9704e4c75927f5669c01b711e9c25a0d47ce44bb" title="Potential new Qwen and ByteDance Seed models are being tested on the Arena. The â€œKarp-001â€ and â€œKarp-002â€ models claim to be Qwen-3.5 models. The â€œPisces-llm-0206aâ€ and â€œPisces-llm-0206bâ€ models claim to be ByteDance models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rtrygqo1p2ig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qydlwi/potential_new_qwen_and_bytedance_seed_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T13:19:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qyg10z</id>
    <title>I tested 11 small LLMs on tool-calling judgment â€” on CPU, no GPU.</title>
    <updated>2026-02-07T15:03:14+00:00</updated>
    <author>
      <name>/u/MikeNonect</name>
      <uri>https://old.reddit.com/user/MikeNonect</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Friday night experiment that got out of hand. I wanted to know: how small can a model be and still reliably do tool-calling on a laptop CPU?&lt;/p&gt; &lt;p&gt;So I benchmarked 11 models (0.5B to 3.8B) across 12 prompts. No GPU, no cloud API. Just Ollama and bitnet.cpp.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The models:&lt;/strong&gt; Qwen 2.5 (0.5B, 1.5B, 3B), LLaMA 3.2:3B, SmolLM2:1.7B, Ministral-3:3B, DeepSeek-R1:1.5B, Gemma3:1B, Phi4-mini:3.8B, BitNet 3B (base), BitNet 2B-4T (instruction-tuned)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The interesting part isn't whether they can call tools â€” they all can.&lt;/strong&gt; The interesting part is whether they know when NOT to.&lt;/p&gt; &lt;p&gt;I designed trick prompts like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Don't check the weather in Antwerp, just find me the quarterly report.&amp;quot; â†’ 3 of 8 models called get_weather anyway&lt;/li&gt; &lt;li&gt;&amp;quot;The weather in Antwerp is 8Â°C and rainy. Should I schedule an indoor meeting with Jan?&amp;quot; â†’ 5 of 8 models called get_weather to look up weather that was already in the prompt&lt;/li&gt; &lt;li&gt;&amp;quot;Can you write a Python script that checks the weather using an API?&amp;quot; â†’ Multiple models called get_weather instead of writing code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Some things that really surprised me:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;qwen2.5:1.5b beat qwen2.5:3b.&lt;/strong&gt; The smaller model won by being more conservative â€” it declined prompts it wasn't sure about instead of guessing wrong. The 3B model called get_weather when asked to write a Python script about weather APIs. The 1.5B didn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LLaMA 3.2 calls a tool on literally everything.&lt;/strong&gt; 9/10 action score, 0/2 restraint. Asked &amp;quot;what tools do you have?&amp;quot; â€” it called search_files. Asked to write code â€” it called search_files. It's a hammer that sees every prompt as a nail. But interesting: it actually picked the &lt;em&gt;right&lt;/em&gt; tool more often than most models on the hard prompts. Its problem is restraint, not selection.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;BitNet 2B-4T gave the unexpected result.&lt;/strong&gt; I threw BitNet in as a wildcard, expecting it to fail. The base BitNet 3B model produces word salad â€” completely incoherent output. The instruction-tuned 2B-4T, however, produces perfect JSON tool calls at 2.3s on CPU. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Practical takeaway:&lt;/strong&gt; Simple tool routing is solved at 1.5B on CPU. But if your agent needs to decide &lt;em&gt;whether&lt;/em&gt; to act â€” not just &lt;em&gt;how&lt;/em&gt; â€” sub-4B models will confidently take the wrong action when keyword triggers are present. &lt;/p&gt; &lt;p&gt;Full benchmark code, detailed report with per-run data: &lt;a href="https://github.com/MikeVeerman/tool-calling-benchmark"&gt;https://github.com/MikeVeerman/tool-calling-benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The benchmark is a single Python file â€” easy to add your own models and prompts. Would love to see what happens with different hardware, different models, or different context window settings (I ran everything at Ollama's default 4K context).&lt;/p&gt; &lt;p&gt;Early attempt at a tool-calling-on-consumer-hardware benchmark. Polite feedback and ideas are very welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeNonect"&gt; /u/MikeNonect &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qyg10z/i_tested_11_small_llms_on_toolcalling_judgment_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T15:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qym566</id>
    <title>I trained a 1.8M params model from scratch on a total of ~40M tokens.</title>
    <updated>2026-02-07T18:57:42+00:00</updated>
    <author>
      <name>/u/SrijSriv211</name>
      <uri>https://old.reddit.com/user/SrijSriv211</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt; &lt;img alt="I trained a 1.8M params model from scratch on a total of ~40M tokens." src="https://preview.redd.it/hv5xc4g794ig1.png?width=140&amp;amp;height=72&amp;amp;auto=webp&amp;amp;s=cef557529cd85b5ecdfb430034c5db51f4d966d7" title="I trained a 1.8M params model from scratch on a total of ~40M tokens." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ok so I've been working &amp;amp; experimenting with my own simple architecture. I call it &lt;a href="https://github.com/SrijanSriv211/Strawberry"&gt;Strawberry&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;This is a very very small experimental model. It has 1.8M params and was trained on a dataset with ~9M tokens (~7M for training and ~2M for val). It model was trained on a batch size of 16 and context length of 256. Making the batch size in token counts to be &lt;code&gt;16*256 = 4096&lt;/code&gt;. Meaning the model saw 4096 tokens per step. It was trained for 10k steps meaning it trained on a total of 40M tokens.&lt;/p&gt; &lt;p&gt;The dataset was manually scraped and cleaned. The dataset contain texts from wikipedia on various topics, personalities, games, movies, companies and more. It also contain texts fandoms of various games such as GTA, RDR, Last of Us, Mafia and all. The dataset also contains storylines, scripts and story dialogues of various games such as RDR 2, GTA 5, Cyperpunk 2077, Mafia The Old Country. It also contain transcripts of some of my favorite youtube videos and it also contain code from some of my personal code bases and other repos such as the Hazel Game Engine repo on github. I tried my best to keep the programming language scale limited to just Python, C#, C++ and JavaScript. The dataset also contains texts from several research papers, academic articles and blogs (mainly revolving around AI and LLMs in general). All of this made ~30M chars in total.&lt;/p&gt; &lt;p&gt;After training for 10k steps the final train loss was around 3.5 and val loss was around 3.8.&lt;/p&gt; &lt;p&gt;This is the exact config for the model: &lt;code&gt;{&amp;quot;dataset&amp;quot;: {&amp;quot;data_division&amp;quot;: 0.8, &amp;quot;load_from_file&amp;quot;: true, &amp;quot;path&amp;quot;: &amp;quot;data/webtext.bin&amp;quot;}, &amp;quot;checkpoints&amp;quot;: {&amp;quot;path&amp;quot;: &amp;quot;bin/ck18&amp;quot;, &amp;quot;interval&amp;quot;: 1000, &amp;quot;create_checkpoints&amp;quot;: true}, &amp;quot;model_hyperparams&amp;quot;: {&amp;quot;vocab_size&amp;quot;: 8192, &amp;quot;block_size&amp;quot;: 256, &amp;quot;r_layer&amp;quot;: 3, &amp;quot;n_layer&amp;quot;: 2, &amp;quot;n_head&amp;quot;: 6, &amp;quot;n_embd&amp;quot;: 96, &amp;quot;n_qkv&amp;quot;: 384, &amp;quot;n_ffn&amp;quot;: 384}, &amp;quot;optimizer_hyperparams&amp;quot;: {&amp;quot;eps&amp;quot;: 1e-08, &amp;quot;beta1&amp;quot;: 0.9, &amp;quot;beta2&amp;quot;: 0.99, &amp;quot;weight_decay&amp;quot;: 0.001, &amp;quot;use_muon&amp;quot;: false, &amp;quot;momentum&amp;quot;: 0.95}, &amp;quot;model_path&amp;quot;: &amp;quot;bin/s1.strawberry&amp;quot;, &amp;quot;encoder_path&amp;quot;: &amp;quot;bin/cl8k.bin&amp;quot;, &amp;quot;init_from&amp;quot;: &amp;quot;scratch&amp;quot;, &amp;quot;seed&amp;quot;: &amp;quot;auto&amp;quot;, &amp;quot;gradient_accumulation_steps&amp;quot;: 1, &amp;quot;batch_size&amp;quot;: 16, &amp;quot;max_iters&amp;quot;: 10000, &amp;quot;eval_interval&amp;quot;: 1000, &amp;quot;log_interval&amp;quot;: 100, &amp;quot;eval_iters&amp;quot;: 100, &amp;quot;decay_lr&amp;quot;: true, &amp;quot;lr_decay_iters&amp;quot;: 10000, &amp;quot;learning_rate&amp;quot;: 0.002, &amp;quot;cooldown_frac&amp;quot;: 0.2, &amp;quot;warmup_iters&amp;quot;: 500, &amp;quot;min_lr&amp;quot;: 0.0002}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;cl8k&lt;/code&gt; is a tokenizer from Andrej Karpathy's tokenizer video trained on the same dataset I explained above and then it was used to tokenize those ~30M chars into just ~9M toks.&lt;/p&gt; &lt;p&gt;The idea for Strawberry and retention was that I wanted to explore whether the attention weights can be generated in-real time rather than being learned. That's why I implemented a &amp;quot;Retention&amp;quot; Mechanism. The retention mechanism generates &amp;quot;weights&amp;quot; based on your input which are then used in attention. The formulation is a little bit similar to standard linear attention formula. This system where the QKV weights are dynamically generated rather than being learned allows to increase the number of attention layers (or model depth) without increasing the number of parameters at all.&lt;/p&gt; &lt;p&gt;However increasing the number of attention layers have a problem. If multiple attention layers are stacked on top of each other without any non-linearity such as FFN, then the performance can decline and the loss can get worse overtime.&lt;/p&gt; &lt;p&gt;That's why I implemented a mini-ffn right after the attention calculation and right before the output projection of each attention layer. So, the weights of qkv, mini-ffn and output projection are generated and updated dynamically by the retention mechanism.&lt;/p&gt; &lt;p&gt;I've two attention mechanisms.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Linear Attention in this case Apple's AFT for global context.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Standard MHA attention for local context. I'm also planning to experiment with &lt;code&gt;mixture of attention experts&lt;/code&gt; approach where each attention expert will get different local window. I haven't implemented it yet cuz this model was too small so it didn't made sense to me but I'll implement it later. Mixture of Attention Experts that's why the SPDA version of attention class is called &lt;code&gt;The Expert Abundance&lt;/code&gt;. Idk why but I like that name so I'm sticking with it.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Currently I'm trying to optimize &amp;amp; improve the architecture more.&lt;/p&gt; &lt;p&gt;So yeah. That's the entire thing. I'd love to know your views and opinions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SrijSriv211"&gt; /u/SrijSriv211 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qym566"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qym566/i_trained_a_18m_params_model_from_scratch_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-07T18:57:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
