<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-09T08:08:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nc0dgg</id>
    <title>ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp</title>
    <updated>2025-09-08T21:25:34+00:00</updated>
    <author>
      <name>/u/Recent-Success-1520</name>
      <uri>https://old.reddit.com/user/Recent-Success-1520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"&gt; &lt;img alt="ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp" src="https://external-preview.redd.it/HwAJGIWkuuQRHBpEXp2R4CbrQfzKoASLgWeZFT1sFIQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=131ad86b79c662a9208b9b395f33e7b7dfcbb355" title="ROCm 7.0.0 nightly based apps for Ryzen AI - unsloth, bitsandbytes and llama-cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HI all,&lt;/p&gt; &lt;p&gt;A few days ago I posted if anyone had any fine tuning working on Strix Halo and many people like me were looking.&lt;br /&gt; I have got a working setup now that allows me to use ROCm based fine tuining and inferencing.&lt;/p&gt; &lt;p&gt;For now the following tools are working with latest ROCm 7.0.0 nightly and available in my repo (linked). From the limited testing unsloth seems to be working and llama-cpp inference is working too.&lt;/p&gt; &lt;p&gt;This is initial setup and I will keep adding more tools all ROCm compiled.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# make help Available targets: all: Installs everything bitsandbytes: Install bitsandbytes from source flash-attn: Install flash-attn from source help: Prints all available targets install-packages: Installs required packages llama-cpp: Installs llama.cpp from source pytorch: Installs torch torchvision torchaudio pytorch-triton-rcom from ROCm nightly rocWMMA: Installs rocWMMA library from source theRock: Installs ROCm in /opt/rocm from theRock Nightly unsloth: Installs unsloth from source &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Sample bench&lt;/p&gt; &lt;p&gt;&lt;code&gt;root@a7aca9cd63bc:/strix-rocm-all# llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -ngl 999 -mmp 0 -fa 0&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;ggml_cuda_init: found 1 ROCm devices:&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Device 0: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| model | size | params | backend | ngl | mmap | test | t/s |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 0 | pp512 | 698.26 ¬± 7.31 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 999 | 0 | tg128 | 46.20 ¬± 0.47 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Got mixed up with &lt;a href="/r/LocalLLM"&gt;r/LocalLLM&lt;/a&gt; so posting here too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent-Success-1520"&gt; /u/Recent-Success-1520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shantur/strix-rocm-all"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc0dgg/rocm_700_nightly_based_apps_for_ryzen_ai_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T21:25:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo0bz</id>
    <title>KittenML released a mini version (80M) of their text to speech model.</title>
    <updated>2025-09-08T13:39:19+00:00</updated>
    <author>
      <name>/u/Yorn2</name>
      <uri>https://old.reddit.com/user/Yorn2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt; &lt;img alt="KittenML released a mini version (80M) of their text to speech model." src="https://external-preview.redd.it/6tEU3HFyV9wrAIlbgWYDqTicViQ2PFk-H0trsfrB-TE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ae1cdc19684b4f8a1d7922e2097495effc92e03" title="KittenML released a mini version (80M) of their text to speech model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yorn2"&gt; /u/Yorn2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/KittenML/kitten-tts-mini-0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo0bz/kittenml_released_a_mini_version_80m_of_their/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:39:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbi95c</id>
    <title>Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages</title>
    <updated>2025-09-08T08:32:01+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt; &lt;img alt="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" src="https://external-preview.redd.it/aoPAPmODv59RqOF8q1zUghKheD5cO88KxVLhosHPVZE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d56ba07b04f6a26463fb99f2d29054bf135f506a" title="Tilde AI Releases TildeOpen LLM: An Open-Source Large Language Model with Over 30 Billion Parameters and Support Most European Languages" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TildeOpen LLM is an open-source foundational language model built to serve underrepresented Nordic and Eastern European languages. Developed with European Commission funding and trained on the LUMI supercomputer, this 30B+ parameter model addresses the performance gaps that speakers of 19 focus languages‚Äîrepresenting over 165 million people‚Äîface with existing AI systems.&lt;/p&gt; &lt;p&gt;The model employs an equitable tokeniser and curriculum-learning approach to ensure fair representation across less-resourced languages, moving beyond the typical English-centric design of most language models. As an open-source project, TildeOpen LLM enables transparent research and community-driven development while maintaining European technological independence.&lt;/p&gt; &lt;p&gt;This foundational model is not yet adapted to follow instructions or aligned with safety features. The next version being built on top of this model will be a specialised translation model, leveraging TildeOpen LLM's multilingual foundation to provide high-quality translation capabilities across the supported European language pairs.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Languages:&lt;/strong&gt; Albanian, Bosnian, Bulgarian, Croatian, Czech, Danish, Dutch, English, Estonian, Finnish, French, German, Hungarian, Icelandic, Irish, Italian, Latgalian, Latvian, Lithuanian, Macedonian, Maltese, Montenegrin, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish, Turkish, Ukrainian as well of mathematical proofs, programming code and XML documents containing translation data&lt;/p&gt; &lt;p&gt;GGUF:&lt;br /&gt; &lt;a href="https://huggingface.co/mradermacher/TildeOpen-30b-GGUF"&gt;https://huggingface.co/mradermacher/TildeOpen-30b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TildeAI/TildeOpen-30b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbi95c/tilde_ai_releases_tildeopen_llm_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T08:32:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbt82m</id>
    <title>Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher.</title>
    <updated>2025-09-08T16:58:12+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"&gt; &lt;img alt="Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher." src="https://external-preview.redd.it/G0pBb0RCd46QXI7ZBwlDv7ScyXXHaae0jNeNWtdfkbk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6496fcee3c3c0005184df9b3cbe9dceaa06a0c4a" title="Drummer's Valkyrie 49B v2 - A finetune of Nemotron Super 49B v1.5, a pack puncher." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also updated my FAQ. Preparing a release on a Largestral 2407 and Small 22B tune too! (If anyone's interested, they're a bit smarter with the 'modern' tuning.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Valkyrie-49B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbt82m/drummers_valkyrie_49b_v2_a_finetune_of_nemotron/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T16:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbfy60</id>
    <title>Finishing touches on dual RTX 6000 build</title>
    <updated>2025-09-08T06:04:43+00:00</updated>
    <author>
      <name>/u/ikkiyikki</name>
      <uri>https://old.reddit.com/user/ikkiyikki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt; &lt;img alt="Finishing touches on dual RTX 6000 build" src="https://preview.redd.it/sez83piasvnf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=599c86ee050439c49f07c7056ff67e6c48ef8381" title="Finishing touches on dual RTX 6000 build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a dream build: 192 gigs of fast VRAM (and another 128 of RAM) but worried I'll burn the house down because of the 15A breakers.&lt;/p&gt; &lt;p&gt;Downloading Qwen 235B q4 :-)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ikkiyikki"&gt; /u/ikkiyikki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sez83piasvnf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbfy60/finishing_touches_on_dual_rtx_6000_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbly7o</id>
    <title>MiniCPM4.1-8B</title>
    <updated>2025-09-08T12:08:09+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model: &lt;a href="https://huggingface.co/openbmb/MiniCPM4.1-8B"&gt;https://huggingface.co/openbmb/MiniCPM4.1-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;8B hybrid reasoning model (/think vs /no_think)&lt;/li&gt; &lt;li&gt;InfLLM v2 sparse attention, natively supports 65K, RoPE scaling validated to 131K&lt;/li&gt; &lt;li&gt;BitCPM ternary quantization, FP8 and multi-token prediction&lt;/li&gt; &lt;li&gt;Eagle3 speculative decoding integrated in vLLM, SGLang, and CPM .cu with up to 3x faster reasoning&lt;/li&gt; &lt;li&gt;On Jetson Orin achieves approximately 7x faster decoding compared to Qwen3-8B and 3x reasoning speedup over MiniCPM4&lt;/li&gt; &lt;li&gt;Available in GPTQ, AutoAWQ, Marlin, GGUF, MLX, and Eagle3 draft variants&lt;/li&gt; &lt;li&gt;Apache 2.0&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbly7o/minicpm418b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T12:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc8r3q</id>
    <title>Sep 2025 : any open source project better than whisper for multilingual ASR?</title>
    <updated>2025-09-09T03:45:57+00:00</updated>
    <author>
      <name>/u/ae_dataviz</name>
      <uri>https://old.reddit.com/user/ae_dataviz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen launched Qwen3-ASR but thats not open source yet.&lt;br /&gt; My use case is *multilingual* ASR and I've been using OpenAI whisper for over 2 years. &lt;/p&gt; &lt;p&gt;Wondering if there were any new options in the market that is better and open source. Appreciate your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ae_dataviz"&gt; /u/ae_dataviz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8r3q/sep_2025_any_open_source_project_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8r3q/sep_2025_any_open_source_project_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8r3q/sep_2025_any_open_source_project_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T03:45:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbgosx</id>
    <title>Apocalyptic scenario: If you could download only one LLM before the internet goes down, which one would it be?</title>
    <updated>2025-09-08T06:50:43+00:00</updated>
    <author>
      <name>/u/sado361</name>
      <uri>https://old.reddit.com/user/sado361</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, a thought crossed my mind and I've been thinking about it for a few days. Let's say we have an apocalyptic scenario, like a zombie apocalypse. You have a Mac Studio with an M3 chip and 512 GB of RAM (it uses little power and can run large models). If such an apocalypse happened today, which local LLM would you download before the internet disappears? You only have a chance to download one. Electricity is not a problem.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sado361"&gt; /u/sado361 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbgosx/apocalyptic_scenario_if_you_could_download_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T06:50:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbslxu</id>
    <title>native tool calling support for DeepSeek V3.1 just merged in llama.cpp</title>
    <updated>2025-09-08T16:35:22+00:00</updated>
    <author>
      <name>/u/createthiscom</name>
      <uri>https://old.reddit.com/user/createthiscom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"&gt; &lt;img alt="native tool calling support for DeepSeek V3.1 just merged in llama.cpp" src="https://external-preview.redd.it/AaDrVOkhJbB5T7DYbjublcje7S3TXjWZXxoeBvGkbYY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bdc3f7c6f3e61642c4575d3c207d062eb6dd4b4" title="native tool calling support for DeepSeek V3.1 just merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I doubt many people are using it, but just FYI: native tool calling support (OpenAI style JSON request/response) for DeepSeek V3.1 was just merged into llama.cpp. To use, I think you have to start the server with `--jinja` and unset `--response_format`, or set it to `auto`. I personally use this feature quite a bit with Open Hands AI via docker with `-e LLM_NATIVE_TOOL_CALLING=true`, but you'll have to check your documentation to see if it is supported and how to enable it if you use a different client. Benefits include reduced context length and possibly better agentic reliability (time will tell).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/createthiscom"&gt; /u/createthiscom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15533"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbslxu/native_tool_calling_support_for_deepseek_v31_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T16:35:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc2w7h</id>
    <title>5090 vs 6000</title>
    <updated>2025-09-08T23:10:04+00:00</updated>
    <author>
      <name>/u/That-Thanks3889</name>
      <uri>https://old.reddit.com/user/That-Thanks3889</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;a student asked me which rig for learning and training models - i recommended the 6000 but with new hardware every month i'm taking it back ... wondering everyone else's opinion ? 5090 seems sufficient to learn and fine tune mistral etc ... and once they proficient they can rent cloud or spend money &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/That-Thanks3889"&gt; /u/That-Thanks3889 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc2w7h/5090_vs_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T23:10:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc7bxw</id>
    <title>Fast local push-to-talk speech-to-text dictation tool using whisper.cpp</title>
    <updated>2025-09-09T02:33:49+00:00</updated>
    <author>
      <name>/u/lxe</name>
      <uri>https://old.reddit.com/user/lxe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nc7bxw/video/v2nq7gt8w1of1/player"&gt;https://reddit.com/link/1nc7bxw/video/v2nq7gt8w1of1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was looking for a push-to-talk tool that allows me to just paste my speech transcription automatically into whatever application I'm using.&lt;/p&gt; &lt;p&gt;I wasn't able to find anything simple enough that works, so I built my own. It's a basic CLI that works on Linux using whisper.cpp.&lt;/p&gt; &lt;p&gt;It is incredibly simple. You hold down the buttons, say stuff, release and then it will pipe the transcription lines to stdout.&lt;/p&gt; &lt;p&gt;I'm using it to write this comment :)&lt;/p&gt; &lt;p&gt;Edit: Forgot the link. &lt;a href="https://github.com/lxe/yapyap"&gt;https://github.com/lxe/yapyap&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lxe"&gt; /u/lxe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc7bxw/fast_local_pushtotalk_speechtotext_dictation_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc7bxw/fast_local_pushtotalk_speechtotext_dictation_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc7bxw/fast_local_pushtotalk_speechtotext_dictation_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:33:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbkxnm</id>
    <title>Introducing IndexTTS-2.0: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech</title>
    <updated>2025-09-08T11:16:22+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are thrilled to announce the official open-sourcing of IndexTTS-2.0 - an emotionally rich and duration-controllable autoregressive zero-shot text-to-speech system. &lt;/p&gt; &lt;p&gt;- We innovatively propose a &amp;quot;time encoding&amp;quot; mechanism applicable to autoregressive systems, solving for the first time the challenge of precise speech duration control in traditional autoregressive models. &lt;/p&gt; &lt;p&gt;- The system also introduces a timbre-emotion decoupling modeling mechanism, offering diverse and flexible emotional control methods. Beyond single-audio reference, it enables precise adjustment of synthesized speech's emotional expression through standalone emotional reference audio, emotion vectors, or text descriptions, significantly enhancing the expressiveness and adaptability of generated speech. &lt;/p&gt; &lt;p&gt;The architecture of IndexTTS-2.0 makes it widely suitable for various creative and application scenarios, including but not limited to: AI voiceovers, audiobooks, dynamic comics, video translation, voice dialogues, podcasts, and more. We believe this system marks a crucial milestone in advancing zero-shot TTS technology toward practical applications. &lt;/p&gt; &lt;p&gt;Currently, the project paper, full code, model weights, and online demo page are all open-sourced. We warmly invite developers, researchers, and content creators to explore and provide valuable feedback. In the future, we will continue optimizing model performance and gradually release more resources and tools, looking forward to collaborating with the developer community to build an open and thriving technology ecosystem. &lt;/p&gt; &lt;p&gt;üëâ Repository: &lt;a href="https://github.com/index-tts/index-tts"&gt;https://github.com/index-tts/index-tts&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üëâ Paper: &lt;a href="https://arxiv.org/abs/2506.21619"&gt;https://arxiv.org/abs/2506.21619&lt;/a&gt; &lt;/p&gt; &lt;p&gt;üëâ Demo: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbkxnm/introducing_indextts20_a_breakthrough_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T11:16:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc74g8</id>
    <title>Open Source Alternative to NotebookLM</title>
    <updated>2025-09-09T02:23:33+00:00</updated>
    <author>
      <name>/u/Uiqueblhats</name>
      <uri>https://old.reddit.com/user/Uiqueblhats</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those of you who aren't familiar with SurfSense, it aims to be the &lt;strong&gt;open-source alternative to NotebookLM, Perplexity, or Glean.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;In short, it's a Highly Customizable AI Research Agent that connects to your personal external sources and Search Engines (Tavily, LinkUp), Slack, Linear, Jira, ClickUp, Confluence, Gmail, Notion, YouTube, GitHub, Discord, Airtable, Google Calendar and more to come.&lt;/p&gt; &lt;p&gt;I'm looking for contributors to help shape the future of SurfSense! If you're interested in AI agents, RAG, browser extensions, or building open-source research tools, this is a great place to jump in.&lt;/p&gt; &lt;p&gt;Here‚Äôs a quick look at what SurfSense offers right now:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports 100+ LLMs&lt;/li&gt; &lt;li&gt;Supports local Ollama or vLLM setups&lt;/li&gt; &lt;li&gt;6000+ Embedding Models&lt;/li&gt; &lt;li&gt;Works with all major rerankers (Pinecone, Cohere, Flashrank, etc.)&lt;/li&gt; &lt;li&gt;Hierarchical Indices (2-tiered RAG setup)&lt;/li&gt; &lt;li&gt;Combines Semantic + Full-Text Search with Reciprocal Rank Fusion (Hybrid Search)&lt;/li&gt; &lt;li&gt;50+ File extensions supported (Added Docling recently)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Podcasts&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Support for local TTS providers (Kokoro TTS)&lt;/li&gt; &lt;li&gt;Blazingly fast podcast generation agent (3-minute podcast in under 20 seconds)&lt;/li&gt; &lt;li&gt;Convert chat conversations into engaging audio&lt;/li&gt; &lt;li&gt;Multiple TTS providers supported&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;External Sources Integration&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Search Engines (Tavily, LinkUp)&lt;/li&gt; &lt;li&gt;Slack&lt;/li&gt; &lt;li&gt;Linear&lt;/li&gt; &lt;li&gt;Jira&lt;/li&gt; &lt;li&gt;ClickUp&lt;/li&gt; &lt;li&gt;Gmail&lt;/li&gt; &lt;li&gt;Confluence&lt;/li&gt; &lt;li&gt;Notion&lt;/li&gt; &lt;li&gt;Youtube Videos&lt;/li&gt; &lt;li&gt;GitHub&lt;/li&gt; &lt;li&gt;Discord&lt;/li&gt; &lt;li&gt;Airtable&lt;/li&gt; &lt;li&gt;Google Calandar&lt;/li&gt; &lt;li&gt;and more to come.....&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cross-Browser Extension&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The SurfSense extension lets you save any dynamic webpage you want, including authenticated content.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Interested in contributing?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;SurfSense is completely open source, with an active roadmap. Whether you want to pick up an existing feature, suggest something new, fix bugs, or help improve docs, you're welcome to join in.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/MODSetter/SurfSense"&gt;https://github.com/MODSetter/SurfSense&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Uiqueblhats"&gt; /u/Uiqueblhats &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc74g8/open_source_alternative_to_notebooklm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc74g8/open_source_alternative_to_notebooklm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc74g8/open_source_alternative_to_notebooklm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:23:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncccri</id>
    <title>Aquif-3.5-8B-Think is the proof that reasoning (and maybe all MoEs) needs larger expert sizes</title>
    <updated>2025-09-09T07:20:23+00:00</updated>
    <author>
      <name>/u/dobomex761604</name>
      <uri>https://old.reddit.com/user/dobomex761604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While waiting for gguf version of aquif-3.5-A4B-Think, I decided to try &lt;a href="https://huggingface.co/mradermacher/aquif-3.5-8B-Think-GGUF"&gt;8B thinking&lt;/a&gt; from the same series. Not only it's quite compact in reasoning, it's also more logical, more reasonable in it: in case of creative writing it sticks to the prompt, sometimes step-by-step, sometimes just gathers a &amp;quot;summary&amp;quot; and makes a plan - but it's always coherent and adheres to the given instructions. It almost feels like the perfect reasoning - clarify, add instructions and a plan, that's it.&lt;/p&gt; &lt;p&gt;Both thinking and the result are much better than Qwen3 30b a3b and 4b (both thinking, of course); and Qwen 4b is sometimes better than Qwen3 30b, so it makes me wonder: 1. What if MoE as a principle has a lower experts size threshold that ensures consistency? 2. What if Qwen3 thinking is missing a version with larger experts size? 3. How large is an experts size where performance drops too low to justify improved quality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dobomex761604"&gt; /u/dobomex761604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T07:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc5k36</id>
    <title>ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute</title>
    <updated>2025-09-09T01:09:13+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Recent advances in Large Language Models (LLMs) have been driven by test-time compute scaling - a strategy that improves reasoning by generating longer, sequential thought processes. While effective, this approach encounters a significant bottleneck as computation increases, where further computation offers only marginal performance gains. We argue this ceiling is not an inherent limit of the model's capability but a flaw in the scaling strategy itself, a phenomenon we term &amp;quot;Tunnel Vision&amp;quot;, where a model's imperfect initial steps lock it into a suboptimal reasoning path. To overcome this, we introduce a new scaling paradigm: native thought parallelism. We present ParaThinker, an end-to-end framework that trains an LLM to generate multiple, diverse reasoning paths in parallel and synthesize them into a superior final answer. By exploring different lines of thoughts simultaneously, ParaThinker effectively sidesteps the Tunnel Vision issue and unlocks the model's latent reasoning potential. Our approach demonstrates that scaling compute in parallel (width) is a more effective and efficient way to superior reasoning than simply scaling sequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves substantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5% for 7B models on average with 8 parallel paths), while adding only negligible latency overhead (7.1%). This enables smaller models to surpass much larger counterparts and establishes parallel thinking as a critical, efficient dimension for scaling future LLMs.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.04475"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc5k36/parathinker_native_parallel_thinking_as_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc5k36/parathinker_native_parallel_thinking_as_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T01:09:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc57zo</id>
    <title>Confusion about VRAM</title>
    <updated>2025-09-09T00:53:34+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I understand that having more GPU‚Äôs is good for inference, but if I remember from the days of SLI and Crossfire, the VRAM doesn‚Äôt stack. So why is it I see some people say that two 20GB cards are going to give them 40GB of VRAM. When I swear VRAM doesn‚Äôt work like that. Am I wrong or not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc57zo/confusion_about_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc57zo/confusion_about_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc57zo/confusion_about_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T00:53:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbyw3b</id>
    <title>3090 is it still a good buy?</title>
    <updated>2025-09-08T20:28:10+00:00</updated>
    <author>
      <name>/u/Ideabile</name>
      <uri>https://old.reddit.com/user/Ideabile</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got the opportunity to buy 2 Nvidia 3090 RTX 24GB for 600‚Ç¨ each.&lt;/p&gt; &lt;p&gt;I want to be run a bunch of llm workflows: this to self host some Claude code and to automate some burocracies I got.&lt;/p&gt; &lt;p&gt;Additionally I want to step up in the llm experimental path, so I can learn more about it and have the ML skill set.&lt;/p&gt; &lt;p&gt;Currently other video cards seems much more expensive I hardly believe it will ever get cheaper.&lt;/p&gt; &lt;p&gt;I saw some people recommending 2 x 3090 which would make 48gb of vram.&lt;/p&gt; &lt;p&gt;Is there any other budget friendly alternatives? Is this a good lasting investment?&lt;/p&gt; &lt;p&gt;Thank you in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ideabile"&gt; /u/Ideabile &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbyw3b/3090_is_it_still_a_good_buy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T20:28:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbqa1p</id>
    <title>Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!</title>
    <updated>2025-09-08T15:08:10+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt; &lt;img alt="Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!" src="https://preview.redd.it/et1syg58iynf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ab48cb0e9692e8d94764a7f031fd34d0db1ae95" title="Qwen released API (only) Qwen3-ASR ‚Äî the all-in-one speech recognition model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üéôÔ∏è Meet Qwen3-ASR ‚Äî the all-in-one speech recognition model!&lt;/p&gt; &lt;p&gt;‚úÖ High-accuracy EN/CN + 9 more languages: ar, de, en, es, fr, it, ja, ko, pt, ru, zh&lt;/p&gt; &lt;p&gt;‚úÖ Auto language detection&lt;/p&gt; &lt;p&gt;‚úÖ Songs? Raps? Voice with BGM? No problem. &amp;lt;8% WER&lt;/p&gt; &lt;p&gt;‚úÖ Works in noise, low quality, far-field&lt;/p&gt; &lt;p&gt;‚úÖ Custom context? Just paste ANY text ‚Äî names, jargon, even gibberish üß†&lt;/p&gt; &lt;p&gt;‚úÖ One model. Zero hassle.Great for edtech, media, customer service &amp;amp; more.&lt;/p&gt; &lt;p&gt;API: &lt;a href="https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031"&gt;https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Modelscope Demo: &lt;a href="https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo"&gt;https://modelscope.cn/studios/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-ASR-Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=41e4c0f6175f9b004a03a07e42343eaaf48329e7&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/et1syg58iynf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbqa1p/qwen_released_api_only_qwen3asr_the_allinone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:08:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbo33p</id>
    <title>UAE Preparing to Launch K2 Think, "the world‚Äôs most advanced open-source reasoning model"</title>
    <updated>2025-09-08T13:42:30+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt; &lt;img alt="UAE Preparing to Launch K2 Think, &amp;quot;the world‚Äôs most advanced open-source reasoning model&amp;quot;" src="https://external-preview.redd.it/3A4olwwXC7kAmitvVkfkfzLywUYc6IvJ9He-QlxgRLY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2afac7f2d1366e35e6945533e06a6756d060e202" title="UAE Preparing to Launch K2 Think, &amp;quot;the world‚Äôs most advanced open-source reasoning model&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;In the coming week, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI) and G42 will release K2 Think, the world‚Äôs most advanced open-source reasoning model. &lt;strong&gt;Designed to be leaner and smarter, K2 Think delivers frontier-class performance in a remarkably compact form&lt;/strong&gt; ‚Äì often matching, or even surpassing, the results of models an order of magnitude larger. The result: greater efficiency, more flexibility, and broader real-world applicability.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.wam.ae/en/article/bll7llv-recognition-sheikh-khalifa%E2%80%99s-contribution"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbo33p/uae_preparing_to_launch_k2_think_the_worlds_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T13:42:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nbr45v</id>
    <title>Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!</title>
    <updated>2025-09-08T15:39:53+00:00</updated>
    <author>
      <name>/u/CornerLimits</name>
      <uri>https://old.reddit.com/user/CornerLimits</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt; &lt;img alt="Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!" src="https://external-preview.redd.it/PecTYmNSbm5tb-T9OW67-xyMoNn-SzofgAKif5I3sUI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e84ff63d5e5bbf73f86b2641f6f74955f0a20dbe" title="Poor man‚Äôs FlashAttention: Llama.cpp-gfx906 fork!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just released a fork of llama.cpp that implements some strong optimizations for the MI50/MI60/Vega7 series. &lt;/p&gt; &lt;p&gt;Thanks to the outstanding work of open source community I made a final effort to actually make flash attention FASTER than no flash attention in almost every case. Yeah‚Ä¶ almost.&lt;/p&gt; &lt;p&gt;The goal is to run ~30B models with ~30K ctx on a single card at decent speed.&lt;/p&gt; &lt;p&gt;You can find benchmarks, compile/launch/bench scripts, references to the original works and explanations of my new kernel in the repo.&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CornerLimits"&gt; /u/CornerLimits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/iacopPBK/llama.cpp-gfx906"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nbr45v/poor_mans_flashattention_llamacppgfx906_fork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T15:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncb2v4</id>
    <title>Do you trust benchmarks?</title>
    <updated>2025-09-09T05:58:20+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncb2v4/do_you_trust_benchmarks/"&gt; &lt;img alt="Do you trust benchmarks?" src="https://preview.redd.it/pq4v9byxw2of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2be542a16aa033aed9446204272af7e657e75006" title="Do you trust benchmarks?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pq4v9byxw2of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncb2v4/do_you_trust_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncb2v4/do_you_trust_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T05:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc8e2o</id>
    <title>My rankings of Huge Local SOTA Models for technical work</title>
    <updated>2025-09-09T03:27:03+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek v3.1 Q4&lt;/p&gt; &lt;p&gt;Qwen3-235B-A22B Q8&lt;/p&gt; &lt;p&gt;GLM-4.5 Q8&lt;/p&gt; &lt;p&gt;Kimi-K2-0905 Q3&lt;/p&gt; &lt;p&gt;GPT-OSS-120b Q8&lt;/p&gt; &lt;p&gt;I have been experimenting with these the last few days, inference engine is llama.cpp.&lt;/p&gt; &lt;p&gt;DeepSeek is great, only model that could answer question that other models failed from my private eval.&lt;/p&gt; &lt;p&gt;Qwen3-235B is great, for the size, but believe it or not, it's slower than DeepSeek, DeepSeek despite it's size is super fast!&lt;/p&gt; &lt;p&gt;GLM-4.5 is great when it has been exposed to that knowledge, but sometimes it gives very stupid answer to unseen knowledge especially when it think it's a trick question. Amazing for UI work.&lt;/p&gt; &lt;p&gt;Kimi-K2 is great, I just might put it on the same performance level as GLM. It's huge at Q3, I really think it would be a heck of a model at Q4 or Q6, but I don't have the system to run it yet.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B is not bad at all for it's size, by bar it's very tiny compared to the others and the main benefit is that it flies. I get 100tk/sec with it. For non difficult task, I would use this first and only go to the big ones if stuck.&lt;/p&gt; &lt;p&gt;I never liked the large Qwen3-Coder model and deleted it after I drove it. This is just about the latest big relevant models, don't ask me to compare any other model. Just my personal ranking based on my private questions/evals. I didn't try GLM-Air with my evals yet, but I reckon it will sit or tie with GPT-OSS-120B based on my mucking around with it.&lt;/p&gt; &lt;p&gt;BTW, I noticed that my eval that was about 15% pass rate at the beginning of the year is now nearing 85%. I need to rebuild with more complex problems. My evals are also pretty much 1 pass! The models are so damn good, for example, I kept expecting to see syntax errors when I had it generate C program with threads, locks, pointers, etc and I will get 500 lines of code that will compile with no errors and run!&lt;/p&gt; &lt;p&gt;I did a little bit of multi turn agent with DeepSeekv3.1 and GLM-4.5 and results were great.&lt;/p&gt; &lt;p&gt;Smaller models are great BTW from my playing around last month, gemma-3-27b, mistral-small-3.2, qwen3-32b/30b. But the QUALITY of code is not even comparable to the huge models. It's the difference between a mid level engineer and a staff/principal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8e2o/my_rankings_of_huge_local_sota_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8e2o/my_rankings_of_huge_local_sota_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8e2o/my_rankings_of_huge_local_sota_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T03:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc1p0a</id>
    <title>Where are people finding RTX PRO 6000 96gb cards for under 7k</title>
    <updated>2025-09-08T22:18:44+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everywhere ive seen, they are like 8.5k, but people comstantly mention that they can be had for around 6.5k. How? Where? I want to start moving away from paid services like claude and start moving towards self-hosting, starting with an rtx pro 6000 + 3090. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T22:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncam9h</id>
    <title>PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp; Web Dev Code, At 1/400th the Size!</title>
    <updated>2025-09-09T05:30:13+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt; &lt;img alt="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" src="https://external-preview.redd.it/aGZzYWtwcWJuMm9mMRQQfge2rofWKaGSqifIYqgzhyk7YhqLzgXg182Z60l8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c92850d1b6d2e678f57f8a6ff40aec39df02bb6" title="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bralynn/pydevmini1"&gt;https://huggingface.co/bralynn/pydevmini1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I'm incredibly excited to release &lt;strong&gt;PyDevMini-1&lt;/strong&gt;, a 4B parameter model to provide GPT-4 level performance for Python and web coding development tasks. Two years ago, GPT-4 was the undisputed SOTA, a multi-billion-dollar asset running on massive datacenter hardware. The open-source community has closed that gap at &lt;strong&gt;1/400th of the size&lt;/strong&gt;, and it runs on an average gaming GPU.&lt;/p&gt; &lt;p&gt;I believe that powerful AI should not be a moat controlled by a few large corporations. Open source is our best tool for the democratization of AI, ensuring that individuals and small teams‚Äîthe little guys‚Äîhave a fighting chance to build the future. This project is my contribution to that &lt;a href="http://effort.You"&gt;effort.You&lt;/a&gt; won't see a list of benchmarks here. Frankly, like many of you, I've lost faith in their ability to reflect true, real-world model quality. Although this model's benchmark scores are still very high, it exaggerates the difference in quality above GPT4, as GPT is much less likely to have benchmarks in its pretraining data from its earlier release, causing lower than reflective model quality scores for GPT4, as newer models tend to be trained directly toward benchmarks, making it unfair for GPT.&lt;/p&gt; &lt;p&gt;Instead, I've prepared a video demonstration showing PyDevMini-1 side-by-side with GPT-4, tackling a very small range of practical Python and web development challenges. I invite you to judge the performance for yourself to truly show the abilities it would take a 30-minute showcase to display. This model consistently punches above the weight of models 4x its size and is highly intelligent and creative&lt;/p&gt; &lt;p&gt;üöÄ &lt;strong&gt;Try It Yourself (for free)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Don't just take my word for it. Test the model right now under the exact conditions shown in the video.&lt;br /&gt; &lt;a href="https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing"&gt;https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model's roadmap will be dictated by you. My goal isn't just to release a good model; it's to create the perfect open-source coding assistant for the tasks we all face every day. To do that, I'm making a personal guarantee. Your Use Case is My Priority. You have a real-world use case where this model struggles‚Äîa complex boilerplate to generate, a tricky debugging session, a niche framework question‚ÄîI will personally make it my mission to solve it. Your posted failures are the training data for the next version. I will not stop tuning until we've addressed every unique, well-documented challenge submitted by the community on top of my own personal training loops to create a top-tier model for us all.&lt;/p&gt; &lt;p&gt;For any and all feedback, simply make a post here and I'll make sure too check in or join our Discord! - &lt;a href="https://discord.gg/RqwqMGhqaC"&gt;https://discord.gg/RqwqMGhqaC&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üôè Acknowledgment &amp;amp; The Foundation&lt;/h1&gt; &lt;p&gt;This project stands on the shoulders of giants. A massive thank you to the &lt;strong&gt;Qwen team&lt;/strong&gt; for the incredible base model, &lt;strong&gt;Unsloth's Duo&lt;/strong&gt; for making high-performance training accessible, and &lt;strong&gt;Tesslate&lt;/strong&gt; for their invaluable contributions to the community. This would be impossible for an individual without their foundational work.&lt;/p&gt; &lt;p&gt;Any and all Web Dev Data is sourced from the wonderful work done by the team at Tesslate. Find their new SOTA webdev model here -&lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking this out. And remember: &lt;strong&gt;This is the worst this model will ever be.&lt;/strong&gt; I can't wait to see what we build together.&lt;/p&gt; &lt;p&gt;Also I suggest using &lt;code&gt;Temperature=0.7&lt;/code&gt;, &lt;code&gt;TopP=0.8&lt;/code&gt;, &lt;code&gt;TopK=20&lt;/code&gt;, and &lt;code&gt;MinP=0&lt;/code&gt;.&lt;br /&gt; As &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt; is the base model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Causal Language Models&lt;/li&gt; &lt;li&gt;Training Stage: Pretraining &amp;amp; Post-training&lt;/li&gt; &lt;li&gt;Number of Parameters: 4.0B&lt;/li&gt; &lt;li&gt;Number of Paramaters (Non-Embedding): 3.6B&lt;/li&gt; &lt;li&gt;Number of Layers: 36&lt;/li&gt; &lt;li&gt;Number of Attention Heads (GQA): 32 for Q and 8 for KV&lt;/li&gt; &lt;li&gt;Context Length: &lt;strong&gt;262,144 natively&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nh9fq7qbn2of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T05:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc79yg</id>
    <title>baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face</title>
    <updated>2025-09-09T02:31:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt; &lt;img alt="baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face" src="https://external-preview.redd.it/PVc8HBAyReu1sVKS98fa6WZXbf4lkkgSEZVgozf_73w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6d7b67af3eb2cf9bcf96edfb103c94299b667a8" title="baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Highlights&lt;/h1&gt; &lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of ERNIE-4.5-21B-A3B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning, thereby advancing the competitiveness of ERNIE &lt;strong&gt;lightweight models&lt;/strong&gt; in complex reasoning tasks. We are pleased to introduce &lt;strong&gt;ERNIE-4.5-21B-A3B-Thinking&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient tool usage&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 128K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF"&gt;https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
