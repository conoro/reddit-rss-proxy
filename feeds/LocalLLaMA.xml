<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-27T23:48:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1px78ty</id>
    <title>Need recommendations LLM fine-tuning experts?</title>
    <updated>2025-12-27T20:07:47+00:00</updated>
    <author>
      <name>/u/ricturner</name>
      <uri>https://old.reddit.com/user/ricturner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We need LLM fine tuning experts to help us fine tune models for our customer support automation and honestly it's been way harder than we thought. We've got around 6k training examples from our actual support tickets but our attempts at fine tuning are giving us mixed results that don't really justify the extra cost over just using base models with better prompts.&lt;/p&gt; &lt;p&gt;Main issue is we can't figure out if our training data needs better cleaning, if we're setting parameters wrong, or if fine tuning even makes sense for our use case. We need experts who've actually done this before and can tell us straight up what's worth doing versus what's just burning money.&lt;/p&gt; &lt;p&gt;Need someone with real production experience, not just people who've read the docs. They should be able to audit our training data, run the fine tuning properly, and actually prove the custom model performs better than our current setup. Initially we shortlisted a few options here and Lexis Solutions seems good based on their LLMs fine-tuning portfolio but wanted to hear from people who've actually hired experts for this kind of thing.&lt;/p&gt; &lt;p&gt;The goal is finalize by the end of holidays. Would definitely appreciate any recommendations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ricturner"&gt; /u/ricturner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px78ty/need_recommendations_llm_finetuning_experts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px78ty/need_recommendations_llm_finetuning_experts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px78ty/need_recommendations_llm_finetuning_experts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T20:07:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1px234n</id>
    <title>Seeking "Abliterated" Gemma 3 or Llama 3.3 that retains logic and multilingual (Slovak/Czech) capabilities</title>
    <updated>2025-12-27T16:37:26+00:00</updated>
    <author>
      <name>/u/FollowingFresh6411</name>
      <uri>https://old.reddit.com/user/FollowingFresh6411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m looking for a specific type of model recommendation. I’ve been testing &lt;strong&gt;Gemma 3 12B/27B&lt;/strong&gt; and I’m impressed by its reasoning and excellent support for Central European languages (especially Slovak and Czech). However, the base/instruct versions are far too restrictive and prone to preachy refusals.&lt;/p&gt; &lt;p&gt;I’m looking for an &lt;strong&gt;uncensored or abliterated&lt;/strong&gt; version, but with a few crucial requirements:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;No &amp;quot;Brain Damage&amp;quot;:&lt;/strong&gt; I’ve tried some uncensored tunes that completely lost their reasoning capabilities or became repetitive. I need the model to stay as smart as the original.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Retention:&lt;/strong&gt; Many abliteration techniques or fine-tunes are heavily slanted toward English. I need a model that hasn't &amp;quot;forgotten&amp;quot; how to speak Slovak/Czech fluently while being uncensored.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logical Consistency:&lt;/strong&gt; It shouldn't just be &amp;quot;edgy&amp;quot;; it should follow complex instructions without breaking character or losing the thread of the conversation.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FollowingFresh6411"&gt; /u/FollowingFresh6411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px234n/seeking_abliterated_gemma_3_or_llama_33_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px234n/seeking_abliterated_gemma_3_or_llama_33_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px234n/seeking_abliterated_gemma_3_or_llama_33_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:37:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1px052k</id>
    <title>Worth the 5090?</title>
    <updated>2025-12-27T15:15:43+00:00</updated>
    <author>
      <name>/u/fgoricha</name>
      <uri>https://old.reddit.com/user/fgoricha</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am debating on getting a 5090. &lt;/p&gt; &lt;p&gt;I currently have a 3090 that is actively being used for projects. It is an old 64gb ddr4 system with an i5 cpu and a 750 w psu. So nothing fancy and got it cheap. I have half a set up for a dual 3090 build. The plan was to create a dual or triple 3090 build. I have been collecting parts slowly over time without without over paying for them. I have 3 x 3090, a mobo that has spacing for 2 x 3090 (would need risers for the third 3090), 64 gb of ram ddr4, and a large psu. &lt;/p&gt; &lt;p&gt;I have used my single 3090 set up for fine tuning small models. Using lm studio, whisper, rag, and knowledge graphs. I then got into Yolo image training. I have not gotten into gaming but maybe some day. I plan to get back into traing LLMs and playing with them after my Yolo model.&lt;/p&gt; &lt;p&gt;I really like my single 3090 set up as it does not take up a lot of table space. All of my use cases have fit nicely on a single 3090 card. I think a draw back I noticed was needing to unload and load new models as part of the pipeline. I thought this could be fixed with dual 3090s so one model can live on each card. Also concerned about electrical and needing to put more thought into a build than just plug and play. One thing I wish I had was a faster card, and see that the 5090 is a significant upgrade over 3090. However the 48 vs 32 gb of vram is always nice, but the 16gb difference might not be a game changer.&lt;/p&gt; &lt;p&gt;I plan on renting a cloud 5090 to compare my current projects and see some hard numbers for speed up for training LLM models, training my Yolo models and LLM inference speeds.&lt;/p&gt; &lt;p&gt;But wht do you all think? If you were me, would finish building the dual 3090 build? Or sell two of the 3090s and put it towards a 5090?&lt;/p&gt; &lt;p&gt;I don't see myself really using more than 24 gb of vram, but I also tend to plan projects around the hardware that I have.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fgoricha"&gt; /u/fgoricha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px052k/worth_the_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px052k/worth_the_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px052k/worth_the_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T15:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1px89i4</id>
    <title>Advice Needed: Gate Model Training / Full Training / LoRA Adapters</title>
    <updated>2025-12-27T20:51:37+00:00</updated>
    <author>
      <name>/u/RefrigeratorCalm9701</name>
      <uri>https://old.reddit.com/user/RefrigeratorCalm9701</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;I’m working on building a &lt;strong&gt;gate model&lt;/strong&gt; and exploring training strategies. I’ve developed a production-grade transformer training framework supporting &lt;strong&gt;Mixture of Experts (MoE)&lt;/strong&gt; and &lt;strong&gt;Mixture of Depths (MoD)&lt;/strong&gt; architectures with custom CUDA kernels and multi-GPU support, capable of scaling from ~500M to 300B+ parameters. The system handles everything from tokenization to gradient optimization, includes sparse routing mechanisms, memory-efficient data processing, and precision-aware training (FP32/FP16/BF16/experimental FP8).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I’m evaluating:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Training a model from scratch versus using &lt;strong&gt;LoRA/adapters&lt;/strong&gt; on an existing base&lt;/li&gt; &lt;li&gt;Token-level gating efficiency and routing dynamics for MoE&lt;/li&gt; &lt;li&gt;Layer-level skipping strategies for MoD and their impact on FLOPs vs quality&lt;/li&gt; &lt;li&gt;Hybrid approaches (MoE + MoD) and tradeoffs in compute vs active parameters&lt;/li&gt; &lt;li&gt;Scaling experiments and performance considerations for large clusters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MoE:&lt;/strong&gt; Top-2 of N experts per token, capacity factors dynamically managed, CUDA-optimized dispatch and combination&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MoD:&lt;/strong&gt; Token-level layer skipping with learnable routing, curriculum for capacity_factor annealing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Precision:&lt;/strong&gt; Mixed FP16/BF16 by default, with full CUDA acceleration and fallback to PyTorch&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Monitoring:&lt;/strong&gt; Expert utilization, routing entropy, FLOPs reduction, convergence tracking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; NVIDIA (Volta→Hopper), Apple Silicon (M1-M4), CPU fallback&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m interested in &lt;strong&gt;practical insights&lt;/strong&gt; from those who have worked on sparse architectures or gating systems:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full training vs adapter-based approaches for gate models&lt;/li&gt; &lt;li&gt;Experience with MoE/MoD hybrid training and capacity tuning&lt;/li&gt; &lt;li&gt;Scaling considerations, token routing stability, or FLOPs tradeoffs&lt;/li&gt; &lt;li&gt;Common pitfalls or strategies for training large-scale sparse models efficiently&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any benchmarks, strategies, or caveats would be really useful. I’m aiming for a system where &lt;strong&gt;active computation is minimized without sacrificing quality&lt;/strong&gt;, and I want to compare approaches for real-world efficiency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RefrigeratorCalm9701"&gt; /u/RefrigeratorCalm9701 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px89i4/advice_needed_gate_model_training_full_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px89i4/advice_needed_gate_model_training_full_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px89i4/advice_needed_gate_model_training_full_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T20:51:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbg4x</id>
    <title>SOCAMM2 - new(ish), screwable (replaceable, non soldered) LPDDR5X RAM standard intended for AI data centers.</title>
    <updated>2025-12-27T23:10:47+00:00</updated>
    <author>
      <name>/u/-InformalBanana-</name>
      <uri>https://old.reddit.com/user/-InformalBanana-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Samsung introduces SOCAMM2 LPDDR5X memory module for AI data centers — new standard set to offer reduced power consumption and &lt;strong&gt;double the bandwidth&lt;/strong&gt; versus DDR5 RDIMMs.&lt;/p&gt; &lt;p&gt;The SOCAMM2 LPDDR5X-based module is being positioned as a standardized, serviceable alternative to soldered memory as AI servers chase higher bandwidth.&lt;/p&gt; &lt;p&gt;Hopefully this gets represented and used more in the consumer market.&lt;/p&gt; &lt;p&gt;More info:&lt;/p&gt; &lt;p&gt;&lt;a href="https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/"&gt;https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers"&gt;https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-InformalBanana-"&gt; /u/-InformalBanana- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1px1z6i</id>
    <title>AI MAX 395 using NPU on linux</title>
    <updated>2025-12-27T16:32:56+00:00</updated>
    <author>
      <name>/u/UnbeliebteMeinung</name>
      <uri>https://old.reddit.com/user/UnbeliebteMeinung</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to find a my way into the topic of local llms. I got this mini pc with the ai max 395 chip.&lt;/p&gt; &lt;p&gt;After hours i now am able to run some LLMs on the GPU (with rocm) instead of only CPU work on a ubuntu linux installed on the system.&lt;/p&gt; &lt;p&gt;But how do i even use the NPU? Any directions for me?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnbeliebteMeinung"&gt; /u/UnbeliebteMeinung &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1z6i/ai_max_395_using_npu_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1z6i/ai_max_395_using_npu_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px1z6i/ai_max_395_using_npu_on_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:32:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1px9usk</id>
    <title>Do you pay for curated datasets, or is scraped/free data good enough?</title>
    <updated>2025-12-27T22:00:28+00:00</updated>
    <author>
      <name>/u/Lost_Transportation1</name>
      <uri>https://old.reddit.com/user/Lost_Transportation1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Genuine question about how people source training data for fine-tuning projects.&lt;/p&gt; &lt;p&gt;If you needed specialist visual data (say, historical documents, architectural drawings, handwritten manuscripts), would you:&lt;/p&gt; &lt;p&gt;a) Scrape what you can find and deal with the noise &lt;/p&gt; &lt;p&gt;b) Use existing open datasets even if they're not ideal &lt;/p&gt; &lt;p&gt;c) Pay for a curated, licensed dataset if the price is right.&lt;/p&gt; &lt;p&gt;And if (c), what price range makes sense? Per image, per dataset, subscription?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lost_Transportation1"&gt; /u/Lost_Transportation1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px9usk/do_you_pay_for_curated_datasets_or_is_scrapedfree/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px9usk/do_you_pay_for_curated_datasets_or_is_scrapedfree/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px9usk/do_you_pay_for_curated_datasets_or_is_scrapedfree/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:00:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxc0p5</id>
    <title>Is this a thing?</title>
    <updated>2025-12-27T23:36:06+00:00</updated>
    <author>
      <name>/u/OldCulprit</name>
      <uri>https://old.reddit.com/user/OldCulprit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More for local or consumer, but instead of video card(s) for Local LLM, how about a video board.&lt;/p&gt; &lt;p&gt;Reading the discussion around the 72GB NVidia card, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt;NVIDIA has 72GB VRAM version now : r/LocalLLaMA&lt;/a&gt; and how there are limitations on RAM density due to trace width, clock speeds, noise etc. - some one should develop a video board. &lt;/p&gt; &lt;p&gt;Roughly the size of a mother board that sits above it in the case and uses one or multiple PCIe connectors for passing data back and forth to the rest of the components in the system. Spread things out a bit providing more memory, wider busses, faster speeds, better cooling etc. &lt;/p&gt; &lt;p&gt;I mean wasn't it a thing with CPU design at one point. There was a limit to scaling out - so they scaled up. Started stacking and layering things.&lt;/p&gt; &lt;p&gt;It would probably kill the aesthetics of a blinged out case with the fancy light shows and such, but that's a good thing right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldCulprit"&gt; /u/OldCulprit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxc0p5/is_this_a_thing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxc0p5/is_this_a_thing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxc0p5/is_this_a_thing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:36:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pweljh</id>
    <title>NVIDIA has 72GB VRAM version now</title>
    <updated>2025-12-26T20:48:17+00:00</updated>
    <author>
      <name>/u/decentralize999</name>
      <uri>https://old.reddit.com/user/decentralize999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt; &lt;img alt="NVIDIA has 72GB VRAM version now" src="https://external-preview.redd.it/sC0_RV1rBP5Nka4zzrlrlknHQcvT_QUrChxq3hP_lVg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e745729c3f7132892c715292c6b31f385f223e8f" title="NVIDIA has 72GB VRAM version now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is 96GB too expensive? And AI community has no interest for 48GB?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/decentralize999"&gt; /u/decentralize999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-5000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pweljh/nvidia_has_72gb_vram_version_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T20:48:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwybpe</id>
    <title>XiaomiMiMo.MiMo-V2-Flash: is there a reason why i see so few ggufs?</title>
    <updated>2025-12-27T13:52:37+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"&gt; &lt;img alt="XiaomiMiMo.MiMo-V2-Flash: is there a reason why i see so few ggufs?" src="https://b.thumbs.redditmedia.com/tLv4egdIALish6T1JvUuABgC-fkC5rwBTFo_swD0v1U.jpg" title="XiaomiMiMo.MiMo-V2-Flash: is there a reason why i see so few ggufs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/n58umc1l4r9g1.png?width=1334&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=538ec36b5f10702f983a6d812e260e470663342e"&gt;xiaomimimo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I've been testing the model for two days. It's incredibly fast at generating tokens compared to other models (certainly faster than both GLM and Minimax).&lt;/p&gt; &lt;p&gt;But I see few people talking about it and few posts. Is there a specific reason? Even Unsloth hasn't released anything yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybpe/xiaomimimomimov2flash_is_there_a_reason_why_i_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T13:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxai05</id>
    <title>RPC-server llama.cpp benchmarks</title>
    <updated>2025-12-27T22:28:25+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Running a few LLM benchmarks to see how RPC-server is doing.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The llama.cpp RPC server is a tool that allows for distributed inference of large language models (LLMs) across multiple machines or GPUs by offloading computations to remote instances.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Benchmarks done on local gigabit network across 3 systems and 5 GPUs.&lt;/p&gt; &lt;p&gt;System 1: AMD FX-8350 CPU, 32GB DDR3, CachyOS Kernel 6.18.2-1 on KDE, &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877"&gt;GTX 1080Ti&lt;/a&gt; (11GB) and&lt;a href="https://www.techpowerup.com/gpu-specs/p102-101.c3284"&gt; Nvidia P102-100&lt;/a&gt; (10GB) 21GB VRAM GPUs.&lt;/p&gt; &lt;p&gt;System 2: Intel i7 7800X CPU, 48GB DDR4, Kubuntu 26.04 Kernel 6.17, GTX 1080Ti (11GB) and Nvidia P102-100 (10GB) 21GB VRAM GPUs.&lt;/p&gt; &lt;p&gt;System 3: AMD Ryzen 5 5600X, 64GB DDR4, Kubuntu 24.04 Kernel 6.14, Radeon &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-rx-7900-gre.c4166"&gt;RX 7900 GRE&lt;/a&gt; (16GB)&lt;/p&gt; &lt;p&gt;Total 53GB of VRAM available on local gigabit network.&lt;/p&gt; &lt;p&gt;Llama.cpp Ubuntu Vulkan build: 06705fdcb (&lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b7552/llama-b7552-bin-ubuntu-vulkan-x64.tar.gz"&gt;7552&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;&lt;code&gt;time /llama-bench --rpc 10.0.0.51:50051,10.0.0.173:50053 -m /&lt;/code&gt;&lt;br /&gt; &lt;code&gt;Nemotron-3-Nano-30B-A3B-Q6_K.gguf&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_backend: loaded RPC backend from /media/czar33/x_2tb/vulkan/llama-b7552/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon RX 7900 GRE (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat load_backend: loaded Vulkan backend from /media/czar33/x_2tb/vulkan/llama-b7552/libggml-vulkan.so load_backend: loaded CPU backend from /media/czar33/x_2tb/vulkan/llama-b7552/libggml-cpu-haswell.so &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Nemotron-3-Nano-30B-A3B-Q6_K&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q6_K&lt;/td&gt; &lt;td align="left"&gt;31.20 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;165.15 ± 12.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nemotron_h_moe 31B.A3.5B Q6_K&lt;/td&gt; &lt;td align="left"&gt;31.20 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;51.05 ± 0.87&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Phi-3.5-MoE-instruct-Q6_K_L&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;phimoe 16x3.8B Q6_K&lt;/td&gt; &lt;td align="left"&gt;32.06 GiB&lt;/td&gt; &lt;td align="left"&gt;41.87 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;79.18 ± 4.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;phimoe 16x3.8B Q6_K&lt;/td&gt; &lt;td align="left"&gt;32.06 GiB&lt;/td&gt; &lt;td align="left"&gt;41.87 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;37.83 ± 2.20&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-R1-Distill-Llama-70B-UD-Q3_K_XL&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B Q3_K - Medium&lt;/td&gt; &lt;td align="left"&gt;32.47 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;37.30 ± 0.66&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 70B Q3_K - Medium&lt;/td&gt; &lt;td align="left"&gt;32.47 GiB&lt;/td&gt; &lt;td align="left"&gt;70.55 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;3.80 ± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct-UD-Q4_K_XL&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;42.01 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;44.95 ± 0.81&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3next 80B.A3B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;42.01 GiB&lt;/td&gt; &lt;td align="left"&gt;79.67 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.00 ± 3.65&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Cerebras_GLM-4.5-Air-REAP-82B-A12B-IQ4_XS&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;43.75 GiB&lt;/td&gt; &lt;td align="left"&gt;84.99 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;45.05 ± 1.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;glm4moe 106B.A12B IQ4_XS - 4.25 bpw&lt;/td&gt; &lt;td align="left"&gt;43.75 GiB&lt;/td&gt; &lt;td align="left"&gt;84.99 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;19.84 ± 0.50&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Llama-4-Scout-17B-16E-Instruct-IQ3_XXS&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;88.90 ± 5.17&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;4.67 ± 1.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Mixtral-8x22B-v0.1.i1-IQ2_M&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8x22B IQ2_M - 2.7 bpw&lt;/td&gt; &lt;td align="left"&gt;43.50 GiB&lt;/td&gt; &lt;td align="left"&gt;140.62 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;23.70 ± 0.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama 8x22B IQ2_M - 2.7 bpw&lt;/td&gt; &lt;td align="left"&gt;43.50 GiB&lt;/td&gt; &lt;td align="left"&gt;140.62 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.14 ± 0.79&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxai05/rpcserver_llamacpp_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxai05/rpcserver_llamacpp_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxai05/rpcserver_llamacpp_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:28:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1px6t93</id>
    <title>Llama 3.2 3B fMRI update (early findings)</title>
    <updated>2025-12-27T19:49:46+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello all! I was exploring some logs, when I noticed something interesting. across multiple layers and steps, one dim kept popping out as active: 3039.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nwq1woi7vs9g1.png?width=1858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9dafbdf4058a87814294f56c1ba2795dab9d0ebc"&gt;step 7, basic greeting prompt. that dim that's constantly engaged is 3039.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gukr15afvs9g1.png?width=1858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=455716c8ec94bd5166727f2bbe162e345732747b"&gt;Here is the same prompt, several steps later. that dim stays consistent on steps in between &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not quite sure what to do with this information yet, but wanted to share because I found it pretty interesting!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px6t93/llama_32_3b_fmri_update_early_findings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px6t93/llama_32_3b_fmri_update_early_findings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px6t93/llama_32_3b_fmri_update_early_findings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T19:49:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbg15</id>
    <title>Best multilingual models for NSFW storytelling?</title>
    <updated>2025-12-27T23:10:40+00:00</updated>
    <author>
      <name>/u/FollowingFresh6411</name>
      <uri>https://old.reddit.com/user/FollowingFresh6411</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What are you currently using for erotic stories in languages other than English? I’ve been trying some Llama 3.1/3.3 variants, but the prose feels a bit robotic once I leave English.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I need something that is &amp;quot;lewd-friendly&amp;quot; and handles language , etc. French/Slovak/Spanish natively. I’ve heard Qwen 2.5/3 is the king of multilingual right now, but is there a specific uncensored version that works well for erotica? Or should I stick with Mistral based models?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any advice on settings or specific models would be much appreciated!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FollowingFresh6411"&gt; /u/FollowingFresh6411 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg15/best_multilingual_models_for_nsfw_storytelling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg15/best_multilingual_models_for_nsfw_storytelling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg15/best_multilingual_models_for_nsfw_storytelling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbgyf</id>
    <title>How to get SOTA opensource models (GLM 4.7, Kimi K2) to do multistep coding automatically? On Claude Code? They keep stopping after 2 or 3 steps...</title>
    <updated>2025-12-27T23:11:47+00:00</updated>
    <author>
      <name>/u/FigZestyclose7787</name>
      <uri>https://old.reddit.com/user/FigZestyclose7787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honest question: How do you guys get K2, GLM 4.7 models to work automatically on Coding AGents such as Claude Code? I know these models are plenty powerful, and may work well on other harnesses (roocode, cline, etc) but on Claude Code they hang... on need me to type continue after every 2 or 3 interactions, making it completely unusable. What has your experience been like? Is there a configuration I'm missing? (I'm using Claude Code Router, and or a custom built endpoint/ api key injector). &lt;/p&gt; &lt;p&gt;I really want to give these models a fair try but I simply can't make it work well enough. Glm 4.7 is slightly better than k2 for multistep iteration, but it also stops after a few steps. &lt;/p&gt; &lt;p&gt;So far, only minimax m2.1 (not 2) has worked well enough to get to completion of tasks on its own. &lt;/p&gt; &lt;p&gt;But I'm sure there's wisdom out there that I might be missing. Please share your tips and experience. Tks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FigZestyclose7787"&gt; /u/FigZestyclose7787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbgyf/how_to_get_sota_opensource_models_glm_47_kimi_k2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:11:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1px3x5s</id>
    <title>More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds</title>
    <updated>2025-12-27T17:51:45+00:00</updated>
    <author>
      <name>/u/EnigmaticEmir</name>
      <uri>https://old.reddit.com/user/EnigmaticEmir</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px3x5s/more_than_20_of_videos_shown_to_new_youtube_users/"&gt; &lt;img alt="More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds" src="https://external-preview.redd.it/ooS_D03GlqwVPelbZ5QA6BTQCahuN4HTtwsZt66twbs.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=056af1543405bd553af9d22df91d4e8918c27ecd" title="More than 20% of videos shown to new YouTube users are ‘AI slop’, study finds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnigmaticEmir"&gt; /u/EnigmaticEmir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theguardian.com/technology/2025/dec/27/more-than-20-of-videos-shown-to-new-youtube-users-are-ai-slop-study-finds"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px3x5s/more_than_20_of_videos_shown_to_new_youtube_users/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px3x5s/more_than_20_of_videos_shown_to_new_youtube_users/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T17:51:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1px940g</id>
    <title>Running MiniMax-M2.1 Locally with Claude Code and vLLM on Dual RTX Pro 6000</title>
    <updated>2025-12-27T21:28:12+00:00</updated>
    <author>
      <name>/u/zmarty</name>
      <uri>https://old.reddit.com/user/zmarty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run Claude Code with your own local MiniMax-M2.1 model using vLLM's native Anthropic API endpoint support.&lt;/p&gt; &lt;h2&gt;Hardware Used&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Specification&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;CPU&lt;/td&gt; &lt;td&gt;AMD Ryzen 9 7950X3D 16-Core Processor&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Motherboard&lt;/td&gt; &lt;td&gt;ROG CROSSHAIR X670E HERO&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GPU&lt;/td&gt; &lt;td&gt;Dual NVIDIA RTX Pro 6000 (96 GB VRAM each)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;RAM&lt;/td&gt; &lt;td&gt;192 GB DDR5 5200 (note the model does not use the RAM, it fits into VRAM entirely)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Install vLLM Nightly&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Prerequisite:&lt;/strong&gt; &lt;a href="https://forum.level1techs.com/t/wip-blackwell-rtx-6000-pro-max-q-quickie-setup-guide-on-ubuntu-24-04-lts-25-04/230521"&gt;Ubuntu 24.04 and the proper NVIDIA drivers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;```bash mkdir vllm-nightly cd vllm-nightly uv venv --python 3.12 --seed source .venv/bin/activate&lt;/p&gt; &lt;p&gt;uv pip install -U vllm \ --torch-backend=auto \ --extra-index-url &lt;a href="https://wheels.vllm.ai/nightly"&gt;https://wheels.vllm.ai/nightly&lt;/a&gt; ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Download MiniMax-M2.1&lt;/h2&gt; &lt;p&gt;Set up a separate environment for downloading models:&lt;/p&gt; &lt;p&gt;```bash mkdir /models cd /models uv venv --python 3.12 --seed source .venv/bin/activate&lt;/p&gt; &lt;p&gt;pip install huggingface_hub ```&lt;/p&gt; &lt;p&gt;Download the AWQ-quantized MiniMax-M2.1 model:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash mkdir /models/awq huggingface-cli download cyankiwi/MiniMax-M2.1-AWQ-4bit \ --local-dir /models/awq/cyankiwi-MiniMax-M2.1-AWQ-4bit &lt;/code&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Start vLLM Server&lt;/h2&gt; &lt;p&gt;From your vLLM environment, launch the server with the Anthropic-compatible endpoint:&lt;/p&gt; &lt;p&gt;```bash cd ~/vllm-nightly source .venv/bin/activate&lt;/p&gt; &lt;p&gt;vllm serve \ /models/awq/cyankiwi-MiniMax-M2.1-AWQ-4bit \ --served-model-name MiniMax-M2.1-AWQ \ --max-num-seqs 10 \ --max-model-len 128000 \ --gpu-memory-utilization 0.95 \ --tensor-parallel-size 2 \ --pipeline-parallel-size 1 \ --enable-auto-tool-choice \ --tool-call-parser minimax_m2 \ --reasoning-parser minimax_m2_append_think \ --trust-remote-code \ --host 0.0.0.0 \ --port 8000 ```&lt;/p&gt; &lt;p&gt;The server exposes &lt;code&gt;/v1/messages&lt;/code&gt; (Anthropic-compatible) at &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Install Claude Code&lt;/h2&gt; &lt;p&gt;Install Claude Code on macOS, Linux, or WSL:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash curl -fsSL https://claude.ai/install.sh | bash &lt;/code&gt;&lt;/p&gt; &lt;p&gt;See the &lt;a href="https://code.claude.com/docs/en/overview"&gt;official Claude Code documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Configure Claude Code&lt;/h2&gt; &lt;h3&gt;Create settings.json&lt;/h3&gt; &lt;p&gt;Create or edit &lt;code&gt;~/.claude/settings.json&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;&lt;code&gt;json { &amp;quot;env&amp;quot;: { &amp;quot;ANTHROPIC_BASE_URL&amp;quot;: &amp;quot;http://localhost:8000&amp;quot;, &amp;quot;ANTHROPIC_AUTH_TOKEN&amp;quot;: &amp;quot;dummy&amp;quot;, &amp;quot;API_TIMEOUT_MS&amp;quot;: &amp;quot;3000000&amp;quot;, &amp;quot;CLAUDE_CODE_DISABLE_NONESSENTIAL_TRAFFIC&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;ANTHROPIC_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_SMALL_FAST_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_SONNET_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_OPUS_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot;, &amp;quot;ANTHROPIC_DEFAULT_HAIKU_MODEL&amp;quot;: &amp;quot;MiniMax-M2.1-AWQ&amp;quot; } } &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;Skip Onboarding (Workaround for Bug)&lt;/h3&gt; &lt;p&gt;Due to a &lt;a href="https://github.com/anthropics/claude-code/issues/13827"&gt;known bug in Claude Code 2.0.65+&lt;/a&gt;, fresh installs may ignore &lt;code&gt;settings.json&lt;/code&gt; during onboarding. Add &lt;code&gt;hasCompletedOnboarding&lt;/code&gt; to &lt;code&gt;~/.claude.json&lt;/code&gt;:&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;If ~/.claude.json doesn't exist, create it:&lt;/h1&gt; &lt;p&gt;echo '{&amp;quot;hasCompletedOnboarding&amp;quot;: true}' &amp;gt; ~/.claude.json&lt;/p&gt; &lt;h1&gt;If it exists, add the field manually or use jq:&lt;/h1&gt; &lt;p&gt;jq '. + {&amp;quot;hasCompletedOnboarding&amp;quot;: true}' ~/.claude.json &amp;gt; tmp.json &amp;amp;&amp;amp; mv tmp.json ~/.claude.json ```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Run Claude Code&lt;/h2&gt; &lt;p&gt;With vLLM running in one terminal, open another and run:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash claude &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Claude Code will now use your local MiniMax-M2.1 model! If you also want to configure the Claude Code VSCode extension, see &lt;a href="https://platform.minimax.io/docs/guides/text-ai-coding-tools#use-m2-1-in-claude-code-extension-for-vs-code"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;References&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/vllm-project/vllm/issues/21313"&gt;vLLM Anthropic API Support (GitHub Issue #21313)&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://platform.minimax.io/docs/guides/text-ai-coding-tools"&gt;MiniMax M2.1 for AI Coding Tools&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/cyankiwi/MiniMax-M2.1-AWQ-4bit"&gt;cyankiwi/MiniMax-M2.1-AWQ-4bit on Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Cross-posted from my blog: &lt;a href="https://www.ovidiudan.com/2025/12/27/running-claude-code-with-minimax-m2-1.html"&gt;Running MiniMax-M2.1 Locally with Claude Code on Dual RTX Pro 6000&lt;/a&gt; (I am not selling or promoting anything)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zmarty"&gt; /u/zmarty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px940g/running_minimaxm21_locally_with_claude_code_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T21:28:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwt6ir</id>
    <title>Asus isn't going into memory manufacturing — Taiwanese tech giant issues statement smashing rumor</title>
    <updated>2025-12-27T08:49:41+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"&gt; &lt;img alt="Asus isn't going into memory manufacturing — Taiwanese tech giant issues statement smashing rumor" src="https://external-preview.redd.it/iOAde8UE4DyQsY7bL3QZWs7PMlmK1Bl2f85BU0xU79M.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c2acdf39b02a0864d08303117e213729c9c540f" title="Asus isn't going into memory manufacturing — Taiwanese tech giant issues statement smashing rumor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/dram/no-asus-isnt-going-into-memory-manufacturing-taiwanese-tech-giant-issues-statement-smashing-rumor"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwt6ir/asus_isnt_going_into_memory_manufacturing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T08:49:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwybg6</id>
    <title>llama.cpp, experimental native mxfp4 support for blackwell (25% preprocessing speedup!)</title>
    <updated>2025-12-27T13:52:16+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17906"&gt;https://github.com/ggml-org/llama.cpp/pull/17906&lt;/a&gt;&lt;/p&gt; &lt;p&gt;love that kind of evolution:&lt;/p&gt; &lt;p&gt;&amp;gt; at the moment this PR is &lt;del&gt;10%&lt;/del&gt; &lt;em&gt;&lt;del&gt;slower&lt;/del&gt;&lt;/em&gt; &lt;del&gt;than master&lt;/del&gt; 25% faster than master on PP.&lt;/p&gt; &lt;p&gt;&amp;gt; To compile &lt;code&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120f&amp;quot;&lt;/code&gt; is required.&lt;/p&gt; &lt;p&gt;probably/currently most useful for gpt-oss models! (also while reading the PR it seems that we might see more native nvfp4 support soon!)&lt;/p&gt; &lt;p&gt;Thanks to &lt;a href="/u/am17an"&gt;u/am17an&lt;/a&gt; (PR author) &amp;amp; llama.cpp devs!!&lt;/p&gt; &lt;p&gt;/edit: better point that also out (although, so far I am not noticing any quality degradation with gpt-oss-120b!):&lt;br /&gt; &amp;gt; [..] we quantize activation to mxfp4 instead of q8, which lead to failures in &lt;code&gt;test-backend-ops&lt;/code&gt;, however PPL tests are okay with this change (though not ruling out correctness issues)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwybg6/llamacpp_experimental_native_mxfp4_support_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T13:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwwsag</id>
    <title>The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?</title>
    <updated>2025-12-27T12:33:15+00:00</updated>
    <author>
      <name>/u/madSaiyanUltra_9789</name>
      <uri>https://old.reddit.com/user/madSaiyanUltra_9789</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt; &lt;img alt="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" src="https://b.thumbs.redditmedia.com/CWbH9aocZDksMFttENvwXtHE-6zGD_eJPUb93Q-jv0M.jpg" title="The Infinite Software Crisis: We're generating complex, unmaintainable code faster than we can understand it. Is 'vibe-coding' the ultimate trap?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I just watched &lt;a href="https://www.youtube.com/watch?v=eIoohUmYpGI"&gt;The Infinite Software Crisis – Jake Nations&lt;/a&gt; on YouTube and it got me thinking... the limitations of software development has never been typing speed, but rather our ability to comprehend and design the system correctly in the first place. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights from the talk:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every developer has shipped code they didn't completely understand. it passed the tests and that was enough validation to deploy it.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The hard part is timeless:&lt;/strong&gt; The hard part isn't the mechanics of coding; it's the conceptual difficulty of designing a solution. Every tool, including AI, just makes implementation easier.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI amplifies the problem:&lt;/strong&gt; We can now generate code as fast as we can describe it. The scale is infinite, but our comprehension isn't. The core challenge of understanding &lt;em&gt;what&lt;/em&gt; to build remains.&lt;/li&gt; &lt;li&gt;The real trap we fall into is confusing easy with simple. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Easy&lt;/strong&gt; is what's within reach. What can you access without effort? Generate it with AI, copy-paste, or install a framework. It's about speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Simple&lt;/strong&gt; is about structure. It means one fold, one braid, no entanglement. It requires thought and design.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;LLMs do not understand logic, they merely relate language and substitute those relations as &amp;quot;code&amp;quot;, so the importance of &lt;em&gt;patterns and architectural decisions&lt;/em&gt; in your codebase are lost. &lt;/li&gt; &lt;li&gt;when &amp;quot;vibe-coding&amp;quot; technical debt doesn't register as debt; it's just more code to preserve. &lt;/li&gt; &lt;li&gt;The result? Complex, highly-coupled, and error-prone code generated in minutes that could take you weeks to understand (if ever).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The real danger here is that we're accumulating complexity faster than we can comprehend it because we're not doing the hard work of understanding our systems.&lt;/p&gt; &lt;p&gt;The proposed solution: SLOW DOWN, DO EVERYTHING MANUALLY; architectural design + scaffolding, etc and only let the LLM in at the last step of filling in the scaffolding. &lt;/p&gt; &lt;p&gt;What's your take, Is 'vibe-coding' a trap, or is there a way to use these tools without losing the ability to understand our systems? &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9"&gt;https://preview.redd.it/c4mknoudlq9g1.png?width=553&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=28a6f37623fb0e0725f5b603f4b3a8ce51653ac9&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madSaiyanUltra_9789"&gt; /u/madSaiyanUltra_9789 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwwsag/the_infinite_software_crisis_were_generating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T12:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwyw36</id>
    <title>MiniMaxAI/MiniMax-M2.1 seems to be the strongest model per param</title>
    <updated>2025-12-27T14:19:07+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going by the Artifical Analysis benchaes, MiniMaxAI/MiniMax-M2.1 can compete with Kimi K2 Thinking, Deepseek 3.2 and GLM 4.7 in performance.&lt;/p&gt; &lt;p&gt;But what feels especially notable is that MiniMaxAI/MiniMax-M2.1 is only 229B param which is around half of GLM 4.7, around a third of Deepseek 3.2 and around a fifth of Kimi K2 Thinking&lt;/p&gt; &lt;p&gt;What this means is that MiniMaxAI/MiniMax-M2.1 seems to be the best value model now&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwyw36/minimaxaiminimaxm21_seems_to_be_the_strongest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T14:19:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1px1c41</id>
    <title>Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT</title>
    <updated>2025-12-27T16:06:19+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt; &lt;img alt="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" src="https://preview.redd.it/1e9anmnmsr9g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7456cd2a6f5b63217ca62ea494cdbf87700184fa" title="Head of Engineering @MiniMax__AI on MiniMax M2 int4 QAT" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1e9anmnmsr9g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px1c41/head_of_engineering_minimax_ai_on_minimax_m2_int4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T16:06:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxad0k</id>
    <title>NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux</title>
    <updated>2025-12-27T22:22:21+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt; &lt;img alt="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" src="https://external-preview.redd.it/Z1W-jCS5853m4eyzALlzqsbFjQ8v2fOj2tdMfCsU0J8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=232d0328fa3a116bc0a1917deae0e1763f4b6c47" title="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hackaday.com/2025/12/26/nvidia-drops-pascal-support-on-linux-causing-chaos-on-arch-linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1px55wg</id>
    <title>GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS</title>
    <updated>2025-12-27T18:42:04+00:00</updated>
    <author>
      <name>/u/ZeeleSama</name>
      <uri>https://old.reddit.com/user/ZeeleSama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"&gt; &lt;img alt="GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS" src="https://preview.redd.it/9wzn809jks9g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a4de55096d77fed72c8e49191b815e4f735ff388" title="GLM 4.7 IS NOW THE #1 OPEN SOURCE MODEL IN ARTIFICIAL ANALYSIS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeeleSama"&gt; /u/ZeeleSama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wzn809jks9g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1px55wg/glm_47_is_now_the_1_open_source_model_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T18:42:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
