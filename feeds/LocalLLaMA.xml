<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-09T22:24:01+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oswo5v</id>
    <title>Qwen3-VL Now EXL3 Supported</title>
    <updated>2025-11-09T22:21:49+00:00</updated>
    <author>
      <name>/u/Unstable_Llama</name>
      <uri>https://old.reddit.com/user/Unstable_Llama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt; &lt;img alt="Qwen3-VL Now EXL3 Supported" src="https://b.thumbs.redditmedia.com/kLLkxVaL2rnkQQgkYiE9gTrskjSAuf018kOnbuPHY1o.jpg" title="Qwen3-VL Now EXL3 Supported" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;⚠️ Requires &lt;a href="https://github.com/turboderp-org/exllamav3"&gt;ExLlamaV3 v0.0.13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/turboderp/Qwen3-VL-8B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-8B-Instruct-exl3&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/Qwen3-VL-30B-A3B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-30B-A3B-Instruct-exl3&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/turboderp/Qwen3-VL-32B-Instruct-exl3"&gt;https://huggingface.co/turboderp/Qwen3-VL-32B-Instruct-exl3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/985mbsz43b0g1.png?width=594&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1a181433478e6fced642a2905b3ba86d70a8ab56"&gt;CatBench results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Questions? Ask here or in the&lt;a href="https://discord.gg/VbR8wQxf"&gt; exllama discord&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unstable_Llama"&gt; /u/Unstable_Llama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oswo5v/qwen3vl_now_exl3_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T22:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1osns7l</id>
    <title>Mixing 3090s and mi60 on same machine in containers?</title>
    <updated>2025-11-09T16:33:31+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have two 3090s and considering a third. However thinking about dual mi60s for the same price as a third and using a container to run rocm models. Whilst I cannot combine the ram I could run two separate models. &lt;/p&gt; &lt;p&gt;Was a post a while back about having these in the same machine, but thought this would be cleaner?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osns7l/mixing_3090s_and_mi60_on_same_machine_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osns7l/mixing_3090s_and_mi60_on_same_machine_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osns7l/mixing_3090s_and_mi60_on_same_machine_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1osrbov</id>
    <title>PhD AI Research: Local LLM Inference — One MacBook Pro or Workstation + Laptop Setup?</title>
    <updated>2025-11-09T18:51:18+00:00</updated>
    <author>
      <name>/u/Anime_Over_Lord</name>
      <uri>https://old.reddit.com/user/Anime_Over_Lord</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm starting a PhD on a topic that leverages AI, and a large part of my work would involve running and evaluating LLMs, comparing model behavior, testing RAG pipelines, and experimenting with different inference setups. I won’t be training large models on my personal machine — my university offers infrastructure for that, though with some access limitations and queue times.&lt;/p&gt; &lt;p&gt;So my personal hardware is mainly for:&lt;/p&gt; &lt;p&gt;Running medium–large LLMs locally (often quantized 30B–70B, and sometimes larger)&lt;/p&gt; &lt;p&gt;Prototyping ideas quickly without waiting on remote resources&lt;/p&gt; &lt;p&gt;Working from different locations (office, library, travel, conferences)&lt;/p&gt; &lt;p&gt;General research computing, writing, coding, etc.&lt;/p&gt; &lt;p&gt;I want something that supports fast, low-friction iteration — because a lot of my thinking/testing happens spontaneously and not always while I’m physically at a workstation.&lt;/p&gt; &lt;p&gt;The Two Options&lt;/p&gt; &lt;p&gt;Option A — One Portable Workhorse&lt;/p&gt; &lt;p&gt;16&amp;quot; MacBook Pro (M4 Max)&lt;/p&gt; &lt;p&gt;128GB unified memory&lt;/p&gt; &lt;p&gt;2TB SSD&lt;/p&gt; &lt;p&gt;~£5400 (potentially less with university procurement/discount)&lt;/p&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;p&gt;Can run large models anywhere.&lt;/p&gt; &lt;p&gt;No need to remote into another machine for inference work.&lt;/p&gt; &lt;p&gt;Reduced workflow friction → faster iteration and idea testing.&lt;/p&gt; &lt;p&gt;Simpler setup: one environment, no sync overhead.&lt;/p&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;p&gt;Laptop thermals = not ideal for very long or sustained high-load jobs.&lt;/p&gt; &lt;p&gt;Single point of failure.&lt;/p&gt; &lt;p&gt;Option B — Workstation + Light Laptop&lt;/p&gt; &lt;p&gt;Mac Studio (M4 Max, 128GB, 2TB)&lt;/p&gt; &lt;p&gt;+&lt;/p&gt; &lt;p&gt;16&amp;quot; MacBook Pro (M4, 24GB, 512GB)&lt;/p&gt; &lt;p&gt;Total ~£6700 (again, possibly lower with university discounts)&lt;/p&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;p&gt;Mac Studio handles longer inference runs more comfortably.&lt;/p&gt; &lt;p&gt;Two machines = redundancy + possible parallel tasks.&lt;/p&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;p&gt;The 24GB laptop cannot run large models locally, so I’d need to remote into the Studio for most LLM work.&lt;/p&gt; &lt;p&gt;That introduces friction: syncing environments, data paths, vector stores, etc.&lt;/p&gt; &lt;p&gt;Higher total cost → reduces budget available for conferences, workshops, and travel, which are important in a PhD.&lt;/p&gt; &lt;p&gt;Unified memory is non-upgradeable, so there’s no scaling the Studio later.&lt;/p&gt; &lt;p&gt;Why I’m Not Considering Linux Laptops Right Now&lt;/p&gt; &lt;p&gt;I’ve used Linux before and I like it but on laptops I found:&lt;/p&gt; &lt;p&gt;Power management issues → significantly worse battery life&lt;/p&gt; &lt;p&gt;Driver/toolchain breakage during updates&lt;/p&gt; &lt;p&gt;Needing to maintain configs rather than just work&lt;/p&gt; &lt;p&gt;Inconsistent GPU support depending on model/vendor&lt;/p&gt; &lt;p&gt;I want this machine to be something I work on, not work to maintain.&lt;/p&gt; &lt;p&gt;That said, a compelling reason for a Linux laptop could make me reconsider.&lt;/p&gt; &lt;p&gt;Where I’m Leaning&lt;/p&gt; &lt;p&gt;I’m leaning toward Option A because having all compute with me would let me experiment freely from anywhere, which fits how I actually work day-to-day. But I also understand the value of a dedicated workstation for stability and sustained performance.&lt;/p&gt; &lt;p&gt;Before I commit, I want to make sure I’m not overlooking something important in the workflow or long-term usability.&lt;/p&gt; &lt;p&gt;Disclaimer / Note&lt;/p&gt; &lt;p&gt;Some of what I’ve written above is based on my assumptions. I specialize in another field, and this is about leveraging AI / LLMs for scientific workflows. My knowledge about AI and LLMs is still limited, so corrections, insights, or better approaches are welcome.&lt;/p&gt; &lt;p&gt;Question for people who run LLMs locally&lt;/p&gt; &lt;p&gt;For those who run medium–large LLMs for inference, evaluation, and RAG prototyping (not training):&lt;/p&gt; &lt;p&gt;Does having all the compute in one portable machine give you noticeably better iteration speed and workflow fluidity?&lt;/p&gt; &lt;p&gt;Or do you find the workstation + lightweight laptop setup more productive in practice?&lt;/p&gt; &lt;p&gt;Any experiences, regrets, or “I wish I had done X instead” stories are welcome.&lt;/p&gt; &lt;p&gt;TL;DR: PhD student looking to run LLMs locally for testing, evaluation, and RAG. Options:&lt;/p&gt; &lt;p&gt;Option A: MacBook Pro M4 Max, 128GB, 2TB — portable, frictionless, ~£5400&lt;/p&gt; &lt;p&gt;Option B: Mac Studio M4 Max 128GB + MacBook Pro 24GB — better sustained performance, but less portable, ~£6700&lt;/p&gt; &lt;p&gt;Leaning toward Option A for portability and faster experimentation, but seeking advice before committing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Anime_Over_Lord"&gt; /u/Anime_Over_Lord &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osrbov/phd_ai_research_local_llm_inference_one_macbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osrbov/phd_ai_research_local_llm_inference_one_macbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osrbov/phd_ai_research_local_llm_inference_one_macbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T18:51:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1osdbxz</id>
    <title>Full Stack Local Deep Research Agent</title>
    <updated>2025-11-09T07:38:12+00:00</updated>
    <author>
      <name>/u/Fun-Wolf-2007</name>
      <uri>https://old.reddit.com/user/Fun-Wolf-2007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/anilsharmay/full-stack-local-deep-research-agent"&gt;https://github.com/anilsharmay/full-stack-local-deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Wolf-2007"&gt; /u/Fun-Wolf-2007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:38:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1osszxh</id>
    <title>Codename Goose Desktop and Goose CLI with Ollama or other local inference</title>
    <updated>2025-11-09T19:55:48+00:00</updated>
    <author>
      <name>/u/NoWorking8412</name>
      <uri>https://old.reddit.com/user/NoWorking8412</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I have been messing around with Goose Desktop and Goose CLI for a while, and I am wondering if anyone has had any luck with getting it to work with local models for function and tool calling. I have been able to get several local models running with it, but none that can actually use the extensions in Goose. So far I've only been successful with Cloud APIs for functions and tool calling. &lt;/p&gt; &lt;p&gt;Would love to learn more about what you did and how you got it working. I am working with 16 GB VRAM and 32 GB RAM, and I am running Ollama, for clarity.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoWorking8412"&gt; /u/NoWorking8412 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osszxh/codename_goose_desktop_and_goose_cli_with_ollama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osszxh/codename_goose_desktop_and_goose_cli_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osszxh/codename_goose_desktop_and_goose_cli_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T19:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ostt83</id>
    <title>Keep the model running?</title>
    <updated>2025-11-09T20:27:57+00:00</updated>
    <author>
      <name>/u/External_Income29</name>
      <uri>https://old.reddit.com/user/External_Income29</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newbie here. I want to train a model locally on my pc. Do I need to keep the model running to train it? If I close the program, do I need to start All over ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Income29"&gt; /u/External_Income29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostt83/keep_the_model_running/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostt83/keep_the_model_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ostt83/keep_the_model_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oskxx9</id>
    <title>Any decent TTS that runs for AMD that runs on llama.cpp?</title>
    <updated>2025-11-09T14:36:57+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The search for Kokoro like quality and speed for a TTS that runs on AMD and llama.cpp has proven quite difficult.&lt;/p&gt; &lt;p&gt;Currently, only Kokoro on CPU offers the quality and runs decently enough on CPU. If they supported AMD GPUs or even the AMD NPU, I’d be grateful. There just seems no way to do that now.&lt;/p&gt; &lt;p&gt;What are you using?&lt;/p&gt; &lt;p&gt;EDIT: I’m on Windows, running Docker with WSL2. I can run Linux but prefer to keep my Windows setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskxx9/any_decent_tts_that_runs_for_amd_that_runs_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskxx9/any_decent_tts_that_runs_for_amd_that_runs_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oskxx9/any_decent_tts_that_runs_for_amd_that_runs_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T14:36:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1orw0fz</id>
    <title>I've been trying to make a real production service that uses LLM and it turned into a pure agony. Here are some of my "experiences".</title>
    <updated>2025-11-08T18:02:08+00:00</updated>
    <author>
      <name>/u/DaniyarQQQ</name>
      <uri>https://old.reddit.com/user/DaniyarQQQ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I hope this won't be an off topic, but I want to share my experience in creating real production service. Like a real deal, that will earn money. &lt;/p&gt; &lt;p&gt;For this service I've been using &lt;strong&gt;ChatGPT-5&lt;/strong&gt; and &lt;strong&gt;Claude Haiku 4.5&lt;/strong&gt; but I think this could be suitable for other LLMs too. &lt;/p&gt; &lt;p&gt;The idea was as simple as rock. Make an assistant bot that will communicate with people and make a scheduled appointments to the doctor. &lt;/p&gt; &lt;p&gt;Well in a short time I've implemented everything. The vector database that will inject doctor specific knowledge to the conversation at the right time. Multiple tools that will work with doctors data, and couple other integrations. I've extensively made very detailed system prompt, and each tool call returns instructive results. Each tools' parameters' descriptions were written in very detailed way. After testing for a week we finally deployed on production and started to receive conversations from real people.&lt;/p&gt; &lt;p&gt;And then real life had showed a lot of annoying and downright frustrating caveats of these LLMs.&lt;/p&gt; &lt;p&gt;The first frustrating thing is that &lt;strong&gt;LLMs makes an assumptions&lt;/strong&gt; &lt;strong&gt;without calling required tool, which deceives people.&lt;/strong&gt; It happened like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Please give me an address where this doctor will be on tomorrow. LLM: Tomorrow is sunday, which is weekend, doctor is unavalable. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is a tool that explicitly returns that address, and doctor actually works on Sundays. It did not call that tool and made up a bullshit excuse. Then I have emulated this question again by myself:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Me: Give me address where this doctor will be on tomorrow. LLM: &amp;lt;DID NOT CALL THE TOOL&amp;gt;. Tomorrow is sunday, which is weekend, doctor is unavalable. Me: Are you sure about that? LLM: &amp;lt;Finally starts calling the tool which returns address for tomorrow and outputs this address.&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This happens always. No matter what kind of prompts you write, telling it not make any assumptions without any tool calls it still made ups bullshit, which deceives people. Even if you explicitly inject the constraints, it fights them and keeps its own bullshit. &lt;/p&gt; &lt;p&gt;Another problem is close to the first one. &lt;strong&gt;LLMs always agrees with requests without calling tools which confuses people.&lt;/strong&gt; Which looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow. Is it possible. LLM: Of course you can make an appointment. I've prepared everything. What time? User: At 18:00 LLM: &amp;lt;Makes a tool call which returns negative result that next day is unavailable&amp;gt;. I'm sorry tomorrow is unavailable. User: WTF? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of asking proper question before agreeing, it agrees and then shits itself, confusing the user. Especially ChatGPT-5 has this problem, Claude is more rarer but still could shit itself.&lt;/p&gt; &lt;p&gt;And another problem is that &lt;strong&gt;LLMs output text which is complete opposite of it's tool results.&lt;/strong&gt; I've seen this only single time, but I'm now getting paranoid that this could be happening for a long time. It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow at 18:00. LLM: &amp;lt;Makes a call to the tool that returns that it is impossible for this user to make an appointment, because user has another pending appointment&amp;gt; LLM: Good. I've scheduled your appointment for tomorrow at 18:00. Don't forget about that &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That was an epic failure, LLM completely lied it's own results. I don't even know what to say about that.&lt;/p&gt; &lt;p&gt;And finally the funny one. &lt;strong&gt;Looks like ChatGPT does not like that tools can return negative results, and it keeps calling until it fully overloads all context and finally shits itself.&lt;/strong&gt; It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for next friday at 18:00 LLM: &amp;lt;Calls a tool for available window for next friday. No available window&amp;gt; LLM: &amp;lt;Calls again this tool but for the next next friday. No available window&amp;gt; LLM: &amp;lt;Cals AGAIN this tool but for the next next next friday. No available window&amp;gt; ------- And so on and so on | By the way, this doctor does not work on fridays, it was explicitly given in a system prompt, but ChatGPT wants to persevere. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These problems are fixable. You can make even more detailed prompts. Make tools return better and understandable results. You can tune some of LLM parameters. However it is game of whack-a-mole, frustrating one. You fix one thing, another thing comes out. I think some of these models, at least ChatGPT and Claude, were so overly trained on positivity, that they generate deceiving or downright wrong results.&lt;/p&gt; &lt;p&gt;Currently It seems to be that these LLMs can at mostly do their jobs correctly, but these fails, even if they happen rarely, are completely negating all of their reliability. It is not a wonderful magic thing that can solve everything. It is very finnicky (and sometimes very frustrating) tool, that maybe can do what you want. You think you have prepared it for everything, but users can make it shit itself just with a single sentence. &lt;/p&gt; &lt;p&gt;At least I've learned a lot, from these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniyarQQQ"&gt; /u/DaniyarQQQ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ossmm8</id>
    <title>Budget system for 30B models revisited</title>
    <updated>2025-11-09T19:41:23+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ossmm8/budget_system_for_30b_models_revisited/"&gt; &lt;img alt="Budget system for 30B models revisited" src="https://b.thumbs.redditmedia.com/BUxvkgQ6ldO1QbvL_Tep3ZEbo1Mar3X83UxXY2-Y3Vg.jpg" title="Budget system for 30B models revisited" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Moved my three Nvidia GTX-1070 GPUs to a DDR4 system. About a year ago I was running these GPUs on a 12 year old &lt;a href="https://www.reddit.com/r/ollama/comments/1gc5hnb/budget_system_for_30b_models/"&gt;DDR3 system&lt;/a&gt; and using Ollama. I was getting 8 t/s for gemma2 and you'll see below that with DDR4 system and gemma3 is getting 9 t/s. GPU matters more than system CPU, and DDR speed, if your system isn't offloading.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/ollama/comments/1gc5hnb/budget_system_for_30b_models/"&gt;https://www.reddit.com/r/ollama/comments/1gc5hnb/budget_system_for_30b_models/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: AMD Ryzen 5 3600 CPU, 32GB DDR4 RAM, three GTX-1070 GPUs, single PSU, power limit via crontab set for:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nvidia-smi -i 0 -pl 110; sudo nvidia-smi -i 1 -pl 111; sudo nvidia-smi -i 2 -pl 112&lt;/code&gt;&lt;/p&gt; &lt;p&gt;OS: Kubuntu 25.10&lt;/p&gt; &lt;p&gt;Llama.cpp: Vulkan build: cb1adf885 (6999)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;*Ling-mini-2.0-Q8_0.gguf (NOT 30B size but about same Vram usage)&lt;/li&gt; &lt;li&gt;gemma-3-27b-it-UD-Q4_K_XL.gguf&lt;/li&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf&lt;/li&gt; &lt;li&gt;granite-4.0-h-small-UD-Q4_K_XL.gguf&lt;/li&gt; &lt;li&gt;GLM-4-32B-0414-UD-Q4_K_XL.gguf&lt;/li&gt; &lt;li&gt;DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;code&gt;llama-bench -m /Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;load_backend: loaded RPC backend from /home/user33/vulkan/build/bin/libggml-rpc.so ggml_vulkan: Found 3 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 1 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none ggml_vulkan: 2 = NVIDIA GeForce GTX 1070 (NVIDIA) | uma: 0 | fp16: 0 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: none load_backend: loaded Vulkan backend from /home/user33/vulkan/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /home/user33/vulkan/build/bin/libggml-cpu-haswell.so &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Sorted by Params size&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;pp512&lt;/th&gt; &lt;th align="left"&gt;tg128&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;*Ling-mini-2.0-Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;16.11 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;227.98&lt;/td&gt; &lt;td align="left"&gt;70.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma-3-27b-it-UD-Q4_K_XL.gguf&lt;/td&gt; &lt;td align="left"&gt;15.66 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;57.26&lt;/td&gt; &lt;td align="left"&gt;8.97&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;81.45&lt;/td&gt; &lt;td align="left"&gt;47.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-4.0-h-small-UD-Q4_K_XL.gguf&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;25.34&lt;/td&gt; &lt;td align="left"&gt;15.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4-32B-0414-UD-Q4_K_XL.gguf&lt;/td&gt; &lt;td align="left"&gt;18.54 GiB&lt;/td&gt; &lt;td align="left"&gt;32.57 B&lt;/td&gt; &lt;td align="left"&gt;48.22&lt;/td&gt; &lt;td align="left"&gt;7.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;18.48 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;52.37&lt;/td&gt; &lt;td align="left"&gt;8.93&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Table below shows reference of model name (Legend) in llama.cpp&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;pp512&lt;/th&gt; &lt;th align="left"&gt;tg128&lt;/th&gt; &lt;th align="left"&gt;Legend&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;*&lt;strong&gt;Ling-mini-2.0-Q8_0.gguf&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16.11 GiB&lt;/td&gt; &lt;td align="left"&gt;16.26 B&lt;/td&gt; &lt;td align="left"&gt;227.98&lt;/td&gt; &lt;td align="left"&gt;70.94&lt;/td&gt; &lt;td align="left"&gt;bailingmoe2 16B.A1B Q8_0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gemma-3-27b-it-UD-Q4_K_XL.gguf&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;15.66 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;57.26&lt;/td&gt; &lt;td align="left"&gt;8.97&lt;/td&gt; &lt;td align="left"&gt;gemma3 27B Q4_K - Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17.28 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;81.45&lt;/td&gt; &lt;td align="left"&gt;47.76&lt;/td&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B Q4_K - Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;granite-4.0-h-small-UD-Q4_K_XL.gguf&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;17.49 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;25.34&lt;/td&gt; &lt;td align="left"&gt;15.41&lt;/td&gt; &lt;td align="left"&gt;granitehybrid 32B Q4_K - Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GLM-4-32B-0414-UD-Q4_K_XL.gguf&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;18.54 GiB&lt;/td&gt; &lt;td align="left"&gt;32.57 B&lt;/td&gt; &lt;td align="left"&gt;48.22&lt;/td&gt; &lt;td align="left"&gt;7.80&lt;/td&gt; &lt;td align="left"&gt;glm4 32B Q4_K - Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;DeepSeek-R1-Distill-Qwen-32B-Q4_K_M.gguf&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;18.48 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;52.37&lt;/td&gt; &lt;td align="left"&gt;8.93&lt;/td&gt; &lt;td align="left"&gt;qwen2 32B Q4_K - Medium&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;AMD motherboard X370 one GPU using 1X PCIe extender, other two mounted to 16X slot. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/truv59oxaa0g1.jpg?width=2252&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ccf335b37f291f935fbcfae8c403bd4e039ea846"&gt;Three Nvidia GTX-1070 8GB VRAM each (24GB VRAM total) power limited using nvidia-smi to 333 watts &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ossmm8/budget_system_for_30b_models_revisited/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ossmm8/budget_system_for_30b_models_revisited/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ossmm8/budget_system_for_30b_models_revisited/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T19:41:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1osw6ki</id>
    <title>[Release] Pre-built llama-cpp-python wheels for Blackwell/Ada/Ampere/Turing, up to CUDA 13.0 &amp; Python 3.13 (Windows x64)</title>
    <updated>2025-11-09T22:02:05+00:00</updated>
    <author>
      <name>/u/dougeeai</name>
      <uri>https://old.reddit.com/user/dougeeai</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building llama-cpp-python with CUDA on Windows can be a pain. So I embraced the suck and pre-compiled 40 wheels for 4 Nvidia architectures across 4 versions of Python and 3 versions of CUDA.&lt;/p&gt; &lt;p&gt;Figured these might be useful if you want to spin up GGUFs rapidly on Windows.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's included:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RTX 50/40/30/20 series support (Blackwell, Ada, Ampere, Turing) &lt;/li&gt; &lt;li&gt;Python 3.10, 3.11, 3.12, 3.13 &lt;/li&gt; &lt;li&gt;CUDA 11.8, 12.1, 13.0 (Blackwell only compiled for CUDA 13)&lt;/li&gt; &lt;li&gt;llama-cpp-python 0.3.16 &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Download:&lt;/strong&gt; &lt;a href="https://github.com/dougeeai/llama-cpp-python-wheels"&gt;https://github.com/dougeeai/llama-cpp-python-wheels&lt;/a&gt; &lt;/p&gt; &lt;p&gt;No Visual Studio. No CUDA Toolkit. Just pip install and run. Windows only for now. Linux wheels coming soon if there's interest. Open to feedback on what other configs would be helpful.&lt;/p&gt; &lt;p&gt;Thanks for letting me post, long time listener, first time caller.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougeeai"&gt; /u/dougeeai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osw6ki/release_prebuilt_llamacpppython_wheels_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osw6ki/release_prebuilt_llamacpppython_wheels_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osw6ki/release_prebuilt_llamacpppython_wheels_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T22:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oskcfd</id>
    <title>Does repurposing this older PC make any sense?</title>
    <updated>2025-11-09T14:11:02+00:00</updated>
    <author>
      <name>/u/Valuable-Question706</name>
      <uri>https://old.reddit.com/user/Valuable-Question706</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My goal is to run models locally for coding (only for some tasks that require privacy, not all). &lt;/p&gt; &lt;p&gt;&lt;em&gt;So far, I’m happy with Qwen3-Coder-30b-A3B level of results.&lt;/em&gt; It runs on my current machine (32RAM+8VRAM) at ~4-6 tokens/s. But it takes the larger part of my RAM - this is what I’m not happy with.&lt;/p&gt; &lt;p&gt;I also have a ~10yr old PC with PCIe 3.0 motherboard, 48GB DDR4 RAM, 5th gen i7 CPU and 9xx-series GPU with 4GB RAM.&lt;/p&gt; &lt;p&gt;I’m thinking of upgrading it with a modern 16GB GPU and setting it up as a dedicated inference server. Also, maybe maxing up RAM to 64 that this system supports. &lt;/p&gt; &lt;p&gt;First, does it make any sense model-wise? &lt;strong&gt;Are there any models with much better output in this RAM+VRAM range?&lt;/strong&gt; Or you need to go much higher (120+) for something &lt;em&gt;not marginally&lt;/em&gt; better?&lt;/p&gt; &lt;p&gt;Second, does a modern GPU make any sense for such a machine?&lt;/p&gt; &lt;p&gt;&lt;em&gt;Where I live, only reasonable 16GB options available are newer PCIe 5.0 GPUs, like 5060 Ti, and higher. Nobody’s selling their older 8-16GB GPUs here yet.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Question706"&gt; /u/Valuable-Question706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskcfd/does_repurposing_this_older_pc_make_any_sense/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oskcfd/does_repurposing_this_older_pc_make_any_sense/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oskcfd/does_repurposing_this_older_pc_make_any_sense/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T14:11:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1osd8ie</id>
    <title>PSA Kimi K2 Thinking seems to currently be broken for most agents because of tool calling within it's thinking tags</title>
    <updated>2025-11-09T07:32:05+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, just what the title says. If any of you are having issues with coding using K2 thinking it's because of this. Only Kimi CLI really supports it atm. Minimax m2 had a similar issue I think and glm 4.6 too, but this could be worked around by disabling tool_calling in thinking, however this can't be done for K2 thinking, hence all the issues people are having with this model for coding. Hopefully most agents will have this fixed soon. I think this is called interleaved thinking, or is something similar to that? Feel free to shed some light on this in the comments if you're more familiar with what's going on.&lt;/p&gt; &lt;p&gt;EDIT - I found the issue: &lt;a href="https://github.com/MoonshotAI/Kimi-K2/issues/89"&gt;https://github.com/MoonshotAI/Kimi-K2/issues/89&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's better explained there. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortopy</id>
    <title>Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs</title>
    <updated>2025-11-08T16:28:21+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" src="https://preview.redd.it/s190tdo2720g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a07ae8f05f136602ecbf12323e286c3cca29d84" title="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! You can now run Kimi K2 Thinking locally with our Unsloth Dynamic 1bit GGUFs. We also collaborated with the Kimi team on a &lt;strong&gt;fix for K2&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking/discussions/12"&gt;&lt;strong&gt;Thinking's chat template&lt;/strong&gt;&lt;/a&gt; not prepending the default system prompt of &lt;code&gt;You are Kimi, an AI assistant created by Moonshot AI.&lt;/code&gt; on the 1st turn.&lt;/p&gt; &lt;p&gt;We also we &lt;strong&gt;fixed llama.cpp custom jinja separators&lt;/strong&gt; for tool calling - Kimi does &lt;code&gt;{&amp;quot;a&amp;quot;:&amp;quot;1&amp;quot;,&amp;quot;b&amp;quot;:&amp;quot;2&amp;quot;}&lt;/code&gt; and not with extra spaces like &lt;code&gt;{&amp;quot;a&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;: &amp;quot;2&amp;quot;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The 1-bit GGUF will run on 247GB RAM. We shrank the 1T model to 245GB (-62%) &amp;amp; the accuracy recovery is comparable to our third-party &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;DeepSeek-V3.1 Aider Polyglot benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All 1bit, 2bit and other bit width GGUFs are at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The suggested temp is &lt;code&gt;temperature = 1.0&lt;/code&gt;. We also suggest a &lt;code&gt;min_p = 0.01&lt;/code&gt;. If you do not see &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;, use &lt;code&gt;--special&lt;/code&gt;. The code for llama-cli is below which offloads MoE layers to CPU RAM, and leaves the rest of the model on GPU VRAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LLAMA_CACHE=&amp;quot;unsloth/Kimi-K2-Thinking-GGUF&amp;quot; ./llama.cpp/llama-cli \ -hf unsloth/Kimi-K2-Thinking-GGUF:UD-TQ1_0 \ --n-gpu-layers 99 \ --temp 1.0 \ --min-p 0.01 \ --ctx-size 16384 \ --seed 3407 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-by-step Guide + fix details: &lt;a href="https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally"&gt;https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally&lt;/a&gt; and GGUFs are &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us know if you have any questions and hope you have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s190tdo2720g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1osh24c</id>
    <title>Best coding agent for GLM-4.6 that's not CC</title>
    <updated>2025-11-09T11:28:22+00:00</updated>
    <author>
      <name>/u/Illustrious-Many-782</name>
      <uri>https://old.reddit.com/user/Illustrious-Many-782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already use GLM with Opencode, Claude Code, and Codex CLI, but since I have the one-year z.ai mini plan, I want to use GLM more than I am right now, Is there a better option than OpenCode (that's not Claude Code, because it's being used by Claude)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Many-782"&gt; /u/Illustrious-Many-782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1osuat7</id>
    <title>Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090</title>
    <updated>2025-11-09T20:47:30+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/"&gt; &lt;img alt="Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090" src="https://preview.redd.it/vvimjdf4na0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b8bba52c5a4592461099dbcbbff0318d56011e9" title="Benchmark Results: GLM-4.5-Air (Q4) at Full Context on Strix Halo vs. Dual RTX 3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I benchmarked the GLM-4.5-Air (Q4) model running at a near-maximum context on two very different systems: a Strix Halo APU and a dual RTX 3090 server. Both tests were conducted under Debian GNU/Linux with the latest llama.cpp builds from the day of testing. But I did overlook and there's a one-revision difference between the two llama.cpp builds. Here are the startup commands, environment details, and a diagram that breaks down the performance and energy efficiency of both setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RTX3090:&lt;/strong&gt; ```bash&lt;/p&gt; &lt;p&gt;$ LLAMA_SET_ROWS=1 llama-server -m GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf --n-cpu-moe 38 \ --tensor-split 28,20 -c 0 --n-gpu-layers 99 --temp 0.9 --flash-attn auto --jinja --host 0.0.0.0 \ --port 8080 -a glm_air --no-context-shift --no-mmap --swa-full --reasoning-format none ```&lt;/p&gt; &lt;p&gt;```bash prompt eval time = 1781631.25 ms / 119702 tokens ( 14.88 ms per token, 67.19 tokens per second) eval time = 1045615.05 ms / 5232 tokens ( 199.85 ms per token, 5.00 tokens per second) total time = 2827246.30 ms / 124934 tokens slot release: id 3 | task 1 | stop processing: n_tokens = 124933, truncated = 0&lt;/p&gt; &lt;p&gt;$ llama-server --version ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 2 CUDA devices: Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes Device 1: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes ggml_vulkan: Found 2 Vulkan devices: ggml_vulkan: 0 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: KHR_coopmat ggml_vulkan: 1 = NVIDIA GeForce RTX 3090 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: KHR_coopmat version: 6990 (53d7d21e6) built with cc (Debian 14.2.0-19) 14.2.0 for x86_64-linux-gnu&lt;/p&gt; &lt;p&gt;Build flags: -DGGML_CUDA=ON -DGGML_CUDA_F16=ON -DGGML_CUDA_FA_ALL_QUANTS=ON -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DCMAKE_CUDA_ARCHITECTURES=86 -DGGML_VULKAN=ON&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&lt;strong&gt;strix halo:&lt;/strong&gt; &lt;code&gt;bash $ llama-server -m GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf --n-gpu-layers 99 --host 0.0.0.0 \ --port 8080 -a glm_air -c 131072 -fa 1 --no-mmap &lt;/code&gt;&lt;/p&gt; &lt;p&gt;```bash prompt eval time = 5175231.01 ms / 119703 tokens ( 43.23 ms per token, 23.13 tokens per second) eval time = 1430449.98 ms / 5778 tokens ( 247.57 ms per token, 4.04 tokens per second) total time = 6605680.99 ms / 125481 tokens slot update_slots: id 2 | task 1577 | prompt done, n_tokens = 119703, batch.n_tokens = 919&lt;/p&gt; &lt;p&gt;$ llama-server --version ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat version: 6989 (eeee367de) built with cc (Debian 15.2.0-7) 15.2.0 for x86_64-linux-gnu&lt;/p&gt; &lt;p&gt;Build flags: -DGGML_VULKAN=ON -DGGML_HIP_ROCWMMA_FATTN=ON -DAMDGPU_TARGETS=gfx1151 ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vvimjdf4na0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osuat7/benchmark_results_glm45air_q4_at_full_context_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:47:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmnlp</id>
    <title>CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]</title>
    <updated>2025-11-09T15:49:05+00:00</updated>
    <author>
      <name>/u/Prize_Cost_7706</name>
      <uri>https://old.reddit.com/user/Prize_Cost_7706</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"&gt; &lt;img alt="CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]" src="https://external-preview.redd.it/OWYxbGQ4MHg1OTBnMQhj1jeEZTm3kJ-2tjF6W5cKTD-ayqoaa3bxlqaqIxQQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9da75c2f2ce7867337a918970b57e5d6235e9ee4" title="CodeWiki: Research-Grade Repository Documentation at Scale [Open Source]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; communities! I'm excited to share &lt;strong&gt;CodeWiki&lt;/strong&gt;, our newly published research project from FSoft-AI4Code that tackles automated repository-level documentation generation. After seeing DeepWiki and its open-source implementations, we thought the community might appreciate a different approach backed by academic research.&lt;/p&gt; &lt;h1&gt;What is CodeWiki?&lt;/h1&gt; &lt;p&gt;CodeWiki is the first &lt;strong&gt;semi-agentic framework&lt;/strong&gt; specifically designed for comprehensive, repository-level documentation across 7 programming languages (Python, Java, JavaScript, TypeScript, C, C++, C#). Currently submitted to ACL ARR 2025. &lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/FSoft-AI4Code/CodeWiki"&gt;FSoft-AI4Code/CodeWiki&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How is CodeWiki Different from DeepWiki?&lt;/h1&gt; &lt;p&gt;I've researched both &lt;a href="https://github.com/AsyncFuncAI/deepwiki-open"&gt;AsyncFuncAI/deepwiki-open&lt;/a&gt; and &lt;a href="https://github.com/AIDotNet/OpenDeepWiki"&gt;AIDotNet/OpenDeepWiki&lt;/a&gt;, and here's an honest comparison:&lt;/p&gt; &lt;h1&gt;CodeWiki's Unique Approach:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hierarchical Decomposition with Dependency Analysis&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Uses static analysis + AST parsing (Tree-Sitter) to build dependency graphs&lt;/li&gt; &lt;li&gt;Identifies architectural entry points and recursively partitions modules&lt;/li&gt; &lt;li&gt;Maintains architectural coherence while scaling to repositories of any size&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Recursive Agentic Processing with Dynamic Delegation&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Agents can dynamically delegate complex sub-modules to specialized sub-agents- Bounded complexity handling through recursive bottom-up processing&lt;/li&gt; &lt;li&gt;Cross-module coherence via intelligent reference management&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Research-Backed Evaluation (CodeWikiBench)&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;First benchmark specifically for repository-level documentation&lt;/li&gt; &lt;li&gt;Hierarchical rubric generation from official docs- Multi-model agentic assessment with reliability metrics&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Outperforms closed-source DeepWiki by 4.73% on average&lt;/strong&gt; (68.79% vs 64.06%)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Differences:&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;CodeWiki&lt;/th&gt; &lt;th align="left"&gt;DeepWiki (Open Source)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Core Focus&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Architectural understanding &amp;amp; scalability&lt;/td&gt; &lt;td align="left"&gt;Quick documentation generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Methodology&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Dependency-driven hierarchical decomposition&lt;/td&gt; &lt;td align="left"&gt;Direct code analysis&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Agent System&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Recursive delegation with specialized sub-agents&lt;/td&gt; &lt;td align="left"&gt;Single-pass generation&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Academic benchmark (CodeWikiBench)&lt;/td&gt; &lt;td align="left"&gt;User-facing features&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Performance Highlights&lt;/h1&gt; &lt;p&gt;On 21 diverse repositories (86K to 1.4M LOC):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TypeScript&lt;/strong&gt;: +18.54% over DeepWiki&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: +9.41% over DeepWiki&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scripting languages avg&lt;/strong&gt;: 79.14% (vs DeepWiki's 68.67%)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consistent cross-language&lt;/strong&gt; generalization&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;We are actively working on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enhanced systems language support&lt;/li&gt; &lt;li&gt;Multi-version documentation tracking&lt;/li&gt; &lt;li&gt;Downstream SE task integration (code migration, bug localization, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your thoughts, especially from folks who've tried the DeepWiki implementations! What features matter most for automated documentation in your workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prize_Cost_7706"&gt; /u/Prize_Cost_7706 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/98rdo80x590g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmnlp/codewiki_researchgrade_repository_documentation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ostdcn</id>
    <title>Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore</title>
    <updated>2025-11-09T20:10:26+00:00</updated>
    <author>
      <name>/u/Previous_Nature_5319</name>
      <uri>https://old.reddit.com/user/Previous_Nature_5319</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"&gt; &lt;img alt="Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore" src="https://preview.redd.it/90im3um0fa0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1880277abb2e16deb196c79509aa47dbb7d349ae" title="Faster Prompt Processing in llama.cpp: Smart Proxy + Slots + Restore" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache"&gt;https://github.com/airnsk/proxycache&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this service is&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#what-this-service-is"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This service is a smart proxy in front of llama.cpp that makes long‑context chat and IDE workflows much faster by managing llama.cpp slots, reusing cached context, and restoring saved caches from disk when needed. It speaks an OpenAI‑compatible Chat Completions API, so existing clients can connect without changes, including both streaming (SSE) and non‑stream responses depending on request settings.&lt;/p&gt; &lt;h1&gt;Why it’s needed&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#why-its-needed"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp provides “slots,” each holding a conversation’s KV cache so repeated requests with the same or very similar prefix can skip recomputing the whole prompt and continue from the first mismatching token, which dramatically cuts latency for large prompts. In real teams the number of users can easily exceed the number of available slots (e.g., 20 developers but only 4 slots), so naive routing causes random slot reuse and cache overwrites that waste time and GPU/CPU cycles. This proxy solves that by steering requests to the right slot, saving evicted caches to disk, and restoring them on demand, so long prompts don’t need to be recomputed from scratch each time.&lt;/p&gt; &lt;h1&gt;How requests are balanced and slots are chosen&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#how-requests-are-balanced-and-slots-are-chosen"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slots and heat: When a request lands in a slot and its cache is valid for reuse, the slot is considered “hot,” and new requests won’t overwrite it if other options exist, preserving useful KV for future reuse.&lt;/li&gt; &lt;li&gt;Similarity matching: The proxy computes a fast, word‑block prefix similarity between the incoming conversation and existing hot slots, and only reuses a hot slot if the similarity meets a single ratio threshold (e.g., 85% of the shorter sequence), otherwise it rejects reuse to avoid polluting the hot cache with a weakly related prompt.&lt;/li&gt; &lt;li&gt;Free and cold first: If reuse is rejected, the proxy sends the request to a free slot or a cold slot (one not currently carrying a valuable hot cache), protecting high‑value contexts from accidental overwrites under load.&lt;/li&gt; &lt;li&gt;Oldest when full: If there are no free or cold slots, the proxy picks the least‑recently used slot and saves its current KV cache to disk before assigning the new request, ensuring nothing valuable is lost when the pool is exhausted.&lt;/li&gt; &lt;li&gt;Restore on demand: When a new request matches a cache that was previously saved, the proxy restores that cache into a free/cold/oldest slot and routes the request there, which takes seconds versus minutes for full prompt recomputation on long contexts, especially in IDE scenarios with 30–60k tokens.&lt;/li&gt; &lt;li&gt;Concurrency safety: Each slot is guarded with an async lock; if all are busy, the request waits for the first LRU slot to free, preventing race conditions and unintended cache overwrites during concurrent generation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Save and restore from disk&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#save-and-restore-from-disk"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp’s HTTP server exposes slot save/restore; saving writes a cache file to the directory provided by --slot‑save‑path, and restore loads by file basename (e.g., slotcache_.bin), which is exactly how this proxy persists and revives caches across requests and restarts. The proxy keeps small local .meta files describing cached prefixes for fast lookup, while llama.cpp owns the actual KV .bin files under --slot‑save‑path for correctness and performance.&lt;/p&gt; &lt;h1&gt;Quick start&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#quick-start"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Start llama.cpp ( &lt;a href="https://github.com/ggml-org/llama.cpp"&gt;https://github.com/ggml-org/llama.cpp&lt;/a&gt; ) with slots and a cache directory:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server -m ./model.gguf -np 4 --slot-save-path /var/kvcache --host 0.0.0.0 --port 8080 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This enables the OpenAI‑compatible HTTP server, a pool of 4 slots, and a directory where slot KV caches are saved and restored by basename.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Run the proxy next to it:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/airnsk/proxycache.git cd proxycache python3 -m venv venv &amp;amp;&amp;amp; source venv/bin/activate &amp;amp;&amp;amp; pip install -r requirements.txt python3 proxycache.py # or: uvicorn app:app --host 0.0.0.0 --port 8081 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your clients should call the proxy’s /v1/chat/completions endpoint; the proxy will handle similarity, slot selection, save/restore, and streaming vs non‑streaming automatically.&lt;/p&gt; &lt;p&gt;If you run into issues using gpt-oss-20b with an IDE like Cline, follow these instructions: &lt;a href="https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/"&gt;https://www.reddit.com/r/CLine/comments/1mtcj2v/making_gptoss_20b_and_cline_work_together/&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Parameters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#parameters"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLAMA_SERVER_URL: The llama.cpp server base URL, e.g., &lt;a href="http://127.0.0.1:8080/"&gt;http://127.0.0.1:8080&lt;/a&gt;, which must expose the OpenAI‑compatible chat completions endpoint.&lt;/li&gt; &lt;li&gt;SLOTS_COUNT: The number of server slots (should match llama.cpp -np) so the proxy can track and plan reuse/restore correctly under load.&lt;/li&gt; &lt;li&gt;SIMILARITY_MIN_RATIO: One similarity threshold (e.g., 0.85) controlling both active reuse and disk restore; if a match is below this ratio, the proxy will prefer a free/cold slot or restore instead of overwriting a hot slot.&lt;/li&gt; &lt;li&gt;MIN_PREFIX_* (chars/words/blocks): Requests below this size are treated as “small” and steered to free/cold/oldest slots to avoid disturbing valuable hot caches used by large, long‑running prompts.&lt;/li&gt; &lt;li&gt;LOCAL_META_DIR and --slot-save-path: The proxy stores small .meta descriptors locally for fast candidate lookup, while llama.cpp reads/writes the real KV cache files under --slot‑save‑path using basename in the HTTP API.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this boosts IDE and long‑context productivity&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/airnsk/proxycache/tree/main#why-this-boosts-ide-and-longcontext-productivity"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For 30–60k‑token contexts typical in project‑wide IDE assistants, recomputing a full prompt can take minutes, whereas restoring a previously cached context and continuing from the first mismatching token typically takes seconds on llama.cpp, dramatically improving iteration speed for large teams with limited slots.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Previous_Nature_5319"&gt; /u/Previous_Nature_5319 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/90im3um0fa0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ostdcn/faster_prompt_processing_in_llamacpp_smart_proxy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1osj4mb</id>
    <title>Worth the switch from Claude to GLM 4.6 for my coding side hustle?</title>
    <updated>2025-11-09T13:16:58+00:00</updated>
    <author>
      <name>/u/Ok_Investigator_5036</name>
      <uri>https://old.reddit.com/user/Ok_Investigator_5036</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been freelancing web development projects for about 8 months now, mostly custom dashboards, client portals, and admin panels. The economics are tough because clients always want &amp;quot;simple&amp;quot; projects that turn into months of iteration hell. (Never trust anything to be &amp;quot;simple&amp;quot;)&lt;/p&gt; &lt;p&gt;I started using Claude API for rapid prototyping and client demos. Problem is my margins were getting narrow, especially when a client would request their fifth redesign of a data visualization component or want to &amp;quot;just tweak&amp;quot; the entire authentication flow.&lt;/p&gt; &lt;p&gt;Someone in a dev Discord mentioned using GLM-4.6 with Claude Code. They were getting 55% off first year, so GLM Coding Pro works out to $13.5/month vs Claude Pro at $20+, with 3x usage quota.&lt;/p&gt; &lt;p&gt;I've tested GLM-4.6's coding output. It seems on par with Claude for most tasks, but with 3x the usage quota. We're talking 600 prompts every 5 hours vs Claude Max's ~200.&lt;/p&gt; &lt;p&gt;My typical project flow:&lt;/p&gt; &lt;p&gt;- Client consultation and mockups&lt;/p&gt; &lt;p&gt;- Use AI to scaffold React components and API routes&lt;/p&gt; &lt;p&gt;- Rapid iteration on UI/UX (this is where the 3x quota matters)&lt;/p&gt; &lt;p&gt;- Testing, refactoring, deployment&lt;/p&gt; &lt;p&gt;Last month I landed three projects: a SaaS dashboard with Stripe integration and two smaller automation tools. But some months it's just one or two projects with endless revision rounds.&lt;/p&gt; &lt;p&gt;Right now my prompt usage is manageable, but I've had months where client iterations alone hit thousands of prompts, especially when they're A/B testing different UI approaches or want real-time previews of changes.&lt;/p&gt; &lt;p&gt;For me, the limiting factor isn't base capability (GLM-4.6 ≈ Claude quality), but having the quota to iterate without stressing about costs.&lt;/p&gt; &lt;p&gt;Wondering how you guys optimizing your AI coding setup costs? With all the client demands and iteration cycles, seems smart to go for affordable with high limits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Investigator_5036"&gt; /u/Ok_Investigator_5036 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osj4mb/worth_the_switch_from_claude_to_glm_46_for_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T13:16:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1osiog7</id>
    <title>Qwen3-VL works really good with Zoom-in Tool</title>
    <updated>2025-11-09T12:56:03+00:00</updated>
    <author>
      <name>/u/indigos661</name>
      <uri>https://old.reddit.com/user/indigos661</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt; &lt;img alt="Qwen3-VL works really good with Zoom-in Tool" src="https://a.thumbs.redditmedia.com/GlsFc4qF0jvYKaVjwuK5hhN3xFcCV40Gl5A3-CDlYE4.jpg" title="Qwen3-VL works really good with Zoom-in Tool" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While Qwen3-VL-30B-A3B(Q6_ud) performs better than previous open-source models in general image recognition, it still has issues with hallucinations and inaccurate recognition. &lt;/p&gt; &lt;p&gt;However, with the zoom_in tool the situation is completely different. On my own frontend implementation with zoom_in, Qwen3-VL can zoom in on the image, significantly improving the accuracy of content recognition. For those who haven't tried it, qwen team has released a reference implementation: &lt;a href="https://github.com/QwenLM/Qwen-Agent/blob/main/examples/cookbook_think_with_images.ipynb"&gt;https://github.com/QwenLM/Qwen-Agent/blob/main/examples/cookbook_think_with_images.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n6zw8zpfa80g1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bffdedb3be6155771d24adcaf1df358854043e2"&gt;https://preview.redd.it/n6zw8zpfa80g1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7bffdedb3be6155771d24adcaf1df358854043e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are using Qwen3-VL, I strongly recommend using it with this tool.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/indigos661"&gt; /u/indigos661 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osiog7/qwen3vl_works_really_good_with_zoomin_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:56:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1osqscj</id>
    <title>LM Studio unlocked for "unsupported" hardware — Testers wanted!</title>
    <updated>2025-11-09T18:30:25+00:00</updated>
    <author>
      <name>/u/TheSpicyBoi123</name>
      <uri>https://old.reddit.com/user/TheSpicyBoi123</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"&gt; &lt;img alt="LM Studio unlocked for &amp;quot;unsupported&amp;quot; hardware — Testers wanted!" src="https://b.thumbs.redditmedia.com/Ar2gzGOrS93zpzzfv2Hdae30Ff1to_7JvKq-h-ba6cU.jpg" title="LM Studio unlocked for &amp;quot;unsupported&amp;quot; hardware — Testers wanted!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;Quick update — a &lt;strong&gt;simple in situ patch&lt;/strong&gt; was found (see GitHub), and the &lt;strong&gt;newest versions of the backends&lt;/strong&gt; are now released for &amp;quot;unsupported&amp;quot; hardware.&lt;/p&gt; &lt;p&gt;Since the last post, &lt;strong&gt;major refinements&lt;/strong&gt; have been made: performance, compatibility, and build stability have all improved. &lt;/p&gt; &lt;p&gt;Here’s the current testing status:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ &lt;strong&gt;AVX1 CPU builds:&lt;/strong&gt; working (confirmed working, Ivy Bridge Xeons)&lt;/li&gt; &lt;li&gt;✅ &lt;strong&gt;AVX1 Vulkan builds:&lt;/strong&gt; working (confirmed working, Ivy Bridge Xeons + Tesla k40 GPUs)&lt;/li&gt; &lt;li&gt;❓ &lt;strong&gt;AVX1 CUDA builds:&lt;/strong&gt; untested (no compatible hardware yet)&lt;/li&gt; &lt;li&gt;❓ &lt;strong&gt;Non-AVX experimental builds:&lt;/strong&gt; untested (no compatible hardware yet)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’d love for more people to &lt;strong&gt;try the patch instructions on their own architectures&lt;/strong&gt; and share results — especially if you have &lt;strong&gt;newer NVIDIA GPUs&lt;/strong&gt; or &lt;strong&gt;non-AVX CPUs&lt;/strong&gt; (like first-gen Intel Core).&lt;/p&gt; &lt;p&gt;👉 &lt;a href="https://github.com/theIvanR/lmstudio-unlocked-backend"&gt;https://github.com/theIvanR/lmstudio-unlocked-backend&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My test setup is dual Ivy Bridge Xeons with Tesla K40 GPUs&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7v3vd9ldx90g1.png?width=1106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58ae1582a47823f049f86ae91ebe2ae368a9b22a"&gt;https://preview.redd.it/7v3vd9ldx90g1.png?width=1106&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=58ae1582a47823f049f86ae91ebe2ae368a9b22a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ou8639ofx90g1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15f853146d4adde2e4dec84aa76a24b17a5eab3c"&gt;https://preview.redd.it/ou8639ofx90g1.png?width=1041&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=15f853146d4adde2e4dec84aa76a24b17a5eab3c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Brief install instructions:&lt;br /&gt; - navigate to backends folder. ex C:\Users\Admin\.lmstudio\extensions\backends&lt;br /&gt; - (recommended for clean install) delete everything except &amp;quot;vendor&amp;quot; folder&lt;br /&gt; - drop contents from compressed backend of your choice&lt;/p&gt; &lt;p&gt;- select it in LM Studio runtimes and enjoy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheSpicyBoi123"&gt; /u/TheSpicyBoi123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osqscj/lm_studio_unlocked_for_unsupported_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T18:30:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1osufxq</id>
    <title>Running DeepSeek-OCR on vLLM 0.11.1rc6.dev7 in Open WebUI as a test</title>
    <updated>2025-11-09T20:53:15+00:00</updated>
    <author>
      <name>/u/AFruitShopOwner</name>
      <uri>https://old.reddit.com/user/AFruitShopOwner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osufxq/running_deepseekocr_on_vllm_0111rc6dev7_in_open/"&gt; &lt;img alt="Running DeepSeek-OCR on vLLM 0.11.1rc6.dev7 in Open WebUI as a test" src="https://preview.redd.it/j14a86mxka0g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21626c1b49c8f27927e9f2fd70e483b0bd2bd4d4" title="Running DeepSeek-OCR on vLLM 0.11.1rc6.dev7 in Open WebUI as a test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Obviously you're not supposed to use DeepSeek-OCR through a chat UI. I'm just testing to see if it works or not. Also, this is not really an OCR task but I was wondering if I could use this model for general image description. Seems like that works just fine. &lt;/p&gt; &lt;p&gt;I have not yet implemented the helper scripts in the &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/tree/main/DeepSeek-OCR-master/DeepSeek-OCR-vllm"&gt;DeepSeek-OCR github repo&lt;/a&gt;. They seem pretty handy for image/pdf/batch OCR workloads.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AFruitShopOwner"&gt; /u/AFruitShopOwner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j14a86mxka0g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osufxq/running_deepseekocr_on_vllm_0111rc6dev7_in_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osufxq/running_deepseekocr_on_vllm_0111rc6dev7_in_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T20:53:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oss784</id>
    <title>Strix Halo inference Cluster</title>
    <updated>2025-11-09T19:25:06+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oss784/strix_halo_inference_cluster/"&gt; &lt;img alt="Strix Halo inference Cluster" src="https://external-preview.redd.it/QLldEh6cHckh0zu3VOF5RuY9ywGiZCt_x-CXw1nKwvM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6be666f0103fd1f705f54f78d0ee69bc9405d6dc" title="Strix Halo inference Cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0cIcth224hk?si=IfW5yysNbNWUDvFx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oss784/strix_halo_inference_cluster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oss784/strix_halo_inference_cluster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T19:25:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1osmr6i</id>
    <title>Is the RTX 5090 that good of a deal?</title>
    <updated>2025-11-09T15:53:07+00:00</updated>
    <author>
      <name>/u/GreenTreeAndBlueSky</name>
      <uri>https://old.reddit.com/user/GreenTreeAndBlueSky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"&gt; &lt;img alt="Is the RTX 5090 that good of a deal?" src="https://preview.redd.it/v9cx89gr690g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c96c392f6cfc52c3e86aaa74147a35d574967307" title="Is the RTX 5090 that good of a deal?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to find a model agnostic approach to estimate which cards to pick &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreenTreeAndBlueSky"&gt; /u/GreenTreeAndBlueSky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v9cx89gr690g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osmr6i/is_the_rtx_5090_that_good_of_a_deal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T15:53:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1osglws</id>
    <title>Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench</title>
    <updated>2025-11-09T11:01:13+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt; &lt;img alt="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" src="https://preview.redd.it/mqize91iq70g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=717d19b768a2a88e4b9a2a6690bbde033a817303" title="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqize91iq70g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1osnnfn</id>
    <title>How to build an AI computer (version 2.0)</title>
    <updated>2025-11-09T16:28:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt; &lt;img alt="How to build an AI computer (version 2.0)" src="https://preview.redd.it/03t3yj51d90g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6d467717022f60865bdfcbb8d96bb265e2bdb541" title="How to build an AI computer (version 2.0)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/03t3yj51d90g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osnnfn/how_to_build_an_ai_computer_version_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T16:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
