<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-24T19:34:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qkxuv1</id>
    <title>Sweep: Open-weights 1.5B model for next-edit autocomplete</title>
    <updated>2026-01-23T17:57:04+00:00</updated>
    <author>
      <name>/u/Kevinlu1248</name>
      <uri>https://old.reddit.com/user/Kevinlu1248</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, we just open-sourced a 1.5B parameter model that predicts your next code edits. You can grab the weights on &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5b"&gt;Hugging Face&lt;/a&gt; or try it out via our &lt;a href="https://plugins.jetbrains.com/plugin/26860-sweep-ai-autocomp"&gt;JetBrains plugin&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes this different from regular autocomplete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next-edit prediction uses your &lt;em&gt;recent edits&lt;/em&gt; as context, not just the code around your cursor. So if you're renaming a variable or making repetitive changes, it anticipates what you're doing next. The model is small enough to run locally and actually outperforms models 4x its size on both speed and accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some things we learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt format matters way more than expected.&lt;/strong&gt; We ran a genetic algorithm over 30+ diff formats and found that simple &lt;code&gt;&amp;lt;original&amp;gt;&lt;/code&gt; / &lt;code&gt;&amp;lt;updated&amp;gt;&lt;/code&gt; blocks beat unified diffs. Turns out verbose formats are just easier for smaller models to grok.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL fixed what SFT couldn't.&lt;/strong&gt; Training was SFT on ~100k examples from permissively-licensed repos (4 hrs on 8xH100), then 2000 steps of RL with tree-sitter parse checking and size regularization. This cleaned up edge cases like unparseable code and overly verbose outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We tested against Mercury (Inception), Zeta (Zed), and Instinct (Continue) across five benchmarks: next-edit above/below cursor, tab-to-jump, standard FIM, and noisiness. Exact-match accuracy ended up correlating best with real-world usability since code is precise and the solution space is small.&lt;/p&gt; &lt;p&gt;We're releasing the weights so anyone can build fast, privacy-preserving autocomplete for whatever editor they use. If you're working on VSCode, Neovim, or anything else, we'd love to see what you build with it!&lt;/p&gt; &lt;p&gt;Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kevinlu1248"&gt; /u/Kevinlu1248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T17:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlvvbm</id>
    <title>Preventing background-image: url('data: tags from being output</title>
    <updated>2026-01-24T19:11:22+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have noticed that smaller models, such as Nemotron 30B, GLM Flash 4.7, and others, frequently get into loops or generate garbage output when outputting HTML, due to one specific pattern&lt;/p&gt; &lt;p&gt;background-image: url('data:image/png.......'&lt;/p&gt; &lt;p&gt;When a model starts writing a block like this, it quickly devolves into a repeating string of gibberish, and the output is useless&lt;/p&gt; &lt;p&gt;Is there a simple way to get the inference server to never output a specific sequence like this? It looks like I can penalize certain tokens, but I am looking to penalize a certain sequence of tokens, which would require the inference server to look ahead a few tokens and then backtrack&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlvvbm/preventing_backgroundimage_urldata_tags_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlvvbm/preventing_backgroundimage_urldata_tags_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlvvbm/preventing_backgroundimage_urldata_tags_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlvz0l</id>
    <title>M2 Mac max 65g ram. Issues</title>
    <updated>2026-01-24T19:15:17+00:00</updated>
    <author>
      <name>/u/Disastrous_Purpose22</name>
      <uri>https://old.reddit.com/user/Disastrous_Purpose22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm trying to use ollama for local coding it‚Äôs slow but tolerable. &lt;/p&gt; &lt;p&gt;When I first set it up it worked fine. Now out of no where. If I type hi in to the chat. It sits and loads indefinitely. &lt;/p&gt; &lt;p&gt;To fix the issue I have to uninstall it and redownload the model. &lt;/p&gt; &lt;p&gt;Anyone experiencing this issue. &lt;/p&gt; &lt;p&gt;Have setup advise? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Purpose22"&gt; /u/Disastrous_Purpose22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlvz0l/m2_mac_max_65g_ram_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlvz0l/m2_mac_max_65g_ram_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlvz0l/m2_mac_max_65g_ram_issues/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlw8vl</id>
    <title>Loki-v2-70B: Narrative/DM-focused fine-tune (600M+ token custom dataset)</title>
    <updated>2026-01-24T19:25:15+00:00</updated>
    <author>
      <name>/u/mentallyburnt</name>
      <uri>https://old.reddit.com/user/mentallyburnt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello from Crucible Labs!&lt;/p&gt; &lt;p&gt;We just finished the 1-epoch fine-tune for Loki-v2-70B, based on Llama-3.3-70B-Instruct.&lt;/p&gt; &lt;p&gt;The goal with this project wasn't to make another &amp;quot;helpful assistant,&amp;quot; but to build a model specifically for long-form narrative, TTRPG-style Dungeon Mastering, and consistent roleplay.&lt;/p&gt; &lt;p&gt;We‚Äôve spent around six months generating and curating a V2 version of our original Loki Dataset in what we believe is the largest custom-generated dataset for this specific niche:&lt;/p&gt; &lt;p&gt;Total Tokens: 600M+&lt;/p&gt; &lt;p&gt;Size: ~2.5 GB&lt;/p&gt; &lt;p&gt;Composition: 46k+ QA lines, 19k+ prose lines, and 12k+ lines focused on dark/high-stakes scenarios.&lt;/p&gt; &lt;p&gt;The model card has a very extensive guide on how to use the model and details on worlds and universes, so please make sure to read through it!&lt;/p&gt; &lt;p&gt;This is an independent project, so we‚Äôre looking for genuine feedback on how it handles long-context narrative and whether the DM bias feels right to you.&lt;/p&gt; &lt;p&gt;L3.3-70B-Loki-V2.0:&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-GGUF"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EXL3: &lt;a href="https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-EXL3"&gt;https://huggingface.co/CrucibleLab/L3.3-70B-Loki-V2.0-EXL3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lower quants seem to have an issue with how we trained in 256 rank, so please be aware of this. Higher rank training=more affected by quantization, and there doesn't seem to be a way to alleviate this.&lt;/p&gt; &lt;p&gt;- The Crucible Labs Team&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mentallyburnt"&gt; /u/mentallyburnt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlw8vl/lokiv270b_narrativedmfocused_finetune_600m_token/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qloehi</id>
    <title>Any good LOCAL alternative or similar to what AI-Studio (Gemini 2.5 Flash) from Google does?</title>
    <updated>2026-01-24T14:29:31+00:00</updated>
    <author>
      <name>/u/VirtualWishX</name>
      <uri>https://old.reddit.com/user/VirtualWishX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I played around with &lt;a href="http://aistudio.google.com"&gt;aistudio.google.com&lt;/a&gt; for a bit, and I could easily make an app to generate multiple images from one image (as a quick test). it created all the nice drag and drop UI and everything worked almost perfect on my first attempt. I'm not sure what is the final result it doesn't look like Gradio but the UI is nice enough to work on a web browser, also it uses online stuff probably.&lt;/p&gt; &lt;p&gt;I have some Questions, as a NOOB sorry but I'm clueless + confused: &lt;/p&gt; &lt;p&gt;I own Nvidia RTX 5090 32GB VRAM and 96GB RAM (if it helps)&lt;br /&gt; I'm aware that this is not enough because LLM are huge, but maybe there is something that can work? ü§î&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Is there a &amp;quot;close&amp;quot; or at least almost, to do something similar locally?&lt;br /&gt; so I can create some LOCAL apps, if needed to use MODELS for the app, such the example I gave on top using Z-Image or Qwen, etc.. so it looks on a local folder (or I don't mind DOWNLOAD them) the thing is: &lt;/p&gt; &lt;p&gt;1Ô∏è‚É£ - I don't know if there is such POWERFUL model I can use on &lt;strong&gt;LM-Studio&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;2Ô∏è‚É£ - I don't know if there is a way to build webUI (Gradio or anything else similar to Gemini 2.5 on AI-Studio by google because I want to create local APPS with easy to use GUI.&lt;/p&gt; &lt;p&gt;3Ô∏è‚É£ - I don't know if any of the LM-Studio models that one of you (awesome people) will recommend can also work ONLINE and look for information such as models, or download what's needed, etc.. (probably not, but I have no idea how thee things working in LM-Studio)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Last thing,&lt;br /&gt; if anyone tried AI-Studio and also LM-Studio with something similar on RTX 5090 32GB and can tell me IT WORKS! please share your experience, what you managed to create with it, and of course... what do I need to download to prepare it to work.&lt;/p&gt; &lt;p&gt;I currently have: VS Code installed + LM Studio (with zero models downloaded)&lt;/p&gt; &lt;p&gt;Thanks ached! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualWishX"&gt; /u/VirtualWishX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloehi/any_good_local_alternative_or_similar_to_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloehi/any_good_local_alternative_or_similar_to_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qloehi/any_good_local_alternative_or_similar_to_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qls7fx</id>
    <title>OpenAPI ‚Üí ‚Äúagent skills‚Äù generator</title>
    <updated>2026-01-24T16:58:14+00:00</updated>
    <author>
      <name>/u/phantom0112</name>
      <uri>https://old.reddit.com/user/phantom0112</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a small CLI that converts an OpenAPI 3.x spec into a set of ‚Äúagent skills‚Äù markdown files (overview + per-operation + schemas), so an agent can load only what it needs instead of the entire spec.&lt;/p&gt; &lt;h2&gt;Why&lt;/h2&gt; &lt;p&gt;With larger APIs, dumping the full OpenAPI into context is expensive and often hurts relevance. I wanted a deterministic, file-based structure that works with any local agent or RAG setup, without special plugins or MCP servers.&lt;/p&gt; &lt;h2&gt;What it outputs&lt;/h2&gt; &lt;p&gt;{skill-name}/ SKILL.md references/ resources/ operations/ schemas/ authentication.md&lt;/p&gt; &lt;h2&gt;Quick demo&lt;/h2&gt; &lt;p&gt;&lt;code&gt; npx openapi-to-skills ./openapi.yaml -o ./skills &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Real-world scale test&lt;/h2&gt; &lt;p&gt;I ran it on the full Stripe OpenAPI spec (~7.2 MB, ~588 operations): - 1 monolithic spec ‚Üí 2,135 skill files - 588 operations ‚Üí 588 individual endpoint files - 1,315 schemas ‚Üí 1,468 grouped schema files&lt;/p&gt; &lt;p&gt;The idea is that an agent first loads SKILL.md, then only fetches the specific endpoint or schema file when needed.&lt;/p&gt; &lt;p&gt;I‚Äôm currently using this with a local agent + file-based retriever, but it should work with any tool-using or RAG-style setup.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/neutree-ai/openapi-to-skills"&gt;https://github.com/neutree-ai/openapi-to-skills&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Author here ‚Äî open-source, free, no hosted service. Would love feedback from people building local agents or tool-calling pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantom0112"&gt; /u/phantom0112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qls7fx/openapi_agent_skills_generator/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qls7fx/openapi_agent_skills_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qls7fx/openapi_agent_skills_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T16:58:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlejvk</id>
    <title>Is anyone else worried about the enshitifciation cycle of AI platforms? What is your plan (personal and corporate)</title>
    <updated>2026-01-24T05:33:33+00:00</updated>
    <author>
      <name>/u/Ngambardella</name>
      <uri>https://old.reddit.com/user/Ngambardella</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm starting to see the oh to familiar pattern of the enshitifcation cycle starting to rear its head in the AI space.&lt;/p&gt; &lt;p&gt;For those unfamiliar, enshitification is a term that defines the ‚Äúdeliberate, gradual degradation of quality in digital platforms‚Äù. Something that we have all seen time and time again.&lt;/p&gt; &lt;p&gt;The cycle is as follows:&lt;/p&gt; &lt;p&gt;Stage 1: Good for users&lt;/p&gt; &lt;p&gt;Stage 2: Good for business customers (defined as extracting money from platform at the users expense, whether through ads, features that make the platform&lt;/p&gt; &lt;p&gt;More unusable, etc.)&lt;/p&gt; &lt;p&gt;Stage 3: Good for shareholders (the final push to squeeze every drop of remaining value out of the product, by making user experience significantly worse, as well as screwing business customers by increasing rates, worse bank for your buck, etc.)&lt;/p&gt; &lt;p&gt;I believe we are starting to enter stage 2. Although I haven‚Äôt seen any (clearly stated) ads, I have seen a lot more discussion about integrated ads in AI chats. I‚Äôve also noticed significantly reduced performance with higher usage, clearly stated rate limiting (even on paid apps), etc.&lt;/p&gt; &lt;p&gt;Right now it would be a death sentence for any company to fully enshitify, but once the competition slows down and companies start to drop out of the race, or if one company jumps significantly above the rest, we will start to really see stage 2 come to fruition.&lt;/p&gt; &lt;p&gt;In a personal setting this bothers me because I work on a lot of highly technical/niche applications and I really need accurate and consistent answers that are consistent over a larger context window, and having to start a new chat/switch apps is honestly a nightmare. To the point where I am looking to refine my workflow to allow me to switch more efficiently mid conversation.&lt;/p&gt; &lt;p&gt;In a corporate setting this is definitely going to be an issue for those not running self hosted models, it is such an easy game plan for the LLM companies to extract revenue. Get all these companies setup on your AI integrated into their internal applications, push the compliance argument, start to deprecate models/increase cost, ???, profit.&lt;/p&gt; &lt;p&gt;Thankfully most corporate applications don‚Äôt require state of the art models. But still, I think everyone should be monitoring value metrics and have contingencies in place for in both settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ngambardella"&gt; /u/Ngambardella &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T05:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql9b7m</id>
    <title>Talk me out of buying an RTX Pro 6000</title>
    <updated>2026-01-24T01:25:33+00:00</updated>
    <author>
      <name>/u/AvocadoArray</name>
      <uri>https://old.reddit.com/user/AvocadoArray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I feel the need to preface my posts saying this was &lt;strong&gt;entirely written by me with zero help from an LLM&lt;/strong&gt;. A lot of people see a long post w/ headers and automatically think it's AI slop (myself included sometimes). This post might be slop, but it's my slop.&lt;/p&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;I've been talking myself out of buying an RTX pro 6000 every day for about a month now. I can &lt;em&gt;almost&lt;/em&gt; rationalize the cost, but keep trying to put it out of my mind. Today's hitting a bit different though.&lt;/p&gt; &lt;p&gt;I can &amp;quot;afford&amp;quot; it, but I'm a cheap bastard that hates spending money because every dollar I spend is one less going to savings/retirement. For reference, this would be the single most expensive item I've bought in the last 10 years, including cars. Since I hardly ever spend this kind of money, I'm sure I could rationalize it to my wife, but it's probably only be fair for her to get similar amount of budget to spend on something fun lol, so I guess it sort of doubles the cost in a way.&lt;/p&gt; &lt;h1&gt;Intended Usage&lt;/h1&gt; &lt;p&gt;I've slowly been using more local AI at work for RAG, research, summarization and even a bit of coding with Seed OSS / Roo Code, and I constantly see ways I can benefit from that in my personal life as well. I try to do what I can with the 16GB VRAM in my 5070ti, but it's just not enough to handle the models at the size and context I want. I'm also a staunch believer in hosting locally, so cloud models are out of the question.&lt;/p&gt; &lt;p&gt;At work, 2x L4 GPUs (48GB VRAM total) is just &lt;em&gt;barely&lt;/em&gt; enough to run Seed OSS at INT4 with enough context for coding. It's also not the fastest at 20 tp/s max, which drops to around 12 tp/s at 100k context. I'd really prefer to run it at a higher quant and more unquantized F16 kv cache. I'm making the case to budget for a proper dual R6000 server at work, but that's just going to make me more jealous at home lol.&lt;/p&gt; &lt;p&gt;I've also considered getting 2x or 4x RTX 4000's (24GB/ea) piece, but that also comes with the same drawbacks of figuring out where to host them, and I suspect the power usage would be even worse. Same thing with multiple 3090s.&lt;/p&gt; &lt;h1&gt;Hardware&lt;/h1&gt; &lt;p&gt;I also just finished replaced a bunch of server/networking hardware in my home lab to drop power costs and save money, which should pay for itself after ~3.5 years. Thankfully I got all that done before the RAM shortage started driving prices up. However, my new server hardware won't support a GPU needing auxiliary power.&lt;/p&gt; &lt;p&gt;I haven't sold my old r720xd yet, and it &lt;em&gt;technically&lt;/em&gt; supports two 300w double-length cards, but that would probably be pushing the limit. The max-q edition has a 300w TDP, but the power adapter looks like it requires 2x 8-pin PCIe input to convert to CEM5, so I'd either have to run it off one cable or rig something up (maybe bring the power over from the other empty riser).&lt;/p&gt; &lt;p&gt;I also have a 4U whitebox NAS using a low-power SuperMicro Xeon E3 motherboard. It has a Corsair 1000w PSU to power the stupid amount of SAS drives I used to have in there, but now it's down to 4x SAS drives and a handful of SATA SSDs, so it could easily power the GPU as well. However, that would require a different motherboard with more PCI-E slots/lanes, which would almost certainly increase the idle power consumption (currently &amp;lt;90w).&lt;/p&gt; &lt;p&gt;I guess I could also slap it in my gaming rig to replace my 5070ti (also a painful purchase), but I'd prefer to run VLLM on a Linux VM (or bare metal) so I can run background inference while gaming as well. I also keep it&lt;/p&gt; &lt;h1&gt;Power&lt;/h1&gt; &lt;p&gt;Speaking of power usage, I'm having trouble finding real idle power usage numbers for the RTX 6000 Pro. My old GTX 1080 idled very low in the PowerEdge (only 6w with models loaded according to nvidia-smi), but somehow the L4 cards we use at work idle around ~30w in the same configuration.&lt;/p&gt; &lt;p&gt;So at this point I'm really just trying to get a solid understanding of what the ideal setup would look like in my situation, and what it would cost in terms of capex and power consumption. Then I can at least make a decision on objective facts rather than the impulsive tickle in my tummy to just pull the trigger.&lt;/p&gt; &lt;p&gt;For those of you running R6000's:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's your idle power usage (per card and whole system)?&lt;/li&gt; &lt;li&gt;Does anyone have any experience running them in &amp;quot;unsupported&amp;quot; hardware like the PowerEdge r720/r730?&lt;/li&gt; &lt;li&gt;What reasons would you &lt;strong&gt;not&lt;/strong&gt; recommend buying one?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Talk me down Reddit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AvocadoArray"&gt; /u/AvocadoArray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T01:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlmu0j</id>
    <title>What should be my coding agent machine under 5k USD? Should I build one or purchase one of those DGX Sparks or get a mac studio? Open to anything that fits in my budget!</title>
    <updated>2026-01-24T13:21:40+00:00</updated>
    <author>
      <name>/u/pacifio</name>
      <uri>https://old.reddit.com/user/pacifio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using claude code for a while and it's pretty annoying when it I have to wait for the rate limit thing, I want to purchase a capable compute to run a capable coding model offline, perhaps GLM? not sure but I think I will figure that out but if anyone is using a local coding station please let me know, I hate just how annoying it is to wait for a couple of hours to continue my coding/brainstorming session!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pacifio"&gt; /u/pacifio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlmu0j/what_should_be_my_coding_agent_machine_under_5k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlmu0j/what_should_be_my_coding_agent_machine_under_5k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlmu0j/what_should_be_my_coding_agent_machine_under_5k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T13:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwcoi</id>
    <title>My Strix Halo beholds itself but believes its in the cloud</title>
    <updated>2026-01-24T19:29:20+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"&gt; &lt;img alt="My Strix Halo beholds itself but believes its in the cloud" src="https://external-preview.redd.it/cnJ2N2V3eWxtY2ZnMQh20pvmbVT22ZdBE-sn9Tc8Ujs4oEH6LQUVqmOmu06-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bff3dbe0b1a561ec97bacc8d8a2a5c4290b4f772" title="My Strix Halo beholds itself but believes its in the cloud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This iPhone app sends photos to a VLM served by the Halo on the local network and gets the response back. &lt;/p&gt; &lt;p&gt;The singularity might require a new system prompt‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o88lli0mmcfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwcoi/my_strix_halo_beholds_itself_but_believes_its_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlfu2b</id>
    <title>engine for GLM 4.7 Flash that doesn't massively slow down as the context grows?</title>
    <updated>2026-01-24T06:42:56+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man, i just tried GLM 4.7 Flash in LMstudio on a 5090 and while the 150 tokens/sec at Q6 is nice on the first prompt, things rapidly go south speedwise after 10k, unlike any other model i've tried.&lt;/p&gt; &lt;p&gt;I am using all the recommended settings and my unsloth quant, llama.cpp runtime, and lmstudio are all up to date.&lt;/p&gt; &lt;p&gt;I see that ik_llama.cpp has a recent patch that reduces this slowdown:&lt;br /&gt; &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/1182"&gt;https://github.com/ikawrakow/ik_llama.cpp/pull/1182&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But i can't figure out how to compile it.&lt;/p&gt; &lt;p&gt;I was wondering if the implementation in vllm or some other engine doesn't suffer of this.&lt;/p&gt; &lt;p&gt;This seems like an otherwise pretty good model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T06:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlkp6x</id>
    <title>Built a library of LLM prompts for RAG</title>
    <updated>2026-01-24T11:30:26+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt; &lt;img alt="Built a library of LLM prompts for RAG" src="https://a.thumbs.redditmedia.com/5xg3Rwo4RIh0LAjeGlCsGR-iU-JyYfNfHJtQ638GqV0.jpg" title="Built a library of LLM prompts for RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gathered a set of RAG prompt templates focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;grounding constraints&lt;/li&gt; &lt;li&gt;citation rules&lt;/li&gt; &lt;li&gt;multi-source + uncertainty handling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Templates are copy-pasteable. If you try one, &lt;strong&gt;upvote/downvote&lt;/strong&gt; it so the best ones float up over time.&lt;/p&gt; &lt;p&gt;And if you have a prompt that consistently works, contribute it - I‚Äôd love to include it.&lt;/p&gt; &lt;p&gt;If useful, the library is here: &lt;a href="https://agentset.ai/rag-prompts"&gt;https://agentset.ai/rag-prompts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vwuxs2jn8afg1.png?width=2660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bee373363c01d0cda6b915cc8fd8902760f8fd7c"&gt;https://preview.redd.it/vwuxs2jn8afg1.png?width=2660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bee373363c01d0cda6b915cc8fd8902760f8fd7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T11:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlu6kh</id>
    <title>The mysterious price of Ada and and Ampere workstation GPUs</title>
    <updated>2026-01-24T18:10:28+00:00</updated>
    <author>
      <name>/u/insulaTropicalis</name>
      <uri>https://old.reddit.com/user/insulaTropicalis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's just something I can't wrap my head around.&lt;/p&gt; &lt;p&gt;An RTX Blackwell Pro 5000 has 48GB memory. Compute is less than an RTX 6000 Ada, but not so much less. If you use FP4 it is much more. QAT with 4-bit seems something that will become prevalent, so FP4 is a big deal. Memory bandwidth is 140% of Ada. Power draw is the same. PCIe is 5.0 vs 4.0.&lt;/p&gt; &lt;p&gt;Seems that Blackwell wins or ties in all important regards, and it costs &lt;em&gt;less&lt;/em&gt; than 6000 Ada. Even more bizzarre, RTX A6000 Ampere, which is inferior in every regard and very old, still costs as much as Pro 5000.&lt;/p&gt; &lt;p&gt;I understand that some people can have an Ada or Ampere multi-GPU set-up and wants to expend it or to change a broken one, but is it enough to explain this weird market? Do these sellers actually find buyers?&lt;/p&gt; &lt;p&gt;Even RTX 4090 costs more today than when I bought mine. Who buys at these prices? What am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/insulaTropicalis"&gt; /u/insulaTropicalis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlu6kh/the_mysterious_price_of_ada_and_and_ampere/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:10:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlie1t</id>
    <title>Running MoE Models on CPU/RAM: A Guide to Optimizing Bandwidth for GLM-4 and GPT-OSS</title>
    <updated>2026-01-24T09:12:49+00:00</updated>
    <author>
      <name>/u/Shoddy_Bed3240</name>
      <uri>https://old.reddit.com/user/Shoddy_Bed3240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The core principle of running Mixture-of-Experts (MoE) models on CPU/RAM is that the CPU doesn't need to extract or calculate all weights from memory simultaneously. Only a fraction of the parameters are &amp;quot;active&amp;quot; for any given token, and since calculations are approximate, memory throughput becomes our primary bottleneck.&lt;/p&gt; &lt;h1&gt;The Math: Model Size vs. Memory Bandwidth&lt;/h1&gt; &lt;p&gt;Let's look at two popular models: &lt;strong&gt;GLM-4.7-Flash&lt;/strong&gt; (3B active params) and &lt;strong&gt;GPT OSS 120B&lt;/strong&gt; (5.1B active params). At Q4_K_M quantization, their active memory footprints are:&lt;/p&gt; &lt;p&gt;Now, let's look at theoretical vs. realistic &lt;strong&gt;DDR5 Dual-Channel Bandwidth&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality Check:&lt;/strong&gt; We rarely hit theoretical peaks when reading small, scattered chunks of data. A realistic &amp;quot;sustained&amp;quot; bandwidth for LLM inference is closer to &lt;strong&gt;35 GB/s&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Doing the math for DDR5-6000:&lt;/p&gt; &lt;p&gt;If you can fully stress your memory bus, these are the speeds you can expect.&lt;/p&gt; &lt;h1&gt;Hardware Optimization (Intel 14700f Example)&lt;/h1&gt; &lt;p&gt;To hit these numbers, your CPU and BIOS settings must be dialed in:&lt;/p&gt; &lt;h1&gt;Software Stack &amp;amp; Compilation&lt;/h1&gt; &lt;p&gt;I‚Äôm running on Linux with the latest drivers (Nvidia 590.48 / CUDA 13.1) and GCC 15.2. For maximum performance, you &lt;strong&gt;must&lt;/strong&gt; compile llama.cpp from source with flags optimized for your specific architecture (Raptor Lake in this case).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Build Command:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;p&gt;cmake .. -DGGML_CUDA=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_GRAPHS=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_USE_CUBLASLT=ON \&lt;/p&gt; &lt;p&gt;-DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120a;86&amp;quot; \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_TENSOR_CORES=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_FP16=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_INT8=ON \&lt;/p&gt; &lt;p&gt;-DGGML_AVX512=OFF \&lt;/p&gt; &lt;p&gt;-DGGML_AVX2=ON \&lt;/p&gt; &lt;p&gt;-DGGML_FMA=ON \&lt;/p&gt; &lt;p&gt;-DGGML_F16C=ON \&lt;/p&gt; &lt;p&gt;-DCMAKE_C_COMPILER=gcc-15 \&lt;/p&gt; &lt;p&gt;-DCMAKE_CXX_COMPILER=g++-15 \&lt;/p&gt; &lt;p&gt;-DCMAKE_C_FLAGS=&amp;quot;-march=raptorlake -mtune=native -O3 -flto=auto&amp;quot; \&lt;/p&gt; &lt;p&gt;-DCMAKE_CXX_FLAGS=&amp;quot;-march=raptorlake -mtune=native -O3 -flto=auto&amp;quot; \&lt;/p&gt; &lt;p&gt;-DGGML_OPENMP=ON \&lt;/p&gt; &lt;p&gt;-DGGML_OPENMP_DYNAMIC=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_ENABLE_UNIFIED_MEMORY=OFF \&lt;/p&gt; &lt;p&gt;-DGGML_LTO=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_BLACKWELL_NATIVE_FP4=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_USE_CUDNN=ON \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_MAX_CONTEXT=32768 \&lt;/p&gt; &lt;p&gt;-DBUILD_SHARED_LIBS=OFF \&lt;/p&gt; &lt;p&gt;-DGGML_CUDA_MAX_STREAMS=8 \&lt;/p&gt; &lt;p&gt;-DCMAKE_BUILD_TYPE=Release&lt;/p&gt; &lt;h1&gt;Running the Server&lt;/h1&gt; &lt;p&gt;The key is to pin the process to your &lt;strong&gt;Performance Cores (P-cores)&lt;/strong&gt; and avoid the Efficiency Cores (E-cores), which can slow down the memory-heavy threads.&lt;/p&gt; &lt;p&gt;For the 14700f, I use taskset to bind to the first 16 logical threads (P-cores):&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;p&gt;taskset -c 0-15 llama-server \&lt;/p&gt; &lt;p&gt;-m /data/gguf/GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf \&lt;/p&gt; &lt;p&gt;--ctx-size 64000 \&lt;/p&gt; &lt;p&gt;--jinja \&lt;/p&gt; &lt;p&gt;-fa 1 \&lt;/p&gt; &lt;p&gt;--no-warmup \&lt;/p&gt; &lt;p&gt;--threads 16 \&lt;/p&gt; &lt;p&gt;--numa distribute \&lt;/p&gt; &lt;p&gt;--threads-batch 16 \&lt;/p&gt; &lt;p&gt;--host 0.0.0.0 \&lt;/p&gt; &lt;p&gt;--port 8080 \&lt;/p&gt; &lt;p&gt;--temp 1.0 \&lt;/p&gt; &lt;p&gt;--top-p 0.95 \&lt;/p&gt; &lt;p&gt;--min-p 0.01 \&lt;/p&gt; &lt;p&gt;--repeat-penalty 1.0&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Pro Tip:&lt;/strong&gt; Don't disable your GPU! Even if the model doesn't fit entirely on the VRAM, llama.cpp can offload specific layers to the GPU, providing a nice speed boost to the overall generation.&lt;/p&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;Thanks for the comments. About the build flags: these are the flags I actually use in my working setup. Not everything here is about raw CPU optimization ‚Äî a good portion is tuned for my specific builds (Blackwell and Ampere). Feel free to use or ignore any flags depending on your own setup.&lt;/p&gt; &lt;h1&gt;Performance Tests (llama-bench, CPU-only / NO GPU)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;System notes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Threads: 16&lt;/li&gt; &lt;li&gt;Backend listed as CUDA by the runner, but &lt;strong&gt;NO GPU used&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Metrics: tokens/sec (t/s)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üîπ GLM-4.7-Flash Q4_K_M (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;101.65 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;84.25 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;23.41 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;22.93 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ GLM-4.7-Flash Q8_0 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;99.59 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;82.94 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.13 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q8_0&lt;/td&gt; &lt;td align="left"&gt;32.70 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;14.93 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ GLM-4.7-Flash BF16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;62.00 ¬± 0.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;55.15 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;10.59 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B BF16&lt;/td&gt; &lt;td align="left"&gt;55.79 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;10.50 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ gpt-oss-120B F16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;56.25 ¬± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;54.31 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;15.18 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gpt-oss-120B F16&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;15.03 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Devstral-Small-2-24B-Instruct-2512 BF16 (NO GPU) - not MoE&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;18.99 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;18.69 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;1.95 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mistral3 14B BF16&lt;/td&gt; &lt;td align="left"&gt;43.91 GiB&lt;/td&gt; &lt;td align="left"&gt;23.57 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;1.94 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üîπ Qwen3-coder-30B-a3b BF16 (NO GPU)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;69.48 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;64.75 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;12.43 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3moe 30B.A3B BF16&lt;/td&gt; &lt;td align="left"&gt;56.89 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;12.34 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;üöÄ GPU Reference (for scale)&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GLM-4.7-Flash Q4_K_M on GPU (5090)&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;NGL&lt;/th&gt; &lt;th align="left"&gt;Threads&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;4638.85 ¬± 13.57&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;pp2048&lt;/td&gt; &lt;td align="left"&gt;5927.16 ¬± 21.69&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;150.21 ¬± 0.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;deepseek2 ?B Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;29.94 B&lt;/td&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;tg512&lt;/td&gt; &lt;td align="left"&gt;143.16 ¬± 0.39&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy_Bed3240"&gt; /u/Shoddy_Bed3240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T09:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltqfx</id>
    <title>The Eval problem for AI Agents</title>
    <updated>2026-01-24T17:54:16+00:00</updated>
    <author>
      <name>/u/AlpineContinus</name>
      <uri>https://old.reddit.com/user/AlpineContinus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;I work at a company that develops AI agents for information retrieval, and I have observed some pretty important problems that are major bottlenecks for us.&lt;/p&gt; &lt;p&gt;I am very curious to hear from other people that work on AI agents companies to know if they face the same problems and how they handle it (approaches, tools, etc).&lt;/p&gt; &lt;p&gt;AI agents based on LLMs are essentially stochastic, and so it is very hard to affirm how well they behave. In order to evaluate it, you would need a relatively big, varied, realistic and bias-free dataset for your specific use case.&lt;/p&gt; &lt;p&gt;The problem is: Most specific use cases don‚Äôt have pre-made datasets available.&lt;/p&gt; &lt;p&gt;The option is to resort to synthetic data generation, but it is a pretty unreliable source of ground truth.&lt;/p&gt; &lt;p&gt;Writing a dataset by hand is not scalable at all.&lt;/p&gt; &lt;p&gt;The usual solution is some data augmentation on top of a curated hand-written dataset.&lt;/p&gt; &lt;p&gt;It feels like the entire AI agents industry is being built on very shaky grounds. It is very hard to affirm anything about these systems with precise metrics. Most of the evaluation is done by hand and based on very subjective metrics. And I believe this is really holding back the adoption of these systems.&lt;/p&gt; &lt;p&gt;I would love to know how other developers see these problems, and how they currently tackle them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlpineContinus"&gt; /u/AlpineContinus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltqfx/the_eval_problem_for_ai_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltqfx/the_eval_problem_for_ai_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltqfx/the_eval_problem_for_ai_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T17:54:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkyex0</id>
    <title>Your post is getting popular and we just featured it on our Discord!</title>
    <updated>2026-01-23T18:16:47+00:00</updated>
    <author>
      <name>/u/roculus</name>
      <uri>https://old.reddit.com/user/roculus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your post is getting popular and we just featured it on our Discord! Come check it out!&lt;/p&gt; &lt;p&gt;You've also been given a special flair for your contribution. We appreciate your post!&lt;/p&gt; &lt;p&gt;I am a bot and this action was performed automatically.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Can you change this marketing bot to make these private messages to the OP of the post instead of pinning it to the top of all the threads? Are you making money off the discord or something? I don't know about anyone else but these bot spam posts are annoying. You make it appear you are talking to the OP so a private message would be better. You already have a pinned thread at the top of this reddit letting everyone know about the discord that's been there for the past 5 months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roculus"&gt; /u/roculus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T18:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql6cz7</id>
    <title>Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR</title>
    <updated>2026-01-23T23:20:23+00:00</updated>
    <author>
      <name>/u/Efficient-Proof-1824</name>
      <uri>https://old.reddit.com/user/Efficient-Proof-1824</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt; &lt;img alt="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" src="https://preview.redd.it/hlrhml65m6fg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=05c26e9e3a4c3182ca841254a0d81f18d6a5901f" title="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;The architecture on this thing is completely wonky, and it's a direct result of me changing ideas and scope midstream, but sharing because I think it's pretty neat&lt;/p&gt; &lt;p&gt;Ultimate goal for me here is to build an agent that can play Pokemon Red, ideally beat it! Plan is to use a mix of LLMs for action plan generation and then using a small neural network to score them. Set a auto-train and you can start stacking up data for training. I bundled everything here as a Svelte app and deployed it on github pages. &lt;/p&gt; &lt;p&gt;Live: &lt;a href="https://sidmohan0.github.io/tesserack/"&gt;https://sidmohan0.github.io/tesserack/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/sidmohan0/tesserack"&gt;https://github.com/sidmohan0/tesserack&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;LLM&lt;/strong&gt;: Qwen 2.5 1.5B running via WebLLM (WebGPU-accelerated) &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Policy network&lt;/strong&gt;: TensorFlow.js neural net that learns from gameplay &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Emulator&lt;/strong&gt;: binjgb compiled to WASM &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Game state&lt;/strong&gt;: Direct RAM reading for ground-truth (badges, party, location, items) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient-Proof-1824"&gt; /u/Efficient-Proof-1824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hlrhml65m6fg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T23:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlanzn</id>
    <title>GLM-4.7-Flash-REAP on RTX 5060 Ti 16 GB - 200k context window!</title>
    <updated>2026-01-24T02:26:28+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my latest local coding setup, the params are mostly based on &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash"&gt;Unsloth's recommendation for tool calling&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"&gt;unsloth/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repeat penalty: disabled&lt;/li&gt; &lt;li&gt;Temperature: 0.7&lt;/li&gt; &lt;li&gt;Top P: 1&lt;/li&gt; &lt;li&gt;Min P: 0.01&lt;/li&gt; &lt;li&gt;Standard Microcenter PC setup: RTX 5060 Ti 16 GB, 32 GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running this in LM Studio for my own convenience, but it can be run in any setup you have.&lt;/p&gt; &lt;p&gt;With 16k context, everything fit within the GPU, so the speed was impressive:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;965.16 tok/s&lt;/td&gt; &lt;td&gt;26.27 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The tool calls were mostly accurate and the generated code was good, but the context window was too little, so the model ran into looping issue after exceeding that. It kept making the same tool call again and again because the conversation history was truncated.&lt;/p&gt; &lt;p&gt;With 64k context, everything still fit, but the speed started to slow down.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;671.48 tok/s&lt;/td&gt; &lt;td&gt;8.84 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm pushing my luck to see if 100k context still fits. It doesn't! Hahaha. The CPU fan started to scream, RAM usage spiked up, GPU copy chart (in Task Manager) started to dance. Completely unusable.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;172.02 tok/s&lt;/td&gt; &lt;td&gt;0.51 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LM Studio just got the new &amp;quot;Force Model Expert Weight onto CPU&amp;quot; feature (basically llama.cpp's &lt;code&gt;--n-cpu-moe&lt;/code&gt;), and yeah, why not? this is also an MoE model, so let's enable that. Still with 100k context. And wow! only half of the GPU memory was used (7 GB), but with 90% RAM now (29 GB), seems like flash attention also got disabled. The speed was impressive.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;485.64 tok/s&lt;/td&gt; &lt;td&gt;8.98 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's push our luck again, this time, 200k context!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;324.84 tok/s&lt;/td&gt; &lt;td&gt;7.70 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What a crazy time. Almost very month we're getting beefier models that somehow fit on even crappier hardware. Just this week I was thinking of selling my 5060 for an old 3090, but that definitely unnecessary now!&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Turned out with CPU MoE offload, I can just run the non-REAP model it self. Here's the speed for UD Q5_K_XL on my card, at 100k token window:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;206.07 tok/s&lt;/td&gt; &lt;td&gt;5.06 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;With more tweak, reducing GPU offload count (36/47), keep KV cache in GPU memory, disable nmap,... the speed increased.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;267.23 tok/s&lt;/td&gt; &lt;td&gt;6.23 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;And yes, I was running this without Flash Attention the whole time, since LM Studio didn't support it this model (at the time of writing).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T02:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qloeu4</id>
    <title>MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations</title>
    <updated>2026-01-24T14:29:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt; &lt;img alt="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" src="https://a.thumbs.redditmedia.com/J6s266XP1Z7JRBZht5FAQGe2o36oOfOTvuiPLWD9El8.jpg" title="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/minimax/minimax-m2-her"&gt;https://openrouter.ai/minimax/minimax-m2-her&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations. Designed to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f"&gt;https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/api-reference/text-chat"&gt;https://platform.minimax.io/docs/api-reference/text-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/guides/models-intro"&gt;https://platform.minimax.io/docs/guides/models-intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qljf7o</id>
    <title>AI &amp; ML Weekly ‚Äî Hugging Face Highlights</title>
    <updated>2026-01-24T10:15:01+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the most notable &lt;strong&gt;AI models released or updated this week on Hugging Face&lt;/strong&gt;, categorized for easy scanning üëá&lt;/p&gt; &lt;h1&gt;Text &amp;amp; Reasoning Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7 (358B)&lt;/strong&gt; ‚Äî Large-scale multilingual reasoning model &lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash (31B)&lt;/strong&gt; ‚Äî Faster, optimized variant for text generation &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;https://huggingface.co/zai-org/GLM-4.7-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth GLM-4.7-Flash GGUF (30B)&lt;/strong&gt; ‚Äî Quantized version for local inference &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiquidAI LFM 2.5 Thinking (1.2B)&lt;/strong&gt; ‚Äî Lightweight reasoning-focused LLM &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alibaba DASD-4B-Thinking&lt;/strong&gt; ‚Äî Compact thinking-style language model &lt;a href="https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking"&gt;https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Agent &amp;amp; Workflow Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Report (8B)&lt;/strong&gt; ‚Äî Agent model optimized for report generation &lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;https://huggingface.co/openbmb/AgentCPM-Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Explore (4B)&lt;/strong&gt; ‚Äî Exploration-focused agent reasoning model &lt;a href="https://huggingface.co/openbmb/AgentCPM-Explore"&gt;https://huggingface.co/openbmb/AgentCPM-Explore&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sweep Next Edit (1.5B)&lt;/strong&gt; ‚Äî Code-editing and refactoring assistant &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"&gt;https://huggingface.co/sweepai/sweep-next-edit-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio: Speech, Voice &amp;amp; TTS&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VibeVoice-ASR (9B)&lt;/strong&gt; ‚Äî High-quality automatic speech recognition &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PersonaPlex 7B&lt;/strong&gt; ‚Äî Audio-to-audio personality-driven voice model &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 TTS (1.7B)&lt;/strong&gt; ‚Äî Custom &amp;amp; base voice text-to-speech models &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pocket-TTS&lt;/strong&gt; ‚Äî Lightweight open TTS model &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HeartMuLa OSS (3B)&lt;/strong&gt; ‚Äî Text-to-audio generation model &lt;a href="https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B"&gt;https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Vision: Image, OCR &amp;amp; Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Step3-VL (10B)&lt;/strong&gt; ‚Äî Vision-language multimodal model &lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;https://huggingface.co/stepfun-ai/Step3-VL-10B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LightOnOCR 2 (1B)&lt;/strong&gt; ‚Äî OCR-focused vision-language model &lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;https://huggingface.co/lightonai/LightOnOCR-2-1B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TranslateGemma (4B / 12B / 27B)&lt;/strong&gt; ‚Äî Multimodal translation models &lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;https://huggingface.co/google/translategemma-4b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-12b-it"&gt;https://huggingface.co/google/translategemma-12b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-27b-it"&gt;https://huggingface.co/google/translategemma-27b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MedGemma 1.5 (4B)&lt;/strong&gt; ‚Äî Medical-focused multimodal model &lt;a href="https://huggingface.co/google/medgemma-1.5-4b-it"&gt;https://huggingface.co/google/medgemma-1.5-4b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image Generation &amp;amp; Editing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-Image&lt;/strong&gt; ‚Äî Text-to-image generation model &lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;https://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FLUX.2 Klein (4B / 9B)&lt;/strong&gt; ‚Äî High-quality image-to-image models &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-4B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-4B&lt;/a&gt; &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-9B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-9B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen Image Edit (LoRA / AIO)&lt;/strong&gt; ‚Äî Advanced image editing &amp;amp; multi-angle edits &lt;a href="https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA"&gt;https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA&lt;/a&gt; &lt;a href="https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO"&gt;https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Z-Image-Turbo&lt;/strong&gt; ‚Äî Fast text-to-image generation &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Video Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LTX-2&lt;/strong&gt; ‚Äî Image-to-video generation model &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;https://huggingface.co/Lightricks/LTX-2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Any-to-Any / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chroma (6B)&lt;/strong&gt; ‚Äî Any-to-any multimodal generation &lt;a href="https://huggingface.co/FlashLabs/Chroma-4B"&gt;https://huggingface.co/FlashLabs/Chroma-4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T10:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlt3pw</id>
    <title>GLM 4.7 Flash uncensored - Balanced &amp; Aggressive variants (GGUF)</title>
    <updated>2026-01-24T17:30:56+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I made uncensored versions of the new GLM 4.7 Flash from Z.ai.&lt;/p&gt; &lt;p&gt;For those who don't know the model, it's 30B-A3B MoE, so only ~3B active params (will have fast inference!) and 200K context. Runs surprisingly well for what it is.&lt;/p&gt; &lt;p&gt;Two variants:&lt;/p&gt; &lt;p&gt;- Balanced - excellent for agentic coding stuff where you still want (uncensored) reliability&lt;/p&gt; &lt;p&gt;- Aggressive - great for every other uncensored topic&lt;/p&gt; &lt;p&gt;Quants available: FP16, Q8_0, Q6_K, Q4_K_M&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GLM-4.7-Flash-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sampling settings from Z.ai:&lt;/p&gt; &lt;p&gt;- General: --temp 1.0 --top-p 0.95&lt;/p&gt; &lt;p&gt;- Agentic/tool use: --temp 0.7 --top-p 1.0&lt;/p&gt; &lt;p&gt;- Keep repeat penalty at 1.0 (or directly off)&lt;/p&gt; &lt;p&gt;- llama.cpp users: --min-p 0.01 and --jinja&lt;/p&gt; &lt;p&gt;Heads up, it currently doesn't play nice with Ollama (has some chat template issues). Works fine with llama.cpp, LM Studio, Jan, koboldcpp.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;p&gt;Edit: P.S. For those looking for smaller models, I also did GPT-OSS 20B, MXFP4 - Lossless:&lt;br /&gt; - &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Balanced&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive"&gt;https://huggingface.co/HauhauCS/GPT-OSS-20B-Uncensored-HauhauCS-Aggressive&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlt3pw/glm_47_flash_uncensored_balanced_aggressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T17:30:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltwza</id>
    <title>Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence.</title>
    <updated>2026-01-24T18:00:50+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt; &lt;img alt="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." src="https://preview.redd.it/66fd18ro6cfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f579cce389f709dbf297867095118be2027f04ea" title="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/2014786516153991339"&gt;https://x.com/ArtificialAnlys/status/2014786516153991339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A key driver of this momentum is the Korean National Sovereign AI Initiative, a government-backed, nationwide competition that incentivizes domestic model development through a multi-stage elimination process. The initiative shortlists national champions, with winners receiving direct government funding and guaranteed access to large-scale GPU capacity.&lt;/p&gt; &lt;p&gt;‚û§ In August 2025, five organizations were selected: Naver, SK Telecom, LG Group, Upstage, and NC AI&lt;/p&gt; &lt;p&gt;‚û§ In the most recent round announced last week, the field narrowed to three: LG, SK Telecom, and Upstage.&lt;/p&gt; &lt;p&gt;‚û§ A fourth finalist is expected to be selected in the coming months as the evaluation process continues&lt;/p&gt; &lt;p&gt;Generally, top Korean AI models tend to be open weights, and vary in size ranging from Motif‚Äòs 12.7B Thinking model to LG‚Äôs 236B K-EXAONE. Other models, such as Korea Telecom (KT)‚Äôs Mi:dm K 2.5 Pro, are proprietary and developed with a focus on business integration with existing KT clients.&lt;/p&gt; &lt;p&gt;Overview of major releases:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ LG | K-EXAONE -&lt;/strong&gt; The current leader in the Korean AI race and a shortlisted model in the Korean National Sovereign AI Initiative. K-EXAONE is a 236B open weights model and scores 32 on the Artificial Analysis Intelligence Index. K-EXAONE performs strongly across various intelligence evaluations from scientific reasoning, instruction following, to agentic coding. However, this model has high verbosity, using 100 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Upstage | Solar Open -&lt;/strong&gt; Another shortlisted model in the Korean National Sovereign AI Initiative. Solar Open is a 100B open-weights model and scores 21 on the Artificial Analysis Intelligence Index. Solar Open performs well in instruction following and has lower hallucination rate compared to peer Korean models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Naver | HyperCLOVA X SEED Think -&lt;/strong&gt; A 32B open weights reasoning model that scores 24 on the Artificial Analysis Intelligence Index. HyperCLOVA X SEED Think demonstrates strong performance on agentic tool-use workflows and scores highly in the Global MMLU Lite multilingual index for Korean, highlighting its potential usefulness in a primarily Korean language environment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Korea Telecom | Mi:dm K 2.5 Pro -&lt;/strong&gt; A proprietary reasoning model that scores 23 on the Artificial Analysis Intelligence Index. Mi:dm K 2.5 Pro sees strong performance in agentic tool-use. Mi:dm K 2.5 Pro currently has no publicly available endpoint. Instead, Korea Telecom primarily intends to package this model into product offerings and use this model to serve KT‚Äôs clients&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Motif | Motif-2-12.7B -&lt;/strong&gt; A small open weights model that scores 24 on the Artificial Analysis Intelligence Index. Motif-2-12.7B performs well in long-context reasoning and knowledge, but is highly token intensive - using 120 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66fd18ro6cfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlr3wj</id>
    <title>I built an open-source audiobook converter using Qwen3 TTS - converts PDFs/EPUBs to high-quality audiobooks with voice cloning support</title>
    <updated>2026-01-24T16:16:15+00:00</updated>
    <author>
      <name>/u/TheyCallMeDozer</name>
      <uri>https://old.reddit.com/user/TheyCallMeDozer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Turn any book into an audiobook with AI voice synthesis!&lt;/strong&gt; I just released an open-source tool that converts PDFs, EPUBs, DOCX, and TXT files into high-quality audiobooks using &lt;strong&gt;Qwen3 TTS&lt;/strong&gt; - the amazing open-source voice model that just went public.&lt;/p&gt; &lt;h2&gt;What it does:&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Converts any document format&lt;/strong&gt; (PDF, EPUB, DOCX, DOC, TXT) into audiobooks &lt;strong&gt;Two voice modes&lt;/strong&gt;: Pre-built speakers (Ryan, Serena, etc.) or clone any voice from a reference audio &lt;strong&gt;Always uses 1.7B model&lt;/strong&gt; for best quality &lt;strong&gt;Smart chunking&lt;/strong&gt; with sentence boundary detection &lt;strong&gt;Intelligent caching&lt;/strong&gt; to avoid re-processing &lt;strong&gt;Auto cleanup&lt;/strong&gt; of temporary files &lt;/p&gt; &lt;h2&gt;Key Features:&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Custom Voice Mode&lt;/strong&gt;: Professional narrators optimized for audiobook reading&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Voice Clone Mode&lt;/strong&gt;: Automatically transcribes reference audio and clones the voice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-format support&lt;/strong&gt;: Works with PDFs, EPUBs, Word docs, and plain text&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sequential processing&lt;/strong&gt;: Ensures chunks are combined in correct order&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progress tracking&lt;/strong&gt;: Real-time updates with time estimates ## Quick Start: Install Qwen3 TTS (one-click install with Pinokio) Install Python dependencies: &lt;code&gt;pip install -r requirements.txt&lt;/code&gt; Place your books in &lt;code&gt;book_to_convert/&lt;/code&gt; folder Run: &lt;code&gt;python audiobook_converter.py&lt;/code&gt; Get your audiobook from &lt;code&gt;audiobooks/&lt;/code&gt; folder! ## Voice Cloning Example: &lt;code&gt;bash python audiobook_converter.py --voice-clone --voice-sample reference.wav &lt;/code&gt; The tool automatically transcribes your reference audio - no manual text input needed! ## Why I built this: I was frustrated with expensive audiobook services and wanted a free, open-source solution. Qwen3 TTS going open-source was perfect timing - the voice quality is incredible and it handles both generic speech and voice cloning really well. ## Performance:&lt;/li&gt; &lt;li&gt;Processing speed: ~4-5 minutes per chunk (1.7B model) it is a little slow im working on it&lt;/li&gt; &lt;li&gt;Quality: High-quality audio suitable for audiobooks&lt;/li&gt; &lt;li&gt;Output: MP3 format, configurable bitrate ## GitHub: üîó &lt;strong&gt;&lt;a href="https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter"&gt;https://github.com/WhiskeyCoder/Qwen3-Audiobook-Converter&lt;/a&gt;&lt;/strong&gt; &lt;strong&gt;What do you think?&lt;/strong&gt; Have you tried Qwen3 TTS? What would you use this for?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheyCallMeDozer"&gt; /u/TheyCallMeDozer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlr3wj/i_built_an_opensource_audiobook_converter_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T16:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlnruw</id>
    <title>Personal experience with GLM 4.7 Flash Q6 (unsloth) + Roo Code + RTX 5090</title>
    <updated>2026-01-24T14:02:56+00:00</updated>
    <author>
      <name>/u/Septerium</name>
      <uri>https://old.reddit.com/user/Septerium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am much more interested in how folks experience quantized versions of new models than just looking at bar graphs, so here is my humble contribution. &lt;/p&gt; &lt;p&gt;I have been using GLM 4.7 Flash to perform a few refactoring tasks in some personal web projects and have been quite impressed by how well the model handles Roo Code without breaking apart. For this agentic tool specifically, it has been much more reliable and precise than GPT-OSS 120b, GLM 4.5 Air, or Devstral 24b.&lt;/p&gt; &lt;p&gt;Here's the llama.cpp command I used to squeeze UD-Q6_K_XL + 48k tokens of context in my RTX 5090 VRAM and get about 150 tok/s (tg):&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server --model downloaded_models/GLM-4.7-Flash-UD-Q6_K_XL.gguf --port 11433 --host &amp;quot;0.0.0.0&amp;quot; -fa on --ctx-size 48000 --temp 0.7 --top-p 1.0 --min-p 0.01 --jinja -ngl 99&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septerium"&gt; /u/Septerium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
