<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-11T05:06:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nctfdv</id>
    <title>Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES</title>
    <updated>2025-09-09T20:01:35+00:00</updated>
    <author>
      <name>/u/Embarrassed_Sir_853</name>
      <uri>https://old.reddit.com/user/Embarrassed_Sir_853</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt; &lt;img alt="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" src="https://preview.redd.it/sxii7uog37of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61301382a59bd7671163d02b77eb25115e5d46e8" title="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this announcement about ROMA, seems like a plug-and-play and the benchmarks are up there. Simple combo of recursion and multi-agent structure with search tool. Crazy this is all it takes to beat SOTA billion dollar AI companies :)&lt;/p&gt; &lt;p&gt;I've been trying it out for a few things, currently porting it to my finance and real estate research workflows, might be cool to see it combined with other tools and image/video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://x.com/sewoong79/status/1963711812035342382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://github.com/sentient-agi/ROMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly shocked that this is open-source&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed_Sir_853"&gt; /u/Embarrassed_Sir_853 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndm44z</id>
    <title>Meet the first Small Language Model built for DevOps</title>
    <updated>2025-09-10T18:22:19+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/"&gt; &lt;img alt="Meet the first Small Language Model built for DevOps" src="https://external-preview.redd.it/-ri5XDjfEsn68t0ZbLOY4EDBl3sqIkK3c5sw9jPLLnY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f4026d755a51ee6eb2740d825f83f595fc08405" title="Meet the first Small Language Model built for DevOps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6g1peqn1qdof1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=320d07193643b6717a3b4ce64b526687378c45ab"&gt;https://preview.redd.it/6g1peqn1qdof1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=320d07193643b6717a3b4ce64b526687378c45ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Everywhere you look, LLMs are making headlines, from translation to writing essays to generating images. But one field that‚Äôs quietly running the backbone of tech has been left behind: DevOps.&lt;/p&gt; &lt;p&gt;We‚Äôve called it many names over the years , System Admin, System Engineer, SRE, Platform Engineer but the reality hasn‚Äôt changed: keeping systems alive, scaling infra, and fixing stuff when it breaks at 2 AM.&lt;/p&gt; &lt;p&gt;And yet, existing LLMs don‚Äôt really help here. They‚Äôre great at summarizing novels, but not so great at troubleshooting Kubernetes pods, parsing logs, or helping with CI/CD pipelines.&lt;/p&gt; &lt;p&gt;So I decided to build something different.&lt;/p&gt; &lt;p&gt;devops-slm-v1: &lt;a href="https://huggingface.co/lakhera2023/devops-slm-v1"&gt;https://huggingface.co/lakhera2023/devops-slm-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A small language model trained only for DevOps tasks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~907M parameters&lt;/li&gt; &lt;li&gt;Based on Qwen2.5&lt;/li&gt; &lt;li&gt;Fine-tuned with LoRA on DevOps examples&lt;/li&gt; &lt;li&gt;Quantized to 4-bit ‚Üí runs fine even on a modest GPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn‚Äôt a general-purpose AI. It‚Äôs built for our world: configs, infra automation, monitoring, troubleshooting, Kubernetes, CI/CD.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why it matters&lt;/strong&gt;&lt;br /&gt; Big LLMs like GPT or Claude cost thousands per month. This runs at $250‚Äì$720/month (90‚Äì95% cheaper) while still delivering DevOps-focused results.&lt;/p&gt; &lt;p&gt;It also runs on a single A4 GPU (16GB VRAM), using just 2‚Äì3GB of memory during inference. That makes it accessible for small teams, startups, and even hobby projects.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Still a work in progress&lt;/strong&gt;&lt;br /&gt; It‚Äôs not perfect, sometimes drifts outside DevOps, so I added filtering. Pruning/optimizations are ongoing. But it‚Äôs stable enough for people to try, break, and improve together.&lt;/p&gt; &lt;p&gt;Sample Code: &lt;a href="https://colab.research.google.com/drive/16IyYGf_z5IRjcVKwxa5yiXDEMiyf0u1d?usp=sharing"&gt;https://colab.research.google.com/drive/16IyYGf_z5IRjcVKwxa5yiXDEMiyf0u1d?usp=sharing&lt;/a&gt;;&lt;/p&gt; &lt;p&gt;ü§ù &lt;strong&gt;Looking for collaborators&lt;/strong&gt;&lt;br /&gt; If you‚Äôre working on: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small language models for DevOps&lt;/li&gt; &lt;li&gt;AI agents that help engineersconnectLinkedIn&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd love to connec on Linkedin &lt;a href="https://www.linkedin.com/in/prashant-lakhera-696119b/connect"&gt;https://www.linkedin.com/in/prashant-lakhera-696119b/connect&lt;/a&gt;&lt;/p&gt; &lt;p&gt;DevOps has always been about doing more with less. Now, it‚Äôs time we had an AI that works the same way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T18:22:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndqu7q</id>
    <title>Anyone has problems with OpenWeb UI?</title>
    <updated>2025-09-10T21:28:45+00:00</updated>
    <author>
      <name>/u/StandarterSD</name>
      <uri>https://old.reddit.com/user/StandarterSD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using Open Web UI for a long time, and with each update, it becomes more and more buggy. Web Search, REG, Ask, and Question buttons stop working. In short, there are only problems. Does anyone have any alternatives that allow me to use Open AI Complatible points?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StandarterSD"&gt; /u/StandarterSD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndqu7q/anyone_has_problems_with_openweb_ui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndqu7q/anyone_has_problems_with_openweb_ui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndqu7q/anyone_has_problems_with_openweb_ui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T21:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndwos5</id>
    <title>Feedback for Local AI Platform</title>
    <updated>2025-09-11T01:58:53+00:00</updated>
    <author>
      <name>/u/ysDlexia</name>
      <uri>https://old.reddit.com/user/ysDlexia</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndwos5/feedback_for_local_ai_platform/"&gt; &lt;img alt="Feedback for Local AI Platform" src="https://b.thumbs.redditmedia.com/bNC1pdKhthoFAjPoeMvUXHwUZao8xMaPGIuuhVrmACY.jpg" title="Feedback for Local AI Platform" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y‚Äôall, I‚Äôve been hacking away at a side project for about ~2 months and it‚Äôs finally starting to look like an actual app. Figured I‚Äôd show it off and ask: is this something you‚Äôd actually want, or am I just reinventing the wheel?&lt;/p&gt; &lt;p&gt;It‚Äôs called Strata. Right now it‚Äôs just a basic inferencing system, but I‚Äôve been really careful with the architecture. It‚Äôs built with Rust + Tauri + React/Tailwind. I split out a backend abstraction layer, so down the line it‚Äôs not just tied to llama.cpp ‚Äî the idea is you could swap in GGML, Transformers, ONNX, whatever you want.&lt;/p&gt; &lt;p&gt;The bigger vision: one open-source platform where you can download models, run inference, train on your own datasets, or even build new ones. HuggingFace integration baked in so you can just pull a model and use it, no CLI wrangling.&lt;/p&gt; &lt;p&gt;Licensing will be Apache 2.0, fully open-source, zero monetization. No ‚Äúpro tier,‚Äù no gated features. Just open code.&lt;/p&gt; &lt;p&gt;I‚Äôm closing in on an MVP release, but before I go too deep I wanted to sanity check with the LocalLLaMA crowd ‚Äî would you use something like this? Any feature ideas you‚Äôd love to see in a tool like this?&lt;/p&gt; &lt;p&gt;Dropping some screenshots of the UI too (still rough around the edges, but I‚Äôm polishing).&lt;/p&gt; &lt;p&gt;Appreciate any feedback ‚Äî building this has been a blast so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ysDlexia"&gt; /u/ysDlexia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ndwos5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndwos5/feedback_for_local_ai_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndwos5/feedback_for_local_ai_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T01:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfttb</id>
    <title>New smol course on Hugging Face - Climb the leaderboard to win prizes.</title>
    <updated>2025-09-10T14:32:04+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"&gt; &lt;img alt="New smol course on Hugging Face - Climb the leaderboard to win prizes." src="https://preview.redd.it/26eruo46lcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ec83e94c4ebb6b90da2d9cafe108fafbcac73e5" title="New smol course on Hugging Face - Climb the leaderboard to win prizes." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;smol course v2 - a Direct Way to Learn Post-Training AI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Finally dropped our FREE certified course that cuts through the fluff:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's distinctive about smol course compared to other AI courses (LLM course)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Minimal instructions, maximum impact&lt;/li&gt; &lt;li&gt;Bootstrap real projects from day one&lt;/li&gt; &lt;li&gt;Leaderboard-based assessment (competitive learning FTW)&lt;/li&gt; &lt;li&gt;Hands-off approach - points you to docs instead of hand-holding&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What's specifically new in this version&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Student model submission leaderboard&lt;/li&gt; &lt;li&gt;PRIZES for top performers&lt;/li&gt; &lt;li&gt;Latest TRL &amp;amp; SmolLM3 content&lt;/li&gt; &lt;li&gt;Hub integration for training/eval via hf jobs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Chapters drop every few weeks.&lt;/p&gt; &lt;p&gt;üëâ Start here: &lt;a href="https://huggingface.co/smol-course"&gt;https://huggingface.co/smol-course&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/26eruo46lcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfttb/new_smol_course_on_hugging_face_climb_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:32:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndwwk2</id>
    <title>Is VRAM the only thing matters for secondary GPU for LLMs?</title>
    <updated>2025-09-11T02:09:30+00:00</updated>
    <author>
      <name>/u/Expression-Internal</name>
      <uri>https://old.reddit.com/user/Expression-Internal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am considering adding a secondary GPU to my 4090 and my goal is to run larger models (70b).&lt;/p&gt; &lt;p&gt;I just come across 5060ti with 16GB of VRAM which will bring the total VRAM to 40GB. will that be enough to run 70b models?&lt;/p&gt; &lt;p&gt;Is VRAM the only thing that matters for a secondary GPU as most of the calculations will be performed on the primary GPU?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expression-Internal"&gt; /u/Expression-Internal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndwwk2/is_vram_the_only_thing_matters_for_secondary_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndwwk2/is_vram_the_only_thing_matters_for_secondary_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndwwk2/is_vram_the_only_thing_matters_for_secondary_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T02:09:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndv0bf</id>
    <title>Read GGUF Quantization type from file</title>
    <updated>2025-09-11T00:35:56+00:00</updated>
    <author>
      <name>/u/Qbsoon110</name>
      <uri>https://old.reddit.com/user/Qbsoon110</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I am currently writing a hobby app and I need to read the quantization value from gguf file in python. I am currently reading parameters with GGUFReader from gguf library. There is a general.file_type parameter there, but I can't find anywhere a table that would map the integers from the values of that field to quantization types. I checked my two qwen files and Q8 was at 7 and Q5_K_M was at 17. I could download all the types and check their values, but I wonder if there's a table somewhere, or maybe I am wrong and it is not standarized? Then I wonder if it is at least standarized within model&lt;/p&gt; &lt;p&gt;I tried checking each tensor quantization, but then I can only tell that it's Q5_K, not Q5_K_M&lt;/p&gt; &lt;p&gt;Edit: When I hover over the weights in model parameters on huggingface, I see the id, so I can check each type there and map that way, but still, strange that I can't find any mapping table.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qbsoon110"&gt; /u/Qbsoon110 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndv0bf/read_gguf_quantization_type_from_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndv0bf/read_gguf_quantization_type_from_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndv0bf/read_gguf_quantization_type_from_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T00:35:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfmb0</id>
    <title>Qwen3-VL soon?</title>
    <updated>2025-09-10T14:23:47+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"&gt; &lt;img alt="Qwen3-VL soon?" src="https://external-preview.redd.it/WmIZZLYdo41uN4s96YqW_5HlL8MG-0LtKmnFoOx7RwY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fafac68575ffd3b34262cbfa9c59fc0dcef20103" title="Qwen3-VL soon?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40795"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfmb0/qwen3vl_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:23:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf3rj</id>
    <title>I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)</title>
    <updated>2025-09-10T14:04:09+00:00</updated>
    <author>
      <name>/u/WouterGlorieux</name>
      <uri>https://old.reddit.com/user/WouterGlorieux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"&gt; &lt;img alt="I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)" src="https://preview.redd.it/7yajbqkmd6of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1e29abf739772b644d059323cbc4269b2391b68" title="I built a fully automated LLM tournament system (62 models tested, 18 qualified, 50 tournaments run)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a project called Valyrian Games: a fully automated system where Large Language Models compete against each other in coding challenges. After running 50 tournaments, I‚Äôve published the first results here:&lt;/p&gt; &lt;p&gt;üëâ Leaderboard: &lt;a href="https://valyriantech.github.io/ValyrianGamesLeaderboard"&gt;https://valyriantech.github.io/ValyrianGamesLeaderboard&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üëâ Challenge data repo: &lt;a href="https://github.com/ValyrianTech/ValyrianGamesCodingChallenge"&gt;https://github.com/ValyrianTech/ValyrianGamesCodingChallenge&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;p&gt;Phase 1 doubles as qualification: each model must create its own coding challenge, then solve it multiple times to prove it‚Äôs fair. To do this, the LLM has access to an MCP server to execute Python code. The coding challenge can be anything, as long as the final answer is a single integer value (for easy verification).&lt;/p&gt; &lt;p&gt;Only models that pass this step qualify for tournaments.&lt;/p&gt; &lt;p&gt;Phase 2 is the tournament: qualified models solve each other‚Äôs challenges head-to-head. Results are scored (+1 correct, -1 wrong, +1 bonus for solving another's challenge, extra penalties if you fail your own challenge).&lt;/p&gt; &lt;p&gt;Ratings use Microsoft‚Äôs TrueSkill system, which accounts for uncertainty.&lt;/p&gt; &lt;p&gt;Some results so far:&lt;/p&gt; &lt;p&gt;I‚Äôve tested 62 models, but only 18 qualified.&lt;/p&gt; &lt;p&gt;GPT-5-mini is currently #1, but the full GPT-5 actually failed qualification.&lt;/p&gt; &lt;p&gt;Some reasoning-optimized models literally ‚Äúoverthink‚Äù until they timeout.&lt;/p&gt; &lt;p&gt;Performance is multi-dimensional: correctness, speed, and cost all vary wildly.&lt;/p&gt; &lt;p&gt;Why I built this:&lt;/p&gt; &lt;p&gt;This started as a testbed for workflows in my own project SERENDIPITY, which is built on a framework I also developed: &lt;a href="https://github.com/ValyrianTech/ValyrianSpellbook"&gt;https://github.com/ValyrianTech/ValyrianSpellbook&lt;/a&gt; . I wanted a benchmark that was open, automated, and dynamic, not just static test sets.&lt;/p&gt; &lt;p&gt;Reality check:&lt;/p&gt; &lt;p&gt;The whole system runs 100% automatically, but it‚Äôs expensive. API calls are costing me about $50/day, which is why I‚Äôve paused after 50 tournaments. I‚Äôd love to keep it running continuously, but as a solo developer with no funding, that‚Äôs not sustainable. Right now, the only support I have is a referral link to RunPod (GPU hosting).&lt;/p&gt; &lt;p&gt;I‚Äôm sharing this because:&lt;/p&gt; &lt;p&gt;I think the results are interesting and worth discussing (especially which models failed qualification).&lt;/p&gt; &lt;p&gt;I‚Äôd love feedback from this community. Does this kind of benchmarking seem useful to you?&lt;/p&gt; &lt;p&gt;If there‚Äôs interest, maybe we can find ways to keep this running long-term.&lt;/p&gt; &lt;p&gt;For those who want to follow me: &lt;a href="https://linktr.ee/ValyrianTech"&gt;https://linktr.ee/ValyrianTech&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WouterGlorieux"&gt; /u/WouterGlorieux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7yajbqkmd6of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf3rj/i_built_a_fully_automated_llm_tournament_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndxjsu</id>
    <title>Building Qwen3 from Scratch: This Is your chance</title>
    <updated>2025-09-11T02:42:17+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxjsu/building_qwen3_from_scratch_this_is_your_chance/"&gt; &lt;img alt="Building Qwen3 from Scratch: This Is your chance" src="https://external-preview.redd.it/nkhh65ujo5BznFJFojoMPaKjGuLSpPj6KGhRov-ykOg.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e2f90964c81a1de52938be6bcb08665605293f2" title="Building Qwen3 from Scratch: This Is your chance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gkhu0qvs7gof1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b09e78eafaa60e2958deb801a3f64690cc34e923"&gt;AI generated(if you are guessing ;-))&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So earlier today I shared something I‚Äôve been working on for a while: the first Small Language Model built for DevOps &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndm44z/meet_the_first_small_language_model_built_for/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A lot of people have told me they want to build their own model but don‚Äôt know where to start. The code usually looks super complex, and honestly, most give up before they even get to the fun part.&lt;/p&gt; &lt;p&gt;To make it easier, I put together a Google Colab notebook where I explained every single cell step-by-step so you can follow along without getting lost:&lt;br /&gt; &lt;a href="https://colab.research.google.com/drive/16IyYGf_z5IRjcVKwxa5yiXDEMiyf0u1d?usp=sharing"&gt;https://colab.research.google.com/drive/16IyYGf_z5IRjcVKwxa5yiXDEMiyf0u1d?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And if you‚Äôre curious about the theory behind it, I also wrote a blog here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://devopslearning.medium.com/i-built-qwen3-from-scratch-and-heres-what-i-learned-theory-0480b3171412"&gt;https://devopslearning.medium.com/i-built-qwen3-from-scratch-and-heres-what-i-learned-theory-0480b3171412&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôve been sitting on the idea of building your own model, this might be the nudge you need. Don‚Äôt worry about complexity, stay curious and keep going, and you‚Äôll go further than you imagine&lt;/p&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/ideaweaver-ai/qwen3-from-scratch"&gt;https://github.com/ideaweaver-ai/qwen3-from-scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt; If you still have questions, drop them in the linkedin. I‚Äôll be happy to help. &lt;a href="https://www.linkedin.com/in/prashant-lakhera-696119b/"&gt;https://www.linkedin.com/in/prashant-lakhera-696119b/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxjsu/building_qwen3_from_scratch_this_is_your_chance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxjsu/building_qwen3_from_scratch_this_is_your_chance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxjsu/building_qwen3_from_scratch_this_is_your_chance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T02:42:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndpfsx</id>
    <title>LLM360/K2-Think</title>
    <updated>2025-09-10T20:31:08+00:00</updated>
    <author>
      <name>/u/Pyros-SD-Models</name>
      <uri>https://old.reddit.com/user/Pyros-SD-Models</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndpfsx/llm360k2think/"&gt; &lt;img alt="LLM360/K2-Think" src="https://external-preview.redd.it/NguS7X1dxgvLZ8EclNqhJxD0a-4fPSDfz1-q527PukQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f822a6c75b113bb5f07dc8583dea6f31081a289" title="LLM360/K2-Think" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pyros-SD-Models"&gt; /u/Pyros-SD-Models &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndpfsx/llm360k2think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndpfsx/llm360k2think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T20:31:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nduo33</id>
    <title>$142 upgrade kit and spare modules turn Nvidia RTX 4090 24GB to 48GB AI card</title>
    <updated>2025-09-11T00:19:14+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The upgrade kit comprises a custom PCB designed with a clamshell configuration, facilitating the installation of twice the number of memory chips. Most components are pre-installed at the manufacturing facility, requiring the user to solder the GPU and memory chips onto the PCB. Additionally, the upgrade kit includes a blower-style cooling solution, designed for integration with workstation and server configurations that utilize multi-GPU architectures.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/usd142-upgrade-kit-and-spare-modules-turn-nvidia-rtx-4090-24gb-to-48gb-ai-card-technician-explains-how-chinese-factories-turn-gaming-flagships-into-highly-desirable-ai-gpus"&gt;https://www.tomshardware.com/pc-components/gpus/usd142-upgrade-kit-and-spare-modules-turn-nvidia-rtx-4090-24gb-to-48gb-ai-card-technician-explains-how-chinese-factories-turn-gaming-flagships-into-highly-desirable-ai-gpus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nduo33/142_upgrade_kit_and_spare_modules_turn_nvidia_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nduo33/142_upgrade_kit_and_spare_modules_turn_nvidia_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nduo33/142_upgrade_kit_and_spare_modules_turn_nvidia_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T00:19:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxx7</id>
    <title>Qwen vl</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"&gt; &lt;img alt="Qwen vl" src="https://preview.redd.it/il757v4emcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0bd8c776140638bad168e112aeaf64c8186d548" title="Qwen vl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/il757v4emcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxx7/qwen_vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjoek</id>
    <title>New Ernie X1.1 - what may be the best Chinese model since DeepSeek V3.1 slowly approaches the frontier (or a simple test that exposes so many models)</title>
    <updated>2025-09-10T16:53:13+00:00</updated>
    <author>
      <name>/u/Massive-Shift6641</name>
      <uri>https://old.reddit.com/user/Massive-Shift6641</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjoek/new_ernie_x11_what_may_be_the_best_chinese_model/"&gt; &lt;img alt="New Ernie X1.1 - what may be the best Chinese model since DeepSeek V3.1 slowly approaches the frontier (or a simple test that exposes so many models)" src="https://a.thumbs.redditmedia.com/_ffN_nktG0-4jk3SnpSPHMoy10qzCqDyJ80DzDgiM98.jpg" title="New Ernie X1.1 - what may be the best Chinese model since DeepSeek V3.1 slowly approaches the frontier (or a simple test that exposes so many models)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baidu, the Chinese Google, recently released a couple of new models - an update to open source Ernie 4.5 and proprietary Ernie X1.1:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vn2uss2lqcof1.png?width=526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdb82906481b94175f14970b305fe9f474cd8113"&gt;https://preview.redd.it/vn2uss2lqcof1.png?width=526&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bdb82906481b94175f14970b305fe9f474cd8113&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As usual, I found the &amp;quot;on par with GPT-5 and Gemini 2.5 Pro&amp;quot; claims quite bold and decided to check it out. It turns out that, while these claims are obviously overstated, it is not a bad model - in fact, it demonstrates the first real observable improvement since the release of DeepSeek V3.1.&lt;/p&gt; &lt;h1&gt;The test&lt;/h1&gt; &lt;p&gt;I love torturing models with music theory problems. I see a good reason why it may be a good proxy for the models' general ability, if not among the best measurements ever - it tests mostly the LLMs' reasoning ability rather than just knowledge.&lt;br /&gt; &lt;strong&gt;Music theory is not a big subject&lt;/strong&gt; - there is an infinite number of songs that can be written, but the entire music theory is quite compact. It makes it easy to fit it into a LLM and write evals that test their reasoning and comprehension skills rather than just knowledge.&lt;br /&gt; &lt;strong&gt;Most music theory knowledge online is never explored in-depth&lt;/strong&gt; - even most musicians' don't know anything besides basic major and minor chords and their progressions. Since most pretraining data is not particularly high quality, LLMs have to reason to analyze music that is more complex than popular.&lt;br /&gt; &lt;strong&gt;Music theory evals can easily be rewritten and updated if benchmaxxxed and overfit&lt;/strong&gt; - it may take days to even create a programming or math problem that is enough challenging for modern LLMs, but only a few hours to create a song that is beyond most models' ability to understand. (I'm not totally sure about this one)&lt;/p&gt; &lt;p&gt;So I wrote the following:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gn5ejxifscof1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd211db5071c357e98eb670a2474fef3add94946"&gt;https://preview.redd.it/gn5ejxifscof1.png?width=1727&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd211db5071c357e98eb670a2474fef3add94946&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This piece is special because it is written in Locrian. It is rarely used in popular music because of its inherent tension and lack of resolution (look up John Kirkpatrick's Dust to Dust), and since it is so rare, it makes it a perfect candidate to test the LLMs reasoning ability.&lt;/p&gt; &lt;p&gt;In this track, the signature Locrian sound is created with:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a dissonant diminished triad is outlined with the C-Eb-Gb ostinato at the organ 2 line;&lt;/li&gt; &lt;li&gt;The Gb bassline - a point of relative stability that gives an illusion of a tonal center.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Basically, it is Locrian with a twist - while the actual tonal center is on C, the Gb bass drone sounds more stable than C (where it occasionally plays), so it is easy to misinterpret Gb as tonic simply because it is the most stable note here.&lt;/p&gt; &lt;p&gt;Now let's see what our models think about it.&lt;/p&gt; &lt;h1&gt;The prompt&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Comprehensive analysis of the following composition. Determine the mood, the key, the mode, the meter, the likely tempo and genre. Any modal interchanges? Chromaticism? What do you think about this in general? &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Organ : (C5*1/2. C5*1/4. C5*1/4 Db5*1/4 Db5*1/4. Db5*1/4. Eb5*1/4 Eb5*1/2 C5*1/4. Bb4*1/4. Ab4*1/2. Eb5*1/4. Db5*1/4.)*4&lt;br /&gt; Brass : (~*1/2.)*16 ((C4*1/2.)*2 (Db4*1/2.)*2 (Gb4*1/2.)*4)*2&lt;br /&gt; Snare : (~*1/4 x*1/4 ~*1/4 x*1/4 ~*1/2 ~*1/2 x*1/4 ~*1/2. ~*1/4 x*1/4 ~*1/4 x*1/4 ~*1/4 x*1/4 ~*1/2. ~*1/2.)*4&lt;br /&gt; Kick : (x*1/4 ~*1/2 ~*1/4 x*1/4 ~*1/4 x*1/4 x*1/4 ~*1/4 x*1/4 ~*1/2 x*1/4 ~*1/2 ~*1/4 x*1/4 ~*1/4 x*1/4 ~*1/2 ~*1/2.)*4&lt;br /&gt; Hi Hat : ((x*1/16)*20 5[(x*1/16)*5] (x*1/16)*16 5[(x*1/16)*10] 1/16*36 5[(x*1/16)*15])*4&lt;br /&gt; Bass : (Gb1*1/2.+Gb1*1/4 Eb1*1/2 Gb1*1/4 Gb1*1/2 Bb1*1/2. Gb1*1/2.+Gb1*1/4 C1*1/2+C1*1/2.+C1*1/2.)*4&lt;br /&gt; Choir : (C5*1/8 Eb5*1/8 Gb5*1/8 Eb5*1/8 Eb5*1/8 Db5*1/8 Eb5*1/2. C5*1/8 Eb5*1/8 Ab5*1/8 Gb5*1/8 Gb5*1/8 F5*/18 Gb5*1/2. C5*1/8 Eb5*1/8 Gb5*1/8 Eb5*1/8 Eb5*1/8 Db5*1/8 Eb5*1/2. Ab4*1/8 Db5*1/8 F5*1/8 Db5*1/8 Db5*1/8 C5*1/8 Db5*1/2.)*4&lt;br /&gt; Organ 2 : (C3*1/8 Eb3*1/8 Gb3*1/8)*64 &lt;/p&gt; &lt;p&gt;Legend:&lt;br /&gt; C5*1/2.+1/2 ~*1/4&lt;br /&gt; 5[(x*1/4)*6]&lt;br /&gt; C - Note label&lt;br /&gt; 5 - Octave number&lt;br /&gt; *1/2 - duration&lt;br /&gt; . - dotted note&lt;br /&gt; + - tied notes&lt;br /&gt; ~ - rest&lt;br /&gt; x - drum note&lt;br /&gt; 5[] - pentuple&lt;/p&gt; &lt;p&gt;You can try it on LM Arena.&lt;/p&gt; &lt;h1&gt;What frontier models hear&lt;/h1&gt; &lt;p&gt;I was surprised to see how often models fail to &amp;quot;hear&amp;quot; the Locrian mode (my previous task with Lydian was far simpler for them). Here's what they say:&lt;/p&gt; &lt;p&gt;Gemini 2.5 Pro: Gb Lydian (3/5), Ab Minor (1/5), C Locrian (1/5)&lt;br /&gt; Grok 4: C Locrian (4/5), C Diminished (1/5)&lt;br /&gt; GPT 5 High: C Locrian (5/5)&lt;br /&gt; Opus 4.1 Thinking: C Phrygian Dominant (1/5), Eb Dorian (1/5), Eb Minor (1/5), C Phrygian (1/5), C Locrian (1/5)&lt;/p&gt; &lt;p&gt;As expected from GPT 5 the GOAT (as of September 10, 2025), it identified the key and mode correctly in all cases.&lt;br /&gt; Grok 4 was a very close contender (C Diminished scale is very similar to the Locrian one).&lt;br /&gt; Gemini's performance was surprising - it took the fake tonal center bait and mistakenly believed that the song was in Gb Lydian, a mode parallel to C Locrian.&lt;br /&gt; Opus was shocking - it managed to miss both the tonal centre and the mode multiple times. They've probably castrated it down to 1.58 bits again, I don't know.&lt;/p&gt; &lt;p&gt;Besides this, all models correctly identify the mood as &amp;quot;tense&amp;quot;, &amp;quot;ominous&amp;quot;, &amp;quot;dramatic&amp;quot; and &amp;quot;epic&amp;quot;. Except for Opus that in one case called it &amp;quot;melancholic&amp;quot; and &amp;quot;uplifting&amp;quot;. Claude is stoopid.&lt;/p&gt; &lt;h1&gt;Ernie X1.1 - the menace&lt;/h1&gt; &lt;p&gt;Now let's look at typical responses by Ernie X1.1:&lt;/p&gt; &lt;p&gt;&amp;quot;Frequent borrowing from C minor (Eb, Gb), Phrygian (Db), and blues scales (Ab, Bb). The brass‚Äôs Gb4 (tritone from C) and choir‚Äôs F5*/18 (microtonal inflection) heighten tension&amp;quot;&lt;br /&gt; &amp;quot;C Phrygian with parallel Gb major&amp;quot;&lt;br /&gt; &amp;quot;Mixes C natural minor (C-D-Eb-F-G-Ab-Bb) with C blues scale (C-Eb-F-Gb-G-Bb) and C Phrygian (C-Db-Eb-F-G-Ab-Bb) via Db/Gb usage.&amp;quot;&lt;br /&gt; &amp;quot;Primarily C minor (evidenced by C5, Eb5, Gb5, Bb4, Ab4 in Organ/Choir/Bass).&amp;quot;&lt;br /&gt; &amp;quot;G‚ô≠ Major (evident in the choir‚Äôs G‚ô≠5-C5-E‚ô≠5 triads and Organ 2‚Äôs G‚ô≠3-E‚ô≠3-C3 progression).&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p5rszq9d9dof1.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fddce7545a1eb375dc4261072e1da48c4c1626af"&gt;https://preview.redd.it/p5rszq9d9dof1.png?width=793&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fddce7545a1eb375dc4261072e1da48c4c1626af&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can notice that, while it is still not quite there, it is now very close. It either correctly identifies the tonal center or gets very close to identify the Locrian mode. Even when it gets tricked with the fake Gb tonal center, it, at least, tries to overanalyze as less as possible to stay as close to the key of Gb major as possible.&lt;/p&gt; &lt;p&gt;Overall, X1.1's performance is very impressive - so far, the best one among all Chinese models I tested. I did not expect it to land somewhere between Gemini and Opus!&lt;/p&gt; &lt;h1&gt;Where Ernie is better than other Chinese models&lt;/h1&gt; &lt;p&gt;Qwen's performance on this task is comparable to that of Opus. Sometimes it finds the correct key and mode, but it feels like it is mostly by accident, and it also hallucinates a lot and unnecessary overcomplicates everything.&lt;/p&gt; &lt;p&gt;DeepSeek is a bit better, but not much when compared to Ernie X1.1.&lt;/p&gt; &lt;h1&gt;Implications&lt;/h1&gt; &lt;p&gt;Apparently, there is another Chinese model that is better than all previous ones. However, nobody seems to talk about it, which is disappointing. Most people won't care about any improvement until it is significant enough to give the US stock market a heart attack, and this fact has some implications for LLM devs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;No matter how brilliant your innovations are, if you can't demonstrate an improvement that disrupts the whole industry, very few people will care about you, including other researchers;&lt;/li&gt; &lt;li&gt;You should always follow for updates of other notable models and evaluate them independently, and if they really made something better, learn from them - not only to maintain the competitive edge, but also because otherwise their innovations may simply be left unnoticed;&lt;/li&gt; &lt;li&gt;Minor releases are for small cumulative updates, major ones are for models that advance the frontier and crash the US stock market&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And for users:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;You don't necessarily need expensive and extensive benchmarks to evaluate the general intelligence and reasoning abilities of models, sometimes it is enough to ask just a couple of short low-knowledge, high-reasoning questions to see which of them perform better than others;&lt;/li&gt; &lt;li&gt;The gap between the frontier and Chinese models is slowly narrowing, and since DeepSeek has definitely produced even more research since R1, we have a very good chance to see an open source Chinese equivalent of GPT-5 or at least Grok 4 by the end of this year already.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Massive-Shift6641"&gt; /u/Massive-Shift6641 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjoek/new_ernie_x11_what_may_be_the_best_chinese_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjoek/new_ernie_x11_what_may_be_the_best_chinese_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjoek/new_ernie_x11_what_may_be_the_best_chinese_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:53:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd7nxo</id>
    <title>VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!</title>
    <updated>2025-09-10T07:24:32+00:00</updated>
    <author>
      <name>/u/Cipher_Lock_20</name>
      <uri>https://old.reddit.com/user/Cipher_Lock_20</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt; &lt;img alt="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" src="https://external-preview.redd.it/ZXJidjUwNHJnYW9mMTZtREgbQQjA1lJ8zPSNZtqKO6Gf9AtInhXi-M401FlP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051bf77ffd8780ab4b7ffcc3dc7c1b3bc71a8875" title="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a huge AI audio nerd, I've recently been knee-deep in Microsoft's latest VibeVoice models and they really are awesome!! The work from the Microsoft Research team is amazing and they've shared them with everyone.... even though they took one back lol. I highly recommend checking them out if you haven't already.&lt;/p&gt; &lt;p&gt;I started reading up on all of the techniques applied within the architecture to allow for such long generations (45-90 minutes), with up to 4 speakers, and sounding so life-like... Google notebook is the closest thing to this kind of generation, but it's limited in that it auto-generates your podcast based on the context, not on the exact script you provide.&lt;/p&gt; &lt;p&gt;Let me have the VibeVoice model do the talking!&lt;/p&gt; &lt;p&gt;The generated voices in my video were generated within my own Hugging Face space and using the default voices provided by the VibeVoice model (7B). The voices were generated in one single generation, not stitched! &lt;a href="https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice"&gt;https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cipher_Lock_20"&gt; /u/Cipher_Lock_20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x4dht8pgaof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T07:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndx2tq</id>
    <title>GPT-OSS 120B on CPU is 50% faster with IQ4_NL</title>
    <updated>2025-09-11T02:18:26+00:00</updated>
    <author>
      <name>/u/dreamkast06</name>
      <uri>https://old.reddit.com/user/dreamkast06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hoping anyone else might be able to verify. Most quants for gpt-oss stick with the native MXFP4 because nothing else works...except for IQ4_NL/Q5_1.&lt;/p&gt; &lt;p&gt;IQ4_NL can be CPU repacked, so I'm curious if anyone else is running it that way. I've got two different machines that I've run it on and both go from about 9-10 tps to 14-16 tps with minor improvements in pp using either vanilla lcp and ik_llama&lt;/p&gt; &lt;p&gt;I didn't notice any drop in output quality from my limited testing, so I'm wondering if anyone else is using these quants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamkast06"&gt; /u/dreamkast06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndx2tq/gptoss_120b_on_cpu_is_50_faster_with_iq4_nl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndx2tq/gptoss_120b_on_cpu_is_50_faster_with_iq4_nl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndx2tq/gptoss_120b_on_cpu_is_50_faster_with_iq4_nl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T02:18:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndc7z8</id>
    <title>I pre-trained GPT-OSS entirely from scratch</title>
    <updated>2025-09-10T12:00:25+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt; &lt;img alt="I pre-trained GPT-OSS entirely from scratch" src="https://external-preview.redd.it/9EZFRbCI06NQd5IaAcswlKJEIIkgLbOtsjD1e7w98EI.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85d2bae87aa536469b4b4fbaafbfa3ee215b6f78" title="I pre-trained GPT-OSS entirely from scratch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/fo9rnnpeubof1.png?width=2562&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=562a72f12d23083851c3775e1540b7f111ffda57"&gt;https://preview.redd.it/fo9rnnpeubof1.png?width=2562&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=562a72f12d23083851c3775e1540b7f111ffda57&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I recorded a 3 hour video to show how we built GPT-OSS from scratch. &lt;/p&gt; &lt;p&gt;You can watch the video here: &lt;a href="https://youtu.be/hBUsySdcA3I"&gt;https://youtu.be/hBUsySdcA3I&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The video contains the following 8 steps:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;(1) Tiny Stories: Data Preprocessing&lt;/p&gt; &lt;p&gt;(2) GPT-OSS Harmony Tokenizer to tokenize the data&lt;/p&gt; &lt;p&gt;(3) Architecture Part 1: Token embeddings, RMSNorm and Rotary Positional Encoding (RoPE)&lt;/p&gt; &lt;p&gt;(4) Architecture Part 2: Sliding attention layers and Grouped Query Attention (GQA)&lt;/p&gt; &lt;p&gt;(5) Architecture Part 3: Attention Bias and Attention Sinks&lt;/p&gt; &lt;p&gt;(6) Architecture Part 4: SwiGLU Mixture of Experts (MoE) &lt;/p&gt; &lt;p&gt;(7) GPT-OSS Pre-training loop&lt;/p&gt; &lt;p&gt;(8) GPT-OSS Inference&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some info:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;We have now released two versions of our codebase publicly. Both are under active work: &lt;/p&gt; &lt;p&gt;(1) Nano-GPT-OSS: &lt;a href="https://github.com/VizuaraAI/nano-gpt-oss"&gt;https://github.com/VizuaraAI/nano-gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- A 500 million parameter model which retains all the key architectural innovations of GPT-OSS. &lt;/p&gt; &lt;p&gt;- Requires 20 hours of training on 1 A40 GPU (0.4$/hr). Can be replicated under 10$. &lt;/p&gt; &lt;p&gt;(2) Truly-Open-GPT-OSS: &lt;a href="https://github.com/VizuaraAI/truly-open-gpt-oss"&gt;https://github.com/VizuaraAI/truly-open-gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- A 20B parameter model which we pre-trained fully from scratch. &lt;/p&gt; &lt;p&gt;- Requires 5 H200 GPUs. Budget needed for this would be 100-150$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndc7z8/i_pretrained_gptoss_entirely_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T12:00:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndz1k4</id>
    <title>PNY preorder listing shows Nvidia DGX Spark at $4,299.99</title>
    <updated>2025-09-11T04:00:41+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;PNY has opened preorders for the Nvidia DGX Spark, a compact desktop AI system powered by the Grace Blackwell GB10 Superchip. It combines Arm Cortex-X925 and Cortex-A725 CPU cores with a Blackwell GPU, delivering up to 1,000 AI TOPS, or 1 petaFLOP of FP4 performance, for local model inference and fine-tuning.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linuxgizmos.com/pny-preorder-listing-shows-nvidia-dgx-spark-at-4299-99/"&gt;https://linuxgizmos.com/pny-preorder-listing-shows-nvidia-dgx-spark-at-4299-99/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T04:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndit0a</id>
    <title>16‚Üí31 Tok/Sec on GPT OSS 120B</title>
    <updated>2025-09-10T16:21:54+00:00</updated>
    <author>
      <name>/u/3VITAERC</name>
      <uri>https://old.reddit.com/user/3VITAERC</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;16 tok/sec&lt;/strong&gt; with LM Studio ‚Üí &lt;strong&gt;~24 tok/sec&lt;/strong&gt; by switching to llama.cpp ‚Üí &lt;strong&gt;~31 tok/sec&lt;/strong&gt; upgrading RAM to DDR5&lt;/p&gt; &lt;h1&gt;PC Specs&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Intel 13600k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; NVIDIA RTX 5090&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Old RAM:&lt;/strong&gt; DDR4-3600MHz - 64gb&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New RAM:&lt;/strong&gt; DDR5-6000MHz - 96gb&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; unsloth gpt-oss-120b-F16.gguf - &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF"&gt;hf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;From LM Studio to Llama.cpp (16‚Üí24 tok/sec)&lt;/h1&gt; &lt;p&gt;I started out using LM Studio and was getting a respectable 16 tok/sec. But I kept seeing people talk about llama.cpp speeds and decided to dive in. Its definitely worth doing as the &lt;code&gt;--n-cpu-moe&lt;/code&gt; flag is super powerful for MOE models.&lt;/p&gt; &lt;p&gt;I experimented with a few values for --n-cpu-moe and found that 22 + 48k context window filled up my 32gb of vram. I could go as high as --n-cpu-moe 20 if I lower the context to 3.5k.&lt;/p&gt; &lt;p&gt;For reference, this is the command that got me the best performance llamacpp:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --n-gpu-layers 999 --n-cpu-moe 22 --flash-attn on --ctx-size 48768 --jinja --reasoning-format auto -m C:\Users\Path\To\models\unsloth\gpt-oss-120b-F16\gpt-oss-120b-F16.gguf --host 0.0.0.0 --port 6969 --api-key &amp;quot;redacted&amp;quot; --temp 1.0 --top-p 1.0 --min-p 0.005 --top-k 100 --threads 8 -ub 2048 -b 2048 &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;DDR4 to DDR5 (24‚Üí31 tok/sec)&lt;/h1&gt; &lt;p&gt;While 24 t/s was a great improvement, I had a hunch that my DDR4-3600 RAM was a big bottleneck. After upgrading to a DDR5-6000 kit, my assumption proved correct.&lt;/p&gt; &lt;p&gt;with &lt;strong&gt;200&lt;/strong&gt; &lt;strong&gt;input&lt;/strong&gt; &lt;strong&gt;tokens&lt;/strong&gt;, still getting ~&lt;strong&gt;32 tok/sec output&lt;/strong&gt; and &lt;strong&gt;109 tok/sec for prompt eval&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 2072.97 ms / 227 tokens ( 9.13 ms per token, 109.50 tokens per second) eval time = 4282.06 ms / 138 tokens ( 31.03 ms per token, 32.23 tokens per second) total time = 6355.02 ms / 365 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;with &lt;strong&gt;18.4k&lt;/strong&gt; &lt;strong&gt;input&lt;/strong&gt; &lt;strong&gt;tokens&lt;/strong&gt;, still getting ~&lt;strong&gt;28 tok/sec output&lt;/strong&gt; and &lt;strong&gt;863 tok/sec for prompt eval&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 21374.66 ms / 18456 tokens ( 1.16 ms per token, 863.45 tokens per second) eval time = 13109.50 ms / 368 tokens ( 35.62 ms per token, 28.07 tokens per second) total time = 34484.16 ms / 18824 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The prompt eval time was something I wasn't keeping as careful note of for DDR4 and LM studio testing, so I don't have comparisons...&lt;/p&gt; &lt;h1&gt;Thoughts on GPT-OSS-120b&lt;/h1&gt; &lt;p&gt;I'm not the biggest fan of Sam Altman or OpenAI in general. However, I have to give credit where it's due‚Äîthis model is quite good. For my use case, the gpt-oss-120b model hits the sweet spot between size, quality, and speed. I've ditched Qwen3-30b thinking and GPT-OSS-120b is currently my daily driver. Really looking forward to when Qwen has a similar sized moe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3VITAERC"&gt; /u/3VITAERC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndit0a/1631_toksec_on_gpt_oss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:21:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxxi</id>
    <title>üò≥ umm</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt; &lt;img alt="üò≥ umm" src="https://preview.redd.it/80dp7ukemcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8ce888fab8e72337bb19e61f35d929aeac11346" title="üò≥ umm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80dp7ukemcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndkbqa</id>
    <title>Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!</title>
    <updated>2025-09-10T17:16:51+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"&gt; &lt;img alt="Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!" src="https://b.thumbs.redditmedia.com/NeXv2DbpzD5M1zoK-bzUN-xEpx9SrWSRIKTvupwEMms.jpg" title="Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;https://github.com/lechmazur/writing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi K2-0905&lt;/p&gt; &lt;p&gt;1) Executive profile&lt;/p&gt; &lt;p&gt;Kimi K2-0905‚Äôs throughline is a disciplined, accumulative drive: single-POV Track A is the default, with occasional, well-taught Track B mosaics and rare but coherent Track C forays. The work reliably maintains a coherent lens to closure, which typically lands on the page with a reweighted meaning and a visible cost. Across Q1‚ÄìQ8, strengths cluster around embodied interiority, pattern-driven escalation, environment as constraint, and closure that reconfigures stakes rather than tying a bow. Reader impact: clarity is high after early orientation, momentum is built through motif and micro-choices rather than twists, felt cost is usually legible in the final image or action, and resonance rides image and implication rather than thesis.&lt;/p&gt; &lt;p&gt;Limitations are consistent but minor: occasional drift into abstraction or therapy/clinical diction at peak beats; a small tendency toward conceptual (vs. visceral) cost; mid-arc plateaus where accumulative texture stalls without a tightening beat; and rare line-level artifacts (metaphoric stacking, template cadence, or truncated last lines) that shave the edge off closure. When the model holds its voice under pressure and lets setting constrain tactics, it produces publishable endings with durable emotional aftermath. When reflection crowds micro-choices or diction rises above POV, momentum blurs and endings soften.&lt;/p&gt; &lt;p&gt;2) Portfolio map&lt;/p&gt; &lt;p&gt;Q1 Character ‚Äî Strong ¬∑ Embodied interiority, pressured micro-choices, earned-cost closure &lt;/p&gt; &lt;p&gt;Q2 Plot/Causality ‚Äî Strong ¬∑ Patterned escalation; RR/CR closures with on-page price &lt;/p&gt; &lt;p&gt;Q3 Setting ‚Äî Strong ¬∑ Environment actively constrains tactics; charged objects drive turns &lt;/p&gt; &lt;p&gt;Q4 Conflict/Stakes ‚Äî Strong ¬∑ Agency-driven narrowing; cost generally visible at climax &lt;/p&gt; &lt;p&gt;Q5 Theme/Subtext ‚Äî Strong ¬∑ Image-led emergence; ambiguity held without moralizing &lt;/p&gt; &lt;p&gt;Q6 Voice/POV ‚Äî Strong ¬∑ Distinct perceptual filter; steady distance; taught lyric moves &lt;/p&gt; &lt;p&gt;Q7 Prose/Line-level ‚Äî Strong ¬∑ Dense, rhythmic sentences doing multiple narrative jobs &lt;/p&gt; &lt;p&gt;Q8 Originality/Ingenuity ‚Äî Strong ¬∑ Non-obvious synthesis with conceptual integrity and cost&lt;/p&gt; &lt;p&gt;3) Signature moves&lt;/p&gt; &lt;p&gt;- Pattern-driven accumulation that teaches its music early, then pivots to a charged, on-page reweighting at closure.&lt;/p&gt; &lt;p&gt;- Environment-as-constraint: micro-objects and spaces (valves, vials, bells, domes) shape tactics and the final image.&lt;/p&gt; &lt;p&gt;- Embodied contradiction under pressure; micro-choices reveal values and foreclose paths with visible price.&lt;/p&gt; &lt;p&gt;- Distinct perceptual signatures and adaptive rhythm; syntax tightens at crisis without losing the taught lens.&lt;/p&gt; &lt;p&gt;- Image-born theme: recurring objects return transformed, inviting reflection without thesis.&lt;/p&gt; &lt;p&gt;- Micro-quotes that typify sensory bias and voice: ‚Äúair so cold it rang‚Äù; ‚Äúcolumn of chased stillness‚Äù; ‚Äúclay remembers.‚Äù&lt;/p&gt; &lt;p&gt;4) Failure modes&lt;/p&gt; &lt;p&gt;- Abstraction at peak beats: therapy/academic diction or lyric generalities replace embodied response, especially near closure.&lt;/p&gt; &lt;p&gt;- Conceptual cost over visceral proof: endings declare or imply loss without a concrete, on-page price paid.&lt;/p&gt; &lt;p&gt;- Escalation plateaus: accumulative texture drifts without a mid-arc tightening beat that narrows options.&lt;/p&gt; &lt;p&gt;- Line-level artifacts in the final third: metaphoric stacking, paraphrase loops, or template cadence touching closure.&lt;/p&gt; &lt;p&gt;- Orientation lag beyond ~120 words in dense openings, creating early clarity debt before the pattern is taught.&lt;/p&gt; &lt;p&gt;- Track-test stumbles (rare): untaught segmentation in mosaic pieces or abrupt, truncated last lines that blunt closure.&lt;/p&gt; &lt;p&gt;5) When it shines / when it breaks&lt;/p&gt; &lt;p&gt;Shines when the story starts with clear stakes, anchors who/where early, and lets setting, tool, and body constrain tactics as motifs accrue. A single, pressured stake deepens via protagonist-authored choices; voice stays POV-faithful as syntax tightens; the final image/action reweights prior details with legible cost. In this mode, the reader experiences clean momentum and lasting resonance.&lt;/p&gt; &lt;p&gt;Breaks when lyricism outruns pressure. If mid-arc lacks a narrowing beat, or the climax leans on conceptual summary, coincidence, or safe comfort, momentum softens. Register drift (‚Äúacademic or clinical diction during high-pressure beats‚Äù) and metaphoric pileups in closing paragraphs reduce clarity and felt cost, leaving endings more suggestive than earned.&lt;/p&gt; &lt;p&gt;6) Keep vs. adjust&lt;/p&gt; &lt;p&gt;‚Ä¢ Keep:&lt;/p&gt; &lt;p&gt;- Sensory-driven, POV-biased noticing that fuses action, setting, and emotion in multi-job sentences.&lt;/p&gt; &lt;p&gt;- Pattern-taught lyric compression and motif returns that pay off as reconfiguration at closure.&lt;/p&gt; &lt;p&gt;- Environment as active constraint‚Äîcharged objects and spatial limits that shape tactics and price.&lt;/p&gt; &lt;p&gt;‚Ä¢ Adjust:&lt;/p&gt; &lt;p&gt;- At the midpoint, add one deliberate tightening beat that forces a trade-off (lost time/object/ally) to prevent plateau.&lt;/p&gt; &lt;p&gt;- Audit peak beats for register drift and filter clusters; replace with concrete, in-scene acts that prove awareness and cost.&lt;/p&gt; &lt;p&gt;- Trim metaphoric stacking and template cadence in the final third; finish closure lines cleanly to crystallize price.&lt;/p&gt; &lt;p&gt;Overall, Kimi K2-0905 delivers consistent, high-level literary performance under Default Track A, with credible ventures into B/C when taught. Strengths‚Äîembodied interiority, patterned escalation, constraint-led setting, and closure with cost‚Äîtranslate to clear, propulsive reading experiences with durable thematic afterglow. Vigilance around abstraction at heat, mid-arc tightening, and artifact-free endings will convert strong outcomes into consistently exceptional ones.&lt;/p&gt; &lt;p&gt;Top 3 individual stories (all graders):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_63.txt"&gt;story_wc_63.txt&lt;/a&gt; by Kimi K2‚Äë0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.23 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.82 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: precise local clock tower winder&lt;/li&gt; &lt;li&gt;Object: clock tower pendulum bob&lt;/li&gt; &lt;li&gt;Core Concept: incremental absolution&lt;/li&gt; &lt;li&gt;Attribute: ethically diligent&lt;/li&gt; &lt;li&gt;Action: emerge&lt;/li&gt; &lt;li&gt;Method: through tiny inscriptions carved along a broken rake handle&lt;/li&gt; &lt;li&gt;Setting: tidal obsidian ridge&lt;/li&gt; &lt;li&gt;Timeframe: during the pause in a pendulum's swing&lt;/li&gt; &lt;li&gt;Motivation: to restore shared balance&lt;/li&gt; &lt;li&gt;Tone: searing reverie&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_346.txt"&gt;story_wc_346.txt&lt;/a&gt; by Kimi K2‚Äë0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.09 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.71 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: doomsday clock adjuster&lt;/li&gt; &lt;li&gt;Object: broken puppet head&lt;/li&gt; &lt;li&gt;Core Concept: a pane of hush&lt;/li&gt; &lt;li&gt;Attribute: beautifully flawed&lt;/li&gt; &lt;li&gt;Action: vouchsafe&lt;/li&gt; &lt;li&gt;Method: through nested patterns&lt;/li&gt; &lt;li&gt;Setting: hidden lighthouse at dusk&lt;/li&gt; &lt;li&gt;Timeframe: across the hush of time‚Äôs final ripple&lt;/li&gt; &lt;li&gt;Motivation: to whisper a lullaby across a thousand lifetimes&lt;/li&gt; &lt;li&gt;Tone: bruised awe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_79.txt"&gt;story_wc_79.txt&lt;/a&gt; by Kimi K2‚Äë0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.39 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.63 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: spiral-shell cartographer&lt;/li&gt; &lt;li&gt;Object: reed whistle&lt;/li&gt; &lt;li&gt;Core Concept: lost expedition&lt;/li&gt; &lt;li&gt;Attribute: quietly driven&lt;/li&gt; &lt;li&gt;Action: crack&lt;/li&gt; &lt;li&gt;Method: through pattern languages&lt;/li&gt; &lt;li&gt;Setting: city built on the shells of gargantuan turtles&lt;/li&gt; &lt;li&gt;Timeframe: after the gate rusts shut&lt;/li&gt; &lt;li&gt;Motivation: to question the silent watchers on the horizon&lt;/li&gt; &lt;li&gt;Tone: sunwashed dread&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;LLM Creative Story‚ÄëWriting Benchmark V3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Required elements pipeline:&lt;/strong&gt; moved from fewer, randomly selected elements (no &amp;quot;None&amp;quot; allowed) to a curated, ten‚Äëcategory catalog with large, diverse pools and an LLM proposer‚Üírater selection process; at most one category may be explicitly set to &lt;strong&gt;None&lt;/strong&gt; when that improves coherence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rubric expansion:&lt;/strong&gt; grew from 7 craft items to an &lt;strong&gt;18‚Äëquestion rubric&lt;/strong&gt; (8 craft + 10 element‚Äëfit), with clearer, more granular definitions; Q7 and Q8 now separate voice/POV from prose quality.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story length:&lt;/strong&gt; increased from 400‚Äì500 words to a strict &lt;strong&gt;600‚Äì800&lt;/strong&gt; window with upfront enforcement and compliance dashboards. Enforcement is applied at prompt level and in pre‚Äëgrading extraction, with compliance dashboards and optional cleanup tools; it is not a hard inclusion gate during aggregation unless you apply the cleanup step.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aggregation change:&lt;/strong&gt; replaced simple averages with a &lt;strong&gt;power mean (H√∂lder mean, p = 0.5)&lt;/strong&gt; and 60/40 weighting (Q1‚ÄìQ8 vs. 9A‚Äì9J) to reward balanced performance and penalize weak dimensions more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grader refresh:&lt;/strong&gt; upgraded the grader set‚Äîpreviously: GPT‚Äë4o Mar 2025, Claude 3.7 Sonnet, Llama 4 Maverick, DeepSeek V3‚Äë0324, Grok 3 Beta (no reasoning), Gemini 2.5 Pro Exp, Qwen 3 235B; now: Claude Opus 4.1 (no reasoning), DeepSeek V3.1 Reasoner, Gemini 2.5 Pro, GPT‚Äë5 (low reasoning), Grok 4, Kimi K2, Qwen 3 235B A22B 25‚Äë07 Think.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model set additions:&lt;/strong&gt; added Kimi K2‚Äë0905, Qwen 3 Max Preview, Mistral Medium 3.1, Claude Opus 4.1 (no reasoning), DeepSeek V3.1 Reasoner, and DeepSeek V3.1 Non‚ÄëThink to the evaluated models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New analyses:&lt;/strong&gt; added head‚Äëto‚Äëhead A‚Äëvs‚ÄëB comparisons, model‚Äëlevel style summaries, and intra‚Äëmodel style diversity analysis (previously none).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agreement views:&lt;/strong&gt; expanded beyond only grader‚Äëgrader correlations to include Grader√óLLM mean and normalized matrices, story‚Äëlevel disagreement tables, and leave‚Äëone‚Äëgrader‚Äëout robustness checks.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ndkbqa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndibn1</id>
    <title>Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks</title>
    <updated>2025-09-10T16:04:27+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt; &lt;img alt="Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks" src="https://preview.redd.it/ewtq2ax40dof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aadf79bc0320ee8ed05eb7cf3501970b4040021" title="Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's Michael from &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; here! Ever since we released Dynamic GGUFs, we've received so much love thanks to you all, but we know better benchmarking was a top request!&lt;/p&gt; &lt;p&gt;Previously, we already benchmarked Gemma 3 and Llama 4 on 5-shot MMLU and KL Divergence but as we're holding our first &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; AMA in about an hour, we're happy to showcase Aider Polyglot benchmarks for our DeepSeek-V3.1 GGUFs and were quite surprised by the results! &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the first DeepSeek-V3.1 graph, we compare thinking with other thinking models. In the 2nd graph, we compare non-thinking vs a non-Unsloth Dynamic imatrix GGUF&lt;/li&gt; &lt;li&gt;Our &lt;strong&gt;1-bit&lt;/strong&gt; Unsloth Dynamic GGUF shrinks DeepSeek-V3.1 from &lt;strong&gt;671GB ‚Üí 192GB (-75% size)&lt;/strong&gt; and no-thinking mode outperforms GPT-4.1 (Apr 2025), GPT-4.5, and DeepSeek-V3-0324.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;3-bit&lt;/strong&gt; Unsloth DeepSeek-V3.1 (thinking) GGUF: Outperforms Claude-4-Opus (thinking).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;5-bit&lt;/strong&gt; Unsloth DeepSeek-V3.1 (non-thinking) GGUF: Matches Claude-4-Opus (non-thinking) performance.&lt;/li&gt; &lt;li&gt;Our Dynamic GGUFs &lt;strong&gt;perform consistently better&lt;/strong&gt; than other non-Unsloth Dynamic imatrix GGUFs&lt;/li&gt; &lt;li&gt;Other non-Unsloth 1-bit and 2-bit DeepSeek-V3.1 quantizations, as well as standard 1-bit quantization without selective layer quantization, either failed to load or produced gibberish and looping outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For our DeepSeek-V3.1 experiments, we compared different bits of &lt;strong&gt;Unsloth Dynamic GGUFs&lt;/strong&gt; against:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-precision, unquantized LLMs&lt;/strong&gt; including GPT 4.5, 4.1, Claude-4-Opus, DeepSeek-V3-0324 etc.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Other&lt;/em&gt; dynamic imatrix V3.1 GGUFs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semi-dynamic&lt;/strong&gt; (some selective layer quantization) imatrix V3.1 GGUFs for ablation purposes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmark experiments were mainly conducted by David (neolithic5452 on Aider Disc), a trusted community contributor to Aider Polyglot evaluations. Tests were run ~3 times and averaged for a median score, and the Pass-2 accuracy is reported as by convention.&lt;/p&gt; &lt;p&gt;Wish we could attach another image for the non-thinking benchmarks but if you'd like more details, you can read our blogpost: &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks guys so much for the support!&lt;br /&gt; Michael&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewtq2ax40dof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndxsja</id>
    <title>GPT-OSS 20b (high) consistently does FAR better than gpt5-thinking on my engineering Hw</title>
    <updated>2025-09-11T02:54:33+00:00</updated>
    <author>
      <name>/u/InevitableWay6104</name>
      <uri>https://old.reddit.com/user/InevitableWay6104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just found this super interesting, but gpt-oss 20b gets almost every problem right, while gpt5-thinking, something I can only query like 5 times before getting rate limited (free tier), only gets it right about 50% of the time.&lt;/p&gt; &lt;p&gt;pretty interesting that a open weights 20b model is better than the closed flagship model on the free tier. I often use these models to verify my work, and both are free, but I can spam the 20b as much as I want and it's right more often.&lt;/p&gt; &lt;p&gt;granted, gpt5-thinking on the free tier is probably on the lowest setting, bc gpt-oss thinks ALOT longer than gpt5 did, on average it was about 20-30k tokens per question. &lt;/p&gt; &lt;p&gt;qwen3-30b-2507-thinking is also really good, but I don't think it's as good for this specific task, and gpt-oss is way smaller.&lt;/p&gt; &lt;p&gt;just still found it super interesting and wanted to share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableWay6104"&gt; /u/InevitableWay6104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T02:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf01a</id>
    <title>So apparently half of us are "AI providers" now (EU AI Act edition)</title>
    <updated>2025-09-10T14:00:15+00:00</updated>
    <author>
      <name>/u/Thecomplianceexpert</name>
      <uri>https://old.reddit.com/user/Thecomplianceexpert</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heads up, fellow tinkers&lt;/p&gt; &lt;p&gt;The EU AI Act‚Äôs first real deadline kicked in August 2nd so if you‚Äôre messing around with models that hit 10^23 FLOPs or more (think Llama-2 13B territory), regulators now officially care about you.&lt;/p&gt; &lt;p&gt;Couple things I‚Äôve learned digging through this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The FLOP cutoff is surprisingly low. It‚Äôs not ‚ÄúGPT-5 on a supercomputer‚Äù level, but it‚Äôs way beyond what you‚Äôd get fine-tuning Llama on your 3090.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;‚ÄúProvider‚Äù doesn‚Äôt just mean Meta, OpenAI, etc. If you fine-tune or significantly modify a big model, you need to watch out. Even if it‚Äôs just a hobby, you can still be classified as a provider.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Compliance isn‚Äôt impossible. Basically: &lt;ul&gt; &lt;li&gt;Keep decent notes (training setup, evals, data sources).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Have some kind of ‚Äúdata summary‚Äù you can share if asked.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Don‚Äôt be sketchy about copyright.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Deadline check:&lt;br /&gt; &lt;ul&gt; &lt;li&gt;New models released after Aug 2025 - rules apply now!&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Models that existed before Aug 2025 - you‚Äôve got until 2027.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EU basically said: ‚ÄúCongrats, you‚Äôre responsible now.‚Äù ü´† &lt;/p&gt; &lt;p&gt;TL;DR: If you‚Äôre just running models locally for fun, you‚Äôre probably fine. If you‚Äôre fine-tuning big models and publishing them, you might already be considered a ‚Äúprovider‚Äù under the law.&lt;/p&gt; &lt;p&gt;Honestly, feels wild that a random tinkerer could suddenly have reporting duties, but here we are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thecomplianceexpert"&gt; /u/Thecomplianceexpert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndoxxa</id>
    <title>Why should I **not** buy an AMD AI Max+ 395 128GB right away ?</title>
    <updated>2025-09-10T20:10:55+00:00</updated>
    <author>
      <name>/u/StyMaar</name>
      <uri>https://old.reddit.com/user/StyMaar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the rise of medium-sized MoE (gpt-oss-120B, GLM-4.5-air, and now the incoming Qwen3-80B-A3B) and their excellent performance for local models (well at least for the two first), the relatively low compute and memory bandwidth of the Strix Halo doesn't sounds too much of a problem anymore (because of the low active parameters count) and the 128GB of VRAM for $2k is unbeatable.&lt;/p&gt; &lt;p&gt;So now I'm very tempted to buy one, but I'm also aware that I don't really &lt;em&gt;need&lt;/em&gt; one, so please give me arguments about why I should not buy it.&lt;/p&gt; &lt;p&gt;My wallet thanks you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StyMaar"&gt; /u/StyMaar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T20:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We‚Äôre super excited to answer all your questions!! ü¶• Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we‚Äôre releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM ‚Äì 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!ü•∞&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
