<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-03T17:25:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pd4vp0</id>
    <title>EchoKit (Voice Interface for Local LLMs) Update: Added Dynamic System Prompts &amp; MCP Tool Wait Messages</title>
    <updated>2025-12-03T13:58:46+00:00</updated>
    <author>
      <name>/u/smileymileycoin</name>
      <uri>https://old.reddit.com/user/smileymileycoin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building &lt;strong&gt;EchoKit&lt;/strong&gt;, a hardware/software stack to give a voice to your local LLMs. It connects to OpenAI-compatible endpoints, meaning you can run it with LlamaEdge, standard LlamaCPP, or even Groq/Gemini.&lt;/p&gt; &lt;p&gt;We just released a server update that makes testing different &amp;quot;Agents&amp;quot; much faster:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Dynamic Prompt Loading:&lt;/strong&gt; Instead of hardcoding the system prompt in a config file and restarting the server every time you want to change the personality, you can now point the server to a URL (like a raw text file or an entry from &lt;code&gt;LLMs.txt&lt;/code&gt;). This lets you swap between a &amp;quot;Coding Assistant&amp;quot; and a &amp;quot;Storyteller&amp;quot; instantly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Better Tool Use (MCP) UX:&lt;/strong&gt; We are betting big on the Model Context Protocol (MCP) for agentic search and tools. The voice agent now speaks a &amp;quot;Please wait&amp;quot; message when it detects it needs to call an external tool, so the user isn't left in silence during the tool-call latency.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smileymileycoin"&gt; /u/smileymileycoin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd4vp0/echokit_voice_interface_for_local_llms_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd4vp0/echokit_voice_interface_for_local_llms_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd4vp0/echokit_voice_interface_for_local_llms_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:58:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcwffb</id>
    <title>Llama 3.1 70B + one prompt now beats Claude 3.5 Sonnet (96.9% on Arena-Hard-Auto, 4% refusals)</title>
    <updated>2025-12-03T06:00:57+00:00</updated>
    <author>
      <name>/u/NoSir261</name>
      <uri>https://old.reddit.com/user/NoSir261</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last few weeks iterating a single system prompt until stock Llama-3.1-70B-Instruct started outperforming Claude 3.5 Sonnet on the hardest blind arena benchmark. Results (100% reproducible):&lt;/p&gt; &lt;p&gt;‚Ä¢ 96.4‚Äì96.9% win rate on Arena-Hard-Auto (vs Sonnet‚Äôs 94.7%) ‚Ä¢ Only 4% refusals (base model is ~25‚Äì30%) ‚Ä¢ Dense, creative, actually useful output&lt;/p&gt; &lt;p&gt;No fine-tune, no LoRA, no quantization tricks. Just one prompt.&lt;/p&gt; &lt;p&gt;Full X thread with JSONL proof + evals: &lt;a href="https://x.com/BrSanch/status/1864123456789012345"&gt;https://x.com/BrSanch/status/1864123456789012345&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think prompt engineering can do a lot more than most people think it can.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoSir261"&gt; /u/NoSir261 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcwffb/llama_31_70b_one_prompt_now_beats_claude_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T06:00:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcbr10</id>
    <title>Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser.</title>
    <updated>2025-12-02T15:43:30+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt; &lt;img alt="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." src="https://external-preview.redd.it/a2FpOGJodms5dDRnMVOJ9FmD9w2-LMCVXdFIiBg8ZPjaS6tgqxX1OyhMPvmT.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f21be91171f3f1c63baa540518e8447e2d1bdca9" title="Ministral WebGPU: Run Mistral's new multimodal models 100% locally in your browser." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Mistral released &lt;strong&gt;Mistral 3&lt;/strong&gt;, a family of multimodal models, including three start-of-the-art dense models (3B, 8B, and 14B) and Mistral Large 3 (675B, 41B active). All Apache 2.0! ü§ó Surprisingly, the 3B is small enough to run 100% locally in your browser with WebGPU acceleration, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Link to demo: &lt;a href="https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU"&gt;https://huggingface.co/spaces/mistralai/Ministral_3B_WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vwrcg6vk9t4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcbr10/ministral_webgpu_run_mistrals_new_multimodal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:43:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd6vxu</id>
    <title>Does anyone use RunPod for SFT? If yes, you train via SSH or Jupyter (web-hosted)</title>
    <updated>2025-12-03T15:15:38+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a week or 2, I want to rent one B200 ($5.2/hr) to do SFT (Supervised Fine-Tuning) for GPT-OSS-120B on a ~30k row dataset. &lt;/p&gt; &lt;p&gt;I have done training for 2-4 hours a few months ago, but sometimes the Jupyter notebook crashed (got a message like &amp;quot;Connection lost&amp;quot; or something like that) and often had to restart training.&lt;/p&gt; &lt;p&gt;If you use RunPod (or any other GPU cloud provider), how do you manage long sessions (4+ hours)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd6vxu/does_anyone_use_runpod_for_sft_if_yes_you_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd6vxu/does_anyone_use_runpod_for_sft_if_yes_you_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd6vxu/does_anyone_use_runpod_for_sft_if_yes_you_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T15:15:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2x8u</id>
    <title>I built an offline AI chat app that automatically pulls Wikipedia articles for factual answers - runs completely locally with Ollama</title>
    <updated>2025-12-03T12:32:49+00:00</updated>
    <author>
      <name>/u/Smart-Competition200</name>
      <uri>https://old.reddit.com/user/Smart-Competition200</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/imDelivered/WikiRAG"&gt;https://github.com/imDelivered/WikiRAG&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Ä¢ Runs 100% offline - no internet needed after initial setup&lt;/p&gt; &lt;p&gt;‚Ä¢ Automatically injects relevant Wikipedia articles into AI responses (RAG)&lt;/p&gt; &lt;p&gt;‚Ä¢ Privacy-focused - everything runs locally on your machine&lt;/p&gt; &lt;p&gt;‚Ä¢ Works with any Ollama model (llama3.2, dolphin-llama3, etc.)&lt;/p&gt; &lt;p&gt;‚Ä¢ Simple setup script handles everything automatically&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart-Competition200"&gt; /u/Smart-Competition200 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2x8u/i_built_an_offline_ai_chat_app_that_automatically/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:32:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcb50r</id>
    <title>Ministral-3 has been released</title>
    <updated>2025-12-02T15:20:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt; &lt;img alt="Ministral-3 has been released" src="https://b.thumbs.redditmedia.com/a0DyjW1DyWh-ddE3J9WOyZjKJiBbmcXRGjqX2TH__QM.jpg" title="Ministral-3 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-14B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-14B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The largest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 14B&lt;/strong&gt; offers frontier capabilities and performance comparable to its larger &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-Instruct-2506"&gt;Mistral Small 3.2 24B&lt;/a&gt; counterpart. A powerful and efficient language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-8B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-8B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A balanced model in the Ministral 3 family, &lt;strong&gt;Ministral 3 8B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Reasoning-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Instruct-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Ministral-3-3B-Base-2512"&gt;https://huggingface.co/mistralai/Ministral-3-3B-Base-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The smallest model in the Ministral 3 family, &lt;strong&gt;Ministral 3 3B&lt;/strong&gt; is a powerful, efficient tiny language model with vision capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13"&gt;https://preview.redd.it/471e4lma6t4g1.png?width=1078&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c23d37e6a361041132ccec451c0a03921acc6e13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5"&gt;https://preview.redd.it/c2szd14b6t4g1.png?width=1210&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d97fc5e8626f25f8c13a5b159e6351976f45de5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-14B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-8B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Reasoning-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF"&gt;https://huggingface.co/unsloth/Ministral-3-3B-Instruct-2512-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcb50r/ministral3_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcv0kd</id>
    <title>Maaza Orchestrator v1.2 ‚Äî 9.6M params, 62.9 % on hard adversarial tool-calling, 39 ms latency</title>
    <updated>2025-12-03T04:45:28+00:00</updated>
    <author>
      <name>/u/CycleCore_Tech</name>
      <uri>https://old.reddit.com/user/CycleCore_Tech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped v1.2 of Maaza Orchestrator (9.6 M params). &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Split&lt;/th&gt; &lt;th align="left"&gt;v1.0&lt;/th&gt; &lt;th align="left"&gt;v1.2&lt;/th&gt; &lt;th align="left"&gt;Œî&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;In-distribution accuracy&lt;/td&gt; &lt;td align="left"&gt;88.0%&lt;/td&gt; &lt;td align="left"&gt;86.0%&lt;/td&gt; &lt;td align="left"&gt;‚àí2.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Adversarial tool-calling&lt;/td&gt; &lt;td align="left"&gt;26.6%&lt;/td&gt; &lt;td align="left"&gt;62.9%&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+36.3%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;p50 latency (CPU)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;33.4ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;39.4ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+6.0ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The adversarial set is 124 held-out examples across 36 tools. A few representative ones so you can judge the difficulty:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚Äúlmao just text that to them‚Äù ‚Üí email_send&lt;/li&gt; &lt;li&gt;‚Äúturn this into spokenshit‚Äù ‚Üí voice_mcp&lt;/li&gt; &lt;li&gt;‚Äútime to rip and tear‚Äù ‚Üí doom_mcp&lt;/li&gt; &lt;li&gt;‚Äúwassup with my ethereum val‚Äù ‚Üí crypto_lookup&lt;/li&gt; &lt;li&gt;‚Äúplz execcute dis py code, gr8 tnx‚Äù ‚Üí code_execute_python&lt;/li&gt; &lt;li&gt;‚Äúweather or not?‚Äù ‚Üí weather_lookup (pun + typo)&lt;/li&gt; &lt;li&gt;‚Äúwiggle to &lt;a href="http://www.example.com%E2%80%9D"&gt;www.example.com‚Äù&lt;/a&gt; ‚Üí puppeteer_navigate&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most examples stack 2‚Äì3 perturbations (slang + typos + abbreviations + cultural references). A vanilla 9.6 M model would probably sit below 30 % here.&lt;/p&gt; &lt;p&gt;The +36% came from one data-centric fine-tune: ~500 diverse adversarial seeds ‚Üí 10√ó upsampled ‚Üí 5 epochs. &lt;/p&gt; &lt;p&gt;‚Ä¢ HF: &lt;a href="https://huggingface.co/CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2"&gt;https://huggingface.co/CycleCoreTechnologies/maaza-nlm-orchestrator-9.6m-v1.2&lt;/a&gt;&lt;br /&gt; ‚Ä¢ Full 124-example held-out adversarial set (JSONL)&lt;br /&gt; ‚Ä¢ Training split &amp;amp; exact upsampling script&lt;br /&gt; ‚Ä¢ Apache 2.0&lt;/p&gt; &lt;p&gt;Happy to share the seed adversarial list. (v1.3 with 18√ó upsampling is already training).&lt;/p&gt; &lt;p&gt;Thanks for reading. Feedback always welcome. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CycleCore_Tech"&gt; /u/CycleCore_Tech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcv0kd/maaza_orchestrator_v12_96m_params_629_on_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T04:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcjqjs</id>
    <title>Ministral 3 models were pruned from Mistral Small 3.1</title>
    <updated>2025-12-02T20:36:32+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt; &lt;img alt="Ministral 3 models were pruned from Mistral Small 3.1" src="https://preview.redd.it/bte4gtp1qu4g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bec6f7045ad754997a36d5294eedaa2112246178" title="Ministral 3 models were pruned from Mistral Small 3.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bte4gtp1qu4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcjqjs/ministral_3_models_were_pruned_from_mistral_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T20:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcurp8</id>
    <title>Q: When will there be fast and competent SLMs for laptops?</title>
    <updated>2025-12-03T04:32:47+00:00</updated>
    <author>
      <name>/u/TomLucidor</name>
      <uri>https://old.reddit.com/user/TomLucidor</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It has been a whole year since Qwen2.5-32B been published for people to self-host their coding models. Similar models for RP probably exists before then, but the ideal of a general purpose portable model is still here. Yet, the news kept showing more techniques!&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Qwen3-30B-A3B and GPT-OSS-20B both uses Mixture-of-Experts instead of dense layers for their SLM&lt;/li&gt; &lt;li&gt;Kimi-Linear and Qwen3-Next-80B-A3B moved along to use &amp;quot;mixed attention&amp;quot; (majority of layers with linear attention) to speed things up AND have longer contexts&lt;/li&gt; &lt;li&gt;Not enough people getting into ternary attention like &lt;strong&gt;BitNet a4.8&lt;/strong&gt; / &lt;strong&gt;BitNet v2&lt;/strong&gt; &lt;a href="https://arxiv.org/html/2504.18415v2"&gt;https://arxiv.org/html/2504.18415v2&lt;/a&gt; or ternary quantization (PTQ) &lt;a href="https://arxiv.org/html/2509.23809v2"&gt;https://arxiv.org/html/2509.23809v2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Whatever layer routing is to reduce the amount of RAM needed, including &lt;strong&gt;Ouro-2.6B-Thinking&lt;/strong&gt; these days and &lt;strong&gt;Mixture-of-Depths&lt;/strong&gt; back in 2024&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Are all of these different techniques conflicting with one another? If it is just a lack of funding for fine-tuning/modding an existing SLM into something fast (assuming QAFT and RL), how much would it cost to crowdfund a project like this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomLucidor"&gt; /u/TomLucidor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcurp8/q_when_will_there_be_fast_and_competent_slms_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T04:32:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9tgj</id>
    <title>A Technical Tour of the DeepSeek Models from V3 to V3.2</title>
    <updated>2025-12-03T17:03:17+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt; &lt;img alt="A Technical Tour of the DeepSeek Models from V3 to V3.2" src="https://external-preview.redd.it/Oy9W7OYOeVO8Z6Sl3EWWZR-9AbREkAwoyEei1XJ7yeY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a5effd7f132f71b2efdd47cc12daa448023c0bf" title="A Technical Tour of the DeepSeek Models from V3 to V3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T17:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcia1t</id>
    <title>DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High</title>
    <updated>2025-12-02T19:41:57+00:00</updated>
    <author>
      <name>/u/kyousukegum</name>
      <uri>https://old.reddit.com/user/kyousukegum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt; &lt;img alt="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" src="https://a.thumbs.redditmedia.com/TJzNTRI6aFSLhjDdzBZtSFW1nl-mDnldDNZ8ONcsRV0.jpg" title="DeepSeek V3.2 Speciale dominates my math bench while being ~15√ó cheaper than GPT-5.1 High" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad"&gt;https://preview.redd.it/cwyrxaxneu4g1.png?width=3600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02ddc7b09fb3264f23ada56612af21febfe93bad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for context on how impressive this is, I couldn't believe my eyes and had to double-check the results multiple times. The problems in this category are very hard like in the same ballpark as IMO P6.&lt;br /&gt; &lt;a href="https://x.com/gum1h0x/status/1995915458612953419"&gt;https://x.com/gum1h0x/status/1995915458612953419&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kyousukegum"&gt; /u/kyousukegum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcia1t/deepseek_v32_speciale_dominates_my_math_bench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T19:41:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcayfs</id>
    <title>Mistral 3 Blog post</title>
    <updated>2025-12-02T15:13:14+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt; &lt;img alt="Mistral 3 Blog post" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Mistral 3 Blog post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/mistral-3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcayfs/mistral_3_blog_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T15:13:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1zeu</id>
    <title>apple/CLaRa-7B-Instruct ¬∑ Hugging Face</title>
    <updated>2025-12-03T11:43:05+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-Instruct"&gt;https://huggingface.co/apple/CLaRa-7B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-Base"&gt;https://huggingface.co/apple/CLaRa-7B-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/apple/CLaRa-7B-E2E"&gt;https://huggingface.co/apple/CLaRa-7B-E2E&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1zeu/appleclara7binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:43:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3uvy</id>
    <title>Can we expect better LLM hardware in 2026?</title>
    <updated>2025-12-03T13:16:07+00:00</updated>
    <author>
      <name>/u/Bitter-College8786</name>
      <uri>https://old.reddit.com/user/Bitter-College8786</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean with a lot of fast(!) VRAM.&lt;/p&gt; &lt;p&gt;DGX spark and AMD AI Max have really low memory speeds.&lt;/p&gt; &lt;p&gt;China is releasing so many open source models, when will they come with cheap hardware that we can run them?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bitter-College8786"&gt; /u/Bitter-College8786 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3uvy/can_we_expect_better_llm_hardware_in_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pceipb</id>
    <title>Mistral just released Mistral 3 ‚Äî a full open-weight model family from 3B all the way up to 675B parameters.</title>
    <updated>2025-12-02T17:26:06+00:00</updated>
    <author>
      <name>/u/InternationalToe2678</name>
      <uri>https://old.reddit.com/user/InternationalToe2678</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are Apache 2.0 and fully usable for research + commercial work.&lt;/p&gt; &lt;p&gt;Quick breakdown:&lt;/p&gt; &lt;p&gt;‚Ä¢ Ministral 3 (3B / 8B / 14B) ‚Äì compact, multimodal, and available in base, instruct, and reasoning variants. Surprisingly strong for their size.&lt;/p&gt; &lt;p&gt;‚Ä¢ Mistral Large 3 (675B MoE) ‚Äì their new flagship. Strong multilingual performance, high efficiency, and one of the most capable open-weight instruct models released so far.&lt;/p&gt; &lt;p&gt;Why it matters: You now get a full spectrum of open models that cover everything from on-device reasoning to large enterprise-scale intelligence. The release pushes the ecosystem further toward distributed, open AI instead of closed black-box APIs.&lt;/p&gt; &lt;p&gt;Full announcement: &lt;a href="https://mistral.ai/news/mistral-3"&gt;https://mistral.ai/news/mistral-3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalToe2678"&gt; /u/InternationalToe2678 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pceipb/mistral_just_released_mistral_3_a_full_openweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T17:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcomhi</id>
    <title>I'm surprised how simple Qwen3 VL's architecture is.</title>
    <updated>2025-12-02T23:50:06+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt; &lt;img alt="I'm surprised how simple Qwen3 VL's architecture is." src="https://preview.redd.it/bfrh4xf5nv4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2643d2d6457fb6e3adfc09a5cf9e18b995e4219f" title="I'm surprised how simple Qwen3 VL's architecture is." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;the new 3D position id logic really got a lot more intuitive compared to qwen2.5 vl. it basically index image patches on width and height dimension in addition to the regular token sequence / temporal dimension (while treating text as one same number across all 3 dimensions). &lt;/p&gt; &lt;p&gt;in addition to this, they added deepstack, which essentially is just some residual connections between vision encoder blocks and downstream LLM blocks.&lt;/p&gt; &lt;p&gt;here's the full repo if you want to read more: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfrh4xf5nv4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcomhi/im_surprised_how_simple_qwen3_vls_architecture_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-02T23:50:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1yqc</id>
    <title>Hot take: We‚Äôre overselling 'semantic search' in RAG.</title>
    <updated>2025-12-03T11:42:02+00:00</updated>
    <author>
      <name>/u/Raisin_False</name>
      <uri>https://old.reddit.com/user/Raisin_False</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building some RAG stuff and 'semantic search' feels way more magical in marketing than in reality.&lt;/p&gt; &lt;p&gt;Embeddings are great &lt;strong&gt;fuzzy matchers in meaning space&lt;/strong&gt; - they shine on paraphrases, synonyms, 'something like this' queries. But whenever I need sharper behavior (logic, constraints, dates, 'papers using X on Y after 2019'), plain bi-encoder vector search starts to fall over unless I add extra machinery.&lt;/p&gt; &lt;p&gt;In practice my setups end up looking more like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;BM25 or dense (or hybrid) &lt;/li&gt; &lt;li&gt;Reranker and/or LLM query rewrite &lt;/li&gt; &lt;li&gt;LLM reasoning also maybe graphs/filters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At that point, calling just the first stage 'semantic search' feels a bit misleading, cause it's more like 'dense/vector retrieval' plus a bunch of stuff on top that actually does the reasoning.&lt;/p&gt; &lt;p&gt;So i have 2 questions for you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is 'semantic search' a fair name for plain vector similarity, or do you avoid that term?&lt;/li&gt; &lt;li&gt;How far did you get with just embeddings before needing reranking / query rewriting / graphs / filters?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raisin_False"&gt; /u/Raisin_False &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pcp8z3</id>
    <title>Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?</title>
    <updated>2025-12-03T00:16:47+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt; &lt;img alt="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" src="https://preview.redd.it/buxyht7ltv4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed225e778fb3ebb1d3e4ff9ac401e09c3aced65e" title="Who‚Äôs got them Q_001_X_S_REAP Mistral Large 3 GGUFs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm looking at you, Unsloth üòÅ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/buxyht7ltv4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pcp8z3/whos_got_them_q_001_x_s_reap_mistral_large_3_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T00:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3mdw</id>
    <title>Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI</title>
    <updated>2025-12-03T13:05:36+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt; &lt;img alt="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" src="https://external-preview.redd.it/0mZ7_HvOTkdLgtq4s_qT3vry9cE_RWRALKiuljZ3Fl8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01be5fde96c8bded5f16d12f17d20ed686c5e29" title="Intel Arc Pro B60 Battlematrix Preview: 192GB of VRAM for On-Premise AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b60-battlematrix-preview-192gb-of-vram-for-on-premise-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3mdw/intel_arc_pro_b60_battlematrix_preview_192gb_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9f4x</id>
    <title>I trained a 7B to learn a niche language and reaching 86% code accuracy</title>
    <updated>2025-12-03T16:49:14+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"&gt; &lt;img alt="I trained a 7B to learn a niche language and reaching 86% code accuracy" src="https://b.thumbs.redditmedia.com/y1P83lnQKXF0ESpmUcRyqN8DslMG6uG2cjIoC8294mY.jpg" title="I trained a 7B to learn a niche language and reaching 86% code accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I just wanted to share a project I did over the last weekend.&lt;/p&gt; &lt;p&gt;I‚Äôm no ML engineer or having any relevant background in AI, just have been toying with the idea of training an LLM myself for a while.&lt;/p&gt; &lt;p&gt;Most of my previous training attempts did not yield and meaningful result, but I‚Äôm still managed to learned a thing or two. And this time, I decided to give it a try again.&lt;/p&gt; &lt;p&gt;The niche language I picked to train the LLM (Qwen2.5-coder-7b) was a less popular text-to-diagram language called Pintora. Since most open source models did not have any knowledge about this language, it‚Äôs a fun project to try.&lt;/p&gt; &lt;p&gt;Long story short, I planned to train this for free on Google Colab, but ended up renting a 48GB A40 for a naive mistake, and doing a lot of the training pipeline myself (in a much smaller scale), from creating the dataset, cleaning them up, to do two phases training: Continued Pretraining and then Instruction Finetune, to teach the model how to either generate diagrams from scratch and editing existing diagrams. &lt;/p&gt; &lt;p&gt;In the end, I‚Äôm quite happy with the result, although it‚Äôs not great, the model was able to generate syntactically correct code, the diagrams are showing up. I did a quick evaluation to confirm how accurate (in terms of of compile-able diagrams) that the model can generate, out of 1000 examples, only about 140 are failing, that‚Äôs about 86% accuracy.&lt;/p&gt; &lt;p&gt;Both the model (safetensors, gguf, full and quantized) are available on HF if you are interested. I also did a write up to document the process, I think it might be helpful to share so I can learn from all of your feedback! &lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language"&gt;https://huy.rocks/everyday/12-01-2025-ai-teaching-an-llm-a-niche-diagraming-language&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/huytd189/pintora-coder-7b"&gt;https://huggingface.co/huytd189/pintora-coder-7b&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/huytd189/pintora-coder-7b-gguf"&gt;https://huggingface.co/huytd189/pintora-coder-7b-gguf&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Dataset:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/huytd189/pintora-instruct"&gt;https://huggingface.co/datasets/huytd189/pintora-instruct&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/datasets/huytd189/pintora-edit-instruct"&gt;https://huggingface.co/datasets/huytd189/pintora-edit-instruct&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pd9f4x"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9f4x/i_trained_a_7b_to_learn_a_niche_language_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T16:49:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3xyp</id>
    <title>Why don't Google and Openai release their old models?</title>
    <updated>2025-12-03T13:19:51+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPt 4 and gemini 2 pro are dated, they should release it... Are they afraid of releasing their data and architecture? They released gemma and gpt oss already. Gemini 2 has a large context window, but the quality degrades when it gets large though and it is replicable.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd5yxy</id>
    <title>My experiences with the new Ministral 3 14B Reasoning 2512 Q8</title>
    <updated>2025-12-03T14:41:03+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt; &lt;img alt="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" src="https://b.thumbs.redditmedia.com/YQNWxn03P5a0Q35GBj3cSIS0Oa0a8pdRn0Pkkl0sUGM.jpg" title="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;45 minutes and 33K tokens of thinking about making html tetris (1 line prompt):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819"&gt;https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tool calling breaks all the time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184"&gt;https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also at some point it stopped using the [think] tags altogether and just started thinking out loud. I'll leave it running for a couple of hours and see if it eventually manages to build the HTML Tetris.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T14:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
