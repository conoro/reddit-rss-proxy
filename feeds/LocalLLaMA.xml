<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-21T05:48:48+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1obq1xw</id>
    <title>Small LLM runs on VPS without GPU</title>
    <updated>2025-10-20T18:46:39+00:00</updated>
    <author>
      <name>/u/RageQuitNub</name>
      <uri>https://old.reddit.com/user/RageQuitNub</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hi guys, &lt;/p&gt; &lt;p&gt;Very new to this community, this is my first post. I been watching and following LLM for quite some time now, and I think the time has come for me to implement my first local LLM. &lt;/p&gt; &lt;p&gt;I am planning to host one on a small VPs without GPU. All I need it to do is taking a text, and do the following tasks:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Extract some data in JSON format, &lt;/li&gt; &lt;li&gt;Do a quick 2-3 paragraph summary. &lt;/li&gt; &lt;li&gt;If it has date, lets say the text mention 2 days from now, it should be able to tell it is Oct 22nd.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;That's all. Pretty simple. Is there any small LLM that can handle these tasks on CPU and Ram alone? If so, what is the minimal CPU core and Ram I need to run it.&lt;/p&gt; &lt;p&gt;Thank you and have a nice day.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RageQuitNub"&gt; /u/RageQuitNub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obq1xw/small_llm_runs_on_vps_without_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obq1xw/small_llm_runs_on_vps_without_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obq1xw/small_llm_runs_on_vps_without_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T18:46:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1obt1la</id>
    <title>Local AI config : Mini ITX single RTX PRO 6000 Workstation for inference ?</title>
    <updated>2025-10-20T20:37:15+00:00</updated>
    <author>
      <name>/u/dvd84x</name>
      <uri>https://old.reddit.com/user/dvd84x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obt1la/local_ai_config_mini_itx_single_rtx_pro_6000/"&gt; &lt;img alt="Local AI config : Mini ITX single RTX PRO 6000 Workstation for inference ?" src="https://preview.redd.it/mwhsvxhoubwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8c45afc1e4c40ba8f82f5dc6c6cbc4e2ec05ff4" title="Local AI config : Mini ITX single RTX PRO 6000 Workstation for inference ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I’m asking your thoughts before creating my first 100% AI inference setup, inspired by Alex Ziskind﻿'s video from a few months ago. It’s meant to be a small AI server, using medium size LLM (llama 3.3 70b / gpt-oss-120b) at decent speed for 4 simultaneous users and built around an RTX PRO 6000 Workstation Edition.&lt;/p&gt; &lt;p&gt;Here’s the core: Ryzen 9 9900X, ASRock X870 Pro RS motherboard, 96GB DDR5 RAM, Cooler Master NR200P V2 case, Lian Li 240mm liquid cooler, and ASUS ROG 1000W PSU.&lt;/p&gt; &lt;p&gt;Total cost would be around 10 000€ tax included here in France and this is the max amount i am happy to spend on this :) Any tips / feedback before doing it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dvd84x"&gt; /u/dvd84x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mwhsvxhoubwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obt1la/local_ai_config_mini_itx_single_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obt1la/local_ai_config_mini_itx_single_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T20:37:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn61p</id>
    <title>Ring-mini-sparse-2.0-exp, yet another experimental open source model from inclusionAI that tries to improve performance over long contexts</title>
    <updated>2025-10-20T16:36:41+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-mini-sparse-2.0-exp, an open-source efficient inference model based on the Ling 2.0 MoE architecture. This sparse variant uses Mixture-of-Block-Attention (MoBA) to slash KV cache overhead by 87.5% (down to ~8K tokens/query at 64K context), enabling up to 3x decode speedup over dense-equivalent Ring-mini-2.0 while matching full softmax performance on reasoning tasks. Built by continual pretraining +100B tokens from Ling-mini-base-2.0-20T (16B total params, ~1.6B active via 1/32 expert ratio). → 128K context via YaRN 4x extrapolation · GQA heads with shared KV blocks per group for head-efficient sparsity → No RLHF, pure supervised finetuning for stability in high-concurrency setups. Delivers competitive results on math (e.g., AIME/HMMT-style), coding (LiveCodeBench), and science (ARC-AGI/HealthBench) evals—on par with 8B dense models like Qwen3-8B-Thinking, but with massive efficiency gains for local deployment. Open weights in BF16/Safetensors; runs on HF Transformers 4.45+ or SGLang 0.4+ (custom wheel needed).&lt;/p&gt; &lt;p&gt;For even longer contexts, check the sibling Ring-mini-linear-2.0: a hybrid linear+softmax attention setup (+600B tokens training) hitting 512K via YaRN, with near-linear O(N) time/compute for ultra-long inputs—but in the benchmarks, the sparse MoBA edged it out on reasoning accuracy/speed tradeoffs at sub-128K lengths without the linear attn quirks. Both crush the original baseline on throughput (see their model cards' figs for prefill/decode curves). Not affiliated, just sharing for local runners since I'm very interested in those experimental models trying to solve context (;&lt;/p&gt; &lt;p&gt;If I'm not mistaken they also open sourced the training code (; &lt;/p&gt; &lt;p&gt;Llama.cpp support wont be easy though /:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-sparse-2.0-exp"&gt;https://huggingface.co/inclusionAI/Ring-mini-sparse-2.0-exp&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/inclusionAI/Ring-mini-linear-2.0"&gt;https://huggingface.co/inclusionAI/Ring-mini-linear-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-sparse-2.0-exp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn61p/ringminisparse20exp_yet_another_experimental_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn61p/ringminisparse20exp_yet_another_experimental_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1obb4c4</id>
    <title>What are your /r/LocalLLaMA "hot-takes"?</title>
    <updated>2025-10-20T04:55:04+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or something that goes against the general opinions of the community? Vibes are the only benchmark that counts after all.&lt;/p&gt; &lt;p&gt;I tend to agree with the flow on most things &lt;em&gt;but&lt;/em&gt; my thoughts that I'd consider going against the grain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;QwQ was think-slop and was never &lt;em&gt;that&lt;/em&gt; good&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3-32B is still SOTA for 32GB and under. I cannot get anything to reliably beat it despite shiny benchmarks&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Deepseek is still open-weight SotA. I've really tried Kimi, GLM, and Qwen3's larger variants but asking Deepseek still feels like asking the adult in the room. Caveat is GLM codes better&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;(proprietary bonus): Grok4 handles news data better than Chatgpt5 or Gemini2.5 and will always win if you ask it about something that happened &lt;em&gt;that day&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T04:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc4tse</id>
    <title>Another llm question</title>
    <updated>2025-10-21T05:42:10+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does it work if multiple people use an llm at the same time or close to it? Does the system just spin up a separate instance of that llm? Or is it all just considered as one instance. And does the max context for the model split between the users? I’m wondering because I’m tempted to let my family use my OpenWebUi when they’re out and about. I know all about ssl, and all that. I’ve secured the OpenWebUi that’s running on my custom URL. I’m just wondering how LLMs handle multiple users. Please help me understand it. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc4tse/another_llm_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc4tse/another_llm_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc4tse/another_llm_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T05:42:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1obpsyb</id>
    <title>Building an open-source tool for multi-agent debugging and production monitoring - what am I missing?</title>
    <updated>2025-10-20T18:36:55+00:00</updated>
    <author>
      <name>/u/Standard_Career_8603</name>
      <uri>https://old.reddit.com/user/Standard_Career_8603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building an open-source observability tool specifically for multi-agent systems and want to learn from your experiences before I get too far down the wrong path.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current debugging process is a mess&lt;/strong&gt;:&lt;br /&gt; - Excessive logging in both frontend and backend&lt;br /&gt; - Manually checking if agents have the correct inputs/outputs&lt;br /&gt; - Trying to figure out which tool calls failed and why&lt;br /&gt; - Testing different prompts and having no systematic way to track how they change agent behavior&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I'm building&lt;/strong&gt;: A tool that helps you:&lt;br /&gt; - Observe information flow between agents&lt;br /&gt; - See which tools are being called and with what parameters&lt;br /&gt; - Track how prompt changes affect agent behavior&lt;br /&gt; - Debug fast in development, then monitor how agents actually perform in production&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here's where I need your input&lt;/strong&gt;: Existing tools (LangSmith, LangFuse, AgentOps) are great at LLM observability (tracking tokens, costs, and latency). But when it comes to multi-agent coordination, I feel like they fall short. They show you &lt;strong&gt;what&lt;/strong&gt; happened but not &lt;strong&gt;why&lt;/strong&gt; your agents failed to coordinate properly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My questions for you&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;What tools have you tried for debugging multi-agent systems?&lt;/li&gt; &lt;li&gt;Where do they work well? Where do they fall short?&lt;/li&gt; &lt;li&gt;What's missing that would actually help you ship faster?&lt;/li&gt; &lt;li&gt;Or am I wrong - are you debugging just fine without specialized tooling?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I want to build something useful, not just another observability tool that collects dust. Honest feedback (including &amp;quot;we don't need this&amp;quot;) is super valuable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Career_8603"&gt; /u/Standard_Career_8603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obpsyb/building_an_opensource_tool_for_multiagent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obpsyb/building_an_opensource_tool_for_multiagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obpsyb/building_an_opensource_tool_for_multiagent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T18:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1obgci1</id>
    <title>Is Meta done with open-source Llama releases?</title>
    <updated>2025-10-20T11:19:17+00:00</updated>
    <author>
      <name>/u/emimix</name>
      <uri>https://old.reddit.com/user/emimix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was cleaning up my local LM stacks and noticed all the old Llama models I had. Brought back memories of how much fun they were — made me wonder, is Meta done releasing open-source models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emimix"&gt; /u/emimix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T11:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1obzfkq</id>
    <title>I'm researching about Tiny and Small Language Models to try to run them local</title>
    <updated>2025-10-21T01:06:06+00:00</updated>
    <author>
      <name>/u/Fearless_One2060</name>
      <uri>https://old.reddit.com/user/Fearless_One2060</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm kind of new on this topic, I'm a gamedev trying to make an AI-powered Text RPG with a SML or TML and a simple RAG system for myself to play with and kind of experiment with this a little more with some kind of novelization system. But I only heard around Llama 3.2 1B as the smallest one... Are there smaller yet smarter models out there? Just language models, I'm not interested on image nor audio generation, not yet... I don't have a limit, tho, I'd like to create this a way someone can run it local even in a phone but if not posible, then limit it to a common-use office desktop...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fearless_One2060"&gt; /u/Fearless_One2060 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obzfkq/im_researching_about_tiny_and_small_language/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obzfkq/im_researching_about_tiny_and_small_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obzfkq/im_researching_about_tiny_and_small_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T01:06:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1obsgrq</id>
    <title>Very slow response on gwen3-4b-thinking model on LM Studio. I need help</title>
    <updated>2025-10-20T20:15:49+00:00</updated>
    <author>
      <name>/u/Pack_Commercial</name>
      <uri>https://old.reddit.com/user/Pack_Commercial</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obsgrq/very_slow_response_on_gwen34bthinking_model_on_lm/"&gt; &lt;img alt="Very slow response on gwen3-4b-thinking model on LM Studio. I need help" src="https://b.thumbs.redditmedia.com/uI9NLXcbU0AtMZ5GGqlpKK2oCWxEaYNfYgvkTkWyTBk.jpg" title="Very slow response on gwen3-4b-thinking model on LM Studio. I need help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm a newbie and set up a local LLM on my PC. I downloaded the qwen3-4b model considering the spec of my laptop.(32GB corei7 + 16GB Intel integrated GPU)&lt;/p&gt; &lt;p&gt;I started with very simple questions for country capitals. But the response time is too bad (1min).&lt;/p&gt; &lt;p&gt;I want to know what is actually taking so long, Is it using the full hardware resources or is something wrong ?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ja1aodivrbwf1.png?width=3831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac0fe526d23f0f7893ae28de63e26b22575946ac"&gt;https://preview.redd.it/ja1aodivrbwf1.png?width=3831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac0fe526d23f0f7893ae28de63e26b22575946ac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c4nflvixrbwf1.png?width=3831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e048225cb7cd03b6012ad89929de8810e5d604e5"&gt;https://preview.redd.it/c4nflvixrbwf1.png?width=3831&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e048225cb7cd03b6012ad89929de8810e5d604e5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pack_Commercial"&gt; /u/Pack_Commercial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obsgrq/very_slow_response_on_gwen34bthinking_model_on_lm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obsgrq/very_slow_response_on_gwen34bthinking_model_on_lm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obsgrq/very_slow_response_on_gwen34bthinking_model_on_lm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T20:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqqdi</id>
    <title>Are Image-Text-to-Text models becoming the next big AI?</title>
    <updated>2025-10-20T19:11:57+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqqdi/are_imagetexttotext_models_becoming_the_next_big/"&gt; &lt;img alt="Are Image-Text-to-Text models becoming the next big AI?" src="https://preview.redd.it/mlml3tzrfbwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d600ef38fb84f739a9f4292ab6f8b5b80796085b" title="Are Image-Text-to-Text models becoming the next big AI?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been checking the trending models lately and it’s crazy how many of them are Image-Text-to-Text. Out of the top 7 right now, 5 fall in that category (PaddleOCR-VL, DeepSeek-OCR, Nanonets-OCR2-3B, Qwen3-VL, etc). DeepSeek even dropped their own model today.&lt;/p&gt; &lt;p&gt;Personally, I have been playing around with a few of them (OCR used to be such a pain earlier, imo) and the jump in quality is wild. They’re getting better at understanding layout, handwriting, tables data.&lt;br /&gt; (ps: My earlier fav was Mistral OCR)&lt;/p&gt; &lt;p&gt;It feels like companies are getting quite focused on multimodal systems that can understand and reason over images directly.&lt;/p&gt; &lt;p&gt;thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mlml3tzrfbwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqqdi/are_imagetexttotext_models_becoming_the_next_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqqdi/are_imagetexttotext_models_becoming_the_next_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:11:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1obvmh6</id>
    <title>How do you handle model licenses when distributing apps with embedded LLMs?</title>
    <updated>2025-10-20T22:17:31+00:00</updated>
    <author>
      <name>/u/Brilliant_Extent3159</name>
      <uri>https://old.reddit.com/user/Brilliant_Extent3159</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm developing an Android app that needs to run LLMs locally and figuring out how to handle model distribution legally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My options:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Host models on my own CDN&lt;/strong&gt; - Show users the original license agreement before downloading each model. They accept terms directly in my app.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Link to Hugging Face&lt;/strong&gt; - Users login to HF and accept terms there. Problem: most users don't have HF accounts and it's too complex for non-technical users.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I prefer Option 1 since users can stay within my app without creating additional accounts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you handling model licensing in your apps that distribute LLM weights?&lt;/li&gt; &lt;li&gt;How does Ollama (MIT licensed) distributes models like Gemma without requiring any license acceptance? When you pull models through Ollama, there's no agreement popup.&lt;/li&gt; &lt;li&gt;For those using Option 1 (self-hosting with license acceptance), has anyone faced legal issues?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Currently focusing on Gemma 3n, but since each model has different license terms, I need ideas that work for other models too.&lt;/p&gt; &lt;p&gt;Thanks in advance. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brilliant_Extent3159"&gt; /u/Brilliant_Extent3159 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obvmh6/how_do_you_handle_model_licenses_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obvmh6/how_do_you_handle_model_licenses_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obvmh6/how_do_you_handle_model_licenses_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T22:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1obftw9</id>
    <title>DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models</title>
    <updated>2025-10-20T10:33:42+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt; &lt;img alt="DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models" src="https://a.thumbs.redditmedia.com/07jxlZQFGtUtHiMuztBNSU_MiE3T0do53uVR780HIi0.jpg" title="DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bw20aruc58wf1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80ef4d2d3b6b194d08a290d37a68cd1f5bd072bb"&gt;https://preview.redd.it/bw20aruc58wf1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80ef4d2d3b6b194d08a290d37a68cd1f5bd072bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guillermo Rauch (&lt;strong&gt;Vercel CEO&lt;/strong&gt;) just shared benchmark results from their internal agent testing. That’s roughly &lt;strong&gt;5× faster&lt;/strong&gt; and &lt;strong&gt;50% higher accuracy&lt;/strong&gt; than the top proprietary models&lt;/p&gt; &lt;p&gt;It’s wild to see open source models not just catching up but starting to outperform in both efficiency and accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T10:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1obo226</id>
    <title>whats up with the crazy amount of OCR models launching?</title>
    <updated>2025-10-20T17:19:16+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"&gt; &lt;img alt="whats up with the crazy amount of OCR models launching?" src="https://preview.redd.it/dfdpiv7fvawf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e0b9278cf631890d722235d0fa392c339e1208e" title="whats up with the crazy amount of OCR models launching?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;aside from these models, we got MinerU2.5 and some other models i forgot. im most interested by DeepSeek launching an OCR model of all things, weren't they into AGI? do you think its for more efficient document parsing for training data or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dfdpiv7fvawf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T17:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1obocly</id>
    <title>Reasoning with Sampling: Your Base Model is Smarter Than You Think</title>
    <updated>2025-10-20T17:32:28+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2510.14901"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obocly/reasoning_with_sampling_your_base_model_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obocly/reasoning_with_sampling_your_base_model_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T17:32:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc1j9i</id>
    <title>dual radeon r9700 benchmarks</title>
    <updated>2025-10-21T02:45:40+00:00</updated>
    <author>
      <name>/u/luminarian721</name>
      <uri>https://old.reddit.com/user/luminarian721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;Just got my 2 radeon pro r9700 32gb cards delivered a couple of days ago.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;I can't seem to get anything other then gibberish with rocm 7.0.2 when using both cards no matter how i configured them or what i turn on or off in the cmake.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;So the benchmarks are single card only, and these cards are stuck on my e5-2697a v4 box until next year. so only pcie 3.0 ftm.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;Any benchmark requests?&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 20B F16 | 12.83 GiB | 20.91 B | ROCm | 999 | ROCm1 | pp512 | 404.28 ± 1.07 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gpt-oss 20B F16 | 12.83 GiB | 20.91 B | ROCm | 999 | ROCm1 | tg128 | 86.12 ± 0.22 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 999 | ROCm1 | pp512 | 197.89 ± 0.62 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| qwen3moe 30B.A3B Q4_K - Medium | 16.49 GiB | 30.53 B | ROCm | 999 | ROCm1 | tg128 | 81.94 ± 0.34 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 8B Q4_K - Medium | 4.64 GiB | 8.03 B | ROCm | 999 | ROCm1 | pp512 | 332.95 ± 3.21 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| llama 8B Q4_K - Medium | 4.64 GiB | 8.03 B | ROCm | 999 | ROCm1 | tg128 | 71.74 ± 0.08 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.66 GiB | 27.01 B | ROCm | 999 | ROCm1 | pp512 | 186.91 ± 0.79 |&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;| gemma3 27B Q4_K - Medium | 15.66 GiB | 27.01 B | ROCm | 999 | ROCm1 | tg128 | 24.47 ± 0.03 |&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luminarian721"&gt; /u/luminarian721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc1j9i/dual_radeon_r9700_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc1j9i/dual_radeon_r9700_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc1j9i/dual_radeon_r9700_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T02:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9vvk</id>
    <title>What happens when Chinese companies stop providing open source models?</title>
    <updated>2025-10-20T03:51:03+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What happens when Chinese companies stop providing open source models? Good example would be Alibaba's WAN. It was open source until the last version WAN2.5, which is closed source and it costs money. What happens when they start doing this across the board? Edit: Qwen Max is another example &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T03:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc0jfl</id>
    <title>What would be the best budget GPU now?</title>
    <updated>2025-10-21T01:57:53+00:00</updated>
    <author>
      <name>/u/Suomi422</name>
      <uri>https://old.reddit.com/user/Suomi422</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I got RTX 3050 OEM now and I'm building a new PC where I would like to have something more powerful for local LLMs - I'm also gaming but only really light stuffs like indie games. I'm planing to use Linux where AMD support works better at Wayland these days, but I also understand that AMD GPUs haven't good support for LLMs...&lt;/p&gt; &lt;p&gt;My budget would be something between Radeon RX 9060 XT 16GB and Nvidia RTX 5060Ti 16GB. Is there something better in this price category? * I was also thinking about Sparkle Intel Arc A770 Titan, but do not have any experience with Intel's GPUs yet...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suomi422"&gt; /u/Suomi422 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0jfl/what_would_be_the_best_budget_gpu_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0jfl/what_would_be_the_best_budget_gpu_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0jfl/what_would_be_the_best_budget_gpu_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T01:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1obq71x</id>
    <title>LM Studio beta resizes images to 1024 px now for VL models</title>
    <updated>2025-10-20T18:51:55+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obq71x/lm_studio_beta_resizes_images_to_1024_px_now_for/"&gt; &lt;img alt="LM Studio beta resizes images to 1024 px now for VL models" src="https://external-preview.redd.it/HLFitx4pf0jk0EXyGsTacqSfluQnnlai3zH8I6-hEEE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bba9f38dcdd34d917cc627645eb8fd5d00d1f612" title="LM Studio beta resizes images to 1024 px now for VL models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gsoayja3cbwf1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9f2d362622335940a2883d9ba9babb930a7a4"&gt;https://preview.redd.it/gsoayja3cbwf1.png?width=800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=69a9f2d362622335940a2883d9ba9babb930a7a4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Up from 500px. And they promise downsize will be configurable in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmstudio.ai/beta-releases"&gt;https://lmstudio.ai/beta-releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obq71x/lm_studio_beta_resizes_images_to_1024_px_now_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obq71x/lm_studio_beta_resizes_images_to_1024_px_now_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obq71x/lm_studio_beta_resizes_images_to_1024_px_now_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T18:51:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1obcm9r</id>
    <title>DeepSeek releases DeepSeek OCR</title>
    <updated>2025-10-20T06:26:26+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt; &lt;img alt="DeepSeek releases DeepSeek OCR" src="https://external-preview.redd.it/ddlXXAanndfx0k3ivMcCdrEJtDQlMZs1JyMP8q81Yms.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54c207b8079de2f72cbaafba0d28b87918c60e33" title="DeepSeek releases DeepSeek OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;https://huggingface.co/deepseek-ai/DeepSeek-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db"&gt;https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T06:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc0a0h</id>
    <title>[Release] gpu-poor: INT8 quantization achieving 74% memory reduction on large LLMs (pure Python, production metrics)</title>
    <updated>2025-10-21T01:45:28+00:00</updated>
    <author>
      <name>/u/BroccoliForsaken3288</name>
      <uri>https://old.reddit.com/user/BroccoliForsaken3288</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a pure Python INT8 quantization library optimized for large language models. Validated on GPT-2-large (774M params):&lt;/p&gt; &lt;p&gt;- 74% memory reduction (3GB → 767MB) &lt;/p&gt; &lt;p&gt;- 0.95× speed (near baseline) &lt;/p&gt; &lt;p&gt;- BLEU 0.90, perplexity +1.9% (industry targets: &amp;gt;0.90, &amp;lt;5%) &lt;/p&gt; &lt;p&gt;Key finding: Quantization overhead is fixed (~0.2s), not proportional to model size. On small models (300MB), this overhead dominates and causes 50%+ slowdown. On large models (3GB+), it's only 5% of total time. This inverse scaling makes gpu-poor ideal for large models where you get 74% memory savings with minimal speed penalty.&lt;/p&gt; &lt;p&gt;GitHub:&lt;a href="https://github.com/averine1/gpu-poor/tree/main"&gt;https://github.com/averine1/gpu-poor/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Documentation: Comprehensive benchmarks, quality analysis, reproducible results&lt;/p&gt; &lt;p&gt;Built this while active duty Navy + Berkeley MIDS. Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BroccoliForsaken3288"&gt; /u/BroccoliForsaken3288 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/averine1/gpu-poor/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0a0h/release_gpupoor_int8_quantization_achieving_74/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc0a0h/release_gpupoor_int8_quantization_achieving_74/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T01:45:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc3f0i</id>
    <title>Qwen3 Omni interactive speech</title>
    <updated>2025-10-21T04:21:40+00:00</updated>
    <author>
      <name>/u/Powerful-Angel-301</name>
      <uri>https://old.reddit.com/user/Powerful-Angel-301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Omni is very interesting. They claim it supports real-time voice, but I couldn't find out how and there was no tutorial for this on their github. &lt;/p&gt; &lt;p&gt;Anyone having any experience with that? Basically continuously talk to the model and get voice responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Angel-301"&gt; /u/Powerful-Angel-301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T04:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1obsdm6</id>
    <title>ROCm 7.9 RC1 released. Supposedly this one supports Strix Halo. Finally, it's listed under supported hardware.</title>
    <updated>2025-10-20T20:12:35+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://rocm.docs.amd.com/en/docs-7.9.0/about/release-notes.html#supported-hardware-and-operating-systems"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obsdm6/rocm_79_rc1_released_supposedly_this_one_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obsdm6/rocm_79_rc1_released_supposedly_this_one_supports/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T20:12:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1obrde8</id>
    <title>Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp; Qwen3-Coder-30B now of HF!</title>
    <updated>2025-10-20T19:35:48+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt; &lt;img alt="Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp;amp; Qwen3-Coder-30B now of HF!" src="https://b.thumbs.redditmedia.com/c9KTXS-jH2CuE3vbdqAs-d7zeKzxIjLJapF1oi1eETU.jpg" title="Cerebras REAP update: pruned checkpoints for GLM4.5-Air &amp;amp; Qwen3-Coder-30B now of HF!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have heard your feedback on our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;initial REAP post&lt;/a&gt; and are excited to released REAP-pruned checkpoints for more lightweight models, GLM4.5-Air and Qwen3-Coder-30B:&lt;/p&gt; &lt;p&gt;25% pruned GLM4.5-Air: &lt;a href="https://hf.co/cerebras/GLM-4.5-Air-REAP-82B-A12B"&gt;https://hf.co/cerebras/GLM-4.5-Air-REAP-82B-A12B&lt;/a&gt;&lt;br /&gt; 20% pruned Qwen3-Coder-30B: &lt;a href="https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B"&gt;https://huggingface.co/cerebras/Qwen3-Coder-REAP-25B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are releasing those in BF16 so more accurate low-bit quantized GGUFs can be created for streamlined local deployment.&lt;/p&gt; &lt;p&gt;TLDR on REAP: &lt;/p&gt; &lt;p&gt;We show that one-shot pruning of experts in large MoEs is better than expert merging when looking at realistic benchmarks, not just perplexity measures.&lt;/p&gt; &lt;p&gt;Using a saliency criterion that measures expected routed contribution of each expert (REAP), we pruned Qwen3-Coder-480B to 363B (25% pruning) and 246B (50% pruning), all in FP8. At 25%, accuracy degradation is minimal across a suite of benchmarks. More on arXiv: &lt;a href="https://arxiv.org/abs/2510.13999"&gt;https://arxiv.org/abs/2510.13999&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let us know which models we should prune next in the comments!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vuu82b8sjbwf1.png?width=6539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8a064e15281f6e830e724e70d86a1b46721dc3"&gt;https://preview.redd.it/vuu82b8sjbwf1.png?width=6539&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc8a064e15281f6e830e724e70d86a1b46721dc3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:35:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1obrvab</id>
    <title>Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp</title>
    <updated>2025-10-20T19:54:08+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"&gt; &lt;img alt="Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp" src="https://external-preview.redd.it/n2_CIH2NdPrPVJO7RzSAqCKKA-IjXoFSmGm_ZeORNmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d670894b141c1fd2de6b21248ecab346ff0c897" title="Support for Ling and Ring models (1000B/103B/16B) has finally been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been following this PR for over a month because it adds support for some interesting MoE, the 103B size sounds cool&lt;/p&gt; &lt;p&gt;1T models:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;https://huggingface.co/inclusionAI/Ring-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-1T"&gt;https://huggingface.co/inclusionAI/Ling-1T&lt;/a&gt;&lt;/p&gt; &lt;p&gt;103B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ring-flash-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;16B models&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0"&gt;https://huggingface.co/inclusionAI/Ring-mini-2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;https://huggingface.co/inclusionAI/Ling-mini-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16063"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obrvab/support_for_ling_and_ring_models_1000b103b16b_has/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:54:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn0q7</id>
    <title>The Innovations in DeepSeek OCR</title>
    <updated>2025-10-20T16:29:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek just released a pretty shocking new paper. They really buried the lede here by referring to it simply as DeepSeek OCR. &lt;/p&gt; &lt;p&gt;While it’s a very strong OCR model, the purpose of it and the implications of their approach go far beyond what you’d expect of “yet another OCR model.”&lt;/p&gt; &lt;p&gt;Traditionally, vision LLM tokens almost seemed like an afterthought or “bolt on” to the LLM paradigm. And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens.&lt;/p&gt; &lt;p&gt;So those 10k words may have turned into 15k tokens, or 30k to 60k “visual tokens.” So vision tokens were way less efficient and really only made sense to use for data that couldn’t be effectively conveyed with words. &lt;/p&gt; &lt;p&gt;But that gets inverted now from the ideas in this paper. DeepSeek figured out how to get 10x better compression using vision tokens than with text tokens! So you could theoretically store those 10k words in just 1,500 of their special compressed visual tokens.&lt;/p&gt; &lt;p&gt;This might not be as unexpected as it sounds if you think of how your own mind works. After all, I know that when I’m looking for a part of a book that I’ve already read, I imagine it visually and always remember which side of the book it was on and approximately where on the page it was, which suggests some kind of visual memory representation at work.&lt;/p&gt; &lt;p&gt;Now, it’s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM; can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality? &lt;/p&gt; &lt;p&gt;But you can imagine that, depending on the exact tradeoffs, it could be a very exciting new axis to greatly expand effective context sizes. Especially when combined with DeepSeek’s other recent paper from a couple weeks ago about sparse attention.&lt;/p&gt; &lt;p&gt;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks. If they did, they probably wouldn’t say because it would be viewed as an important trade secret.&lt;/p&gt; &lt;p&gt;But the nice thing about DeepSeek is that they’ve made the entire thing open source and open weights and explained how they did it, so now everyone can try it out and explore.&lt;/p&gt; &lt;p&gt;Even if these tricks make attention more lossy, the potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting. &lt;/p&gt; &lt;p&gt;You could basically cram all of a company’s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective. &lt;/p&gt; &lt;p&gt;Or put an entire code base into the context and cache it, and then just keep appending the equivalent of the git diffs as you make changes to the code. &lt;/p&gt; &lt;p&gt;If you’ve ever read stories about the great physicist Hans Bethe, he was known for having vast amounts of random physical facts memorized (like the entire periodic table; boiling points of various substances, etc.) so that he could seamlessly think and compute without ever having to interrupt his flow to look something up in a reference table. &lt;/p&gt; &lt;p&gt;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;https://x.com/doodlestein/status/1980282222893535376&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
