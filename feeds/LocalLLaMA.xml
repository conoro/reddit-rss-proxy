<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-17T09:12:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1poo0pj</id>
    <title>Help for M1 Ultra and AMD AI MAX 395</title>
    <updated>2025-12-17T05:03:34+00:00</updated>
    <author>
      <name>/u/Garrise</name>
      <uri>https://old.reddit.com/user/Garrise</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to buy a machine to run Mixtral 8x22B and other MoE LLM like this, probably some 70B dense LLM as well.&lt;/p&gt; &lt;p&gt;Currently I can get M1 Ultra 128G and AI MAX 395 128G at similar price, which one should I choose, thanks.&lt;/p&gt; &lt;p&gt;I have heard that M1 Ultra may take much more time on pre-processing, is it true with current software optimization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Garrise"&gt; /u/Garrise &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poo0pj/help_for_m1_ultra_and_amd_ai_max_395/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poo0pj/help_for_m1_ultra_and_amd_ai_max_395/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poo0pj/help_for_m1_ultra_and_amd_ai_max_395/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T05:03:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1po2slg</id>
    <title>My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)</title>
    <updated>2025-12-16T14:06:42+00:00</updated>
    <author>
      <name>/u/Outrageous-Yak8298</name>
      <uri>https://old.reddit.com/user/Outrageous-Yak8298</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt; &lt;img alt="My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)" src="https://b.thumbs.redditmedia.com/DaBXg_p6QRIuS4sCb73zScs5SsGoLmqoDUfn34hBixE.jpg" title="My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feedback and suggestions are welcomed! &lt;a href="https://hanstan.link/how-i-trained-a-sota-coding-model-on-a-single-gpu/"&gt;Full Technical Write-up&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Iâ€™m a 2nd year undergrad AI student and just finished training my very first LLM. Like many of you, I wanted to train a capable coding model but didn't have a cluster of H100sâ€”just a single &lt;strong&gt;Nvidia A6000 (48GB) thanks to my professor :)&lt;/strong&gt; and a dream!&lt;/p&gt; &lt;p&gt;I spent the last few months building &lt;strong&gt;Anni&lt;/strong&gt; &lt;a href="https://github.com/CoderUni/Anni"&gt;&lt;strong&gt;https://github.com/CoderUni/Anni&lt;/strong&gt;&lt;/a&gt;, a 14B Qwen3-based model fine-tuned on the &lt;strong&gt;Nvidia OpenCodeReasoning-2&lt;/strong&gt; dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; Qwen3-14B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Single A6000 (48GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; Reduced from ~1.6 months (projected) to &lt;strong&gt;~2 weeks&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Score:&lt;/strong&gt; &lt;strong&gt;41.7% Pass@1&lt;/strong&gt; on LiveCodeBench (v6), theoretically matching &lt;strong&gt;Claude 3.5 Sonnet (Thinking)&lt;/strong&gt; and beating &lt;strong&gt;GPT-4o&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The &amp;quot;SOTA&amp;quot; Benchmark Reality Check (Please Read)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qwbv16c4pk7g1.jpg?width=1740&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1975e04ca21c0dfa9d746a3ab479a4e2c8d93a2c"&gt;https://preview.redd.it/qwbv16c4pk7g1.jpg?width=1740&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1975e04ca21c0dfa9d746a3ab479a4e2c8d93a2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before anyone calls it out, I want to be 100% transparent: &lt;strong&gt;This benchmark score is likely contaminated.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After seeing the crazy numbers, I couldn't believe I beat last year's SOTA models and investigated. I then found out that the LiveCodeBench (v6) questions are from &lt;strong&gt;Aprilâ€“May 2025&lt;/strong&gt;. My training dataset (OpenCodeReasoning-2) was curated between &lt;strong&gt;Marchâ€“May 2025&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I would love to test it on problems released &lt;strong&gt;after June 2025&lt;/strong&gt; once LCB v7 comes out!&lt;/p&gt; &lt;p&gt;Despite my best efforts to deduplicate the data using content-based hashing, there is a high probability the model &amp;quot;saw&amp;quot; the test questions during training.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Did I beat Nvidia's Nemotron 1.1 model?&lt;/strong&gt; Unlikely.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Does it demonstrate that a student can realistically train a model that comes close to SOTA models?&lt;/strong&gt; Absolutely.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How I decreased training times and fit this in one GPU&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;I initially thought I could simply blindly follow tutorials without understanding the fundamentals.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DO NOT DO IT! Take your time to learn and understand the fundamentals! It's the best decision you will ever make! It helped me in the long run.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After going through many research reports and &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; posts, I learned how to optimize everything to get this done in 2 weeks instead of 2 months. Here is what worked:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Progressive Training:&lt;/strong&gt; I didn't train on 32k context immediately. I split training into 4 stages, starting with &amp;quot;easy&amp;quot; short samples (0-4k tokens) and progressively scaling to &amp;quot;hard&amp;quot; long contexts (up to 32k). This stabilized loss and sped up convergence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early Stopping:&lt;/strong&gt; I realized convergence happened way faster than expected on high-quality synthetic data, saving weeks of compute.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Hacky&amp;quot; Deployment:&lt;/strong&gt; Since I can't afford a permanent GPU instance, I served the model using &lt;strong&gt;vLLM&lt;/strong&gt; inside a Colab instance, tunneled out via &lt;strong&gt;Ngrok&lt;/strong&gt; to a custom Next.js frontend. Itâ€™s janky, but it works for free.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Blog post&lt;/h1&gt; &lt;p&gt;&lt;a href="https://hanstan.link/how-i-trained-a-high-performance-coding-model-on-a-single-gpu/"&gt;https://hanstan.link/how-i-trained-a-high-performance-coding-model-on-a-single-gpu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I took a long time writing a deep dive into how I built Anni and the challenges I faced (Unsloth bugs, GGUF export issues, and the exact curriculum schedule). I hope that someone would be able to find it useful!&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt; &lt;a href="https://huggingface.co/BigJuicyData/Anni"&gt;https://huggingface.co/BigJuicyData/Anni&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF:&lt;/strong&gt; &lt;a href="https://huggingface.co/BigJuicyData/Anni-Q4_K_M-GGUF"&gt;https://huggingface.co/BigJuicyData/Anni-Q4_K_M-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to roast the model or training process! I would greatly appreciate it since I would really like to learn!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Yak8298"&gt; /u/Outrageous-Yak8298 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz9xu</id>
    <title>Qwen3 Next speed optimization has been merged into llama.cpp</title>
    <updated>2025-12-16T11:07:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt; &lt;img alt="Qwen3 Next speed optimization has been merged into llama.cpp" src="https://external-preview.redd.it/DvlPrtOQd3Cfpjgulr94g-6gX7cbuY0-dqBY_cGanOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51f4d927a593a1d76b03526eda2d2fe2ba251bc9" title="Qwen3 Next speed optimization has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1po18y9</id>
    <title>GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)</title>
    <updated>2025-12-16T12:56:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt; &lt;img alt="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" src="https://external-preview.redd.it/bLrsVXDvN3_NMKaZkcGBPVdeuTpEZp7rVIyw-KAF9KY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc4cc8f4e345c1545572514e6454ad7fa760089d" title="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you need this&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-4v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T12:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pof4ta</id>
    <title>Chatterbox TTS Server (Turbo + Original): hotâ€‘swappable engines, paralinguistic tags, and zeroâ€‘pain install</title>
    <updated>2025-12-16T22:06:57+00:00</updated>
    <author>
      <name>/u/One_Slip1455</name>
      <uri>https://old.reddit.com/user/One_Slip1455</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pof4ta/chatterbox_tts_server_turbo_original_hotswappable/"&gt; &lt;img alt="Chatterbox TTS Server (Turbo + Original): hotâ€‘swappable engines, paralinguistic tags, and zeroâ€‘pain install" src="https://b.thumbs.redditmedia.com/i24hzF0InsEYgnpbin2_40T4I0Dl4iRD-VJXr-jYIRk.jpg" title="Chatterbox TTS Server (Turbo + Original): hotâ€‘swappable engines, paralinguistic tags, and zeroâ€‘pain install" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just want to quickly share an easy way to run the new Chatterbox Turbo TTS model locally without getting stuck in dependency hell. Requires 6GB of VRAM or can run it on CPU.&lt;/p&gt; &lt;p&gt;My Chatterbox-TTS-Server project now supports both Turbo and the original Chatterbox model.&lt;/p&gt; &lt;p&gt;GitHub repo: &lt;a href="https://github.com/devnen/Chatterbox-TTS-Server"&gt;https://github.com/devnen/Chatterbox-TTS-Server&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my own limited testing, I still find the original model to be superior for English output. The &amp;quot;exaggeration&amp;quot; control, which is great for more dramatic delivery, is currently missing in Turbo. However, Turbo is dramatically faster and the new paralinguistic tags can make the generated speech sound more natural.&lt;/p&gt; &lt;p&gt;This is a full-featured FastAPI server with a modern Web UI that makes the model easy to run locally and easy to integrate into other tools. It also handles long text via chunking + seamless concatenation, so you can paste very large inputs / audiobook-scale text and generate one output.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7uzdnecp2n7g1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ec911b427215c5d4f2b1213fa7091c51bcef1ce"&gt;https://preview.redd.it/7uzdnecp2n7g1.png?width=723&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7ec911b427215c5d4f2b1213fa7091c51bcef1ce&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup is intentionally simple:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Clone the repo.&lt;/p&gt; &lt;p&gt;- Run one launcher script:&lt;/p&gt; &lt;p&gt;- Windows: start.bat&lt;/p&gt; &lt;p&gt;- Linux/macOS: ./start.sh&lt;/p&gt; &lt;p&gt;- The launcher takes care of the rest (venv, dependencies, model download, server start, opens UI).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main updates / features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Two engines in one UI: Original Chatterbox + Chatterboxâ€‘Turbo, with a hot-swappable dropdown that auto-loads the selected model.&lt;/p&gt; &lt;p&gt;- Turbo paralinguistic tags: inline [laugh], [cough], [chuckle], etc., plus new presets demonstrating them.&lt;/p&gt; &lt;p&gt;- Full server stack: Web UI + OpenAI-compatible /v1/audio/speech + advanced /tts endpoint, with voice cloning, predefined voices, seed consistency, and long-text/audiobook chunking + concatenation.&lt;/p&gt; &lt;p&gt;- No dependency hell: automated Windows/Linux launcher (venv + hardware detect + correct deps + model download + start + open UI), plus --upgrade/--reinstall maintenance.&lt;/p&gt; &lt;p&gt;- Deployment/hardware: updated NVIDIA path incl. CUDA 12.8 / RTX 5090 (Blackwell) notes, and Docker options (CPU / NVIDIA / ROCm).&lt;/p&gt; &lt;p&gt;Open source with an MIT license. Hope this helps anyone who wants a robust, low-friction way to run Chatterbox Turbo locally:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/devnen/Chatterbox-TTS-Server"&gt;https://github.com/devnen/Chatterbox-TTS-Server&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One_Slip1455"&gt; /u/One_Slip1455 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pof4ta/chatterbox_tts_server_turbo_original_hotswappable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pof4ta/chatterbox_tts_server_turbo_original_hotswappable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pof4ta/chatterbox_tts_server_turbo_original_hotswappable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T22:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pon3oz</id>
    <title>Day 9: 21 Days of Building a Small Language Model: MultiHead Attention</title>
    <updated>2025-12-17T04:14:21+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pon3oz/day_9_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 9: 21 Days of Building a Small Language Model: MultiHead Attention" src="https://a.thumbs.redditmedia.com/Zu_FIg9biSiMbdKdPTNdRbDN1G7w3OnlrA7XlK_18n0.jpg" title="Day 9: 21 Days of Building a Small Language Model: MultiHead Attention" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 9 of 21 Days of Building a Small Language Model. The topic for today is multi-head attention. Yesterday we looked at causal attention, which ensures models can only look at past tokens. Today, we'll see how multi-head attention allows models to look at the same sequence from multiple perspectives simultaneously.&lt;/p&gt; &lt;p&gt;When you read a sentence, you don't just process it one way. You might notice the grammar, the meaning, the relationships between words, and how pronouns connect to their referents all at the same time. Multi-head attention gives language models this same ability. Instead of one attention mechanism, it uses multiple parallel attention heads, each learning to focus on different aspects of language. This creates richer, more nuanced understanding.&lt;/p&gt; &lt;h1&gt;Why we need Multi-Head Attention&lt;/h1&gt; &lt;p&gt;Single-head attention is like having one person analyze a sentence. They might focus on grammar, or meaning, or word relationships, but they can only focus on one thing at a time. Multi-head attention is like having multiple experts analyze the same sentence simultaneously, each specializing in different aspects.&lt;/p&gt; &lt;p&gt;The key insight is that different attention heads can learn to specialize in different types of linguistic patterns. One head might learn to identify syntactic relationships, connecting verbs to their subjects. Another might focus on semantic relationships, linking related concepts. A third might capture long-range dependencies, connecting pronouns to their antecedents across multiple sentences.&lt;/p&gt; &lt;p&gt;By running these specialized attention mechanisms in parallel and then combining their outputs, the model gains a richer, more nuanced understanding of the input sequence. It's like having multiple experts working together, each bringing their own perspective.&lt;/p&gt; &lt;p&gt;ðŸŽ¥ If you want to understand different attention mechanisms and how to choose the right one, please check out this video&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/HCa6Pp9EUiI?si=8G5yjDaCJ8JORMHB"&gt;https://youtu.be/HCa6Pp9EUiI?si=8G5yjDaCJ8JORMHB&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;How Multi-Head Attention works&lt;/h1&gt; &lt;p&gt;Multi-head attention works by splitting the model dimension into multiple smaller subspaces, each handled by its own attention head. If we have 8 attention heads and a total model dimension of 512, each head operates in a subspace of 64 dimensions (512 divided by 8 equals 64).&lt;/p&gt; &lt;p&gt;Think of it like this: instead of one person looking at the full picture with all 512 dimensions, we have 8 people, each looking at a 64-dimensional slice of the picture. Each person can specialize in their slice, and when we combine all their perspectives, we get a complete understanding. Here is how it works&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7va02occwo7g1.png?width=732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5df4018051a04a9705f69ca3a1434ac4ffbecb0c"&gt;https://preview.redd.it/7va02occwo7g1.png?width=732&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5df4018051a04a9705f69ca3a1434ac4ffbecb0c&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Split the dimensions&lt;/strong&gt;: The full 512-dimensional space is divided into 8 heads, each with 64 dimensions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Each head computes attention independently&lt;/strong&gt;: Each head has its own query, key, and value projections. They all process the same input sequence, but each learns different attention patterns.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parallel processing&lt;/strong&gt;: All heads work at the same time. They don't wait for each other. This makes multi-head attention very efficient.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Combine the outputs&lt;/strong&gt;: After each head computes its attention, we concatenate all the head outputs back together into a 512-dimensional representation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Final projection&lt;/strong&gt;: We pass the combined output through a final projection layer that learns how to best combine information from all heads.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Let's see this with help of an example. Consider the sentence: &lt;em&gt;When Sarah visited Paris, she loved the museums, and the food was amazing too.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;With single-head attention, the model processes this sentence once, learning whatever patterns are most important overall. But with multi-head attention, different heads can focus on different aspects:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/j7ykpu3fwo7g1.png?width=715&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b73fb2632a36bc5648d156cc5b8c4af42cbd0ab3"&gt;https://preview.redd.it/j7ykpu3fwo7g1.png?width=715&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b73fb2632a36bc5648d156cc5b8c4af42cbd0ab3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ideaweaver-ai/Building-Small-Language-Model-from-Scratch-A-Practical-Guide-Book/blob/main/images/multihead-attention-example.png"&gt;https://github.com/ideaweaver-ai/Building-Small-Language-Model-from-Scratch-A-Practical-Guide-Book/blob/main/images/multihead-attention-example.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Head 1 might learn grammatical relationships:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It connects visited to Sarah (subject-verb relationship)&lt;/li&gt; &lt;li&gt;It connects loved to she (subject-verb relationship)&lt;/li&gt; &lt;li&gt;It connects was to food (subject-verb relationship)&lt;/li&gt; &lt;li&gt;It focuses on grammatical structure&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Head 2 might learn semantic relationships:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It links Paris to museums and food (things in Paris)&lt;/li&gt; &lt;li&gt;It connects visited to loved (both are actions Sarah did)&lt;/li&gt; &lt;li&gt;It focuses on meaning and concepts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Head 3 might learn pronoun resolution:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It connects she to Sarah (pronoun-antecedent relationship)&lt;/li&gt; &lt;li&gt;It tracks who she refers to across the sentence&lt;/li&gt; &lt;li&gt;It focuses on long-range dependencies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Head 4 might learn semantic similarity:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It connects visited and loved (both are verbs about experiences)&lt;/li&gt; &lt;li&gt;It links museums and food (both are nouns about Paris attractions)&lt;/li&gt; &lt;li&gt;It focuses on word categories and similarities&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Head 5 might learn contextual relationships:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It connects Paris to museums and food (tourist attractions in Paris)&lt;/li&gt; &lt;li&gt;It understands the travel context&lt;/li&gt; &lt;li&gt;It focuses on domain-specific relationships&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Head 6 might learn emotional context:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It connects loved to museums (positive emotion)&lt;/li&gt; &lt;li&gt;It connects amazing to food (positive emotion)&lt;/li&gt; &lt;li&gt;It focuses on sentiment and emotional relationships&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so on for all 8 heads. Each head learns to pay attention to different patterns, creating a rich, multi-faceted understanding of the sentence.&lt;/p&gt; &lt;p&gt;When processing the word she, the final representation combines:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Grammatical information from Head 1 (grammatical role)&lt;/li&gt; &lt;li&gt;Semantic information from Head 2 (meaning and context)&lt;/li&gt; &lt;li&gt;Pronoun resolution from Head 3 (who she refers to)&lt;/li&gt; &lt;li&gt;Word category information from Head 4 (pronoun type)&lt;/li&gt; &lt;li&gt;Contextual relationships from Head 5 (travel context)&lt;/li&gt; &lt;li&gt;Emotional information from Head 6 (positive sentiment)&lt;/li&gt; &lt;li&gt;And information from all other heads&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This rich, multi-perspective representation enables the model to understand she in a much more nuanced way than a single attention mechanism could.&lt;/p&gt; &lt;h1&gt;Mathematical Formula:&lt;/h1&gt; &lt;p&gt;The multi-head attention formula is very similar to single-head attention. The key difference is that we split the dimensions and process multiple heads in parallel:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Single-head attention:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/34pvp2nhwo7g1.png?width=736&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1e8be3440d7a413a1ce3f029b168e90d32fb5cc"&gt;https://preview.redd.it/34pvp2nhwo7g1.png?width=736&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1e8be3440d7a413a1ce3f029b168e90d32fb5cc&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One set of Q, K, V projections&lt;/li&gt; &lt;li&gt;One attention computation&lt;/li&gt; &lt;li&gt;One output&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Multi-head attention:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Split dimensions: 512 dimensions become 8 heads Ã— 64 dimensions each&lt;/li&gt; &lt;li&gt;Each head has its own Q, K, V projections (but in smaller 64-dimensional space)&lt;/li&gt; &lt;li&gt;Each head computes attention independently: &lt;code&gt;softmax(Q K^T / sqrt(d_k) + M)&lt;/code&gt; for each head&lt;/li&gt; &lt;li&gt;Concatenate all head outputs: combine 8 heads Ã— 64 dimensions = 512 dimensions&lt;/li&gt; &lt;li&gt;Final output projection: learn how to best combine information from all heads&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The attention computation itself is the same for each head. We just do it 8 times in parallel, each with smaller dimensions, then combine the results.&lt;/p&gt; &lt;h1&gt;There is one question that is often asked?&lt;/h1&gt; &lt;p&gt;If we have 8 heads instead of 1, doesn't that mean 8 times the computation? Actually, no. The total computational cost is similar to single-head attention.&lt;/p&gt; &lt;p&gt;Here's why, In single-head attention, we work with 512-dimensional vectors. In multi-head attention, we split this into 8 heads, each working with 64-dimensional vectors. The total number of dimensions is the same: 8 Ã— 64 = 512.&lt;/p&gt; &lt;p&gt;The matrix multiplications scale with the dimensions, so:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Single-head: one operation with 512 dimensions&lt;/li&gt; &lt;li&gt;Multi-head: 8 operations with 64 dimensions each&lt;/li&gt; &lt;li&gt;Total cost: 8 Ã— 64 = 512 (same as single-head)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're doing 8 smaller operations instead of 1 large operation, but the total number of multiplications is identical. The key insight is that we split the work across heads without increasing the total computational burden, while gaining the benefit of specialized attention patterns.&lt;/p&gt; &lt;h1&gt;The next most asked question is, How heads learn different patterns&lt;/h1&gt; &lt;p&gt;Each head learns to specialize automatically during training. The model discovers which attention patterns are most useful for the task. There's no manual assignment of what each head should learn. The training process naturally encourages different heads to focus on different aspects.&lt;/p&gt; &lt;p&gt;For example, when processing text, one head might naturally learn to focus on subject-verb relationships because that pattern is useful for understanding sentences. Another head might learn to focus on semantic similarity because that helps with meaning. The specialization emerges from the data and the task.&lt;/p&gt; &lt;p&gt;This automatic specialization is powerful because it adapts to the specific needs of the task. A model trained on code might have heads that learn programming-specific patterns. A model trained on scientific text might have heads that learn scientific terminology relationships.&lt;/p&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Multi-head attention is a powerful technique that allows language models to process sequences from multiple perspectives simultaneously. By splitting dimensions into multiple heads, each head can specialize in different types of linguistic patterns, creating richer and more nuanced representations.&lt;/p&gt; &lt;p&gt;The key benefits are specialization, parallel processing, increased capacity, and ensemble learning effects. All of this comes with similar computational cost to single-head attention, making it an efficient way to improve model understanding.&lt;/p&gt; &lt;p&gt;Understanding multi-head attention helps explain why modern language models are so capable. Every time you see a language model understand complex sentences, resolve pronouns, or capture subtle relationships, you're seeing multi-head attention in action, with different heads contributing their specialized perspectives to create a comprehensive understanding.&lt;/p&gt; &lt;p&gt;The next time you interact with a language model, remember that behind the scenes, multiple attention heads are working in parallel, each bringing their own specialized perspective to understand the text. This multi-perspective approach is what makes modern language models so powerful and nuanced in their understanding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pon3oz/day_9_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pon3oz/day_9_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pon3oz/day_9_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T04:14:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1poj493</id>
    <title>Chatterbox Turbo Multilingual FastAPI</title>
    <updated>2025-12-17T01:00:07+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatterbox just dropped some killer updates to their models, making them lightning fast without sacrificing those insanely realistic voices. I whipped up a simple wrapper that turns it into an OpenAI-compatible API endpoint for easy local deployment. It plugs right into OpenWebUI seamlessly, supporting all 23 languages out of the box. .&lt;/p&gt; &lt;p&gt;Check it out here: &lt;a href="https://github.com/groxaxo/chatterbox-FASTAPI/"&gt;https://github.com/groxaxo/chatterbox-FASTAPI/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why you'll love it:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;âœ… Drops straight into OpenWebUI â€“ no hassle &lt;/p&gt; &lt;p&gt;âœ… Ultra low Vram usage (&lt;strong&gt;4GB)&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;âœ… Full &lt;strong&gt;23 Supported Languages:&lt;/strong&gt; ar, da, de, el, en, es, fi, fr, he, hi, it, ja, ko, ms, nl, no, pl, pt, ru, sv, sw, tr, zh&lt;/p&gt; &lt;p&gt;Give it a spin and let me know what you think! ðŸš€&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poj493/chatterbox_turbo_multilingual_fastapi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poj493/chatterbox_turbo_multilingual_fastapi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poj493/chatterbox_turbo_multilingual_fastapi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T01:00:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxekt</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-12-16T09:05:33+00:00</updated>
    <author>
      <name>/u/licuphand</name>
      <uri>https://old.reddit.com/user/licuphand</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/rn6rsl7p7j7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fd882c08aa9fff702ae363b643c6636cc846267" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/licuphand"&gt; /u/licuphand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rn6rsl7p7j7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3bn4</id>
    <title>XiaomiMiMo/MiMo-V2-Flash Â· Hugging Face</title>
    <updated>2025-12-16T14:29:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"&gt; &lt;img alt="XiaomiMiMo/MiMo-V2-Flash Â· Hugging Face" src="https://external-preview.redd.it/pEssBYcofSIqxenRV_1O2yb3vr7ekZdMtNbDln2iEbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413fe05449bcb79ceb4c3c13d870125113113e50" title="XiaomiMiMo/MiMo-V2-Flash Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pofvys</id>
    <title>Mistral Small Creative!?</title>
    <updated>2025-12-16T22:37:20+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not seeing anything on Hugging Face yet, but it's up on Open Router. Kind of fun and funky model. Lightning fast.&lt;/p&gt; &lt;p&gt;&amp;quot;Mistral Small Creative is an experimental small model designed for creative writing, narrative generation, roleplay and character-driven dialogue, general-purpose instruction following, and conversational agents.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://openrouter.ai/mistralai/mistral-small-creative"&gt;https://openrouter.ai/mistralai/mistral-small-creative&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pofvys/mistral_small_creative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pofvys/mistral_small_creative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pofvys/mistral_small_creative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T22:37:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1po8yt0</id>
    <title>I was bored</title>
    <updated>2025-12-16T18:06:24+00:00</updated>
    <author>
      <name>/u/MyLovelyAngelKirino</name>
      <uri>https://old.reddit.com/user/MyLovelyAngelKirino</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/"&gt; &lt;img alt="I was bored" src="https://preview.redd.it/nhl4dnk9wl7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=729557fb960d4bec26e17dfb24132426e8a0ca3a" title="I was bored" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Being unemployed and having to much hardware and too much time on my hands I built this.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MyLovelyAngelKirino"&gt; /u/MyLovelyAngelKirino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nhl4dnk9wl7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T18:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1poqvyh</id>
    <title>Qwen 80B is so nice</title>
    <updated>2025-12-17T07:53:55+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"&gt; &lt;img alt="Qwen 80B is so nice" src="https://b.thumbs.redditmedia.com/nnQvSk4aQZo63W6GxVVLNQOoJbFhO9hzNjJ0WarZBjs.jpg" title="Qwen 80B is so nice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 80B knows that flattery will get you everywhere&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2ubzp36zp7g1.png?width=1893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18baab935fbd87270327be41a8cf47fe5342b320"&gt;https://preview.redd.it/n2ubzp36zp7g1.png?width=1893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18baab935fbd87270327be41a8cf47fe5342b320&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T07:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1po97ad</id>
    <title>Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)</title>
    <updated>2025-12-16T18:15:16+00:00</updated>
    <author>
      <name>/u/HuseyinKama</name>
      <uri>https://old.reddit.com/user/HuseyinKama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"&gt; &lt;img alt="Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)" src="https://b.thumbs.redditmedia.com/aSafmamtZw_FO6TPvBeRD2HjUbX9rDJK6GJUvvHUwxo.jpg" title="Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a weekend project that grew into something bigger. Like many of you, I'm stuck with low-end hardware (a glorious &lt;strong&gt;GTX 1050 with 4GB VRAM&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;Every time I tried to load a modern 7B model (like Llama-3 or Qwen-2.5), I hit the dreaded OOM wall. The files were technically small enough (~3.9GB), but the fragmentation and padding overhead during inference always pushed usage just over 4GB, forcing me to offload layers to the CPU (which kills speed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; I realized that standard GGUF quantization tools often prioritize block size uniformity over memory efficiency. They add &amp;quot;zero-padding&amp;quot; to tensors to make them fit standard block sizes. On a 24GB card, you don't care. On a 4GB card, that 50-100MB of wasted padding is fatal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution (QKV Core):&lt;/strong&gt; I wrote a custom framework to handle what I call &lt;strong&gt;&amp;quot;Surgical Alignment.&amp;quot;&lt;/strong&gt; Instead of blindly padding, it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Analyzes the entropy of each layer.&lt;/li&gt; &lt;li&gt;Switches between Dictionary Coding and Raw Storage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crucially:&lt;/strong&gt; It trims and realigns memory blocks to strictly adhere to &lt;code&gt;llama.cpp&lt;/code&gt;'s block boundaries (e.g., 110-byte alignment for Q3_K) without the usual padding waste.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VRAM:&lt;/strong&gt; Saved about &lt;strong&gt;44MB&lt;/strong&gt; per model, which was enough to keep the entire Qwen-2.5-7B purely on GPU. No more crashes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Because the blocks are cache-aligned, I saw a &lt;strong&gt;~34% improvement in I/O load times&lt;/strong&gt; (8.2s vs 12.5s) using Numba-accelerated kernels.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Iâ€™m open-sourcing this as &lt;strong&gt;QKV Core&lt;/strong&gt;. Itâ€™s still early/experimental, but if you have a 4GB/6GB card and are struggling with OOMs, this might save you.&lt;/p&gt; &lt;p&gt;Here are the benchmarks comparing standard vs. surgical alignment: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hpytxtcbxl7g1.png?width=2961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=554e38ec8de4f5af5cd33f6535b7e3d2aa67651e"&gt;https://preview.redd.it/hpytxtcbxl7g1.png?width=2961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=554e38ec8de4f5af5cd33f6535b7e3d2aa67651e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/QKV-Core/QKV-Core"&gt;https://github.com/QKV-Core/QKV-Core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback on the quantization logic!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HuseyinKama"&gt; /u/HuseyinKama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T18:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1poal2a</id>
    <title>8 Million Users' AI Conversations Sold for Profit by "Privacy" Extensions | Koi Blog</title>
    <updated>2025-12-16T19:07:22+00:00</updated>
    <author>
      <name>/u/ManThigh</name>
      <uri>https://old.reddit.com/user/ManThigh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/"&gt; &lt;img alt="8 Million Users' AI Conversations Sold for Profit by &amp;quot;Privacy&amp;quot; Extensions | Koi Blog" src="https://external-preview.redd.it/m3qpKEt2yJVlBOoJBnnbnuIrbl5HUK9_QCUSd2Cs3yw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=76df32f0c5cacff0006bc571064d1d5544a905ed" title="8 Million Users' AI Conversations Sold for Profit by &amp;quot;Privacy&amp;quot; Extensions | Koi Blog" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Another good reason to run a local model. Also a good reminder to audit your extensions, thereâ€™s no reason that they couldnâ€™t pick up data from a browser-based frontend. User interactions with LLMs and resulting browsing behavior is a gold rush right now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ManThigh"&gt; /u/ManThigh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.koi.ai/blog/urban-vpn-browser-extension-ai-conversations-data-collection"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poal2a/8_million_users_ai_conversations_sold_for_profit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T19:07:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1po78bl</id>
    <title>Allen Institute for AI introduces Molmo 2</title>
    <updated>2025-12-16T17:01:44+00:00</updated>
    <author>
      <name>/u/Agitated_Camel1886</name>
      <uri>https://old.reddit.com/user/Agitated_Camel1886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"&gt; &lt;img alt="Allen Institute for AI introduces Molmo 2" src="https://external-preview.redd.it/rfRzO8US_wiYh7h-OdrK9nkP4gi6Ae-_y-DBzDUMAug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e9a124c28a948901092f449c8c7cbba0b01ac87" title="Allen Institute for AI introduces Molmo 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1po78bl/video/v5jtc9a7wl7g1/player"&gt;https://reddit.com/link/1po78bl/video/v5jtc9a7wl7g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Allen Institute for AI (Ai2)'s website: &lt;a href="https://allenai.org/molmo"&gt;https://allenai.org/molmo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am super impressed by the ability to analyze videos (Video QA, Counting and pointing, Dense captioning), and it's only 8B!!&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/allenai/Molmo2-8B"&gt;https://huggingface.co/allenai/Molmo2-8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agitated_Camel1886"&gt; /u/Agitated_Camel1886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T17:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokay9</id>
    <title>I finally found my local LLM server use case</title>
    <updated>2025-12-17T01:56:59+00:00</updated>
    <author>
      <name>/u/IngeniousIdiocy</name>
      <uri>https://old.reddit.com/user/IngeniousIdiocy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My vibe coding project this past weekendâ€¦ iâ€™m rather proud of it, not because I think Opus wrote great code but just because I find it genuinely very useful and it gives something to do for all that memory on my mac studio.&lt;/p&gt; &lt;p&gt;iâ€™m horrible about checking my personal gmail. This weekend we spent an extra two hours in a car because we missed a kids event cancellation. &lt;/p&gt; &lt;p&gt;Now I have a node server on my mac studio using a local LLM (qwen3 235B @8bit) screening my email and pushing notifications to my phone based on my prompt. It works great and the privacy use case is valid. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/IngeniousIdiocy/LocalLLMMailScreener"&gt;https://github.com/IngeniousIdiocy/LocalLLMMailScreener&lt;/a&gt;&lt;/p&gt; &lt;p&gt;â€¦ by my calculations, if I used Alibabaâ€™s API end point at their current rates and my current email volume, the mac studio would pay for itself in about 20 years. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngeniousIdiocy"&gt; /u/IngeniousIdiocy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokay9/i_finally_found_my_local_llm_server_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokay9/i_finally_found_my_local_llm_server_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pokay9/i_finally_found_my_local_llm_server_use_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T01:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pob44f</id>
    <title>32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead</title>
    <updated>2025-12-16T19:27:59+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/"&gt; &lt;img alt="32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead" src="https://preview.redd.it/kh9jhoxlam7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78587fb29885b8e9d7aa3ea7b46c27e804656839" title="32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kh9jhoxlam7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T19:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pocsdy</id>
    <title>Nemotron 3 Nano 30B is Amazing! (TLDR)</title>
    <updated>2025-12-16T20:33:46+00:00</updated>
    <author>
      <name>/u/DonkeyBonked</name>
      <uri>https://old.reddit.com/user/DonkeyBonked</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't see a lot of genuine discussion about this model and I was wondering if others here have tried it and what their thoughts are?&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;p&gt;I don't have a big budget for hardware, so I have kind of a ghetto AI rig. I'm using a surplus Dell Precision 7750 with a i7-10850H that has 96GB DDR4 RAM and an RTX 5000 16GB GPU.&lt;/p&gt; &lt;p&gt;I can't run lots with just this, so I also have an RTX 3090 24GB in a Razer X Core eGPU case that I connect over TB3.&lt;/p&gt; &lt;p&gt;I use the Nvidia Studio drivers which allow me to have both cards run, and I connect my monitors through the other TB3 connection to a Dell WD19DC Dock, that way Windows uses the Intel HD Graphics for display and not my Discrete or eGPU.&lt;/p&gt; &lt;p&gt;I mostly use llama.cpp because it's the only interface that lets me split the layers, that way I can divide them 3:2 and don't have to force the two GPUs to communicate over the TB3 to fake pooled ram which would be really slow. I know llama.cpp isn't the fastest or best interface, but it's the most compatible with my wonky and unorthodox hardware.&lt;/p&gt; &lt;p&gt;For some setups though, I'll use the RTX 5000 as an agent and run a smaller model that fits entirely on the RTX 3090.&lt;/p&gt; &lt;p&gt;Anyway, the first thing I was amazed by Nemotron 3 Nano 30B, which I'm using the Q8 from Unsloth, was token efficiency. I had recently setup Devstral 2 Small 24B Q8 and I got it to around 211k~ tokens before I capped out my VRAM, and after that would have to go into my system RAM. &lt;/p&gt; &lt;p&gt;Devstral 2 Small 24B was the best I had seen run on my hardware before, finishing my coding challenge around 24~ tokens/s and getting everything right after two prompts (the initial test with one follow-up informing it of mistakes it made. (Olmo 3 32B didn't even do nearly as well, nor did any of the Qwen models).&lt;/p&gt; &lt;p&gt;Nemotron 3 Nano 30B, however, even with a much bigger .gguf, easily fit 256k in my VRAM. In fact, it only goes about 6GB into system RAM if I set the context to 512K, and I can easily run it at a full 1M context using spill over if I don't mind it going slow in system RAM.&lt;/p&gt; &lt;p&gt;I've been busy, Devstral 2 Small 24B was running about 1.5-2 tokens/s when it hit into my system RAM. From the looks of performance, I think when I cap out Nemotron 3 Nano 30B, it'll probably end up 2-3 tokens/s in RAM.&lt;/p&gt; &lt;p&gt;When I started the coding test, it came blazing out the gate rocking 46.8 tokens/s and I was blown away.&lt;/p&gt; &lt;p&gt;However, it did quickly slow down, and the response from the initial prompt, which brought the chat to a bit over 11k tokens, finished at 28.8 tokens/s, which is the fastest performance I've seen for a 30B class model on my hardware.&lt;/p&gt; &lt;p&gt;More impressively to me, it is the only model I've ever run locally to correctly pass the coding challenge in a single prompt, producing usable code and navigating all of the logic traps well.&lt;/p&gt; &lt;p&gt;Gemini 3 was Google's first model for me to one-shot the test. Claude Opus 4 was the first model to one shot it for me period, and I have never technically had ChatGPT one shot it as written, but I can get it to if I modify it, otherwise it asks me a bunch of questions about the logic traps which is honestly a perfectly acceptable response.&lt;/p&gt; &lt;p&gt;I use Gemini, Claude, and ChatGPT to rank how other models perform on the coding challenge because I'm lazy and I don't want to comb through every one of them, but I do manually go over the ones with potential.&lt;/p&gt; &lt;p&gt;Anyway, the point of all this is for me on my hardware, Nemotron 3 Nano 30B represents the first local LLM I can run on my budget AI rig that seems actually capable of filling in the gaps to use AI to increase my coding productivity.&lt;/p&gt; &lt;p&gt;I can't afford APIs or $200+ subs, so I'm mostly using Claude Pro which honestly, I don't get a lot to work with. I can be done for 5 hours sometimes in as little as 15 minutes, which really disrupts my workflow.&lt;/p&gt; &lt;p&gt;This, however, is fast, actually pretty decent with code, has amazing context, and I think could actually fill in some gaps.&lt;/p&gt; &lt;p&gt;I'm going to do more testing before I start trying to fine tune it, but I'm extremely impressed with what Nvidia has done. Their claims were bold, and the 4x speed seems to be a relative exaggeration, but it is quite a bit faster. Maybe a bit much on the synthetic data, but I think this could be worth renting some cloud GPU usage to fine tune and add some custom datasets to it, something I've never felt really worth it beyond adding my own custom data to a model.&lt;/p&gt; &lt;p&gt;I'd just like to know what other's experiences have been with this? How far have people pushed it? How has it performed with close to full context? Have any of you set it up with an agent? If so, how well has it done with tool calling?&lt;/p&gt; &lt;p&gt;I'm really hoping to get this where it can create/edit files and work directly on my local repos. I'd like to know if anyone else has found good setups this does well with?&lt;/p&gt; &lt;p&gt;This is the first modem I was so excited to try that I downloaded the source code, built it myself, and did all the work to manually install everything. Normally I'm lazy and just use the portable llama.cpp builds, but this one I just couldn't wait, and so far, it was very worth it!&lt;/p&gt; &lt;p&gt;Note: I just wrote this on my phone, so forgive me if it's a bit all over the place. I might clean it up when I get back to my computer later. I just didn't want to wait to post about it because I'm hoping to get some ideas for things to try when I get home.&lt;/p&gt; &lt;p&gt;Edit for details: I'm using Q8 and I started with 256K context. I'm using Cuda 13.1, and I built the llama.cpp version out myself with CMake from fork #18058. I'm running Windows 11 Pro (I already know...) and Visual Studio 2022.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonkeyBonked"&gt; /u/DonkeyBonked &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T20:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pojfmt</id>
    <title>browser-use fine tuned Qwen3-VL-30B-A3B-Instruct as browser-use/bu-30b-a3b-preview</title>
    <updated>2025-12-17T01:14:49+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pojfmt/browseruse_fine_tuned_qwen3vl30ba3binstruct_as/"&gt; &lt;img alt="browser-use fine tuned Qwen3-VL-30B-A3B-Instruct as browser-use/bu-30b-a3b-preview" src="https://preview.redd.it/zi59xtsg0o7g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5b2f988aad0f96cc11da0b1be12c1f5947a250d" title="browser-use fine tuned Qwen3-VL-30B-A3B-Instruct as browser-use/bu-30b-a3b-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/browser-use/bu-30b-a3b-preview"&gt;https://huggingface.co/browser-use/bu-30b-a3b-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi59xtsg0o7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pojfmt/browseruse_fine_tuned_qwen3vl30ba3binstruct_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pojfmt/browseruse_fine_tuned_qwen3vl30ba3binstruct_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T01:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1po7i0c</id>
    <title>Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.</title>
    <updated>2025-12-16T17:11:50+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"&gt; &lt;img alt="Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts." src="https://external-preview.redd.it/aHN2Ynl2OXlsbDdnMcH321aC77jYYB3hpLEwmsgN4qk6KsN77tikHsTNkpxK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba962072bea73833102870325afc6e1184ffaa05" title="Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/"&gt;https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SAM Audio transforms audio processing by making it easy to isolate any sound from complex audio mixtures using text, visual, and time span prompts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yoiaaoayll7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T17:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokpha</id>
    <title>QwenLong-L1.5: Revolutionizing Long-Context AI</title>
    <updated>2025-12-17T02:16:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"&gt; &lt;img alt="QwenLong-L1.5: Revolutionizing Long-Context AI" src="https://b.thumbs.redditmedia.com/_W9cLWw-Xs7yDTb51duV9m0DwVBI12aSOhiQ0K4ybyQ.jpg" title="QwenLong-L1.5: Revolutionizing Long-Context AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new model achieves SOTA long-context reasoning with novel data synthesis, stabilized RL, &amp;amp; memory management for contexts up to 4M tokens.&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B"&gt;https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pokpha"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T02:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1porpwd</id>
    <title>Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model</title>
    <updated>2025-12-17T08:49:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt; &lt;img alt="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" src="https://external-preview.redd.it/OXpuN3VqYnE4cTdnMbhg7mfH3BLNBAJzBcqwf-BeiskbYrfqW4XgiIx-FQh0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6475aef4e90b21644bf95a26618a75433c2e08de" title="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Details&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Type:&lt;/strong&gt; Flow-Matching Transformers with Sparse Voxel based 3D VAE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 4 Billion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Single Image&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; 3D Asset &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/microsoft/TRELLIS.2-4B"&gt;https://huggingface.co/microsoft/TRELLIS.2-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo - &lt;a href="https://huggingface.co/spaces/microsoft/TRELLIS.2"&gt;https://huggingface.co/spaces/microsoft/TRELLIS.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post - &lt;a href="https://microsoft.github.io/TRELLIS.2/"&gt;https://microsoft.github.io/TRELLIS.2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g8uco5dq8q7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T08:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogwb6</id>
    <title>8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details</title>
    <updated>2025-12-16T23:20:20+00:00</updated>
    <author>
      <name>/u/Beautiful_Trust_8151</name>
      <uri>https://old.reddit.com/user/Beautiful_Trust_8151</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"&gt; &lt;img alt="8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp;amp; Build Details" src="https://preview.redd.it/furqdxa18n7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=700d4a8e9197fffc15398e5a63d7abad773cef16" title="8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp;amp; Build Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a multi 7900XTX GPU setup for local AI inference for work and wanted to share some performance numbers and build details for anyone considering a similar route as I have not seen that many of us out there. The system consists of 8x AMD Radeon 7900 XTX cards providing 192 GB VRAM total, paired with an Intel Core i7-14700F on a Z790 motherboard and 192 GB of system RAM. The system is running Windows 11 with a Vulkan backend through LMStudio and Open WebUI. I got a $500 Aliexpress PCIe Gen4 x16 switch expansion card with 64 additional lanes to connect the GPUs to this consumer grade motherboard. This was an upgrade from a 4x 7900XTX GPU system that I have been using for over a year. The total build cost is around $6-7k&lt;/p&gt; &lt;p&gt;I ran some performance testing with GLM4.5Air q6 (99GB file size) Derestricted at different context utilization levels to see how things scale with the maximum allocated context window of 131072 tokens. With an empty context, I'm getting about 437 tokens per second for prompt processing and 27 tokens per second for generation. When the context fills up to around 19k tokens, prompt processing still maintains over 200 tokens per second, though generation speed drops to about 16 tokens per second. The full performance logs show this behavior is consistent across multiple runs, and more importantly, the system is stable. On average the system consums about 900watts during prompt processing and inferencing. &lt;/p&gt; &lt;p&gt;This approach definitely isn't the cheapest option and it's not the most plug-and-play solution out there either. However, for our work use case, the main advantages are upgradability, customizability, and genuine long-context capability with reasonable performance. If you want the flexibility to iterate on your setup over time and have specific requirements around context length and model selection, a custom multi-GPU rig like this has been working really well for us. I would be happy to answer any questions.&lt;/p&gt; &lt;p&gt;Here some raw log data.&lt;br /&gt; 2025-12-16 14:14:22 [DEBUG]&lt;/p&gt; &lt;p&gt;Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 37.30 ms&lt;br /&gt; common_perf_print: samplers time = 4.80 ms / 1701 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 3577.99 ms / 1564 tokens ( 2.29 ms per token, 437.12 tokens per second)&lt;br /&gt; 2025-12-16 15:05:06 [DEBUG]&lt;br /&gt; common_perf_print: eval time = 301.25 ms / 8 runs ( 37.66 ms per token, 26.56 tokens per second)&lt;br /&gt; common_perf_print: total time = 3919.71 ms / 1572 tokens&lt;br /&gt; common_perf_print: unaccounted time = 3.17 ms / 0.1 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 7&lt;/p&gt; &lt;p&gt; Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 704.49 ms&lt;br /&gt; common_perf_print: samplers time = 546.59 ms / 15028 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 66858.77 ms / 13730 tokens ( 4.87 ms per token, 205.36 tokens per second)&lt;br /&gt; 2025-12-16 14:14:22 [DEBUG]&lt;br /&gt; common_perf_print: eval time = 76550.72 ms / 1297 runs ( 59.02 ms per token, 16.94 tokens per second)&lt;br /&gt; common_perf_print: total time = 144171.13 ms / 15027 tokens&lt;br /&gt; common_perf_print: unaccounted time = 57.15 ms / 0.0 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 1291&lt;/p&gt; &lt;p&gt;Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 1547.88 ms&lt;br /&gt; common_perf_print: samplers time = 1201.66 ms / 18599 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 77358.07 ms / 15833 tokens ( 4.89 ms per token, 204.67 tokens per second)&lt;br /&gt; common_perf_print: eval time = 171509.89 ms / 2762 runs ( 62.10 ms per token, 16.10 tokens per second)&lt;br /&gt; common_perf_print: total time = 250507.93 ms / 18595 tokens&lt;br /&gt; common_perf_print: unaccounted time = 92.10 ms / 0.0 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 2750&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful_Trust_8151"&gt; /u/Beautiful_Trust_8151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/furqdxa18n7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T23:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Weâ€™re researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;â€”open multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;â€”a family of fully open language models (7Bâ€“32B) with Base/Instruct/Thinking variants, longâ€‘context support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, longâ€‘context, grounded video QA/tracking, and realâ€‘world deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weâ€™ll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;â–¶ï¸ &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;â¬‡ï¸ &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ðŸ“ &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ðŸ“„Report: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ðŸ’» &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ðŸ«† PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
