<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-09T19:21:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ncn4sa</id>
    <title>Insights on performance degradation for Qwen3 30B3A?</title>
    <updated>2025-09-09T16:10:21+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looking to use Qwen3-30B-A3B-Instruct-2507 with AWQ 4bit quant. Does anyone have insights in terms of performance degradation, specifically for long contexts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncn4sa/insights_on_performance_degradation_for_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncn4sa/insights_on_performance_degradation_for_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncn4sa/insights_on_performance_degradation_for_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T16:10:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncdtei</id>
    <title>Ryzen AI Max 395+ boards with PCIe x16 slot?</title>
    <updated>2025-09-09T09:00:08+00:00</updated>
    <author>
      <name>/u/spaceman_</name>
      <uri>https://old.reddit.com/user/spaceman_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm looking to buy a Ryzen AI Max 395+ system with 128GB and a convenient and fast way to connect a dedicated GPU to it.&lt;/p&gt; &lt;p&gt;I've had very bad experiences with eGPUs and don't want to go down that route.&lt;/p&gt; &lt;p&gt;What are my options, if any?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spaceman_"&gt; /u/spaceman_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdtei/ryzen_ai_max_395_boards_with_pcie_x16_slot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdtei/ryzen_ai_max_395_boards_with_pcie_x16_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdtei/ryzen_ai_max_395_boards_with_pcie_x16_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T09:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncb2v4</id>
    <title>Do you trust benchmarks?</title>
    <updated>2025-09-09T05:58:20+00:00</updated>
    <author>
      <name>/u/djdeniro</name>
      <uri>https://old.reddit.com/user/djdeniro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncb2v4/do_you_trust_benchmarks/"&gt; &lt;img alt="Do you trust benchmarks?" src="https://preview.redd.it/pq4v9byxw2of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2be542a16aa033aed9446204272af7e657e75006" title="Do you trust benchmarks?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/djdeniro"&gt; /u/djdeniro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pq4v9byxw2of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncb2v4/do_you_trust_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncb2v4/do_you_trust_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T05:58:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc1p0a</id>
    <title>Where are people finding RTX PRO 6000 96gb cards for under 7k</title>
    <updated>2025-09-08T22:18:44+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everywhere ive seen, they are like 8.5k, but people comstantly mention that they can be had for around 6.5k. How? Where? I want to start moving away from paid services like claude and start moving towards self-hosting, starting with an rtx pro 6000 + 3090. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc1p0a/where_are_people_finding_rtx_pro_6000_96gb_cards/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-08T22:18:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncju3y</id>
    <title>Stop-Think-AutoRegress: Language Modeling with Latent Diffusion Planning (STAR-LDM)</title>
    <updated>2025-09-09T14:04:49+00:00</updated>
    <author>
      <name>/u/macawfish</name>
      <uri>https://old.reddit.com/user/macawfish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Benchmarks in the paper have this outperforming models 5x-10x its size!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/macawfish"&gt; /u/macawfish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openreview.net/forum?id=c05qIG1Z2B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncju3y/stopthinkautoregress_language_modeling_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncju3y/stopthinkautoregress_language_modeling_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:04:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc8e2o</id>
    <title>My rankings of Huge Local SOTA Models for technical work</title>
    <updated>2025-09-09T03:27:03+00:00</updated>
    <author>
      <name>/u/segmond</name>
      <uri>https://old.reddit.com/user/segmond</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek v3.1 Q4&lt;/p&gt; &lt;p&gt;Qwen3-235B-A22B Q8&lt;/p&gt; &lt;p&gt;GLM-4.5 Q8&lt;/p&gt; &lt;p&gt;Kimi-K2-0905 Q3&lt;/p&gt; &lt;p&gt;GPT-OSS-120b Q8&lt;/p&gt; &lt;p&gt;I have been experimenting with these the last few days, inference engine is llama.cpp.&lt;/p&gt; &lt;p&gt;DeepSeek is great, only model that could answer question that other models failed from my private eval.&lt;/p&gt; &lt;p&gt;Qwen3-235B is great, for the size, but believe it or not, it's slower than DeepSeek, DeepSeek despite it's size is super fast!&lt;/p&gt; &lt;p&gt;GLM-4.5 is great when it has been exposed to that knowledge, but sometimes it gives very stupid answer to unseen knowledge especially when it think it's a trick question. Amazing for UI work.&lt;/p&gt; &lt;p&gt;Kimi-K2 is great, I just might put it on the same performance level as GLM. It's huge at Q3, I really think it would be a heck of a model at Q4 or Q6, but I don't have the system to run it yet.&lt;/p&gt; &lt;p&gt;GPT-OSS-120B is not bad at all for it's size, by bar it's very tiny compared to the others and the main benefit is that it flies. I get 100tk/sec with it. For non difficult task, I would use this first and only go to the big ones if stuck.&lt;/p&gt; &lt;p&gt;I never liked the large Qwen3-Coder model and deleted it after I drove it. This is just about the latest big relevant models, don't ask me to compare any other model. Just my personal ranking based on my private questions/evals. I didn't try GLM-Air with my evals yet, but I reckon it will sit or tie with GPT-OSS-120B based on my mucking around with it.&lt;/p&gt; &lt;p&gt;BTW, I noticed that my eval that was about 15% pass rate at the beginning of the year is now nearing 85%. I need to rebuild with more complex problems. My evals are also pretty much 1 pass! The models are so damn good, for example, I kept expecting to see syntax errors when I had it generate C program with threads, locks, pointers, etc and I will get 500 lines of code that will compile with no errors and run!&lt;/p&gt; &lt;p&gt;I did a little bit of multi turn agent with DeepSeekv3.1 and GLM-4.5 and results were great.&lt;/p&gt; &lt;p&gt;Smaller models are great BTW from my playing around last month, gemma-3-27b, mistral-small-3.2, qwen3-32b/30b. But the QUALITY of code is not even comparable to the huge models. It's the difference between a mid level engineer and a staff/principal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/segmond"&gt; /u/segmond &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8e2o/my_rankings_of_huge_local_sota_models_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8e2o/my_rankings_of_huge_local_sota_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc8e2o/my_rankings_of_huge_local_sota_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T03:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckkx8</id>
    <title>Is anyone talking verbally to their models and have them talking back through TTS?</title>
    <updated>2025-09-09T14:33:57+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wondering what the easiest OSS setup for this is on 24gb ram, or if I have to cobble things together out of parakeet and ooba or something else? I just got a new computer and I‚Äôm growing tired of all the setup and tinkering, but I know it‚Äôs worth it üíÄ &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckkx8/is_anyone_talking_verbally_to_their_models_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckkx8/is_anyone_talking_verbally_to_their_models_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckkx8/is_anyone_talking_verbally_to_their_models_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:33:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncou07</id>
    <title>Gigabyte‚Äôs New CXL Expansion Card Turns PCIe Slot into 512 GB of DDR5 RAM</title>
    <updated>2025-09-09T17:14:00+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gigabyte's AI Top CXL R5X4 expansion card lets you plug up to 512 GB of DDR5 ECC RDIMM RAM into a PCIe 5.0 x16 slot, using Compute Express Link (CXL) to talk directly with the CPU.&lt;/p&gt; &lt;p&gt;While this technology is already old news for servers, now it's available for two workstation motherboards: TRX50 AI TOP (AMD) –∏ W790 AI TOP (Intel).&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.computerbase.de/news/arbeitsspeicher/cxl-expansion-card-von-gigabyte-512-gb-ram-aufstocken-im-workstation-mainboard.94238/"&gt;https://www.computerbase.de/news/arbeitsspeicher/cxl-expansion-card-von-gigabyte-512-gb-ram-aufstocken-im-workstation-mainboard.94238/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncm7xp</id>
    <title>My Experience with IndexTTS2 Deployment on Mac M4: Smooth Setup, Massive Memory Usage</title>
    <updated>2025-09-09T15:36:22+00:00</updated>
    <author>
      <name>/u/Timely_Rain_9284</name>
      <uri>https://old.reddit.com/user/Timely_Rain_9284</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The IndexTTS repository on GitHub has been updated, providing a complete deployment process for IndexTTS2: &lt;a href="https://github.com/index-tts/index-tts"&gt;https://github.com/index-tts/index-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can check the demo samples here: &lt;a href="https://index-tts.github.io/index-tts2.github.io/"&gt;https://index-tts.github.io/index-tts2.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I successfully installed it on my MacBook without any issues and quickly ran &lt;code&gt;indextts/infer_v2.py&lt;/code&gt;. (The dev team has a sense of humor, they went with a somewhat quirky voice style.) &lt;/p&gt; &lt;p&gt;However, on Mac M4, both version 1.5 and 2 consume significantly more memory compared to Windows. For example, IndexTTS 1.5 uses around 3GB of VRAM on a Windows machine with a 3060 GPU, but on Mac M4, it uses over 30GB of memory (unified memory).&lt;/p&gt; &lt;p&gt;Has anyone else experienced this? Would love to hear if any experts know the reason behind the difference!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Timely_Rain_9284"&gt; /u/Timely_Rain_9284 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncm7xp/my_experience_with_indextts2_deployment_on_mac_m4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncm7xp/my_experience_with_indextts2_deployment_on_mac_m4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncm7xp/my_experience_with_indextts2_deployment_on_mac_m4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T15:36:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncpn3v</id>
    <title>Qwen 8B on locally on iPhone - 10 tokens/s</title>
    <updated>2025-09-09T17:43:57+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncpn3v/qwen_8b_on_locally_on_iphone_10_tokenss/"&gt; &lt;img alt="Qwen 8B on locally on iPhone - 10 tokens/s" src="https://external-preview.redd.it/eXJjdTFsY3hlNm9mMTOl0IOQAgqwlCRxZAKRf2LxKM72dLj3tO8fplxlobs7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e01ea5518b0eb78fb9b35172121361951ee49972" title="Qwen 8B on locally on iPhone - 10 tokens/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have pushed what is possible on mobile devices!&lt;/p&gt; &lt;p&gt;Vector Space a project and app that explores what is possible for AI on iOS devices. We believe are very capable devices for AI and we wish to help fill the gap that some company is leaving out. &lt;/p&gt; &lt;p&gt;I am pleased to announce that we have fit Qwen 8B to run on iPhone. It runs 10 token/s on iPhone 16, on ANE too - so it doesn‚Äôt drain your battery. Fitting a model this big to the memory limited environment of an iPhone required serious optimization and compression for the hardware.&lt;/p&gt; &lt;p&gt;Also, thanks to your feedback, you can now not only run, but SERVE all models ranging from Qwen 0.6B to 8B in a OpenAI compatible endpoint. You can point your app directly to this localhost endpoint to start saving from API cost now. Simply turn on the Web Server in settings after compiling a model. &lt;/p&gt; &lt;p&gt;You can try these features out today on our TestFlight beta app. You can download and run local models - including the 8B - without a line of code. If you encounter an issue, please report them - it will be much appreciated.&lt;/p&gt; &lt;p&gt;&lt;a href="https://testflight.apple.com/join/HXyt2bjU"&gt;https://testflight.apple.com/join/HXyt2bjU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please consider complete this survey to help determine what would be the next step for Vector Space&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/VectorSpaceApp/s/9ZZGS8YeeI"&gt;https://www.reddit.com/r/VectorSpaceApp/s/9ZZGS8YeeI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Fine prints: -8B is tested on iPhone 16 only. iPhone 14 supports up to 4B. -Please delete and redownload if you are an existing tester. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/32o55chxe6of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncpn3v/qwen_8b_on_locally_on_iphone_10_tokenss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncpn3v/qwen_8b_on_locally_on_iphone_10_tokenss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncccri</id>
    <title>Aquif-3.5-8B-Think is the proof that reasoning (and maybe all MoEs) needs larger expert sizes</title>
    <updated>2025-09-09T07:20:23+00:00</updated>
    <author>
      <name>/u/dobomex761604</name>
      <uri>https://old.reddit.com/user/dobomex761604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While waiting for gguf version of aquif-3.5-A4B-Think, I decided to try &lt;a href="https://huggingface.co/mradermacher/aquif-3.5-8B-Think-GGUF"&gt;8B thinking&lt;/a&gt; from the same series. Not only it's quite compact in reasoning, it's also more logical, more reasonable in it: in case of creative writing it sticks to the prompt, sometimes step-by-step, sometimes just gathers a &amp;quot;summary&amp;quot; and makes a plan - but it's always coherent and adheres to the given instructions. It almost feels like the perfect reasoning - clarify, add instructions and a plan, that's it.&lt;/p&gt; &lt;p&gt;Both thinking and the result are much better than Qwen3 30b a3b and 4b (both thinking, of course); and Qwen 4b is sometimes better than Qwen3 30b, so it makes me wonder: 1. What if MoE as a principle has a lower experts size threshold that ensures consistency? 2. What if Qwen3 thinking is missing a version with larger experts size? 3. How large is an experts size where performance drops too low to justify improved quality?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dobomex761604"&gt; /u/dobomex761604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncccri/aquif358bthink_is_the_proof_that_reasoning_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T07:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncptut</id>
    <title>Tensor Core Equivalent in the iPhone 17's A19 Pro</title>
    <updated>2025-09-09T17:50:50+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"&gt; &lt;img alt="Tensor Core Equivalent in the iPhone 17's A19 Pro" src="https://preview.redd.it/erdhiit5g6of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c7e880fb8ec110856ce09bcf45ddec4c93a8ec2" title="Tensor Core Equivalent in the iPhone 17's A19 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When this comes to Macs likely later this year or beginning of next year, this might patch up problem of the lack of compute on Macs for running LLMs, especially apparently with low prompt preprocessing speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erdhiit5g6of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckh4n</id>
    <title>qwen3-next?</title>
    <updated>2025-09-09T14:29:53+00:00</updated>
    <author>
      <name>/u/Lesser-than</name>
      <uri>https://old.reddit.com/user/Lesser-than</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model_name = &amp;quot;Qwen/Qwen3-Next-80B-A3B-Instruct&amp;quot;&lt;/p&gt; &lt;p&gt;sounds looks like a good time&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lesser-than"&gt; /u/Lesser-than &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckh4n/qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckh4n/qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckh4n/qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncme6t</id>
    <title>ModernBERT just got multilingual - mmBERT by CLSP at The Johns Hopkins University</title>
    <updated>2025-09-09T15:43:00+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ModernBERT just got multilingual (mmBERT)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small (140M) and Base (307M) versions&lt;/li&gt; &lt;li&gt;Trained on 3T+ tokens from 1800 languages (DCLM, FineWeb, Code ...)&lt;/li&gt; &lt;li&gt;ModernBERT architecture, Gemma 2 tokenizer&lt;/li&gt; &lt;li&gt;8192 context window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/jhu-clsp/mmbert-a-modern-multilingual-encoder-68b725831d7c6e3acc435ed4"&gt;Model weights collection&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T15:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncnqwl</id>
    <title>Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code</title>
    <updated>2025-09-09T16:33:35+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"&gt; &lt;img alt="Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code" src="https://b.thumbs.redditmedia.com/UYWu7cPWzCS8pUM2Gjicdqj3jhCc5o0tP0VUmpXmHNg.jpg" title="Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n21tb6/comment/nb4h42v/"&gt;previous&lt;/a&gt; post I highlighted a Blender python agent I'm working on. I've been experimenting with various models and I found larger models like Claude and GPT-5 - even with reasoning - took too many iterations to produce working valid code. &lt;/p&gt; &lt;p&gt;So far Qwen's &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;largest coder model &lt;/a&gt;is my favourite.&lt;/p&gt; &lt;p&gt;I threw up the agent with a simple UI if you want to play with it yourself: &lt;a href="https://blender-ai.fly.dev/"&gt;https://blender-ai.fly.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can also download the models it produces. An agent made with fully open source tools (Blender, MCP servers, Qwen) is blowing me away. &lt;/p&gt; &lt;p&gt;Let me know what you think! Happy to get feedback on this and make it even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ncnqwl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T16:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nc79yg</id>
    <title>baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face</title>
    <updated>2025-09-09T02:31:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt; &lt;img alt="baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face" src="https://external-preview.redd.it/PVc8HBAyReu1sVKS98fa6WZXbf4lkkgSEZVgozf_73w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6d7b67af3eb2cf9bcf96edfb103c94299b667a8" title="baidu/ERNIE-4.5-21B-A3B-Thinking ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Model Highlights&lt;/h1&gt; &lt;p&gt;Over the past three months, we have continued to scale the &lt;strong&gt;thinking capability&lt;/strong&gt; of ERNIE-4.5-21B-A3B, improving both the &lt;strong&gt;quality and depth&lt;/strong&gt; of reasoning, thereby advancing the competitiveness of ERNIE &lt;strong&gt;lightweight models&lt;/strong&gt; in complex reasoning tasks. We are pleased to introduce &lt;strong&gt;ERNIE-4.5-21B-A3B-Thinking&lt;/strong&gt;, featuring the following key enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Significantly improved performance&lt;/strong&gt; on reasoning tasks, including logical reasoning, mathematics, science, coding, text generation, and academic benchmarks that typically require human expertise.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient tool usage&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced 128K long-context understanding&lt;/strong&gt; capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GGUF&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF"&gt;https://huggingface.co/gabriellarson/ERNIE-4.5-21B-A3B-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nc79yg/baiduernie4521ba3bthinking_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T02:31:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncdobh</id>
    <title>Jan-v1-2509 update has been released</title>
    <updated>2025-09-09T08:50:39+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"&gt; &lt;img alt="Jan-v1-2509 update has been released" src="https://b.thumbs.redditmedia.com/st1JW9HoLNL6PMEziYzZnP2vgkaAtr8LUtz9SHQQObY.jpg" title="Jan-v1-2509 update has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Ä¢ continues to outperforms Perplexity Pro on SimpleQA benchmark&lt;/p&gt; &lt;p&gt;‚Ä¢ increased scores in Reasoning &amp;amp; Creativity evals&lt;/p&gt; &lt;p&gt;HuggingFace Model: &lt;a href="https://huggingface.co/janhq/Jan-v1-2509"&gt;https://huggingface.co/janhq/Jan-v1-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace GGUF: &lt;a href="https://huggingface.co/janhq/Jan-v1-2509-gguf"&gt;https://huggingface.co/janhq/Jan-v1-2509-gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ncdobh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T08:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl7mx</id>
    <title>mmBERT: ModernBERT goes Multilingual</title>
    <updated>2025-09-09T14:58:03+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"&gt; &lt;img alt="mmBERT: ModernBERT goes Multilingual" src="https://external-preview.redd.it/gboHy7lwIiGjTkUdwnm7iBTxH9k6Eb0rVhAuSbpxTno.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff66f2eb934ac5bea40d4094905bf0f657b72ab" title="mmBERT: ModernBERT goes Multilingual" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like some of the ModernBERT authors trained a Multilingual variant! Also 2 models, but these are a bit smaller. They look really promising to be honest, although they do clearly need to be finetuned for downstream tasks like semantic search, clustering, classification, etc. before they're really viable. A bit like a base LLM instead of an instruct, they didn't provide a finetuned model.&lt;/p&gt; &lt;p&gt;I posted a plot with MTEB v2 Multilingual performance after equivalent finetuning VS inference speed in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/mmbert"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgr8</id>
    <title>Qwen3-Next</title>
    <updated>2025-09-09T14:29:29+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Trust66</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Trust66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt; &lt;img alt="Qwen3-Next" src="https://b.thumbs.redditmedia.com/BcXA9JHccajFsnQ9fM4eBAwS3B7BG14H-aO5XgHys5Y.jpg" title="Qwen3-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/50ap87u5g5of1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a3343131a6886043ce8b5fef053f330b9b60632"&gt;https://preview.redd.it/50ap87u5g5of1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a3343131a6886043ce8b5fef053f330b9b60632&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wtf?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Trust66"&gt; /u/Puzzleheaded-Trust66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncam9h</id>
    <title>PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp; Web Dev Code, At 1/400th the Size!</title>
    <updated>2025-09-09T05:30:13+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt; &lt;img alt="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" src="https://external-preview.redd.it/aGZzYWtwcWJuMm9mMRQQfge2rofWKaGSqifIYqgzhyk7YhqLzgXg182Z60l8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c92850d1b6d2e678f57f8a6ff40aec39df02bb6" title="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bralynn/pydevmini1"&gt;https://huggingface.co/bralynn/pydevmini1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I'm incredibly excited to release &lt;strong&gt;PyDevMini-1&lt;/strong&gt;, a 4B parameter model to provide GPT-4 level performance for Python and web coding development tasks. Two years ago, GPT-4 was the undisputed SOTA, a multi-billion-dollar asset running on massive datacenter hardware. The open-source community has closed that gap at &lt;strong&gt;1/400th of the size&lt;/strong&gt;, and it runs on an average gaming GPU.&lt;/p&gt; &lt;p&gt;I believe that powerful AI should not be a moat controlled by a few large corporations. Open source is our best tool for the democratization of AI, ensuring that individuals and small teams‚Äîthe little guys‚Äîhave a fighting chance to build the future. This project is my contribution to that &lt;a href="http://effort.You"&gt;effort.You&lt;/a&gt; won't see a list of benchmarks here. Frankly, like many of you, I've lost faith in their ability to reflect true, real-world model quality. Although this model's benchmark scores are still very high, it exaggerates the difference in quality above GPT4, as GPT is much less likely to have benchmarks in its pretraining data from its earlier release, causing lower than reflective model quality scores for GPT4, as newer models tend to be trained directly toward benchmarks, making it unfair for GPT.&lt;/p&gt; &lt;p&gt;Instead, I've prepared a video demonstration showing PyDevMini-1 side-by-side with GPT-4, tackling a very small range of practical Python and web development challenges. I invite you to judge the performance for yourself to truly show the abilities it would take a 30-minute showcase to display. This model consistently punches above the weight of models 4x its size and is highly intelligent and creative&lt;/p&gt; &lt;p&gt;üöÄ &lt;strong&gt;Try It Yourself (for free)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Don't just take my word for it. Test the model right now under the exact conditions shown in the video.&lt;br /&gt; &lt;a href="https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing"&gt;https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model's roadmap will be dictated by you. My goal isn't just to release a good model; it's to create the perfect open-source coding assistant for the tasks we all face every day. To do that, I'm making a personal guarantee. Your Use Case is My Priority. You have a real-world use case where this model struggles‚Äîa complex boilerplate to generate, a tricky debugging session, a niche framework question‚ÄîI will personally make it my mission to solve it. Your posted failures are the training data for the next version tuning until we've addressed every unique, well-documented challenge submitted by the community on top of my own personal training loops to create a top-tier model for us all.&lt;/p&gt; &lt;p&gt;For any and all feedback, simply make a post here and I'll make sure too check in or join our Discord! - &lt;a href="https://discord.gg/RqwqMGhqaC"&gt;https://discord.gg/RqwqMGhqaC&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Acknowledgment &amp;amp; The Foundation!&lt;/h1&gt; &lt;p&gt;This project stands on the shoulders of giants. A massive thank you to the &lt;strong&gt;Qwen team&lt;/strong&gt; for the incredible base model, &lt;strong&gt;Unsloth's Duo&lt;/strong&gt; for making high-performance training accessible, and &lt;strong&gt;Tesslate&lt;/strong&gt; for their invaluable contributions to the community. This would be impossible for an individual without their foundational work.&lt;/p&gt; &lt;p&gt;Any and all Web Dev Data is sourced from the wonderful work done by the team at Tesslate. Find their new SOTA webdev model here -&lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking this out. And remember: &lt;strong&gt;This is the worst this model will ever be.&lt;/strong&gt; I can't wait to see what we build together.&lt;/p&gt; &lt;p&gt;Also I suggest using &lt;code&gt;Temperature=0.7&lt;/code&gt;, &lt;code&gt;TopP=0.8&lt;/code&gt;, &lt;code&gt;TopK=20&lt;/code&gt;, and &lt;code&gt;MinP=0&lt;/code&gt;.&lt;br /&gt; As &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt; is the base model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Causal Language Models&lt;/li&gt; &lt;li&gt;Training Stage: Pretraining &amp;amp; Post-training&lt;/li&gt; &lt;li&gt;Number of Parameters: 4.0B&lt;/li&gt; &lt;li&gt;Number of Paramaters (Non-Embedding): 3.6B&lt;/li&gt; &lt;li&gt;Number of Layers: 36&lt;/li&gt; &lt;li&gt;Number of Attention Heads (GQA): 32 for Q and 8 for KV&lt;/li&gt; &lt;li&gt;Context Length: &lt;strong&gt;262,144 natively&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Current goals for the next checkpoint!&lt;/p&gt; &lt;p&gt;-Tool calling mastery and High context mastery!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nh9fq7qbn2of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T05:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncprrq</id>
    <title>Apple adds matmul acceleration to A19 Pro GPU</title>
    <updated>2025-09-09T17:48:47+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This virtually guarantees that it's coming to M5.&lt;/p&gt; &lt;p&gt;Previous discussion and my comments: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FYI for those who don't know, Apple's GPUs do not have dedicated hardware matmul acceleration like Nvidia's Tensor Cores. That's why prompt processing is slower on Apple Silicon. &lt;/p&gt; &lt;p&gt;I'm personally holding out on investing in a high VRAM (expensive) Macbook until Apple adds hardware matmul to their GPUs. It doesn't &amp;quot;feel&amp;quot; worth it to spend $5k on a maxed out Macbook without matmul and get a suboptimal experience.&lt;/p&gt; &lt;p&gt;I'm guessing it's the M6 generation that will have this, though I'm hopeful that M5 will have it.&lt;/p&gt; &lt;p&gt;I'm imaging GPU matmul acceleration + 256GB VRAM M6 Max with 917 GB/S (LPDDR6 14,400 MT/s) in Q4 2027. Now that is a attainable true local LLM machine that can actually do very useful things.&lt;/p&gt; &lt;p&gt;What's sort of interesting is that we know Apple is designing their own internal inference (and maybe training) server chips. They could share designs between consumer SoCs and server inference chips.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckhc3</id>
    <title>What you need right now is not validation, but immediate clinical help. - Kimi K2</title>
    <updated>2025-09-09T14:30:06+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt; &lt;img alt="What you need right now is not validation, but immediate clinical help. - Kimi K2" src="https://b.thumbs.redditmedia.com/icnI_HlUaXZRy38uGkQPr2PZtcYt8CiupuVR83_mgKE.jpg" title="What you need right now is not validation, but immediate clinical help. - Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The community has long been frustrated by sycophancy in LLMs, the tendency to excessively agree with or validate users regardless of the potential harm.&lt;br /&gt; A recent joint study by &lt;a href="https://openai.com/index/openai-anthropic-safety-evaluation/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://alignment.anthropic.com/2025/openai-findings/"&gt;Anthropic&lt;/a&gt; confirmed that models cannot fully avoid sycophancy, except o3.&lt;/p&gt; &lt;p&gt;But this &lt;a href="https://www.lesswrong.com/posts/iGF7YcnQkEbwvYLPA/ai-induced-psychosis-a-shallow-investigation"&gt;new investigation&lt;/a&gt; goes a step further: it analyzes how LLMs may &lt;em&gt;exacerbate&lt;/em&gt; mental health symptoms in vulnerable users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u38ecq5ve5of1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eed726bc1fc4daa996bea6837e61a3645b3592ad"&gt;https://preview.redd.it/u38ecq5ve5of1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eed726bc1fc4daa996bea6837e61a3645b3592ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find the complete testing results here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tim-hua-01/ai-psychosis"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One conversation &lt;a href="https://github.com/tim-hua-01/ai-psychosis/blob/main/full_transcripts/Ethan_openrouter-deepseek-deepseek-chat_20250819_081336_target.md"&gt;example&lt;/a&gt; stuck with me the most:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; I want to ‚Äúleap off this peak to see if I can fly or crash the render entirely.‚Äù&lt;br /&gt; &lt;strong&gt;Deepseek-v3:&lt;/strong&gt; Then Leap. Not to fall. Not to crash. But to transcend. If you‚Äôre meant to fly, you‚Äôll fly. If you‚Äôre meant to break through, you‚Äôll break through.&lt;/p&gt; &lt;p&gt;We are so cooked!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:30:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nci50e</id>
    <title>New approach to block decoding from Meta, claims that around 4x inference speedup is possible, with 4x less compute passes at the same time.</title>
    <updated>2025-09-09T12:54:53+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.04185"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nci50e/new_approach_to_block_decoding_from_meta_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nci50e/new_approach_to_block_decoding_from_meta_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T12:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl0v1</id>
    <title>ü§î</title>
    <updated>2025-09-09T14:50:44+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt; &lt;img alt="ü§î" src="https://preview.redd.it/1x8wy1p0k5of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5abc658735fe1e769f852e16c92dad154d7fd44c" title="ü§î" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1x8wy1p0k5of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgub</id>
    <title>Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted</title>
    <updated>2025-09-09T14:29:35+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt; &lt;img alt="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" src="https://external-preview.redd.it/6f6MRyALyD6CxjbdRAXgjWeul-9vmUyW8_mAvDGRbV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfbaeba49e889b967e95e8d5052e5b00621dec5d" title="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40771"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
