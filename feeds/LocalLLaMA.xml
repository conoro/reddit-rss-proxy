<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-24T15:36:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qkqjer</id>
    <title>A full AI powered cooking game, where literally any ingredient is possible with infinite combinations.</title>
    <updated>2026-01-23T13:16:42+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"&gt; &lt;img alt="A full AI powered cooking game, where literally any ingredient is possible with infinite combinations." src="https://external-preview.redd.it/YTNmcmg4Z3ltM2ZnMUJwJOA_Kqm7OwiZxEbYxXgv1YYIXAs9kE9ZTKKEhyEN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8a4a5616e09101d8d4a75226f626871f4272100" title="A full AI powered cooking game, where literally any ingredient is possible with infinite combinations." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built with Claude Code&lt;br /&gt; Game Logic - Gemini&lt;br /&gt; Sprites - Flux&lt;/p&gt; &lt;p&gt;Try it out at: &lt;a href="https://infinite-kitchen.com/kitchen"&gt;https://infinite-kitchen.com/kitchen&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a2wy0mdym3fg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkqjer/a_full_ai_powered_cooking_game_where_literally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T13:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qks7ua</id>
    <title>Scaling PostgreSQL to power 800 million ChatGPT users</title>
    <updated>2026-01-23T14:27:07+00:00</updated>
    <author>
      <name>/u/buntyshah2020</name>
      <uri>https://old.reddit.com/user/buntyshah2020</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Must Read!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buntyshah2020"&gt; /u/buntyshah2020 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://openai.com/index/scaling-postgresql/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qks7ua/scaling_postgresql_to_power_800_million_chatgpt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qks7ua/scaling_postgresql_to_power_800_million_chatgpt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T14:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlpg0l</id>
    <title>Am I doing this wrong? AI almost delete my DB</title>
    <updated>2026-01-24T15:12:28+00:00</updated>
    <author>
      <name>/u/yaront1111</name>
      <uri>https://old.reddit.com/user/yaront1111</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been messing around with local coding agents (mostly using custom scripts), but I'm paranoid about giving them actual shell access or full write permissions to my project folders.&lt;/p&gt; &lt;p&gt;I didn't want to sandbox everything in Docker every single time, so I ended up writing a &amp;quot;sudo&amp;quot; wrapper in Go - im DEVOPS.. &lt;/p&gt; &lt;p&gt;. Basically, the agent can &amp;quot;read&amp;quot; whatever it wants, but if it tries to &amp;quot;write&amp;quot; or run a command, it pauses and I have to approve it manually (like a sudo prompt).&lt;/p&gt; &lt;p&gt;It works for me, but it feels like I might be reinventing the wheel. &lt;/p&gt; &lt;p&gt;Is there a standard way to handle this governance already? Or is everyone just running agents with full root access and hoping for the best?&lt;/p&gt; &lt;p&gt;If anyone wants to see how I handled the blocking logic, the repo is here: &lt;a href="https://github.com/cordum-io/cordum"&gt;https://github.com/cordum-io/cordum&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yaront1111"&gt; /u/yaront1111 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlpg0l/am_i_doing_this_wrong_ai_almost_delete_my_db/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlpg0l/am_i_doing_this_wrong_ai_almost_delete_my_db/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlpg0l/am_i_doing_this_wrong_ai_almost_delete_my_db/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T15:12:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql48at</id>
    <title>Strix Halo + Minimax Q3 K_XL surprisingly fast</title>
    <updated>2026-01-23T21:55:08+00:00</updated>
    <author>
      <name>/u/Reasonable_Goat</name>
      <uri>https://old.reddit.com/user/Reasonable_Goat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A llama-bench on Ubuntu 25.10 Strix Halo 128gb (Bosgame M5):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;$ ./build/bin/llama-bench -m ~/models/MiniMax-M2.1-UD-Q3_K_XL-00001-of-00003.gguf -ngl 999 -p 256 -n 256 -t 16 -r 3 --device Vulkan0 -fa 1 ggml_cuda_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------ | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | pp256 | 104.80 ¬± 7.95 | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | tg256 | 31.13 ¬± 0.02 |$ ./build/bin/llama-bench -m ~/models/MiniMax-M2.1-UD-Q3_K_XL-00001-of-00003.gguf -ngl 999 -p 256 -n 256 -t 16 -r 3 --device Vulkan0 -fa 1 ggml_cuda_init: found 1 ROCm devices: Device 0: Radeon 8060S Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = Radeon 8060S Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 0 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | dev | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------ | --------------: | -------------------: | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | pp256 | 104.80 ¬± 7.95 | | minimax-m2 230B.A10B Q3_K - Medium | 94.33 GiB | 228.69 B | ROCm,Vulkan | 999 | 1 | Vulkan0 | tg256 | 31.13 ¬± 0.02 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;About 30 token per second TG is actually really useful!&lt;/p&gt; &lt;p&gt;It's the only model I found sufficiently coherent/knowledgable in discussing/brainstorming general topics. Sure, gpt-oss-120b is faster especially in PP, so for coding probably better, but you can use MiniMax Q3 for general questions and it's quite good and reasonably fast for that purpose. A good complement to gpt-oss-120b and GLM-4.5-AIR in my opinion!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Reasonable_Goat"&gt; /u/Reasonable_Goat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql48at/strix_halo_minimax_q3_k_xl_surprisingly_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql48at/strix_halo_minimax_q3_k_xl_surprisingly_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql48at/strix_halo_minimax_q3_k_xl_surprisingly_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T21:55:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlestx</id>
    <title>GLM 4.7 / Minimax M2.1 + Opencode Orchestration</title>
    <updated>2026-01-24T05:46:43+00:00</updated>
    <author>
      <name>/u/pratiknarola</name>
      <uri>https://old.reddit.com/user/pratiknarola</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heyy everyone, &lt;/p&gt; &lt;p&gt;I wanted to understand what kind of multiagent / orchestration setup everyone is using or would use if you have unlimited tokens available at 100 tokens/s &lt;/p&gt; &lt;p&gt;To give some prior context, &lt;/p&gt; &lt;p&gt;I am software developer with 4 yoe. so I prefer to have some oversight on what llm is doing and if its getting sidetracked or not. &lt;/p&gt; &lt;p&gt;I get almost unlimited Claude Sonnet/Opus 4.5 usage (more than 2x 200$ plans), I have 4 server nodes each having 8 x H200 GPUs. 3 are running GLM 4.7 BF16 and last one running Minimax M2.1&lt;br /&gt; So basically I have unlimited glm 4.7 and minimax m2.1 tokens. and 2x 200$ plans worth Claude Sonnet/Opus 4.5 access. &lt;/p&gt; &lt;p&gt;I started using Claude code since its early days.. had a decent setup with few subagents, custom commands and custom skills with mcp like context7, exa, perplexity etc. and because i was actively using it and claude code is actively developed, my setup was up to date. &lt;/p&gt; &lt;p&gt;Then during our internal quality evals, we noticed that Opencode has better score/harness for same models, same tasks, I wanted to try it out and since new year, I have been using Opencode and I love it. &lt;/p&gt; &lt;p&gt;Thanks to Oh-my-opencode and Dynamic context pruning, i already feel the difference. and I am planning to continue using opencode. &lt;/p&gt; &lt;p&gt;Okay so now the main point. &lt;/p&gt; &lt;p&gt;How do i utilise these unlimited tokens. In theory I have idea like I can have an orchestrator opencode session which can spawn worker, tester, reviewer opencode sessions instead of just subagents ? or even simple multiple subagent spawning works ??&lt;br /&gt; Since I have unlimited tokens, I can also integrate ralph loop or run multiple sessions working on same task and so on.&lt;br /&gt; But my only concern is, how do you make sure that everything is working as expected? &lt;/p&gt; &lt;p&gt;In my experience, it has happened few times where model just hallucinates. or hardcode things or does things that looks like working but very very fragile and its basically a mess. &lt;/p&gt; &lt;p&gt;and so I am not able to figure out what kind of orchestration I can do where everything is tracable. &lt;/p&gt; &lt;p&gt;I have tried using Git worktree with tmux and just let 2-3 agents work on same tasks. but again, a lot of stuff is just broken. &lt;/p&gt; &lt;p&gt;so am i expecting a lot from the first run ? is it normal to let llm do things good or bad and let tester and reviewer agents figure out next set of changes? I've seen that many times testers and reviewer agents dont cache these obvious mistakes. so how would you approach it? &lt;/p&gt; &lt;p&gt;would something like Spec-kit or BMAD type thing help ? &lt;/p&gt; &lt;p&gt;Just want to know your thoughts on how you would orchestrate things if you have unlimited &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pratiknarola"&gt; /u/pratiknarola &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlestx/glm_47_minimax_m21_opencode_orchestration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlestx/glm_47_minimax_m21_opencode_orchestration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlestx/glm_47_minimax_m21_opencode_orchestration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T05:46:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qleb9n</id>
    <title>What's holding back AMD GPU prompt processing more? ROCm / Vulkan or the actual hardware?</title>
    <updated>2026-01-24T05:21:12+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title - it keeps steadily getting better on Llama CPP over time, but how much more can really be squeezed out of existing RDNA1-4 GPU's?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qleb9n/whats_holding_back_amd_gpu_prompt_processing_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qleb9n/whats_holding_back_amd_gpu_prompt_processing_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qleb9n/whats_holding_back_amd_gpu_prompt_processing_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T05:21:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql5fzr</id>
    <title>South Korea‚Äôs ‚ÄúAI Squid Game:‚Äù a ruthless race to build sovereign AI</title>
    <updated>2026-01-23T22:43:16+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cybernews.com/ai-news/south-korea-squid-games-ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql5fzr/south_koreas_ai_squid_game_a_ruthless_race_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql5fzr/south_koreas_ai_squid_game_a_ruthless_race_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T22:43:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlbsv1</id>
    <title>Self-hosted code search for your LLMs - built this to stop wasting context on irrelevant files</title>
    <updated>2026-01-24T03:18:35+00:00</updated>
    <author>
      <name>/u/SnooBeans4154</name>
      <uri>https://old.reddit.com/user/SnooBeans4154</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, been working on this for a while and finally got it to a point worth sharing.&lt;/p&gt; &lt;p&gt;Context Engine is basically a self-hosted retrieval system specifically for codebases. Works with any MCP client (Cursor, Cline, Windsurf, Claude, and vscode etc).&lt;/p&gt; &lt;p&gt;The main thing: hybrid search that actually understands code structure. It combines dense embeddings with lexical search, AST parsing for symbols/imports, and optional micro-chunking when you need tight context windows.&lt;/p&gt; &lt;p&gt;Why we built it: got tired of either (a) dumping entire repos into context or (b) manually picking files and still missing important stuff. Wanted something that runs locally, works with whatever models you have, and doesn't send your code anywhere.&lt;/p&gt; &lt;p&gt;Tech: Qdrant for vectors, pluggable embedding models, reranking, the whole deal. One docker-compose and you're running.&lt;/p&gt; &lt;p&gt;Site: &lt;a href="https://context-engine.ai"&gt;https://context-engine.ai&lt;/a&gt; GitHub: &lt;a href="https://github.com/m1rl0k/Context-Engine"&gt;https://github.com/m1rl0k/Context-Engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Still adding features but it's stable enough for daily use. Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooBeans4154"&gt; /u/SnooBeans4154 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlbsv1/selfhosted_code_search_for_your_llms_built_this/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlbsv1/selfhosted_code_search_for_your_llms_built_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlbsv1/selfhosted_code_search_for_your_llms_built_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T03:18:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qloehi</id>
    <title>Any good LOCAL alternative or similar to what AI-Studio (Gemini 2.5 Flash) from Google does?</title>
    <updated>2026-01-24T14:29:31+00:00</updated>
    <author>
      <name>/u/VirtualWishX</name>
      <uri>https://old.reddit.com/user/VirtualWishX</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I played around with &lt;a href="http://aistudio.google.com"&gt;aistudio.google.com&lt;/a&gt; for a bit, and I could easily make an app to generate multiple images from one image (as a quick test). it created all the nice drag and drop UI and everything worked almost perfect on my first attempt. I'm not sure what is the final result it doesn't look like Gradio but the UI is nice enough to work on a web browser, also it uses online stuff probably.&lt;/p&gt; &lt;p&gt;I have some Questions, as a NOOB sorry but I'm clueless + confused: &lt;/p&gt; &lt;p&gt;I own Nvidia RTX 5090 32GB VRAM and 96GB RAM (if it helps)&lt;br /&gt; I'm aware that this is not enough because LLM are huge, but maybe there is something that can work? ü§î&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Is there a &amp;quot;close&amp;quot; or at least almost, to do something similar locally?&lt;br /&gt; so I can create some LOCAL apps, if needed to use MODELS for the app, such the example I gave on top using Z-Image or Qwen, etc.. so it looks on a local folder (or I don't mind DOWNLOAD them) the thing is: &lt;/p&gt; &lt;p&gt;1Ô∏è‚É£ - I don't know if there is such POWERFUL model I can use on &lt;strong&gt;LM-Studio&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;2Ô∏è‚É£ - I don't know if there is a way to build webUI (Gradio or anything else similar to Gemini 2.5 on AI-Studio by google because I want to create local APPS with easy to use GUI.&lt;/p&gt; &lt;p&gt;3Ô∏è‚É£ - I don't know if any of the LM-Studio models that one of you (awesome people) will recommend can also work ONLINE and look for information such as models, or download what's needed, etc.. (probably not, but I have no idea how thee things working in LM-Studio)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;Last thing,&lt;br /&gt; if anyone tried AI-Studio and also LM-Studio with something similar on RTX 5090 32GB and can tell me IT WORKS! please share your experience, what you managed to create with it, and of course... what do I need to download to prepare it to work.&lt;/p&gt; &lt;p&gt;I currently have: VS Code installed + LM Studio (with zero models downloaded)&lt;/p&gt; &lt;p&gt;Thanks ached! üôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualWishX"&gt; /u/VirtualWishX &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloehi/any_good_local_alternative_or_similar_to_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloehi/any_good_local_alternative_or_similar_to_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qloehi/any_good_local_alternative_or_similar_to_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:29:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qllmbi</id>
    <title>Home hardware coders: what's your workflow/tooling?</title>
    <updated>2026-01-24T12:22:01+00:00</updated>
    <author>
      <name>/u/Mean_Employment_7679</name>
      <uri>https://old.reddit.com/user/Mean_Employment_7679</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NOT MODELS. DONT TALK MODELS. &lt;/p&gt; &lt;p&gt;I used cursor, windsurf, kiro, Claude code, codex... etc etc.. &lt;/p&gt; &lt;p&gt;but I use them so heavily across multiple projects and run out of credits/usage extremely quickly. &lt;/p&gt; &lt;p&gt;But decided I'd love to be able to get my work done locally, especially with sensitive information. &lt;/p&gt; &lt;p&gt;So I bought a 5090, followed some guides, set up cline in vscode with ollama and the biggest models I can fit... and it's terrible. &lt;/p&gt; &lt;p&gt;I feel like it doesn't make the most of the environment and doesn't use any tools, like the model is struggling with it's limited training to even begin a task, but I feel if it intelligently searched online to get context for tasks it would be fine! But from all my trials, any model used, they just seem to fail and make life harder. &lt;/p&gt; &lt;p&gt;So I was wondering, should it be this hard with models this size? &lt;/p&gt; &lt;p&gt;Is it just going to be painful and not useful compared to cloud IDEs? I had such high hopes that running locally would allow for more micro tasking subagents to gather context and latest information before working, ensuring that although limited in size, they could actually perform well. &lt;/p&gt; &lt;p&gt;I hope I'm making sense. &lt;/p&gt; &lt;p&gt;TLDR: 5090. Models not good. Can they be good? How make good? What tools I need? I need special setup? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mean_Employment_7679"&gt; /u/Mean_Employment_7679 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qllmbi/home_hardware_coders_whats_your_workflowtooling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qllmbi/home_hardware_coders_whats_your_workflowtooling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qllmbi/home_hardware_coders_whats_your_workflowtooling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T12:22:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkxuv1</id>
    <title>Sweep: Open-weights 1.5B model for next-edit autocomplete</title>
    <updated>2026-01-23T17:57:04+00:00</updated>
    <author>
      <name>/u/Kevinlu1248</name>
      <uri>https://old.reddit.com/user/Kevinlu1248</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, we just open-sourced a 1.5B parameter model that predicts your next code edits. You can grab the weights on &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5b"&gt;Hugging Face&lt;/a&gt; or try it out via our &lt;a href="https://plugins.jetbrains.com/plugin/26860-sweep-ai-autocomp"&gt;JetBrains plugin&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes this different from regular autocomplete?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Next-edit prediction uses your &lt;em&gt;recent edits&lt;/em&gt; as context, not just the code around your cursor. So if you're renaming a variable or making repetitive changes, it anticipates what you're doing next. The model is small enough to run locally and actually outperforms models 4x its size on both speed and accuracy.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some things we learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Prompt format matters way more than expected.&lt;/strong&gt; We ran a genetic algorithm over 30+ diff formats and found that simple &lt;code&gt;&amp;lt;original&amp;gt;&lt;/code&gt; / &lt;code&gt;&amp;lt;updated&amp;gt;&lt;/code&gt; blocks beat unified diffs. Turns out verbose formats are just easier for smaller models to grok.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RL fixed what SFT couldn't.&lt;/strong&gt; Training was SFT on ~100k examples from permissively-licensed repos (4 hrs on 8xH100), then 2000 steps of RL with tree-sitter parse checking and size regularization. This cleaned up edge cases like unparseable code and overly verbose outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Benchmarks:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We tested against Mercury (Inception), Zeta (Zed), and Instinct (Continue) across five benchmarks: next-edit above/below cursor, tab-to-jump, standard FIM, and noisiness. Exact-match accuracy ended up correlating best with real-world usability since code is precise and the solution space is small.&lt;/p&gt; &lt;p&gt;We're releasing the weights so anyone can build fast, privacy-preserving autocomplete for whatever editor they use. If you're working on VSCode, Neovim, or anything else, we'd love to see what you build with it!&lt;/p&gt; &lt;p&gt;Happy to answer questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kevinlu1248"&gt; /u/Kevinlu1248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkxuv1/sweep_openweights_15b_model_for_nextedit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T17:57:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qloeu4</id>
    <title>MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations</title>
    <updated>2026-01-24T14:29:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt; &lt;img alt="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" src="https://a.thumbs.redditmedia.com/J6s266XP1Z7JRBZht5FAQGe2o36oOfOTvuiPLWD9El8.jpg" title="MiniMax Launches M2-her for Immersive Role-Play and Multi-Turn Conversations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://openrouter.ai/minimax/minimax-m2-her"&gt;https://openrouter.ai/minimax/minimax-m2-her&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MiniMax M2-her is a dialogue-first large language model built for immersive roleplay, character-driven chat, and expressive multi-turn conversations. Designed to stay consistent in tone and personality, it supports rich message roles (user_system, group, sample_message_user, sample_message_ai) and can learn from example dialogue to better match the style and pacing of your scenario, making it a strong choice for storytelling, companions, and conversational experiences where natural flow and vivid interaction matter most.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f"&gt;https://preview.redd.it/k78dwbe65bfg1.png?width=1226&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aafeaac57dbbd8cebdaa6e13bd59d657abaec09f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/api-reference/text-chat"&gt;https://platform.minimax.io/docs/api-reference/text-chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://platform.minimax.io/docs/guides/models-intro"&gt;https://platform.minimax.io/docs/guides/models-intro&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qloeu4/minimax_launches_m2her_for_immersive_roleplay_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:29:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlpwuz</id>
    <title>But a browser based AI media player: Automatic subtitles (100 languages), video chat, summaries, CTRL + F inside videos - no downloads or installs required</title>
    <updated>2026-01-24T15:30:50+00:00</updated>
    <author>
      <name>/u/ral_techspecs</name>
      <uri>https://old.reddit.com/user/ral_techspecs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlpwuz/but_a_browser_based_ai_media_player_automatic/"&gt; &lt;img alt="But a browser based AI media player: Automatic subtitles (100 languages), video chat, summaries, CTRL + F inside videos - no downloads or installs required" src="https://b.thumbs.redditmedia.com/oWMXKidE9KytLq-RPH_psOIKoQFPpy4nL2EoG48rPmo.jpg" title="But a browser based AI media player: Automatic subtitles (100 languages), video chat, summaries, CTRL + F inside videos - no downloads or installs required" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;We've been working on an all-in-one AI media player that runs entirely in the browser - no installation, no downloads, no extensions.&lt;/p&gt; &lt;p&gt;Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-generate subtitles for any video/audio&lt;/li&gt; &lt;li&gt;Translate subtitles into 100+ languages&lt;/li&gt; &lt;li&gt;Built-in dictionary for word/phrase lookup&lt;/li&gt; &lt;li&gt;Summarization of video content&lt;/li&gt; &lt;li&gt;Chat with videos (ask questions about the content, get contextual answers)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check it out &lt;a href="https://web.ray.techspecs.io/start"&gt;https://web.ray.techspecs.io/start&lt;/a&gt; and let me know what features you think we should build next.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ral_techspecs"&gt; /u/ral_techspecs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qlpwuz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlpwuz/but_a_browser_based_ai_media_player_automatic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlpwuz/but_a_browser_based_ai_media_player_automatic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T15:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlmu0j</id>
    <title>What should be my coding agent machine under 5k USD? Should I build one or purchase one of those DGX Sparks or get a mac studio? Open to anything that fits in my budget!</title>
    <updated>2026-01-24T13:21:40+00:00</updated>
    <author>
      <name>/u/pacifio</name>
      <uri>https://old.reddit.com/user/pacifio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using claude code for a while and it's pretty annoying when it I have to wait for the rate limit thing, I want to purchase a capable compute to run a capable coding model offline, perhaps GLM? not sure but I think I will figure that out but if anyone is using a local coding station please let me know, I hate just how annoying it is to wait for a couple of hours to continue my coding/brainstorming session!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pacifio"&gt; /u/pacifio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlmu0j/what_should_be_my_coding_agent_machine_under_5k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlmu0j/what_should_be_my_coding_agent_machine_under_5k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlmu0j/what_should_be_my_coding_agent_machine_under_5k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T13:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql9b7m</id>
    <title>Talk me out of buying an RTX Pro 6000</title>
    <updated>2026-01-24T01:25:33+00:00</updated>
    <author>
      <name>/u/AvocadoArray</name>
      <uri>https://old.reddit.com/user/AvocadoArray</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lately I feel the need to preface my posts saying this was &lt;strong&gt;entirely written by me with zero help from an LLM&lt;/strong&gt;. A lot of people see a long post w/ headers and automatically think it's AI slop (myself included sometimes). This post might be slop, but it's my slop.&lt;/p&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;I've been talking myself out of buying an RTX pro 6000 every day for about a month now. I can &lt;em&gt;almost&lt;/em&gt; rationalize the cost, but keep trying to put it out of my mind. Today's hitting a bit different though.&lt;/p&gt; &lt;p&gt;I can &amp;quot;afford&amp;quot; it, but I'm a cheap bastard that hates spending money because every dollar I spend is one less going to savings/retirement. For reference, this would be the single most expensive item I've bought in the last 10 years, including cars. Since I hardly ever spend this kind of money, I'm sure I could rationalize it to my wife, but it's probably only be fair for her to get similar amount of budget to spend on something fun lol, so I guess it sort of doubles the cost in a way.&lt;/p&gt; &lt;h1&gt;Intended Usage&lt;/h1&gt; &lt;p&gt;I've slowly been using more local AI at work for RAG, research, summarization and even a bit of coding with Seed OSS / Roo Code, and I constantly see ways I can benefit from that in my personal life as well. I try to do what I can with the 16GB VRAM in my 5070ti, but it's just not enough to handle the models at the size and context I want. I'm also a staunch believer in hosting locally, so cloud models are out of the question.&lt;/p&gt; &lt;p&gt;At work, 2x L4 GPUs (48GB VRAM total) is just &lt;em&gt;barely&lt;/em&gt; enough to run Seed OSS at INT4 with enough context for coding. It's also not the fastest at 20 tp/s max, which drops to around 12 tp/s at 100k context. I'd really prefer to run it at a higher quant and more unquantized F16 kv cache. I'm making the case to budget for a proper dual R6000 server at work, but that's just going to make me more jealous at home lol.&lt;/p&gt; &lt;p&gt;I've also considered getting 2x or 4x RTX 4000's (24GB/ea) piece, but that also comes with the same drawbacks of figuring out where to host them, and I suspect the power usage would be even worse. Same thing with multiple 3090s.&lt;/p&gt; &lt;h1&gt;Hardware&lt;/h1&gt; &lt;p&gt;I also just finished replaced a bunch of server/networking hardware in my home lab to drop power costs and save money, which should pay for itself after ~3.5 years. Thankfully I got all that done before the RAM shortage started driving prices up. However, my new server hardware won't support a GPU needing auxiliary power.&lt;/p&gt; &lt;p&gt;I haven't sold my old r720xd yet, and it &lt;em&gt;technically&lt;/em&gt; supports two 300w double-length cards, but that would probably be pushing the limit. The max-q edition has a 300w TDP, but the power adapter looks like it requires 2x 8-pin PCIe input to convert to CEM5, so I'd either have to run it off one cable or rig something up (maybe bring the power over from the other empty riser).&lt;/p&gt; &lt;p&gt;I also have a 4U whitebox NAS using a low-power SuperMicro Xeon E3 motherboard. It has a Corsair 1000w PSU to power the stupid amount of SAS drives I used to have in there, but now it's down to 4x SAS drives and a handful of SATA SSDs, so it could easily power the GPU as well. However, that would require a different motherboard with more PCI-E slots/lanes, which would almost certainly increase the idle power consumption (currently &amp;lt;90w).&lt;/p&gt; &lt;p&gt;I guess I could also slap it in my gaming rig to replace my 5070ti (also a painful purchase), but I'd prefer to run VLLM on a Linux VM (or bare metal) so I can run background inference while gaming as well. I also keep it&lt;/p&gt; &lt;h1&gt;Power&lt;/h1&gt; &lt;p&gt;Speaking of power usage, I'm having trouble finding real idle power usage numbers for the RTX 6000 Pro. My old GTX 1080 idled very low in the PowerEdge (only 6w with models loaded according to nvidia-smi), but somehow the L4 cards we use at work idle around ~30w in the same configuration.&lt;/p&gt; &lt;p&gt;So at this point I'm really just trying to get a solid understanding of what the ideal setup would look like in my situation, and what it would cost in terms of capex and power consumption. Then I can at least make a decision on objective facts rather than the impulsive tickle in my tummy to just pull the trigger.&lt;/p&gt; &lt;p&gt;For those of you running R6000's:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's your idle power usage (per card and whole system)?&lt;/li&gt; &lt;li&gt;Does anyone have any experience running them in &amp;quot;unsupported&amp;quot; hardware like the PowerEdge r720/r730?&lt;/li&gt; &lt;li&gt;What reasons would you &lt;strong&gt;not&lt;/strong&gt; recommend buying one?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Talk me down Reddit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AvocadoArray"&gt; /u/AvocadoArray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql9b7m/talk_me_out_of_buying_an_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T01:25:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlfu2b</id>
    <title>engine for GLM 4.7 Flash that doesn't massively slow down as the context grows?</title>
    <updated>2026-01-24T06:42:56+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man, i just tried GLM 4.7 Flash in LMstudio on a 5090 and while the 150 tokens/sec at Q6 is nice on the first prompt, things rapidly go south speedwise after 10k, unlike any other model i've tried.&lt;/p&gt; &lt;p&gt;I am using all the recommended settings and my unsloth quant, llama.cpp runtime, and lmstudio are all up to date.&lt;/p&gt; &lt;p&gt;I see that ik_llama.cpp has a recent patch that reduces this slowdown:&lt;br /&gt; &lt;a href="https://github.com/ikawrakow/ik_llama.cpp/pull/1182"&gt;https://github.com/ikawrakow/ik_llama.cpp/pull/1182&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But i can't figure out how to compile it.&lt;/p&gt; &lt;p&gt;I was wondering if the implementation in vllm or some other engine doesn't suffer of this.&lt;/p&gt; &lt;p&gt;This seems like an otherwise pretty good model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlfu2b/engine_for_glm_47_flash_that_doesnt_massively/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T06:42:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlejvk</id>
    <title>Is anyone else worried about the enshitifciation cycle of AI platforms? What is your plan (personal and corporate)</title>
    <updated>2026-01-24T05:33:33+00:00</updated>
    <author>
      <name>/u/Ngambardella</name>
      <uri>https://old.reddit.com/user/Ngambardella</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôm starting to see the oh to familiar pattern of the enshitifcation cycle starting to rear its head in the AI space.&lt;/p&gt; &lt;p&gt;For those unfamiliar, enshitification is a term that defines the ‚Äúdeliberate, gradual degradation of quality in digital platforms‚Äù. Something that we have all seen time and time again.&lt;/p&gt; &lt;p&gt;The cycle is as follows:&lt;/p&gt; &lt;p&gt;Stage 1: Good for users&lt;/p&gt; &lt;p&gt;Stage 2: Good for business customers (defined as extracting money from platform at the users expense, whether through ads, features that make the platform&lt;/p&gt; &lt;p&gt;More unusable, etc.)&lt;/p&gt; &lt;p&gt;Stage 3: Good for shareholders (the final push to squeeze every drop of remaining value out of the product, by making user experience significantly worse, as well as screwing business customers by increasing rates, worse bank for your buck, etc.)&lt;/p&gt; &lt;p&gt;I believe we are starting to enter stage 2. Although I haven‚Äôt seen any (clearly stated) ads, I have seen a lot more discussion about integrated ads in AI chats. I‚Äôve also noticed significantly reduced performance with higher usage, clearly stated rate limiting (even on paid apps), etc.&lt;/p&gt; &lt;p&gt;Right now it would be a death sentence for any company to fully enshitify, but once the competition slows down and companies start to drop out of the race, or if one company jumps significantly above the rest, we will start to really see stage 2 come to fruition.&lt;/p&gt; &lt;p&gt;In a personal setting this bothers me because I work on a lot of highly technical/niche applications and I really need accurate and consistent answers that are consistent over a larger context window, and having to start a new chat/switch apps is honestly a nightmare. To the point where I am looking to refine my workflow to allow me to switch more efficiently mid conversation.&lt;/p&gt; &lt;p&gt;In a corporate setting this is definitely going to be an issue for those not running self hosted models, it is such an easy game plan for the LLM companies to extract revenue. Get all these companies setup on your AI integrated into their internal applications, push the compliance argument, start to deprecate models/increase cost, ???, profit.&lt;/p&gt; &lt;p&gt;Thankfully most corporate applications don‚Äôt require state of the art models. But still, I think everyone should be monitoring value metrics and have contingencies in place for in both settings.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ngambardella"&gt; /u/Ngambardella &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlejvk/is_anyone_else_worried_about_the_enshitifciation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T05:33:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlkp6x</id>
    <title>Built a library of LLM prompts for RAG</title>
    <updated>2026-01-24T11:30:26+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt; &lt;img alt="Built a library of LLM prompts for RAG" src="https://a.thumbs.redditmedia.com/5xg3Rwo4RIh0LAjeGlCsGR-iU-JyYfNfHJtQ638GqV0.jpg" title="Built a library of LLM prompts for RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I gathered a set of RAG prompt templates focused on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;grounding constraints&lt;/li&gt; &lt;li&gt;citation rules&lt;/li&gt; &lt;li&gt;multi-source + uncertainty handling&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Templates are copy-pasteable. If you try one, &lt;strong&gt;upvote/downvote&lt;/strong&gt; it so the best ones float up over time.&lt;/p&gt; &lt;p&gt;And if you have a prompt that consistently works, contribute it - I‚Äôd love to include it.&lt;/p&gt; &lt;p&gt;If useful, the library is here: &lt;a href="https://agentset.ai/rag-prompts"&gt;https://agentset.ai/rag-prompts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vwuxs2jn8afg1.png?width=2660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bee373363c01d0cda6b915cc8fd8902760f8fd7c"&gt;https://preview.redd.it/vwuxs2jn8afg1.png?width=2660&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bee373363c01d0cda6b915cc8fd8902760f8fd7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlkp6x/built_a_library_of_llm_prompts_for_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T11:30:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlie1t</id>
    <title>Running MoE Models on CPU/RAM: A Guide to Optimizing Bandwidth for GLM-4 and GPT-OSS</title>
    <updated>2026-01-24T09:12:49+00:00</updated>
    <author>
      <name>/u/Shoddy_Bed3240</name>
      <uri>https://old.reddit.com/user/Shoddy_Bed3240</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The core principle of running Mixture-of-Experts (MoE) models on CPU/RAM is that the CPU doesn't need to extract or calculate all weights from memory simultaneously. Only a fraction of the parameters are &amp;quot;active&amp;quot; for any given token, and since calculations are approximate, memory throughput becomes our primary bottleneck.&lt;/p&gt; &lt;h1&gt;The Math: Model Size vs. Memory Bandwidth&lt;/h1&gt; &lt;p&gt;Let's look at two popular models: &lt;strong&gt;GLM-4.7-Flash&lt;/strong&gt; (3B active params) and &lt;strong&gt;GPT OSS 120B&lt;/strong&gt; (5.1B active params). At Q4_K_M quantization, their active memory footprints are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash:&lt;/strong&gt; ~1.7 GB&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 120B:&lt;/strong&gt; ~2.55 GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, let's look at theoretical vs. realistic &lt;strong&gt;DDR5 Dual-Channel Bandwidth&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DDR5-4800:&lt;/strong&gt; 76.8 GB/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DDR5-6000:&lt;/strong&gt; 96.0 GB/s&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DDR5-6400:&lt;/strong&gt; 102.4 GB/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Reality Check:&lt;/strong&gt; We rarely hit theoretical peaks when reading small, scattered chunks of data. A realistic &amp;quot;sustained&amp;quot; bandwidth for LLM inference is closer to &lt;strong&gt;35 GB/s&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Doing the math for DDR5-6000:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash:&lt;/strong&gt; 35 GB/s / 1.7GB = 20.5 tokens/sec&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT OSS 120B:&lt;/strong&gt; 35 GB/s / 2.55 GB = 13.7 tokens/sec&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you can fully stress your memory bus, these are the speeds you can expect.&lt;/p&gt; &lt;h1&gt;Hardware Optimization (Intel 14700f Example)&lt;/h1&gt; &lt;p&gt;To hit these numbers, your CPU and BIOS settings must be dialed in:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;XMP/EXPO:&lt;/strong&gt; Enable your XMP profile in BIOS. I successfully ran 4x16GB DDR5 sticks at 6000MT/s in dual-channel mode.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Power Limits:&lt;/strong&gt; You need the CPU to stay at its maximum boost clock to keep the memory controller saturated. I increased my Power Level (PL1/PL2) to &lt;strong&gt;219W&lt;/strong&gt; (up from the 65W default).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Thermal Management:&lt;/strong&gt; To prevent throttling at 219W, you need high-end cooling. I recommend undervolting (I used MSI Lite Load Mode 7 and disabled IA CEP) to keep temps manageable without losing performance.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Software Stack &amp;amp; Compilation&lt;/h1&gt; &lt;p&gt;I‚Äôm running on Linux with the latest drivers (Nvidia 590.48 / CUDA 13.1) and GCC 15.2. For maximum performance, you &lt;strong&gt;must&lt;/strong&gt; compile &lt;code&gt;llama.cpp&lt;/code&gt; from source with flags optimized for your specific architecture (Raptor Lake in this case).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Build Command:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cmake .. -DGGML_CUDA=ON \ -DGGML_CUDA_GRAPHS=ON \ -DGGML_CUDA_USE_CUBLASLT=ON \ -DCMAKE_CUDA_ARCHITECTURES=&amp;quot;120a;86&amp;quot; \ -DGGML_CUDA_TENSOR_CORES=ON \ -DGGML_CUDA_FP16=ON \ -DGGML_CUDA_INT8=ON \ -DGGML_AVX512=OFF \ -DGGML_AVX2=ON \ -DGGML_FMA=ON \ -DGGML_F16C=ON \ -DCMAKE_C_COMPILER=gcc-15 \ -DCMAKE_CXX_COMPILER=g++-15 \ -DCMAKE_C_FLAGS=&amp;quot;-march=raptorlake -mtune=native -O3 -flto=auto&amp;quot; \ -DCMAKE_CXX_FLAGS=&amp;quot;-march=raptorlake -mtune=native -O3 -flto=auto&amp;quot; \ -DGGML_OPENMP=ON \ -DGGML_OPENMP_DYNAMIC=ON \ -DGGML_CUDA_ENABLE_UNIFIED_MEMORY=OFF \ -DGGML_LTO=ON \ -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \ -DGGML_CUDA_BLACKWELL_NATIVE_FP4=ON \ -DGGML_CUDA_USE_CUDNN=ON \ -DGGML_CUDA_MAX_CONTEXT=32768 \ -DBUILD_SHARED_LIBS=OFF \ -DGGML_CUDA_MAX_STREAMS=8 \ -DCMAKE_BUILD_TYPE=Release &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Running the Server&lt;/h1&gt; &lt;p&gt;The key is to pin the process to your &lt;strong&gt;Performance Cores (P-cores)&lt;/strong&gt; and avoid the Efficiency Cores (E-cores), which can slow down the memory-heavy threads.&lt;/p&gt; &lt;p&gt;For the 14700f, I use &lt;code&gt;taskset&lt;/code&gt; to bind to the first 16 logical threads (P-cores):&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;taskset -c 0-15 llama-server \ -m /data/gguf/GLM-4.7-Flash/GLM-4.7-Flash-Q4_K_M.gguf \ --ctx-size 64000 \ --jinja \ -fa 1 \ --no-warmup \ --threads 16 \ --numa distribute \ --threads-batch 16 \ --host 0.0.0.0 \ --port 8080 \ --temp 1.0 \ --top-p 0.95 \ --min-p 0.01 \ --repeat-penalty 1.0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Pro Tip:&lt;/strong&gt; Don't disable your GPU! Even if the model doesn't fit entirely on the VRAM, &lt;code&gt;llama.cpp&lt;/code&gt; can offload specific layers to the GPU, providing a nice speed boost to the overall generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shoddy_Bed3240"&gt; /u/Shoddy_Bed3240 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlie1t/running_moe_models_on_cpuram_a_guide_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T09:12:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qkyex0</id>
    <title>Your post is getting popular and we just featured it on our Discord!</title>
    <updated>2026-01-23T18:16:47+00:00</updated>
    <author>
      <name>/u/roculus</name>
      <uri>https://old.reddit.com/user/roculus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Your post is getting popular and we just featured it on our Discord! Come check it out!&lt;/p&gt; &lt;p&gt;You've also been given a special flair for your contribution. We appreciate your post!&lt;/p&gt; &lt;p&gt;I am a bot and this action was performed automatically.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Can you change this marketing bot to make these private messages to the OP of the post instead of pinning it to the top of all the threads? Are you making money off the discord or something? I don't know about anyone else but these bot spam posts are annoying. You make it appear you are talking to the OP so a private message would be better. You already have a pinned thread at the top of this reddit letting everyone know about the discord that's been there for the past 5 months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roculus"&gt; /u/roculus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qkyex0/your_post_is_getting_popular_and_we_just_featured/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T18:16:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1ql6cz7</id>
    <title>Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR</title>
    <updated>2026-01-23T23:20:23+00:00</updated>
    <author>
      <name>/u/Efficient-Proof-1824</name>
      <uri>https://old.reddit.com/user/Efficient-Proof-1824</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt; &lt;img alt="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" src="https://preview.redd.it/hlrhml65m6fg1.gif?width=640&amp;amp;crop=smart&amp;amp;s=05c26e9e3a4c3182ca841254a0d81f18d6a5901f" title="Built a 100% client-side AI that plays Pokemon Red - Qwen 2.5 1.5B via WebLLM + neural network policy . Fork/check it out! BYOR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! &lt;/p&gt; &lt;p&gt;The architecture on this thing is completely wonky, and it's a direct result of me changing ideas and scope midstream, but sharing because I think it's pretty neat&lt;/p&gt; &lt;p&gt;Ultimate goal for me here is to build an agent that can play Pokemon Red, ideally beat it! Plan is to use a mix of LLMs for action plan generation and then using a small neural network to score them. Set a auto-train and you can start stacking up data for training. I bundled everything here as a Svelte app and deployed it on github pages. &lt;/p&gt; &lt;p&gt;Live: &lt;a href="https://sidmohan0.github.io/tesserack/"&gt;https://sidmohan0.github.io/tesserack/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/sidmohan0/tesserack"&gt;https://github.com/sidmohan0/tesserack&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stack:&lt;/strong&gt; &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;LLM&lt;/strong&gt;: Qwen 2.5 1.5B running via WebLLM (WebGPU-accelerated) &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Policy network&lt;/strong&gt;: TensorFlow.js neural net that learns from gameplay &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Emulator&lt;/strong&gt;: binjgb compiled to WASM &lt;/p&gt; &lt;p&gt; - &lt;strong&gt;Game state&lt;/strong&gt;: Direct RAM reading for ground-truth (badges, party, location, items) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Efficient-Proof-1824"&gt; /u/Efficient-Proof-1824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hlrhml65m6fg1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ql6cz7/built_a_100_clientside_ai_that_plays_pokemon_red/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-23T23:20:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlanzn</id>
    <title>GLM-4.7-Flash-REAP on RTX 5060 Ti 16 GB - 200k context window!</title>
    <updated>2026-01-24T02:26:28+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR: Here's my latest local coding setup, the params are mostly based on &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash#tool-calling-with-glm-4.7-flash"&gt;Unsloth's recommendation for tool calling&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-REAP-23B-A3B-GGUF"&gt;unsloth/GLM-4.7-Flash-REAP-23B-A3B-UD-Q3_K_XL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Repeat penalty: disabled&lt;/li&gt; &lt;li&gt;Temperature: 0.7&lt;/li&gt; &lt;li&gt;Top P: 1&lt;/li&gt; &lt;li&gt;Min P: 0.01&lt;/li&gt; &lt;li&gt;Standard Microcenter PC setup: RTX 5060 Ti 16 GB, 32 GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I'm running this in LM Studio for my own convenience, but it can be run in any setup you have.&lt;/p&gt; &lt;p&gt;With 16k context, everything fit within the GPU, so the speed was impressive:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;965.16 tok/s&lt;/td&gt; &lt;td&gt;26.27 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The tool calls were mostly accurate and the generated code was good, but the context window was too little, so the model ran into looping issue after exceeding that. It kept making the same tool call again and again because the conversation history was truncated.&lt;/p&gt; &lt;p&gt;With 64k context, everything still fit, but the speed started to slow down.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;671.48 tok/s&lt;/td&gt; &lt;td&gt;8.84 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I'm pushing my luck to see if 100k context still fits. It doesn't! Hahaha. The CPU fan started to scream, RAM usage spiked up, GPU copy chart (in Task Manager) started to dance. Completely unusable.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;172.02 tok/s&lt;/td&gt; &lt;td&gt;0.51 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LM Studio just got the new &amp;quot;Force Model Expert Weight onto CPU&amp;quot; feature (basically llama.cpp's &lt;code&gt;--n-cpu-moe&lt;/code&gt;), and yeah, why not? this is also an MoE model, so let's enable that. Still with 100k context. And wow! only half of the GPU memory was used (7 GB), but with 90% RAM now (29 GB), seems like flash attention also got disabled. The speed was impressive.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;485.64 tok/s&lt;/td&gt; &lt;td&gt;8.98 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's push our luck again, this time, 200k context!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;pp speed&lt;/th&gt; &lt;th&gt;tg speed&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;324.84 tok/s&lt;/td&gt; &lt;td&gt;7.70 tok/s&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What a crazy time. Almost very month we're getting beefier models that somehow fit on even crappier hardware. Just this week I was thinking of selling my 5060 for an old 3090, but that definitely unnecessary now!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlanzn/glm47flashreap_on_rtx_5060_ti_16_gb_200k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T02:26:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qljf7o</id>
    <title>AI &amp; ML Weekly ‚Äî Hugging Face Highlights</title>
    <updated>2026-01-24T10:15:01+00:00</updated>
    <author>
      <name>/u/techlatest_net</name>
      <uri>https://old.reddit.com/user/techlatest_net</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here are the most notable &lt;strong&gt;AI models released or updated this week on Hugging Face&lt;/strong&gt;, categorized for easy scanning üëá&lt;/p&gt; &lt;h1&gt;Text &amp;amp; Reasoning Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7 (358B)&lt;/strong&gt; ‚Äî Large-scale multilingual reasoning model &lt;a href="https://huggingface.co/zai-org/GLM-4.7"&gt;https://huggingface.co/zai-org/GLM-4.7&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7-Flash (31B)&lt;/strong&gt; ‚Äî Faster, optimized variant for text generation &lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;https://huggingface.co/zai-org/GLM-4.7-Flash&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unsloth GLM-4.7-Flash GGUF (30B)&lt;/strong&gt; ‚Äî Quantized version for local inference &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiquidAI LFM 2.5 Thinking (1.2B)&lt;/strong&gt; ‚Äî Lightweight reasoning-focused LLM &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alibaba DASD-4B-Thinking&lt;/strong&gt; ‚Äî Compact thinking-style language model &lt;a href="https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking"&gt;https://huggingface.co/Alibaba-Apsara/DASD-4B-Thinking&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Agent &amp;amp; Workflow Models&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Report (8B)&lt;/strong&gt; ‚Äî Agent model optimized for report generation &lt;a href="https://huggingface.co/openbmb/AgentCPM-Report"&gt;https://huggingface.co/openbmb/AgentCPM-Report&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AgentCPM-Explore (4B)&lt;/strong&gt; ‚Äî Exploration-focused agent reasoning model &lt;a href="https://huggingface.co/openbmb/AgentCPM-Explore"&gt;https://huggingface.co/openbmb/AgentCPM-Explore&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sweep Next Edit (1.5B)&lt;/strong&gt; ‚Äî Code-editing and refactoring assistant &lt;a href="https://huggingface.co/sweepai/sweep-next-edit-1.5B"&gt;https://huggingface.co/sweepai/sweep-next-edit-1.5B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Audio: Speech, Voice &amp;amp; TTS&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VibeVoice-ASR (9B)&lt;/strong&gt; ‚Äî High-quality automatic speech recognition &lt;a href="https://huggingface.co/microsoft/VibeVoice-ASR"&gt;https://huggingface.co/microsoft/VibeVoice-ASR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PersonaPlex 7B&lt;/strong&gt; ‚Äî Audio-to-audio personality-driven voice model &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;https://huggingface.co/nvidia/personaplex-7b-v1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 TTS (1.7B)&lt;/strong&gt; ‚Äî Custom &amp;amp; base voice text-to-speech models &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-Base&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-CustomVoice&lt;/a&gt; &lt;a href="https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign"&gt;https://huggingface.co/Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pocket-TTS&lt;/strong&gt; ‚Äî Lightweight open TTS model &lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;https://huggingface.co/kyutai/pocket-tts&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HeartMuLa OSS (3B)&lt;/strong&gt; ‚Äî Text-to-audio generation model &lt;a href="https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B"&gt;https://huggingface.co/HeartMuLa/HeartMuLa-oss-3B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Vision: Image, OCR &amp;amp; Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Step3-VL (10B)&lt;/strong&gt; ‚Äî Vision-language multimodal model &lt;a href="https://huggingface.co/stepfun-ai/Step3-VL-10B"&gt;https://huggingface.co/stepfun-ai/Step3-VL-10B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LightOnOCR 2 (1B)&lt;/strong&gt; ‚Äî OCR-focused vision-language model &lt;a href="https://huggingface.co/lightonai/LightOnOCR-2-1B"&gt;https://huggingface.co/lightonai/LightOnOCR-2-1B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TranslateGemma (4B / 12B / 27B)&lt;/strong&gt; ‚Äî Multimodal translation models &lt;a href="https://huggingface.co/google/translategemma-4b-it"&gt;https://huggingface.co/google/translategemma-4b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-12b-it"&gt;https://huggingface.co/google/translategemma-12b-it&lt;/a&gt; &lt;a href="https://huggingface.co/google/translategemma-27b-it"&gt;https://huggingface.co/google/translategemma-27b-it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MedGemma 1.5 (4B)&lt;/strong&gt; ‚Äî Medical-focused multimodal model &lt;a href="https://huggingface.co/google/medgemma-1.5-4b-it"&gt;https://huggingface.co/google/medgemma-1.5-4b-it&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Image Generation &amp;amp; Editing&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM-Image&lt;/strong&gt; ‚Äî Text-to-image generation model &lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;https://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FLUX.2 Klein (4B / 9B)&lt;/strong&gt; ‚Äî High-quality image-to-image models &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-4B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-4B&lt;/a&gt; &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-9B"&gt;https://huggingface.co/black-forest-labs/FLUX.2-klein-9B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen Image Edit (LoRA / AIO)&lt;/strong&gt; ‚Äî Advanced image editing &amp;amp; multi-angle edits &lt;a href="https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA"&gt;https://huggingface.co/fal/Qwen-Image-Edit-2511-Multiple-Angles-LoRA&lt;/a&gt; &lt;a href="https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO"&gt;https://huggingface.co/Phr00t/Qwen-Image-Edit-Rapid-AIO&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Z-Image-Turbo&lt;/strong&gt; ‚Äî Fast text-to-image generation &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;https://huggingface.co/Tongyi-MAI/Z-Image-Turbo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Video Generation&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LTX-2&lt;/strong&gt; ‚Äî Image-to-video generation model &lt;a href="https://huggingface.co/Lightricks/LTX-2"&gt;https://huggingface.co/Lightricks/LTX-2&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Any-to-Any / Multimodal&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chroma (6B)&lt;/strong&gt; ‚Äî Any-to-any multimodal generation &lt;a href="https://huggingface.co/FlashLabs/Chroma-4B"&gt;https://huggingface.co/FlashLabs/Chroma-4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/techlatest_net"&gt; /u/techlatest_net &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qljf7o/ai_ml_weekly_hugging_face_highlights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T10:15:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlnruw</id>
    <title>Personal experience with GLM 4.7 Flash Q6 (unsloth) + Roo Code + RTX 5090</title>
    <updated>2026-01-24T14:02:56+00:00</updated>
    <author>
      <name>/u/Septerium</name>
      <uri>https://old.reddit.com/user/Septerium</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am much more interested in how folks experience quantized versions of new models than just looking at bar graphs, so here is my humble contribution. &lt;/p&gt; &lt;p&gt;I have been using GLM 4.7 Flash to perform a few refactoring tasks in some personal web projects and have been quite impressed by how well the model handles Roo Code without breaking apart. For this agentic tool specifically, it has been much more reliable and precise than GPT-OSS 120b, GLM 4.5 Air, or Devstral 24b.&lt;/p&gt; &lt;p&gt;Here's the llama.cpp command I used to squeeze UD-Q6_K_XL + 48k tokens of context in my RTX 5090 VRAM and get about 150 tok/s (tg):&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server --model downloaded_models/GLM-4.7-Flash-UD-Q6_K_XL.gguf --port 11433 --host &amp;quot;0.0.0.0&amp;quot; -fa on --ctx-size 48000 --temp 0.7 --top-p 1.0 --min-p 0.01 --jinja -ngl 99&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Septerium"&gt; /u/Septerium &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlnruw/personal_experience_with_glm_47_flash_q6_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T14:02:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
