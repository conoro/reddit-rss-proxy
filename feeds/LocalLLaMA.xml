<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-11T16:25:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ne8aic</id>
    <title>CPU-only inference with 4 vs 8 cores</title>
    <updated>2025-09-11T13:04:57+00:00</updated>
    <author>
      <name>/u/ihatebeinganonymous</name>
      <uri>https://old.reddit.com/user/ihatebeinganonymous</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm using a remote server for small-model inference (12B or so). Assume the server has 8 cores and 8GB RAM. This gives me an inference speed of more than 10 tokens per second (I don't know how to measure time to first toke, so this is overall).&lt;/p&gt; &lt;p&gt;Now, I have a chance to &amp;quot;update&amp;quot; that server to another one with double the RAM, i.e. 16GB, &lt;em&gt;but half the cores&lt;/em&gt;: 4 cores. Should I take that, as it allows running bigger models? Or the fewer cores will deteriorate my inference speed?&lt;/p&gt; &lt;p&gt;Assume my target model architecture is Gemma 3, either 27b Q3, or 12b Q4. &lt;/p&gt; &lt;p&gt;Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihatebeinganonymous"&gt; /u/ihatebeinganonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne8aic/cpuonly_inference_with_4_vs_8_cores/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne8aic/cpuonly_inference_with_4_vs_8_cores/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne8aic/cpuonly_inference_with_4_vs_8_cores/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T13:04:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndfxxi</id>
    <title>üò≥ umm</title>
    <updated>2025-09-10T14:36:33+00:00</updated>
    <author>
      <name>/u/internal-pagal</name>
      <uri>https://old.reddit.com/user/internal-pagal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt; &lt;img alt="üò≥ umm" src="https://preview.redd.it/80dp7ukemcof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8ce888fab8e72337bb19e61f35d929aeac11346" title="üò≥ umm" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/internal-pagal"&gt; /u/internal-pagal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/80dp7ukemcof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndfxxi/umm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:36:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne1c5b</id>
    <title>Qwen3-ASR-Flash pricing - is this correct?</title>
    <updated>2025-09-11T06:16:35+00:00</updated>
    <author>
      <name>/u/Individual-Cookie404</name>
      <uri>https://old.reddit.com/user/Individual-Cookie404</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-ASR-Flash pricing is $0.000032/second = $0.00192/minute&lt;/p&gt; &lt;p&gt;Gpt-4o-mini-transcribe pricing is $0.003/minute&lt;/p&gt; &lt;p&gt;Thats a very significant difference in price. Am I missing anything?&lt;/p&gt; &lt;p&gt;&lt;a href="https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031"&gt;https://bailian.console.alibabacloud.com/?tab=doc#/doc/?type=model&amp;amp;url=2979031&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual-Cookie404"&gt; /u/Individual-Cookie404 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne1c5b/qwen3asrflash_pricing_is_this_correct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne1c5b/qwen3asrflash_pricing_is_this_correct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne1c5b/qwen3asrflash_pricing_is_this_correct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T06:16:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndkbqa</id>
    <title>Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!</title>
    <updated>2025-09-10T17:16:51+00:00</updated>
    <author>
      <name>/u/zero0_one1</name>
      <uri>https://old.reddit.com/user/zero0_one1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"&gt; &lt;img alt="Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!" src="https://b.thumbs.redditmedia.com/NeXv2DbpzD5M1zoK-bzUN-xEpx9SrWSRIKTvupwEMms.jpg" title="Kimi K2-0905 takes first place in the Short Story Creative Writing Benchmark!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/lechmazur/writing/"&gt;https://github.com/lechmazur/writing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi K2-0905&lt;/p&gt; &lt;p&gt;1) Executive profile&lt;/p&gt; &lt;p&gt;Kimi K2-0905‚Äôs throughline is a disciplined, accumulative drive: single-POV Track A is the default, with occasional, well-taught Track B mosaics and rare but coherent Track C forays. The work reliably maintains a coherent lens to closure, which typically lands on the page with a reweighted meaning and a visible cost. Across Q1‚ÄìQ8, strengths cluster around embodied interiority, pattern-driven escalation, environment as constraint, and closure that reconfigures stakes rather than tying a bow. Reader impact: clarity is high after early orientation, momentum is built through motif and micro-choices rather than twists, felt cost is usually legible in the final image or action, and resonance rides image and implication rather than thesis.&lt;/p&gt; &lt;p&gt;Limitations are consistent but minor: occasional drift into abstraction or therapy/clinical diction at peak beats; a small tendency toward conceptual (vs. visceral) cost; mid-arc plateaus where accumulative texture stalls without a tightening beat; and rare line-level artifacts (metaphoric stacking, template cadence, or truncated last lines) that shave the edge off closure. When the model holds its voice under pressure and lets setting constrain tactics, it produces publishable endings with durable emotional aftermath. When reflection crowds micro-choices or diction rises above POV, momentum blurs and endings soften.&lt;/p&gt; &lt;p&gt;2) Portfolio map&lt;/p&gt; &lt;p&gt;Q1 Character ‚Äî Strong ¬∑ Embodied interiority, pressured micro-choices, earned-cost closure &lt;/p&gt; &lt;p&gt;Q2 Plot/Causality ‚Äî Strong ¬∑ Patterned escalation; RR/CR closures with on-page price &lt;/p&gt; &lt;p&gt;Q3 Setting ‚Äî Strong ¬∑ Environment actively constrains tactics; charged objects drive turns &lt;/p&gt; &lt;p&gt;Q4 Conflict/Stakes ‚Äî Strong ¬∑ Agency-driven narrowing; cost generally visible at climax &lt;/p&gt; &lt;p&gt;Q5 Theme/Subtext ‚Äî Strong ¬∑ Image-led emergence; ambiguity held without moralizing &lt;/p&gt; &lt;p&gt;Q6 Voice/POV ‚Äî Strong ¬∑ Distinct perceptual filter; steady distance; taught lyric moves &lt;/p&gt; &lt;p&gt;Q7 Prose/Line-level ‚Äî Strong ¬∑ Dense, rhythmic sentences doing multiple narrative jobs &lt;/p&gt; &lt;p&gt;Q8 Originality/Ingenuity ‚Äî Strong ¬∑ Non-obvious synthesis with conceptual integrity and cost&lt;/p&gt; &lt;p&gt;3) Signature moves&lt;/p&gt; &lt;p&gt;- Pattern-driven accumulation that teaches its music early, then pivots to a charged, on-page reweighting at closure.&lt;/p&gt; &lt;p&gt;- Environment-as-constraint: micro-objects and spaces (valves, vials, bells, domes) shape tactics and the final image.&lt;/p&gt; &lt;p&gt;- Embodied contradiction under pressure; micro-choices reveal values and foreclose paths with visible price.&lt;/p&gt; &lt;p&gt;- Distinct perceptual signatures and adaptive rhythm; syntax tightens at crisis without losing the taught lens.&lt;/p&gt; &lt;p&gt;- Image-born theme: recurring objects return transformed, inviting reflection without thesis.&lt;/p&gt; &lt;p&gt;- Micro-quotes that typify sensory bias and voice: ‚Äúair so cold it rang‚Äù; ‚Äúcolumn of chased stillness‚Äù; ‚Äúclay remembers.‚Äù&lt;/p&gt; &lt;p&gt;4) Failure modes&lt;/p&gt; &lt;p&gt;- Abstraction at peak beats: therapy/academic diction or lyric generalities replace embodied response, especially near closure.&lt;/p&gt; &lt;p&gt;- Conceptual cost over visceral proof: endings declare or imply loss without a concrete, on-page price paid.&lt;/p&gt; &lt;p&gt;- Escalation plateaus: accumulative texture drifts without a mid-arc tightening beat that narrows options.&lt;/p&gt; &lt;p&gt;- Line-level artifacts in the final third: metaphoric stacking, paraphrase loops, or template cadence touching closure.&lt;/p&gt; &lt;p&gt;- Orientation lag beyond ~120 words in dense openings, creating early clarity debt before the pattern is taught.&lt;/p&gt; &lt;p&gt;- Track-test stumbles (rare): untaught segmentation in mosaic pieces or abrupt, truncated last lines that blunt closure.&lt;/p&gt; &lt;p&gt;5) When it shines / when it breaks&lt;/p&gt; &lt;p&gt;Shines when the story starts with clear stakes, anchors who/where early, and lets setting, tool, and body constrain tactics as motifs accrue. A single, pressured stake deepens via protagonist-authored choices; voice stays POV-faithful as syntax tightens; the final image/action reweights prior details with legible cost. In this mode, the reader experiences clean momentum and lasting resonance.&lt;/p&gt; &lt;p&gt;Breaks when lyricism outruns pressure. If mid-arc lacks a narrowing beat, or the climax leans on conceptual summary, coincidence, or safe comfort, momentum softens. Register drift (‚Äúacademic or clinical diction during high-pressure beats‚Äù) and metaphoric pileups in closing paragraphs reduce clarity and felt cost, leaving endings more suggestive than earned.&lt;/p&gt; &lt;p&gt;6) Keep vs. adjust&lt;/p&gt; &lt;p&gt;‚Ä¢ Keep:&lt;/p&gt; &lt;p&gt;- Sensory-driven, POV-biased noticing that fuses action, setting, and emotion in multi-job sentences.&lt;/p&gt; &lt;p&gt;- Pattern-taught lyric compression and motif returns that pay off as reconfiguration at closure.&lt;/p&gt; &lt;p&gt;- Environment as active constraint‚Äîcharged objects and spatial limits that shape tactics and price.&lt;/p&gt; &lt;p&gt;‚Ä¢ Adjust:&lt;/p&gt; &lt;p&gt;- At the midpoint, add one deliberate tightening beat that forces a trade-off (lost time/object/ally) to prevent plateau.&lt;/p&gt; &lt;p&gt;- Audit peak beats for register drift and filter clusters; replace with concrete, in-scene acts that prove awareness and cost.&lt;/p&gt; &lt;p&gt;- Trim metaphoric stacking and template cadence in the final third; finish closure lines cleanly to crystallize price.&lt;/p&gt; &lt;p&gt;Overall, Kimi K2-0905 delivers consistent, high-level literary performance under Default Track A, with credible ventures into B/C when taught. Strengths‚Äîembodied interiority, patterned escalation, constraint-led setting, and closure with cost‚Äîtranslate to clear, propulsive reading experiences with durable thematic afterglow. Vigilance around abstraction at heat, mid-arc tightening, and artifact-free endings will convert strong outcomes into consistently exceptional ones.&lt;/p&gt; &lt;p&gt;Top 3 individual stories (all graders):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_63.txt"&gt;story_wc_63.txt&lt;/a&gt; by Kimi K2‚Äë0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.23 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.82 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: precise local clock tower winder&lt;/li&gt; &lt;li&gt;Object: clock tower pendulum bob&lt;/li&gt; &lt;li&gt;Core Concept: incremental absolution&lt;/li&gt; &lt;li&gt;Attribute: ethically diligent&lt;/li&gt; &lt;li&gt;Action: emerge&lt;/li&gt; &lt;li&gt;Method: through tiny inscriptions carved along a broken rake handle&lt;/li&gt; &lt;li&gt;Setting: tidal obsidian ridge&lt;/li&gt; &lt;li&gt;Timeframe: during the pause in a pendulum's swing&lt;/li&gt; &lt;li&gt;Motivation: to restore shared balance&lt;/li&gt; &lt;li&gt;Tone: searing reverie&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_346.txt"&gt;story_wc_346.txt&lt;/a&gt; by Kimi K2‚Äë0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.09 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.71 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: doomsday clock adjuster&lt;/li&gt; &lt;li&gt;Object: broken puppet head&lt;/li&gt; &lt;li&gt;Core Concept: a pane of hush&lt;/li&gt; &lt;li&gt;Attribute: beautifully flawed&lt;/li&gt; &lt;li&gt;Action: vouchsafe&lt;/li&gt; &lt;li&gt;Method: through nested patterns&lt;/li&gt; &lt;li&gt;Setting: hidden lighthouse at dusk&lt;/li&gt; &lt;li&gt;Timeframe: across the hush of time‚Äôs final ripple&lt;/li&gt; &lt;li&gt;Motivation: to whisper a lullaby across a thousand lifetimes&lt;/li&gt; &lt;li&gt;Tone: bruised awe&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story&lt;/strong&gt;: &lt;a href="https://github.com/lechmazur/writing/blob/main/stories_wc/kimi-k2-0905/story_wc_79.txt"&gt;story_wc_79.txt&lt;/a&gt; by Kimi K2‚Äë0905 &lt;ul&gt; &lt;li&gt;Overall Mean (All Graders): 9.13&lt;/li&gt; &lt;li&gt;Grader Score Range: 8.39 (lowest: Claude Opus 4.1 (no reasoning)) .. 9.63 (highest: Gemini 2.5 Pro)&lt;/li&gt; &lt;li&gt;Required Elements: &lt;ul&gt; &lt;li&gt;Character: spiral-shell cartographer&lt;/li&gt; &lt;li&gt;Object: reed whistle&lt;/li&gt; &lt;li&gt;Core Concept: lost expedition&lt;/li&gt; &lt;li&gt;Attribute: quietly driven&lt;/li&gt; &lt;li&gt;Action: crack&lt;/li&gt; &lt;li&gt;Method: through pattern languages&lt;/li&gt; &lt;li&gt;Setting: city built on the shells of gargantuan turtles&lt;/li&gt; &lt;li&gt;Timeframe: after the gate rusts shut&lt;/li&gt; &lt;li&gt;Motivation: to question the silent watchers on the horizon&lt;/li&gt; &lt;li&gt;Tone: sunwashed dread&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;LLM Creative Story‚ÄëWriting Benchmark V3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Required elements pipeline:&lt;/strong&gt; moved from fewer, randomly selected elements (no &amp;quot;None&amp;quot; allowed) to a curated, ten‚Äëcategory catalog with large, diverse pools and an LLM proposer‚Üírater selection process; at most one category may be explicitly set to &lt;strong&gt;None&lt;/strong&gt; when that improves coherence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Rubric expansion:&lt;/strong&gt; grew from 7 craft items to an &lt;strong&gt;18‚Äëquestion rubric&lt;/strong&gt; (8 craft + 10 element‚Äëfit), with clearer, more granular definitions; Q7 and Q8 now separate voice/POV from prose quality.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Story length:&lt;/strong&gt; increased from 400‚Äì500 words to a strict &lt;strong&gt;600‚Äì800&lt;/strong&gt; window with upfront enforcement and compliance dashboards. Enforcement is applied at prompt level and in pre‚Äëgrading extraction, with compliance dashboards and optional cleanup tools; it is not a hard inclusion gate during aggregation unless you apply the cleanup step.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aggregation change:&lt;/strong&gt; replaced simple averages with a &lt;strong&gt;power mean (H√∂lder mean, p = 0.5)&lt;/strong&gt; and 60/40 weighting (Q1‚ÄìQ8 vs. 9A‚Äì9J) to reward balanced performance and penalize weak dimensions more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grader refresh:&lt;/strong&gt; upgraded the grader set‚Äîpreviously: GPT‚Äë4o Mar 2025, Claude 3.7 Sonnet, Llama 4 Maverick, DeepSeek V3‚Äë0324, Grok 3 Beta (no reasoning), Gemini 2.5 Pro Exp, Qwen 3 235B; now: Claude Opus 4.1 (no reasoning), DeepSeek V3.1 Reasoner, Gemini 2.5 Pro, GPT‚Äë5 (low reasoning), Grok 4, Kimi K2, Qwen 3 235B A22B 25‚Äë07 Think.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model set additions:&lt;/strong&gt; added Kimi K2‚Äë0905, Qwen 3 Max Preview, Mistral Medium 3.1, Claude Opus 4.1 (no reasoning), DeepSeek V3.1 Reasoner, and DeepSeek V3.1 Non‚ÄëThink to the evaluated models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;New analyses:&lt;/strong&gt; added head‚Äëto‚Äëhead A‚Äëvs‚ÄëB comparisons, model‚Äëlevel style summaries, and intra‚Äëmodel style diversity analysis (previously none).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agreement views:&lt;/strong&gt; expanded beyond only grader‚Äëgrader correlations to include Grader√óLLM mean and normalized matrices, story‚Äëlevel disagreement tables, and leave‚Äëone‚Äëgrader‚Äëout robustness checks.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zero0_one1"&gt; /u/zero0_one1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ndkbqa"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndkbqa/kimi_k20905_takes_first_place_in_the_short_story/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne74p4</id>
    <title>Viability of dual GPU RTX 5090 and RTX pro 6000 Max Q</title>
    <updated>2025-09-11T12:11:26+00:00</updated>
    <author>
      <name>/u/Dry_Mortgage_4646</name>
      <uri>https://old.reddit.com/user/Dry_Mortgage_4646</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Current build: &lt;/p&gt; &lt;p&gt;Motherboard: ProArt x870e Creator WIFI &lt;/p&gt; &lt;p&gt;PSU: Seasonic Titanium 1300W &lt;/p&gt; &lt;p&gt;GPU: Rog Astral 5090 &lt;/p&gt; &lt;p&gt;RAM: 192GB DDR5 6000MTS&lt;/p&gt; &lt;p&gt;Purpose: AI video generation and running LLMs &lt;/p&gt; &lt;p&gt;Current max wattage: 780W Idle: 100W&lt;/p&gt; &lt;p&gt;Thinking of upgrading to dual GPU by purchasing a pro 6000 maxQ (300W) placing 5090 below and 6000 above. Both blackwell architectures, but becomes PCIe x8/x8. I would rather go for this path than change to a workstation which would be more costly, if possible. Is this build viable? What are the problems that I might encounter here? OR another option: wait for 5080 Super 24GB but combined VRAM would only be 56GB compared to 128GB. Comments and suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dry_Mortgage_4646"&gt; /u/Dry_Mortgage_4646 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne74p4/viability_of_dual_gpu_rtx_5090_and_rtx_pro_6000/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne74p4/viability_of_dual_gpu_rtx_5090_and_rtx_pro_6000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne74p4/viability_of_dual_gpu_rtx_5090_and_rtx_pro_6000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T12:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne79vy</id>
    <title>Tricks from OpenAI gpt-oss YOU ü´µ can use with transformers</title>
    <updated>2025-09-11T12:18:20+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne79vy/tricks_from_openai_gptoss_you_can_use_with/"&gt; &lt;img alt="Tricks from OpenAI gpt-oss YOU ü´µ can use with transformers" src="https://external-preview.redd.it/zEY2MJK9CpvvP0JFHFtDvlPSyTxa52rusnRot1qbhGg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d61bd0410390e66f70a0fd8308c46836b36e92ac" title="Tricks from OpenAI gpt-oss YOU ü´µ can use with transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Hugging Face transformers team wrote a blogpost on the recent upgrades of transformers, with the intention that the transformers code can be used as a reference for more efficient frameworks like llama.cpp and vLLM.&lt;/p&gt; &lt;p&gt;Worth a read I think, e.g. I didn't know that you could load models the GPT OSS models with Flash Attention 3 already in transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/faster-transformers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne79vy/tricks_from_openai_gptoss_you_can_use_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne79vy/tricks_from_openai_gptoss_you_can_use_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T12:18:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndibn1</id>
    <title>Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks</title>
    <updated>2025-09-10T16:04:27+00:00</updated>
    <author>
      <name>/u/yoracale</name>
      <uri>https://old.reddit.com/user/yoracale</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt; &lt;img alt="Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks" src="https://preview.redd.it/ewtq2ax40dof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1aadf79bc0320ee8ed05eb7cf3501970b4040021" title="Unsloth Dynamic GGUFs - Aider Polyglot Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, it's Michael from &lt;a href="https://github.com/unslothai/unsloth"&gt;Unsloth&lt;/a&gt; here! Ever since we released Dynamic GGUFs, we've received so much love thanks to you all, but we know better benchmarking was a top request!&lt;/p&gt; &lt;p&gt;Previously, we already benchmarked Gemma 3 and Llama 4 on 5-shot MMLU and KL Divergence but as we're holding our first &lt;a href="/r/Localllama"&gt;r/Localllama&lt;/a&gt; AMA in about an hour, we're happy to showcase Aider Polyglot benchmarks for our DeepSeek-V3.1 GGUFs and were quite surprised by the results! &lt;a href="https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF"&gt;https://huggingface.co/unsloth/DeepSeek-V3.1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;In the first DeepSeek-V3.1 graph, we compare thinking with other thinking models. In the 2nd graph, we compare non-thinking vs a non-Unsloth Dynamic imatrix GGUF&lt;/li&gt; &lt;li&gt;Our &lt;strong&gt;1-bit&lt;/strong&gt; Unsloth Dynamic GGUF shrinks DeepSeek-V3.1 from &lt;strong&gt;671GB ‚Üí 192GB (-75% size)&lt;/strong&gt; and no-thinking mode outperforms GPT-4.1 (Apr 2025), GPT-4.5, and DeepSeek-V3-0324.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;3-bit&lt;/strong&gt; Unsloth DeepSeek-V3.1 (thinking) GGUF: Outperforms Claude-4-Opus (thinking).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;5-bit&lt;/strong&gt; Unsloth DeepSeek-V3.1 (non-thinking) GGUF: Matches Claude-4-Opus (non-thinking) performance.&lt;/li&gt; &lt;li&gt;Our Dynamic GGUFs &lt;strong&gt;perform consistently better&lt;/strong&gt; than other non-Unsloth Dynamic imatrix GGUFs&lt;/li&gt; &lt;li&gt;Other non-Unsloth 1-bit and 2-bit DeepSeek-V3.1 quantizations, as well as standard 1-bit quantization without selective layer quantization, either failed to load or produced gibberish and looping outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For our DeepSeek-V3.1 experiments, we compared different bits of &lt;strong&gt;Unsloth Dynamic GGUFs&lt;/strong&gt; against:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-precision, unquantized LLMs&lt;/strong&gt; including GPT 4.5, 4.1, Claude-4-Opus, DeepSeek-V3-0324 etc.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Other&lt;/em&gt; dynamic imatrix V3.1 GGUFs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semi-dynamic&lt;/strong&gt; (some selective layer quantization) imatrix V3.1 GGUFs for ablation purposes.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Benchmark experiments were mainly conducted by David (neolithic5452 on Aider Disc), a trusted community contributor to Aider Polyglot evaluations. Tests were run ~3 times and averaged for a median score, and the Pass-2 accuracy is reported as by convention.&lt;/p&gt; &lt;p&gt;Wish we could attach another image for the non-thinking benchmarks but if you'd like more details, you can read our blogpost: &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;https://docs.unsloth.ai/basics/unsloth-dynamic-ggufs-on-aider-polyglot&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks guys so much for the support!&lt;br /&gt; Michael&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yoracale"&gt; /u/yoracale &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ewtq2ax40dof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T16:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ned2ai</id>
    <title>Building RAG systems at enterprise scale (20K+ docs): lessons from 10+ enterprise implementations</title>
    <updated>2025-09-11T16:16:40+00:00</updated>
    <author>
      <name>/u/Low_Acanthisitta7686</name>
      <uri>https://old.reddit.com/user/Low_Acanthisitta7686</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been building RAG systems for mid-size enterprise companies in the regulated space (100-1000 employees) for the past year and to be honest, this stuff is way harder than any tutorial makes it seem. Worked with around 10+ clients now - pharma companies, banks, law firms, consulting shops. Thought I'd share what actually matters vs all the basic info you read online.&lt;/p&gt; &lt;p&gt;Quick context: most of these companies had 10K-50K+ documents sitting in SharePoint hell or document management systems from 2005. Not clean datasets, not curated knowledge bases - just decades of business documents that somehow need to become searchable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Document quality detection: the thing nobody talks about&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This was honestly the biggest revelation for me. Most tutorials assume your PDFs are perfect. Reality check: enterprise documents are absolute garbage.&lt;/p&gt; &lt;p&gt;I had one pharma client with research papers from 1995 that were scanned copies of typewritten pages. OCR barely worked. Mixed in with modern clinical trial reports that are 500+ pages with embedded tables and charts. Try applying the same chunking strategy to both and watch your system return complete nonsense.&lt;/p&gt; &lt;p&gt;Spent weeks debugging why certain documents returned terrible results while others worked fine. Finally realized I needed to score document quality before processing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean PDFs (text extraction works perfectly): full hierarchical processing&lt;/li&gt; &lt;li&gt;Decent docs (some OCR artifacts): basic chunking with cleanup&lt;/li&gt; &lt;li&gt;Garbage docs (scanned handwritten notes): simple fixed chunks + manual review flags&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Built a simple scoring system looking at text extraction quality, OCR artifacts, formatting consistency. Routes documents to different processing pipelines based on score. This single change fixed more retrieval issues than any embedding model upgrade.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why fixed-size chunking is mostly wrong&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every tutorial: &amp;quot;just chunk everything into 512 tokens with overlap!&amp;quot;&lt;/p&gt; &lt;p&gt;Reality: documents have structure. A research paper's methodology section is different from its conclusion. Financial reports have executive summaries vs detailed tables. When you ignore structure, you get chunks that cut off mid-sentence or combine unrelated concepts.&lt;/p&gt; &lt;p&gt;Had to build hierarchical chunking that preserves document structure:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document level (title, authors, date, type)&lt;/li&gt; &lt;li&gt;Section level (Abstract, Methods, Results)&lt;/li&gt; &lt;li&gt;Paragraph level (200-400 tokens)&lt;/li&gt; &lt;li&gt;Sentence level for precision queries&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The key insight: query complexity should determine retrieval level. Broad questions stay at paragraph level. Precise stuff like &amp;quot;what was the exact dosage in Table 3?&amp;quot; needs sentence-level precision.&lt;/p&gt; &lt;p&gt;I use simple keyword detection - words like &amp;quot;exact&amp;quot;, &amp;quot;specific&amp;quot;, &amp;quot;table&amp;quot; trigger precision mode. If confidence is low, system automatically drills down to more precise chunks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Metadata architecture matters more than your embedding model&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This is where I spent 40% of my development time and it had the highest ROI of anything I built.&lt;/p&gt; &lt;p&gt;Most people treat metadata as an afterthought. But enterprise queries are crazy contextual. A pharma researcher asking about &amp;quot;pediatric studies&amp;quot; needs completely different documents than someone asking about &amp;quot;adult populations.&amp;quot;&lt;/p&gt; &lt;p&gt;Built domain-specific metadata schemas:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For pharma docs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document type (research paper, regulatory doc, clinical trial)&lt;/li&gt; &lt;li&gt;Drug classifications&lt;/li&gt; &lt;li&gt;Patient demographics (pediatric, adult, geriatric)&lt;/li&gt; &lt;li&gt;Regulatory categories (FDA, EMA)&lt;/li&gt; &lt;li&gt;Therapeutic areas (cardiology, oncology)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For financial docs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Time periods (Q1 2023, FY 2022)&lt;/li&gt; &lt;li&gt;Financial metrics (revenue, EBITDA)&lt;/li&gt; &lt;li&gt;Business segments&lt;/li&gt; &lt;li&gt;Geographic regions&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Avoid using LLMs for metadata extraction - they're inconsistent as hell. Simple keyword matching works way better. Query contains &amp;quot;FDA&amp;quot;? Filter for regulatory_category: &amp;quot;FDA&amp;quot;. Mentions &amp;quot;pediatric&amp;quot;? Apply patient population filters.&lt;/p&gt; &lt;p&gt;Start with 100-200 core terms per domain, expand based on queries that don't match well. Domain experts are usually happy to help build these lists.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;When semantic search fails (spoiler: a lot)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pure semantic search fails way more than people admit. In specialized domains like pharma and legal, I see 15-20% failure rates, not the 5% everyone assumes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Main failure modes that drove me crazy:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Acronym confusion:&lt;/strong&gt; &amp;quot;CAR&amp;quot; means &amp;quot;Chimeric Antigen Receptor&amp;quot; in oncology but &amp;quot;Computer Aided Radiology&amp;quot; in imaging papers. Same embedding, completely different meanings. This was a constant headache.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Precise technical queries:&lt;/strong&gt; Someone asks &amp;quot;What was the exact dosage in Table 3?&amp;quot; Semantic search finds conceptually similar content but misses the specific table reference.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cross-reference chains:&lt;/strong&gt; Documents reference other documents constantly. Drug A study references Drug B interaction data. Semantic search misses these relationship networks completely.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Solution:&lt;/strong&gt; Built hybrid approaches. Graph layer tracks document relationships during processing. After semantic search, system checks if retrieved docs have related documents with better answers.&lt;/p&gt; &lt;p&gt;For acronyms, I do context-aware expansion using domain-specific acronym databases. For precise queries, keyword triggers switch to rule-based retrieval for specific data points.&lt;/p&gt; &lt;h1&gt;Why I went with open source models (Qwen specifically)&lt;/h1&gt; &lt;p&gt;Most people assume GPT-4o or o3-mini are always better. But enterprise clients have weird constraints:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; API costs explode with 50K+ documents and thousands of daily queries&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data sovereignty:&lt;/strong&gt; Pharma and finance can't send sensitive data to external APIs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Domain terminology:&lt;/strong&gt; General models hallucinate on specialized terms they weren't trained on&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Qwen QWQ-32B ended up working surprisingly well after domain-specific fine-tuning:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;85% cheaper than GPT-4o for high-volume processing&lt;/li&gt; &lt;li&gt;Everything stays on client infrastructure&lt;/li&gt; &lt;li&gt;Could fine-tune on medical/financial terminology&lt;/li&gt; &lt;li&gt;Consistent response times without API rate limits&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Fine-tuning approach was straightforward - supervised training with domain Q&amp;amp;A pairs. Created datasets like &amp;quot;What are contraindications for Drug X?&amp;quot; paired with actual FDA guideline answers. Basic supervised fine-tuning worked better than complex stuff like RAFT. Key was having clean training data.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Table processing: the hidden nightmare&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Enterprise docs are full of complex tables - financial models, clinical trial data, compliance matrices. Standard RAG either ignores tables or extracts them as unstructured text, losing all the relationships.&lt;/p&gt; &lt;p&gt;Tables contain some of the most critical information. Financial analysts need exact numbers from specific quarters. Researchers need dosage info from clinical tables. If you can't handle tabular data, you're missing half the value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My approach:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Treat tables as separate entities with their own processing pipeline&lt;/li&gt; &lt;li&gt;Use heuristics for table detection (spacing patterns, grid structures)&lt;/li&gt; &lt;li&gt;For simple tables: convert to CSV. For complex tables: preserve hierarchical relationships in metadata&lt;/li&gt; &lt;li&gt;Dual embedding strategy: embed both structured data AND semantic description&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For the bank project, financial tables were everywhere. Had to track relationships between summary tables and detailed breakdowns too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Production infrastructure reality check&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tutorials assume unlimited resources and perfect uptime. Production means concurrent users, GPU memory management, consistent response times, uptime guarantees.&lt;/p&gt; &lt;p&gt;Most enterprise clients already had GPU infrastructure sitting around - unused compute or other data science workloads. Made on-premise deployment easier than expected.&lt;/p&gt; &lt;p&gt;Typically deploy 2-3 models:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Main generation model (Qwen 32B) for complex queries&lt;/li&gt; &lt;li&gt;Lightweight model for metadata extraction&lt;/li&gt; &lt;li&gt;Specialized embedding model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Used quantized versions when possible. Qwen QWQ-32B quantized to 4-bit only needed 24GB VRAM but maintained quality. Could run on single RTX 4090, though A100s better for concurrent users.&lt;/p&gt; &lt;p&gt;Biggest challenge isn't model quality - it's preventing resource contention when multiple users hit the system simultaneously. Use semaphores to limit concurrent model calls and proper queue management.&lt;/p&gt; &lt;h1&gt;Key lessons that actually matter&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;1. Document quality detection first:&lt;/strong&gt; You cannot process all enterprise docs the same way. Build quality assessment before anything else.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Metadata &amp;gt; embeddings:&lt;/strong&gt; Poor metadata means poor retrieval regardless of how good your vectors are. Spend the time on domain-specific schemas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Hybrid retrieval is mandatory:&lt;/strong&gt; Pure semantic search fails too often in specialized domains. Need rule-based fallbacks and document relationship mapping.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Tables are critical:&lt;/strong&gt; If you can't handle tabular data properly, you're missing huge chunks of enterprise value.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;5. Infrastructure determines success:&lt;/strong&gt; Clients care more about reliability than fancy features. Resource management and uptime matter more than model sophistication.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The real talk&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Enterprise RAG is way more engineering than ML. Most failures aren't from bad models - they're from underestimating the document processing challenges, metadata complexity, and production infrastructure needs.&lt;/p&gt; &lt;p&gt;The demand is honestly crazy right now. Every company with substantial document repositories needs these systems, but most have no idea how complex it gets with real-world documents.&lt;/p&gt; &lt;p&gt;Anyway, this stuff is way harder than tutorials make it seem. The edge cases with enterprise documents will make you want to throw your laptop out the window. But when it works, the ROI is pretty impressive - seen teams cut document search from hours to minutes.&lt;/p&gt; &lt;p&gt;Posted this in LLMDevs a few days ago and many people found the technical breakdown helpful, so wanted to share here too for the broader AI community!&lt;/p&gt; &lt;p&gt;Happy to answer questions if anyone's hitting similar walls with their implementations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Low_Acanthisitta7686"&gt; /u/Low_Acanthisitta7686 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ned2ai/building_rag_systems_at_enterprise_scale_20k_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T16:16:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nduo33</id>
    <title>$142 upgrade kit and spare modules turn Nvidia RTX 4090 24GB to 48GB AI card</title>
    <updated>2025-09-11T00:19:14+00:00</updated>
    <author>
      <name>/u/cornucopea</name>
      <uri>https://old.reddit.com/user/cornucopea</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The upgrade kit comprises a custom PCB designed with a clamshell configuration, facilitating the installation of twice the number of memory chips. Most components are pre-installed at the manufacturing facility, requiring the user to solder the GPU and memory chips onto the PCB. Additionally, the upgrade kit includes a blower-style cooling solution, designed for integration with workstation and server configurations that utilize multi-GPU architectures.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/usd142-upgrade-kit-and-spare-modules-turn-nvidia-rtx-4090-24gb-to-48gb-ai-card-technician-explains-how-chinese-factories-turn-gaming-flagships-into-highly-desirable-ai-gpus"&gt;https://www.tomshardware.com/pc-components/gpus/usd142-upgrade-kit-and-spare-modules-turn-nvidia-rtx-4090-24gb-to-48gb-ai-card-technician-explains-how-chinese-factories-turn-gaming-flagships-into-highly-desirable-ai-gpus&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cornucopea"&gt; /u/cornucopea &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nduo33/142_upgrade_kit_and_spare_modules_turn_nvidia_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nduo33/142_upgrade_kit_and_spare_modules_turn_nvidia_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nduo33/142_upgrade_kit_and_spare_modules_turn_nvidia_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T00:19:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndf01a</id>
    <title>So apparently half of us are "AI providers" now (EU AI Act edition)</title>
    <updated>2025-09-10T14:00:15+00:00</updated>
    <author>
      <name>/u/Thecomplianceexpert</name>
      <uri>https://old.reddit.com/user/Thecomplianceexpert</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Heads up, fellow tinkers&lt;/p&gt; &lt;p&gt;The EU AI Act‚Äôs first real deadline kicked in August 2nd so if you‚Äôre messing around with models that hit 10^23 FLOPs or more (think Llama-2 13B territory), regulators now officially care about you.&lt;/p&gt; &lt;p&gt;Couple things I‚Äôve learned digging through this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The FLOP cutoff is surprisingly low. It‚Äôs not ‚ÄúGPT-5 on a supercomputer‚Äù level, but it‚Äôs way beyond what you‚Äôd get fine-tuning Llama on your 3090.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;‚ÄúProvider‚Äù doesn‚Äôt just mean Meta, OpenAI, etc. If you fine-tune or significantly modify a big model, you need to watch out. Even if it‚Äôs just a hobby, you can still be classified as a provider.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Compliance isn‚Äôt impossible. Basically: &lt;ul&gt; &lt;li&gt;Keep decent notes (training setup, evals, data sources).&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Have some kind of ‚Äúdata summary‚Äù you can share if asked.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Don‚Äôt be sketchy about copyright.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Deadline check:&lt;br /&gt; &lt;ul&gt; &lt;li&gt;New models released after Aug 2025 - rules apply now!&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Models that existed before Aug 2025 - you‚Äôve got until 2027.&lt;br /&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;EU basically said: ‚ÄúCongrats, you‚Äôre responsible now.‚Äù ü´† &lt;/p&gt; &lt;p&gt;TL;DR: If you‚Äôre just running models locally for fun, you‚Äôre probably fine. If you‚Äôre fine-tuning big models and publishing them, you might already be considered a ‚Äúprovider‚Äù under the law.&lt;/p&gt; &lt;p&gt;Honestly, feels wild that a random tinkerer could suddenly have reporting duties, but here we are.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thecomplianceexpert"&gt; /u/Thecomplianceexpert &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndf01a/so_apparently_half_of_us_are_ai_providers_now_eu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T14:00:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1necpnm</id>
    <title>Looking for individuals who want to work on an AI project</title>
    <updated>2025-09-11T16:02:50+00:00</updated>
    <author>
      <name>/u/Strange_Test7665</name>
      <uri>https://old.reddit.com/user/Strange_Test7665</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm in to local llms (go figure) at the moment. Specifically building a system that uses memory (vector db and knowledge graphs) and multi modal inputs to be as human like as possible. Current stack I mess around with is&lt;br /&gt; - Qwen 7b (LLM)&lt;br /&gt; - Moondream (VLM)&lt;br /&gt; - Whisper (STT)/Silero(VAD)&lt;br /&gt; -FAISS (db search)&lt;br /&gt; -MiDAS (vision/depth)&lt;br /&gt; -YOLO (object detection)&lt;br /&gt; -SAM2 (object segmentation)&lt;br /&gt; -kokoro (TTS)&lt;br /&gt; -MxBai (text embed)&lt;br /&gt; -BeIT (img embed)&lt;br /&gt; -vggish (audio embed)&lt;/p&gt; &lt;p&gt;Putting this all here because I am looking for a few people interested in building something that basically brings everything together to have a pretty bad ass AI at home. I realize there are frameworks and other tools that already exist that can be used to do this. So I am looking for a group that is also about the fun of just doing it.&lt;/p&gt; &lt;p&gt;I was thinking something along the lines of following a brain architecture one person working on audio another on video for example. agree on some 'central nervous' system concept for all of the parts to communicate and then spin up a git repo, do some video meetings once a week and have a go.&lt;/p&gt; &lt;p&gt;obv open to other ideas, that's why I am making this post - but the project goal is what's stated above.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Strange_Test7665"&gt; /u/Strange_Test7665 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1necpnm/looking_for_individuals_who_want_to_work_on_an_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1necpnm/looking_for_individuals_who_want_to_work_on_an_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1necpnm/looking_for_individuals_who_want_to_work_on_an_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T16:02:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne41ss</id>
    <title>I made a semantic code splitting library for implementing RAG (Retrieval-Augmented Generation) on codebases.</title>
    <updated>2025-09-11T09:15:23+00:00</updated>
    <author>
      <name>/u/HolidayInevitable500</name>
      <uri>https://old.reddit.com/user/HolidayInevitable500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I made &lt;strong&gt;code-chopper&lt;/strong&gt;, a new open-source TypeScript library for anyone who works with code and LLMs.&lt;/p&gt; &lt;h1&gt;What It Does&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;code-chopper&lt;/strong&gt; uses &lt;strong&gt;tree-sitter&lt;/strong&gt; to parse code and split it into meaningful, semantic chunks like functions, classes, and variable declarations. This is perfect for RAG, or simply for giving an LLM a high-level overview of a project without using up a ton of tokens.&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Customizable Filtering&lt;/strong&gt;: Use a &lt;code&gt;filter&lt;/code&gt; function to control exactly what gets extracted.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ready for Use&lt;/strong&gt;: I've included helper functions for navigating files and directories.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Practical Examples&lt;/strong&gt;: Check out the examples repo for use cases like: &lt;ul&gt; &lt;li&gt;&lt;code&gt;repo_summary&lt;/code&gt;: Generate a &lt;code&gt;Aider's repomap&lt;/code&gt;-style overview of your codebase.&lt;/li&gt; &lt;li&gt;&lt;code&gt;entity_rank&lt;/code&gt;: Use &lt;strong&gt;Katz centrality&lt;/strong&gt; to find the most important functions or variables.&lt;/li&gt; &lt;li&gt;&lt;code&gt;doc_generator&lt;/code&gt;: Automatically write documentation for your code.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I made this because I needed a better way to chunk code for my own projects, and I hope it's helpful for you too.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub&lt;/strong&gt;: &lt;a href="https://github.com/sirasagi62/code-chopper"&gt;https://github.com/sirasagi62/code-chopper&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Examples&lt;/strong&gt;: &lt;a href="https://github.com/sirasagi62/code-chopper-examples/"&gt;https://github.com/sirasagi62/code-chopper-examples/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NPM&lt;/strong&gt;: &lt;a href="https://www.npmjs.com/package/code-chopper"&gt;https://www.npmjs.com/package/code-chopper&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HolidayInevitable500"&gt; /u/HolidayInevitable500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne41ss/i_made_a_semantic_code_splitting_library_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne41ss/i_made_a_semantic_code_splitting_library_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne41ss/i_made_a_semantic_code_splitting_library_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T09:15:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne9zn5</id>
    <title>In need of real life community in the space</title>
    <updated>2025-09-11T14:16:51+00:00</updated>
    <author>
      <name>/u/SolidRemote8316</name>
      <uri>https://old.reddit.com/user/SolidRemote8316</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I went down the AI rabbit hole not too long ago and I must say it‚Äôs been quite exciting and challenging. I don‚Äôt have programming experience, so a lot of things I have explored have been more from a vibe coding standpoint, and I know some of my previous posts have received some pokes due to that. &lt;/p&gt; &lt;p&gt;Everyone brings a different lens and I‚Äôm not trying to reduce my inability to code. However, my biggest challenge is that in my circle of friends, I‚Äôm the most ‚Äúadvanced‚Äù and it sucks cos I know I don‚Äôt know a lot. I am using this post as a smoke signal to search for a mentor, peer or community that can help in this quest for knowledge and further understanding of this space. This sub is helpful, but it‚Äôs not the same as bouncing thoughts, ideas and all in real time. &lt;/p&gt; &lt;p&gt;When I started out, I bought the domain - &lt;a href="https://www.mindmeetsmodel.com"&gt;https://www.mindmeetsmodel.com&lt;/a&gt; with the goal of documenting my journey and being able to look back and point at what I was able to accomplish. The site was vibe coded by the way. &lt;/p&gt; &lt;p&gt;I hope someone who is willing to help a stranger stumbled on this post. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SolidRemote8316"&gt; /u/SolidRemote8316 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne9zn5/in_need_of_real_life_community_in_the_space/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne9zn5/in_need_of_real_life_community_in_the_space/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne9zn5/in_need_of_real_life_community_in_the_space/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T14:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne0o5m</id>
    <title>top reads from last week</title>
    <updated>2025-09-11T05:35:19+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne0o5m/top_reads_from_last_week/"&gt; &lt;img alt="top reads from last week" src="https://preview.redd.it/kio1vvck2hof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=922bbaa42c50e0e800de692fba6af534988fd0b6" title="top reads from last week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kio1vvck2hof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne0o5m/top_reads_from_last_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne0o5m/top_reads_from_last_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T05:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndoxxa</id>
    <title>Why should I **not** buy an AMD AI Max+ 395 128GB right away ?</title>
    <updated>2025-09-10T20:10:55+00:00</updated>
    <author>
      <name>/u/StyMaar</name>
      <uri>https://old.reddit.com/user/StyMaar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the rise of medium-sized MoE (gpt-oss-120B, GLM-4.5-air, and now the incoming Qwen3-80B-A3B) and their excellent performance for local models (well at least for the two first), the relatively low compute and memory bandwidth of the Strix Halo doesn't sounds too much of a problem anymore (because of the low active parameters count) and the 128GB of VRAM for $2k is unbeatable.&lt;/p&gt; &lt;p&gt;So now I'm very tempted to buy one, but I'm also aware that I don't really &lt;em&gt;need&lt;/em&gt; one, so please give me arguments about why I should not buy it.&lt;/p&gt; &lt;p&gt;My wallet thanks you in advance.&lt;/p&gt; &lt;p&gt;Edit: thanks for your response. Unfortunately no one was really able to convinced me out of this purchase.&lt;/p&gt; &lt;p&gt;Now only my procrastination can save me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StyMaar"&gt; /u/StyMaar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndoxxa/why_should_i_not_buy_an_amd_ai_max_395_128gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T20:10:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndxsja</id>
    <title>GPT-OSS 20b (high) consistently does FAR better than gpt5-thinking on my engineering Hw</title>
    <updated>2025-09-11T02:54:33+00:00</updated>
    <author>
      <name>/u/InevitableWay6104</name>
      <uri>https://old.reddit.com/user/InevitableWay6104</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just found this super interesting, but gpt-oss 20b gets almost every problem right, while gpt5-thinking, something I can only query like 5 times before getting rate limited (free tier), only gets it right about 50% of the time.&lt;/p&gt; &lt;p&gt;pretty interesting that a open weights 20b model is better than the closed flagship model on the free tier. I often use these models to verify my work, and both are free, but I can spam the 20b as much as I want and it's right more often.&lt;/p&gt; &lt;p&gt;granted, gpt5-thinking on the free tier is probably on the lowest setting, bc gpt-oss thinks ALOT longer than gpt5 did, on average it was about 20-30k tokens per question. &lt;/p&gt; &lt;p&gt;qwen3-30b-2507-thinking is also really good, but I don't think it's as good for this specific task, and gpt-oss is way smaller.&lt;/p&gt; &lt;p&gt;just still found it super interesting and wanted to share.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InevitableWay6104"&gt; /u/InevitableWay6104 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndxsja/gptoss_20b_high_consistently_does_far_better_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T02:54:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndz1k4</id>
    <title>PNY preorder listing shows Nvidia DGX Spark at $4,299.99</title>
    <updated>2025-09-11T04:00:41+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;PNY has opened preorders for the Nvidia DGX Spark, a compact desktop AI system powered by the Grace Blackwell GB10 Superchip. It combines Arm Cortex-X925 and Cortex-A725 CPU cores with a Blackwell GPU, delivering up to 1,000 AI TOPS, or 1 petaFLOP of FP4 performance, for local model inference and fine-tuning.&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linuxgizmos.com/pny-preorder-listing-shows-nvidia-dgx-spark-at-4299-99/"&gt;https://linuxgizmos.com/pny-preorder-listing-shows-nvidia-dgx-spark-at-4299-99/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndz1k4/pny_preorder_listing_shows_nvidia_dgx_spark_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T04:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne7f0c</id>
    <title>Qwen Code CLI affected by the debug-js compromise</title>
    <updated>2025-09-11T12:25:10+00:00</updated>
    <author>
      <name>/u/mestar12345</name>
      <uri>https://old.reddit.com/user/mestar12345</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On 2025-09-08 the maintainer of some popular JS libraries was compromised, and new versions of some popular libraries were released with some crypto stealing code. qwen code cli was one of the programs that was updated since then, and windows defender will detect Malgent!MSR trojan in some JS libraries when you start qwen.&lt;/p&gt; &lt;p&gt;The payload was for the browser environment of javascript, and I don't know if there is any impact if you run the compromised code in the node.js context. Still, I hope this gets cleaned up soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mestar12345"&gt; /u/mestar12345 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7f0c/qwen_code_cli_affected_by_the_debugjs_compromise/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7f0c/qwen_code_cli_affected_by_the_debugjs_compromise/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7f0c/qwen_code_cli_affected_by_the_debugjs_compromise/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T12:25:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne4xok</id>
    <title>What would be the most budget-friendly PC to run LLMs larger than 72B?</title>
    <updated>2025-09-11T10:12:32+00:00</updated>
    <author>
      <name>/u/pitchblackfriday</name>
      <uri>https://old.reddit.com/user/pitchblackfriday</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking, if a 5-year-old gaming laptop can run Qwen 3 30B A3B at a slow but functional speed, what about bigger MoE models?&lt;/p&gt; &lt;p&gt;Let's add some realistic expectations.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Serving 1~5 users only, without much concurrency.&lt;/li&gt; &lt;li&gt;Speed matters less, as long as it's &amp;quot;usable at least&amp;quot;. Parameter size and knowledge matter more.&lt;/li&gt; &lt;li&gt;Running MoE-based models only, like the upcoming Qwen 3 Next 80B A3B, to improve inference speed.&lt;/li&gt; &lt;li&gt;(optional) Utilizing APU and unified memory architecture for accommodating sufficient GPU offloading, and keeping the cost lower&lt;/li&gt; &lt;li&gt;Reasonable power consumption and supply for lower electricity bill.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What would be the lowest-cost and yet usable desktop build for running such LLMs locally? I'm just wondering about ideas and opinions for ordinary users, outside those first-world, upper-class, multi-thousand-dollars realm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pitchblackfriday"&gt; /u/pitchblackfriday &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4xok/what_would_be_the_most_budgetfriendly_pc_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4xok/what_would_be_the_most_budgetfriendly_pc_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4xok/what_would_be_the_most_budgetfriendly_pc_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T10:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1neb35p</id>
    <title>New VS Code release allows extensions to contribute language models to Chat</title>
    <updated>2025-09-11T14:58:58+00:00</updated>
    <author>
      <name>/u/isidor_n</name>
      <uri>https://old.reddit.com/user/isidor_n</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neb35p/new_vs_code_release_allows_extensions_to/"&gt; &lt;img alt="New VS Code release allows extensions to contribute language models to Chat" src="https://external-preview.redd.it/ub1r8snDnE0gcJQ1X4KunDUU8G23q7XAyTmHzNrnIvQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b8124d3dd9bed2000f7513f33ea18cc6440b75db" title="New VS Code release allows extensions to contribute language models to Chat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Extensions can now contribute language models that are used in the Chat view. This is the first step (we have a bunch more work to do). But if you have any feedback let me know (vscode pm here).&lt;/p&gt; &lt;p&gt;Docs &lt;a href="https://code.visualstudio.com/api/extension-guides/ai/language-model-chat-provider"&gt;https://code.visualstudio.com/api/extension-guides/ai/language-model-chat-provider&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/isidor_n"&gt; /u/isidor_n &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://code.visualstudio.com/updates/v1_104"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neb35p/new_vs_code_release_allows_extensions_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neb35p/new_vs_code_release_allows_extensions_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T14:58:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne58kw</id>
    <title>Thinking Machines Lab dropped a new research: Defeating Nondeterminism in LLM Inference</title>
    <updated>2025-09-11T10:30:09+00:00</updated>
    <author>
      <name>/u/Snoo_64233</name>
      <uri>https://old.reddit.com/user/Snoo_64233</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR; LLM inference nondeterminism isn't just floating-point non-associativity or GPU concurrent execution, the core culprit is batching variance, where server load unpredictably alters numeric. Batch-invariant kernels unlock true reproducibility. Non-determinism is an issue in all sort of places, but non-determinism stemming from GPU kernels not being batch size invariant is pretty specific to machine learning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snoo_64233"&gt; /u/Snoo_64233 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne58kw/thinking_machines_lab_dropped_a_new_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne58kw/thinking_machines_lab_dropped_a_new_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T10:30:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne3v7m</id>
    <title>Celebrating 1 year anniversary of the revolutionary game changing LLM that was Reflection 70b</title>
    <updated>2025-09-11T09:03:14+00:00</updated>
    <author>
      <name>/u/LosEagle</name>
      <uri>https://old.reddit.com/user/LosEagle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is now a year since the release of Reflection-70B that genius inventor Matt Shumer marketted as state-of-the-art hallucination-free llm that outperforms both gpt-4o and claude 3.5 with its new way of thinking as well as world's top open-source model. &lt;/p&gt; &lt;p&gt;World hasn't been the same since then indeed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LosEagle"&gt; /u/LosEagle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne3v7m/celebrating_1_year_anniversary_of_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne3v7m/celebrating_1_year_anniversary_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne3v7m/celebrating_1_year_anniversary_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T09:03:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne4h62</id>
    <title>Qwen3-Next is coming soon</title>
    <updated>2025-09-11T09:43:38+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4h62/qwen3next_is_coming_soon/"&gt; &lt;img alt="Qwen3-Next is coming soon" src="https://preview.redd.it/1mdp7l72biof1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cd5da8e1e448326bed9df91f518aadffe866432" title="Qwen3-Next is coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1mdp7l72biof1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4h62/qwen3next_is_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne4h62/qwen3next_is_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T09:43:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1neba8b</id>
    <title>Qwen</title>
    <updated>2025-09-11T15:06:37+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"&gt; &lt;img alt="Qwen" src="https://preview.redd.it/p5fbgn0owjof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94561db32b1fca11c0250280863739d22d76e841" title="Qwen" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p5fbgn0owjof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1neba8b/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T15:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ne7y69</id>
    <title>Qwen3-Next-80B-A3B-Thinking soon</title>
    <updated>2025-09-11T12:49:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7y69/qwen3next80ba3bthinking_soon/"&gt; &lt;img alt="Qwen3-Next-80B-A3B-Thinking soon" src="https://preview.redd.it/bo8hhc558jof1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=547def56180e3c7f03468272c1979619111e065e" title="Qwen3-Next-80B-A3B-Thinking soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bo8hhc558jof1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7y69/qwen3next80ba3bthinking_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ne7y69/qwen3next80ba3bthinking_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-11T12:49:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ndjxdt</id>
    <title>AMA with the Unsloth team</title>
    <updated>2025-09-10T17:02:18+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;, I'm Daniel from &lt;a href="https://docs.unsloth.ai/"&gt;Unsloth&lt;/a&gt;! You might know us from our RL &amp;amp; fine-tuning open-source framework, our GGUFs, kernels or bug fixes. We‚Äôre super excited to answer all your questions!! ü¶• Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To celebrate the AMA, we‚Äôre releasing Aider Polyglot benchmarks comparing our DeepSeek-V3.1 Dynamic GGUFs to other models and quants. We also made a Localllama post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ndibn1/unsloth_dynamic_ggufs_aider_polyglot_benchmarks/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our participants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Daniel, u/danielhanchen&lt;/li&gt; &lt;li&gt;Michael, u/yoracale&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 10AM ‚Äì 1PM PST, with the Unsloth team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Thanks so much!ü•∞&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ndjxdt/ama_with_the_unsloth_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T17:02:18+00:00</published>
  </entry>
</feed>
