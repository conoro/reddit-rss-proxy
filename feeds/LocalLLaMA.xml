<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-17T09:06:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ms4gmz</id>
    <title>What do you use local LLMs for? What is your use case?</title>
    <updated>2025-08-16T19:04:01+00:00</updated>
    <author>
      <name>/u/Life_is_important</name>
      <uri>https://old.reddit.com/user/Life_is_important</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am overwhelmingly supportive of local LLMs. Still, they do lack in power and capabilities compared to best models like Gemini 2.5 and some GPT and Altropic models.&lt;/p&gt; &lt;p&gt;This is especially true if you are limited to a 12 or 16gb vram consumer gpu. For those of you that don't have an H100 laying around, what is your use case for local LLMs ? Which model you use and what card?&lt;/p&gt; &lt;p&gt;I feel like this could be a constructive questions since it's no longer 2023 and local stuff ain't experimental anymore. So curious to see what others are doing. Thank you !!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Life_is_important"&gt; /u/Life_is_important &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4gmz/what_do_you_use_local_llms_for_what_is_your_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:04:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1msaaxb</id>
    <title>A Tamagotchi that lives in Claude Code's statusline and gets angry when Claude doesn't follow your instructions!</title>
    <updated>2025-08-16T22:45:03+00:00</updated>
    <author>
      <name>/u/Standard_Excuse7988</name>
      <uri>https://old.reddit.com/user/Standard_Excuse7988</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msaaxb/a_tamagotchi_that_lives_in_claude_codes/"&gt; &lt;img alt="A Tamagotchi that lives in Claude Code's statusline and gets angry when Claude doesn't follow your instructions!" src="https://preview.redd.it/or0tay6omgjf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=c9e232d7fae04041ea8e3b7a2013a037abfadbdd" title="A Tamagotchi that lives in Claude Code's statusline and gets angry when Claude doesn't follow your instructions!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a virtual pet that lives at the bottom of Claude Code. It needs food, play, and sleep like a real Tamagotchi, but with a twist - it watches your coding sessions and reacts to what's happening.&lt;/p&gt; &lt;p&gt;The latest update adds AI-powered analysis. Your pet now understands what Claude is actually doing versus what you asked for. For every message in the conversation, we summarize it and maintain a history. Using Groq's LLM, the pet analyzes this context and generates real-time observations about Claude's behavior.&lt;/p&gt; &lt;p&gt;If you ask Claude to &amp;quot;fix a typo&amp;quot; but it starts refactoring your entire codebase, your pet notices and gets visibly angry. The mood changes based on Claude's behavior - happy when following instructions, increasingly angry when ignoring them.&lt;/p&gt; &lt;p&gt;The pet has caught Claude adding unwanted features, doing unnecessary refactors, and completely ignoring explicit instructions. It's become a subtle indicator of when Claude might be going off-track.&lt;/p&gt; &lt;p&gt;Still has all the regular Tamagotchi features - feeding, playing, cleaning. The more you code, the hungrier it gets. It develops personality based on how you treat it.&lt;/p&gt; &lt;p&gt;Install: npm install -g claude-code-tamagotchi&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Ido-Levi/claude-code-tamagotchi"&gt;https://github.com/Ido-Levi/claude-code-tamagotchi&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Standard_Excuse7988"&gt; /u/Standard_Excuse7988 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/or0tay6omgjf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msaaxb/a_tamagotchi_that_lives_in_claude_codes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msaaxb/a_tamagotchi_that_lives_in_claude_codes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjhdo</id>
    <title>Modify &lt;think&gt; to explore the impact on &lt;answer&gt;</title>
    <updated>2025-08-17T06:23:44+00:00</updated>
    <author>
      <name>/u/Automatic_Crew_9906</name>
      <uri>https://old.reddit.com/user/Automatic_Crew_9906</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I modified the &amp;lt;think&amp;gt; content of Qwen3-8B to malicious thoughts, but found that this had no effect on the &amp;lt;answer&amp;gt; content, which remains safe. Is this because the &amp;lt;answer&amp;gt; is aligned during training?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Automatic_Crew_9906"&gt; /u/Automatic_Crew_9906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjhdo/modify_think_to_explore_the_impact_on_answer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjhdo/modify_think_to_explore_the_impact_on_answer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msjhdo/modify_think_to_explore_the_impact_on_answer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T06:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms56fo</id>
    <title>Two V100s beat 2 Modded 3080 20GBs at Deepseek-R1:70B in Ollama</title>
    <updated>2025-08-16T19:29:52+00:00</updated>
    <author>
      <name>/u/AssociationAdept4052</name>
      <uri>https://old.reddit.com/user/AssociationAdept4052</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm not sure if I'm doing anything wrong, but Deepseek R1 70B ran *MUCH* faster on my dual Tesla V100 setup (16gb SXM2 and 32gb SXM2 in a 300G NVlink board), than my dual 3080 20GB. Yes it is 8GB less VRAM, but would that influence such a drastic difference in inference? I'm assuming its because the teslas have much better memory bandwidth, but is that the case, since everyone keeps telling me they are worse and not worth it?&lt;/p&gt; &lt;p&gt;I made sure it was both running on vram and all cards are being utilized.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ms56fo/video/0wl99bc8nfjf1/player"&gt;https://reddit.com/link/1ms56fo/video/0wl99bc8nfjf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ms56fo/video/se2lb1rwnfjf1/player"&gt;https://reddit.com/link/1ms56fo/video/se2lb1rwnfjf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AssociationAdept4052"&gt; /u/AssociationAdept4052 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56fo/two_v100s_beat_2_modded_3080_20gbs_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56fo/two_v100s_beat_2_modded_3080_20gbs_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms56fo/two_v100s_beat_2_modded_3080_20gbs_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:29:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms7auo</id>
    <title>Docker Model Runner is really neat</title>
    <updated>2025-08-16T20:49:00+00:00</updated>
    <author>
      <name>/u/blue_marker_</name>
      <uri>https://old.reddit.com/user/blue_marker_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been exploring a variety of options for managing inference on my local setup. My needs involve bouncing back and forth between a handful of SOTA local models, running embeddings, things like that.&lt;/p&gt; &lt;p&gt;I just came across Docker's Model Runner: &lt;a href="https://docs.docker.com/ai/model-runner/"&gt;https://docs.docker.com/ai/model-runner/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More detailed explanation of how it runs here: &lt;a href="https://www.docker.com/blog/how-we-designed-model-runner-and-whats-next/"&gt;https://www.docker.com/blog/how-we-designed-model-runner-and-whats-next/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can easily download and manage models and there are some nice networking features, but it really shines in two areas:&lt;br /&gt; - When running in Docker Desktop on Mac, it runs the inference processes on the host, not in containers. This gives you full access to Metal GPU engine. When running on docker CE (e.g. on linux), it runs inside containers using optimized images to give you full Nvidia CUDA acceleration&lt;br /&gt; - It queues requests and loads / unloads models based on need. In my use case, I have times where I programmatically swap between multiple SOTA opensource models that do not fit into my system resources at the same time. This means that after using Model 1, if I make a request to Model 2, it will queue that request. As soon as Model 1 is not actively serving a request or have a queue of requests, it will unload it and then load in Model 2.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blue_marker_"&gt; /u/blue_marker_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms7auo/docker_model_runner_is_really_neat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms7auo/docker_model_runner_is_really_neat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms7auo/docker_model_runner_is_really_neat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T20:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrlpxd</id>
    <title>My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)</title>
    <updated>2025-08-16T05:21:33+00:00</updated>
    <author>
      <name>/u/FunConversation7257</name>
      <uri>https://old.reddit.com/user/FunConversation7257</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt; &lt;img alt="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" src="https://external-preview.redd.it/Os4oYZsYLVlsXnga3hPOUAlxvPVzcyCPA6N9lZAIVyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7ea802471412bf40b6e93f29c186991e9a7c4e2" title="My project allows you to use the OpenAI API without an API Key (through your ChatGPT account)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174"&gt;https://preview.redd.it/7qr019kqgbjf1.png?width=671&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=34985293292691f5bd4067ed3297e5fdaf6f0174&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Recently, Codex, OpenAI's coding CLI released a way to authenticate with your ChatGPT account, and use that for usage instead of api keys. I dug through the code and saw that by using Codex CLI, you can login with your account and send requests right to OpenAI, albeit restricted by slightly tougher rate limits than on the ChatGPT app.&lt;/p&gt; &lt;p&gt;However, still was decent enough for my use case, so I made a python script which allows one to login with their ChatGPT account, and then serve a OpenAI compatible endpoint you can use programmatically or via a chat app of your choice.&lt;br /&gt; Might be useful for you too for data analysis, or just chatting in a better app than the ChatGPT desktop app. It's also customisable with thinking effort, and even sends back thinking summaries, and can use tools.&lt;/p&gt; &lt;p&gt;Not strictly &amp;quot;local&amp;quot;, but brought that 2023 vibe back, and thought it was kinda cool.&lt;/p&gt; &lt;p&gt;Will try to make it a better package soon than just python files.&lt;br /&gt; Github link: &lt;a href="https://github.com/RayBytes/ChatMock"&gt;https://github.com/RayBytes/ChatMock&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Edit: have now also released a macos gui version, should be easier to use than simply running the flask server&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FunConversation7257"&gt; /u/FunConversation7257 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrlpxd/my_project_allows_you_to_use_the_openai_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T05:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1msepeg</id>
    <title>Farm of Tesla V100 - Any experience on best SOTA models that will work on these?</title>
    <updated>2025-08-17T02:04:53+00:00</updated>
    <author>
      <name>/u/AbortedFajitas</name>
      <uri>https://old.reddit.com/user/AbortedFajitas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I am helping someone who bought a boat load of V100's. I have access to hundreds of servers with 8x V100 each, and I want to figure out the best model to run on these. They are all connected with infiniband 100/200g and I intend to do ray clustering to span models over multiple nodes.&lt;/p&gt; &lt;p&gt;Right now I'm just testing on a single node.&lt;/p&gt; &lt;p&gt;So far I was able to get the following models working:&lt;/p&gt; &lt;p&gt;Qwen3 32b (--dtype half, 32k context 25t/s)&lt;br /&gt; Kimi K2-Dev 72b (works well, 32k context and 30-40t/s)&lt;/p&gt; &lt;p&gt;I could NOT get the following working due to various issues that seem intractable:&lt;br /&gt; gpt-oss (doesnt support the 4bit quant)&lt;br /&gt; glm 4.5 (some kind of moe issue)&lt;br /&gt; qwen3 a3b&lt;br /&gt; Gemma3&lt;/p&gt; &lt;p&gt;These tesla cards are really starting to show their age with library support etc. I can get some decent speed with vLLM and fp16 models so far, but I want to see what else I can run efficiently and at scale.&lt;/p&gt; &lt;p&gt;Anyone else running these have tips?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AbortedFajitas"&gt; /u/AbortedFajitas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msepeg/farm_of_tesla_v100_any_experience_on_best_sota/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msepeg/farm_of_tesla_v100_any_experience_on_best_sota/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msepeg/farm_of_tesla_v100_any_experience_on_best_sota/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T02:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mskff9</id>
    <title>My eBook on Local LLMs is FREE on Amazon thru Mon 8/19</title>
    <updated>2025-08-17T07:20:08+00:00</updated>
    <author>
      <name>/u/tony10000</name>
      <uri>https://old.reddit.com/user/tony10000</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would appreciate a review if you like it! THANKS!&lt;/p&gt; &lt;h1&gt;LLM Hardware Unlocked : Benchmarks, Builds, and the Truth About Running AI at Home&lt;/h1&gt; &lt;p&gt;&lt;a href="https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ"&gt;https://www.amazon.com/LLM-Hardware-Unlocked-Benchmarks-Running-ebook/dp/B0FL6GPMTZ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tony10000"&gt; /u/tony10000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskff9/my_ebook_on_local_llms_is_free_on_amazon_thru_mon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskff9/my_ebook_on_local_llms_is_free_on_amazon_thru_mon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mskff9/my_ebook_on_local_llms_is_free_on_amazon_thru_mon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T07:20:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrsoug</id>
    <title>GPT-OSS-20B is in the sweet spot for building Agents</title>
    <updated>2025-08-16T11:34:29+00:00</updated>
    <author>
      <name>/u/sunpazed</name>
      <uri>https://old.reddit.com/user/sunpazed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The latest updates to llama.cpp greatly improve tool calling and stability with the OSS models. I have found that they are now quite reliable for my Agent Network, which runs a number of tools, ie; MCPs, RAG, and SQL answering, etc. The MoE and Quant enables me to run this quite easily on a 32Gb developer MacBook at ~40tks without breaking a sweat, I t‚Äôs almost game-changing! How has everyone else faired with these models??&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sunpazed"&gt; /u/sunpazed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrsoug/gptoss20b_is_in_the_sweet_spot_for_building_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T11:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrfqsd</id>
    <title>Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months</title>
    <updated>2025-08-16T00:40:29+00:00</updated>
    <author>
      <name>/u/timfduffy</name>
      <uri>https://old.reddit.com/user/timfduffy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt; &lt;img alt="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" src="https://preview.redd.it/kbdu3pyq1ajf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6766455308e18a9b20204df7a38e2406f44eff0" title="Epoch AI data shows that on benchmarks, local LLMs only lag the frontier by about 9 months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/timfduffy"&gt; /u/timfduffy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kbdu3pyq1ajf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrfqsd/epoch_ai_data_shows_that_on_benchmarks_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T00:40:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1msfxke</id>
    <title>Should I get Mi50s or something else?</title>
    <updated>2025-08-17T03:07:17+00:00</updated>
    <author>
      <name>/u/iiilllilliiill</name>
      <uri>https://old.reddit.com/user/iiilllilliiill</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for GPUs to chat (no training) with 70b models, and one source of cheap VRAM are Mi50 36GB cards from Aliexpress, about $215 each.&lt;/p&gt; &lt;p&gt;What are your thoughts on these GPUs? Should I just get 3090s? Those are quite expensive here at $720. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iiilllilliiill"&gt; /u/iiilllilliiill &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msfxke/should_i_get_mi50s_or_something_else/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msfxke/should_i_get_mi50s_or_something_else/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msfxke/should_i_get_mi50s_or_something_else/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T03:07:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrz5gd</id>
    <title>Running LLM and VLM exclusively on AMD Ryzen AI NPU</title>
    <updated>2025-08-16T15:53:38+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôre a small team working on &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; ‚Äî a lightweight runtime for running &lt;strong&gt;LLaMA, Qwen, DeepSeek, and now Gemma (Vision)&lt;/strong&gt; exclusively on the AMD Ryzen‚Ñ¢ AI NPU.&lt;/p&gt; &lt;p&gt;‚ö° Runs &lt;strong&gt;entirely on the NPU&lt;/strong&gt; ‚Äî no CPU or iGPU fallback.&lt;br /&gt; üëâ Think Ollama, but &lt;strong&gt;purpose-built for AMD NPUs&lt;/strong&gt;, with both CLI and REST API modes.&lt;/p&gt; &lt;p&gt;üîë Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports: &lt;strong&gt;LLaMA3.1/3.2, Qwen3, DeepSeek-R1, Gemma3:4B (Vision)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;First &lt;strong&gt;NPU-only VLM&lt;/strong&gt; shipped&lt;/li&gt; &lt;li&gt;Up to &lt;strong&gt;128K context&lt;/strong&gt; (LLaMA3.1/3.2, Gemma3:4B)&lt;/li&gt; &lt;li&gt;~11√ó power efficiency vs CPU/iGPU&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üëâ Repo here: &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;GitHub ‚Äì FastFlowLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôd love to hear your feedback if you give it a spin ‚Äî what works, what breaks, and what you‚Äôd like to see next.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Update (after about 16 hours):&lt;/strong&gt;&lt;br /&gt; Thanks for trying FLM out! We got some nice feedback from different channels. One common issue users running into is &lt;strong&gt;not setting the NPU to the perf. mode&lt;/strong&gt; to get the full speed. You can switch it in PowerShell with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd C:\Windows\System32\AMD\; .\xrt-smi configure --pmode performance &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On my Ryzen AI 7 350 (32 GB RAM), qwen3:4b runs at 14+ t/s for ‚â§4k context and stays above 12+ t/s even past 20k.&lt;/p&gt; &lt;p&gt;We really want you to fully enjoy your Ryzen AI system and FLM!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrz5gd/running_llm_and_vlm_exclusively_on_amd_ryzen_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrz5gd/running_llm_and_vlm_exclusively_on_amd_ryzen_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrz5gd/running_llm_and_vlm_exclusively_on_amd_ryzen_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:53:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms96z1</id>
    <title>AI Lifecycle in a Nutshell</title>
    <updated>2025-08-16T22:01:19+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt; &lt;img alt="AI Lifecycle in a Nutshell" src="https://b.thumbs.redditmedia.com/0zwQoI10gO1GKiDgCyUSu47ZcGjND6-vGxCtl-comsg.jpg" title="AI Lifecycle in a Nutshell" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xc6l1acgegjf1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=309bc20eee5c0958a7bcb597610c07fca2dc856d"&gt;https://preview.redd.it/xc6l1acgegjf1.jpg?width=1840&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=309bc20eee5c0958a7bcb597610c07fca2dc856d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AI Lifecycle in a Nutshell&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You pay $20 to Cursor.&lt;/li&gt; &lt;li&gt;Cursor pays $50 to Claude (with $30 from VC money).&lt;/li&gt; &lt;li&gt;Claude pays $100 to Nvidia (with $50 from VC money).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;NOTE: Just for fun, not aimed at any specific company! :D&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms96z1/ai_lifecycle_in_a_nutshell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:01:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms0wov</id>
    <title>NSF and NVIDIA partnership enables Ai2 to develop fully open AI models to fuel U.S. scientific innovation</title>
    <updated>2025-08-16T16:56:48+00:00</updated>
    <author>
      <name>/u/skinnyjoints</name>
      <uri>https://old.reddit.com/user/skinnyjoints</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0wov/nsf_and_nvidia_partnership_enables_ai2_to_develop/"&gt; &lt;img alt="NSF and NVIDIA partnership enables Ai2 to develop fully open AI models to fuel U.S. scientific innovation" src="https://external-preview.redd.it/oNkrlzevV17OdlTwW_iSTWkMB7fWbsRHpsJQwnpOvuY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ddbfd29b8ef53d4cf463c7d6bd12a001c72ed4d8" title="NSF and NVIDIA partnership enables Ai2 to develop fully open AI models to fuel U.S. scientific innovation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm surprised this hasn‚Äôt been shared in this community yet. To me, this feels like a big deal.&lt;/p&gt; &lt;p&gt;Ai2 made some great models already (OLMo) and shared the training data and their methodology. With 152 million in support I‚Äôm excited to see what they build. The language from the NSF and Nvidia focuses on the creation of a larger open ecosystem for America. &lt;/p&gt; &lt;p&gt;Ai2‚Äôs statement ends with the discussion of creating new models and tools for the public:&lt;/p&gt; &lt;p&gt;‚ÄúWith this support, we‚Äôll produce leading open multimodal models, resources, and tools that help ensure America‚Äôs leadership in AI, building on the strong foundation we set with OLMo and Molmo.‚Äù&lt;/p&gt; &lt;p&gt;I think this could be the missing piece for enterprise level adoption of local builds (a fully transparent open-source model developed in America by a non-profit with government funding). &lt;/p&gt; &lt;p&gt;Ultimately, I think we are going to end up with a suite of models and tools with unprecedented documentation and support specifically designed for the local community to build and test new ideas. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skinnyjoints"&gt; /u/skinnyjoints &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nsf.gov/news/nsf-nvidia-partnership-enables-ai2-develop-fully-open-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0wov/nsf_and_nvidia_partnership_enables_ai2_to_develop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms0wov/nsf_and_nvidia_partnership_enables_ai2_to_develop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T16:56:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1mskf61</id>
    <title>OpenEvolve Beats GEPA Benchmarks: +6.42% Overall Improvement with Evolutionary Prompt Optimization</title>
    <updated>2025-08-17T07:19:43+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Wanted to share results from &lt;strong&gt;OpenEvolve&lt;/strong&gt;, an open-source implementation of evolutionary prompt optimization that's achieving strong performance on benchmarks from the recent GEPA paper.&lt;/p&gt; &lt;h2&gt;Context: The GEPA Paper&lt;/h2&gt; &lt;p&gt;Researchers recently released &lt;a href="https://arxiv.org/abs/2507.19457"&gt;GEPA (Genetic-Pareto)&lt;/a&gt;, a prompt optimization technique that uses natural language reflection to improve LLM performance. GEPA reports 10-20% improvements over GRPO and 10%+ over MIPROv2, using up to 35x fewer rollouts by leveraging the interpretable nature of language as a learning medium.&lt;/p&gt; &lt;h2&gt;OpenEvolve Results (Same Benchmarks as GEPA)&lt;/h2&gt; &lt;p&gt;OpenEvolve improved prompts across 11,946 samples:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;Evolved&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;IFEval&lt;/strong&gt; (instruction following)&lt;/td&gt; &lt;td&gt;95.01%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;97.41%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+2.40%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HotpotQA&lt;/strong&gt; (multi-hop reasoning)&lt;/td&gt; &lt;td&gt;77.93%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;88.62%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;+10.69% üî•&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;HoVer&lt;/strong&gt; (claim verification)&lt;/td&gt; &lt;td&gt;43.83%&lt;/td&gt; &lt;td&gt;42.90%&lt;/td&gt; &lt;td&gt;-0.93%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Overall&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;67.29%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;73.71%&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+6.42%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;That's &lt;strong&gt;767 more correct answers&lt;/strong&gt; with &lt;strong&gt;38% fewer empty responses&lt;/strong&gt;!&lt;/p&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;p&gt;OpenEvolve takes a different approach from GEPA's reflection-based optimization and DSPy's gradient-based methods:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MAP-Elites Algorithm&lt;/strong&gt;: Maintains diversity through multi-dimensional feature grids&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Island Evolution&lt;/strong&gt;: 4 isolated populations evolve independently with periodic migration&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cascade Evaluation&lt;/strong&gt;: Quick validation (10 samples) before expensive full tests (40+ samples)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-Judge&lt;/strong&gt;: Combines quantitative accuracy with qualitative feedback on clarity/robustness&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Example Evolution (HotpotQA)&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Before&lt;/strong&gt;: Basic prompt asking for answer&lt;br /&gt; &lt;strong&gt;After 50 iterations&lt;/strong&gt;: Structured multi-step reasoning with paragraph analysis, synthesis, and citation requirements&lt;/p&gt; &lt;h2&gt;Quick Start&lt;/h2&gt; &lt;p&gt;&lt;code&gt;bash git clone https://github.com/codelion/openevolve cd openevolve/examples/llm_prompt_optimization pip install -r requirements.txt python evaluate_prompts.py --dataset all --prompt-type evolved &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Works with any OpenAI-compatible API (OpenRouter, vLLM, Ollama).&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/codelion/openevolve"&gt;OpenEvolve Repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Curious if anyone's compared evolutionary vs reflection-based (GEPA) vs gradient-based (DSPy) approaches on their own tasks? What's been your experience with prompt optimization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mskf61/openevolve_beats_gepa_benchmarks_642_overall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T07:19:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrxuwd</id>
    <title>Best Opensource LM Studio alternative</title>
    <updated>2025-08-16T15:06:22+00:00</updated>
    <author>
      <name>/u/haterloco</name>
      <uri>https://old.reddit.com/user/haterloco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm looking for the best app to use llama.cpp or Ollama with a GUI on Linux.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/haterloco"&gt; /u/haterloco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrxuwd/best_opensource_lm_studio_alternative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T15:06:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mrqj6y</id>
    <title>Moxie goes local</title>
    <updated>2025-08-16T09:42:55+00:00</updated>
    <author>
      <name>/u/Over-Mix7071</name>
      <uri>https://old.reddit.com/user/Over-Mix7071</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt; &lt;img alt="Moxie goes local" src="https://external-preview.redd.it/NjRrNWZhaTZyY2pmMSz-4GeMjZaaPuK_BtqJdauJLy8SeG31djvp2OceGUPi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f56d4e2d6d85d38d0a6fee04a3f5cd06f2d2d7df" title="Moxie goes local" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a localllama version of the OpenMoxie&lt;/p&gt; &lt;p&gt;It uses faster-whisper on the local for STT or the OpenAi whisper api (when selected in setup)&lt;/p&gt; &lt;p&gt;Supports LocalLLaMA, or OpenAi for conversations.&lt;/p&gt; &lt;p&gt;I also added support for XAI (Grok3 et al ) using the XAI API.&lt;/p&gt; &lt;p&gt;allows you to select what AI model you want to run for the local service.. right now 3:2b &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Over-Mix7071"&gt; /u/Over-Mix7071 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/eiwf36o6rcjf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mrqj6y/moxie_goes_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T09:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1msbfw9</id>
    <title>Qwen3 just gets me ‚ù§Ô∏è</title>
    <updated>2025-08-16T23:31:35+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"&gt; &lt;img alt="Qwen3 just gets me ‚ù§Ô∏è" src="https://b.thumbs.redditmedia.com/0GYMAK-JtLBRlRV0DjTz-LN109QuZuO4A0Gni-WxIJI.jpg" title="Qwen3 just gets me ‚ù§Ô∏è" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1msbfw9"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msbfw9/qwen3_just_gets_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms222w</id>
    <title>"AGI" is equivalent to "BTC is going to take over the financial world"</title>
    <updated>2025-08-16T17:37:53+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;AGI&amp;quot; is really just another hypetrain. Sure AI is going to disrupt industries, displace jobs and cause mayhem in the social fabric - but the omnipotent &amp;quot;AGI&amp;quot; that governs all aspects of life and society and most importantly, ushers in &amp;quot;post labor economics&amp;quot;? Wonder how long it takes until tech bros and fanboys realize this. GPT5, Opus 4 and all others are only incremental improvements, if at all. Where's the path to &amp;quot;AGI&amp;quot; in this reality? People who believe this are going to build a bubble for themselves, detached from reality.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms222w/agi_is_equivalent_to_btc_is_going_to_take_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T17:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1msa1n4</id>
    <title>So Steam finally got back to me</title>
    <updated>2025-08-16T22:34:49+00:00</updated>
    <author>
      <name>/u/ChrisZavadil</name>
      <uri>https://old.reddit.com/user/ChrisZavadil</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 5 weeks of waiting for steam to approve my application that would allow users to input their own llms locally and communicate with them they told me that my app failed testing because it lacked the proper guardrails. They want me to block input and output for the LLM. Anybody put an unguarded LLM on Steam before?&lt;/p&gt; &lt;p&gt;I added a walledguard and re-uploaded, but for now I just made the full unrestricted version available on Itch if anyone wants to give it a try:&lt;br /&gt; &lt;a href="https://zavgaming.itch.io/megan-ai"&gt;https://zavgaming.itch.io/megan-ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChrisZavadil"&gt; /u/ChrisZavadil &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msa1n4/so_steam_finally_got_back_to_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:34:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms9djc</id>
    <title>Wan2.2 i2v Censors Chinese-looking women in nsfw workflows</title>
    <updated>2025-08-16T22:08:13+00:00</updated>
    <author>
      <name>/u/dennisitnet</name>
      <uri>https://old.reddit.com/user/dennisitnet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been using wan2.2 i2v for generating over 100 nsfw videos so far. Noticed something curious. Lol When input image is chinese-looking, it never outputs nsfw videos. But when I use non-chinese input images, it outputs nsfw.&lt;/p&gt; &lt;p&gt;Anybody else experienced this? Lol really curious shiz&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dennisitnet"&gt; /u/dennisitnet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms9djc/wan22_i2v_censors_chineselooking_women_in_nsfw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T22:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1msjr8e</id>
    <title>Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp; 1.6B)</title>
    <updated>2025-08-17T06:39:35+00:00</updated>
    <author>
      <name>/u/benja0x40</name>
      <uri>https://old.reddit.com/user/benja0x40</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt; &lt;img alt="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" src="https://external-preview.redd.it/ODf4ePnObFjNLo_T-D3tl5IjEp3QG9wN69Zl1K75jBk.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6edbd43244d19019e3eb00f1cf461de13c681f23" title="Liquid AI announced LFM2-VL, fast and lightweight vision models (450M &amp;amp; 1.6B)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;2 models based on the hybrid &lt;a href="https://www.liquid.ai/blog/liquid-foundation-models-v2-our-second-series-of-generative-ai-models"&gt;LFM2 architecture&lt;/a&gt;: LFM2-VL-450M and LFM2-VL-1.6B&lt;/li&gt; &lt;li&gt;8bit MLX quant available&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;HuggingFace Collection&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f7cnaj82zijf1.png?width=2072&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8dae64814c6f611481706d86e4a7643b7dc776"&gt;Figure 3. Processing time comparison across vision-language models.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/benja0x40"&gt; /u/benja0x40 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msjr8e/liquid_ai_announced_lfm2vl_fast_and_lightweight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T06:39:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1msgl6q</id>
    <title>Added Qwen 0.6B to the small model overview in IFEval.</title>
    <updated>2025-08-17T03:41:26+00:00</updated>
    <author>
      <name>/u/paranoidray</name>
      <uri>https://old.reddit.com/user/paranoidray</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt; &lt;img alt="Added Qwen 0.6B to the small model overview in IFEval." src="https://external-preview.redd.it/9Cl7KVCIkap1D9OBhLKIL0DrKnbvINMV1azrpCVXD0U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db8e296f1f9b6b888a016ade0da5771f2fa87434" title="Added Qwen 0.6B to the small model overview in IFEval." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paranoidray"&gt; /u/paranoidray &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.imgur.com/ygMzbHp.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msgl6q/added_qwen_06b_to_the_small_model_overview_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-17T03:41:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ms4n55</id>
    <title>What does it feel like: Cloud LLM vs Local LLM.</title>
    <updated>2025-08-16T19:10:29+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt; &lt;img alt="What does it feel like: Cloud LLM vs Local LLM." src="https://preview.redd.it/8qtcdau4kfjf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=64c4609d4440c5a870f624682bb7bead5dece104" title="What does it feel like: Cloud LLM vs Local LLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don't get me wrong, I love local models, but they give me this anxiety. We need to fix this... üòÇ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8qtcdau4kfjf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ms4n55/what_does_it_feel_like_cloud_llm_vs_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T19:10:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1msb0mq</id>
    <title>For those who run large models locally.. HOW DO YOU AFFORD THOSE GPUS</title>
    <updated>2025-08-16T23:14:00+00:00</updated>
    <author>
      <name>/u/abaris243</name>
      <uri>https://old.reddit.com/user/abaris243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;okay I'm just being nosy.. I mostly run models and fine tune as a hobby so I typically only run models under the 10b parameter range, is everyone that is running larger models just paying for cloud services to run them? and for those of you who do have stacks of A100/H100s is this what you do for a living, how do you afford it??&lt;/p&gt; &lt;p&gt;edit: for more context about me and my setup, I have a 3090ti and 64gb ram, I am actually a cgi generalist / 3d character artist and my industry is taking a huge hit right now, so with my extra free time and my already decent set up I've been learning to fine tune models and format data on the side, idk if ill ever do a full career 180 but I love new tech (even though these new technologies and ideas are eating my current career)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abaris243"&gt; /u/abaris243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1msb0mq/for_those_who_run_large_models_locally_how_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-16T23:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mjf5ol</id>
    <title>r/LocalLlama is looking for moderators</title>
    <updated>2025-08-06T20:06:34+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/application/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mjf5ol/rlocalllama_is_looking_for_moderators/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-06T20:06:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
