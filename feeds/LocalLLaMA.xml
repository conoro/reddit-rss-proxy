<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-29T14:48:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ntcnqi</id>
    <title>Which samplers at this point are outdated</title>
    <updated>2025-09-29T08:32:06+00:00</updated>
    <author>
      <name>/u/Long_comment_san</name>
      <uri>https://old.reddit.com/user/Long_comment_san</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which samplers would you say at this moment are superceded by other samplers/combos and why? IMHO: temperature has not been replaced as a baseline sampler. Min p seems like a common pick from what I can see on the sub. So what about: typical p, top a, top K, smooth sampling, XTC, mirostat (1,2), dynamic temperature. Would you say some are outright better pick over the others? Personally I feel &amp;quot;dynamic samplers&amp;quot; are a more interesting alternative but have some weird tendencies to overshoot, but feel a lot less &amp;quot;robotic&amp;quot; over min p + top k.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_comment_san"&gt; /u/Long_comment_san &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcnqi/which_samplers_at_this_point_are_outdated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcnqi/which_samplers_at_this_point_are_outdated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcnqi/which_samplers_at_this_point_are_outdated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T08:32:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nthpwc</id>
    <title>Google AI edge Gallery , oppo reno 13F , 12 ram</title>
    <updated>2025-09-29T13:14:52+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nthpwc/google_ai_edge_gallery_oppo_reno_13f_12_ram/"&gt; &lt;img alt="Google AI edge Gallery , oppo reno 13F , 12 ram" src="https://b.thumbs.redditmedia.com/8x0vCtwDvTg8XH3WRr4uJnSELa6IqTBYnRxkMEf9mGU.jpg" title="Google AI edge Gallery , oppo reno 13F , 12 ram" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it should go faster on Snapdragon 7, 8, necessarily 12 ram for it to serve,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nthpwc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nthpwc/google_ai_edge_gallery_oppo_reno_13f_12_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nthpwc/google_ai_edge_gallery_oppo_reno_13f_12_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T13:14:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsu3og</id>
    <title>Drummer's Cydonia R1 24B v4.1 ¬∑ A less positive, less censored, better roleplay, creative finetune with reasoning!</title>
    <updated>2025-09-28T17:25:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"&gt; &lt;img alt="Drummer's Cydonia R1 24B v4.1 ¬∑ A less positive, less censored, better roleplay, creative finetune with reasoning!" src="https://external-preview.redd.it/pROfhPfXKnMC7ws-LUjL0JI_7ZtgvBAu9hx7L06rI9c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc893240ec75e9bd482665fa32ded5d1badc97c0" title="Drummer's Cydonia R1 24B v4.1 ¬∑ A less positive, less censored, better roleplay, creative finetune with reasoning!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Backlog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cydonia v4.2.0,&lt;/li&gt; &lt;li&gt;Snowpiercer 15B v3,&lt;/li&gt; &lt;li&gt;Anubis Mini 8B v1&lt;/li&gt; &lt;li&gt;Behemoth ReduX 123B v1.1 (v4.2.0 treatment)&lt;/li&gt; &lt;li&gt;RimTalk Mini (showcase)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can't wait to release v4.2.0. I think it's proof that I still have room to grow. You can test it out here: &lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and I went ahead and gave Largestral 2407 the same treatment here: &lt;a href="https://huggingface.co/BeaverAI/Behemoth-ReduX-123B-v1b-GGUF"&gt;https://huggingface.co/BeaverAI/Behemoth-ReduX-123B-v1b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T17:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2c38</id>
    <title>Llama.cpp MoE models find best --n-cpu-moe value</title>
    <updated>2025-09-28T23:00:55+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Being able to run larger LLM on consumer equipment keeps getting better. Running MoE models is a big step and now with CPU offloading it's an even bigger step.&lt;/p&gt; &lt;p&gt;Here is what is working for me on my RX 7900 GRE 16GB GPU running the Llama4 Scout 108B parameter beast. I use &lt;em&gt;--n-cpu-moe 30,40,50,60&lt;/em&gt; to find my focus range. &lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-bench -m /meta-llama_Llama-4-Scout-17B-16E-Instruct-IQ3_XXS.gguf --n-cpu-moe 30,40,50,60&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_cpu_moe&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.50 ¬± 0.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;30&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.58 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;40&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;150.33 ¬± 0.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;40&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.30 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;136.62 ¬± 0.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;50&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.36 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;137.33 ¬± 1.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;60&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.33 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Here we figured out where to start. 30 didn't have boost but 40 did so lets try around those values.&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-bench -m /meta-llama_Llama-4-Scout-17B-16E-Instruct-IQ3_XXS.gguf --n-cpu-moe 31,32,33,34,35,36,37,38,39,41,42,43&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_cpu_moe&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;31&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.52 ¬± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;31&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;6.82 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.92 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.09 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.95 ¬± 0.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;33&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.35 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;34&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;23.06 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;34&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.47 ¬± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;35&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.89 ¬± 0.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;35&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.96 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;36&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;23.09 ¬± 0.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;36&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.96 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;37&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.95 ¬± 0.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;37&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.28 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;38&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;22.46 ¬± 0.39&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;38&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.41 ¬± 0.22&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;153.23 ¬± 0.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.42 ¬± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;41&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;148.07 ¬± 1.28&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;41&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.15 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;144.90 ¬± 0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;42&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;8.01 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;144.11 ¬± 1.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama4 17Bx16E (Scout) IQ3_XXS - 3.0625 bpw&lt;/td&gt; &lt;td align="left"&gt;41.86 GiB&lt;/td&gt; &lt;td align="left"&gt;107.77 B&lt;/td&gt; &lt;td align="left"&gt;RPC,Vulkan&lt;/td&gt; &lt;td align="left"&gt;99&lt;/td&gt; &lt;td align="left"&gt;43&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;7.87 ¬± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So for best performance I can run: &lt;code&gt;./llama-server -m /meta-llama_Llama-4-Scout-17B-16E-Instruct-IQ3_XXS.gguf --n-cpu-moe 39&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Huge improvements!&lt;/p&gt; &lt;p&gt;pp512 = 20.67, tg128 = 4.00 t/s no moe &lt;/p&gt; &lt;p&gt;pp512 = 153.23, tg128 = 8.42 t.s with --n-cpu-moe 39&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2c38/llamacpp_moe_models_find_best_ncpumoe_value/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2c38/llamacpp_moe_models_find_best_ncpumoe_value/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2c38/llamacpp_moe_models_find_best_ncpumoe_value/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T23:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntas8u</id>
    <title>KoboldCpp &amp; Croco.Cpp - Updated versions</title>
    <updated>2025-09-29T06:26:37+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR .... &lt;a href="https://github.com/LostRuins/koboldcpp"&gt;KoboldCpp&lt;/a&gt; for llama.cpp &amp;amp; &lt;a href="https://github.com/Nexesenex/croco.cpp"&gt;Croco.Cpp&lt;/a&gt; for ik_llama.cpp&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;KoboldCpp is an easy-to-use AI text-generation software for GGML and GGUF models, inspired by the original KoboldAI. It's a single self-contained distributable that builds off &lt;strong&gt;llama.cpp&lt;/strong&gt; and adds many additional powerful features. &lt;/p&gt; &lt;p&gt;Croco.Cpp is fork of KoboldCPP infering GGML/GGUF models on CPU/Cuda with KoboldAI's UI. It's powered partly by &lt;strong&gt;IK_LLama.cpp&lt;/strong&gt;, and compatible with most of Ikawrakow's quants except Bitnet.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Though I'm using KoboldCpp for sometime(along with Jan), I haven't tried Croco.Cpp yet &amp;amp; I was waiting for latest version which is ready now. Both are so useful for people who doesn't prefer command line stuff.&lt;/p&gt; &lt;p&gt;I see KoboldCpp's &lt;a href="https://github.com/LostRuins/koboldcpp/releases/tag/v1.99.4"&gt;current version&lt;/a&gt; is so nice due to changes like QOL change &amp;amp; UI design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntas8u/koboldcpp_crococpp_updated_versions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntas8u/koboldcpp_crococpp_updated_versions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntas8u/koboldcpp_crococpp_updated_versions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T06:26:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntglg5</id>
    <title>I built EdgeBox, an open-source local sandbox with a full GUI desktop, all controllable via the MCP protocol.</title>
    <updated>2025-09-29T12:25:04+00:00</updated>
    <author>
      <name>/u/Diao_nasing</name>
      <uri>https://old.reddit.com/user/Diao_nasing</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntglg5/i_built_edgebox_an_opensource_local_sandbox_with/"&gt; &lt;img alt="I built EdgeBox, an open-source local sandbox with a full GUI desktop, all controllable via the MCP protocol." src="https://preview.redd.it/2w6bjp03k3sf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=3bbe7dc260557332836c22fecb2f56bc0083ce5c" title="I built EdgeBox, an open-source local sandbox with a full GUI desktop, all controllable via the MCP protocol." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey LocalLLaMa community,&lt;/p&gt; &lt;p&gt;I always wanted my MCP agents to do more than just execute code‚ÄîI wanted them to actually use a GUI. So, I built &lt;strong&gt;EdgeBox&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It's a free, open-source desktop app that gives your agent a &lt;strong&gt;local sandbox with a full GUI desktop&lt;/strong&gt;, all controllable via the MCP protocol.&lt;/p&gt; &lt;h1&gt;Core Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-Config Local MCP Server&lt;/strong&gt;: Works out of the box, no setup required.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Control the Desktop via MCP&lt;/strong&gt;: Provides tools like &lt;code&gt;desktop_mouse_click&lt;/code&gt; and &lt;code&gt;desktop_screenshot&lt;/code&gt; to let the agent operate the GUI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Built-in Code Interpreter &amp;amp; Filesystem&lt;/strong&gt;: Includes all the core tools you need, like &lt;code&gt;execute_python&lt;/code&gt; and &lt;code&gt;fs_write&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is open-source, and I'd love for you to try it out and give some feedback!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo (includes downloads):&lt;/strong&gt; &lt;a href="https://github.com/BIGPPWONG/edgebox"&gt;https://github.com/BIGPPWONG/edgebox&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks, everyone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Diao_nasing"&gt; /u/Diao_nasing &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2w6bjp03k3sf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntglg5/i_built_edgebox_an_opensource_local_sandbox_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntglg5/i_built_edgebox_an_opensource_local_sandbox_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntf2pk</id>
    <title>Does anyone have a link to the paper for the new sparse attention arch of Deepseek-v3.2?</title>
    <updated>2025-09-29T11:07:05+00:00</updated>
    <author>
      <name>/u/Euphoric_Ad9500</name>
      <uri>https://old.reddit.com/user/Euphoric_Ad9500</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The only thing I have found is the Native Sparse Attention paper they released in February. It seems like they could be using Native Sparse Attention, but I can't be sure. Whatever they are using is compatible with MLA.&lt;/p&gt; &lt;p&gt;NSA paper: &lt;a href="https://arxiv.org/abs/2502.11089"&gt;https://arxiv.org/abs/2502.11089&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Euphoric_Ad9500"&gt; /u/Euphoric_Ad9500 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntf2pk/does_anyone_have_a_link_to_the_paper_for_the_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntf2pk/does_anyone_have_a_link_to_the_paper_for_the_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntf2pk/does_anyone_have_a_link_to_the_paper_for_the_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T11:07:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsy2ak</id>
    <title>GLM4.6 soon ?</title>
    <updated>2025-09-28T20:01:14+00:00</updated>
    <author>
      <name>/u/Angel-Karlsson</name>
      <uri>https://old.reddit.com/user/Angel-Karlsson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt; &lt;img alt="GLM4.6 soon ?" src="https://b.thumbs.redditmedia.com/dskyWZciazzyTx2dXG0mytJV0Yq5yzQ46bgrpLsQ1-s.jpg" title="GLM4.6 soon ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/usggbqdmoyrf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1677630b65f1d4dee3a6776247a0a5f31be050d"&gt;https://preview.redd.it/usggbqdmoyrf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1677630b65f1d4dee3a6776247a0a5f31be050d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While browsing the &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; website, I noticed this... maybe GLM4.6 is coming soon? Given the digital shift, I don't expect major changes... I ear some context lenght increase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Angel-Karlsson"&gt; /u/Angel-Karlsson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsmksq</id>
    <title>What are Kimi devs smoking</title>
    <updated>2025-09-28T12:02:02+00:00</updated>
    <author>
      <name>/u/Thechae9</name>
      <uri>https://old.reddit.com/user/Thechae9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt; &lt;img alt="What are Kimi devs smoking" src="https://preview.redd.it/t8wfkk09bwrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ec5f0e9de05cf0aafc8bc507d4950ca47e8ef09" title="What are Kimi devs smoking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strangee&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thechae9"&gt; /u/Thechae9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8wfkk09bwrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntfkjq</id>
    <title>Why no small &amp; medium size models from Deepseek?</title>
    <updated>2025-09-29T11:34:23+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last time I downloaded something was their Distillations(Qwen 1.5B, 7B, 14B &amp;amp; Llama 8B) during R1 release last Jan/Feb. After that, most of their models are 600B+ size. My hardware(8GB VRAM, 32B RAM) can't even touch those.&lt;/p&gt; &lt;p&gt;It would be great if they release small &amp;amp; medium size models like how Qwen done. Also couple of MOE models particularly one with 30-40B size.&lt;/p&gt; &lt;p&gt;BTW lucky big rig folks, enjoy DeepSeek-V3.2-Exp soon onwards.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfkjq/why_no_small_medium_size_models_from_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfkjq/why_no_small_medium_size_models_from_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfkjq/why_no_small_medium_size_models_from_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T11:34:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntheaj</id>
    <title>New to LLMs - What‚Äôs the Best Local AI Stack for a Complete ChatGPT Replacement?</title>
    <updated>2025-09-29T13:00:56+00:00</updated>
    <author>
      <name>/u/Live_Drive_6256</name>
      <uri>https://old.reddit.com/user/Live_Drive_6256</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I‚Äôm looking to set up my own private, local LLM on my PC. I‚Äôve got a pretty powerful setup with 20TB of storage, 256GB of RAM, an RTX 3090, and an i9 CPU.&lt;/p&gt; &lt;p&gt;I‚Äôm super new to LLMs but just discovered I can host them private and locally on my own PC with an actual WebUI like ChatGPT. I‚Äôm after something that can basically interpret images and files, generate images and code, handle long conversations or scripts without losing context, delusion, repetitiveness. Ideally act as a complete offline alternative to ChatGPT-5.&lt;/p&gt; &lt;p&gt;Is this possible to even achieve? Am I delusional??? Can I even host an AI model stack that can do everything ChatGPT does like reasoning, vision, coding, creativity, but fully private and running on my own machine with these specs? &lt;/p&gt; &lt;p&gt;If anyone has experience building this kind of all-in-one local setup or can recommend the best models and tools for it, I‚Äôd really appreciate the advice.&lt;/p&gt; &lt;p&gt;Thanks!!!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Live_Drive_6256"&gt; /u/Live_Drive_6256 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntheaj/new_to_llms_whats_the_best_local_ai_stack_for_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntheaj/new_to_llms_whats_the_best_local_ai_stack_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntheaj/new_to_llms_whats_the_best_local_ai_stack_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T13:00:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt0kn3</id>
    <title>Someone pinch me .! ü§£ Am I seeing this right ?.üôÑ</title>
    <updated>2025-09-28T21:43:22+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"&gt; &lt;img alt="Someone pinch me .! ü§£ Am I seeing this right ?.üôÑ" src="https://b.thumbs.redditmedia.com/uZSQcPB3gvUjO5vlPxt8AX_gBhOg-2Up3FDO2_kjoZE.jpg" title="Someone pinch me .! ü§£ Am I seeing this right ?.üôÑ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A what looks like 4080S with 32GB vRam ..! üßê . I just got 2X 3080 20GB üò´&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nt0kn3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T21:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt2l57</id>
    <title>Qwen3 Omni AWQ released</title>
    <updated>2025-09-28T23:12:33+00:00</updated>
    <author>
      <name>/u/No_Information9314</name>
      <uri>https://old.reddit.com/user/No_Information9314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Information9314"&gt; /u/No_Information9314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T23:12:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntgbag</id>
    <title>Literally me this weekend, after 2+ hours of trying I did not manage to make AWQ quant work on a100, meanwhile the same quant works in vLLM without any problems...</title>
    <updated>2025-09-29T12:11:55+00:00</updated>
    <author>
      <name>/u/Theio666</name>
      <uri>https://old.reddit.com/user/Theio666</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntgbag/literally_me_this_weekend_after_2_hours_of_trying/"&gt; &lt;img alt="Literally me this weekend, after 2+ hours of trying I did not manage to make AWQ quant work on a100, meanwhile the same quant works in vLLM without any problems..." src="https://preview.redd.it/7mx203wgh3sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf661e0987faa8d24ab8bfb1709f07e3b7f14ac5" title="Literally me this weekend, after 2+ hours of trying I did not manage to make AWQ quant work on a100, meanwhile the same quant works in vLLM without any problems..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Theio666"&gt; /u/Theio666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7mx203wgh3sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntgbag/literally_me_this_weekend_after_2_hours_of_trying/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntgbag/literally_me_this_weekend_after_2_hours_of_trying/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:11:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt1jaa</id>
    <title>Good ol gpu heat</title>
    <updated>2025-09-28T22:24:45+00:00</updated>
    <author>
      <name>/u/animal_hoarder</name>
      <uri>https://old.reddit.com/user/animal_hoarder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt; &lt;img alt="Good ol gpu heat" src="https://preview.redd.it/94k8168cezrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7614d55e91893f545ea06888d5bbbc047c1ec146" title="Good ol gpu heat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live at 9600ft in a basement with extremely inefficient floor heaters, so it‚Äôs usually 50-60F inside year round. I‚Äôve been fine tuning Mistral 7B for a dungeons and dragons game I‚Äôve been working on and oh boy does my 3090 pump out some heat. Popped the front cover off for some more airflow. My cat loves my new hobby, he just waits for me to run another training script so he can soak it in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/animal_hoarder"&gt; /u/animal_hoarder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94k8168cezrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T22:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt8jf0</id>
    <title>I have discovered DeepSeeker V3.2-Base</title>
    <updated>2025-09-29T04:10:45+00:00</updated>
    <author>
      <name>/u/ReceptionExternal344</name>
      <uri>https://old.reddit.com/user/ReceptionExternal344</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt; &lt;img alt="I have discovered DeepSeeker V3.2-Base" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="I have discovered DeepSeeker V3.2-Base" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discovered the deepseek-3.2-base repository on Hugging Face just half an hour ago, but within minutes it returned a 404 error. Another model is on its way!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/al21vk9t31sf1.png?width=2690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=067b5daef487efac4fba9699c13a24294088dc42"&gt;https://preview.redd.it/al21vk9t31sf1.png?width=2690&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=067b5daef487efac4fba9699c13a24294088dc42&lt;/a&gt;&lt;/p&gt; &lt;p&gt;unfortunately, I forgot to check the config.json file and only took a screenshot of the repository. I'll just wait for the release now.&lt;/p&gt; &lt;p&gt;Now we have discoveredÔºö&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2/"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2/&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ReceptionExternal344"&gt; /u/ReceptionExternal344 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt8jf0/i_have_discovered_deepseeker_v32base/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T04:10:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntiua9</id>
    <title>We just open-sourced Kroko ASR: a fast, streaming alternative to Whisper. It‚Äôs early days, we‚Äôd love testers, feedback, and contributors.</title>
    <updated>2025-09-29T14:01:58+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;First batch&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Streaming models (CC-BY-SA), ready for CPU, mobile, or browser&lt;/li&gt; &lt;li&gt;More extreme but affordable commercial models (with Apache inference code)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Languages&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A dozen to start, more on the way (Polish and Japanese coming next.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why it‚Äôs different&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Much smaller download than Whisper&lt;/li&gt; &lt;li&gt;Much faster on CPU (runs on mobile or even in the browser, try the the demo on android)&lt;/li&gt; &lt;li&gt;(Almost) hallucination-free&lt;/li&gt; &lt;li&gt;Streaming support: great for voice assistants, live agent assist, note taking, or just yelling at your computer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Quality&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Offline models beat Whisper v3-large while being about 10√ó smaller&lt;/li&gt; &lt;li&gt;Streaming models are comparable (or better) at 1s chunk size&lt;/li&gt; &lt;li&gt;There‚Äôs a trade-off in quality at ultra-low latency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Project goals&lt;/strong&gt;&lt;br /&gt; Build a community and democratize speech-to-text, making it easier to train models and run them at the edge (without needing a PhD in speech AI).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;website &amp;amp; cloud demo: &lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Android model explorer: &lt;a href="https://play.google.com/store/apps/details?id=com.krokoasr.demo&amp;amp;hl=en"&gt;Google Play&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Discord: &lt;a href="https://discord.gg/nnY9nQac"&gt;discord.gg/nnY9nQac&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/kroko-ai/kroko-onnx"&gt;https://github.com/kroko-ai/kroko-onnx&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face Demo: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;Kroko Streaming ASR Wasm&lt;/a&gt; (older models, updates coming soon)&lt;/li&gt; &lt;li&gt;community models page: &lt;a href="https://huggingface.co/Banafo/Kroko-ASR"&gt;https://huggingface.co/Banafo/Kroko-ASR&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Thoughts / caveats&lt;/strong&gt;&lt;br /&gt; We‚Äôre still ironing out some things, especially around licensing limits and how to release models in the fairest way. Our philosophy is: easier to give more than to give less later. Some details may change as we learn from the community.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Future&lt;/strong&gt;&lt;br /&gt; There is plenty of room to improve the models, as most are still trained on our older pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;&lt;br /&gt; Smaller, faster, (almost) hallucination-free Whisper replacement that streams on CPU/mobile. Looking for testers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T14:01:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntcy33</id>
    <title>DeepSeek online model updated</title>
    <updated>2025-09-29T08:51:48+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcy33/deepseek_online_model_updated/"&gt; &lt;img alt="DeepSeek online model updated" src="https://preview.redd.it/9is51ljzh2sf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a258b226765fa5eefbb3d3e92ca70908860d33ed" title="DeepSeek online model updated" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sender: DeepSeek Assistant DeepSeek&lt;br /&gt; Message: The DeepSeek online model has been updated to a new version. Everyone is welcome to test it and provide feedback~&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9is51ljzh2sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcy33/deepseek_online_model_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntcy33/deepseek_online_model_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T08:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntfbm0</id>
    <title>DeepSeek Updates API Pricing (DeepSeek-V3.2-Exp)</title>
    <updated>2025-09-29T11:21:00+00:00</updated>
    <author>
      <name>/u/Agwinao</name>
      <uri>https://old.reddit.com/user/Agwinao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfbm0/deepseek_updates_api_pricing_deepseekv32exp/"&gt; &lt;img alt="DeepSeek Updates API Pricing (DeepSeek-V3.2-Exp)" src="https://preview.redd.it/0hwzyvjr83sf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=81278c017a6d397ce1e552fdf94c7948c0595869" title="DeepSeek Updates API Pricing (DeepSeek-V3.2-Exp)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;$0.028 / 1M Input Tokens (Cache Hit), $0.28 / 1M Input Tokens (Cache Miss), $0.42 / 1M Output Tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agwinao"&gt; /u/Agwinao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0hwzyvjr83sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfbm0/deepseek_updates_api_pricing_deepseekv32exp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntfbm0/deepseek_updates_api_pricing_deepseekv32exp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T11:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte4j1</id>
    <title>Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base ‚Ä¢ HuggingFace</title>
    <updated>2025-09-29T10:09:56+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt; &lt;img alt="Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base ‚Ä¢ HuggingFace" src="https://external-preview.redd.it/VTfzQkd7AA6Y5gHEsOqxIa7Bzf1OPlvJrdnEYbmotnQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba6d5dd6d41a5218ce7ddbe2cce44e354d9f63ea" title="Deepseek-Ai/DeepSeek-V3.2-Exp and Deepseek-ai/DeepSeek-V3.2-Exp-Base ‚Ä¢ HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp-Base&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jrpylyzjx2sf1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8df476ca80a0fa1d01539f59049856b2e15979"&gt;https://preview.redd.it/jrpylyzjx2sf1.png?width=714&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8df476ca80a0fa1d01539f59049856b2e15979&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0hamcb6x2sf1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5abcd9c9073ae9a56e24b51c4c3fc7a7d13d5c33"&gt;https://preview.redd.it/d0hamcb6x2sf1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5abcd9c9073ae9a56e24b51c4c3fc7a7d13d5c33&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/celk1fb6x2sf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6713f8594e8a66987bade6a83ca404877e077646"&gt;https://preview.redd.it/celk1fb6x2sf1.png?width=552&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6713f8594e8a66987bade6a83ca404877e077646&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5xnbatckw2sf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2e365d129160081fc292cbac7c1c5b2dc50814"&gt;https://preview.redd.it/5xnbatckw2sf1.png?width=939&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fa2e365d129160081fc292cbac7c1c5b2dc50814&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:09:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntb5ab</id>
    <title>deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face</title>
    <updated>2025-09-29T06:49:56+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New Link &lt;a href="https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66"&gt;https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntb5ab/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T06:49:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt99fp</id>
    <title>GLM-4.6 now accessible via API</title>
    <updated>2025-09-29T04:52:14+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"&gt; &lt;img alt="GLM-4.6 now accessible via API" src="https://preview.redd.it/yrpnx9o7b1sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=034a4f48d831abff53602bf4adefb90e5b28a82b" title="GLM-4.6 now accessible via API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using the official API, I was able to access GLM 4.6. Looks like release is imminent.&lt;/p&gt; &lt;p&gt;On a side note, the reasoning traces look very different from previous Chinese releases, much more like Gemini models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yrpnx9o7b1sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt99fp/glm46_now_accessible_via_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T04:52:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1ntg6sp</id>
    <title>Chinese AI Labs Tier List</title>
    <updated>2025-09-29T12:05:39+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"&gt; &lt;img alt="Chinese AI Labs Tier List" src="https://preview.redd.it/ur65noupg3sf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19a3d6f6cec05bb06281985755fbed368e5c9ecf" title="Chinese AI Labs Tier List" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ur65noupg3sf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ntg6sp/chinese_ai_labs_tier_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nth7cb</id>
    <title>The reason why Deepseek V3.2 is so cheap</title>
    <updated>2025-09-29T12:52:22+00:00</updated>
    <author>
      <name>/u/Js8544</name>
      <uri>https://old.reddit.com/user/Js8544</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt; &lt;img alt="The reason why Deepseek V3.2 is so cheap" src="https://external-preview.redd.it/FcVX6tzRZ8tGOgZD8bxf6fQ4_S6KT4J6UgLFbHWcrYo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2dddb9f56e7fc9cdc0774a534e8c31cbb7079188" title="The reason why Deepseek V3.2 is so cheap" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TLDR: It's a linear model with almost O(kL) attention complexity.&lt;/p&gt; &lt;p&gt;Paper link: &lt;a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf"&gt;https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to their paper, the Deepseek Sparse Attention computes attention for only k selected previous tokens, meaning it's a linear attention model with decoding complexity O(kL). What's different from previous linear models is it has a O(L^2) index selector to select the tokens to compute attention for. Even though the index selector has square complexity but it's fast enough to be neglected. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h0zys7b4o3sf1.png?width=1390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00a7ea8ada91109d417b8d6e3f490ae9743c18b2"&gt;https://preview.redd.it/h0zys7b4o3sf1.png?width=1390&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00a7ea8ada91109d417b8d6e3f490ae9743c18b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/has2qyz7o3sf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0742135b2cb1be9bd853b614097597d521a4ef54"&gt;https://preview.redd.it/has2qyz7o3sf1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0742135b2cb1be9bd853b614097597d521a4ef54&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/053i7pdro3sf1.png?width=1356&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=52adfb1bf9d0ee03f0a7d8e7b31340ab63b2f4b4"&gt;Cost for V3.2 only increase very little thanks to linear attention&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Previous linear model attempts for linear models from other teams like Google and Minimax have not been successful. Let's see if DS can make the breakthrough this time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Js8544"&gt; /u/Js8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nth7cb/the_reason_why_deepseek_v32_is_so_cheap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T12:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nte1kr</id>
    <title>DeepSeek-V3.2 released</title>
    <updated>2025-09-29T10:04:40+00:00</updated>
    <author>
      <name>/u/Leather-Term-30</name>
      <uri>https://old.reddit.com/user/Leather-Term-30</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66"&gt;https://huggingface.co/collections/deepseek-ai/deepseek-v32-68da2f317324c70047c28f66&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leather-Term-30"&gt; /u/Leather-Term-30 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nte1kr/deepseekv32_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-29T10:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
