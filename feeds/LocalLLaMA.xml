<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-20T18:09:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1obk1ta</id>
    <title>In the current Alpha Arena AI live trading rankings, DeepSeek V3.1 Chat is #1, outperforming all major closed-source models so far.</title>
    <updated>2025-10-20T14:14:14+00:00</updated>
    <author>
      <name>/u/SkyWorld007</name>
      <uri>https://old.reddit.com/user/SkyWorld007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk1ta/in_the_current_alpha_arena_ai_live_trading/"&gt; &lt;img alt="In the current Alpha Arena AI live trading rankings, DeepSeek V3.1 Chat is #1, outperforming all major closed-source models so far." src="https://preview.redd.it/t77n3fnty9wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f364eed7f89ae6a88814f4f1f7836b6449ead5f6" title="In the current Alpha Arena AI live trading rankings, DeepSeek V3.1 Chat is #1, outperforming all major closed-source models so far." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyWorld007"&gt; /u/SkyWorld007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t77n3fnty9wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk1ta/in_the_current_alpha_arena_ai_live_trading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obk1ta/in_the_current_alpha_arena_ai_live_trading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T14:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1obkw41</id>
    <title>Finetuning LLMs on Strix Halo – Full, LoRA, and QLoRA on Gemma-3, Qwen-3, and GPT-OSS-20B</title>
    <updated>2025-10-20T14:50:08+00:00</updated>
    <author>
      <name>/u/Inevitable_Ant_2924</name>
      <uri>https://old.reddit.com/user/Inevitable_Ant_2924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=nxugSRDg_jg"&gt;https://www.youtube.com/watch?v=nxugSRDg_jg&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inevitable_Ant_2924"&gt; /u/Inevitable_Ant_2924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obkw41/finetuning_llms_on_strix_halo_full_lora_and_qlora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obkw41/finetuning_llms_on_strix_halo_full_lora_and_qlora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obkw41/finetuning_llms_on_strix_halo_full_lora_and_qlora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T14:50:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1obfodq</id>
    <title>DreamOmni2 — multimodal instruction-based editing &amp; generation (web demo + code)</title>
    <updated>2025-10-20T09:55:55+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source, unified model that uses text + reference images to do precise edits or full generations, including abstract attributes and multi-reference workflows. See the project page demos, try the HF Web demo, and grab code + weights. • Capabilities shown: object replacement, lighting/style transfer, pose/expression/hair edits, in-context &amp;amp; multi-reference examples. ￼ • Try it now: DreamOmni2-Edit Space on Hugging Face. ￼&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit"&gt;https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dvlab-research/DreamOmni2"&gt;https://github.com/dvlab-research/DreamOmni2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfodq/dreamomni2_multimodal_instructionbased_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfodq/dreamomni2_multimodal_instructionbased_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obfodq/dreamomni2_multimodal_instructionbased_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T09:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1obl7fn</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-10-20T15:04:35+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI, here are the local/edge highlights from last week:&lt;/p&gt; &lt;p&gt;PaddleOCR VL 0.9B - Multilingual VLM for OCR&lt;br /&gt; •0.9B parameters deliver efficient OCR performance across languages.&lt;br /&gt; •Runs smoothly on local setups with low resource needs.&lt;br /&gt; •&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2510.14528"&gt;Pape&lt;/a&gt;r&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img 7l29ffib8awf1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Qwen3-VL 4B/8B - Vision-Language Models with Instruct and Thinking Variants&lt;br /&gt; •4B and 8B sizes provide frontier VLM capabilities at edge-friendly scales.&lt;br /&gt; •Open weights support local deployment for vision tasks.&lt;br /&gt; •&lt;a href="https://x.com/Alibaba_Qwen/status/1978150959621734624"&gt;Announcement&lt;/a&gt; | &lt;a href="https://huggingface.co/collections/Qwen/qwen3-vl-68d2a7c1b8a8afce4ebd2dbe"&gt;Models&lt;/a&gt; | &lt;a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks"&gt;Cookbooks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing img u9rzxci88awf1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;ComfyUI-QwenVL - Multimodal AI in ComfyUI Workflows&lt;br /&gt; •Integrates text generation and image understanding into local ComfyUI setups.&lt;br /&gt; •Seamless for edge-based creative pipelines.&lt;br /&gt; •&lt;a href="https://github.com/1038lab/ComfyUI-QwenVL"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;FlashWorld - High-Quality 3D Scene Generation in Seconds&lt;br /&gt; •Generates 3D scenes from text or images in 5-10 seconds on consumer hardware.&lt;br /&gt; •Direct 3D Gaussian output combines 2D diffusion quality with geometric consistency.&lt;br /&gt; •Ideal for fast local 3D asset creation.&lt;br /&gt; •&lt;a href="https://imlixinyang.github.io/FlashWorld-Project-Page/"&gt;Project Page&lt;/a&gt;(w/ demo) | &lt;a href="https://arxiv.org/abs/2510.13678"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/imlixinyang/FlashWorld"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Trace Anything - Representing Videos in 4D via Trajectory Fields&lt;br /&gt; •Maps every video pixel to continuous 3D trajectories in a single pass.&lt;br /&gt; •State-of-the-art on trajectory estimation and point-tracking, faster than iterative methods.&lt;br /&gt; •Enables motion-based video search for edge applications.&lt;br /&gt; •&lt;a href="https://trace-anything.github.io/"&gt;Project Page&lt;/a&gt; | &lt;a href="https://huggingface.co/papers/2510.13802"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/ByteDance-Seed/TraceAnything"&gt;Code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Processing video lxw5pw9byawf1...&lt;/em&gt;&lt;/p&gt; &lt;p&gt;See the full newsletter for more demos, papers, more): &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-29-sampling-smarts"&gt;https://thelivingedge.substack.com/p/multimodal-monday-29-sampling-smarts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obl7fn/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obl7fn/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obl7fn/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T15:04:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1obgdae</id>
    <title>Which LLM to use to replace Gemma3?</title>
    <updated>2025-10-20T11:20:25+00:00</updated>
    <author>
      <name>/u/PSInvader</name>
      <uri>https://old.reddit.com/user/PSInvader</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I build a complex program that uses Gemma 3 27b to add a memory node graph, drives, emotions, goals, needs, identity, dreaming onto it, but I'm still using Gemma 3 to run the whole thing.&lt;/p&gt; &lt;p&gt;Is there any non-thinking LLM as of now that I can fully fit on my 3090 that can also handle complex JSON output and is good at conversations and would be an improvement?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/DAPRDNQ.png"&gt;Here is a screenshot of the program&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastes.io/cognitive-architecture-initialization-log-for-memtest"&gt;Link to terminal output of the start sequence of the program and a single reply generation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PSInvader"&gt; /u/PSInvader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgdae/which_llm_to_use_to_replace_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgdae/which_llm_to_use_to_replace_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obgdae/which_llm_to_use_to_replace_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T11:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1oblejc</id>
    <title>Some practical notes on Google’s newly released C2S-Scale 27B model</title>
    <updated>2025-10-20T15:13:36+00:00</updated>
    <author>
      <name>/u/thalacque</name>
      <uri>https://old.reddit.com/user/thalacque</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across community posts about this model a few days ago and ended up digging in much deeper than I expected. Google×Yale treat single-cell RNA-seq as cell sentences, built on Gemma-2 with 27B parameters. Officially, it’s trained on 57 million cells and over a billion tokens of transcriptomics plus text. Beyond cell-type prediction, it can also infer perturbation responses.&lt;/p&gt; &lt;p&gt;Two things matter most to me. First, both the scale and the representation hit the sweet spot: “translating” the expression matrix into tokens makes cross-dataset transfer and few-shot learning more plausible. Second, the openness is unusually friendly: model, weights, code, and paper are all released under CC BY 4.0. Reproducibility, head-to-head evaluations, and boundary testing, people can jump in right away.&lt;/p&gt; &lt;p&gt;I asked friends in the healthcare space, and they’d treat this kind of model as “experimental navigation.” For legacy projects, run annotations first to see if it surfaces overlooked small populations; for new topics, use it to suggest perturbation directions so experimental resources can be allocated toward trajectories that look more promising. It saves trial-and-error without compromising rigor.&lt;/p&gt; &lt;p&gt;27B is not small. FP16 on a single GPU typically needs 60–70 GB; 8-bit is around 28–35 GB; 4-bit can be compressed to about 16–22 GB, balancing speed and stability. 24 GB of VRAM is a comfortable starting point. It can run on CPU but it’s very slow. If you go with Transformers + bitsandbytes, bootstrapping from the Hugging Face reference code is smoother.&lt;/p&gt; &lt;p&gt;A few caveats. In vitro positives don’t equate to clinical closure; biases in single-cell data are hard to fully avoid; and the engineering bar of 27B will block a fair bit of reproduction. The good news is the resources are open, so cross-team repro, ablations, and distribution-shift checks the “solid work”, can move forward quickly.&lt;/p&gt; &lt;p&gt;I’m more keen to hear hands-on experience: which tasks would you try first, annotation, perturbation, or a small-scale reproduction to sketch out the boundaries?&lt;/p&gt; &lt;p&gt;&lt;a href="https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/"&gt;https://blog.google/technology/ai/google-gemma-ai-cancer-therapy-discovery/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B"&gt;https://huggingface.co/vandijklab/C2S-Scale-Gemma-2-27B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thalacque"&gt; /u/thalacque &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oblejc/some_practical_notes_on_googles_newly_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oblejc/some_practical_notes_on_googles_newly_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oblejc/some_practical_notes_on_googles_newly_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T15:13:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready 🎉</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready 🎉" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn0q7</id>
    <title>The Innovations in DeepSeek OCR</title>
    <updated>2025-10-20T16:29:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek just released a pretty shocking new paper. They really buried the lede here by referring to it simply as DeepSeek OCR. &lt;/p&gt; &lt;p&gt;While it’s a very strong OCR model, the purpose of it and the implications of their approach go far beyond what you’d expect of “yet another OCR model.”&lt;/p&gt; &lt;p&gt;Traditionally, vision LLM tokens almost seemed like an afterthought or “bolt on” to the LLM paradigm. And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens.&lt;/p&gt; &lt;p&gt;So those 10k words may have turned into 15k tokens, or 30k to 60k “visual tokens.” So vision tokens were way less efficient and really only made sense to use for data that couldn’t be effectively conveyed with words. &lt;/p&gt; &lt;p&gt;But that gets inverted now from the ideas in this paper. DeepSeek figured out how to get 10x better compression using vision tokens than with text tokens! So you could theoretically store those 10k words in just 1,500 of their special compressed visual tokens.&lt;/p&gt; &lt;p&gt;This might not be as unexpected as it sounds if you think of how your own mind works. After all, I know that when I’m looking for a part of a book that I’ve already read, I imagine it visually and always remember which side of the book it was on and approximately where on the page it was, which suggests some kind of visual memory representation at work.&lt;/p&gt; &lt;p&gt;Now, it’s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM; can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality? &lt;/p&gt; &lt;p&gt;But you can imagine that, depending on the exact tradeoffs, it could be a very exciting new axis to greatly expand effective context sizes. Especially when combined with DeepSeek’s other recent paper from a couple weeks ago about sparse attention.&lt;/p&gt; &lt;p&gt;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks. If they did, they probably wouldn’t say because it would be viewed as an important trade secret.&lt;/p&gt; &lt;p&gt;But the nice thing about DeepSeek is that they’ve made the entire thing open source and open weights and explained how they did it, so now everyone can try it out and explore.&lt;/p&gt; &lt;p&gt;Even if these tricks make attention more lossy, the potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting. &lt;/p&gt; &lt;p&gt;You could basically cram all of a company’s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective. &lt;/p&gt; &lt;p&gt;Or put an entire code base into the context and cache it, and then just keep appending the equivalent of the git diffs as you make changes to the code. &lt;/p&gt; &lt;p&gt;If you’ve ever read stories about the great physicist Hans Bethe, he was known for having vast amounts of random physical facts memorized (like the entire periodic table; boiling points of various substances, etc.) so that he could seamlessly think and compute without ever having to interrupt his flow to look something up in a reference table. &lt;/p&gt; &lt;p&gt;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;https://x.com/doodlestein/status/1980282222893535376&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn61p</id>
    <title>Ring-mini-sparse-2.0-exp, yet another experimental open source model from inclusionAI that tries to improve performance over long contexts</title>
    <updated>2025-10-20T16:36:41+00:00</updated>
    <author>
      <name>/u/Finanzamt_Endgegner</name>
      <uri>https://old.reddit.com/user/Finanzamt_Endgegner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-mini-sparse-2.0-exp, an open-source efficient inference model based on the Ling 2.0 MoE architecture. This sparse variant uses Mixture-of-Block-Attention (MoBA) to slash KV cache overhead by 87.5% (down to ~8K tokens/query at 64K context), enabling up to 3x decode speedup over dense-equivalent Ring-mini-2.0 while matching full softmax performance on reasoning tasks. Built by continual pretraining +100B tokens from Ling-mini-base-2.0-20T (16B total params, ~1.6B active via 1/32 expert ratio). → 128K context via YaRN 4x extrapolation · GQA heads with shared KV blocks per group for head-efficient sparsity → No RLHF, pure supervised finetuning for stability in high-concurrency setups. Delivers competitive results on math (e.g., AIME/HMMT-style), coding (LiveCodeBench), and science (ARC-AGI/HealthBench) evals—on par with 8B dense models like Qwen3-8B-Thinking, but with massive efficiency gains for local deployment. Open weights in BF16/Safetensors; runs on HF Transformers 4.45+ or SGLang 0.4+ (custom wheel needed).&lt;/p&gt; &lt;p&gt;For even longer contexts, check the sibling Ring-mini-linear-2.0: a hybrid linear+softmax attention setup (+600B tokens training) hitting 512K via YaRN, with near-linear O(N) time/compute for ultra-long inputs—but in the benchmarks, the sparse MoBA edged it out on reasoning accuracy/speed tradeoffs at sub-128K lengths without the linear attn quirks. Both crush the original baseline on throughput (see their model cards' figs for prefill/decode curves). Not affiliated, just sharing for local runners since I'm very interested in those experimental models trying to solve context (;&lt;/p&gt; &lt;p&gt;If I'm not mistaken they also open sourced the training code (; &lt;/p&gt; &lt;p&gt;Llama.cpp support wont be easy though /:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-sparse-2.0-exp"&gt;https://huggingface.co/inclusionAI/Ring-mini-sparse-2.0-exp&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/inclusionAI/Ring-mini-linear-2.0"&gt;https://huggingface.co/inclusionAI/Ring-mini-linear-2.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Finanzamt_Endgegner"&gt; /u/Finanzamt_Endgegner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-sparse-2.0-exp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn61p/ringminisparse20exp_yet_another_experimental_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn61p/ringminisparse20exp_yet_another_experimental_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:36:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob7q6m</id>
    <title>Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</title>
    <updated>2025-10-20T02:02:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"&gt; &lt;img alt="Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" src="https://external-preview.redd.it/T8a1JuGOfWYN7yWqfBi5-bruC3MzoVLZu36ygPTxd0o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264bdb16d9e4730080c65c21ffa671e23a0de176" title="Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/omnivinci"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T02:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1obexku</id>
    <title>Hands-on tutorial on fine-tuning Small Vision Models</title>
    <updated>2025-10-20T08:49:12+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this repository you will learn how to build and deploy high-accuracy-and-low-latency image classifers into your phone using local Visual Language Models.&lt;/p&gt; &lt;p&gt;We will use&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a sequence of increasingly complex classification tasks, to uncover step-by-step how to build highly-specialized image classification systems, tailored to your specific use case.&lt;/li&gt; &lt;li&gt;the &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;&lt;strong&gt;LFM2-VL&lt;/strong&gt; family of open-weight Visual Language Models (aka VLMs) by Liquid AI&lt;/a&gt; to classify images for these tasks.&lt;/li&gt; &lt;li&gt;the &lt;a href="https://leap.liquid.ai/docs"&gt;&lt;strong&gt;Leap Edge SDK&lt;/strong&gt;&lt;/a&gt; for iOS to deploy the final models into an iOS app.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the github repo: &lt;a href="https://github.com/Paulescu/image-classification-with-local-vlms"&gt;https://github.com/Paulescu/image-classification-with-local-vlms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obexku/handson_tutorial_on_finetuning_small_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obexku/handson_tutorial_on_finetuning_small_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obexku/handson_tutorial_on_finetuning_small_vision_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T08:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9bli</id>
    <title>How I Built Lightning-Fast Vector Search for Legal Documents</title>
    <updated>2025-10-20T03:22:16+00:00</updated>
    <author>
      <name>/u/Neon0asis</name>
      <uri>https://old.reddit.com/user/Neon0asis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9bli/how_i_built_lightningfast_vector_search_for_legal/"&gt; &lt;img alt="How I Built Lightning-Fast Vector Search for Legal Documents" src="https://external-preview.redd.it/yTyQwb8h92SADq3dGPHjZVD8A4qDhAKutT4IHEH7UFE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71cacbcc942a0d80e2aee0bed02c52803264926a" title="How I Built Lightning-Fast Vector Search for Legal Documents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon0asis"&gt; /u/Neon0asis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@adlumal/how-i-built-lightning-fast-vector-search-for-legal-documents-fbc3eaad55ea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9bli/how_i_built_lightningfast_vector_search_for_legal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9bli/how_i_built_lightningfast_vector_search_for_legal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T03:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1obo226</id>
    <title>whats up with the crazy amount of OCR models launching?</title>
    <updated>2025-10-20T17:19:16+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;aside from these models, we got MinerU2.5 and some other models i forgot. im most interested by DeepSeek launching an OCR model of all things, weren't they into AGI? do you think its for more efficient document parsing for training data or something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dfdpiv7fvawf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obo226/whats_up_with_the_crazy_amount_of_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T17:19:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob6ydq</id>
    <title>GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop</title>
    <updated>2025-10-20T01:23:37+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"&gt; &lt;img alt="GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop" src="https://external-preview.redd.it/9USPaHCqnaWZUhhwpPcmVYuxokNlKHBzm3mdxx2L9rE.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c5a9785f74812c007fd190cf17bd9645963730e" title="GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://linuxgizmos.com/gigabyte-ai-top-atom-introduces-nvidia-grace-blackwell-gb10-performance-for-the-desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T01:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1obk7k5</id>
    <title>Qwen3-VL-8B + vllm on 3060 12gb</title>
    <updated>2025-10-20T14:21:00+00:00</updated>
    <author>
      <name>/u/vava2603</name>
      <uri>https://old.reddit.com/user/vava2603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I used qwen2.5-vl-7b-awq during multiple weeks on my 3060 with vllm and was super satisfied with the perf. The model was maximizing the VRam usage &lt;/p&gt; &lt;p&gt;Now I’m trying to upgrade to qwen3-vl-8B but unfortunately I cannot managed to fit into the 12Gb of vram and it is crashing while trying to allocate KV cache . I’m using vllm 0.11&lt;/p&gt; &lt;p&gt;was wondering is someone managed to make it run ? was trying some options to offload the kvcache to cpu ram but it is not working … maybe using LMCache ? any clues are welcome &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vava2603"&gt; /u/vava2603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk7k5/qwen3vl8b_vllm_on_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk7k5/qwen3vl8b_vllm_on_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obk7k5/qwen3vl8b_vllm_on_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T14:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavxt8</id>
    <title>I built a 1B CAD generator model</title>
    <updated>2025-10-19T17:43:33+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt; &lt;img alt="I built a 1B CAD generator model" src="https://external-preview.redd.it/ZGFhNmE0bzJ2M3dmMdhv6U5XLy0vFYTB3BWLA3H-O3YDxkmUtGbojZ8LN3lz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a21d6d0c153a39bacb389fe42d52137134b86925" title="I built a 1B CAD generator model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a weekend, I decided to build a small language model to generate me 3d files. No reason except for pure curiosity. Here's what I did:&lt;/p&gt; &lt;p&gt;- Gather dataset on OpenSCAD: This turns out to be quite bad because people's code quality is low &amp;amp; in-consistent.&lt;/p&gt; &lt;p&gt;- Generate synthetic data (prompt -&amp;gt; openscad): This was the most wasteful per dollar part. I spent 150$+ on Claude API (70% are on reasoning token). Ended up using Gemma3-12b running in 48 hours continuously.&lt;/p&gt; &lt;p&gt;- Finetune Gemma3-270M, 1B &amp;amp; 4B: 270M lacks fundamental code &amp;amp; object understanding and failed badly. 1B is a good balance between render-ability rate &amp;amp; speed.&lt;/p&gt; &lt;p&gt;Overall, I spent 150$ on Claude (totally wasted) &amp;amp; 25$ on GPU. Both given as credits and grants.&lt;/p&gt; &lt;p&gt;I also made a CLI app if you wanna try on Mac, Linux or Raspberry Pi 4/5: &lt;a href="https://github.com/ThomasVuNguyen/MakeMe"&gt;https://github.com/ThomasVuNguyen/MakeMe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models, dataset &amp;amp; code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasVuNguyen/K"&gt;https://github.com/ThomasVuNguyen/K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b"&gt;https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pn0yo3o2v3wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1obha86</id>
    <title>What is the best ocr model for converting PDF pages to markdown (or any text based format) for embedding?</title>
    <updated>2025-10-20T12:07:44+00:00</updated>
    <author>
      <name>/u/PM_ME_COOL_SCIENCE</name>
      <uri>https://old.reddit.com/user/PM_ME_COOL_SCIENCE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on converting thousands of scientific pdfs to markdown for llm ingestion and embedding. The PDFs range from nice digital first PDFs to just images of pages in a .pdf format. I’d like the most accurate model to extract the text, tables, graphs, etc. I’ve been considering evaluating docling, paddlepaddle ocr VL, qwen 3 vl, dots.ocr, and now the new deepseek ocr. &lt;/p&gt; &lt;p&gt;Anyone have any suggestions for their most accurate model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_COOL_SCIENCE"&gt; /u/PM_ME_COOL_SCIENCE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obha86/what_is_the_best_ocr_model_for_converting_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obha86/what_is_the_best_ocr_model_for_converting_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obha86/what_is_the_best_ocr_model_for_converting_pdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T12:07:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1obmhh1</id>
    <title>Speculative decoding for on-CPU MoE?</title>
    <updated>2025-10-20T16:03:44+00:00</updated>
    <author>
      <name>/u/NickNau</name>
      <uri>https://old.reddit.com/user/NickNau</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have AM5 PC with 96gb RAM + 4090.&lt;/p&gt; &lt;p&gt;I can run gpt-oss-120b on llama.cpp with --cpu-moe and get ~28 t/s on small context.&lt;/p&gt; &lt;p&gt;I can run gpt-oss-20b fully in VRAM and get ~200 t/s.&lt;/p&gt; &lt;p&gt;The question is - can 20b be used as a draft for 120b and run fully in VRAM while 120b will be with --cpu-moe? It seem like 4090 has enough VRAM for this (for small context).&lt;/p&gt; &lt;p&gt;I tried to play with it but it does not work. I am getting same or less t/s with this setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The question&lt;/strong&gt;: is it a limitation of speculative decoding, misconfiguration on my side, or llama.cpp can not do this properly?&lt;/p&gt; &lt;p&gt;Command that I tried:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;./llama-server -m ./gpt-oss-120b-MXFP4-00001-of-00002.gguf -md ./gpt-oss-20b-MXFP4.gguf --jinja --cpu-moe --mlock --n-cpu-moe-draft 0 --gpu-layers-draft 999&lt;/p&gt; &lt;/blockquote&gt; &lt;pre&gt;&lt;code&gt;prompt eval time = 2560.86 ms / 74 tokens ( 34.61 ms per token, 28.90 tokens per second) eval time = 8880.45 ms / 256 tokens ( 34.69 ms per token, 28.83 tokens per second) total time = 11441.30 ms / 330 tokens slot print_timing: id 0 | task 1 | draft acceptance rate = 0.73494 ( 122 accepted / 166 generated) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NickNau"&gt; /u/NickNau &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obmhh1/speculative_decoding_for_oncpu_moe/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obmhh1/speculative_decoding_for_oncpu_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obmhh1/speculative_decoding_for_oncpu_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1obfwt9</id>
    <title>Practical takeaways from recent hands-on use of PaddleOCR‑VL 0.9B</title>
    <updated>2025-10-20T10:50:26+00:00</updated>
    <author>
      <name>/u/contportvas</name>
      <uri>https://old.reddit.com/user/contportvas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bottom line up front: I care most about whether complex layouts can be restored into structured data, whether handwriting tables and formulas are stable, and local inference speed and cost. Paddleocr‑VL 0.9B feels purpose built for production, especially for multi column PDFs, table structures, and formulas. Cloud models like GPT‑4o and Gemini 2.5 Pro are more general for commonsense cross domain understanding and conversational interaction, but you need to factor in cost and privacy compliance.&lt;/p&gt; &lt;p&gt;Scope and Constraints&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Task domain: Document parsing and OCR, including text, tables, formulas, handwriting, and chart annotations.&lt;/li&gt; &lt;li&gt;Versions and sources: PaddleOCR‑VL 0.9B based on public materials and official demos. Baselines include GPT‑4o, Gemini 2.5 Pro, Mineru2.5, and dots.ocr using public information.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;On multi column complex layouts and whether they can be directly restored into structured data, which I value highly because it decides how much human cleanup downstream automation needs. Paddleocr‑VL takes an engineering first approach: a NaViT dynamic visual encoder plus a lightweight ERNIE, combining layout understanding with structured outputs. In my experience with academic PDFs and financial reports that mix multi columns, formulas, and footnotes, it less often produces results that look correct but have broken structure. If your core goal is structured outputs that minimize rework, the default path of Paddleocr‑VL is steadier. General VLMs can understand the content, but often need extra prompt engineering or postprocessing to guarantee structure.&lt;/p&gt; &lt;p&gt;Handwriting, tables, and formulas: which is steadier? I would not claim any model absolutely dominates, but considering both recognition accuracy and structural usability together, PaddleOCR‑VL feels more production ready. It emphasizes strong performance on printed Chinese and English, handwritten English, and even Chinese handwriting and pinyin. Tables and formulas are traditional strengths of OCR systems, and emitting Markdown, html, or latex can save a lot of time. Cloud models are strong at formula inference and cross page linkage, but they sometimes output plausible looking yet misgridded or misaligned structures, which requires an extra verification pass.&lt;/p&gt; &lt;p&gt;Multilingual support is a classic ocr topic. This generation of Paddleocr‑VL highlights coverage of 109 languages and continues the pp‑ocr family’s lightweight design without sacrificing multilingual capability. Traditional ocr recognition modules can even be kept within hundreds of megabytes. My hunch is that common European languages plus Chinese Japanese Korean pose no pressure, while long tail scripts and rare character sets depend on your data distribution, so it is best to pilot with a small batch first.&lt;/p&gt; &lt;p&gt;I'm not an expert either; I'm just sharing as a newbie with everyone:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If your goal is to extract multi column PDFs, reports, and papers into structured data in as close to one pass as possible, and you need to run extensively on an enterprise intranet or at the edge, prioritize Paddleocr‑VL.&lt;/li&gt; &lt;li&gt;If you need to chat with documents, do cross domain summarization reasoning rewriting, and the volume is small with no hard privacy constraints, use GPT‑4o or Gemini 2.5 pro, then add some postprocessing for structure.&lt;/li&gt; &lt;li&gt;If you already have Mineru2.5 or dots.ocr pipelines and costs are under control, there is no need to churn if production is good enough. If you must tackle complex layouts with structured export, run another head‑to‑head focusing on rework volume.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Reference links&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;https://huggingface.co/PaddlePaddle/PaddleOCR-VL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aistudio.baidu.com/paddleocr"&gt;https://aistudio.baidu.com/paddleocr&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contportvas"&gt; /u/contportvas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfwt9/practical_takeaways_from_recent_handson_use_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfwt9/practical_takeaways_from_recent_handson_use_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obfwt9/practical_takeaways_from_recent_handson_use_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T10:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1obgci1</id>
    <title>Is Meta done with open-source Llama releases?</title>
    <updated>2025-10-20T11:19:17+00:00</updated>
    <author>
      <name>/u/emimix</name>
      <uri>https://old.reddit.com/user/emimix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was cleaning up my local LM stacks and noticed all the old Llama models I had. Brought back memories of how much fun they were — made me wonder, is Meta done releasing open-source models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emimix"&gt; /u/emimix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T11:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;/p&gt; &lt;p&gt;1: &lt;a href="https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV"&gt;https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV&lt;/a&gt;&lt;br /&gt; 2: &lt;a href="https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi"&gt;https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi&lt;/a&gt;&lt;br /&gt; 3: &lt;a href="https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1"&gt;https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1obb4c4</id>
    <title>What are your /r/LocalLLaMA "hot-takes"?</title>
    <updated>2025-10-20T04:55:04+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or something that goes against the general opinions of the community? Vibes are the only benchmark that counts after all.&lt;/p&gt; &lt;p&gt;I tend to agree with the flow on most things &lt;em&gt;but&lt;/em&gt; my thoughts that I'd consider going against the grain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;QwQ was think-slop and was never &lt;em&gt;that&lt;/em&gt; good&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3-32B is still SOTA for 32GB and under. I cannot get anything to reliably beat it despite shiny benchmarks&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Deepseek is still open-weight SotA. I've really tried Kimi, GLM, and Qwen3's larger variants but asking Deepseek still feels like asking the adult in the room. Caveat is GLM codes better&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;(proprietary bonus): Grok4 handles news data better than Chatgpt5 or Gemini2.5 and will always win if you ask it about something that happened &lt;em&gt;that day&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T04:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1obftw9</id>
    <title>DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models</title>
    <updated>2025-10-20T10:33:42+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt; &lt;img alt="DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models" src="https://a.thumbs.redditmedia.com/07jxlZQFGtUtHiMuztBNSU_MiE3T0do53uVR780HIi0.jpg" title="DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bw20aruc58wf1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80ef4d2d3b6b194d08a290d37a68cd1f5bd072bb"&gt;https://preview.redd.it/bw20aruc58wf1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80ef4d2d3b6b194d08a290d37a68cd1f5bd072bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guillermo Rauch (&lt;strong&gt;Vercel CEO&lt;/strong&gt;) just shared benchmark results from their internal agent testing. That’s roughly &lt;strong&gt;5× faster&lt;/strong&gt; and &lt;strong&gt;50% higher accuracy&lt;/strong&gt; than the top proprietary models&lt;/p&gt; &lt;p&gt;It’s wild to see open source models not just catching up but starting to outperform in both efficiency and accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T10:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9vvk</id>
    <title>What happens when Chinese companies stop providing open source models?</title>
    <updated>2025-10-20T03:51:03+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What happens when Chinese companies stop providing open source models? Good example would be Alibaba's WAN. It was open source until the last version WAN2.5, which is closed source and it costs money. What happens when they start doing this across the board? Edit: Qwen Max is another example &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T03:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1obcm9r</id>
    <title>DeepSeek releases DeepSeek OCR</title>
    <updated>2025-10-20T06:26:26+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt; &lt;img alt="DeepSeek releases DeepSeek OCR" src="https://external-preview.redd.it/ddlXXAanndfx0k3ivMcCdrEJtDQlMZs1JyMP8q81Yms.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54c207b8079de2f72cbaafba0d28b87918c60e33" title="DeepSeek releases DeepSeek OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;https://huggingface.co/deepseek-ai/DeepSeek-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db"&gt;https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T06:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
