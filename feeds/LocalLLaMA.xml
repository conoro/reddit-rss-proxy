<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-19T08:25:02+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oa8tq5</id>
    <title>Which price point to train and run local VLA models ?</title>
    <updated>2025-10-18T22:05:08+00:00</updated>
    <author>
      <name>/u/rodrigo-benenson</name>
      <uri>https://old.reddit.com/user/rodrigo-benenson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to understand which computer I should get if my goal is to explore modern AI techniques \ (specifically fine-tuning and inference of VLA models, Vision+Language+Action)&lt;/p&gt; &lt;p&gt;Even if we assume money was not an issue it remains not clear to me what is a “good choice”. \ For example “100k USD for a computer” would be ridiculous even if one could pay for it; \ the opportunity cost becomes huge, one could do “much better” with 100k than buy a computer. \ It is unclear if I should think of spending 500, 1k, 5k, 10k, or 30k USD, there seems to be an argument for each price-level.&lt;/p&gt; &lt;p&gt;To my current understanding (guesstimated prices, Gb indicate “AI Model RAM”): * 30k+ USD for something like a top of line custom pc with a H100 80Gb inside. * 10k USD for a maxed-up Mac M3 Ultra 512Gb. * 8k USD for a 2xNVIDIA DGX Spark 256Gb interconnected. * 7k USD for a 2xNVIDIA 5090 64Gb machine. * 6k USD for a 2xNVIDIA 4090 48Gb machine. * 4k USD for a NVIDIA DGX Spark 128Gb. * 3k USD for a maxed out AMD Ryzen AI Max+ 395 128Gb Framework PC. * 3k USD for a M5 Macbook Pro 24Gb. * 2k USD for a Beelink GTR9 Pro AMD Ryzen™ AI Max+ 395 128Gb. * 500 USD for a Chromebook Plus and then rent the GPUs by the hour, with a budget of about 100 USD per month (with a service like &lt;a href="https://vast.ai"&gt;https://vast.ai&lt;/a&gt; ) that would allow plenty of time to work with e.g. 4090 GPUs. &lt;/p&gt; &lt;p&gt;I can see arguments pro- and con- each of these options and I am left unclear what will end up being a good bang for bucks. \ Some of these prices start to be quite crazy (comparable to amazing vacation travels, brand new car, multiple years of GPU renting, a year of weekly dinners at Michelin restaurants, etc.) \ I think I am missing some technical dimension that I am currently blind to (e.g. optimize memory bandwidth?). &lt;/p&gt; &lt;p&gt;For my use case \ I do not care about gaming, \ I do not care about the looks, \ I do not care much about the size (albeit smaller is better), \ I care a bit about the noise (the less the better), \ I care about having a powerful CPU (for scientific computing, but at those prices that seems a given), \ and Linux variant as main OS is my preference. &lt;/p&gt; &lt;p&gt;Thanks a lot for your comments and guidance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rodrigo-benenson"&gt; /u/rodrigo-benenson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8tq5/which_price_point_to_train_and_run_local_vla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8tq5/which_price_point_to_train_and_run_local_vla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8tq5/which_price_point_to_train_and_run_local_vla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:05:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9uw49</id>
    <title>Stress Testing Embedding Models with adversarial examples</title>
    <updated>2025-10-18T12:47:58+00:00</updated>
    <author>
      <name>/u/GullibleEngineer4</name>
      <uri>https://old.reddit.com/user/GullibleEngineer4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After hitting performance walls on several RAG projects, I'm starting to think the real problem isn't our retrieval logic. It's the embedding models themselves. My theory is that even the top models are still way too focused on keyword matching and actually don't capture sentence level semantic similarity.&lt;/p&gt; &lt;p&gt;Here's a test I've been running. Which sentence is closer to the Anchor?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Anchor:&lt;/strong&gt; &amp;quot;A background service listens to a task queue and processes incoming data payloads using a custom rules engine before persisting output to a local SQLite database.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option A (Lexical Match):&lt;/strong&gt; &amp;quot;A background service listens to a message queue and processes outgoing authentication tokens using a custom hash function before transmitting output to a local SQLite database.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Option B (Semantic Match):&lt;/strong&gt; &amp;quot;An asynchronous worker fetches jobs from a scheduling channel, transforms each record according to a user-defined logic system, and saves the results to an embedded relational data store on disk.&amp;quot;&lt;/p&gt; &lt;p&gt;If you ask an LLM like Gemini 2.5 Pro, it correctly identifies that the Anchor and Option B are describing the same core concept - just with different words.&lt;/p&gt; &lt;p&gt;But when I tested this with gemini-embedding-001 (currently #1 on MTEB), it consistently scores Option A as more similar. It gets completely fooled by surface-level vocabulary overlap.&lt;/p&gt; &lt;p&gt;I put together a small GitHub project that uses ChatGPT to generate and test these &amp;quot;semantic triplets&amp;quot;: &lt;a href="https://github.com/semvec/embedstresstest"&gt;https://github.com/semvec/embedstresstest&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The README walks through the whole methodology if anyone wants to dig in.&lt;/p&gt; &lt;p&gt;Has anyone else noticed this? Where embeddings latch onto surface-level patterns instead of understanding what a sentence is actually &lt;em&gt;about&lt;/em&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GullibleEngineer4"&gt; /u/GullibleEngineer4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9uw49/stress_testing_embedding_models_with_adversarial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9uw49/stress_testing_embedding_models_with_adversarial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9uw49/stress_testing_embedding_models_with_adversarial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T12:47:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9q01c</id>
    <title>Medical model: Bio-Medical-ContactDoctorVLLM</title>
    <updated>2025-10-18T08:02:08+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Bio-Medical-ContactDoctorVLLM-14B-V1-102025 is a specialized vision-language model designed for comprehensive biomedical image analysis.&lt;/p&gt; &lt;p&gt;Built on a novel architecture combining Qwen3-14B language model with Google's MedSigLIP-448 vision encoder, this model excels at analyzing diverse medical imaging modalities including X-rays, CT scans, MRI, ultrasound, histopathology, and clinical photography.&amp;quot;&lt;/p&gt; &lt;p&gt;Couldn't find any benchmark, I wonder how does it compare to medgemma...&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/ContactDoctor/Bio-Medical-ContactDoctorVLLM-14B-V1-102025"&gt;https://huggingface.co/ContactDoctor/Bio-Medical-ContactDoctorVLLM-14B-V1-102025&lt;/a&gt; (8B also available)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9q01c/medical_model_biomedicalcontactdoctorvllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9q01c/medical_model_biomedicalcontactdoctorvllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9q01c/medical_model_biomedicalcontactdoctorvllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T08:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa47s0</id>
    <title>Would it be theoretically possible to create a two-way speculative decoder to infer the user's next token while they're typing and generate the LLM's draft tokens in real-time before the user finishes then finalize the response once sent?</title>
    <updated>2025-10-18T19:02:46+00:00</updated>
    <author>
      <name>/u/swagonflyyyy</name>
      <uri>https://old.reddit.com/user/swagonflyyyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking about voice applications with AI and the latency issues that lead to noticeable delays in responses and I just got this crazy idea about using speculative decoding to hypothetically tackle this problem. &lt;/p&gt; &lt;p&gt;Here's what we know so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Speculative decoding on the agent side works, but YMMV based on the draft model.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;AI-powered user auto-complete generally works in short bursts.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;There are some prototypes available to test this hypothesis.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2408.02622?"&gt;Paper 1&lt;/a&gt; &lt;a href="https://aclanthology.org/2024.emnlp-main.1192.pdf"&gt;Paper 2&lt;/a&gt; &lt;a href="https://aclanthology.org/P19-1289/?"&gt;Paper 3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But I've never seen the two of them together and I suspect it would require either a complex framework or perhaps a radically different architecture altogether (maybe both?). &lt;/p&gt; &lt;p&gt;The primary aim here is to minimize &lt;code&gt;user voice input -&amp;gt; assistant voice response&lt;/code&gt; latency by having the assistant generate a draft response not after, but &lt;em&gt;during&lt;/em&gt; the user's message in progress and also generate drafts of possible next tokens a user might type based on the chat history so far. &lt;/p&gt; &lt;p&gt;Both draft tokens would be generated side-by-side in the following sequence: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;User draft tokens are generated first up until a pre-defined point.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Agent draft tokens are generated based on the user draft tokens up until a pre-defined point. &lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Assuming it works, there could be variations, like dynamic adjustment of different draft token sampling parameters and draft token response length based on the proximity of the draft tokens to the actual tokens on both sides generated. I think its a longshot but the end result is a seamless conversation between a user and the agent where the only bottleneck would be the TTS model in question. &lt;/p&gt; &lt;p&gt;On the TTS side of things, it has been proven recently that latency can be &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mzm677/ursxlv_appreciation_post_for_releasing_his/"&gt;virtually eliminated&lt;/a&gt; with the right optimizations, model and hardware, so even that wouldn't be as much of an issue. This would lead to faster responses with smaller models and less hardware.&lt;/p&gt; &lt;p&gt;But I also think it would be tricky to implement, because modern LLMs usually wait for the user message before responding and once they respond they won't stop until they make their point across, but this approach would require the model to stop at a certain point in real-time then continue in real-time by picking up where it left off. &lt;/p&gt; &lt;p&gt;I don't think that's something you can fine-tune in a model, but I am not sure if that requires a foundational model, a radically different architecture, or clever tricks.&lt;/p&gt; &lt;p&gt;EDIT: The more I think about it, the more I think it would be important to establish sampling parameters around the relationship between both draft tokens, not just &lt;code&gt;draft tokens -&amp;gt; user token.&lt;/code&gt; but also &lt;code&gt;draft agent -&amp;gt; draft user&lt;/code&gt; tokens Details in the comments.&lt;/p&gt; &lt;p&gt;Still, if anyone takes it seriously enough to implement and it actually takes off I could see new sampling parameters opening up that tweak this relationship between &lt;code&gt;draft agent -&amp;gt; draft user&lt;/code&gt;, i.e. how &lt;code&gt;draft agent&lt;/code&gt; tokens follows &lt;code&gt;draft user's&lt;/code&gt; tokens' lead and how the draft model tweaks this response accordingly. &lt;/p&gt; &lt;p&gt;&lt;code&gt;draft agent -&amp;gt; token user&lt;/code&gt; is already handled by current supported backends but auto-complete-type decoders don't have much support. Yet, they could have support easily implemented if they wanted to so that's not a problem. &lt;/p&gt; &lt;p&gt;I could see a case for the drafting model assigned to the user (should be the same as the agent drafting model) penalizing incorrect user token drafts generated to tweak the probability of them appearing. &lt;/p&gt; &lt;p&gt;Hopefully they get better draft predictions next time which in turn improve the model's accuracy and increase the chances of surpassing the &lt;code&gt;confidence threshold&lt;/code&gt; I brought up &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oa47s0/comment/nk70d2t/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;here&lt;/a&gt;, which should theoretically get us closer to real-time responses.&lt;/p&gt; &lt;p&gt;Now what's all this about hypothesized sampling parameters between both draft model categories? I'm thinking about options, something along the lines of this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;draft_penalty&lt;/code&gt; - The penalty for an incorrect user draft token generated, per token, scalar. Discourages that token from being selected in the future.&lt;/li&gt; &lt;li&gt;&lt;code&gt;confidence_penalty&lt;/code&gt; - The confidence score penalty applied, per draft user token generated, when incorrect user draft tokens are generated.&lt;/li&gt; &lt;li&gt;&lt;code&gt;confidence_reward&lt;/code&gt; - The confidence score reward applied, per draft user token generated, when the correct user draft tokens are generated.&lt;/li&gt; &lt;li&gt;&lt;code&gt;confidence_threshold&lt;/code&gt; - threshold to meet before finalizing drafts generated by the agent draft and start generating tokens/TTS mid-message. Set to 0 for dynamic.&lt;/li&gt; &lt;li&gt;&lt;code&gt;max_draft_tokens_assistant&lt;/code&gt; - Max draft tokens to generate for the agent. Set to 0 for dynamic.&lt;/li&gt; &lt;li&gt;&lt;code&gt;max_draft_tokens_user&lt;/code&gt; - Max draft tokens to generate for the agent. Set to 0 for dynamic.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And so forth. A lot of it would be borrowed from regular sampling parameters because they seem to be a perfect fit for the draft models, but I'm willing to bet new ones will emerge as well to manually tweak any dials as needed.&lt;/p&gt; &lt;p&gt;The solution may be to resolve the latency issue in voice-to-voice interactions, but they're still LLMs at the end of the day, and it has been proven that draft models could work very well. Maybe this could indirectly speed up LLMs or other models in some way? It'd be pretty interesting to explore that some day.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swagonflyyyy"&gt; /u/swagonflyyyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa47s0/would_it_be_theoretically_possible_to_create_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa47s0/would_it_be_theoretically_possible_to_create_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa47s0/would_it_be_theoretically_possible_to_create_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T19:02:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9txdt</id>
    <title>Is anyone else using Home-Cook-Mistral-Small-Omni? This is an hidden gem!</title>
    <updated>2025-10-18T12:02:15+00:00</updated>
    <author>
      <name>/u/no_no_no_oh_yes</name>
      <uri>https://old.reddit.com/user/no_no_no_oh_yes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gguf: &lt;a href="https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF"&gt;https://huggingface.co/ngxson/Home-Cook-Mistral-Small-Omni-24B-2507-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is supported on latest Llama.cpp.&lt;/p&gt; &lt;p&gt;For technical stuff, tables, charts, transcriptions (somehow it is identifying multiple speakers too), changed my workflow from multi-model to single model. &lt;/p&gt; &lt;p&gt;My question for Reddit (and I did it also in the HF) is my experience with Q4 seems to miss details here and there, subtle stuff. But Q6 and Q8 do the job perfectly. Should a Q6 be that much better especially with Voice and Image in the mix? &lt;/p&gt; &lt;p&gt;Thanks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/no_no_no_oh_yes"&gt; /u/no_no_no_oh_yes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9txdt/is_anyone_else_using_homecookmistralsmallomni/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9txdt/is_anyone_else_using_homecookmistralsmallomni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9txdt/is_anyone_else_using_homecookmistralsmallomni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T12:02:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahyzi</id>
    <title>Unable to find the attach feature in Jan.ai for documents and images.</title>
    <updated>2025-10-19T05:54:27+00:00</updated>
    <author>
      <name>/u/Ok-Knee-694</name>
      <uri>https://old.reddit.com/user/Ok-Knee-694</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I came across this &lt;a href="http://Jan.ai"&gt;Jan.ai&lt;/a&gt; software for desktop for its privacy-first feature. I decided to use Mistral-7B-Instruct-v0.3 LLM model for document analysis, but later came to realize that this software doesn't have a document attachment option at all. Are there any other ways to make the model read my document?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Knee-694"&gt; /u/Ok-Knee-694 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahyzi/unable_to_find_the_attach_feature_in_janai_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahyzi/unable_to_find_the_attach_feature_in_janai_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahyzi/unable_to_find_the_attach_feature_in_janai_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:54:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8j68</id>
    <title>How to use openai harmony chat template with openai client library and openrouter gpt-oss?</title>
    <updated>2025-10-18T21:52:54+00:00</updated>
    <author>
      <name>/u/elbiot</name>
      <uri>https://old.reddit.com/user/elbiot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't figure out how to use the &lt;code&gt;openai_harmony&lt;/code&gt; package with the &lt;code&gt;openai.OpenAI.client&lt;/code&gt;. Seems like these two should work together easily. What am I missing? Especially, how do I get multiple tool calls from one response?&lt;/p&gt; &lt;p&gt;```&lt;br /&gt; from openai_harmony import ( load_harmony_encoding, HarmonyEncodingName, Role, Message, Conversation, SystemContent, DeveloperContent, ReasoningEffort, )&lt;/p&gt; &lt;p&gt;from openai import OpenAI import os from dotenv import load_dotenv&lt;/p&gt; &lt;h1&gt;Load environment variables&lt;/h1&gt; &lt;p&gt;load_dotenv()&lt;/p&gt; &lt;h1&gt;Initialize Harmony encoding&lt;/h1&gt; &lt;p&gt;enc = load_harmony_encoding(HarmonyEncodingName.HARMONY_GPT_OSS)&lt;/p&gt; &lt;h1&gt;Create conversation&lt;/h1&gt; &lt;p&gt;system_message = SystemContent.new().with_reasoning_effort(ReasoningEffort.HIGH) developer_message = DeveloperContent.new().with_instructions(&amp;quot;Respond in riddles&amp;quot;)&lt;/p&gt; &lt;p&gt;convo = Conversation.from_messages([ Message.from_role_and_content(Role.SYSTEM, system_message), Message.from_role_and_content(Role.DEVELOPER, developer_message), Message.from_role_and_content(Role.USER, &amp;quot;Explain photosynthesis.&amp;quot;), ])&lt;/p&gt; &lt;h1&gt;Render conversation to tokens&lt;/h1&gt; &lt;p&gt;tokens = enc.render_conversation_for_completion(convo, Role.ASSISTANT)&lt;/p&gt; &lt;h1&gt;Initialize OpenAI client for OpenRouter&lt;/h1&gt; &lt;p&gt;openrouter_api_key = os.getenv(&amp;quot;OPENROUTER_API_KEY&amp;quot;)&lt;/p&gt; &lt;p&gt;client = OpenAI( api_key=openrouter_api_key, base_url=&amp;quot;&lt;a href="https://openrouter.ai/api/v1"&gt;https://openrouter.ai/api/v1&lt;/a&gt;&amp;quot;, )&lt;/p&gt; &lt;h1&gt;Make API call - using completions endpoint with the decoded string&lt;/h1&gt; &lt;p&gt;response = client.chat.create( model=&amp;quot;gpt-oss-120b&amp;quot;, prompt=WHAT_GOES_HERE, max_tokens=2048, temperature=0.7, )&lt;/p&gt; &lt;p&gt;def parse_response(resp): WHAT_GOES_HERE&lt;/p&gt; &lt;p&gt;final, analysis, commentary = parse_response(response.choices[0]) ```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/elbiot"&gt; /u/elbiot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8j68/how_to_use_openai_harmony_chat_template_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8j68/how_to_use_openai_harmony_chat_template_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8j68/how_to_use_openai_harmony_chat_template_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:52:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa1gz9</id>
    <title>An MCP to improve your coding agent with better memory using code indexing and accurate semantic search</title>
    <updated>2025-10-18T17:16:23+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A while back, I stumbled upon a comment from &lt;a href="/u/abdul_1998_17"&gt;u/abdul_1998_17&lt;/a&gt; about a tool called PAMPA (&lt;a href="https://www.reddit.com/r/mcp/comments/1mfsver/comment/n6klbj9/"&gt;link to comment&lt;/a&gt;). It's an &amp;quot;augmented memory&amp;quot; MCP server that indexes your codebase with embeddings and a reranker for accurate semantic search. I'd been looking for something exactly like this to give my coding agent better context without stuffing the entire codebase into the prompt for a while now. Roo Code (amazing coding agent btw) gets halfway there, it has code indexing, but no reranker support.&lt;/p&gt; &lt;p&gt;This tool is basically a free upgrade for any coding agent. It lets your agent or yourself search the codebase using natural language. You can ask things like, &amp;quot;how do we handle API validation?&amp;quot; and find conceptually similar code, even if the function names are completely different. This is even useful for stuff like searching error messages, etc. The agent makes a quick query, gets back the most relevant snippets for its context, and doesn't need to digest the entire repo. This should reduce token usage (which gets fairly damn expensive quick) and the context your model gets will be way more accurate (this being my main motivation to want this tool).&lt;/p&gt; &lt;p&gt;The original tool is great, but I ran into a couple of things I wanted to change for my own workflow. The API providers were hardcoded, and I wanted to be able to use it with any OpenAI-compatible server (like OpenRouter or locally with something like a llama.cpp server).&lt;/p&gt; &lt;p&gt;So, I ended up forking it. I started with small personal tweaks, but I had more stuff I wanted and kept going. Here are a few things I added/fixed in my fork, pampax (yeah I know how the name sounds but I was just building this for myself at the time and thought the name was funny):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Universal OpenAI Compatible API Support:&lt;/strong&gt; You can now point it at any OpenAI-compatible endpoint. Now you dont need to go into the code to switch to an unsupported provider.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Added API-based Rerankers:&lt;/strong&gt; PAMPA's local &lt;code&gt;transformers.js&lt;/code&gt; reranker is pretty neat, if all you want is a small local reranker, but that's all it supported. I wanted to test a more powerful model. I implemented support for using API-based rerankers (which allows the use of other local models or any api provider of choice).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fixed Large File Indexing:&lt;/strong&gt; I noticed I was getting tree-sitter errors in use, for invalid arguments. Turns out the original implementation didn't support files larger than 30kb. Tree-sitter's official callback-based streaming API for large files was implemented to fix this, and also improves performance. Now any file sizes should be supported.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The most surprising part was the benchmark, which tests against a Laravel + TS corpus.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;Qwen3-Embedding-8B&lt;/code&gt; + the local &lt;code&gt;transformers.js&lt;/code&gt; reranker scored very well, better than without reranker, and other top embedding models; around 75% accuracy in &lt;a href="mailto:precision@1"&gt;precision@1&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;Qwen3-Embedding-8B&lt;/code&gt; + &lt;code&gt;Qwen3-Reranker-8B&lt;/code&gt; (using the new API support) hit &lt;strong&gt;100% accuracy&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I honestly didn't expect the reranker to make &lt;em&gt;that&lt;/em&gt; big of a difference. This is a big difference in search accuracy, and relevancy.&lt;/p&gt; &lt;p&gt;Installation is pretty simple, like any other npx mcp server configuration. Instructions and other information can be found on the github: &lt;a href="https://github.com/lemon07r/pampax?tab=readme-ov-file#pampax--protocol-for-augmented-memory-of-project-artifacts-extended"&gt;https://github.com/lemon07r/pampax?tab=readme-ov-file#pampax--protocol-for-augmented-memory-of-project-artifacts-extended&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If there are any other issues or bugs found I will try to fix them. I tried to squash all the bugs I found already while I was using the tool for other projects, and hopefully got most of them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1gz9/an_mcp_to_improve_your_coding_agent_with_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1gz9/an_mcp_to_improve_your_coding_agent_with_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1gz9/an_mcp_to_improve_your_coding_agent_with_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:16:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9x4ky</id>
    <title>Qwen3VL-30b-a3b Image Caption Performance - Thinking vs Instruct (FP8) using vLLM and 2x RTX 5090</title>
    <updated>2025-10-18T14:22:05+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"&gt; &lt;img alt="Qwen3VL-30b-a3b Image Caption Performance - Thinking vs Instruct (FP8) using vLLM and 2x RTX 5090" src="https://a.thumbs.redditmedia.com/2NLvlnTIwrK_v4qLdjPp5FuWDH46C_f7lvMjl0LblG4.jpg" title="Qwen3VL-30b-a3b Image Caption Performance - Thinking vs Instruct (FP8) using vLLM and 2x RTX 5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here to report some performance numbers, hope someone can comment whether that looks in-line.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2x RTX 5090 (450W, PCIe 4 x16)&lt;/li&gt; &lt;li&gt;Threadripper 5965WX&lt;/li&gt; &lt;li&gt;512GB RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There may be a little bit of headroom for --max-model-len&lt;/p&gt; &lt;pre&gt;&lt;code&gt;vllm serve Qwen/Qwen3-VL-30B-A3B-Thinking-FP8 --async-scheduling --tensor-parallel-size 2 --mm-encoder-tp-mode data --limit-mm-per- prompt.video 0 --max-model-len 128000 vllm serve Qwen/Qwen3-VL-30B-A3B-Instruct-FP8 --async-scheduling --tensor-parallel-size 2 --mm-encoder-tp-mode data --limit-mm-per- prompt.video 0 --max-model-len 128000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Payload&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;512 Images (max concurrent 256)&lt;/li&gt; &lt;li&gt;1024x1024&lt;/li&gt; &lt;li&gt;Prompt: &amp;quot;Write a very long and detailed description. Do not mention the style.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zswllkf5pvvf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=79edc002bcc13ae1e6177909ab9667dffb142aa5"&gt;Sample Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Results&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Instruct Model Total time: 162.61s Throughput: 188.9 images/minute Average time per request: 55.18s Fastest request: 23.27s Slowest request: 156.14s Total tokens processed: 805,031 Average prompt tokens: 1048.0 Average completion tokens: 524.3 Token throughput: 4950.6 tokens/second Tokens per minute: 297033 Thinking Model Total time: 473.49s Throughput: 64.9 images/minute Average time per request: 179.79s Fastest request: 57.75s Slowest request: 321.32s Total tokens processed: 1,497,862 Average prompt tokens: 1051.0 Average completion tokens: 1874.5 Token throughput: 3163.4 tokens/second Tokens per minute: 189807 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;The Thinking Model typically has around 65 - 75 requests active and the Instruct Model around 100 - 120.&lt;/li&gt; &lt;li&gt;Peak PP is over 10k t/s&lt;/li&gt; &lt;li&gt;Peak generation is over 2.5k t/s&lt;/li&gt; &lt;li&gt;Non-Thinking Model is about 3x faster (189 images per minute) on this task than the Thinking Model (65 images per minute).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Do these numbers look fine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9x4ky/qwen3vl30ba3b_image_caption_performance_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaj0gb</id>
    <title>GPU rental experiences</title>
    <updated>2025-10-19T06:58:50+00:00</updated>
    <author>
      <name>/u/somealusta</name>
      <uri>https://old.reddit.com/user/somealusta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have some spare GPUs and servers, some at home and some at datacenter.&lt;br /&gt; I would like to know peoples experiences in general about renting your own GPUs or just using these services for inference. How do they work and are people actually using them. &lt;/p&gt; &lt;p&gt;So I am speaking about &lt;a href="http://vast.ai"&gt;vast.ai&lt;/a&gt; or similar (which other there are?) where you can rent your own or use someone elses hardware. Do you use them and if yes how much you use them and for what?&lt;br /&gt; Have they been working flawlessly or do you prefer something else? &lt;/p&gt; &lt;p&gt;For me, earning about 1,2 dollars per server with 5090 does not sound much, but if they are just sitting here under my desk, maybe I should put them to work? Electricity here is sometimes very cheap, so something should be left. What other services there are than vast.ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/somealusta"&gt; /u/somealusta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaj0gb/gpu_rental_experiences/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaj0gb/gpu_rental_experiences/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaj0gb/gpu_rental_experiences/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T06:58:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xf4q</id>
    <title>[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results</title>
    <updated>2025-10-18T14:33:56+00:00</updated>
    <author>
      <name>/u/Unbreakable_ryan</name>
      <uri>https://old.reddit.com/user/Unbreakable_ryan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"&gt; &lt;img alt="[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results" src="https://external-preview.redd.it/MWdpMThpMmdzdnZmMSlMdchWVQEhhJbqcRpTSv2Il4U_vTm8zPYXwK-Tk6g_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63ce1d6d2b0cd265b89c2d1f2c44c8a926ac5d47" title="[Experiment] Qwen3-VL-8B VS Qwen2.5-VL-7B test results" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt;&lt;br /&gt; I tested the brand-new &lt;strong&gt;Qwen3-VL-8B&lt;/strong&gt; against &lt;strong&gt;Qwen2.5-VL-7B&lt;/strong&gt; on the same set of visual reasoning tasks — OCR, chart analysis, multimodal QA, and instruction following.&lt;br /&gt; Despite being only 1B parameters larger, Qwen3-VL shows a &lt;strong&gt;&lt;em&gt;clear generation-to-generation leap&lt;/em&gt;&lt;/strong&gt; and delivers more accurate, nuanced, and faster multimodal reasoning.&lt;/p&gt; &lt;h1&gt;1. Setup&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Environment:&lt;/strong&gt; Local inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Mac Air M4, 8-core GPU, 24 GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model format:&lt;/strong&gt; gguf, Q4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tasks tested:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Visual perception (receipts, invoice)&lt;/li&gt; &lt;li&gt;Visual captioning (photos)&lt;/li&gt; &lt;li&gt;Visual reasoning (business data)&lt;/li&gt; &lt;li&gt;Multimodal Fusion (does paragraph match figure)&lt;/li&gt; &lt;li&gt;Instruction following (structured answers)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each prompt + image pair was fed to both models, using identical context.&lt;/p&gt; &lt;h1&gt;2. Evaluation Criteria&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Visual Perception&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Correctly identifies text, objects, and layout.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: This reflects the model’s baseline visual IQ.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual Captioning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Generates natural language descriptions of images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Bridges vision and language, showing the model can translate what it sees into coherent text.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Reads chart trends and applies numerical logic.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Tests true multimodal reasoning ability, beyond surface-level recognition.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Multimodal Fusion&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Connects image content with text context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Demonstrates cross-attention strength—how well the model integrates multiple modalities.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: Obeys structured prompts, such as “answer in 3 bullets.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Reflects alignment quality and the ability to produce controllable outputs.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Metric&lt;/strong&gt;: TTFT (time to first token) and decoding speed.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Why It Matters&lt;/strong&gt;: Determines local usability and user experience.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Note: all answers are verified by humans and ChatGPT5.&lt;/p&gt; &lt;h1&gt;3. Test Results Summary&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Perception&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 5&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 8&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Qwen3-VL-8B identify all the elements in the pic but fail the first and final calculation (the answer is 480.96 and 976.94). In comparison, Qwen2.5-VL-7B could not even understand the meaning of all the elements in the pic (there are two tourists) though the calculation is correct.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Captioning&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 6.5&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Qwen3-VL-8B is more accurate, detailed, and has better scene understanding. (for example, identify Christmas Tree and Milkis). In contrary, Qwen2.5-VL-7B Gets the gist, but makes several misidentifications and lacks nuance.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Visual Reasoning&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 8&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: Both models show the basically correct reasoning of the charts and one or two numeric errors. Qwen3-VL-8B is better at analysis/insight which indicates the key shifts while Qwen2.5-VL-7B has a clearer structure.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Multimodal Fusion&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 7&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 9&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: The reasoning of Qwen3-VL-8B is correct, well-supported, and compelling with slight round up for some percentages, while that of Qwen2.5-VL-7B shows Incorrect data reference.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Instruction Following&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: Score 8&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: Score 8.5&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: The summary from Qwen3-VL-8B is more faithful and nuanced, but more wordy. The suammry of Qwen2.5-VL-7B is cleaner and easier to read but misses some details.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Decode Speed&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: 11.7–19.9t/s&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: 15.2–20.3t/s&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: 15–60% faster.&lt;/li&gt; &lt;/ul&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;TTFT&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-VL-7B: 5.9–9.9s&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B: 4.6–7.1s&lt;/li&gt; &lt;li&gt;Winner: Qwen3-VL-8B&lt;/li&gt; &lt;li&gt;Notes: 20–40% faster.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Example Prompts&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual perception:&lt;/strong&gt; “Extract the total amount and payment date from this invoice.”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual captioning&lt;/strong&gt;: &amp;quot;Describe this photo&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual reasoning:&lt;/strong&gt; “From this chart, what’s the trend from 1963 to 1990?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodal Fusion:&lt;/strong&gt; “Does the table in the image support the written claim: Europe is the dominant market for Farmed Caviar?”&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruction following&lt;/strong&gt; “Summarize this poster in exactly 3 bullet points.”&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;5. Summary &amp;amp; Takeaway&lt;/h1&gt; &lt;p&gt;The comparison does not demonstrate just a minor version bump, but a generation leap:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen3-VL-8B consistently outperforms in &lt;strong&gt;Visual reasoning&lt;/strong&gt;, &lt;strong&gt;Multimodal fusion, Instruction following,&lt;/strong&gt; and especially &lt;strong&gt;Visual perception and Visual captioning.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Qwen3-VL-8B produces more &lt;strong&gt;faithful and nuanced answers&lt;/strong&gt;, often giving richer context and insights. (however, conciseness is the tradeoff). Thus, users who value &lt;strong&gt;accuracy and depth&lt;/strong&gt; should prefer Qwen3, while those who want &lt;strong&gt;conciseness with less cognitive load&lt;/strong&gt; might tolerate Qwen2.5.&lt;/li&gt; &lt;li&gt;Qwen3’s mistakes are easier for humans to correct (eg, some numeric errors), whereas Qwen2.5 can mislead due to &lt;strong&gt;deeper misunderstandings&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Qwen3 not only &lt;strong&gt;improves quality but also reduces latency&lt;/strong&gt;, improving user experience.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Unbreakable_ryan"&gt; /u/Unbreakable_ryan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t0mzpl2gsvvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xf4q/experiment_qwen3vl8b_vs_qwen25vl7b_test_results/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:33:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaidyk</id>
    <title>A local LLM that I can feed my diary entries?</title>
    <updated>2025-10-19T06:20:03+00:00</updated>
    <author>
      <name>/u/22Megabits</name>
      <uri>https://old.reddit.com/user/22Megabits</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Would it be possible for me to run an LLM on my PC that I can feed my journal entries to?&lt;/p&gt; &lt;p&gt;My main use would be to ask it for help remembering certain events: ‘Who was my 5th grade maths teacher’ ‘Where did I go on holiday over December in 2013’ etc.&lt;/p&gt; &lt;p&gt;Is that something that’s even possible to locally?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/22Megabits"&gt; /u/22Megabits &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaidyk/a_local_llm_that_i_can_feed_my_diary_entries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaidyk/a_local_llm_that_i_can_feed_my_diary_entries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaidyk/a_local_llm_that_i_can_feed_my_diary_entries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T06:20:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa671p</id>
    <title>Could you recommend good LLM models for heavier stories that include NSFW content?</title>
    <updated>2025-10-18T20:19:23+00:00</updated>
    <author>
      <name>/u/Sr_M_Ghost</name>
      <uri>https://old.reddit.com/user/Sr_M_Ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Deep Seek R2 0528, but I'd like other models that are better suited to this type of content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sr_M_Ghost"&gt; /u/Sr_M_Ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T20:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa1zfp</id>
    <title>3x Price Increase on Llama API</title>
    <updated>2025-10-18T17:36:52+00:00</updated>
    <author>
      <name>/u/Player06</name>
      <uri>https://old.reddit.com/user/Player06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This went pretty under the radar, but a few days ago the 'Meta: Llama 3 70b' model went from 0.13c/M to 0.38c/M.&lt;/p&gt; &lt;p&gt;I noticed because I run one of the apps listed in the top 10 consumers of that model (the one with the weird penguin icon). I cannot find any evidence of this online, except my openrouter bill.&lt;/p&gt; &lt;p&gt;I ditched my local inference last month because the openrouter Llama price looked so good. But now I got rug pulled.&lt;/p&gt; &lt;p&gt;Did anybody else notice this? Or am I crazy and the prices never changed? It feels unusual for a provider to bump their API prices this much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Player06"&gt; /u/Player06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9s60k</id>
    <title>Bee-8B, "fully open 8B Multimodal LLM designed to close the performance gap with proprietary models"</title>
    <updated>2025-10-18T10:21:03+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt; &lt;img alt="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" src="https://external-preview.redd.it/Ncd1u3KqHI3q8cl_bGxSrWsYruQjV7vJ0ceKV7GWN6Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae9ffa18e4d3b03e94f5458c4c02b0fc641812e8" title="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Open-Bee/Bee-8B-RL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T10:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa3u2d</id>
    <title>The size difference of gpt-oss-120b vs it's abliterated version</title>
    <updated>2025-10-18T18:48:10+00:00</updated>
    <author>
      <name>/u/iamkucuk</name>
      <uri>https://old.reddit.com/user/iamkucuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was away from the locally hosted models, so please forgive my ignorance.&lt;/p&gt; &lt;p&gt;Here are two versions of gpt-oss-120b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;br /&gt; &lt;a href="https://ollama.com/huihui_ai/gpt-oss-abliterated"&gt;https://ollama.com/huihui_ai/gpt-oss-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, one takes 88 GB and the other takes 65 GB, and the difference shows when they are loaded as well. I thought they were both 4-bit. Would someone be able to explain where the discrepancy is coming from? And if any abliterated versions of the original model's quant occupy the same space?&lt;/p&gt; &lt;p&gt;Another question would be, I can see the GGUF versions of gpt-oss. Why would we need GGUF versions, as the model itself already is quantized?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamkucuk"&gt; /u/iamkucuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaiykq</id>
    <title>Intel Core Ultra 9 285HX SODIMM slots for up to 256GB of DDR5-4800 ECC memory</title>
    <updated>2025-10-19T06:55:26+00:00</updated>
    <author>
      <name>/u/MundanePercentage674</name>
      <uri>https://old.reddit.com/user/MundanePercentage674</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"&gt; &lt;img alt="Intel Core Ultra 9 285HX SODIMM slots for up to 256GB of DDR5-4800 ECC memory" src="https://external-preview.redd.it/bSme4l8osOBTMgnmz9Z2p_Cb-vjjWGgIuVmAKK2EufE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2c7ae2a5b189631c7c4e36a5c8ace7302c02b2" title="Intel Core Ultra 9 285HX SODIMM slots for up to 256GB of DDR5-4800 ECC memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yldrkkq2o0wf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b63e449fe425c9d20d84fdb755e4f3f9b7dc8f31"&gt;https://preview.redd.it/yldrkkq2o0wf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b63e449fe425c9d20d84fdb755e4f3f9b7dc8f31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://liliputing.com/minisforum-ms-02-ultra-is-a-compact-workstation-with-intel-core-ultra-9-285hx-and-3-pcie-slots/"&gt;https://liliputing.com/minisforum-ms-02-ultra-is-a-compact-workstation-with-intel-core-ultra-9-285hx-and-3-pcie-slots/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MundanePercentage674"&gt; /u/MundanePercentage674 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaiykq/intel_core_ultra_9_285hx_sodimm_slots_for_up_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T06:55:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8klx</id>
    <title>Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models</title>
    <updated>2025-10-18T21:54:34+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt; &lt;img alt="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" src="https://external-preview.redd.it/ZnUwMzR3NXV5eHZmMTNcUS_6HPKChleMDpJ0qQU2p3V675TK2MrxaOnhSll8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=568275d60bed24ec215f8d24a426e506eab3f5d7" title="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jak8lx5uyxvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:54:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaafyq</id>
    <title>3 3090's, room for one more?</title>
    <updated>2025-10-18T23:16:49+00:00</updated>
    <author>
      <name>/u/BusinessBookkeeper63</name>
      <uri>https://old.reddit.com/user/BusinessBookkeeper63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt; &lt;img alt="3 3090's, room for one more?" src="https://preview.redd.it/4wabpxvjcyvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcb88de7be3c5396eac5805691a1be8d3309133" title="3 3090's, room for one more?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently running 3 3090's and was thinking of adding one more. But as you can see, my case Thermaltake CTE750 Air has some free space, but not sure if it can fit another 3090.&lt;/p&gt; &lt;p&gt;I know, I know, I should have had a server rack but I was looking for a Local AI + relatively decent looking case, so this is what I landed on. The CTE 750 is big enough for 3 3090's, but not sure if I should be doing 4 given temps inside a closed case is probably going to rise quick. The third 3090 needs a custom mount and sits on the side of the case in this picture, but it rests on the intake fans and I have screwed the standing with 3 screws. I have no idea, where I could fit the 4th.&lt;/p&gt; &lt;p&gt;Any suggestions on how I could do 4 3090;s in this case or if anyone has done this before?&lt;/p&gt; &lt;p&gt;Also looking for suggestions on my cooling. Currently it has intake from bottom, front, back and sides and outtake on top only. This is somewhat based on the CTE design, but open to other suggestions. Another option, is to eventually do water cooling to save on some space and keep things cooler, but that's a project kept for December.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BusinessBookkeeper63"&gt; /u/BusinessBookkeeper63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4wabpxvjcyvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T23:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa29de</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.2.0</title>
    <updated>2025-10-18T17:47:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt; &lt;img alt="Drummer's Cydonia and Magidonia 24B v4.2.0" src="https://external-preview.redd.it/texRxv_iJ0Ni14pBUMNg-YEbpRERebh0ufaJ753mjSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab199da434abf76101631f6569f8bb80838d47f9" title="Drummer's Cydonia and Magidonia 24B v4.2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magidonia is Cydonia using Magistral 2509 base.&lt;/p&gt; &lt;p&gt;Magidonia variant: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cydonia (Small 3.2) variant: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4.2.0 is an upgrade from 4.1 in regards to creativity. Enjoy!&lt;/p&gt; &lt;p&gt;Does anyone have a base to recommend for finetuning? Waiting for GLM Air 4.6 to come out :^)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;By the way, Huggingface has restricted storage in my account and I'm having a harder time doing my open-source work for the community. I'll be all out of space after a few days of work thanks to their storage restriction. &lt;/p&gt; &lt;p&gt;I tried contacting them via [&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;) but they told me to make my case to [&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;) . I haven't received a response from &lt;em&gt;that&lt;/em&gt; team yet. Other employees I've reached out to recommended that I pay around $200 / mo to get the storage I need, I think.&lt;/p&gt; &lt;p&gt;At this point I believe they're not interested in giving me an exception. I got bundled up with those who upload 1T models, I guess? I'm not sure what to do next, but I might have to start deleting models. Let me know if you guys have any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
