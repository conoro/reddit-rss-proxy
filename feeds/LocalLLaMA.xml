<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-17T04:05:15+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qe5p2f</id>
    <title>Black Forest Labs releases FLUX.2 [klein]</title>
    <updated>2026-01-16T04:01:07+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Black Forest Labs released their new FLUX.2 [klein] model&lt;/p&gt; &lt;p&gt;&lt;a href="https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence"&gt;https://bfl.ai/blog/flux2-klein-towards-interactive-visual-intelligence&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FLUX.2 [klein]: Towards Interactive Visual Intelligence&lt;/p&gt; &lt;p&gt;Today, we release the FLUX.2 [klein] model family, our fastest image models to date. FLUX.2 [klein] unifies generation and editing in a single compact architecture, delivering state-of-the-art quality with end-to-end inference as low as under a second. Built for applications that require real-time image generation without sacrificing quality, and runs on consumer hardware with as little as 13GB VRAM.&lt;/p&gt; &lt;p&gt;The klein name comes from the German word for &amp;quot;small&amp;quot;, reflecting both the compact model size and the minimal latency. But FLUX.2 [klein] is anything but limited. These models deliver exceptional performance in text-to-image generation, image editing and multi-reference generation, typically reserved for much larger models.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;What's New&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Sub-second inference. Generate or edit images in under 0.5s on modern hardware.&lt;/li&gt; &lt;li&gt;Photorealistic outputs and high diversity, especially in the base variants.&lt;/li&gt; &lt;li&gt;Unified generation and editing. Text-to-image, image editing, and multi-reference support in a single model while delivering frontier performance.&lt;/li&gt; &lt;li&gt;Runs on consumer GPUs. The 4B model fits in ~13GB VRAM (RTX 3090/4070 and above).&lt;/li&gt; &lt;li&gt;Developer-friendly &amp;amp; Accessible: Apache 2.0 on 4B models, open weights for 9B models. Full open weights for customization and fine-tuning.&lt;/li&gt; &lt;li&gt;API and open weights. Production-ready API or run locally with full weights.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Try it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://bfl.ai/models/flux-2-klein#try-demo"&gt;Demo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://bfl.ai/play"&gt;Playground&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/spaces/black-forest-labs/FLUX.2-klein-9B"&gt;HF Space for [klein] 9B&lt;/a&gt;, &lt;a href="https://huggingface.co/spaces/black-forest-labs/FLUX.2-klein-4B"&gt;HF Space for [klein] 4B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Build with it&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://docs.bfl.ai/flux_2/flux2_overview#flux-2-%5Bklein%5D-models"&gt;Documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/black-forest-labs/flux2"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/black-forest-labs/flux2"&gt;Model Weights&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Learn more&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://bfl.ai/models/flux-2-klein"&gt;https://bfl.ai/models/flux-2-klein&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe5p2f/black_forest_labs_releases_flux2_klein/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T04:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qetv24</id>
    <title>Local models to try other than Qwen-30b-a3b?</title>
    <updated>2026-01-16T22:10:57+00:00</updated>
    <author>
      <name>/u/exciting_kream</name>
      <uri>https://old.reddit.com/user/exciting_kream</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wondering if anyone has any recommendations for local coding models I should try. I have a Mac with 96gb of ram, and my favorite local model for coding is qwen-30b-a3b-2507 (8 bit quantization, LM Studio). I'm wondering if there are any new models that will have will have better results with similar performance on my machine?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/exciting_kream"&gt; /u/exciting_kream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qetv24/local_models_to_try_other_than_qwen30ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qetv24/local_models_to_try_other_than_qwen30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qetv24/local_models_to_try_other_than_qwen30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefz88</id>
    <title>GLM-Image trained on Huawei chips hits SOTA for text rendering</title>
    <updated>2026-01-16T13:30:02+00:00</updated>
    <author>
      <name>/u/Consistent_Damage824</name>
      <uri>https://old.reddit.com/user/Consistent_Damage824</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefz88/glmimage_trained_on_huawei_chips_hits_sota_for/"&gt; &lt;img alt="GLM-Image trained on Huawei chips hits SOTA for text rendering" src="https://preview.redd.it/m1zahy20ppdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4f474ae62f4add4f72c0edfe3db9bb3ee01d3801" title="GLM-Image trained on Huawei chips hits SOTA for text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;saw people talking about glm-image in a few threads but wanted to look at this from a different angle cause theres something interesting beyond the usual model release stuff&lt;/p&gt; &lt;p&gt;so the architecture is kinda a hybrid autoregressive (9B params from their GLM-4 base) plus a diffusion decoder (7B DiT). basically the AR part handles semantic understanding and what the layout should be, while the diffusion decoder does the heavy lifting on high-freq details and text rendering with a glyph encoder. its like they split &amp;quot;understand what to draw&amp;quot; from &amp;quot;actually draw it well&amp;quot; into seperate specialized components which... idk makes sense when you think about it?&lt;/p&gt; &lt;p&gt;couple things,&lt;/p&gt; &lt;p&gt;text rendering is actually SOTA for open source models. tops CVTG-2K and LongText-Bench for complex multi-region text and long text scenarios, especially strong with chinese characters. if youve ever tried generating posters or infographics with SDXL/FLUX and gotten complete garbled nonsense for text this might actually be worth testing&lt;/p&gt; &lt;p&gt;but heres the intresting part, trained entirely on Huawei Ascend chips. like soup-to-nuts on non-NVIDIA hardware (Atlas 800T A2 + MindSpore framework). whether you care about geopolitics or not its kinda cool that competitive results are achieveable outside the CUDA ecosystem. first SOTA multimodal model done this way apparently&lt;/p&gt; &lt;p&gt;its actually open too, MIT license, full weights on HF, integrates with transformers/diffusers pipelines. supports both T2I and I2I stuff (editing, style transfer, identity preservation etc)&lt;/p&gt; &lt;p&gt;tradeoffs tho: inference is expensive rn, needs 80gb single gpu or multi-gpu setup. theyre working on vllm/sglang optimization but yeah. also uses semantic-VQ tokens instead of traditional VQVAE which gives better semantic correlation but requires the two-stage architechture&lt;/p&gt; &lt;p&gt;some benchmarks: CVTG-2K hit 0.9116 word accuracy vs Qwen-Image's 0.8288. supports 1024x1024 to 2048x2048 natively without retraining. apparently few cents per image via API and they mention a faster version comming&lt;/p&gt; &lt;p&gt;curious if anyones actually tested this against FLUX.1-dev for text-heavy use cases? the semantic-VQ approach seems like a meaninful architectural choice rather then just throwing more parameters at the problem&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_Damage824"&gt; /u/Consistent_Damage824 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m1zahy20ppdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefz88/glmimage_trained_on_huawei_chips_hits_sota_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefz88/glmimage_trained_on_huawei_chips_hits_sota_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T13:30:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe9xfi</id>
    <title>New FLUX.2 [Klein] 9B is INSANELY Fast</title>
    <updated>2026-01-16T07:48:50+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;BFL is has done a good job with this new Klein model, though in my testing text-to-image in distilled flavor is the best:&lt;/p&gt; &lt;p&gt;üîπ Sub-second inference on RTX 4090 hardware&lt;/p&gt; &lt;p&gt;üîπ 9B parameters matching models 5x its size&lt;/p&gt; &lt;p&gt;üîπ Step-distilled from 50 ‚Üí 4 steps, zero quality loss&lt;/p&gt; &lt;p&gt;üîπ Unified text-to-image + multi-reference editing&lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/black-forest-labs/FLUX.2-klein-base-9B"&gt;black-forest-labs/FLUX.2-klein-base-9B ¬∑ Hugging Face&lt;/a&gt;&lt;br /&gt; Detailed testing is here: &lt;a href="https://youtu.be/j3-vJuVwoWs?si=XPh7_ZClL8qoKFhl"&gt;https://youtu.be/j3-vJuVwoWs?si=XPh7_ZClL8qoKFhl&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe9xfi/new_flux2_klein_9b_is_insanely_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T07:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe4so5</id>
    <title>Dang, M2 drives are the new DDR5 apparently.</title>
    <updated>2026-01-16T03:18:52+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"&gt; &lt;img alt="Dang, M2 drives are the new DDR5 apparently." src="https://preview.redd.it/c8pq1jm6qmdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9e429f423f72a245e2fe28f8b56d773252a3eec" title="Dang, M2 drives are the new DDR5 apparently." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c8pq1jm6qmdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe4so5/dang_m2_drives_are_the_new_ddr5_apparently/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T03:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe0cxc</id>
    <title>Latest upgrade‚Ä¶A100 40 GB</title>
    <updated>2026-01-16T00:03:21+00:00</updated>
    <author>
      <name>/u/inserterikhere</name>
      <uri>https://old.reddit.com/user/inserterikhere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"&gt; &lt;img alt="Latest upgrade‚Ä¶A100 40 GB" src="https://preview.redd.it/f66wnmearldg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=703e9ae4724a0bb0a84546215e98301f06d28541" title="Latest upgrade‚Ä¶A100 40 GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Originally this was my gaming rig but I went ITX and basically bought a new computer. So I had the case, fans, AIO, 64 GB DDR5, motherboard, PSU, and 3080 (upgraded to 5070ti RIP). I was going to sell these parts, but I started running models on my 5070ti and eventually I wanted to start running larger models. I found a 3090 on eBay for $680, and 7950x for $350. I put that together with the parts and it‚Äôs been a great AI rig for me. I really didn‚Äôt plan on upgrading this for a while, especially now with the current price surges. Welp, I saw an A100 get listed for $1000 on eBay. The catch? Listed for parts, and the description just said ‚Äúcard reports CUDA error‚Äù. So I figured it was worth the risk (for me), I could‚Äôve probably sold it for the price I paid. Well, I swapped out the 3080 and on the first boot it was recognized instantly by nvidia-smi. I was able to run and train models immediately. Nice. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inserterikhere"&gt; /u/inserterikhere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f66wnmearldg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe0cxc/latest_upgradea100_40_gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T00:03:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qexkwb</id>
    <title>Llama.cpp vs vllm</title>
    <updated>2026-01-17T00:27:14+00:00</updated>
    <author>
      <name>/u/Evening_Tooth_1913</name>
      <uri>https://old.reddit.com/user/Evening_Tooth_1913</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which one is better for model serving? And which one is faster?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Evening_Tooth_1913"&gt; /u/Evening_Tooth_1913 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qexkwb/llamacpp_vs_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qexkwb/llamacpp_vs_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qexkwb/llamacpp_vs_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T00:27:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeogub</id>
    <title>My Ralph Wiggum prompt for Qwen3 Coder 480B, reliable and predictable, cheap alternative from Sonnet 4.5</title>
    <updated>2026-01-16T18:45:07+00:00</updated>
    <author>
      <name>/u/dheetoo</name>
      <uri>https://old.reddit.com/user/dheetoo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Coder 480B is powerful and cheap model to run on the daily basis, here is my Ralph loop prompt for it.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/bin/bash set -e opencode --prompt \ &amp;quot;You are typical software engineer, you only work for a narrow scoped that you been told to do, nothing more, nothing less. \ Reading the specification from /spec.md and current progress from /progress.txt then \ 1. Decide which task to work on next in /prd.json file. \ This should be the one YOU decide has the highest priority \ - not necessarily the first in the list. \ 2. Check any feedback loops, such as types and tests. \ 3. Append your progress to the /progress.txt file. \ 4. Update /prd.json file after each task completed. \ 5. Make a git commit of that feature. \ ONLY WORK ON A SINGLE FEATURE At A TIME. \ After you finished each task in /prd.json, exit and let other agent continue. \ If, while implementing the feature, you notice that **ALL** work items \ is complete, output &amp;lt;promise&amp;gt;COMPLETE&amp;lt;/promise&amp;gt;. \ Let me repeat that again, only output &amp;lt;promise&amp;gt;COMPLETE&amp;lt;/promise&amp;gt; \ when **ALL** work items in /prd.json is completed, otherwise just exit with out output anything. \ Always kill all background process if you start any before you exit the session.&amp;quot; --model nvidia/qwen/qwen3-coder-480b-a35b-instruct &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dheetoo"&gt; /u/dheetoo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeogub/my_ralph_wiggum_prompt_for_qwen3_coder_480b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeogub/my_ralph_wiggum_prompt_for_qwen3_coder_480b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeogub/my_ralph_wiggum_prompt_for_qwen3_coder_480b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T18:45:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qexs07</id>
    <title>Is there a local/self-hosted alternative to Google NotebookLM?</title>
    <updated>2026-01-17T00:34:31+00:00</updated>
    <author>
      <name>/u/RadiantCandy1600</name>
      <uri>https://old.reddit.com/user/RadiantCandy1600</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is an Alternate to &lt;strong&gt;Google NotebookLM&lt;/strong&gt;?&lt;/p&gt; &lt;p&gt;I would like something local because of concern of uploading sensitive work documents or personal research to Google‚Äôs cloud. I‚Äôm looking for something I can run &lt;strong&gt;locally on my own hardware&lt;/strong&gt; (or a private VPS) that replicates that &amp;quot;Notebook&amp;quot; experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Ideally, I‚Äôm looking for:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Privacy:&lt;/strong&gt; No data leaving my machine.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Source Grounding:&lt;/strong&gt; The ability to chat with specific &amp;quot;Notebooks&amp;quot; or collections of PDFs/Markdown/Text files.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Citations:&lt;/strong&gt; It needs to tell me exactly which page/document the answer came from (this is the best part of NotebookLM).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio/Podcasts (Optional):&lt;/strong&gt; The AI podcast generator in NotebookLM is cool, but document analysis is my priority.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What are the best options in 2026?&lt;/strong&gt; I‚Äôve heard names like &lt;strong&gt;AnythingLLM&lt;/strong&gt;, &lt;strong&gt;GPT4All&lt;/strong&gt;, and &lt;strong&gt;Open Notebook&lt;/strong&gt; (the GitHub project) thrown around. Which one is currently the most stable and &amp;quot;NotebookLM-like&amp;quot;? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RadiantCandy1600"&gt; /u/RadiantCandy1600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qexs07/is_there_a_localselfhosted_alternative_to_google/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qexs07/is_there_a_localselfhosted_alternative_to_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qexs07/is_there_a_localselfhosted_alternative_to_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T00:34:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qf1msz</id>
    <title>Cowork but with local models not to send all your data to a remote cloud!</title>
    <updated>2026-01-17T03:05:24+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt; &lt;img alt="Cowork but with local models not to send all your data to a remote cloud!" src="https://external-preview.redd.it/bXh5cnloeGpzdGRnMQQmqaqY6IxBgH-vwmsMWuXB8i4MNI5FplyyvMhZJ44G.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c0f045206bb7f1efd15ba454efa839b61d2ec86" title="Cowork but with local models not to send all your data to a remote cloud!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wmrdxexjstdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qf1msz/cowork_but_with_local_models_not_to_send_all_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-17T03:05:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeupi8</id>
    <title>PersonaPlex: Voice and role control for full duplex conversational speech models</title>
    <updated>2026-01-16T22:44:34+00:00</updated>
    <author>
      <name>/u/Lopsided_Dot_4557</name>
      <uri>https://old.reddit.com/user/Lopsided_Dot_4557</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NVIDIA released Personaplex is a real-time speech-to-speech conversational model that jointly performs streaming speech understanding and speech generation. &lt;/p&gt; &lt;p&gt;üîπInspired by Moshi&lt;/p&gt; &lt;p&gt;üîπ Full duplex = AI listens WHILE talking (no more robotic pauses)&lt;/p&gt; &lt;p&gt;üîπ Any voice + any role through simple text prompts&lt;/p&gt; &lt;p&gt;üîπ Handles interruptions, backchannels &amp;amp; natural turn-taking&lt;/p&gt; &lt;p&gt;üîπ 7B params, runs locally, Good progress with room for improvement. &lt;/p&gt; &lt;p&gt;HF Model: &lt;a href="https://huggingface.co/nvidia/personaplex-7b-v1"&gt;nvidia/personaplex-7b-v1 ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Install and Test Demo: &lt;a href="https://youtu.be/5_mOTtWouCk?si=uFJeToxcjqlzvcqN"&gt;https://youtu.be/5_mOTtWouCk?si=uFJeToxcjqlzvcqN&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided_Dot_4557"&gt; /u/Lopsided_Dot_4557 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupi8/personaplex_voice_and_role_control_for_full/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupi8/personaplex_voice_and_role_control_for_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupi8/personaplex_voice_and_role_control_for_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:44:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeimyi</id>
    <title>7 GPUs at X16 (5.0 and 4.0) on AM5 with Gen5/4 switches with the P2P driver. Some results on inference and training!</title>
    <updated>2026-01-16T15:15:34+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"&gt; &lt;img alt="7 GPUs at X16 (5.0 and 4.0) on AM5 with Gen5/4 switches with the P2P driver. Some results on inference and training!" src="https://b.thumbs.redditmedia.com/oT5YU0t-mdDb6dXVqwnx9uFeB1IvH9XzAha2HR-inYc.jpg" title="7 GPUs at X16 (5.0 and 4.0) on AM5 with Gen5/4 switches with the P2P driver. Some results on inference and training!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you're fine!&lt;/p&gt; &lt;p&gt;As I mentioned in the past in this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pt0av6/plxpex_pcie_40_seems_to_help_for_llms_and_p2p_ie/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With the P2P driver (&lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules/?tab=readme-ov-file"&gt;https://github.com/aikitoria/open-gpu-kernel-modules/?tab=readme-ov-file&lt;/a&gt;) you can do P2P on same gen GPUs, including consumer ones!&lt;/p&gt; &lt;p&gt;So, also, you can connect GPUs on the same PCIe switch, and with the P2P driver the info is passed directly on the switch fabric instead by going by the CPU root complex, so for example:&lt;/p&gt; &lt;p&gt;5090 &amp;lt;-&amp;gt; 5090 directly on the same switch with the P2P driver would be possible. Since PCIe it is bidirectional, you can read at 64GiB/s on one GPU and write at 64GiB/s on the other at the same time!&lt;/p&gt; &lt;p&gt;So here we go with the info. Also I will mention some products I got from Aliexpress, but without a link, else the post gets removed. I can post the links on a comment for those products if you're interested.&lt;/p&gt; &lt;p&gt;A sneakpeek:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ea7itij34qdg1.png?width=859&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=96db6103a3838accb9eea239f2fa0712b14d13d2"&gt;X16 on 7 GPUs on AM5&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Setup including switches&lt;/h1&gt; &lt;p&gt;So for my setup, I have this:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gigabyte Aorus Master X670E&lt;/li&gt; &lt;li&gt;AMD Ryzen 9 9900X&lt;/li&gt; &lt;li&gt;192GB DDR5 6000Mhz&lt;/li&gt; &lt;li&gt;2 Asrock 1600W PSU (PG 1600G ATX 3.1)&lt;/li&gt; &lt;li&gt;1 Corsair 1500W PSU (Corsair HX1500i)&lt;/li&gt; &lt;li&gt;RTX 5090*2 (PCIe 5.0)&lt;/li&gt; &lt;li&gt;RTX 4090*2 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;RTX 3090 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;RTX A6000 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;NVIDIA A40 (PCIe 4.0)&lt;/li&gt; &lt;li&gt;Multiple SSDs, a 40Gbps NIC, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Switch 1: 100 lanes PCIe 5.0 switch, Microchip Switchtec PM50100 from c-payne, from &lt;a href="https://c-payne.com/products/pcie-gen5-mcio-switch-100-lane-microchip-switchtec-pm50100"&gt;here&lt;/a&gt;, for 2000 EUR (about 2500USD post taxes in Chile)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/srwwml1p0qdg1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d032f2a2606fd6603bbe8bffa005f9a14622f52b"&gt;PCIe 5.0 100 lane switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This switch has one X16 5.0 upstream, to 5*X16 5.0 downstream + 1*X4 5.0 downstream, via MCIO.&lt;/p&gt; &lt;p&gt;For this, I got a MCIO Retimer from aliexpress, that looks like this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/zc917jy21qdg1.png?width=1000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de574e29fbb36bf0bf833b9d8d9e3da87ba5bdac"&gt;MCIO 5.0 Retimer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Else, with a passive MCIO adapter, some GPUs would drop randomly.&lt;/p&gt; &lt;p&gt;For the other switch, I got a PLX88096 switch one from aliexpress, for about 400USD. This is a 96 lane PCIe 4.0 switch.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/smp1c0671qdg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=41d150605391d7b25f44a12356eb71c256285097"&gt;PLX88096 4.0 switch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This switch has X16 upstream from the PCIe slot, and it has 10 SlimSAS downstream ports.&lt;/p&gt; &lt;p&gt;This means you can do, with the dip switch, either: 5*X16 4.0, or 10*X8 4.0, or 20*X4 4.0.&lt;/p&gt; &lt;h1&gt;Connection of the GPUs&lt;/h1&gt; &lt;p&gt;For this, I basically connected the MCIO 5.0 retimer on the main X16 5.0 slot from the motherboard, and then, on this switch, I connected 2 5090s directly on 4 MCIO ports, and on other 2 MCIO ports, I connected the PLX88096 SlimSAS switch.&lt;/p&gt; &lt;p&gt;Basically, it looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PM50100 Switch (01:00.0) ‚îú‚îÄ‚îÄ Port 02.0 ‚Üí GPU2 (5090) direct ‚îú‚îÄ‚îÄ Port 03.0 ‚Üí PLX88096 (cascaded) ‚îÇ ‚îî‚îÄ‚îÄ Complex internal structure: ‚îÇ ‚îú‚îÄ‚îÄ GPU0 (4090) ‚îÇ ‚îú‚îÄ‚îÄ GPU1 (4090) ‚îÇ ‚îú‚îÄ‚îÄ GPU4 (A40) ‚îÇ ‚îú‚îÄ‚îÄ GPU5 (A6000) ‚îÇ ‚îî‚îÄ‚îÄ GPU6 (3090) ‚îî‚îÄ‚îÄ Port 04.0 ‚Üí GPU3 (5090) direct ‚îî‚îÄ‚îÄ Other ports unused ATM &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;What is CPU root complex? Why it is worse?&lt;/h1&gt; &lt;p&gt;When we talk about GPUs communicating via the CPU root complex, it's when the data has to move from the PCIe slot to the RAM, and viceversa on the case of no P2P. For this to happen, it HAS to pass by the CPU. If you use P2P, then it is directly via PCIe to PCIe via the CPU root complex.&lt;/p&gt; &lt;p&gt;So normally, let¬¥s say you take a motherboard that has 2*X8 5.0 slots. You connect a 5090 on each slot.&lt;/p&gt; &lt;p&gt;If you do TP (tensor parallel), or training with multiGPU, either by using P2P or not, the data has to pass between the 2 GPUs.&lt;/p&gt; &lt;p&gt;If you don't use a switch, this data has to pass by the CPU first.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If no P2P: 5090(1) -&amp;gt; CPU -&amp;gt; RAM -&amp;gt; CPU -&amp;gt; 5090(2)&lt;/li&gt; &lt;li&gt;If P2P: 5090(1) -&amp;gt; CPU -&amp;gt; 5090(2)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This adds extra latency by doing extra hops, specially on the case of no P2P.&lt;/p&gt; &lt;h1&gt;Topology&lt;/h1&gt; &lt;p&gt;Topology looks like this (GPU 0 and 1: 5090s, 2 and 3: 4090s, 4,5 and 6: A6000, A40 and 3090):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ nvidia-smi topo -m GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 NIC0 CPU Affinity NUMA Affinity GPU NUMA ID GPU0 X PXB PXB PXB PXB PXB PIX PHB 0-23 0 N/A GPU1 PXB X PXB PXB PXB PXB PXB PHB 0-23 0 N/A GPU2 PXB PXB X PIX PXB PXB PXB PHB 0-23 0 N/A GPU3 PXB PXB PIX X PXB PXB PXB PHB 0-23 0 N/A GPU4 PXB PXB PXB PXB X PIX PXB PHB 0-23 0 N/A GPU5 PXB PXB PXB PXB PIX X PXB PHB 0-23 0 N/A GPU6 PIX PXB PXB PXB PXB PXB X PHB 0-23 0 N/A NIC0 PHB PHB PHB PHB PHB PHB PHB X Legend: X = Self SYS = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI) NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node PHB = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU) PXB = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge) PIX = Connection traversing at most a single PCIe bridge NV# = Connection traversing a bonded set of # NVLinks NIC Legend: NIC0: mlx4_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As you can see, 5090 pair, or 4090 pair, or Ampere trio have PIX. That means as it says, the connection traverses at most a single PCIe bridge, without going by the CPU root complex.&lt;/p&gt; &lt;p&gt;When the GPUs have to communicate with another of other gen, then it is PXB. This is because it has to pass by the switch via hops.&lt;/p&gt; &lt;p&gt;If you don't use a switch, with or without the P2P driver, you would normally see PHB.&lt;/p&gt; &lt;h1&gt;Bandwidth&lt;/h1&gt; &lt;p&gt;For bandwidth, I did this test on cuda samples:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 4090, pciBusID: e, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 4090, pciBusID: 11, pciDeviceID: 0, pciDomainID:0 Device: 2, NVIDIA GeForce RTX 5090, pciBusID: 5, pciDeviceID: 0, pciDomainID:0 Device: 3, NVIDIA GeForce RTX 5090, pciBusID: 18, pciDeviceID: 0, pciDomainID:0 Device: 4, NVIDIA A40, pciBusID: d, pciDeviceID: 0, pciDomainID:0 Device: 5, NVIDIA RTX A6000, pciBusID: 12, pciDeviceID: 0, pciDomainID:0 Device: 6, NVIDIA GeForce RTX 3090, pciBusID: a, pciDeviceID: 0, pciDomainID:0 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 2 3 4 5 6 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 2 0 0 1 1 0 0 0 3 0 0 1 1 0 0 0 4 0 0 0 0 1 1 1 5 0 0 0 0 1 1 1 6 0 0 0 0 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 915.89 8.31 12.75 12.75 8.30 8.30 5.83 1 8.32 927.85 12.75 12.75 8.30 8.30 5.79 2 12.26 12.26 1562.55 23.21 12.21 12.21 7.99 3 12.26 12.26 23.22 1556.32 12.21 12.21 7.98 4 8.31 8.31 12.70 12.70 644.33 8.29 5.78 5 8.31 8.31 12.70 12.70 8.30 766.68 5.80 6 5.82 5.81 8.07 8.12 5.82 5.79 833.78 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 920.20 26.37 12.75 12.75 8.30 8.30 5.85 1 26.36 944.11 12.75 12.74 8.30 8.30 5.81 2 12.26 12.26 1540.97 57.23 12.21 12.21 7.99 3 12.25 12.26 57.25 1543.97 12.21 12.21 7.98 4 8.31 8.31 12.70 12.70 643.53 26.36 26.36 5 8.31 8.31 12.70 12.70 26.36 767.06 26.36 6 5.83 5.81 8.07 8.07 26.37 26.37 835.56 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 921.29 9.49 15.20 15.21 9.48 9.49 6.27 1 9.49 926.20 15.21 15.23 9.48 9.50 6.29 2 14.18 14.15 1541.62 23.43 14.12 14.17 9.71 3 14.18 14.17 23.27 1540.12 14.13 14.21 9.71 4 9.46 9.48 15.15 15.14 647.80 9.48 6.28 5 9.51 9.48 15.23 15.24 9.49 770.65 6.29 6 6.27 6.29 10.70 10.69 6.32 6.26 839.38 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 922.10 52.18 15.20 15.15 9.49 9.50 6.32 1 52.18 922.92 15.19 15.19 9.49 9.50 6.26 2 14.16 14.17 1540.86 110.82 14.13 14.20 9.72 3 14.16 14.17 110.77 1537.09 14.09 14.20 9.72 4 9.48 9.47 15.12 15.12 647.53 52.19 52.19 5 9.51 9.50 15.27 15.25 52.17 769.89 52.19 6 6.31 6.28 10.69 10.67 52.18 52.18 838.25 P2P=Disabled Latency Matrix (us) GPU 0 1 2 3 4 5 6 0 1.30 15.32 14.38 14.41 15.74 15.09 14.85 1 15.17 1.35 14.71 14.39 14.26 14.26 14.25 2 14.34 14.35 2.07 14.46 14.37 14.36 14.35 3 14.33 14.34 14.34 2.07 14.34 14.44 14.35 4 14.80 14.25 14.48 15.24 1.78 15.96 14.70 5 16.10 14.73 14.45 14.36 14.37 1.77 14.33 6 14.24 14.25 14.38 14.53 15.11 14.33 1.60 CPU 0 1 2 3 4 5 6 0 1.40 4.21 4.15 4.14 3.95 4.14 4.16 1 4.19 1.35 4.14 4.14 3.93 4.09 4.10 2 4.19 4.12 1.55 4.09 3.92 4.10 4.12 3 4.14 4.10 3.95 1.51 3.73 3.91 3.94 4 3.83 4.01 4.00 3.97 1.28 4.03 4.00 5 4.22 4.15 4.12 4.11 3.91 1.35 4.14 6 4.11 4.08 4.09 4.11 3.88 4.11 1.35 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 2 3 4 5 6 0 1.28 1.41 14.47 14.38 14.91 14.26 18.66 1 1.41 1.29 14.41 14.39 14.26 14.26 16.30 2 14.34 14.41 2.07 0.36 14.40 14.34 14.37 3 14.34 14.35 0.36 2.07 14.40 14.36 14.36 4 14.35 16.30 14.49 14.44 1.80 1.62 1.58 5 16.66 14.24 14.37 14.40 1.58 1.76 1.60 6 15.08 15.27 14.37 14.43 1.52 1.51 1.56 CPU 0 1 2 3 4 5 6 0 1.39 1.13 4.16 4.13 3.94 4.19 4.17 1 1.14 1.36 4.17 4.14 3.93 4.17 4.15 2 4.17 4.19 1.54 1.08 3.94 4.12 4.14 3 4.17 4.17 1.10 1.57 3.94 4.14 4.15 4 4.04 4.02 4.04 4.01 1.29 1.02 1.03 5 4.18 4.18 4.19 4.18 1.10 1.37 1.09 6 4.17 4.14 4.14 4.15 1.09 1.09 1.35 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Like that, we have this bidirectional bandwidth:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090 ‚Üî 5090: 110.82 GB/s (via PM50100 switch)&lt;/li&gt; &lt;li&gt;4090 ‚Üî 4090: 52.18 GB/s (via PLX88096 switch connected to the PM50100 switch)&lt;/li&gt; &lt;li&gt;Ampere Trio A40 ‚Üî A6000 ‚Üî 3090: 52.19 GB/s (via PLX88096 switch connected to the PM50100 switch)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Remember that when having a PCIe switch, P2P and GPUs on the same switch, they communicate directly via the switch fabric without having to pass by the CPU root complex. So you can surpass the uplink bandwidth as long you keep it inside the switch.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NOTE:&lt;/strong&gt; P2P does not work across different GPU gens, so on that case (i.e. 5090 to 4090, or 5090 to 3090) bandwidth is reduced.&lt;/p&gt; &lt;p&gt;On that case, if using all the GPUs at the same time, bandwidth between them is about 15GB/s. About PCIe 4.0 X8 speeds (thanks to PCIe being bidirectional).&lt;/p&gt; &lt;h1&gt;Performance (on limited tests, and why I want to you to give me some ideas to test)&lt;/h1&gt; &lt;p&gt;Because I had only X4 4.0 lanes at most, I mostly only used llamacpp. But I think with the switches, for 4 GPUs at least, something like vLLM would make sense.&lt;/p&gt; &lt;p&gt;So for my tests, I only have some diffusion training, and some LLMs on llamacpp, where even with this it makes a difference.&lt;/p&gt; &lt;h1&gt;Training (diffusion)&lt;/h1&gt; &lt;p&gt;For this, I did a full finetune on a SDXL model. Not good results at all per se but it was mostly to take the time it took.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1 5090: ~24 hours&lt;/li&gt; &lt;li&gt;2 5090s (no P2P, X8/X8): ~16 hours (mostly by increasing the effective batch size, speed was the same but steps were halved)&lt;/li&gt; &lt;li&gt;2 5090s (P2P driver, X8/X8): ~13 hours&lt;/li&gt; &lt;li&gt;2 5090s (P2P driver, X16/X16 via switch): ~8 hours&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That is a huge uplink, mostly by using the P2P driver first. So if you have 2 5090s at X8/X8, make sure to install the P2P driver!&lt;/p&gt; &lt;h1&gt;Inference (don't kill me, just llamacpp for now)&lt;/h1&gt; &lt;p&gt;For this, I have tested 3 models, on different configurations, so it took a bit of time. I hope it helps for info!&lt;/p&gt; &lt;p&gt;First I set the device order like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;5090, 5090, 4090, 4090, 3090, A40, A6000 export CUDA_VISIBLE_DEVICES=2,3,0,1,6,5,4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Also all the tests were made with the P2P driver in use (but should make no difference on llamacpp (but it does on ikllamacpp)).&lt;/p&gt; &lt;p&gt;First:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM 4.7 Q4_K_XL (about 196GB in size), fully loaded on GPU:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For this one, loading with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m '/run/media/pancho/MyDrive/models_llm_2tb/GLM-4.7-UD-Q4_K_XL.gguf' \ -c 32768 \ --no-mmap \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6|7|8|9|10|11|12|13|14).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(15|16|17|18|19|20|21|22|23|24|25|26).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(27|28|29|30|31|32|33|34|35).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(36|37|38|39|40|41|42|43|44).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(45|46|47|48|49|50|51|52|53).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(54|55|56|57|58|59|60|61|62|63|64|65|66|67|68|69|70|71|72|73).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(74|75|76|77|78|79|80|81|82|83|84|85|86|87|88|89|90|91|92).ffn.=CUDA6&amp;quot; \ -mg 0 \ -ub 2048 -b 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have these results for different setups (PP = Prompt processing, TG = Text generation):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, A6000, A40 at X4 4.0 and 3090 at X1 3.0: 665.46 t/s PP, 25.90 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, and Ampere trio at X4 4.0: 765.51 t/s PP, 26.18 t/s TG.&lt;/li&gt; &lt;li&gt;5090(1) at X16 5.0, 5090(2) at X4 5.0, all the rest at X4 4.0: 940 t/s PP, 26.75 t/s TG.&lt;/li&gt; &lt;li&gt;5090s at X16 5.0, all the rest at X16 4.0: 1170 t/s PP, 27.64 t/s TG.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;DeepSeek V3 0324, IQ4_XS, offloading about 120GB to CPU:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Loading with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server -m '/run/media/pancho/MyDrive2/HuggingFaceModelDownloader/Storage/GGUFs/DeepSeek-V3-0324-IQ4_XS.gguf' -c 32768 --no-mmap -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3|4|5|6).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(7|8|9|10|11|12).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(13|14|15).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(16|17|18).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(19|20|21).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(22|23|24).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(25|26|27|28).ffn.=CUDA6&amp;quot; \ -ot &amp;quot;blk.30.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.30.ffn_gate_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.30.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.31.ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.31.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.31.ffn_up_exps.weight=CUDA6&amp;quot; \ -ot &amp;quot;blk.32.ffn_gate_exps.weight=CUDA6&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -mg 0 -ub 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have these results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, A6000, A40 at X4 4.0 and 3090 at X1 3.0: 195.66 t/s PP, 10.1 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, and Ampere trio at X4 4.0: 244 t/s PP, 11.52 t/s TG&lt;/li&gt; &lt;li&gt;5090(1) at X16 5.0, 5090(2) at X4 5.0, all the rest at X4 4.0: 312.64 t/s PP, 11.58 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X16 5.0, all the rest at X16 4.0: 360.86 t/s PP, 11.71 t/s TG&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Kimi K2 Instruct Q2_K_XL, offloading about 160GB to CPU:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Loading with:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m '/run/media/pancho/Drive954GB/models_llm_1tb/Kimi-K2-Thinking-UD-Q2_K_XL-00001-of-00008.gguf' \ -c 32768 \ --no-mmap \ -ngl 999 \ -ot &amp;quot;blk.(0|1|2|3).ffn.=CUDA0&amp;quot; \ -ot &amp;quot;blk.(4|5|6|7).ffn.=CUDA1&amp;quot; \ -ot &amp;quot;blk.(8|9|10).ffn.=CUDA2&amp;quot; \ -ot &amp;quot;blk.(11|12|13).ffn.=CUDA3&amp;quot; \ -ot &amp;quot;blk.(14|15|16).ffn.=CUDA4&amp;quot; \ -ot &amp;quot;blk.(17|18|19|20|21|22|23).ffn.=CUDA5&amp;quot; \ -ot &amp;quot;blk.(24|25|26|27|28|29|30).ffn.=CUDA6&amp;quot; \ -ot &amp;quot;blk.31.ffn_down_exps.weight=CUDA0&amp;quot; \ -ot &amp;quot;blk.32.ffn_down_exps.weight=CUDA2&amp;quot; \ -ot &amp;quot;blk.33.ffn_down_exps.weight=CUDA3&amp;quot; \ -ot &amp;quot;blk.33.ffn_gate_exps.weight=CUDA1&amp;quot; \ -ot &amp;quot;blk.(31|32|33).ffn_(norm|gate_inp|gate_shexp|down_shexp|up_shexp).weight=CUDA1&amp;quot; \ -ot &amp;quot;exps=CPU&amp;quot; \ -mg 0 \ -ub 2048 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I have these results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, A6000, A40 at X4 4.0 and 3090 at X1 3.0: 179 t/s PP, 11.34t/s TG.&lt;/li&gt; &lt;li&gt;5090s at X8/X8 5.0, 4090s, and Ampere trio at X4 4.0: 198 t/s PP y 11.6 t/s TG.&lt;/li&gt; &lt;li&gt;5090(1) at X16 5.0, 5090(2) at X4 5.0, all the rest at X4 4.0: 219.08 t/s PP, 11.91 t/s TG&lt;/li&gt; &lt;li&gt;5090s at X16 5.0, all the rest at X16 4.0: 248 t/s PP, 11.95 t/s TG&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Table for TL:DR&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Configuration&lt;/th&gt; &lt;th align="left"&gt;GLM 4.7 Q4_K_XL(196GB, GPU only)&lt;/th&gt; &lt;th align="left"&gt;DeepSeek V3 IQ4_XS(~120GB CPU offload)&lt;/th&gt; &lt;th align="left"&gt;Kimi K2 Q2_K_XL(~160GB CPU offload)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Data&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;PP / TG (t/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;PP / TG (t/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;PP / TG (t/s)&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 1&lt;/strong&gt;:5090s: X8/X8 Gen5, 4090s/A6000/A40: X4 Gen4, 3090: X1 Gen3&lt;/td&gt; &lt;td align="left"&gt;665.46 / 25.90&lt;/td&gt; &lt;td align="left"&gt;195.66 / 10.10&lt;/td&gt; &lt;td align="left"&gt;179.00 / 11.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 2&lt;/strong&gt;:5090s: X8/X8 Gen5, All others: X4 Gen4&lt;/td&gt; &lt;td align="left"&gt;765.51 / 26.18 &lt;em&gt;(+15% / +1%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;244.00 / 11.52 &lt;em&gt;(+25% / +14%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;198.00 / 11.60 &lt;em&gt;(+11% / +2%)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 3&lt;/strong&gt;:5090#1: X16 Gen5, 5090#2: X4 Gen5,Others: X4 Gen4&lt;/td&gt; &lt;td align="left"&gt;940.00 / 26.75 &lt;em&gt;(+41% / +3%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;312.64 / 11.58 &lt;em&gt;(+60% / +15%)&lt;/em&gt;&lt;/td&gt; &lt;td align="left"&gt;219.08 / 11.91 &lt;em&gt;(+22% / +5%)&lt;/em&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Config 4&lt;/strong&gt;:5090s: X16 Gen5, All others: X16 Gen4&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1170.00 / 27.64&lt;/strong&gt; (+76% / +7%)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;360.86 / 11.71&lt;/strong&gt; (+84% / +16%)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;248.00 / 11.95&lt;/strong&gt; (+39% / +5%)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see here, TG is not that impacted by PCIe, but PP for sure it is, even on llamacpp!&lt;/p&gt; &lt;h1&gt;Some questions you may have&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Why?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Well, on this case it was mostly about cost. I already had the GPUs, the RAM and I was planning to get a Theadripper 9955WX plus a WRX90 motherboard.&lt;/p&gt; &lt;p&gt;But well, you know, RAM prices now are absurd.&lt;/p&gt; &lt;p&gt;On Chile, I have these prices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Theadripper 9955WX: 2000USD&lt;/li&gt; &lt;li&gt;Cheapest WRX90 board: 1800USD (alternative is Gigabyte AI TOP for 1500USD)&lt;/li&gt; &lt;li&gt;Cheapest 128GB DDR5 RDIMM, 4800Mhz: 4000USD (yes, I'm not even joking)&lt;/li&gt; &lt;li&gt;256GB DDR5 RDIMM 4800Mhz: 6500USD&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;RAM bandwidth would have been a bit better, and also 128 5.0 lanes, I know.&lt;/p&gt; &lt;p&gt;But you're comparing a 5.0 switch (2500USD) a 4.0 switch (400USD) for a total of 2900USD, vs 7800 to 10300USD. So about 3x-4x the price.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why not a 6000 PRO?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;There was no stock of the 6000 PRO for most of the 2025. Just on December they arrived, but they go for 12000USD each. You can get 4x5090s for that price here.&lt;/p&gt; &lt;p&gt;But I understand you save: power, space and heat. I'm still thinking about it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How do you fit so many GPUs?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;With a custom self made wood rack! I have some pics. It's not the prettiest, but it works.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0jlsnu6s9qdg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fbde9de64eeb52ee942786486b16fdf870a7cd6a"&gt;Multiple fans&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ddhnurlt9qdg1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=388ba71d88968adc89321ff1a80c3b84416fed71"&gt;ConnectX 3 with a fan, and MCIO retimer behind&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Final words, and please let me know what can I test!&lt;/h1&gt; &lt;p&gt;Hope you guys find informative, and if you can let me know what can I test here, let me know.&lt;/p&gt; &lt;p&gt;Have fun on the LLM side!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeimyi/7_gpus_at_x16_50_and_40_on_am5_with_gen54/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T15:15:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qequei</id>
    <title>WorldModel-Qwen-0.6B: Proof of Concept WASM Computation-as-Reasoning in small LLMs</title>
    <updated>2026-01-16T20:13:40+00:00</updated>
    <author>
      <name>/u/bigattichouse</name>
      <uri>https://old.reddit.com/user/bigattichouse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm building a prototype fine-tune that has layers that create and execute WASM code as part of inference - for internal calculation and external tool calling.&lt;/p&gt; &lt;p&gt;So instead of a tiny model guessing at something like a sum or unit conversion, it will create WASM code internal to the model that is immediately executed to generate the next set of tokens for consideration.&lt;/p&gt; &lt;p&gt;My previous iteration was really a glorified &amp;lt;think&amp;gt; tag. Now I'm generating WASM code in layers the way visual and audio models do.&lt;/p&gt; &lt;p&gt;Article (no paywall): &lt;a href="https://bigattichouse.medium.com/worldmodel-qwen-0-6b-proof-of-concept-computation-as-reasoning-in-small-llms-95092b8b7aef?sk=d1a9ff8ab1415e99ab668769828ea90f"&gt;https://bigattichouse.medium.com/worldmodel-qwen-0-6b-proof-of-concept-computation-as-reasoning-in-small-llms-95092b8b7aef?sk=d1a9ff8ab1415e99ab668769828ea90f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/bigattichouse/worldmodel"&gt;https://github.com/bigattichouse/worldmodel&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bigattichouse"&gt; /u/bigattichouse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bigattichouse.medium.com/worldmodel-qwen-0-6b-proof-of-concept-computation-as-reasoning-in-small-llms-95092b8b7aef?sk=d1a9ff8ab1415e99ab668769828ea90f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qequei/worldmodelqwen06b_proof_of_concept_wasm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qequei/worldmodelqwen06b_proof_of_concept_wasm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T20:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeupjy</id>
    <title>Made one more step towards getting Offloom on steam! (for free).</title>
    <updated>2026-01-16T22:44:38+00:00</updated>
    <author>
      <name>/u/Little-Put6364</name>
      <uri>https://old.reddit.com/user/Little-Put6364</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupjy/made_one_more_step_towards_getting_offloom_on/"&gt; &lt;img alt="Made one more step towards getting Offloom on steam! (for free)." src="https://preview.redd.it/m5gfmmmy8sdg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a53a44384c69445581fe10c8acfc0a066c9b0488" title="Made one more step towards getting Offloom on steam! (for free)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's taken quite some time to get this to where it is now. But one thing I noticed is most open source tools are designed with technical folks in mind. I wanted to create a tool that comes preset up. Something for the less technical folks who are interested in AI but don't want to spend time learning how to use local tooling and models. Basically chatGPT levels of ease of use and set up.&lt;/p&gt; &lt;p&gt;Offloom will ship with Image generation. RAG (document and web) all powered by locally ran open source models. It's designed with 12GB VRAM in mind. I might be able to drop it to 8GB, but that's untested so far in the quality sense. It juggles multiple models in an agentic way to help with answer quality. It's a step above the basic implementations you'll find all over the place, but by no means is this ground breaking in the field. Just bringing architectures available in the online third party tools to local users. &lt;/p&gt; &lt;p&gt;I'm probably still a bit from launch as I have a lot of UI/UX polishing that needs to be done. But sometime soon I'll be making a call for some beta testers. Keep an eye out if you're interested! The steam page is currently under review. As long as I filled everything out correctly it should pop up in the next 3-5 days for wish listing! I'm setting a tentative launch date for March. However, that largely depends on how many beta testers I can get with different hardware, and how busy my day job gets between now and then. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Little-Put6364"&gt; /u/Little-Put6364 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/m5gfmmmy8sdg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupjy/made_one_more_step_towards_getting_offloom_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeupjy/made_one_more_step_towards_getting_offloom_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:44:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qewdza</id>
    <title>What are you building with sub-4B LLMs in early 2025? Real-world use wins?</title>
    <updated>2026-01-16T23:44:29+00:00</updated>
    <author>
      <name>/u/Whiplashorus</name>
      <uri>https://old.reddit.com/user/Whiplashorus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, It's early 2025, and I'm diving deep into tiny LLMs (under 4B params) like Qwen3 4B, LFM2.5 1.2B, or LFM2.5 VL 1.6B.&lt;/p&gt; &lt;p&gt;These base models (no fine-tuning) are super lightweight and run anywhere, but I'm curious: what real-world use cases have you found that actually stick ?&lt;/p&gt; &lt;p&gt;Stuff that's genuinely useful day-to-day, not just benchmarks.Have you plugged them into pipelines like n8n, Make.com, or custom scripts? How's that working out?Any cool automations, agents, or edge deployments (phone, Raspberry Pi, etc.)? Please share your successes, setups, or even failure&lt;/p&gt; &lt;p&gt;I'm all ears! What's the most practical thing you've pulled off?&lt;/p&gt; &lt;p&gt;I wished to do something with my vacant homelab &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Whiplashorus"&gt; /u/Whiplashorus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qewdza/what_are_you_building_with_sub4b_llms_in_early/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:44:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeley8</id>
    <title>vLLM-MLX: Native Apple Silicon LLM inference - 464 tok/s on M4 Max</title>
    <updated>2026-01-16T16:56:21+00:00</updated>
    <author>
      <name>/u/waybarrios</name>
      <uri>https://old.reddit.com/user/waybarrios</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I built vLLM-MLX - a framework that uses Apple's MLX for native GPU acceleration.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- OpenAI-compatible API (drop-in replacement for your existing code)&lt;/p&gt; &lt;p&gt;- Multimodal support: Text, Images, Video, Audio - all in one server&lt;/p&gt; &lt;p&gt;- Continuous batching for concurrent users (3.4x speedup)&lt;/p&gt; &lt;p&gt;- TTS in 10+ languages (Kokoro, Chatterbox models)&lt;/p&gt; &lt;p&gt;- MCP tool calling support&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Performance on M4 Max:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Llama-3.2-1B-4bit ‚Üí 464 tok/s&lt;/p&gt; &lt;p&gt;- Qwen3-0.6B ‚Üí 402 tok/s&lt;/p&gt; &lt;p&gt;- Whisper STT ‚Üí 197x real-time&lt;/p&gt; &lt;p&gt;Works with standard OpenAI Python SDK - just point it to localhost.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/waybarrios/vllm-mlx"&gt;https://github.com/waybarrios/vllm-mlx&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or take feature requests!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waybarrios"&gt; /u/waybarrios &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeley8/vllmmlx_native_apple_silicon_llm_inference_464/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T16:56:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qew9df</id>
    <title>Best coding models for RTX 6000 Pro Blackwell</title>
    <updated>2026-01-16T23:39:49+00:00</updated>
    <author>
      <name>/u/az_6</name>
      <uri>https://old.reddit.com/user/az_6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I have a RTX 6000 Pro Blackwell (96GB VRAM) and trying to decide what model is best for agentic coding with Aider/OpenCode. What have folks tried and anyone found anything that gets close to Sonnet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/az_6"&gt; /u/az_6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qew9df/best_coding_models_for_rtx_6000_pro_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T23:39:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qe2i88</id>
    <title>My story of underestimating /r/LocalLLaMA's thirst for VRAM</title>
    <updated>2026-01-16T01:36:54+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"&gt; &lt;img alt="My story of underestimating /r/LocalLLaMA's thirst for VRAM" src="https://preview.redd.it/lwod7dtv7mdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b9563f79c6dad7bb50c952f2b76bf9062955b" title="My story of underestimating /r/LocalLLaMA's thirst for VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lwod7dtv7mdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qe2i88/my_story_of_underestimating_rlocalllamas_thirst/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T01:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qehf0p</id>
    <title>Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM</title>
    <updated>2026-01-16T14:28:44+00:00</updated>
    <author>
      <name>/u/reps_up</name>
      <uri>https://old.reddit.com/user/reps_up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"&gt; &lt;img alt="Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM" src="https://external-preview.redd.it/L3K-FrP5rshi6B9P4GPKPRWImqHp_K0A7GfSUbA2aKk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e8fca654e67fa2c4d7aa0fa049bfeb96b0c4231" title="Maxsun joins Sparkle in making Intel Arc B60 Pro GPUs available to regular consumers, with up to 48GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reps_up"&gt; /u/reps_up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.pcguide.com/news/maxsun-joins-sparkle-in-making-intel-arc-b60-pro-gpus-available-to-regular-consumers-with-up-to-48gb-vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qehf0p/maxsun_joins_sparkle_in_making_intel_arc_b60_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T14:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qennp2</id>
    <title>performance benchmarks (72GB VRAM) - llama.cpp server - January 2026</title>
    <updated>2026-01-16T18:15:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/"&gt; &lt;img alt="performance benchmarks (72GB VRAM) - llama.cpp server - January 2026" src="https://b.thumbs.redditmedia.com/srFJwhozimC0YMyd6HH4eWUiiMyisE_g6jJwdqP9TVg.jpg" title="performance benchmarks (72GB VRAM) - llama.cpp server - January 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is meant to demonstrate what models can (or can't) be realistically run and used on 72 GB VRAM.&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Three RTX 3090 GPUs&lt;/li&gt; &lt;li&gt;X399 motherboard + Ryzen Threadripper 1920X&lt;/li&gt; &lt;li&gt;DDR4 RAM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I use the default &lt;code&gt;llama-fit&lt;/code&gt; mechanism, so you can probably get better performance with manual &lt;code&gt;--n-cpu-moe&lt;/code&gt; or &lt;code&gt;-ot&lt;/code&gt; tuning.&lt;/p&gt; &lt;p&gt;I always use all three GPUs, smaller models often run faster with one or two GPUs.&lt;/p&gt; &lt;p&gt;I measure &lt;strong&gt;speed only&lt;/strong&gt;, not accuracy, this says nothing about the quality of these models.&lt;/p&gt; &lt;p&gt;This is &lt;strong&gt;not scientific at all&lt;/strong&gt; (see the screenshots). I simply generate two short sentences per model.&lt;/p&gt; &lt;p&gt;tokens/s:&lt;/p&gt; &lt;p&gt;ERNIE-4.5-21B-A3B-Thinking-Q8_0 ‚Äî &lt;strong&gt;147.85&lt;/strong&gt;&lt;br /&gt; Qwen_Qwen3-VL-30B-A3B-Instruct-Q8_0 ‚Äî &lt;strong&gt;131.20&lt;/strong&gt;&lt;br /&gt; gpt-oss-120b-mxfp4 ‚Äî &lt;strong&gt;130.23&lt;/strong&gt;&lt;br /&gt; nvidia_Nemotron-3-Nano-30B-A3B ‚Äî &lt;strong&gt;128.16&lt;/strong&gt;&lt;br /&gt; inclusionAI_Ling-flash-2.0-Q4_K_M ‚Äî &lt;strong&gt;116.49&lt;/strong&gt;&lt;br /&gt; GroveMoE-Inst.Q8_0 ‚Äî &lt;strong&gt;91.00&lt;/strong&gt;&lt;br /&gt; Qwen_Qwen3-Next-80B-A3B-Instruct-Q5_K_M ‚Äî &lt;strong&gt;68.58&lt;/strong&gt;&lt;br /&gt; Solar-Open-100B.q4_k_m ‚Äî &lt;strong&gt;67.15&lt;/strong&gt;&lt;br /&gt; ai21labs_AI21-Jamba2-Mini-Q8_0 ‚Äî &lt;strong&gt;58.53&lt;/strong&gt;&lt;br /&gt; ibm-granite_granite-4.0-h-small-Q8_0 ‚Äî &lt;strong&gt;57.79&lt;/strong&gt;&lt;br /&gt; GLM-4.5-Air-UD-Q4_K_XL ‚Äî &lt;strong&gt;54.31&lt;/strong&gt;&lt;br /&gt; Hunyuan-A13B-Instruct-UD-Q6_K_XL ‚Äî &lt;strong&gt;45.85&lt;/strong&gt;&lt;br /&gt; dots.llm1.inst-Q4_0 ‚Äî &lt;strong&gt;33.27&lt;/strong&gt;&lt;br /&gt; Llama-4-Scout-17B-16E-Instruct-Q5_K_M ‚Äî &lt;strong&gt;33.03&lt;/strong&gt;&lt;br /&gt; mistralai_Magistral-Small-2507-Q8_0 ‚Äî &lt;strong&gt;32.98&lt;/strong&gt;&lt;br /&gt; google_gemma-3-27b-it-Q8_0 ‚Äî &lt;strong&gt;26.96&lt;/strong&gt;&lt;br /&gt; MiniMax-M2.1-Q3_K_M ‚Äî &lt;strong&gt;24.68&lt;/strong&gt;&lt;br /&gt; EXAONE-4.0-32B.Q8_0 ‚Äî &lt;strong&gt;24.11&lt;/strong&gt;&lt;br /&gt; Qwen3-32B-Q8_0 ‚Äî &lt;strong&gt;23.67&lt;/strong&gt;&lt;br /&gt; allenai_Olmo-3.1-32B-Think-Q8_0 ‚Äî &lt;strong&gt;23.23&lt;/strong&gt;&lt;br /&gt; NousResearch_Hermes-4.3-36B-Q8_0 ‚Äî &lt;strong&gt;21.91&lt;/strong&gt;&lt;br /&gt; ByteDance-Seed_Seed-OSS-36B-Instruct-Q8_0 ‚Äî &lt;strong&gt;21.61&lt;/strong&gt;&lt;br /&gt; Falcon-H1-34B-Instruct-UD-Q8_K_XL ‚Äî &lt;strong&gt;19.56&lt;/strong&gt;&lt;br /&gt; Llama-3.3-70B-Instruct-Q4_K_M ‚Äî &lt;strong&gt;19.18&lt;/strong&gt;&lt;br /&gt; swiss-ai_Apertus-70B-Instruct-2509-Q4_K_M ‚Äî &lt;strong&gt;18.37&lt;/strong&gt;&lt;br /&gt; Qwen2.5-72B-Instruct-Q4_K_M ‚Äî &lt;strong&gt;17.51&lt;/strong&gt;&lt;br /&gt; Llama-3.3-Nemotron-Super-49B-v1_5-Q8_0 ‚Äî &lt;strong&gt;16.16&lt;/strong&gt;&lt;br /&gt; Qwen3-VL-235B-A22B-Instruct-Q3_K_M ‚Äî &lt;strong&gt;13.54&lt;/strong&gt;&lt;br /&gt; Mistral-Large-Instruct-2407-Q4_K_M ‚Äî &lt;strong&gt;6.40&lt;/strong&gt;&lt;br /&gt; grok-2.Q2_K ‚Äî &lt;strong&gt;4.63&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qennp2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qennp2/performance_benchmarks_72gb_vram_llamacpp_server/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T18:15:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qek917</id>
    <title>I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode.</title>
    <updated>2026-01-16T16:14:54+00:00</updated>
    <author>
      <name>/u/poisson_labs</name>
      <uri>https://old.reddit.com/user/poisson_labs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt; &lt;img alt="I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode." src="https://b.thumbs.redditmedia.com/zJw14CUcgynBl13gd34PUFykjliAEc5ivAIvrmPoRJA.jpg" title="I reproduced DeepSeek's mHC at 1.7B params (8xH100). The instability is 3x worse than reported (10k vs 3k), but the model didn't explode." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Following up on my previous post about reproducing the DeepSeek-V2/V3 architecture. I decided to bite the bullet and rent an H100 cluster to scale the &amp;quot;Hyper-Connections&amp;quot; (HC) experiment from 10M to 1.7B parameter&lt;/p&gt; &lt;p&gt;The DeepSeek paper warned that standard Hyper-Connections cause signal variance to explode by ~3,000x at 27B parameters. I wanted to see if that held true or if it was a theoretical upper bound.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;It's worse than they said.&lt;/strong&gt; At just 1.7B parameters, I measured signal amplification of &lt;strong&gt;10,924x&lt;/strong&gt;. The &amp;quot;Instability Bomb&amp;quot; is real.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Twist&amp;quot;:&lt;/strong&gt; Despite signals amplifying by 10,000x, the loss &lt;strong&gt;didn't diverge&lt;/strong&gt;. The model kept learning. My theory is that modern optimizers (AdamW) and gradient clipping work overtime to mask the issue, but it's basically a ticking time bomb for longer runs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Fix:&lt;/strong&gt; Verified that Manifold Hyper-Connections (mHC) with Sinkhorn projection completely solves this. Variance stays locked at 1.0x with zero compute overhead.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a1gsgd87kqdg1.png?width=4160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d75dc5207b1401eed9fe3a8e3425e24fe560fc0"&gt;https://preview.redd.it/a1gsgd87kqdg1.png?width=4160&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d75dc5207b1401eed9fe3a8e3425e24fe560fc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote up the full breakdown with the loss curves and Amax graphs here: &lt;a href="https://taylorkolasinski.com/notes/mhc-reproduction-part2/"&gt;https://taylorkolasinski.com/notes/mhc-reproduction-part2/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 1 can be found here: &lt;a href="https://taylorkolasinski.com/notes/mhc-reproduction/"&gt;https://taylorkolasinski.com/notes/mhc-reproduction/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also, there's a discussion on HN right now if you want to chat there: &lt;a href="https://news.ycombinator.com/newest?next=46647671&amp;amp;n=31"&gt;https://news.ycombinator.com/newest?next=46647671&amp;amp;n=31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions about the H100 setup or the implementation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/poisson_labs"&gt; /u/poisson_labs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qek917/i_reproduced_deepseeks_mhc_at_17b_params_8xh100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T16:14:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qeuh0z</id>
    <title>Prompt Repetition Improves Non-Reasoning LLMs - a paper</title>
    <updated>2026-01-16T22:35:01+00:00</updated>
    <author>
      <name>/u/Foreign-Beginning-49</name>
      <uri>https://old.reddit.com/user/Foreign-Beginning-49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/pdf/2512.14982"&gt;https://arxiv.org/pdf/2512.14982&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I love these little tiny prompt techniques that can potentially lead to greater model accuracy and performance. Simply repeating the prompt twice lead to notable performance gains.&lt;/p&gt; &lt;p&gt;From the paper:&lt;/p&gt; &lt;p&gt;&amp;quot;We show that repeating the prompts consistently improves model performance for a range of models and benchmarks, when not using reasoning. In addition, latency is not impacted, as only the parallelizable pre-fill stage is affected. Prompt repetition does not change the lengths or formats of the generated outputs, and it might be a good default for many models and tasks, when reasoning is not used.&lt;/p&gt; &lt;p&gt;So simple but they demonstrate impressive gains on several benchmark scores. Looks like Deepseek is the only open weights model put through the wringer.&lt;/p&gt; &lt;p&gt;Best of wishes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Foreign-Beginning-49"&gt; /u/Foreign-Beginning-49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qeuh0z/prompt_repetition_improves_nonreasoning_llms_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T22:35:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qee2de</id>
    <title>I fucking love this community</title>
    <updated>2026-01-16T11:57:48+00:00</updated>
    <author>
      <name>/u/alhinai_03</name>
      <uri>https://old.reddit.com/user/alhinai_03</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Thank you guys, thanks to everyone who took the time to write a comment or a post explaining, teaching people how things work, the people behind llama.cpp, vllm, and all the contributors who keep the open-source community thriving.&lt;/p&gt; &lt;p&gt;I'm able to run huge models on my weak ass pc from 10 years ago relatively fast, my fastest one being nemotron-3-nano-30B-a3b-iq4_nl running @14-13.5 t/s with 65k context. While my actual GPU having only 4GB of vram, that's fucking ridiculous and it blows my mind everytime that I'm able to run these models.&lt;/p&gt; &lt;p&gt;What's been key for me is having a good amount of system memory, and as long as the model is a MoE architecture they run pretty decently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alhinai_03"&gt; /u/alhinai_03 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qee2de/i_fucking_love_this_community/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T11:57:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qefa7q</id>
    <title>GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)</title>
    <updated>2026-01-16T12:59:07+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt; &lt;img alt="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="GPT-5.2 xhigh, GLM-4.7, Kimi K2 Thinking, DeepSeek v3.2 on Fresh SWE-rebench (December 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-bench leaderboard&lt;/strong&gt; with our &lt;strong&gt;December runs&lt;/strong&gt; on &lt;strong&gt;48 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). The setup is standard SWE-bench: models read real PR issues, edit code, run tests, and must make the full suite pass.&lt;/p&gt; &lt;p&gt;A few observations from this release:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Claude Opus 4.5&lt;/strong&gt; leads this snapshot at &lt;strong&gt;63.3% resolved rate&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-5.2 (extra high effort)&lt;/strong&gt; follows closely at &lt;strong&gt;61.5%&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemini 3 Flash Preview&lt;/strong&gt; slightly outperforms &lt;strong&gt;Gemini 3 Pro Preview&lt;/strong&gt; (60.0% vs 58.9%), despite being smaller and cheaper.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GLM-4.7&lt;/strong&gt; is currently the strongest open-source model on the leaderboard, ranking alongside closed models like GPT-5.1-codex.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS-120B&lt;/strong&gt; shows a large jump in performance when run in high-effort reasoning mode, highlighting the impact of inference-time scaling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your thoughts and feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=dec_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qefa7q/gpt52_xhigh_glm47_kimi_k2_thinking_deepseek_v32/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-16T12:59:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
