<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-17T14:24:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o91suj</id>
    <title>LLM speed on my system (R5 5600G, 5060Ti 16GB, 32GB RAM)</title>
    <updated>2025-10-17T13:47:48+00:00</updated>
    <author>
      <name>/u/R_dva</name>
      <uri>https://old.reddit.com/user/R_dva</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;LLM speed on my system (R5 5600G, 5060Ti 16GB, 32GB RAM)&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;I tested several models on my system, i asked &amp;quot;Write a short text about the future of technology&amp;quot;. Here are the results:&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Total Duration (s)&lt;/th&gt; &lt;th align="left"&gt;Load Duration (s)&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Count (tokens)&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Duration (ms)&lt;/th&gt; &lt;th align="left"&gt;Prompt Eval Rate (tokens/s)&lt;/th&gt; &lt;th align="left"&gt;Eval Count (tokens)&lt;/th&gt; &lt;th align="left"&gt;Eval Duration (s)&lt;/th&gt; &lt;th align="left"&gt;Eval Rate (tokens/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Gemma3:12B-IT-Q4_K_M&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;11.004048&lt;/td&gt; &lt;td align="left"&gt;6.0978792&lt;/td&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;39.096&lt;/td&gt; &lt;td align="left"&gt;460.41&lt;/td&gt; &lt;td align="left"&gt;198&lt;/td&gt; &lt;td align="left"&gt;4.7246764&lt;/td&gt; &lt;td align="left"&gt;41.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3-Coder:30B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16.0636496&lt;/td&gt; &lt;td align="left"&gt;8.3487872&lt;/td&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;158.467&lt;/td&gt; &lt;td align="left"&gt;107.28&lt;/td&gt; &lt;td align="left"&gt;236&lt;/td&gt; &lt;td align="left"&gt;7.4952974&lt;/td&gt; &lt;td align="left"&gt;31.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Mistral-Small3.2:24B-Instruct-2506-Q4_K_M&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;28.5862299&lt;/td&gt; &lt;td align="left"&gt;8.6925738&lt;/td&gt; &lt;td align="left"&gt;516&lt;/td&gt; &lt;td align="left"&gt;4340.0461&lt;/td&gt; &lt;td align="left"&gt;118.89&lt;/td&gt; &lt;td align="left"&gt;228&lt;/td&gt; &lt;td align="left"&gt;15.4800842&lt;/td&gt; &lt;td align="left"&gt;14.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3:30B-A3B-Thinking-2507-Q4_K_M&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;30.5642031&lt;/td&gt; &lt;td align="left"&gt;9.23035&lt;/td&gt; &lt;td align="left"&gt;19&lt;/td&gt; &lt;td align="left"&gt;180.8996&lt;/td&gt; &lt;td align="left"&gt;105.03&lt;/td&gt; &lt;td align="left"&gt;627&lt;/td&gt; &lt;td align="left"&gt;20.9965337&lt;/td&gt; &lt;td align="left"&gt;29.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPT-OSS:20B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;4.8795305&lt;/td&gt; &lt;td align="left"&gt;0.1652446&lt;/td&gt; &lt;td align="left"&gt;76&lt;/td&gt; &lt;td align="left"&gt;204.101&lt;/td&gt; &lt;td align="left"&gt;372.36&lt;/td&gt; &lt;td align="left"&gt;357&lt;/td&gt; &lt;td align="left"&gt;4.3407544&lt;/td&gt; &lt;td align="left"&gt;82.24&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key Takeaways:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GPT-OSS:20B&lt;/strong&gt; remains the fastest in both prompt evaluation (372.36 tokens/s) and response generation (82.24 tokens/s).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma3:12B-IT-Q4_K_M&lt;/strong&gt; shows strong prompt processing speed (460.41 tokens/s) but slower generation (41.91 tokens/s).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral-Small3.2:24B-Instruct-2506-Q4_K_M&lt;/strong&gt; has the highest prompt evaluation rate (118.89 tokens/s) but the slowest response generation (14.73 tokens/s).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3:30B-A3B-Thinking-2507-Q4_K_M&lt;/strong&gt; generates the longest outputs (627 tokens) but is slower in both prompt and response speed.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Testing was done with a browser running in the background, just normal PC usage with parallel testing&lt;/p&gt; &lt;p&gt;The question was simple, is there a universal question for the test? &lt;/p&gt; &lt;p&gt;Test was made with this command from blobs folder -&amp;quot;ollama run gpt-oss:20b p &amp;quot;Write a short text about future technologies.&amp;quot; --verbose&amp;quot; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R_dva"&gt; /u/R_dva &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o91suj/llm_speed_on_my_system_r5_5600g_5060ti_16gb_32gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o91suj/llm_speed_on_my_system_r5_5600g_5060ti_16gb_32gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o91suj/llm_speed_on_my_system_r5_5600g_5060ti_16gb_32gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T13:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8st45</id>
    <title>Built Overtab: An On-device AI browsing assistant powered by Gemini Nano (no cloud, no data sent out)!</title>
    <updated>2025-10-17T05:20:11+00:00</updated>
    <author>
      <name>/u/Consistent_One7493</name>
      <uri>https://old.reddit.com/user/Consistent_One7493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone üëã&lt;/p&gt; &lt;p&gt;I‚Äôve been obsessed with making browsing smarter, so I built what I wished existed: &lt;strong&gt;Overtab&lt;/strong&gt;, an on-device AI Chrome assistant I created for the &lt;strong&gt;Google Chrome Built-in AI Challenge 2025&lt;/strong&gt; that gives instant insights right in your browser.&lt;/p&gt; &lt;p&gt;Highlight text, ask by voice, or right-click images: all processed locally with &lt;strong&gt;Gemini Nano&lt;/strong&gt;!&lt;br /&gt; (And if you don‚Äôt have Nano set up yet, there‚Äôs an OpenAI fallback!)&lt;/p&gt; &lt;p&gt;üé¨ &lt;a href="https://www.youtube.com/watch?v=Wq5pnpnK9r0"&gt;Demo Video&lt;/a&gt; | üåê &lt;a href="https://chromewebstore.google.com/detail/overtab/oloejollcmhnbacdkfgbdlgcgbeegcje"&gt;Chrome Web Store&lt;/a&gt; | üíª &lt;a href="https://github.com/riyanshibohra/Overtab"&gt;GitHub&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Consistent_One7493"&gt; /u/Consistent_One7493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8st45/built_overtab_an_ondevice_ai_browsing_assistant/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8st45/built_overtab_an_ondevice_ai_browsing_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8st45/built_overtab_an_ondevice_ai_browsing_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T05:20:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m1v0</id>
    <title>I got Kokoro TTS running natively on iOS! üéâ Natural-sounding speech synthesis entirely on-device</title>
    <updated>2025-10-16T23:44:22+00:00</updated>
    <author>
      <name>/u/Living_Commercial_10</name>
      <uri>https://old.reddit.com/user/Living_Commercial_10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Just wanted to share something cool I built this weekend.&lt;/p&gt; &lt;p&gt;I managed to get Kokoro TTS (the high-quality open-source text-to-speech model) running completely natively on iOS - no server, no API calls, 100% on-device inference!&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Converts text to natural-sounding speech directly on your iPhone/iPad&lt;/li&gt; &lt;li&gt;Uses the full ONNX model (325MB) with real voice embeddings&lt;/li&gt; &lt;li&gt;50+ voices in multiple languages (English, Spanish, French, Japanese, Chinese, etc.)&lt;/li&gt; &lt;li&gt;24kHz audio output at ~4 seconds generation time for a sentence&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The audio quality is surprisingly good! It's not real-time yet (takes a few seconds per sentence), but for a 325MB model running entirely on a phone with no quantization, I'm pretty happy with it. &lt;/p&gt; &lt;p&gt;Planning on integrating it in my iOS apps. &lt;/p&gt; &lt;p&gt;Has anyone else tried running TTS models locally on mobile? Would love to hear about your experiences!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Living_Commercial_10"&gt; /u/Living_Commercial_10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m1v0/i_got_kokoro_tts_running_natively_on_ios/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m1v0/i_got_kokoro_tts_running_natively_on_ios/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m1v0/i_got_kokoro_tts_running_natively_on_ios/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:44:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8c9ta</id>
    <title>new 1B LLM by meta</title>
    <updated>2025-10-16T17:20:06+00:00</updated>
    <author>
      <name>/u/BreakfastFriendly728</name>
      <uri>https://old.reddit.com/user/BreakfastFriendly728</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt; &lt;img alt="new 1B LLM by meta" src="https://external-preview.redd.it/jpWPa_miOIzMa-yE2f-cKYYuEKEP1Bglx0QJxru_lGs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e2417b6932d5a1fcaf49ac7d2561558b6a9873e" title="new 1B LLM by meta" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41"&gt;https://preview.redd.it/5adjo04ecivf1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c09b47aee7f7597d580d6dbe1eec7e6429504d41&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;facebook/MobileLLM-Pro ¬∑ Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BreakfastFriendly728"&gt; /u/BreakfastFriendly728 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8c9ta/new_1b_llm_by_meta/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T17:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8n9ym</id>
    <title>We built an open-source coding agent CLI that can be run locally</title>
    <updated>2025-10-17T00:41:19+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8n9ym/we_built_an_opensource_coding_agent_cli_that_can/"&gt; &lt;img alt="We built an open-source coding agent CLI that can be run locally" src="https://preview.redd.it/k22utme4jkvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7cd5da76109133d6785e5504cde25419f8caf5ff" title="We built an open-source coding agent CLI that can be run locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically, it‚Äôs like Claude Code but with native support for local LLMs and a universal tool parser that works even on inference platforms without built-in tool call support.&lt;/p&gt; &lt;p&gt;Kolosal CLI is an open-source, cross-platform agentic command-line tool that lets you discover, download, and run models locally using an ultra-lightweight inference server. It supports coding agents, Hugging Face model integration, and a memory calculator to estimate model memory requirements.&lt;/p&gt; &lt;p&gt;It‚Äôs a fork of Qwen Code, and we also host GLM 4.6 and Kimi K2 if you prefer to use them without running them yourself.&lt;/p&gt; &lt;p&gt;You can try it at kolosal.ai and check out the source code on GitHub: github.com/KolosalAI/kolosal-cli&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k22utme4jkvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8n9ym/we_built_an_opensource_coding_agent_cli_that_can/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8n9ym/we_built_an_opensource_coding_agent_cli_that_can/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T00:41:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1o91thq</id>
    <title>The real 'privacy' challenge for local models isn't just data on disk, it's the biometric key.</title>
    <updated>2025-10-17T13:48:32+00:00</updated>
    <author>
      <name>/u/ExtraAlien1</name>
      <uri>https://old.reddit.com/user/ExtraAlien1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We spend a lot of time here talking about running models locally for privacy and control over our data. We hash out quantization, VRAM, and efficient inference. But I think we're overlooking a criticl, persistet privacy challenge that's already solved by external AI: biometric identity fuson.... I ran a test on my own fragmented digital presence, partly out of paranoia. I used faceseek as an external benchmark. I uploaded a low res photo of myself that was only ever on a private, archived forum..... The external tool immediately linked that photo to two completely separate, pseudonymous accounts I use locally, offline, for testing personal LLM projects. This implies the AI has already generated a permanent biometric template of me that can unify identities across platforms, regardless of local storage or pseudonymity. If external AI can already stitch together our separate online and offline personas using just a face, what does that mean for the long-term privacy guarantees of local models? Even if our local LLM never leaves our machine, our identity can be mapped to its outputs if that identity is indexed elsewhere. How do we build truly private local agent architectures when our biometric identity is already public?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExtraAlien1"&gt; /u/ExtraAlien1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o91thq/the_real_privacy_challenge_for_local_models_isnt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o91thq/the_real_privacy_challenge_for_local_models_isnt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o91thq/the_real_privacy_challenge_for_local_models_isnt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T13:48:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8pj1k</id>
    <title>Waiting on Ryzen Max 395+ w/ 128gb RAM to be delivered. How should I set it up for AI?</title>
    <updated>2025-10-17T02:28:29+00:00</updated>
    <author>
      <name>/u/atomicpapa210</name>
      <uri>https://old.reddit.com/user/atomicpapa210</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title pretty much says it all.&lt;/p&gt; &lt;p&gt;Beelink GTR9 Pro&lt;br /&gt; Ryzen Max AI 395+&lt;br /&gt; 128 gb LPDDR5x-8000&lt;br /&gt; 2TB SSD&lt;br /&gt; Radeon 8060S iGPU&lt;/p&gt; &lt;p&gt;Comes with Windows 11&lt;/p&gt; &lt;p&gt;Planning on using it for Home Assistant and learning more about AI&lt;/p&gt; &lt;p&gt;Should I switch to Linux? This is of course what I am leaning toward.&lt;br /&gt; What should I run for AI? Lemonade Server? Something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atomicpapa210"&gt; /u/atomicpapa210 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8pj1k/waiting_on_ryzen_max_395_w_128gb_ram_to_be/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8pj1k/waiting_on_ryzen_max_395_w_128gb_ram_to_be/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8pj1k/waiting_on_ryzen_max_395_w_128gb_ram_to_be/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T02:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o866vl</id>
    <title>PaddleOCR-VL, is better than private models</title>
    <updated>2025-10-16T13:29:48+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt; &lt;img alt="PaddleOCR-VL, is better than private models" src="https://b.thumbs.redditmedia.com/X4U2z8D2mUefEIEuBe11hVIGCJrPT-oF1EOiA8c0dyw.jpg" title="PaddleOCR-VL, is better than private models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19"&gt;https://x.com/PaddlePaddle/status/1978809999263781290?t=mcHYAF7osq3MmicjMLi0IQ&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o866vl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o866vl/paddleocrvl_is_better_than_private_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T13:29:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x4ta</id>
    <title>just added Qwen3-VL support for MNN Chat android</title>
    <updated>2025-10-17T09:55:15+00:00</updated>
    <author>
      <name>/u/Juude89</name>
      <uri>https://old.reddit.com/user/Juude89</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt; &lt;img alt="just added Qwen3-VL support for MNN Chat android" src="https://external-preview.redd.it/zNY5eQnZ-x8ORX22UUI4aGsd0-StGUHm-Z-wi5X4Vb4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d5a7f8b045bac80ec011d43c579c53bf810f8f7" title="just added Qwen3-VL support for MNN Chat android" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1o8x4ta/video/juu7ycgm9nvf1/player"&gt;https://reddit.com/link/1o8x4ta/video/juu7ycgm9nvf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also support qwen3-vl-4b and qwen3-vl-8b &lt;/p&gt; &lt;p&gt;Download the 0.7.5version to experience: &lt;a href="https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-075"&gt;https://github.com/alibaba/MNN/blob/master/apps/Android/MnnLlmChat/README.md#version-075&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juude89"&gt; /u/Juude89 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x4ta/just_added_qwen3vl_support_for_mnn_chat_android/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:55:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8xd0p</id>
    <title>How do you define acceptance criteria when delivering LLM projects for companies?</title>
    <updated>2025-10-17T10:09:10+00:00</updated>
    <author>
      <name>/u/piske_usagi</name>
      <uri>https://old.reddit.com/user/piske_usagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, I‚Äôd like to ask‚Äîwhen you take on large language model (LLM) projects for companies, how do you usually discuss and agree on acceptance criteria?&lt;/p&gt; &lt;p&gt;My initial idea was to collaborate with the client to build an evaluation set (perhaps in the form of multiple-choice questions), and once the model achieves a mutually agreed score, it would be considered successful.&lt;/p&gt; &lt;p&gt;However, I‚Äôve found that most companies that commission these projects have trouble accepting this approach. First, they often struggle to translate their internal knowledge into concrete evaluation steps. Second, they tend to rely more on subjective impressions to judge whether the model performs well or not.&lt;/p&gt; &lt;p&gt;I‚Äôm wondering how others handle this situation‚Äîany experiences or frameworks you can share? Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/piske_usagi"&gt; /u/piske_usagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8xd0p/how_do_you_define_acceptance_criteria_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T10:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8k4gc</id>
    <title>DGX Spark is here, give me your non-inference workloads</title>
    <updated>2025-10-16T22:20:50+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"&gt; &lt;img alt="DGX Spark is here, give me your non-inference workloads" src="https://preview.redd.it/17hmoj42ujvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aca471462e384551e5b0d09c075c93f99e741ab1" title="DGX Spark is here, give me your non-inference workloads" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just received my DGX Spark. We all know it's trash for inference, so give me your non-inference test ideas (e.g., RL) to see what else it's trash at. I can also compare the numbers with my 4090 and H100.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/17hmoj42ujvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8k4gc/dgx_spark_is_here_give_me_your_noninference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T22:20:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8b00e</id>
    <title>GLM 4.6 is hilarious, I wish I could run this on my own PC lol</title>
    <updated>2025-10-16T16:34:48+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Prompt:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I have a fun task for you! Search the web to find the most (infamously) popular annoying LLM clich√©s (aka AI slop) in roleplay and creative writing and generate a sentence using only those phrases. Make the sentence as long as possible to cover them all.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;GLM 4.6 response:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Based on the search results, I've compiled the most notoriously annoying LLM clich√©s in roleplay and creative writing. Here's a sentence that combines as many of these overused phrases as possible:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;In a world where the tapestry of human emotions unfolds like a delicate symphony, I'm sorry, but as an AI language model, I cannot delve into unlocking your full potential without first embarking on a journey to harness the vast landscape of creativity, though I'm just an AI and don't have feelings, but I'm here to help you explore the intricate dance of storytelling where every thread weaves a new chapter in the grand narrative of imagination.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8b00e/glm_46_is_hilarious_i_wish_i_could_run_this_on_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:34:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8h9ye</id>
    <title>China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down</title>
    <updated>2025-10-16T20:26:18+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"&gt; &lt;img alt="China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down" src="https://external-preview.redd.it/h0AXu77KNbLZvwy9Mngk-UUEgeAjkwRBjGOdlujFn7U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=436c9e5446c362e3900f824dd945b76349b991ff" title="China's GPU Competition: 96GB Huawei Atlas 300I Duo Dual-GPU Tear-Down" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We need benchmarks ..&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/qGe_fq68x-Q?si=0w_3qjLvmHTXPWxI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8h9ye/chinas_gpu_competition_96gb_huawei_atlas_300i_duo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8q4xt</id>
    <title>North Dakota using Llama3.2 1B with Ollama to summarize bills</title>
    <updated>2025-10-17T02:57:56+00:00</updated>
    <author>
      <name>/u/SM8085</name>
      <uri>https://old.reddit.com/user/SM8085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Didn't see this posted here yet.&lt;/p&gt; &lt;p&gt;Apparently North Dakota has been using &lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"&gt;Llama3.2 1B&lt;/a&gt; with Ollama to summarize their bills and are seeing positive results.&lt;/p&gt; &lt;p&gt;Video: &lt;a href="https://www.youtube.com/watch?v=PYqH1aYhLY0"&gt;North Dakota Legislature innovates with AI - KX News (Youtube)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm surprised they went with Llama3.2 1B, but I think it's interesting they're using a local model.&lt;/p&gt; &lt;p&gt;Somebody in ND had a spare raspberry pi 5 to give the state an AI system?&lt;/p&gt; &lt;p&gt;When I mention summarizing things with small models 4B and under people will ask what kind of accuracy I get and I'm never sure how to quantify it. I get nervous with bots under 2B, but maybe less is more when you're asking them to simply summarize things without injecting what they may or may not know on the subject?&lt;/p&gt; &lt;p&gt;I'll have to check how many bills are over 128k tokens long. I wonder what their plan is at that point? I suppose just do it the old fashioned way.&lt;/p&gt; &lt;p&gt;What does &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; think about this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SM8085"&gt; /u/SM8085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://markets.financialcontent.com/stocks/article/tokenring-2025-10-15-north-dakota-pioneers-ai-in-government-legislative-council-adopts-meta-ai-to-revolutionize-bill-summarization"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8q4xt/north_dakota_using_llama32_1b_with_ollama_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8q4xt/north_dakota_using_llama32_1b_with_ollama_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T02:57:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8no7i</id>
    <title>Best Open Source TTS That Sounds Most Natural Voice For Storytelling? That You Can Run With 12GB Vram?</title>
    <updated>2025-10-17T01:00:05+00:00</updated>
    <author>
      <name>/u/Head-Investigator540</name>
      <uri>https://old.reddit.com/user/Head-Investigator540</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last I heard Higgs was great - but have heard it takes 24gb vram (and I only have 12GB on my machine). So wanted to see if anyone had suggested on the best free to use (commercial or otherwise) that I can run from my own machine.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Head-Investigator540"&gt; /u/Head-Investigator540 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8no7i/best_open_source_tts_that_sounds_most_natural/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T01:00:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8hacw</id>
    <title>Since DGX Spark is a disappointment... What is the best value for money hardware today?</title>
    <updated>2025-10-16T20:26:47+00:00</updated>
    <author>
      <name>/u/goto-ca</name>
      <uri>https://old.reddit.com/user/goto-ca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My current compute box (2√ó1080 Ti) is failing, so I‚Äôve been renting GPUs by the hour. I‚Äôd been waiting for DGX Spark, but early reviews look disappointing for the price/perf.&lt;/p&gt; &lt;p&gt;I‚Äôm ready to build a new PC and I‚Äôm torn between a &lt;strong&gt;single&lt;/strong&gt; high-end GPU or &lt;strong&gt;dual&lt;/strong&gt; mid/high GPUs. What‚Äôs the &lt;strong&gt;best price/performance configuration&lt;/strong&gt; I can build &lt;strong&gt;for ‚â§ $3,999&lt;/strong&gt; (tower, not a rack server)?&lt;/p&gt; &lt;p&gt;I don't care about RGBs and things like that - it will be kept in the basement and not looked at. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/goto-ca"&gt; /u/goto-ca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8hacw/since_dgx_spark_is_a_disappointment_what_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T20:26:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8sbv1</id>
    <title>üöÄ HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs</title>
    <updated>2025-10-17T04:52:56+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"&gt; &lt;img alt="üöÄ HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs" src="https://preview.redd.it/tmc0nl14pjvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6edd535c0dbc7303cd47479e890c56a0f66c45b" title="üöÄ HuggingFaceChat Omni: Dynamic policy-baed routing to 115+ LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing: &lt;a href="https://github.com/huggingface/chat-ui"&gt;HuggingChat Omni &lt;/a&gt; &lt;/p&gt; &lt;p&gt;Select the best model for every prompt automatically &lt;/p&gt; &lt;p&gt;- Automatic model selection for your queries&lt;br /&gt; - 115 models available across 15 providers &lt;/p&gt; &lt;p&gt;Available now all Hugging Face users. 100% open source.&lt;/p&gt; &lt;p&gt;Omni uses a policy-based approach to model selection (after experimenting with different methods). Credits to &lt;a href="https://huggingface.co/katanemo"&gt;Katanemo&lt;/a&gt; for their small routing model: &lt;a href="https://huggingface.co/katanemo/Arch-Router-1.5B"&gt;katanemo/Arch-Router-1.5B&lt;/a&gt;. The model is natively integrated in &lt;a href="https://github.com/katanemo/archgw"&gt;archgw&lt;/a&gt; for those who want to build their own chat experiences with policy-based dynamic routing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tmc0nl14pjvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8sbv1/huggingfacechat_omni_dynamic_policybaed_routing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T04:52:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8ay34</id>
    <title>GLM 4.6 air when?</title>
    <updated>2025-10-16T16:32:48+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt; &lt;img alt="GLM 4.6 air when?" src="https://preview.redd.it/1bhgri4w3ivf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=660b0d931d85bfb508e26c3e8647b3b8a75f8708" title="GLM 4.6 air when?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1bhgri4w3ivf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8ay34/glm_46_air_when/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T16:32:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o90jvf</id>
    <title>do 2x MCIO to PCIe x16 adapters exist?</title>
    <updated>2025-10-17T12:55:48+00:00</updated>
    <author>
      <name>/u/MelodicRecognition7</name>
      <uri>https://old.reddit.com/user/MelodicRecognition7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90jvf/do_2x_mcio_to_pcie_x16_adapters_exist/"&gt; &lt;img alt="do 2x MCIO to PCIe x16 adapters exist?" src="https://b.thumbs.redditmedia.com/TJ59uHrnaDqTSwhgsFixmhauzFcf1mPJXYCciNQy3pE.jpg" title="do 2x MCIO to PCIe x16 adapters exist?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want some kind of a &amp;quot;reverse bifurcation&amp;quot;, 2 separate x8 ports combined into one x16. Is it possible to insert a x16 GPU into these two MCIO x8 ports? I've found some cables but not sure if they will work. Where do I put that 4 pin cable on the 2nd pic? Will the adapter on the 3rd pic work if I ditch the left card and plug both cables directly into the motherboard? Any other ways of expanding PCIe x16 slots on Supermicro H13SSL or H14SSL? These motherboards have just 3 full size PCIe slots.&lt;/p&gt; &lt;p&gt;Edit: motherboard manual shows that PCIe1A and PCIe1B are connected to one PCIe x16 port, however there is no information about possibility to recombine two MCIO x8 into one PCIe x16.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MelodicRecognition7"&gt; /u/MelodicRecognition7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o90jvf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90jvf/do_2x_mcio_to_pcie_x16_adapters_exist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o90jvf/do_2x_mcio_to_pcie_x16_adapters_exist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T12:55:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8x2w0</id>
    <title>What in the Black Friday hell is happening with the DDR5-5600 128GB SODIMM kits ?</title>
    <updated>2025-10-17T09:51:44+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In summer Amazon was selling them with something like 320‚Ç¨, not they are almost 500‚Ç¨ and increasing, I wanted to update my 64GB to 128, but this is obscene :(&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8x2w0/what_in_the_black_friday_hell_is_happening_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:51:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1o90ixr</id>
    <title>LlamaBarn ‚Äî A macOS menu bar app for running local LLMs (open source)</title>
    <updated>2025-10-17T12:54:41+00:00</updated>
    <author>
      <name>/u/erusev_</name>
      <uri>https://old.reddit.com/user/erusev_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"&gt; &lt;img alt="LlamaBarn ‚Äî A macOS menu bar app for running local LLMs (open source)" src="https://preview.redd.it/nmcd9kwwvnvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c988f1b4b79b949ebee5a449831ea15aab801ef" title="LlamaBarn ‚Äî A macOS menu bar app for running local LLMs (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;code&gt;r/LocalLLaMA&lt;/code&gt;! We just released this in beta and would love to get your feedback.&lt;/p&gt; &lt;p&gt;Here: &lt;a href="https://github.com/ggml-org/LlamaBarn"&gt;https://github.com/ggml-org/LlamaBarn&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What it does: - Download models from a curated catalog - Run models with one click ‚Äî it auto-configures them for your system - Built-in web UI and REST API (via &lt;code&gt;llama.cpp&lt;/code&gt; server)&lt;/p&gt; &lt;p&gt;It's a small native app (~12 MB, 100% Swift) that wraps &lt;code&gt;llama.cpp&lt;/code&gt; to make running local models easier.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/erusev_"&gt; /u/erusev_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nmcd9kwwvnvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o90ixr/llamabarn_a_macos_menu_bar_app_for_running_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T12:54:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m0ti</id>
    <title>We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source</title>
    <updated>2025-10-16T23:43:03+00:00</updated>
    <author>
      <name>/u/TerrificMist</name>
      <uri>https://old.reddit.com/user/TerrificMist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt; &lt;img alt="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" src="https://b.thumbs.redditmedia.com/m8yoOZ6gwR1CBn-AO5mIRDQSzZ5G3sXEIoAl_NdSUMk.jpg" title="We built 3B and 8B models that rival GPT-5 at HTML extraction while costing 40-80x less - fully open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;Disclaimer: I work for&lt;/em&gt; &lt;a href="http://Inference.net"&gt;&lt;em&gt;Inference.net&lt;/em&gt;&lt;/a&gt;&lt;em&gt;, creator of the Schematron model family&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Hey everyone, wanted to share something we've been working on at Inference.net: Schematron, a family of small models for web extraction.&lt;/p&gt; &lt;p&gt;Our goal was to make a small, fast model for taking HTML from website and extracting JSON that perfectly adheres to a schema.&lt;/p&gt; &lt;p&gt;We distilled a frontier model down to 8B params and managed to keep basically all the output quality for this task. Schematron-8B scores 4.64 on LLM-as-a-judge evals vs GPT-4.1's 4.74 and Gemma 3B's 2.24. Schematron-3B scores 4.41 while being even faster. The main benefit of this model is that it costs 40-80x less than GPT-5 at comparable quality (slightly worse than GPT-5, better than Gemini 2-5 Flash).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Technical details:&lt;/strong&gt; We fine-tuned &lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B&lt;/a&gt;, expanded it to a 128K context window, quantized to FP8 without quality loss, and trained until it outputted strict JSON with 100% schema compliance. We also built a smaller 3B variant that's even cheaper and faster, but still maintains most of the accuracy of the 8B variant. We recommend using the 3B for most tasks, and trying 8B if it fails or most of your documents are pushing the context limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How we trained it:&lt;/strong&gt; We started with 1M real web pages from Common Crawl and built a synthetic dataset by clustering websites and generating schemas that mirror real-world usage patterns. We used a frontier model as a teacher and applied curriculum learning to progressively train on longer context lengths--training with context parallelism and FSDP to scale efficiently--which is why the models stay accurate even at the 128K token limit.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; Processing 1 million pages daily with GPT-5 would cost you around $20,000. With Schematron-8B, that same workload runs about $480. With Schematron-3B, it's $240.&lt;/p&gt; &lt;p&gt;The speed matters too. Schematron processes pages 10x faster than frontier models. On average, Schamatron can scrape a page in 0.54 seconds, compared to 6 seconds for GPT-5. These latency gains compound very quickly for something like a browser-use agent.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Real-world impact on LLM factuality:&lt;/strong&gt; We tested this on SimpleQA to see how much it improves accuracy when paired with web search. When GPT-5 Nano was paired with Schematron-8B to extract structured data from search results provided by Exa, it went from answering barely any questions correctly (8.54% on SimpleQA) to getting over 85% right. The structured extraction approach means this was done processing lean, clean JSON (very little additional cost) instead of dumping ~8k tokens of raw HTML into your context window per page retrieved (typically LLMs are grounded with 5-10 pages/search).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Getting started:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you're using our &lt;a href="https://inference.net/models/schematron-3b"&gt;serverless API&lt;/a&gt;, you only need to pass your Pydantic, Zod, or JSON Schema and the HTML. We handle all the prompting in the backend for you in the backend. You get $10 in free credits to start.&lt;/p&gt; &lt;p&gt;If you're running locally, there are a few things to watch out for. You need to follow the prompting guidelines carefully and make sure you're using structured extraction properly, otherwise the model won't perform as well. &lt;/p&gt; &lt;p&gt;The models are on &lt;a href="https://huggingface.co/inference-net/Schematron-3B"&gt;HuggingFace&lt;/a&gt; and &lt;a href="https://ollama.com/Inference/Schematron"&gt;Ollama&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Full benchmarks and code examples are in our blog post: &lt;a href="https://inference.net/blog/schematron"&gt;https://inference.net/blog/schematron&lt;/a&gt;, &lt;a href="https://docs.inference.net/workhorse-models/schematron"&gt;docs&lt;/a&gt;, and &lt;a href="https://github.com/context-labs/inference-samples/blob/main/examples/schematron-scrape-companies/schematron-scrape-companies.ipynb"&gt;samples repo&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Happy to answer any technical questions about the training process or architecture. Also interested in how this would be helpful in your current scraping workflows!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerrificMist"&gt; /u/TerrificMist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8m0ti"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m0ti/we_built_3b_and_8b_models_that_rival_gpt5_at_html/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8m5ua</id>
    <title>Meta just dropped MobileLLM-Pro, a new 1B foundational language model on Huggingface</title>
    <updated>2025-10-16T23:49:16+00:00</updated>
    <author>
      <name>/u/Sad_Consequence5629</name>
      <uri>https://old.reddit.com/user/Sad_Consequence5629</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta just published MobileLLM-Pro, a new 1B parameter foundational language model (pre-trained and instruction fine-tuned) on Huggingface&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/facebook/MobileLLM-Pro"&gt;https://huggingface.co/facebook/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model seems to outperform Gemma 3-1B and Llama 3-1B by quite a large margin in pre-training and shows decent performance after instruction-tuning (Looks like it works pretty well for API calling, rewriting, coding and summarization).&lt;br /&gt; The model is already in GradIO and can be directly chatted with in the browser:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/akhaliq/MobileLLM-Pro"&gt;https://huggingface.co/spaces/akhaliq/MobileLLM-Pro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Tweet source: &lt;a href="https://x.com/_akhaliq/status/1978916251456925757"&gt;https://x.com/_akhaliq/status/1978916251456925757&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Consequence5629"&gt; /u/Sad_Consequence5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8m5ua/meta_just_dropped_mobilellmpro_a_new_1b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-16T23:49:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8wuyj</id>
    <title>Valve Developer Contributes Major Improvement To RADV Vulkan For Llama.cpp AI</title>
    <updated>2025-10-17T09:37:37+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/RADV-Valve-Boost-Llama.cpp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8wuyj/valve_developer_contributes_major_improvement_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T09:37:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o8uxh6</id>
    <title>Write three times the word potato</title>
    <updated>2025-10-17T07:32:29+00:00</updated>
    <author>
      <name>/u/TooManyPascals</name>
      <uri>https://old.reddit.com/user/TooManyPascals</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt; &lt;img alt="Write three times the word potato" src="https://b.thumbs.redditmedia.com/VWK4WzyVVfvV7xJuANrLzK-bH1UfvcQckXM3kS4Llno.jpg" title="Write three times the word potato" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing how well Qwen3-0.6B could follow simple instructions... &lt;/p&gt; &lt;p&gt;and it accidentally created a trolling masterpiece.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TooManyPascals"&gt; /u/TooManyPascals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1o8uxh6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o8uxh6/write_three_times_the_word_potato/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-17T07:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
