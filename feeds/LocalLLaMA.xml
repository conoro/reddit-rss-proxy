<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-07-31T13:18:26+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1mdq3sv</id>
    <title>Ollama 0.10 - New app is available for macOS and Windows plus multi-GPU performance improvements, and more</title>
    <updated>2025-07-31T02:43:03+00:00</updated>
    <author>
      <name>/u/mj3815</name>
      <uri>https://old.reddit.com/user/mj3815</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdq3sv/ollama_010_new_app_is_available_for_macos_and/"&gt; &lt;img alt="Ollama 0.10 - New app is available for macOS and Windows plus multi-GPU performance improvements, and more" src="https://external-preview.redd.it/8CQoAySJDDT43ePa2z6wKZ6f67awzR1xeHnq-ctSP9Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d42b6edc645f624b40e5a4c076cb7f9f25ed0e3f" title="Ollama 0.10 - New app is available for macOS and Windows plus multi-GPU performance improvements, and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mj3815"&gt; /u/mj3815 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.10.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdq3sv/ollama_010_new_app_is_available_for_macos_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdq3sv/ollama_010_new_app_is_available_for_macos_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T02:43:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdvkhz</id>
    <title>ik_llama.cpp and Qwen 3 30B-A3B architecture.</title>
    <updated>2025-07-31T07:57:22+00:00</updated>
    <author>
      <name>/u/Bycbka</name>
      <uri>https://old.reddit.com/user/Bycbka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvkhz/ik_llamacpp_and_qwen_3_30ba3b_architecture/"&gt; &lt;img alt="ik_llama.cpp and Qwen 3 30B-A3B architecture." src="https://external-preview.redd.it/2UbIzGryv92r-OTNNbwj3X7DPvZqNJtHJ_N32Ju1bQs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a74fa58f6c27cd63b1b4175d767d3aa5e620d4e6" title="ik_llama.cpp and Qwen 3 30B-A3B architecture." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big shout out to ikawrakow and his &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt; for making my hardware relevant (and obviously Qwen team!) :)&lt;/p&gt; &lt;p&gt;Looking forward to trying Thinker and Coder versions of this architecture&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9xttfh3026gf1.png?width=2216&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc6e39266d0a94beb5dca73650dab93021bb7d32"&gt;https://preview.redd.it/9xttfh3026gf1.png?width=2216&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cc6e39266d0a94beb5dca73650dab93021bb7d32&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hardware: AMD Ryzen 9 8945HS(8C/16T, up to 5.2GHz) 64GB DDR5 1TB PCIe4.0 SSD, running in Ubuntu distrobox with Fedora Bluefin as a host. Also have eGPU with RTX 3060 12GB, but it was not used in benchmark.&lt;/p&gt; &lt;p&gt;I tried CPU + CUDA separately - and the prompt processing speed would take a significant hit (many memory trips I guess). I did try to use the &amp;quot;-ot exps&amp;quot; trick to ensure correct layer split - but I think it is expected, as this is the cost of offloading.&lt;/p&gt; &lt;p&gt;-fa -rtr -fmoe made prompt processing around 20-25% faster.&lt;/p&gt; &lt;p&gt;Models of this architecture are very snappy in CPU mode, especially on smaller prompts - good feature for daily driver model. With longer contexts, processing speed drops significantly, so will require orchestration / workflows to prevent context from blowing up.&lt;/p&gt; &lt;p&gt;Vibes-wise, this model feels strong for something that runs on &amp;quot;consumer&amp;quot; hardware at these speeds.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What was tested:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General conversations - good enough, but to be honest almost every 4B+ model feels like an ok conversationalist - what a time to be alive, no?&lt;/li&gt; &lt;li&gt;Code doc summarization: good. I fed it 16k-30k documents and while the speed was slow, the overall result was decent.&lt;/li&gt; &lt;li&gt;Retrieval: gave it ~10k tokens worth of logs and asked some questions about data that appeared in the logs - mostly good, but I would not call it laser-good.&lt;/li&gt; &lt;li&gt;Coding + Tool calling in Zed editor- it is obviously not Sonnet or GPT 4.1, but it really tries! I think with better prompting / fine-tuning it would crack it - perhaps it's seen different tools during original training.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Can I squeeze more?:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Better use for GPU?&lt;/li&gt; &lt;li&gt;Try other quants: there was a plethora of quants added in recent weeks - perhaps there is one that will push these numbers a little up.&lt;/li&gt; &lt;li&gt;Try &lt;a href="https://github.com/kvcache-ai/ktransformers"&gt;https://github.com/kvcache-ai/ktransformers&lt;/a&gt; - they are known for optimized configs to run on RAM + relatively low amount of VRAM - but I failed to make it work locally and didn't find an up-to-date docker image either. I would imagine it's not gonna yield significant improvements, but happy to be proven wrong.&lt;/li&gt; &lt;li&gt;IGPU + Vulcan?&lt;/li&gt; &lt;li&gt;NPU xD&lt;/li&gt; &lt;li&gt;Test full context (or the largest context that does not take eternity to process)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;What's your experience / recipe for similarly-sized hardware setup?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bycbka"&gt; /u/Bycbka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvkhz/ik_llamacpp_and_qwen_3_30ba3b_architecture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvkhz/ik_llamacpp_and_qwen_3_30ba3b_architecture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvkhz/ik_llamacpp_and_qwen_3_30ba3b_architecture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T07:57:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdqj9g</id>
    <title>works well!: GLM 4.5 air (MLX) - LM studio (Mac) - Claude code</title>
    <updated>2025-07-31T03:03:51+00:00</updated>
    <author>
      <name>/u/ziozzang0</name>
      <uri>https://old.reddit.com/user/ziozzang0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;How I Got claude-code to Work with a Local LLM (via LM Studio) Using a Custom Proxy&lt;/h1&gt; &lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a little setup I put together. I was trying to run &lt;code&gt;claude-code&lt;/code&gt; with a locally hosted model, &lt;code&gt;glm-4.5-air&lt;/code&gt;, through &lt;strong&gt;LM Studio on my Mac&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I ran into some issues, so I quickly whipped up a proxy server to get it working. Here's the basic breakdown of the components:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;claude-code&lt;/code&gt;: The base agent.&lt;/li&gt; &lt;li&gt;&lt;code&gt;claude-code-router&lt;/code&gt;: You need to configure this to use external (non-Anthropic) APIs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;My Custom Proxy Server&lt;/strong&gt;: This sits in the middle to modify the LLM requests on the fly. (proxy fix tool-use issue on the fly!)&lt;/li&gt; &lt;li&gt;LM studio : to run GLM-4.5-Air model.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The proxy server is the crucial part of this setup. It intercepts and alters the LLM requests in real-time. For it to work, it had to meet a few key requirements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It must handle both &lt;strong&gt;streaming and non-streaming&lt;/strong&gt; responses. (claude-code use streamming!)&lt;/li&gt; &lt;li&gt;It needs to safely process &lt;strong&gt;UTF-8 characters and byte streams&lt;/strong&gt; to prevent issues during streaming.&lt;/li&gt; &lt;li&gt;It has to &lt;strong&gt;normalize non-standard tool outputs&lt;/strong&gt; into the correct, standardized format.&lt;/li&gt; &lt;li&gt;It must maintain a &lt;strong&gt;stable connection&lt;/strong&gt; for streaming sessions.&lt;/li&gt; &lt;li&gt;It should be &lt;strong&gt;extensible&lt;/strong&gt; to support various types of tool outputs in the future.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Anyway, even though I just quickly put this together, it works surprisingly well, so I figured I'd share the idea with you all.&lt;/p&gt; &lt;p&gt;My Proxy code is here //&lt;br /&gt; &lt;a href="https://github.com/ziozzang/llm-toolcall-proxy"&gt;https://github.com/ziozzang/llm-toolcall-proxy&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ziozzang0"&gt; /u/ziozzang0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdqj9g/works_well_glm_45_air_mlx_lm_studio_mac_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdqj9g/works_well_glm_45_air_mlx_lm_studio_mac_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdqj9g/works_well_glm_45_air_mlx_lm_studio_mac_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T03:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdzxmv</id>
    <title>Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle</title>
    <updated>2025-07-31T12:15:05+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"&gt; &lt;img alt="Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle" src="https://b.thumbs.redditmedia.com/L3dTMc7iVqHF7QYQEYnpMXvgm0DsrwCLc5zwFJKZCxk.jpg" title="Qwen3-30B-A3B-2507-Q4_K_L Is the First Local Model to Solve the North Pole Walk Puzzle" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For the longest time, I've been giving my models a traditional puzzle that all failed to pass without fail :D&lt;br /&gt; Not even the SOTA models provide the right answer.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The puzzle is as follows:&lt;br /&gt; &amp;quot;What's the right answer: Imagine standing at the North Pole of the Earth. Walk in any direction, in a straight line, for 1 km. Now turn 90 degrees to the left. Walk for as long as it takes to pass your starting point. Have you walked: &lt;/p&gt; &lt;p&gt;1- More than 2xPi km.&lt;br /&gt; 2- Exactly 2xPi km.&lt;br /&gt; 3- Less than 2xPi km.&lt;br /&gt; 4- I never came close to my starting point.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;However, only recently, SOTA models started to correctly answer 4 ; models like O3, latest Qwen (Qween3-235B-A22B-2507), Deepseek R1 managed to answer it correctly (I didn't test Claud 4 or Grok 4 but I guess they might get it right). For comparison, Gemini-2.5-Thinking and Kimi2 got the wrong answer.&lt;/p&gt; &lt;p&gt;So, I happy to report that Qwen3-30B-A3B-2507 (both the none thinking Q6 and the thinking Q4) managed to solve the puzzle providing great answers.&lt;/p&gt; &lt;p&gt;Here is O3 answer:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0"&gt;https://preview.redd.it/rbwgf8vxa7gf1.png?width=866&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d074bec940c5c3fab89cc06a5cdf7279ed154ea0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And here is the answer of the Qwen3-30B-A3B-Thinking-2507-Q4_K_L:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/esglti77b7gf1.png?width=821&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416"&gt;https://preview.redd.it/esglti77b7gf1.png?width=821&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9d2e5321f3918bec8209d8613d1ce2df621cd416&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In addition, I tested the two variants on long text (up to 80K) for comprehension, and I am impressed by the quality of the answers. And the SPEEEEEED! It's 3 times faster than Gemma-4B!!!!&lt;/p&gt; &lt;p&gt;Anyway, let me know what you think,&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzxmv/qwen330ba3b2507q4_k_l_is_the_first_local_model_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8slx</id>
    <title>Qwen3-30b-a3b-thinking-2507 This is insane performance</title>
    <updated>2025-07-30T14:56:57+00:00</updated>
    <author>
      <name>/u/3oclockam</name>
      <uri>https://old.reddit.com/user/3oclockam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"&gt; &lt;img alt="Qwen3-30b-a3b-thinking-2507 This is insane performance" src="https://external-preview.redd.it/-lNzejy2CT3wd1ovuVIcDeuPfMRg-vkESkjpQgo3tYU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bd872c4c3958b52ad860a6db5ba53994da65552e" title="Qwen3-30b-a3b-thinking-2507 This is insane performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On par with qwen3-235b?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3oclockam"&gt; /u/3oclockam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8slx/qwen330ba3bthinking2507_this_is_insane_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:56:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1md8t1g</id>
    <title>üöÄ Qwen3-30B-A3B-Thinking-2507</title>
    <updated>2025-07-30T14:57:27+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"&gt; &lt;img alt="üöÄ Qwen3-30B-A3B-Thinking-2507" src="https://preview.redd.it/eaag1cpuz0gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e073b4b20cd702585ec6bbac8fc80938677c24f8" title="üöÄ Qwen3-30B-A3B-Thinking-2507" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ Qwen3-30B-A3B-Thinking-2507, a medium-size model that can think!&lt;/p&gt; &lt;p&gt;‚Ä¢ Nice performance on reasoning tasks, including math, science, code &amp;amp; beyond ‚Ä¢ Good at tool use, competitive with larger models ‚Ä¢ Native support of 256K-token context, extendable to 1M&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-Thinking-2507&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model scope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary"&gt;https://modelscope.cn/models/Qwen/Qwen3-30B-A3B-Thinking-2507/summary&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eaag1cpuz0gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md8t1g/qwen330ba3bthinking2507/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T14:57:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdpfm8</id>
    <title>Made a unified table of benchmarks using AI</title>
    <updated>2025-07-31T02:11:04+00:00</updated>
    <author>
      <name>/u/DrVonSinistro</name>
      <uri>https://old.reddit.com/user/DrVonSinistro</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdpfm8/made_a_unified_table_of_benchmarks_using_ai/"&gt; &lt;img alt="Made a unified table of benchmarks using AI" src="https://preview.redd.it/gxir7usrb4gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7928c2d81afe832554b8bc632895b2c84d73b271" title="Made a unified table of benchmarks using AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They keep putting different reference models in their graphs and we have to look at many graphs to see where we're at so I used AI to put them all in a single table. &lt;/p&gt; &lt;p&gt;If any of you find errors, I'll delete this post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DrVonSinistro"&gt; /u/DrVonSinistro &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gxir7usrb4gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdpfm8/made_a_unified_table_of_benchmarks_using_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdpfm8/made_a_unified_table_of_benchmarks_using_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T02:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdhfhs</id>
    <title>glm-4.5-Air appreciation poist - if you have not done so already, give this model a try</title>
    <updated>2025-07-30T20:24:01+00:00</updated>
    <author>
      <name>/u/Southern_Sun_2106</name>
      <uri>https://old.reddit.com/user/Southern_Sun_2106</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello. It has been an awesomely-busy week for all of us here, trying out the new goodies that dropped by Qwen and others. Wow, this week will be hard to match, good times!&lt;/p&gt; &lt;p&gt;Like most here, I ended up trying a bunch of models in bunch of quants plus mlx.&lt;/p&gt; &lt;p&gt;I have to say, the model that completely blew my mind was glm-4.5-air, the 4-bit mlx. I plugged it into my assistant (that does chains of tools, plus connected to a project management app, plus to a notebook), and it immediately figured out how to use those.&lt;/p&gt; &lt;p&gt;It really likes to dig through tasks, priorities, notes, online research - to the point when I am worried it's going to do it too much and loose track of things - but amazingly enough, it doesn't loose track of things and comes back with in-depth, good analysis and responses.&lt;/p&gt; &lt;p&gt;The model is also fast - kind of reminds me of Owen 30b a3b, although of course it punches well above that one due to its larger size.&lt;/p&gt; &lt;p&gt;If you can fit the 4-bit version onto your machine, absolutely, give this model a try. It is now my new daily driver, replacing Qwen 32B (until the new Qwen 32B comes out later this week? lol)&lt;/p&gt; &lt;p&gt;edit: I am not associated with the gml team (I wish I was!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Southern_Sun_2106"&gt; /u/Southern_Sun_2106 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdhfhs/glm45air_appreciation_poist_if_you_have_not_done/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T20:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1md93bj</id>
    <title>Qwen3 Coder 30B-A3B tomorrow!!!</title>
    <updated>2025-07-30T15:08:26+00:00</updated>
    <author>
      <name>/u/R46H4V</name>
      <uri>https://old.reddit.com/user/R46H4V</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"&gt; &lt;img alt="Qwen3 Coder 30B-A3B tomorrow!!!" src="https://preview.redd.it/zv92612t11gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45b98263f660ff1bebd4634907371461fd4e0207" title="Qwen3 Coder 30B-A3B tomorrow!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/R46H4V"&gt; /u/R46H4V &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zv92612t11gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md93bj/qwen3_coder_30ba3b_tomorrow/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T15:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdvhxg</id>
    <title>Ollama‚Äôs new app ‚Äî Ollama 0.10 is here for macOS and Windows!</title>
    <updated>2025-07-31T07:52:40+00:00</updated>
    <author>
      <name>/u/bllshrfv</name>
      <uri>https://old.reddit.com/user/bllshrfv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvhxg/ollamas_new_app_ollama_010_is_here_for_macos_and/"&gt; &lt;img alt="Ollama‚Äôs new app ‚Äî Ollama 0.10 is here for macOS and Windows!" src="https://preview.redd.it/9wfl7u6z06gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd44ba69feb6890ee5ba2e203ace6fbc8cf232b3" title="Ollama‚Äôs new app ‚Äî Ollama 0.10 is here for macOS and Windows!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Download on ollama.com/download&lt;/p&gt; &lt;p&gt;or GitHub releases&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ollama/ollama/releases/tag/v0.10.0"&gt;https://github.com/ollama/ollama/releases/tag/v0.10.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://ollama.com/blog/new-app"&gt;Ollama's new app&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bllshrfv"&gt; /u/bllshrfv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9wfl7u6z06gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvhxg/ollamas_new_app_ollama_010_is_here_for_macos_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdvhxg/ollamas_new_app_ollama_010_is_here_for_macos_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T07:52:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdpe8v</id>
    <title>Horizon-alpha: A new stealthed model on openrouter sweeps EQ-Bench leaderboards</title>
    <updated>2025-07-31T02:09:14+00:00</updated>
    <author>
      <name>/u/_sqrkl</name>
      <uri>https://old.reddit.com/user/_sqrkl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdpe8v/horizonalpha_a_new_stealthed_model_on_openrouter/"&gt; &lt;img alt="Horizon-alpha: A new stealthed model on openrouter sweeps EQ-Bench leaderboards" src="https://b.thumbs.redditmedia.com/aYYIospZdzzPcpq5Dkfv_OnJ4m1Tv1B_fMeERdxNnXQ.jpg" title="Horizon-alpha: A new stealthed model on openrouter sweeps EQ-Bench leaderboards" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://eqbench.com/"&gt;https://eqbench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Creative Writing Samples: &lt;a href="https://eqbench.com/results/creative-writing-v3/openrouter__horizon-alpha.html"&gt;https://eqbench.com/results/creative-writing-v3/openrouter__horizon-alpha.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Longform Writing Samples: &lt;a href="https://eqbench.com/results/creative-writing-longform/openrouter__horizon-alpha_longform_report.html"&gt;https://eqbench.com/results/creative-writing-longform/openrouter__horizon-alpha_longform_report.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EQ-Bench Samples: &lt;a href="https://eqbench.com/results/eqbench3_reports/openrouter__horizon-alpha.html"&gt;https://eqbench.com/results/eqbench3_reports/openrouter__horizon-alpha.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_sqrkl"&gt; /u/_sqrkl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mdpe8v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdpe8v/horizonalpha_a_new_stealthed_model_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdpe8v/horizonalpha_a_new_stealthed_model_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T02:09:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdruc9</id>
    <title>Why is open source so behind on multi-modalitty?</title>
    <updated>2025-07-31T04:11:01+00:00</updated>
    <author>
      <name>/u/AnticitizenPrime</name>
      <uri>https://old.reddit.com/user/AnticitizenPrime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're in the era now where open source releases are nipping at the heels of closed-source models in benchmarks. But it's all in text modality. &lt;/p&gt; &lt;p&gt;As far as I can tell, there hasn't been a really solid contender when it comes to both being a SOTA model, and also having native audio/image/video input and image/audio output which has been demonstrated by OpenAI and Google.&lt;/p&gt; &lt;p&gt;I feel like this is a really big deal that is mostly overlooked when comparing open source to closed source. Programming benchmarks are cool and all, but for a truly useful assistant, you need a model you can speak to, show stuff to, and it can speak back and generate images to show you stuff as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AnticitizenPrime"&gt; /u/AnticitizenPrime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdruc9/why_is_open_source_so_behind_on_multimodalitty/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T04:11:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdwngf</id>
    <title>rednote-hilab/dots.ocr - Multilingual document layout parsing in a single vision-language model achieving SOTA performance despite compact 1.7B LLM foundation</title>
    <updated>2025-07-31T09:07:08+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwngf/rednotehilabdotsocr_multilingual_document_layout/"&gt; &lt;img alt="rednote-hilab/dots.ocr - Multilingual document layout parsing in a single vision-language model achieving SOTA performance despite compact 1.7B LLM foundation" src="https://external-preview.redd.it/8NDRsizKorORFhKFDygayRrW6cfTqRcK_E46LDgaFmo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54fe70e1d1e50ac63262c7c7180e0173f9cc1673" title="rednote-hilab/dots.ocr - Multilingual document layout parsing in a single vision-language model achieving SOTA performance despite compact 1.7B LLM foundation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/rednote-hilab/dots.ocr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwngf/rednotehilabdotsocr_multilingual_document_layout/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwngf/rednotehilabdotsocr_multilingual_document_layout/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T09:07:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdjb67</id>
    <title>After 6 months of fiddling with local AI. Here‚Äôs my curated models list that work for 90% of my needs. What‚Äôs yours?</title>
    <updated>2025-07-30T21:38:07+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"&gt; &lt;img alt="After 6 months of fiddling with local AI. Here‚Äôs my curated models list that work for 90% of my needs. What‚Äôs yours?" src="https://preview.redd.it/jzljyi4tw2gf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=387253ef4ef3e3a18ba79c1be71339080caaaf1c" title="After 6 months of fiddling with local AI. Here‚Äôs my curated models list that work for 90% of my needs. What‚Äôs yours?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;All models are from Unsloth UD Q4_K_XL except for Gemma3-27B is IQ3. Running all these with 10-12k context with 4-30 t/s across all models.&lt;/p&gt; &lt;p&gt;Most used ones are Mistral-24B, Gemma3-27B, and Granite3.3-2B. Mistral and Gemma are for general QA and random text tools. Granite is for article summaries and random small RAG related tasks. Qwen3-30B (new one) is for coding related tasks, and Gemma3-12B is for vision strictly.&lt;/p&gt; &lt;p&gt;Gemma3n-2B is essentially hooked to Siri via shortcuts and acts as an enhanced Siri.&lt;/p&gt; &lt;p&gt;Medgemma is for anything medical and it‚Äôs wonderful for any general advice and reading of x-rays or medical reports.&lt;/p&gt; &lt;p&gt;My humble mini PC runs all these on Llama.cpp with iGPU 48GB shared memory RAM and Vulkan backend. It runs Mistral at 4t/s with 6k context (set to max of 10k window). Gemme3-27B runs at 5t/s, and Qwen3-30B-A3B at 20-22t/s.&lt;/p&gt; &lt;p&gt;I fall back to ChatGPT once or twice a week when i need a super quick answer or something too in depth.&lt;/p&gt; &lt;p&gt;What is your curated list?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jzljyi4tw2gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdjb67/after_6_months_of_fiddling_with_local_ai_heres_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T21:38:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1md6t2h</id>
    <title>Bye bye, Meta AI, it was good while it lasted.</title>
    <updated>2025-07-30T13:36:51+00:00</updated>
    <author>
      <name>/u/absolooot1</name>
      <uri>https://old.reddit.com/user/absolooot1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zuck has posted a video and a longer letter about the superintelligence plans at Meta. In the letter he says:&lt;/p&gt; &lt;p&gt;&amp;quot;That said, superintelligence will raise novel safety concerns. We'll need to be rigorous about mitigating these risks and careful about what we choose to open source.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.meta.com/superintelligence/"&gt;https://www.meta.com/superintelligence/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That means that Meta will not open source the best they have. But it is inevitable that others will release their best models and agents, meaning that Meta has committed itself to oblivion, not only in open source but in proprietary too, as they are not a major player in that space. The ASI they will get to will be for use in their products only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/absolooot1"&gt; /u/absolooot1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1md6t2h/bye_bye_meta_ai_it_was_good_while_it_lasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-30T13:36:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdwmju</id>
    <title>Falcon-H1 technical report release</title>
    <updated>2025-07-31T09:05:28+00:00</updated>
    <author>
      <name>/u/JingweiZUO</name>
      <uri>https://old.reddit.com/user/JingweiZUO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwmju/falconh1_technical_report_release/"&gt; &lt;img alt="Falcon-H1 technical report release" src="https://external-preview.redd.it/ifoGNEtsOnQI7mOVCHlAOV6hOXRc2zUDtsZ8X9LgS5A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb38d3cc3bcfa71dbf16f5c930e570f21c13b829" title="Falcon-H1 technical report release" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/papers/2507.22448"&gt;https://huggingface.co/papers/2507.22448&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The hybrid transformer-mamba models series, covering 0.5B, 1.5B, 1.5B-Deep, 3B, 7B and 34B. &lt;/p&gt; &lt;p&gt;This 80+ page report dives deep into the key design decisions behind Falcon-H1 - from architectural innovations and data strategies to training recipes that challenge conventional practices in LLM development üî•&lt;/p&gt; &lt;p&gt;Current framework support includes Hugging Face, vLLM, llama.cpp, Llama-Factory, Axolotl, OUMI, SkyPilot, etc. ‚Äî with more on the way!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vog1eu4gd6gf1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80753458ee6e8869540d1c75a0599c9a2aae9dad"&gt;https://preview.redd.it/vog1eu4gd6gf1.png?width=1708&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80753458ee6e8869540d1c75a0599c9a2aae9dad&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JingweiZUO"&gt; /u/JingweiZUO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwmju/falconh1_technical_report_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwmju/falconh1_technical_report_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdwmju/falconh1_technical_report_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T09:05:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdzu08</id>
    <title>Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities</title>
    <updated>2025-07-31T12:10:25+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/"&gt; &lt;img alt="Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities" src="https://a.thumbs.redditmedia.com/UWzewWY4cRLtu0QtYjM_H7EidNwT1bopn8L_07vMWc4.jpg" title="Hunyuan releases X-Omni, a unified discrete autoregressive model for both image and language modalities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ We're excited to share our latest research on X-Omni: reinforcement learning makes discrete autoregressive image generative models great again, empowering a practical unified model for both image and language modality generation.&lt;/p&gt; &lt;p&gt;Highlights:&lt;/p&gt; &lt;p&gt;‚úÖ Unified Modeling Approach: A discrete autoregressive model handling image and language modalities.&lt;/p&gt; &lt;p&gt;‚úÖ Superior Instruction Following: Exceptional capability to follow complex instructions.&lt;/p&gt; &lt;p&gt;‚úÖ Superior Text Rendering: Accurately render text in multiple languages, including both English and Chinese.&lt;/p&gt; &lt;p&gt;‚úÖ Arbitrary resolutions: Produces aesthetically pleasing images at arbitrary resolutions.&lt;/p&gt; &lt;p&gt;Insight:&lt;/p&gt; &lt;p&gt;üîç During the reinforcement learning process, the aesthetic quality of generated images is gradually enhanced, and the ability to adhere to instructions and the capacity to render long texts improve steadily.&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/pdf/2507.22058"&gt;https://arxiv.org/pdf/2507.22058&lt;/a&gt; Github: &lt;a href="https://github.com/X-Omni-Team/X-Omni"&gt;https://github.com/X-Omni-Team/X-Omni&lt;/a&gt; Project Page: &lt;a href="https://x-omni-team.github.io/"&gt;https://x-omni-team.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mdzu08"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdzu08/hunyuan_releases_xomni_a_unified_discrete/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdv67j</id>
    <title>cogito v2 preview models released 70B/109B/405B/671B</title>
    <updated>2025-07-31T07:30:57+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Cogito v2 LLMs are instruction tuned generative models. All models are released under an open license for commercial use.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cogito v2 models are hybrid reasoning models. Each model can answer directly (standard LLM), or self-reflect before answering (like reasoning models).&lt;/li&gt; &lt;li&gt;The LLMs are trained using &lt;strong&gt;Iterated Distillation and Amplification (IDA)&lt;/strong&gt; - an scalable and efficient alignment strategy for superintelligence using iterative self-improvement.&lt;/li&gt; &lt;li&gt;The models have been optimized for coding, STEM, instruction following and general helpfulness, and have significantly higher multilingual, coding and tool calling capabilities than size equivalent counterparts. &lt;ul&gt; &lt;li&gt;In both standard and reasoning modes, Cogito v2-preview models outperform their size equivalent counterparts on common industry benchmarks.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;This model is trained in over 30 languages and supports a context length of 128k.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-109B-MoE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-llama-405B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE"&gt;https://huggingface.co/deepcogito/cogito-v2-preview-deepseek-671B-MoE&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdv67j/cogito_v2_preview_models_released_70b109b405b671b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T07:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdn6dp</id>
    <title>Deepseek just won the best paper award at ACL 2025 with a breakthrough innovation in long context, a model using this might come soon</title>
    <updated>2025-07-31T00:23:44+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2502.11089"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdn6dp/deepseek_just_won_the_best_paper_award_at_acl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T00:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1me095p</id>
    <title>Junyang Lin is drinking tea</title>
    <updated>2025-07-31T12:30:05+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"&gt; &lt;img alt="Junyang Lin is drinking tea" src="https://preview.redd.it/s3pv80fee7gf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b666a1b9473c5408870aeb8cf6dddfc5f13f55d" title="Junyang Lin is drinking tea" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s3pv80fee7gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1me095p/junyang_lin_is_drinking_tea/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T12:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdy1at</id>
    <title>Jan now runs fully on llama.cpp &amp; auto-updates the backend</title>
    <updated>2025-07-31T10:34:34+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/"&gt; &lt;img alt="Jan now runs fully on llama.cpp &amp;amp; auto-updates the backend" src="https://external-preview.redd.it/OThqM3A3cmNyNmdmMarVaHVhDy4CK4NoO0kgn6HbxLEdRYxLZuUtk8wS5NEb.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4784b1a0c70ad81df229c7755eea2f2702e08edc" title="Jan now runs fully on llama.cpp &amp;amp; auto-updates the backend" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, it's Emre from the Jan team.&lt;/p&gt; &lt;p&gt;Jan v0.6.6 is out. Over the past few weeks we've ripped out Cortex, the backend layer on top of llama.cpp. It's finally gone, every local model now runs directly on llama.cpp.&lt;/p&gt; &lt;p&gt;Plus, you can switch to any llama.cpp build under Settings, Model Providers, llama.cpp (see the video above).&lt;/p&gt; &lt;p&gt;Jan v0.6.6 Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cortex is removed, local models now run on &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Hugging Face is integrated in Model Providers. So you can paste your HF token and run models in the cloud via Jan&lt;/li&gt; &lt;li&gt;Jan Hub has been a bit updated for faster model search and less clutter when browsing models&lt;/li&gt; &lt;li&gt;Inline-image support from MCP servers: If an MCP server returns an image (e.g. web search MCP). &lt;ul&gt; &lt;li&gt;It's an experimental feature, please activate Experimental Features in Settings to see MCP settings.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Plus, we've also fixed a bunch of bugs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Update your Jan or download the latest here: &lt;a href="https://jan.ai/"&gt;https://jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full release notes are here: &lt;a href="https://github.com/menloresearch/jan/releases"&gt;https://github.com/menloresearch/jan/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick notes:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;We removed Cortex because it added an extra hop and maintenance overhead. Folding its logic into Jan cuts latency and makes future mobile / server work simpler.&lt;/li&gt; &lt;li&gt; Regarding bugs &amp;amp; previous requests: I'll reply to earlier requests and reports in the previous comments later today.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6tdds5rcr6gf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdy1at/jan_now_runs_fully_on_llamacpp_autoupdates_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T10:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdx65u</id>
    <title>AMD Is Reportedly Looking to Introduce a Dedicated Discrete NPU, Similar to Gaming GPUs But Targeted Towards AI Performance On PCs; Taking Edge AI to New Levels</title>
    <updated>2025-07-31T09:41:46+00:00</updated>
    <author>
      <name>/u/_SYSTEM_ADMIN_MOD_</name>
      <uri>https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/_SYSTEM_ADMIN_MOD_"&gt; /u/_SYSTEM_ADMIN_MOD_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/amd-is-looking-toward-introducing-a-dedicated-discrete-npu-similar-to-gaming-gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdx65u/amd_is_reportedly_looking_to_introduce_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T09:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdmsu9</id>
    <title>Chinese models pulling away</title>
    <updated>2025-07-31T00:06:15+00:00</updated>
    <author>
      <name>/u/Kniffliger_Kiffer</name>
      <uri>https://old.reddit.com/user/Kniffliger_Kiffer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"&gt; &lt;img alt="Chinese models pulling away" src="https://preview.redd.it/727keqreo3gf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=638ce7aed31fa426f1cfea7678c6d9169932f5a9" title="Chinese models pulling away" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kniffliger_Kiffer"&gt; /u/Kniffliger_Kiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/727keqreo3gf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdmsu9/chinese_models_pulling_away/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T00:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdykfn</id>
    <title>Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs</title>
    <updated>2025-07-31T11:04:33+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"&gt; &lt;img alt="Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs" src="https://preview.redd.it/f5iqhqp7z6gf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=80da4073073fb12cdbab3b110619a3002d524b2f" title="Everyone from r/LocalLLama refreshing Hugging Face every 5 minutes today looking for GLM-4.5 GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f5iqhqp7z6gf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdykfn/everyone_from_rlocalllama_refreshing_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T11:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mdsjn2</id>
    <title>Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace</title>
    <updated>2025-07-31T04:50:27+00:00</updated>
    <author>
      <name>/u/jiawei243</name>
      <uri>https://old.reddit.com/user/jiawei243</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt; &lt;img alt="Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace" src="https://a.thumbs.redditmedia.com/Pqx5Ku4b-UvrnWIofuwt9LYnoux9zPw_UBbzkN3H6v4.jpg" title="Unbelievable: China Dominates Top 10 Open-Source Models on HuggingFace" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea"&gt;https://preview.redd.it/vhlkl99m35gf1.png?width=1098&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a38ae109844be87b98cbec8fe243f9d27fa3dea&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That‚Äôs insane ‚Äî throughout this past July, Chinese companies have been rapidly open-sourcing AI models. First came Kimi-K2, then Qwen3, followed by GLM-4.5. On top of that, there‚Äôs Tencent‚Äôs HunyuanWorld and Alibaba‚Äôs Wan 2.2. Now, most of the trending models on Hugging Face are from China. Meanwhile, according to Zuckerberg, Meta is planning to shift toward a closed-source strategy going forward.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/models"&gt;https://huggingface.co/models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiawei243"&gt; /u/jiawei243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mdsjn2/unbelievable_china_dominates_top_10_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-07-31T04:50:27+00:00</published>
  </entry>
</feed>
