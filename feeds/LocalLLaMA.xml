<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-23T14:37:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nnr7ys</id>
    <title>The Qwen3-TTS demo is now out!</title>
    <updated>2025-09-22T16:28:50+00:00</updated>
    <author>
      <name>/u/nonredditaccount</name>
      <uri>https://old.reddit.com/user/nonredditaccount</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Qwen3-TTS! Our new text-to-speech model is designed to be multi-timbre, multi-lingual, and multi-dialect for natural, expressive audio. It delivers strong performance in English &amp;amp; Chinese, and we're excited for you to hear it for yourself!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nonredditaccount"&gt; /u/nonredditaccount &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/Ali_TongyiLab/status/1970160304748437933"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnr7ys/the_qwen3tts_demo_is_now_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnr7ys/the_qwen3tts_demo_is_now_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T16:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1no3qka</id>
    <title>GLM 4.5 Air Template Breaking llamacpp Prompt Caching</title>
    <updated>2025-09-23T00:52:21+00:00</updated>
    <author>
      <name>/u/Most_Client4958</name>
      <uri>https://old.reddit.com/user/Most_Client4958</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hope this saves someone some time - it took me a while to figure this out. I'm using GLM 4.5 Air from unsloth with a template I found in a PR. Initially, I didn't realize why prompt processing was taking so long until I discovered that llamacpp wasn't caching my requests because the template was changing the messages with every request.&lt;/p&gt; &lt;p&gt;After simplifying the template, I got caching back, and the performance improvement with tools like roo is dramatic - many times faster. Tool calling is still working fine as well.&lt;/p&gt; &lt;p&gt;To confirm your prompt caching is working, look for similar messages in your llama server console:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot get_availabl: id 0 | task 3537 | selected slot by lcs similarity, lcs_len = 13210, similarity = 0.993 (&amp;gt; 0.100 thold) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The template that was breaking caching is here: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/15186"&gt;https://github.com/ggml-org/llama.cpp/pull/15186&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Most_Client4958"&gt; /u/Most_Client4958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no3qka/glm_45_air_template_breaking_llamacpp_prompt/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no3qka/glm_45_air_template_breaking_llamacpp_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no3qka/glm_45_air_template_breaking_llamacpp_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T00:52:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nohx9q</id>
    <title>Sample dataset to fine-tune Gemma3 - 270m model</title>
    <updated>2025-09-23T13:49:39+00:00</updated>
    <author>
      <name>/u/Independent-Golf-754</name>
      <uri>https://old.reddit.com/user/Independent-Golf-754</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Folks,&lt;/p&gt; &lt;p&gt;I am trying to learn how to fine-tune AI models. I am specifically interested in fine-tuning the Google Gemma 3 - 270m model. Could someone suggest a suitable dataset for fine-tuning this model? Would prefer something practical rather than a toy example. Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Golf-754"&gt; /u/Independent-Golf-754 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohx9q/sample_dataset_to_finetune_gemma3_270m_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohx9q/sample_dataset_to_finetune_gemma3_270m_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nohx9q/sample_dataset_to_finetune_gemma3_270m_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:49:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntdok</id>
    <title>Qwen3-Omni looks insane</title>
    <updated>2025-09-22T17:48:53+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntdok/qwen3omni_looks_insane/"&gt; &lt;img alt="Qwen3-Omni looks insane" src="https://external-preview.redd.it/B4ZZCzuzrlsMnQHNhsoc21qTthMSpFr8qrrtucUS_RU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7250574f82c5b2852f214b634dbe23e3e38e029b" title="Qwen3-Omni looks insane" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Truly a multimodal model that can handle inputs in audio, video, text, and images. Outputs include text and audio with near real-time responses. &lt;/p&gt; &lt;p&gt;# of use cases this can support is wild: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Real-time conversational agents: low-latency speech-to-speech assistants for customer support, tutoring, or accessibility.&lt;/li&gt; &lt;li&gt;Multilingual: cross-language text chat and voice translation across 100+ languages.&lt;/li&gt; &lt;li&gt;Audio and video understanding: transcription, summarization, and captioning of meetings, lectures, or media (up to 30 mins of audio, short video clips).&lt;/li&gt; &lt;li&gt;Content accessibility: generating captions and descriptions for audio and video content.&lt;/li&gt; &lt;li&gt;Interactive multimodal apps: applications that need to handle text, images, audio, and video seamlessly.&lt;/li&gt; &lt;li&gt;Tool-integrated agents: assistants that can call APIs or external services (e.g., booking systems, productivity apps).&lt;/li&gt; &lt;li&gt;Personalized AI experiences: customizable personas or characters for therapy, entertainment, education, or branded interactions.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Wonder how OpenAI and other closed models are feeling right about now .... &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=_zdOrPju4_g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntdok/qwen3omni_looks_insane/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nntdok/qwen3omni_looks_insane/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:48:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnsx1a</id>
    <title>Qwen3-Omni has been released</title>
    <updated>2025-09-22T17:31:45+00:00</updated>
    <author>
      <name>/u/eu-thanos</name>
      <uri>https://old.reddit.com/user/eu-thanos</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsx1a/qwen3omni_has_been_released/"&gt; &lt;img alt="Qwen3-Omni has been released" src="https://external-preview.redd.it/ktbPqK316US_xKrngLchajXyzydUl2qGgd_RzyQVGrw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0142c63f372aedb3019c9bac509f1f0cc9e58e3e" title="Qwen3-Omni has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eu-thanos"&gt; /u/eu-thanos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsx1a/qwen3omni_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnsx1a/qwen3omni_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:31:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwxaj</id>
    <title>Ling mini 2.0 16B MoE on iPhone 17 Pro at ~120tk/s</title>
    <updated>2025-09-22T20:01:43+00:00</updated>
    <author>
      <name>/u/adrgrondin</name>
      <uri>https://old.reddit.com/user/adrgrondin</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnwxaj/ling_mini_20_16b_moe_on_iphone_17_pro_at_120tks/"&gt; &lt;img alt="Ling mini 2.0 16B MoE on iPhone 17 Pro at ~120tk/s" src="https://external-preview.redd.it/Zm10N2kzd2N2cnFmMf97jWHNFdIh6ev_NX49Pv9woYvZ8KRQujn8v7_MjAqF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a8153c9c4d6574b73c2468262e84773829075b5" title="Ling mini 2.0 16B MoE on iPhone 17 Pro at ~120tk/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here I’m running &lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0"&gt;Ling mini 2.0&lt;/a&gt; 16B MoE (1.4B active parameters) with MLX DWQ 2-bit quants at ~120tk/s for a ~30 tokens prompt.&lt;/p&gt; &lt;p&gt;Take it more as a tech demo of the new iPhones, as I don’t have any benchmarks on how the DWQ 2-bit impacted the model, but my first impression with it is good.&lt;/p&gt; &lt;p&gt;And it’s also not really usable as it crashes on multi-turn as the model here is extremely close to the limit allowed by iOS for these iPhones. It’s annoying that the limit here is iOS and not the iPhone. I wish that Apple would up that limit just a bit on the new models, it’s definitely possible.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adrgrondin"&gt; /u/adrgrondin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/mzm4vr0dvrqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnwxaj/ling_mini_20_16b_moe_on_iphone_17_pro_at_120tks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnwxaj/ling_mini_20_16b_moe_on_iphone_17_pro_at_120tks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T20:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nns09y</id>
    <title>What should I do with this DGX H100?</title>
    <updated>2025-09-22T16:57:50+00:00</updated>
    <author>
      <name>/u/Naneet_Aleart_Ok</name>
      <uri>https://old.reddit.com/user/Naneet_Aleart_Ok</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nns09y/what_should_i_do_with_this_dgx_h100/"&gt; &lt;img alt="What should I do with this DGX H100?" src="https://preview.redd.it/6ja1u2zsxqqf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2d21a0f333a3eaf78858f3e727fe5da1d437a94" title="What should I do with this DGX H100?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys. Basically the college have a terrible resource management and they shut down the MIG layer and I got complete access to DGX H100. Suggest me some idea, what should I do with it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Naneet_Aleart_Ok"&gt; /u/Naneet_Aleart_Ok &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6ja1u2zsxqqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nns09y/what_should_i_do_with_this_dgx_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nns09y/what_should_i_do_with_this_dgx_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T16:57:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1no6hox</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-09-23T03:06:27+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI, here are the local/edge highlights from today's edition:&lt;/p&gt; &lt;p&gt;Moondream 3 Preview &lt;/p&gt; &lt;ul&gt; &lt;li&gt;9B total, 2B active through MoE&lt;/li&gt; &lt;li&gt;Matches GPT-4V/Claude performance&lt;/li&gt; &lt;li&gt;32k context window (up from 2k)&lt;/li&gt; &lt;li&gt;Visual grounding shows what it's looking at&lt;/li&gt; &lt;li&gt;Runs on consumer hardware&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/moondream/moondream3-preview"&gt;HuggingFace&lt;/a&gt; | &lt;a href="https://moondream.ai/blog/moondream-3-preview"&gt;Blog&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;RecA Post-Training - Fix Models Locally&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Transform multimodal models in 27 GPU-hours&lt;/li&gt; &lt;li&gt;Boosts performance from 0.73 to 0.90&lt;/li&gt; &lt;li&gt;No cloud compute needed&lt;/li&gt; &lt;li&gt;&lt;a href="https://reconstruction-alignment.github.io/"&gt;Project Page&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;IBM Granite-Docling-258M&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Document conversion at 258M params&lt;/li&gt; &lt;li&gt;Handles complex layouts locally&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-docling-67a896eaa0366a3a564087bb"&gt;HuggingFace Collection&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other highlights&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Decart Lucy Edit: Open-source video editing with ComfyUI&lt;/li&gt; &lt;li&gt;Alibaba DeepResearch: 30B (3B active) matching OpenAI&lt;/li&gt; &lt;li&gt;Theory-of-Mind video models for local deployment&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full newsletter(free): &lt;a href="https://thelivingedge.substack.com/p/multimodal-monday-25-mind-reading"&gt;https://thelivingedge.substack.com/p/multimodal-monday-25-mind-reading&lt;/a&gt; (links to code/demos/models)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no6hox/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no6hox/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no6hox/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T03:06:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnmhai</id>
    <title>🚀 DeepSeek released DeepSeek-V3.1-Terminus</title>
    <updated>2025-09-22T13:27:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"&gt; &lt;img alt="🚀 DeepSeek released DeepSeek-V3.1-Terminus" src="https://preview.redd.it/729mf2l1xpqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e9ce1e335c2a491a3d422d6f4d70b33dbf6a25f" title="🚀 DeepSeek released DeepSeek-V3.1-Terminus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 DeepSeek-V3.1 → DeepSeek-V3.1-Terminus The latest update builds on V3.1’s strengths while addressing key user feedback.&lt;/p&gt; &lt;p&gt;✨ What’s improved?&lt;/p&gt; &lt;p&gt;🌐 Language consistency: fewer CN/EN mix-ups &amp;amp; no more random chars.&lt;/p&gt; &lt;p&gt;🤖 Agent upgrades: stronger Code Agent &amp;amp; Search Agent performance.&lt;/p&gt; &lt;p&gt;📊 DeepSeek-V3.1-Terminus delivers more stable &amp;amp; reliable outputs across benchmarks compared to the previous version.&lt;/p&gt; &lt;p&gt;👉 Available now on: App / Web / API 🔗 Open-source weights here: &lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Terminus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks to everyone for your feedback. It drives us to keep improving and refining the experience! 🚀&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/729mf2l1xpqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnmhai/deepseek_released_deepseekv31terminus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T13:27:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1noe3wq</id>
    <title>Best open model for generating audiobooks?</title>
    <updated>2025-09-23T10:50:05+00:00</updated>
    <author>
      <name>/u/Amgadoz</name>
      <uri>https://old.reddit.com/user/Amgadoz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I read a lot of novels that don't have an audiobook version. I want to develop a solution where I can feed in the chatper text and get back a narrated version. Which TTS would you recommend?&lt;/p&gt; &lt;p&gt;Most chapters are 2k tokens .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amgadoz"&gt; /u/Amgadoz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe3wq/best_open_model_for_generating_audiobooks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe3wq/best_open_model_for_generating_audiobooks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noe3wq/best_open_model_for_generating_audiobooks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nof8l9</id>
    <title>🤗 benchmarking tool !</title>
    <updated>2025-09-23T11:50:48+00:00</updated>
    <author>
      <name>/u/HauntingMoment</name>
      <uri>https://old.reddit.com/user/HauntingMoment</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nof8l9/benchmarking_tool/"&gt; &lt;img alt="🤗 benchmarking tool !" src="https://external-preview.redd.it/qksLxUPSD9ZsRcKhdz7LwdtA1hXx3hBkllOWc4Vtw38.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=79e7eacd109647242f3209f7861a35b2c5eb362e" title="🤗 benchmarking tool !" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Hey everyone!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I’ve been working on l&lt;strong&gt;ighteval&lt;/strong&gt; for a while now, but never really shared it here.&lt;/p&gt; &lt;p&gt;Lighteval is an evaluation library with &lt;strong&gt;thousands of tasks&lt;/strong&gt;, including &lt;strong&gt;state-of-the-art support for multilingual evaluations&lt;/strong&gt;. It lets you evaluate models in multiple ways: via inference endpoints, local models, or even models already loaded in memory with Transformers.&lt;/p&gt; &lt;p&gt;We just released a &lt;strong&gt;new version&lt;/strong&gt; with more stable tests, so I’d love to hear your thoughts if you try it out!&lt;/p&gt; &lt;p&gt;Also curious—&lt;strong&gt;what are the biggest friction points you face when evaluating models right now?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HauntingMoment"&gt; /u/HauntingMoment &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/lighteval"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nof8l9/benchmarking_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nof8l9/benchmarking_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T11:50:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nohcgs</id>
    <title>How can we run Qwen3-omni-30b-a3b?</title>
    <updated>2025-09-23T13:26:04+00:00</updated>
    <author>
      <name>/u/PermanentLiminality</name>
      <uri>https://old.reddit.com/user/PermanentLiminality</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This looks awesome, but I can't run it. At least not yet and I sure want to run it. &lt;/p&gt; &lt;p&gt;It looks like it needs to be run with straight python transformer. I could be wrong, but none of the usual suspects like vllm, llama.cpp, etc support the multimodal nature of the model. Can we expect support in any of these?&lt;/p&gt; &lt;p&gt;Given the above, will there be quants? I figured there would at least be some placeholders on HFm but I didn't see any when I just looked. The native 16 bit format is 70GB and my best system will maybe just barely fit that in combined VRAM and system RAM. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PermanentLiminality"&gt; /u/PermanentLiminality &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nohcgs/how_can_we_run_qwen3omni30ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:26:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnwdnq</id>
    <title>...stay tuned, Qwen is coming</title>
    <updated>2025-09-22T19:41:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnwdnq/stay_tuned_qwen_is_coming/"&gt; &lt;img alt="...stay tuned, Qwen is coming" src="https://preview.redd.it/29j60g9nrrqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c20ca441cd91aaf2259cd38fe5ab87b60a77b17" title="...stay tuned, Qwen is coming" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/29j60g9nrrqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnwdnq/stay_tuned_qwen_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnwdnq/stay_tuned_qwen_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T19:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnt539</id>
    <title>Qwen-Image-Edit-2509 has been released</title>
    <updated>2025-09-22T17:40:02+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This September, we are pleased to introduce Qwen-Image-Edit-2509, the monthly iteration of Qwen-Image-Edit. To experience the latest model, please visit &lt;a href="https://qwen.ai"&gt;Qwen Chat&lt;/a&gt; and select the &amp;quot;Image Editing&amp;quot; feature. Compared with Qwen-Image-Edit released in August, the main improvements of Qwen-Image-Edit-2509 include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multi-image Editing Support&lt;/strong&gt;: For multi-image inputs, Qwen-Image-Edit-2509 builds upon the Qwen-Image-Edit architecture and is further trained via image concatenation to enable multi-image editing. It supports various combinations such as &amp;quot;person + person,&amp;quot; &amp;quot;person + product,&amp;quot; and &amp;quot;person + scene.&amp;quot; Optimal performance is currently achieved with 1 to 3 input images.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Single-image Consistency&lt;/strong&gt;: For single-image inputs, Qwen-Image-Edit-2509 significantly improves editing consistency, specifically in the following areas: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Improved Person Editing Consistency&lt;/strong&gt;: Better preservation of facial identity, supporting various portrait styles and pose transformations;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improved Product Editing Consistency&lt;/strong&gt;: Better preservation of product identity, supporting product poster editing；&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Improved Text Editing Consistency&lt;/strong&gt;: In addition to modifying text content, it also supports editing text fonts, colors, and materials；&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Support for ControlNet&lt;/strong&gt;: Including depth maps, edge maps, keypoint maps, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt539/qwenimageedit2509_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1noao31</id>
    <title>MAESTRO v0.1.6 Update: Better support for models that struggle with JSON mode (DeepSeek, Kimi K2, etc.)</title>
    <updated>2025-09-23T07:06:49+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noao31/maestro_v016_update_better_support_for_models/"&gt; &lt;img alt="MAESTRO v0.1.6 Update: Better support for models that struggle with JSON mode (DeepSeek, Kimi K2, etc.)" src="https://preview.redd.it/7odksx6x5vqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d509b4d013113b628a0d86a1c714cedb17800243" title="MAESTRO v0.1.6 Update: Better support for models that struggle with JSON mode (DeepSeek, Kimi K2, etc.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Just pushed a quick update for my AI research agent, MAESTRO (v0.1.6-alpha).&lt;/p&gt; &lt;p&gt;The main focus was improving compatibility with great open models that don't always play nice with forced &lt;code&gt;json_schema&lt;/code&gt; outputs. I added a fallback system for structured data, so MAESTRO now works much more reliably with models like DeepSeek, Kimi K2, and others in the same boat.&lt;/p&gt; &lt;p&gt;On the API side, for those who use it, I also added support for GPT-5 models with the ability to select different &amp;quot;thinking levels&amp;quot; for more control over the reasoning process.&lt;/p&gt; &lt;p&gt;If you want to check it out, the docs have everything you need. You can find the &lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/quickstart/"&gt;Quick Start&lt;/a&gt;. see some &lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports&lt;/a&gt;. and read the full &lt;a href="https://murtaza-nasir.github.io/maestro/getting-started/installation/"&gt;Installation guide&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7odksx6x5vqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noao31/maestro_v016_update_better_support_for_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noao31/maestro_v016_update_better_support_for_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T07:06:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnu9b2</id>
    <title>🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥</title>
    <updated>2025-09-22T18:20:53+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"&gt; &lt;img alt="🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥" src="https://preview.redd.it/taitk409drqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=216fec124343c71e7a56513855309026dafdb0d2" title="🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🔥 Qwen-Image-Edit-2509 IS LIVE — and it’s a GAME CHANGER. 🔥&lt;/p&gt; &lt;p&gt;We didn’t just upgrade it. We rebuilt it for creators, designers, and AI tinkerers who demand pixel-perfect control.&lt;/p&gt; &lt;p&gt;✅ Multi-Image Editing? YES.&lt;/p&gt; &lt;p&gt;Drag in “person + product” or “person + scene” — it blends them like magic. No more Franken-images.&lt;/p&gt; &lt;p&gt;✅ Single-Image? Rock-Solid Consistency.&lt;/p&gt; &lt;p&gt;• 👤 Faces stay you — through poses, filters, and wild styles.&lt;/p&gt; &lt;p&gt;• 🛍️ Products keep their identity — ideal for ads &amp;amp; posters.&lt;/p&gt; &lt;p&gt;• ✍️ Text? Edit everything: content, font, color, even material texture.&lt;/p&gt; &lt;p&gt;✅ ControlNet Built-In.&lt;/p&gt; &lt;p&gt;Depth. Edges. Keypoints. Plug &amp;amp; play precision.&lt;/p&gt; &lt;p&gt;✨ Blog: &lt;a href="https://qwen.ai/blog?id=7a90090115ee193ce6a7f619522771dd9696dd93&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=7a90090115ee193ce6a7f619522771dd9696dd93&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💬 QwenChat: &lt;a href="https://chat.qwen.ai/?inputFeature=image_edit"&gt;https://chat.qwen.ai/?inputFeature=image_edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🐙 GitHub: &lt;a href="https://github.com/QwenLM/Qwen-Image"&gt;https://github.com/QwenLM/Qwen-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 HuggingFace: &lt;a href="https://huggingface.co/Qwen/Qwen-Image-Edit-2509"&gt;https://huggingface.co/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🧩 ModelScope: &lt;a href="https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509"&gt;https://modelscope.cn/models/Qwen/Qwen-Image-Edit-2509&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/taitk409drqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnu9b2/qwenimageedit2509_is_live_and_its_a_game_changer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T18:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nntr5a</id>
    <title>🚀 Qwen released Qwen3-Omni!</title>
    <updated>2025-09-22T18:02:33+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"&gt; &lt;img alt="🚀 Qwen released Qwen3-Omni!" src="https://b.thumbs.redditmedia.com/G9yWrU9TAoh7X4gkG7uSbhBJ3c763zZhRkFx6pNI0wo.jpg" title="🚀 Qwen released Qwen3-Omni!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;🚀 Introducing Qwen3-Omni — the first natively end-to-end omni-modal AI unifying text, image, audio &amp;amp; video in one model — no modality trade-offs!&lt;/p&gt; &lt;p&gt;🏆 SOTA on 22/36 audio &amp;amp; AV benchmarks&lt;/p&gt; &lt;p&gt;🌍 119L text / 19L speech in / 10L speech out&lt;/p&gt; &lt;p&gt;⚡ 211ms latency | 🎧 30-min audio understanding&lt;/p&gt; &lt;p&gt;🎨 Fully customizable via system prompts&lt;/p&gt; &lt;p&gt;🔗 Built-in tool calling&lt;/p&gt; &lt;p&gt;🎤 Open-source Captioner model (low-hallucination!)&lt;/p&gt; &lt;p&gt;🌟 What’s Open-Sourced? &lt;/p&gt; &lt;p&gt;We’ve open-sourced Qwen3-Omni-30B-A3B-Instruct, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner, to empower developers to explore a variety of applications from instruction-following to creative tasks.&lt;/p&gt; &lt;p&gt;Try it now 👇&lt;/p&gt; &lt;p&gt;💬 Qwen Chat: &lt;a href="https://chat.qwen.ai/?models=qwen3-omni-flash"&gt;https://chat.qwen.ai/?models=qwen3-omni-flash&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 GitHub: &lt;a href="https://github.com/QwenLM/Qwen3-Omni"&gt;https://github.com/QwenLM/Qwen3-Omni&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 HF Models: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤖 MS Models: &lt;a href="https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f"&gt;https://modelscope.cn/collections/Qwen3-Omni-867aef131e7d4f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🎬 Demo: &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo"&gt;https://huggingface.co/spaces/Qwen/Qwen3-Omni-Demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nntr5a"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nntr5a/qwen_released_qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T18:02:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nogrv2</id>
    <title>Computer literally warms my room by 5 degrees Celsius during sustained generations</title>
    <updated>2025-09-23T13:01:27+00:00</updated>
    <author>
      <name>/u/nad_lab</name>
      <uri>https://old.reddit.com/user/nad_lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don’t know how to even go about fixing this other than opening a window but for a workflow I have gpt-oss 20 b running for hours and my room acc heats up, I usually love mechanical and technological heat like 3d printing heat or heat when I play video games / pcvr BUT THIS, these ai workloads literally feel like a warm updraft from my computer, any thoughts on what to do? Anything helps on the software side to help not be so hot, yes I can and do open a window, and I live in Canada so I’m very very excited to not pay a heating bill this month cuz of this RTX 5060 ti 16 gb ram with a 3950x, cuz istg rn in the summer/fall my room avgs 30 deg c&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nad_lab"&gt; /u/nad_lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T13:01:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnnws0</id>
    <title>Qwen 😁</title>
    <updated>2025-09-22T14:25:11+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt; &lt;img alt="Qwen 😁" src="https://preview.redd.it/milakcbb7qqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7af57edddeef91bdc75a874fb95e8ac60d2746ae" title="Qwen 😁" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/milakcbb7qqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnnws0/qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T14:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1no4exb</id>
    <title>I Upgrade 4090's to have 48gb VRAM: Comparative LLM Performance</title>
    <updated>2025-09-23T01:24:25+00:00</updated>
    <author>
      <name>/u/computune</name>
      <uri>https://old.reddit.com/user/computune</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no4exb/i_upgrade_4090s_to_have_48gb_vram_comparative_llm/"&gt; &lt;img alt="I Upgrade 4090's to have 48gb VRAM: Comparative LLM Performance" src="https://b.thumbs.redditmedia.com/TbEBDmv-f-LDRp2-dobGGwXS3g7rJq4IQ94R8DZo70c.jpg" title="I Upgrade 4090's to have 48gb VRAM: Comparative LLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tested the 48gb 4090 against the stock 24gb 4090, 80gb A100, and 48gb A6000&lt;/p&gt; &lt;p&gt;It blew the A6000 out of the water (of course it is one generation newer), though doesn't have nvlink. But at $3500 for second hand A6000's, these 4090's are very competitive at around $3000.&lt;/p&gt; &lt;p&gt;Compared to the stock 4090, i see (what could be variance) a 1-2% increase in small model latency compared to the stock 24gb 4090.&lt;/p&gt; &lt;p&gt;The graphed results are based off of this &lt;a href="https://github.com/chigkim/prompt-test"&gt;llm testing suite on github&lt;/a&gt; by chigkim&lt;/p&gt; &lt;h1&gt;Physical specs:&lt;/h1&gt; &lt;p&gt;The blower fan makes it run at 70 dB under load, noticeably audible and you wouldn't be comfortable doing work next to it. Its an &amp;quot;in the other room&amp;quot; type of card. Water block is in development.&lt;/p&gt; &lt;p&gt;Rear side back-plate heats to about 54 degrees C. Well within operating spec of the micron memory modules.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I upgrade and make these cards in the USA (no tariffs or long wait)&lt;/strong&gt;. My process involves careful attention to thermal management during every step of the process to ensure the chips don't have a degraded lifespan. I have more info on my website. (been an online video card repair shop since 2021)&lt;/p&gt; &lt;p&gt;&lt;a href="https://gpvlab.com/rtx-info.html"&gt;https://gpvlab.com/rtx-info.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ZaJnjfcOPpI"&gt;https://www.youtube.com/watch?v=ZaJnjfcOPpI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let me know what other testing youd like done. Im open to it. I have room for 4x of these in a 4x x16 (pcie 4.0) intel server for testing.&lt;/p&gt; &lt;p&gt;Exporting to the UK/EU/Cad and other countries is possible- though export control to CN will be followed as described by EAR&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/computune"&gt; /u/computune &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1no4exb"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no4exb/i_upgrade_4090s_to_have_48gb_vram_comparative_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no4exb/i_upgrade_4090s_to_have_48gb_vram_comparative_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T01:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnt1bw</id>
    <title>3 Qwen3-Omni models have been released</title>
    <updated>2025-09-22T17:36:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Captioner&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-Omni-30B-A3B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni-modal foundation models. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several architectural upgrades to improve performance and efficiency. Key features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-art across modalities&lt;/strong&gt;: Early text-first pretraining and mixed multimodal training provide native multimodal support. While achieving strong audio and audio-video results, unimodal text and image performance does not regress. Reaches SOTA on 22 of 36 audio/video benchmarks and open-source SOTA on 32 of 36; ASR, audio understanding, and voice conversation performance is comparable to Gemini 2.5 Pro.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual&lt;/strong&gt;: Supports 119 text languages, 19 speech input languages, and 10 speech output languages. &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Speech Input&lt;/strong&gt;: English, Chinese, Korean, Japanese, German, Russian, Italian, French, Spanish, Portuguese, Malay, Dutch, Indonesian, Turkish, Vietnamese, Cantonese, Arabic, Urdu.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speech Output&lt;/strong&gt;: English, Chinese, French, German, Russian, Italian, Spanish, Portuguese, Japanese, Korean.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel Architecture&lt;/strong&gt;: MoE-based Thinker–Talker design with AuT pretraining for strong general representations, plus a multi-codebook design that drives latency to a minimum.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Real-time Audio/Video Interaction&lt;/strong&gt;: Low-latency streaming with natural turn-taking and immediate text or speech responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Control&lt;/strong&gt;: Customize behavior via system prompts for fine-grained control and easy adaptation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Detailed Audio Captioner&lt;/strong&gt;: Qwen3-Omni-30B-A3B-Captioner is now open source: a general-purpose, highly detailed, low-hallucination audio captioning model that fills a critical gap in the open-source community.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Below is the description of all Qwen3-Omni models. Please select and download the model that fits your needs.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model Name&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Instruct&lt;/td&gt; &lt;td align="left"&gt;The Instruct model of Qwen3-Omni-30B-A3B, containing both thinker and talker, supporting audio, video, and text input, with audio and text output. For more information, please read the &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;Qwen3-Omni Technical Report&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Thinking&lt;/td&gt; &lt;td align="left"&gt;The Thinking model of Qwen3-Omni-30B-A3B, containing the thinker component, equipped with chain-of-thought reasoning, supporting audio, video, and text input, with text output. For more information, please read the &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/assets/Qwen3_Omni.pdf"&gt;Qwen3-Omni Technical Report&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Omni-30B-A3B-Captioner&lt;/td&gt; &lt;td align="left"&gt;A downstream audio fine-grained caption model fine-tuned from Qwen3-Omni-30B-A3B-Instruct, which produces detailed, low-hallucination captions for arbitrary audio inputs. It contains the thinker, supporting audio input and text output. For more information, you can refer to the model's &lt;a href="https://github.com/QwenLM/Qwen3-Omni/blob/main/cookbooks/omni_captioner.ipynb"&gt;cookbook&lt;/a&gt;.&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnt1bw/3_qwen3omni_models_have_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T17:36:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1no765m</id>
    <title>how is qwen shipping so hard</title>
    <updated>2025-09-23T03:41:27+00:00</updated>
    <author>
      <name>/u/Background-Pepper-38</name>
      <uri>https://old.reddit.com/user/Background-Pepper-38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt; &lt;img alt="how is qwen shipping so hard" src="https://b.thumbs.redditmedia.com/Ix-BRKnwE48eZZgCfEQXo4d7D9ctp0BS15z0d4MC0UE.jpg" title="how is qwen shipping so hard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d0jhab945uqf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d657e419c81e1261e3204d12b2f4c3a658aaa428"&gt;https://preview.redd.it/d0jhab945uqf1.png?width=950&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d657e419c81e1261e3204d12b2f4c3a658aaa428&lt;/a&gt;&lt;/p&gt; &lt;p&gt;yes, how is qwen shipping so hard&lt;br /&gt; but too many variants exist that I can't decide which one to use&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Background-Pepper-38"&gt; /u/Background-Pepper-38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1no765m/how_is_qwen_shipping_so_hard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T03:41:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1noefxl</id>
    <title>Parkiet: Fine-tuning Dia for any language</title>
    <updated>2025-09-23T11:09:00+00:00</updated>
    <author>
      <name>/u/pevers</name>
      <uri>https://old.reddit.com/user/pevers</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"&gt; &lt;img alt="Parkiet: Fine-tuning Dia for any language" src="https://preview.redd.it/r8293025cwqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=755e9d237792b112ecde2c2f108d85f0258fc59a" title="Parkiet: Fine-tuning Dia for any language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;A lot of the open-source TTS models are released for English or Chinese and lack support for other languages. I was curious to see if I could train a state-of-the-art text-to-speech (TTS) model for Dutch by using Google's free TPU Research credits. I open-sourced the weights, and documented the whole journey, from Torch model conversion, data preparation, JAX training code and inference pipeline here &lt;a href="https://github.com/pevers/parkiet"&gt;https://github.com/pevers/parkiet&lt;/a&gt; . Hopefully it can serve as a guide for others that are curious to train these models for other languages (without burning through all the credits trying to fix the pipeline). &lt;/p&gt; &lt;p&gt;Spoiler: the results are great! I believe they are *close* to samples generated with ElevenLabs. I spent about $300, mainly on GCS egress. Sample comparison can be found here &lt;a href="https://peterevers.nl/posts/2025/09/parkiet/"&gt;https://peterevers.nl/posts/2025/09/parkiet/&lt;/a&gt; .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pevers"&gt; /u/pevers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r8293025cwqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noefxl/parkiet_finetuning_dia_for_any_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T11:09:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1noe09l</id>
    <title>2 new open source models from Qwen today</title>
    <updated>2025-09-23T10:44:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt; &lt;img alt="2 new open source models from Qwen today" src="https://preview.redd.it/goah9v2r8wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07a67dbd5f99e7851c1f27295952913b340ead4d" title="2 new open source models from Qwen today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/goah9v2r8wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nodc6q</id>
    <title>How are they shipping so fast 💀</title>
    <updated>2025-09-23T10:04:43+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt; &lt;img alt="How are they shipping so fast 💀" src="https://preview.redd.it/8higdv9r1wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6b83f7127ef234c48c0416953381b9c3c6004a7" title="How are they shipping so fast 💀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well good for us &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8higdv9r1wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building 🔨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio 👾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
