<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-26T22:06:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1og9ije</id>
    <title>How good is Ling-1T?</title>
    <updated>2025-10-26T02:35:28+00:00</updated>
    <author>
      <name>/u/Aware_Magician7958</name>
      <uri>https://old.reddit.com/user/Aware_Magician7958</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"&gt; &lt;img alt="How good is Ling-1T?" src="https://preview.redd.it/cs7bb6igbdxf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be874b0392daf9b87a138381fb588243291b71a3" title="How good is Ling-1T?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Apparently there's been a new model by Ant Group (InclusionAI) that is an open-weight non-thinking model with 1000B parameters. According to their article their performance is better than paid models. Has anyone run this yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aware_Magician7958"&gt; /u/Aware_Magician7958 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cs7bb6igbdxf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og9ije/how_good_is_ling1t/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T02:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogqv0p</id>
    <title>Anyone have experience with Local Motion Capture models?</title>
    <updated>2025-10-26T17:43:10+00:00</updated>
    <author>
      <name>/u/onil34</name>
      <uri>https://old.reddit.com/user/onil34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can only find datasets on hugging face but not the models. if anyone has any ideas. that would be appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil34"&gt; /u/onil34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqv0p/anyone_have_experience_with_local_motion_capture/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqv0p/anyone_have_experience_with_local_motion_capture/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogqv0p/anyone_have_experience_with_local_motion_capture/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T17:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1og3cnt</id>
    <title>Llama.cpp model conversion guide</title>
    <updated>2025-10-25T21:35:28+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt; &lt;img alt="Llama.cpp model conversion guide" src="https://external-preview.redd.it/9H8ID2ho6Hmpcg_iEkUBl0dzrALfa1J8gFxkkoi6ojc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=960cfd58e78583a22cdad3922567cd461d36ac4b" title="Llama.cpp model conversion guide" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since the open source community always benefits by having more people do stuff, I figured I would capitalize on my experiences with a few architectures I've done and add a guide for people who, like me, would like to gain practical experience by porting a model architecture.&lt;/p&gt; &lt;p&gt;Feel free to propose any topics / clarifications and ask any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16770"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og3cnt/llamacpp_model_conversion_guide/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T21:35:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1og87pd</id>
    <title>If you had $4k, would you invest in a DGX Spark?</title>
    <updated>2025-10-26T01:28:00+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Guys, I am very curious what everyone's opinion is regarding the DGX Spark. &lt;/p&gt; &lt;p&gt;If you had $4k and you needed to use that money to start building out your own personal AI data center, would you buy a DGX Spark... or go a different direction?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1og87pd/if_you_had_4k_would_you_invest_in_a_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T01:28:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogfmt4</id>
    <title>GLM 4.5 air for coding</title>
    <updated>2025-10-26T08:46:12+00:00</updated>
    <author>
      <name>/u/Magnus114</name>
      <uri>https://old.reddit.com/user/Magnus114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You who use a local glm 4.5 air for coding, can you please share your software setup?&lt;/p&gt; &lt;p&gt;I have had some success with unsloth q4_k_m on llama.cpp with opencode. To get the tool usage to work I had to use a jinja template from a pull request, and still the tool calling fails occasionally. Tried unsloth jinja template from glm 4.6, but no success. Also experimented with claude code with open router with a similar result. Considering to trying to write my own template and also trying with vllm.&lt;/p&gt; &lt;p&gt;Would love to hear how others are using glm 4.5 air. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Magnus114"&gt; /u/Magnus114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogfmt4/glm_45_air_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T08:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ognbw2</id>
    <title>Call for feedback on an open-source RAG API platform that can run with local LLMs</title>
    <updated>2025-10-26T15:22:29+00:00</updated>
    <author>
      <name>/u/brodagaita</name>
      <uri>https://old.reddit.com/user/brodagaita</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We've just launched &lt;a href="https://github.com/skaldlabs/skald"&gt;Skald&lt;/a&gt;, an API platform for building AI apps. It's MIT-licensed and self-hostable, and we've actually made it work with both local embedding models and a locally-hosted LLM. We're new to this space but we believe it's important for people to have the option to run AI applications without sending the data to third-parties.&lt;/p&gt; &lt;p&gt;Keen to hear from people in this community if this works with your setup and what improvement suggestions you'd have! Here are &lt;a href="https://docs.useskald.com/docs/self-host/full-local"&gt;our docs for self-hosting with no third-parties&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brodagaita"&gt; /u/brodagaita &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ognbw2/call_for_feedback_on_an_opensource_rag_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ognbw2/call_for_feedback_on_an_opensource_rag_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ognbw2/call_for_feedback_on_an_opensource_rag_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T15:22:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oguocr</id>
    <title>Running local models with multiple backends &amp; search capabilities</title>
    <updated>2025-10-26T20:13:44+00:00</updated>
    <author>
      <name>/u/Ibz04</name>
      <uri>https://old.reddit.com/user/Ibz04</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oguocr/running_local_models_with_multiple_backends/"&gt; &lt;img alt="Running local models with multiple backends &amp;amp; search capabilities" src="https://external-preview.redd.it/NmtybGkwa2draXhmMWYfCBFJq_CgHTgLeaR86wcT1wa8nSuVfXl8XGpmTK5H.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0a5be46fac7dcbac9219ee8930d5d26e6cc02bde" title="Running local models with multiple backends &amp;amp; search capabilities" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, I’m currently using this desktop app to run llms with ollama,llama.cpp and web gpu at the same place, there’s also a web version that stores the models to cache memory What do you guys suggest for extension of capabilities &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ibz04"&gt; /u/Ibz04 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/oixys9qgkixf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oguocr/running_local_models_with_multiple_backends/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oguocr/running_local_models_with_multiple_backends/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T20:13:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogrjo2</id>
    <title>Choosing the right model</title>
    <updated>2025-10-26T18:09:44+00:00</updated>
    <author>
      <name>/u/Bowdenzug</name>
      <uri>https://old.reddit.com/user/Bowdenzug</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need your opinion/help. I'm looking for a self-hosted LLM that's perfect at tool calling and also has logical reasoning/understanding (it should be somewhat familiar with tax/invoicing and legal issues). I currently have 48 GB of VRAM available. I was thinking about using llama3.1 70b instruct awq. I would describe everything in detail in the system prompt, what it should do and how, what superficial rules there are, etc. I've already tested a few models, like Llama3.1 8b Instruct, but it's quite poor in terms of the context for tool calling. Qwen3 32b works quite well but unfortunately fails at tool calling with VLLM openapi and langchain ChatOpenAi. Thanks in advance :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bowdenzug"&gt; /u/Bowdenzug &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogrjo2/choosing_the_right_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogrjo2/choosing_the_right_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogrjo2/choosing_the_right_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T18:09:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogvgad</id>
    <title>LLMs Keep Messing Up My Code After 600 Lines</title>
    <updated>2025-10-26T20:44:45+00:00</updated>
    <author>
      <name>/u/haterloco</name>
      <uri>https://old.reddit.com/user/haterloco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi! I’ve been testing various local LLMs, even closed Gemini and ChatGPT, but once my code exceeds ~600 lines, they start deleting or adding placeholder content instead of finishing the task. Oddly, sometimes they handle 1,000+ lines just fine. &lt;/p&gt; &lt;p&gt;Do you know any that can manage that amount of code reliably?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/haterloco"&gt; /u/haterloco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogvgad/llms_keep_messing_up_my_code_after_600_lines/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogvgad/llms_keep_messing_up_my_code_after_600_lines/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogvgad/llms_keep_messing_up_my_code_after_600_lines/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T20:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogq54x</id>
    <title>Ryzen AI Max+ 395 vs RTX 4000 ada SFF</title>
    <updated>2025-10-26T17:15:17+00:00</updated>
    <author>
      <name>/u/dougmaitelli</name>
      <uri>https://old.reddit.com/user/dougmaitelli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;Quick question to you all.&lt;/p&gt; &lt;p&gt;Context: I have a RTX 4000 ada that was just sitting in a drawer here. Also had a unused machine with a 10th gen i7 and 64gb of ram collecting dust. I decided to put them together and try to run ollama on Ubuntu.&lt;/p&gt; &lt;p&gt;I am getting about 31 tokens per second with Gemma3:12b.&lt;/p&gt; &lt;p&gt;However, the system is too big and I want something compact, so I bought a GMKtec with the Ryzen AI Max+ 395 and 64gb of shared memory.&lt;/p&gt; &lt;p&gt;The GMKtec is doing 24 tokens per second on the same model on windows ollama.&lt;/p&gt; &lt;p&gt;I saw some people here having like 40 tokens per second with the Ryzen AI Max+ 395 with models of like 37b parameters.&lt;/p&gt; &lt;p&gt;So, what am I missing here? Is my expectation that the Ryzen should be faster for llm wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dougmaitelli"&gt; /u/dougmaitelli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogq54x/ryzen_ai_max_395_vs_rtx_4000_ada_sff/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogq54x/ryzen_ai_max_395_vs_rtx_4000_ada_sff/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogq54x/ryzen_ai_max_395_vs_rtx_4000_ada_sff/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T17:15:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogaohi</id>
    <title>MiniMax: MiniMax M2 seems to VERY, VERY good</title>
    <updated>2025-10-26T03:39:02+00:00</updated>
    <author>
      <name>/u/klippers</name>
      <uri>https://old.reddit.com/user/klippers</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Generally use GLM4.6 , been at a few problems most of the week, today threw these at MiniMax: MiniMax M2 and it sorted them with no fuss......Very impressed!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/klippers"&gt; /u/klippers &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogaohi/minimax_minimax_m2_seems_to_very_very_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:39:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oga7um</id>
    <title>All the models seem to love using the same names.</title>
    <updated>2025-10-26T03:13:40+00:00</updated>
    <author>
      <name>/u/AI_Renaissance</name>
      <uri>https://old.reddit.com/user/AI_Renaissance</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In particular thorn and vance when doing horror or science fiction, for a woman its almost always elara vance, and if there is a male doctor or scientist, usually thomas thorn. Has anyone else experienced this?&lt;/p&gt; &lt;p&gt;Right now I mostly use Cydonia which is a pretty good local model, but this even happens on the perchance ai website. It's funny, but annoying. I think maybe the training data eating itself with merges.&lt;/p&gt; &lt;p&gt;For example, try a prompt like &amp;quot;write a story about a mad scientist that creates a monster&amp;quot;. The name of the scientist will most likely be something like Dr. Aris or Thomas Thorne. Its not a that big of a deal if you come up with your own names for characters. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AI_Renaissance"&gt; /u/AI_Renaissance &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oga7um/all_the_models_seem_to_love_using_the_same_names/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T03:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogsng2</id>
    <title>[P] VibeVoice-Hindi-7B: Open-Source Expressive Hindi TTS with Multi-Speaker + Voice Cloning</title>
    <updated>2025-10-26T18:53:08+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Released VibeVoice-Hindi-7B and VibeVoice-Hindi-LoRA — fine-tuned versions of the Microsoft VibeVoice model, bringing frontier Hindi text-to-speech with long-form synthesis, multi-speaker support, and voice cloning.&lt;/p&gt; &lt;p&gt;• Full Model: &lt;a href="https://huggingface.co/tarun7r/vibevoice-hindi-7b"&gt;https://huggingface.co/tarun7r/vibevoice-hindi-7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;• LoRA Adapters: &lt;a href="https://huggingface.co/tarun7r/vibevoice-hindi-lora"&gt;https://huggingface.co/tarun7r/vibevoice-hindi-lora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;• Base Model: &lt;a href="https://huggingface.co/vibevoice/VibeVoice-7B"&gt;https://huggingface.co/vibevoice/VibeVoice-7B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Features: • Natural Hindi speech synthesis with expressive prosody&lt;/p&gt; &lt;p&gt;• Multi-speaker dialogue generation&lt;/p&gt; &lt;p&gt;• Voice cloning from short reference samples (10–30 seconds)&lt;/p&gt; &lt;p&gt;• Long-form audio generation (up to 45 minutes context)&lt;/p&gt; &lt;p&gt;• Works with VibeVoice community pipeline and ComfyUI&lt;/p&gt; &lt;p&gt;Tech Stack: • Qwen2.5-7B LLM backbone with LoRA fine-tuning&lt;/p&gt; &lt;p&gt;• Acoustic (σ-VAE) + semantic tokenizers @ 7.5 Hz&lt;/p&gt; &lt;p&gt;• Diffusion head (~600M params) for high-fidelity acoustics&lt;/p&gt; &lt;p&gt;• 32k token context window&lt;/p&gt; &lt;p&gt;Released under MIT License. Feedback and contributions welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogsng2/p_vibevoicehindi7b_opensource_expressive_hindi/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogsng2/p_vibevoicehindi7b_opensource_expressive_hindi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogsng2/p_vibevoicehindi7b_opensource_expressive_hindi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T18:53:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogh8ec</id>
    <title>Using GLM 4.6 to understand it's limitations</title>
    <updated>2025-10-26T10:27:40+00:00</updated>
    <author>
      <name>/u/Vozer_bros</name>
      <uri>https://old.reddit.com/user/Vozer_bros</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt; &lt;img alt="Using GLM 4.6 to understand it's limitations" src="https://a.thumbs.redditmedia.com/E-b1AYjpK-0wB4j8lY7BJQb9A8ucue0uF7crpypRrQ0.jpg" title="Using GLM 4.6 to understand it's limitations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gq9yiommnfxf1.png?width=1994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04be61d0c1fe988448c06878ea77b577ddd6aee1"&gt;https://preview.redd.it/gq9yiommnfxf1.png?width=1994&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=04be61d0c1fe988448c06878ea77b577ddd6aee1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The actual loosing point will start at 30% less than the number in the table. For example, tool calling actually starting to fail randomly at 70k context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vozer_bros"&gt; /u/Vozer_bros &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogh8ec/using_glm_46_to_understand_its_limitations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T10:27:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogjkfj</id>
    <title>Cheaper &amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI</title>
    <updated>2025-10-26T12:37:45+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt; &lt;img alt="Cheaper &amp;amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI" src="https://b.thumbs.redditmedia.com/v4DjQ3ybawpfEVteS_F8SfFp6OReWcTch-9YAwPpLfs.jpg" title="Cheaper &amp;amp; faster LLM stack in 2025: Kimi/Qwen vs OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/v0ddm42g9gxf1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f75f7809cead99b006dc49dc76a53f453f06a8f"&gt;Chamath Palihapitiya&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dmbx1rcl9gxf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=154810c46f400e52c2ef4cef6c6a44c79fab9fef"&gt;https://preview.redd.it/dmbx1rcl9gxf1.png?width=1196&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=154810c46f400e52c2ef4cef6c6a44c79fab9fef&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The valley is built on open-source models?&lt;/p&gt; &lt;p&gt;On the All-In podcast, Chamath Palihapitiya says his team redirected a ton of workloads to &lt;strong&gt;Kimi K2&lt;/strong&gt; because it was “&lt;strong&gt;way more performant&lt;/strong&gt;” and “&lt;strong&gt;a ton cheaper&lt;/strong&gt;” than OpenAI and Anthropic.&lt;/p&gt; &lt;p&gt;Airbnb CEO Brian Chesky says they’re relying a lot on Alibaba’s &lt;strong&gt;Qwen&lt;/strong&gt; in production because it’s “&lt;strong&gt;fast and cheap.&lt;/strong&gt;” They still use OpenAI’s latest models, but “typically don’t use them that much in production” due to &lt;strong&gt;faster/cheaper&lt;/strong&gt; options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjkfj/cheaper_faster_llm_stack_in_2025_kimiqwen_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:37:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ofu15a</id>
    <title>I rebuilt DeepSeek’s OCR model in Rust so anyone can run it locally (no Python!)</title>
    <updated>2025-10-25T15:13:52+00:00</updated>
    <author>
      <name>/u/Outrageous-Voice</name>
      <uri>https://old.reddit.com/user/Outrageous-Voice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks! After wrestling with the original DeepSeek-OCR release (Python + Transformers, tons of dependencies, zero UX), I decided to port the whole inference stack to Rust. The repo is deepseek-ocr.rs (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt;) and it ships both a CLI and an OpenAI-compatible server so you can drop it straight into existing clients like Open WebUI.&lt;/p&gt; &lt;h1&gt;Why bother?&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No Python, no conda—just a single Rust binary.&lt;/li&gt; &lt;li&gt;Works offline and keeps documents private.&lt;/li&gt; &lt;li&gt;Fully OpenAI-compatible, so existing SDKs/ChatGPT-style UIs “just work”.&lt;/li&gt; &lt;li&gt;Apple Silicon support with optional Metal acceleration (FP16).&lt;/li&gt; &lt;li&gt;Built-in Hugging Face downloader: config/tokenizer/weights (≈6.3 GB) fetch automatically; needs about 13 GB RAM to run.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What’s inside the Rust port?&lt;/h1&gt; &lt;p&gt;- Candle-based reimplementation of the language model (DeepSeek-V2) with KV caches + optional FlashAttention.&lt;/p&gt; &lt;p&gt;- Full SAM + CLIP vision pipeline, image tiling, projector, and tokenizer alignment identical to the PyTorch release.&lt;/p&gt; &lt;p&gt;- Rocket server that exposes /v1/responses and /v1/chat/completions (OpenAI-compatible streaming included).&lt;/p&gt; &lt;p&gt;- Single-turn prompt compaction so OCR doesn’t get poisoned by multi-turn history.&lt;/p&gt; &lt;p&gt;- Debug hooks to compare intermediate tensors against the official model (parity is already very close).&lt;/p&gt; &lt;h1&gt;Getting started&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;You can download prebuilt archives (macOS with Metal, Windows) from the latest successful run of the repo’s GitHub Actions “build-binaries (&lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs/actions/workflows/build-binaries.yml)”&lt;/a&gt;”) workflow—no local build required.&lt;/li&gt; &lt;li&gt;Prefer compiling? git clone &lt;a href="https://github.com/TimmyOVO/deepseek-ocr.rs"&gt;https://github.com/TimmyOVO/deepseek-ocr.rs&lt;/a&gt; → cargo fetch&lt;/li&gt; &lt;li&gt;CLI: cargo run -p deepseek-ocr-cli -- --prompt &amp;quot;&amp;lt;image&amp;gt;...&amp;quot; --image mydoc.png&lt;/li&gt; &lt;li&gt;Server: cargo run -p deepseek-ocr-server -- --host &lt;a href="http://0.0.0.0"&gt;0.0.0.0&lt;/a&gt; --port 8000&lt;/li&gt; &lt;li&gt;On macOS, add --features metal plus --device metal --dtype f16 for GPU acceleration.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Use cases&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Batch document conversion (receipts → markdown, contracts → summaries, etc.).&lt;/li&gt; &lt;li&gt;Plugging into Open WebUI (looks/feels like ChatGPT but runs YOUR OCR model).&lt;/li&gt; &lt;li&gt;Building document QA bots that need faithful extraction.&lt;strong&gt;If you try it, I’d love to hear your feedback—feature requests, edge cases, performance reports, all welcome. And if it saves you from Python dependency hell, toss the repo a ⭐️.&lt;/strong&gt;Cheers!&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Voice"&gt; /u/Outrageous-Voice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ofu15a/i_rebuilt_deepseeks_ocr_model_in_rust_so_anyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-25T15:13:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogrnxv</id>
    <title>780M IGPU for Rocm and Vulkan Ubuntu instructions. (Original from MLDataScientist)</title>
    <updated>2025-10-26T18:14:20+00:00</updated>
    <author>
      <name>/u/Mnemoc</name>
      <uri>https://old.reddit.com/user/Mnemoc</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Getting llama.cpp Running on AMD 780M (Ubuntu Server 25.04)&lt;/h1&gt; &lt;p&gt;I cannot take credit for this guide—it builds on the work shared by MLDataScientist in this thread:&lt;br /&gt; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"&gt;gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM : r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what I had to do to get everything running on my MinisForum UM890 Pro (Ryzen 9 8945HS, 96 GB DDR5-5600).&lt;br /&gt; &lt;a href="https://www.amazon.com/dp/B0D9YLQMHX"&gt;https://www.amazon.com/dp/B0D9YLQMHX&lt;/a&gt;&lt;/p&gt; &lt;p&gt;These notes capture a working configuration for running llama.cpp with both ROCm and Vulkan backends on a MinisForum mini PC with a Radeon 780M iGPU. Steps were validated on Ubuntu 25.04.&lt;/p&gt; &lt;h2&gt;Step 1: Base Install&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Install Ubuntu 25.04 (or newer) on the mini PC.&lt;/li&gt; &lt;li&gt;Create an admin user (referenced as &lt;code&gt;myusername&lt;/code&gt;).&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Step 2: Kernel 6.17.5&lt;/h2&gt; &lt;p&gt;Upgrade the kernel with &lt;a href="http://ubuntu-mainline-kernel.sh"&gt;&lt;code&gt;ubuntu-mainline-kernel.sh&lt;/code&gt;&lt;/a&gt; and reboot into the new kernel. &lt;code&gt;bash sudo apt update sudo apt upgrade lsb_release -a git clone https://github.com/pimlie/ubuntu-mainline-kernel.sh.git cd ubuntu-mainline-kernel.sh sudo ./ubuntu-mainline-kernel.sh -i 6.17.5 &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Step 3: GTT/TTM Memory Tuning&lt;/h2&gt; &lt;p&gt;&lt;code&gt;bash sudo tee /etc/modprobe.d/amdgpu_llm_optimized.conf &amp;gt; /dev/null &amp;lt;&amp;lt;'EOF' options amdgpu gttsize=89000 options ttm pages_limit=23330816 options ttm page_pool_size=23330816 EOF &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This reserves roughly 87 GiB of RAM for the iGPU GTT pool. Reduce &lt;code&gt;gttsize&lt;/code&gt; (e.g., &lt;code&gt;87000&lt;/code&gt;) if the allocation fails.&lt;/p&gt; &lt;p&gt;Reboot, then verify the allocation:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash sudo dmesg | egrep &amp;quot;amdgpu: .*memory&amp;quot; &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Expected lines:&lt;/p&gt; &lt;p&gt;&lt;code&gt;text amdgpu: 1024M of VRAM memory ready amdgpu: 89000M of GTT memory ready &lt;/code&gt;&lt;/p&gt; &lt;h3&gt;GRUB Flags&lt;/h3&gt; &lt;p&gt;I did not need to tweak GRUB flags. See the original thread if you want to experiment there.&lt;/p&gt; &lt;h2&gt;Step 4: Grab llama.cpp Builds&lt;/h2&gt; &lt;p&gt;Keep two directories so you can swap backends freely:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Vulkan build (official ggml):&lt;/strong&gt; &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt; → &lt;code&gt;~/llama-vulkan/&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ROCm build (lemonade SDK, gfx110x):&lt;/strong&gt; &lt;a href="https://github.com/lemonade-sdk/llamacpp-rocm/releases/tag/b1090"&gt;https://github.com/lemonade-sdk/llamacpp-rocm/releases/tag/b1090&lt;/a&gt; → &lt;code&gt;~/llama-rocm/&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;After extracting, make the binaries executable:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash chmod +x ~/llama-*/llama-* &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Step 5: Render Node Permissions&lt;/h2&gt; &lt;p&gt;If you hit &lt;code&gt;Permission denied&lt;/code&gt; on &lt;code&gt;/dev/dri/renderD128&lt;/code&gt;, add yourself to the &lt;code&gt;render&lt;/code&gt; group and re-login (or reboot).&lt;/p&gt; &lt;p&gt;```bash vulkaninfo | grep &amp;quot;deviceName&amp;quot;&lt;/p&gt; &lt;p&gt;ls -l /dev/dri/renderD128&lt;/p&gt; &lt;h1&gt;crw-rw---- 1 root render 226, 128 Oct 26 03:35 /dev/dri/renderD128&lt;/h1&gt; &lt;p&gt;sudo usermod -aG render myusername ```&lt;/p&gt; &lt;h2&gt;Step 6: Vulkan Runtime Packages&lt;/h2&gt; &lt;p&gt;Sample startup output from the Vulkan build:&lt;/p&gt; &lt;p&gt;&lt;code&gt;text ./llama-cli load_backend: loaded RPC backend from /home/myuser/llama-vulkan/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV PHOENIX) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat load_backend: loaded Vulkan backend from /home/myuser/llama-vulkan/libggml-vulkan.so load_backend: loaded CPU backend from /home/myuser/llama-vulkan/libggml-cpu-icelake.so build: 6838 (226f295f4) with cc (Ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 for x86_64-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon Graphics (RADV PHOENIX)) (0000:c6:00.0) - 60638 MiB free &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Step 7: Sanity Check ROCm Build&lt;/h2&gt; &lt;p&gt;Sample startup output:&lt;/p&gt; &lt;p&gt;&lt;code&gt;text ./llama-cli ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx1103 (0x1103), VMM: no, Wave Size: 32 build: 1 (226f295) with AMD clang version 20.0.0git (https://github.com/ROCm/llvm-project.git a7d47b26ca0ec0b3e9e4da83825cace5d761f4bc+PATCHED:e34a5237ae1cb2b3c21abdf38b24bb3e634f7537) for x86_64-unknown-linux-gnu main: llama backend init main: load the model and apply lora adapter, if any llama_model_load_from_file_impl: using device ROCm0 (AMD Radeon Graphics) (0000:c6:00.0) - 89042 MiB free &lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Step 8: Sanity Check Vulkan Build&lt;/h2&gt; &lt;p&gt;Sample startup output:&lt;/p&gt; &lt;p&gt;&lt;code&gt;text ./llama-cli ggml_vulkan: Found 1 Vulkan devices: 0 = AMD Radeon Graphics (RADV PHOENIX) (radv) | uma: 1 | fp16: 1 | bf16: 0 load_backend: loaded Vulkan backend ... llama_model_load_from_file_impl: using device Vulkan0 (AMD Radeon Graphics (RADV PHOENIX)) (0000:c6:00.0) - 60638 MiB free &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Maybe this helps someone else navigate the setup. Sharing in case it saves you a few hours.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: Fixing Reddit markdown because I suck at it.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mnemoc"&gt; /u/Mnemoc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogrnxv/780m_igpu_for_rocm_and_vulkan_ubuntu_instructions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogrnxv/780m_igpu_for_rocm_and_vulkan_ubuntu_instructions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogrnxv/780m_igpu_for_rocm_and_vulkan_ubuntu_instructions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T18:14:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogigyu</id>
    <title>Is SSM dead now?</title>
    <updated>2025-10-26T11:39:31+00:00</updated>
    <author>
      <name>/u/Spapoxl</name>
      <uri>https://old.reddit.com/user/Spapoxl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried researching about it and found almost all of the news and information is 1 years ago. Is it discontinued? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spapoxl"&gt; /u/Spapoxl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogigyu/is_ssm_dead_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T11:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogo2jv</id>
    <title>I made a 1B model to generate 3d files (barely)</title>
    <updated>2025-10-26T15:52:22+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;2 weeks ago, I finetuned Gemma3 1B on Synthetic 3D file data. I called the model K-1B.&lt;/p&gt; &lt;p&gt;Yesterday I packaged it into an app, hosting the model on Modal.&lt;/p&gt; &lt;p&gt;I would appreciate any feedback as this is a hobby project that I will keep on training the model etc.&lt;/p&gt; &lt;p&gt;Thanks :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://cadmonkey.web.app"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo2jv/i_made_a_1b_model_to_generate_3d_files_barely/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogo2jv/i_made_a_1b_model_to_generate_3d_files_barely/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T15:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogjtkn</id>
    <title>Poor GPU Club : Good Worthy Pruned models?</title>
    <updated>2025-10-26T12:50:38+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Wanted to explore more on this after seeing recent threads( &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oefu29/cerebras_reapd_glm46_25_30_40_pruned_fp8/"&gt;3&lt;/a&gt; , &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1obrde8/cerebras_reap_update_pruned_checkpoints_for/"&gt;2&lt;/a&gt; , &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o98f57/new_from_cerebras_reap_the_experts_why_pruning/"&gt;1&lt;/a&gt; ) from Cerebras. They already pruned few MOE models such as Qwen3-Coder-30B, Qwen3-Coder-480B, GLM-4.5-Air, GLM-4.6. I'm just waiting for few small MOE models from them, hope they do soon or later.&lt;/p&gt; &lt;p&gt;Meanwhile &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1octe2s/pruned_moe_reap_quants_for_testing/"&gt;one other person pruned few other MOE models&lt;/a&gt;(Qwen3-30B, Qwen3-30B-Instruct, Qwen3-Coder-30B, GPT-OSS-20B, GPT-OSS-120B) using same Reap by Cerebras.&lt;/p&gt; &lt;p&gt;I'll be trying those small pruned models for sure since I have only 8GB VRAM(and 32GB RAM).&lt;/p&gt; &lt;p&gt;I'm sure some of you might have tried few pruned models before. HuggingFace has 100s of pruned models. Below are links to pruned models with different tags. Of course there must be some more pruned models without below tags. &lt;a href="https://huggingface.co/models?other=pruned"&gt;Pruned&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=prune"&gt;Prune&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=pruning"&gt;Pruning&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=pruned-model"&gt;pruned-model&lt;/a&gt; , &lt;a href="https://huggingface.co/models?other=expert-pruning"&gt;expert-pruning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1]&lt;/strong&gt; Please recommend good worthy pruned models particularly small ones under 50B&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2]&lt;/strong&gt; Cerebras Reap method is only for MOE models. Does anyone came across anything for Dense models? Recently I posted a thread about Q3/Q2 quants of Dense models since I couldn't run those models with high quants like Q4 &amp;amp; above. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1o3zz30/poor_gpu_club_anyone_use_q3q2_quants_of_2040b/"&gt;Anyone use Q3/Q2 quants of 20-40B Dense models? How's it?&lt;/a&gt; Unfortunately I couldn't run even Q3 with bearable t/s.&lt;/p&gt; &lt;p&gt;Currently I'm looking for Pruned models of below ones:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Seed-OSS-36B-Instruct&lt;/li&gt; &lt;li&gt;Devstral-Small-2507&lt;/li&gt; &lt;li&gt;Magistral-Small-2509&lt;/li&gt; &lt;li&gt;Mistral-Small-3.2-24B-Instruct-2506&lt;/li&gt; &lt;li&gt;reka-flash-3.1&lt;/li&gt; &lt;li&gt;Gemma-3-27B-it&lt;/li&gt; &lt;li&gt;Qwen3-32B&lt;/li&gt; &lt;li&gt;GLM-4-32B-0414&lt;/li&gt; &lt;li&gt;And lot of 20B+ finetunes from sources like TheDrummer, SicariusSicariiStuff, etc.,&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It would be great if someone shrink those dense models to 50%(at least 25-35%) so I could use Q4 with decent/bearable t/s with my 8GB VRAM(and 32GB RAM).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogjtkn/poor_gpu_club_good_worthy_pruned_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T12:50:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwjdj</id>
    <title>What is the best local Large Language Model setup for coding on a budget of approximately $2,000?</title>
    <updated>2025-10-26T21:29:22+00:00</updated>
    <author>
      <name>/u/Independent-Band7571</name>
      <uri>https://old.reddit.com/user/Independent-Band7571</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My initial research has highlighted three main hardware options:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;A dedicated GPU with 16–32GB of VRAM.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;A Mac Ultra with 64GB+ of Unified Memory.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;An AMD Strix Halo system with 64–128GB of RAM.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My understanding is that all three options can run similar models at an acceptable t/s speed. In fact, they might even be overpowered if we are focusing on Mixture-of-Experts (MoE) models.&lt;/p&gt; &lt;p&gt;I'm also weighing the following trade-offs:&lt;/p&gt; &lt;p&gt;Mac Ultra: Appears to be the &amp;quot;sweet spot&amp;quot; due to its ease of setup and strong all-around performance, but I have a strong preference against the Apple ecosystem.&lt;/p&gt; &lt;p&gt;Strix Halo: The fully-specced mini-PC versions, often from Chinese manufacturers, already push the $2,000 budget limit. While the lower power consumption is appealing, I'm concerned about a potentially complicated setup and performance bottlenecks from its memory bandwidth and/or throttling due to thermals.&lt;/p&gt; &lt;p&gt;Multi-GPU PC: Building a system with multiple GPUs seems the most future-proof, but the high peak power consumption is a significant concern and hard limits on the models it can run.&lt;/p&gt; &lt;p&gt;What other considerations should I keep in mind? Are there any exciting new developments coming soon (either hardware or models), and should I hold off on buying anything right now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Band7571"&gt; /u/Independent-Band7571 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwjdj/what_is_the_best_local_large_language_model_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwjdj/what_is_the_best_local_large_language_model_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwjdj/what_is_the_best_local_large_language_model_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T21:29:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogwf6b</id>
    <title>M5 Neural Accelerator benchmark results from Llama.cpp</title>
    <updated>2025-10-26T21:24:33+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;LLaMA 7B&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;SoC&lt;/th&gt; &lt;th align="right"&gt;BW [GB/s]&lt;/th&gt; &lt;th align="right"&gt;GPU Cores&lt;/th&gt; &lt;th align="right"&gt;F16 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;F16 TG [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q8_0 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q8_0 TG [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q4_0 PP [t/s]&lt;/th&gt; &lt;th align="right"&gt;Q4_0 TG [t/s]&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 [1]&lt;/td&gt; &lt;td align="right"&gt;68&lt;/td&gt; &lt;td align="right"&gt;7&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;108.21&lt;/td&gt; &lt;td align="right"&gt;7.92&lt;/td&gt; &lt;td align="right"&gt;107.81&lt;/td&gt; &lt;td align="right"&gt;14.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 [1]&lt;/td&gt; &lt;td align="right"&gt;68&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;117.25&lt;/td&gt; &lt;td align="right"&gt;7.91&lt;/td&gt; &lt;td align="right"&gt;117.96&lt;/td&gt; &lt;td align="right"&gt;14.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Pro [1]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="right"&gt;262.65&lt;/td&gt; &lt;td align="right"&gt;12.75&lt;/td&gt; &lt;td align="right"&gt;235.16&lt;/td&gt; &lt;td align="right"&gt;21.95&lt;/td&gt; &lt;td align="right"&gt;232.55&lt;/td&gt; &lt;td align="right"&gt;35.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Pro [1]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;302.14&lt;/td&gt; &lt;td align="right"&gt;12.75&lt;/td&gt; &lt;td align="right"&gt;270.37&lt;/td&gt; &lt;td align="right"&gt;22.34&lt;/td&gt; &lt;td align="right"&gt;266.25&lt;/td&gt; &lt;td align="right"&gt;36.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Max [1]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;24&lt;/td&gt; &lt;td align="right"&gt;453.03&lt;/td&gt; &lt;td align="right"&gt;22.55&lt;/td&gt; &lt;td align="right"&gt;405.87&lt;/td&gt; &lt;td align="right"&gt;37.81&lt;/td&gt; &lt;td align="right"&gt;400.26&lt;/td&gt; &lt;td align="right"&gt;54.61&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Max [1]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;32&lt;/td&gt; &lt;td align="right"&gt;599.53&lt;/td&gt; &lt;td align="right"&gt;23.03&lt;/td&gt; &lt;td align="right"&gt;537.37&lt;/td&gt; &lt;td align="right"&gt;40.20&lt;/td&gt; &lt;td align="right"&gt;530.06&lt;/td&gt; &lt;td align="right"&gt;61.19&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Ultra [1]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;48&lt;/td&gt; &lt;td align="right"&gt;875.81&lt;/td&gt; &lt;td align="right"&gt;33.92&lt;/td&gt; &lt;td align="right"&gt;783.45&lt;/td&gt; &lt;td align="right"&gt;55.69&lt;/td&gt; &lt;td align="right"&gt;772.24&lt;/td&gt; &lt;td align="right"&gt;74.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M1 Ultra [1]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;64&lt;/td&gt; &lt;td align="right"&gt;1168.89&lt;/td&gt; &lt;td align="right"&gt;37.01&lt;/td&gt; &lt;td align="right"&gt;1042.95&lt;/td&gt; &lt;td align="right"&gt;59.87&lt;/td&gt; &lt;td align="right"&gt;1030.04&lt;/td&gt; &lt;td align="right"&gt;83.73&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 [2]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;8&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;147.27&lt;/td&gt; &lt;td align="right"&gt;12.18&lt;/td&gt; &lt;td align="right"&gt;145.91&lt;/td&gt; &lt;td align="right"&gt;21.70&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 [2]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;201.34&lt;/td&gt; &lt;td align="right"&gt;6.72&lt;/td&gt; &lt;td align="right"&gt;181.40&lt;/td&gt; &lt;td align="right"&gt;12.21&lt;/td&gt; &lt;td align="right"&gt;179.57&lt;/td&gt; &lt;td align="right"&gt;21.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Pro [2]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;312.65&lt;/td&gt; &lt;td align="right"&gt;12.47&lt;/td&gt; &lt;td align="right"&gt;288.46&lt;/td&gt; &lt;td align="right"&gt;22.70&lt;/td&gt; &lt;td align="right"&gt;294.24&lt;/td&gt; &lt;td align="right"&gt;37.87&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Pro [2]&lt;/td&gt; &lt;td align="right"&gt;200&lt;/td&gt; &lt;td align="right"&gt;19&lt;/td&gt; &lt;td align="right"&gt;384.38&lt;/td&gt; &lt;td align="right"&gt;13.06&lt;/td&gt; &lt;td align="right"&gt;344.50&lt;/td&gt; &lt;td align="right"&gt;23.01&lt;/td&gt; &lt;td align="right"&gt;341.19&lt;/td&gt; &lt;td align="right"&gt;38.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Max [2]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="right"&gt;600.46&lt;/td&gt; &lt;td align="right"&gt;24.16&lt;/td&gt; &lt;td align="right"&gt;540.15&lt;/td&gt; &lt;td align="right"&gt;39.97&lt;/td&gt; &lt;td align="right"&gt;537.60&lt;/td&gt; &lt;td align="right"&gt;60.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Max [2]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;38&lt;/td&gt; &lt;td align="right"&gt;755.67&lt;/td&gt; &lt;td align="right"&gt;24.65&lt;/td&gt; &lt;td align="right"&gt;677.91&lt;/td&gt; &lt;td align="right"&gt;41.83&lt;/td&gt; &lt;td align="right"&gt;671.31&lt;/td&gt; &lt;td align="right"&gt;65.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Ultra [2]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;60&lt;/td&gt; &lt;td align="right"&gt;1128.59&lt;/td&gt; &lt;td align="right"&gt;39.86&lt;/td&gt; &lt;td align="right"&gt;1003.16&lt;/td&gt; &lt;td align="right"&gt;62.14&lt;/td&gt; &lt;td align="right"&gt;1013.81&lt;/td&gt; &lt;td align="right"&gt;88.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M2 Ultra [2]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;76&lt;/td&gt; &lt;td align="right"&gt;1401.85&lt;/td&gt; &lt;td align="right"&gt;41.02&lt;/td&gt; &lt;td align="right"&gt;1248.59&lt;/td&gt; &lt;td align="right"&gt;66.64&lt;/td&gt; &lt;td align="right"&gt;1238.48&lt;/td&gt; &lt;td align="right"&gt;94.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;🟨 M3 [3]&lt;/td&gt; &lt;td align="right"&gt;100&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;187.52&lt;/td&gt; &lt;td align="right"&gt;12.27&lt;/td&gt; &lt;td align="right"&gt;186.75&lt;/td&gt; &lt;td align="right"&gt;21.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;🟨 M3 Pro [3]&lt;/td&gt; &lt;td align="right"&gt;150&lt;/td&gt; &lt;td align="right"&gt;14&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;272.11&lt;/td&gt; &lt;td align="right"&gt;17.44&lt;/td&gt; &lt;td align="right"&gt;269.49&lt;/td&gt; &lt;td align="right"&gt;30.65&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Pro [3]&lt;/td&gt; &lt;td align="right"&gt;150&lt;/td&gt; &lt;td align="right"&gt;18&lt;/td&gt; &lt;td align="right"&gt;357.45&lt;/td&gt; &lt;td align="right"&gt;9.89&lt;/td&gt; &lt;td align="right"&gt;344.66&lt;/td&gt; &lt;td align="right"&gt;17.53&lt;/td&gt; &lt;td align="right"&gt;341.67&lt;/td&gt; &lt;td align="right"&gt;30.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Max [3]&lt;/td&gt; &lt;td align="right"&gt;300&lt;/td&gt; &lt;td align="right"&gt;30&lt;/td&gt; &lt;td align="right"&gt;589.41&lt;/td&gt; &lt;td align="right"&gt;19.54&lt;/td&gt; &lt;td align="right"&gt;566.40&lt;/td&gt; &lt;td align="right"&gt;34.30&lt;/td&gt; &lt;td align="right"&gt;567.59&lt;/td&gt; &lt;td align="right"&gt;56.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Max [3]&lt;/td&gt; &lt;td align="right"&gt;400&lt;/td&gt; &lt;td align="right"&gt;40&lt;/td&gt; &lt;td align="right"&gt;779.17&lt;/td&gt; &lt;td align="right"&gt;25.09&lt;/td&gt; &lt;td align="right"&gt;757.64&lt;/td&gt; &lt;td align="right"&gt;42.75&lt;/td&gt; &lt;td align="right"&gt;759.70&lt;/td&gt; &lt;td align="right"&gt;66.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Ultra [3]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;60&lt;/td&gt; &lt;td align="right"&gt;1121.80&lt;/td&gt; &lt;td align="right"&gt;42.24&lt;/td&gt; &lt;td align="right"&gt;1085.76&lt;/td&gt; &lt;td align="right"&gt;63.55&lt;/td&gt; &lt;td align="right"&gt;1073.09&lt;/td&gt; &lt;td align="right"&gt;88.40&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M3 Ultra [3]&lt;/td&gt; &lt;td align="right"&gt;800&lt;/td&gt; &lt;td align="right"&gt;80&lt;/td&gt; &lt;td align="right"&gt;1538.34&lt;/td&gt; &lt;td align="right"&gt;39.78&lt;/td&gt; &lt;td align="right"&gt;1487.51&lt;/td&gt; &lt;td align="right"&gt;63.93&lt;/td&gt; &lt;td align="right"&gt;1471.24&lt;/td&gt; &lt;td align="right"&gt;92.14&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 [4]&lt;/td&gt; &lt;td align="right"&gt;120&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;230.18&lt;/td&gt; &lt;td align="right"&gt;7.43&lt;/td&gt; &lt;td align="right"&gt;223.64&lt;/td&gt; &lt;td align="right"&gt;13.54&lt;/td&gt; &lt;td align="right"&gt;221.29&lt;/td&gt; &lt;td align="right"&gt;24.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 Pro [4]&lt;/td&gt; &lt;td align="right"&gt;273&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;381.14&lt;/td&gt; &lt;td align="right"&gt;17.19&lt;/td&gt; &lt;td align="right"&gt;367.13&lt;/td&gt; &lt;td align="right"&gt;30.54&lt;/td&gt; &lt;td align="right"&gt;364.06&lt;/td&gt; &lt;td align="right"&gt;49.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 Pro [4]&lt;/td&gt; &lt;td align="right"&gt;273&lt;/td&gt; &lt;td align="right"&gt;20&lt;/td&gt; &lt;td align="right"&gt;464.48&lt;/td&gt; &lt;td align="right"&gt;17.18&lt;/td&gt; &lt;td align="right"&gt;449.62&lt;/td&gt; &lt;td align="right"&gt;30.69&lt;/td&gt; &lt;td align="right"&gt;439.78&lt;/td&gt; &lt;td align="right"&gt;50.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ M4 Max [4]&lt;/td&gt; &lt;td align="right"&gt;546&lt;/td&gt; &lt;td align="right"&gt;40&lt;/td&gt; &lt;td align="right"&gt;922.83&lt;/td&gt; &lt;td align="right"&gt;31.64&lt;/td&gt; &lt;td align="right"&gt;891.94&lt;/td&gt; &lt;td align="right"&gt;54.05&lt;/td&gt; &lt;td align="right"&gt;885.68&lt;/td&gt; &lt;td align="right"&gt;83.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ &lt;strong&gt;M5 (Neural Accel)&lt;/strong&gt; [5]&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;608.05&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;26.59&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;✅ &lt;strong&gt;M5 (no Accel)&lt;/strong&gt; [5]&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;10&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;252.82&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;27.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;M5 source: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16634"&gt;https://github.com/ggml-org/llama.cpp/pull/16634&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All Apple Silicon results: &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/4167"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogwf6b/m5_neural_accelerator_benchmark_results_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T21:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogtdbg</id>
    <title>What is the real world hit of using PCIe 4.0 instead of PCIe 5.0 with a 5090?</title>
    <updated>2025-10-26T19:21:18+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m trying to be a bit “cheap” and just buy a 5090 for my desktop that is currently running a 3060. It’s a high end build 128gb RAM, video card is the worst part. I’ll probably slowly end up upgrading everything, but I would like to start with the GPU. &lt;/p&gt; &lt;p&gt;I’m assuming someone might have tried this already?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogtdbg/what_is_the_real_world_hit_of_using_pcie_40/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogtdbg/what_is_the_real_world_hit_of_using_pcie_40/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogtdbg/what_is_the_real_world_hit_of_using_pcie_40/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T19:21:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogoyza</id>
    <title>Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)</title>
    <updated>2025-10-26T16:28:19+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"&gt; &lt;img alt="Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)" src="https://preview.redd.it/8a00jiy4ghxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d362439263cafe886a82048ec21177d435463df4" title="Qwen3-VL-32B is really good. Quick test vs several other local models I keep on my workstation (details in comments)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8a00jiy4ghxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogoyza/qwen3vl32b_is_really_good_quick_test_vs_several/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T16:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ogg6sz</id>
    <title>Why didn't LoRA catch on with LLMs?</title>
    <updated>2025-10-26T09:22:15+00:00</updated>
    <author>
      <name>/u/dtdisapointingresult</name>
      <uri>https://old.reddit.com/user/dtdisapointingresult</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Explanation of LoRA for the folks at home&lt;/h2&gt; &lt;p&gt;(skip to next section if you already know what Lora is)&lt;/p&gt; &lt;p&gt;I only know it from the image generation Stable Diffusion world, and I only tried that briefly, so this won't be 100% exact.&lt;/p&gt; &lt;p&gt;Let's say your image generation model is Stable Diffusion 1.5, which came out a few years ago. It can't know the artstyle of a new artist that came up in the past year, let's say his name his Bobsolete.&lt;/p&gt; &lt;p&gt;What lora creators did is create a small dataset of Bobsolete's art, and use it to train SD 1.5 for like 1-2 days. This outputs a small lora file (the SD 1.5 model is 8GB, a lora is like 20MB). Users can download this lora, and when loading SD 1.5, say &amp;quot;also attach Bobsolete.lora to the model&amp;quot;. Now the user is interacting with SD 1.5 that has been augmented with knowledge of Bobsolete. The user can specify &amp;quot;drawn in the style of Bobsolete&amp;quot; and it will work.&lt;/p&gt; &lt;p&gt;Loras are used to add new styles to a model, new unique characters, and so on.&lt;/p&gt; &lt;h2&gt;Back to LLMs&lt;/h2&gt; &lt;p&gt;LLMs apparently support loras, but no one seems to use them. I've never ever seen them discussed on this sub in my 2 years of casual browsing, although I see they exist in the search results.&lt;/p&gt; &lt;p&gt;I was wondering why this hasn't caught on. People could add little bodies of knowledge to an already-released model. For example, you take a solid general model like Gemma 3 27B. Someone could release a lora trained on all scifi books, another based on all major movie scripts, etc. You could then &amp;quot;./llama.cpp -m models/gemma3.gguf --lora models/scifi-books-rev6.lora --lora models/movie-scripts.lora&amp;quot; and try to get Gemma 3 to help you write a modern scifi movie script. You could even focus even more on specific authors, cormac-mccarthy.lora etc.&lt;/p&gt; &lt;p&gt;A more useful/legal example would be attaching current-events-2025.lora to a model whose cutoff date was December 2024. &lt;/p&gt; &lt;p&gt;So why didn't this catch on the way it did in the image world? Is this technology inherently more limited on LLMs? Why does it seem like companies interested in integrating their doc with AI are more focused on RAG than training a Lora on their internal docs?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dtdisapointingresult"&gt; /u/dtdisapointingresult &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ogg6sz/why_didnt_lora_catch_on_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-26T09:22:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
