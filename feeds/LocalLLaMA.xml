<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-10T07:34:53+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nd2ny0</id>
    <title>Local LLM suite on iOS powered by llama cpp - with web search and RAG</title>
    <updated>2025-09-10T02:39:31+00:00</updated>
    <author>
      <name>/u/Independent_Air8026</name>
      <uri>https://old.reddit.com/user/Independent_Air8026</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2ny0/local_llm_suite_on_ios_powered_by_llama_cpp_with/"&gt; &lt;img alt="Local LLM suite on iOS powered by llama cpp - with web search and RAG" src="https://external-preview.redd.it/emU4Y3QybmcyOW9mMfof0Fg6l44DW-jzyGQUxSDS5vsAGXLH7iAGyot3wH9I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=559e6be3e77af0bebef790ddcb0b4e8a93178c46" title="Local LLM suite on iOS powered by llama cpp - with web search and RAG" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on this for a bit and nearly ready to officially release but I‚Äôm building an LLM suite on top of llama with react native and built in some web search and embedding / RAG features and settings.&lt;/p&gt; &lt;p&gt;will be 100% free on App Store soon &lt;/p&gt; &lt;p&gt;just recorded this little demo where llama 3.2 1B Q4 tells me about today‚Äôs news and then the new iPhone 17 &lt;/p&gt; &lt;p&gt;runs significantly faster on real phone and not simulator &lt;/p&gt; &lt;p&gt;I have file upload- has web search- I don‚Äôt have image gen yet&lt;/p&gt; &lt;p&gt;What else am I missing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent_Air8026"&gt; /u/Independent_Air8026 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5dferowg29of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2ny0/local_llm_suite_on_ios_powered_by_llama_cpp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd2ny0/local_llm_suite_on_ios_powered_by_llama_cpp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T02:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd5doy</id>
    <title>I wrote an open source CLI tool to unify multiple openai-compatible servers into a single server</title>
    <updated>2025-09-10T05:02:25+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can check it out &lt;a href="https://github.com/k-koehler/multillama"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wrote this tool because I have multiple &lt;code&gt;llama.cpp&lt;/code&gt; servers spread across many devices, but I wanted to expose a single server from my homelab domain (homelab-ai.example.com) that inherits all of them on a single URL&lt;/p&gt; &lt;p&gt;It works by intercepting the requests (for example to /v1/chat/completions) and forwarding them to the correct model URL&lt;/p&gt; &lt;p&gt;Not sure if anyone finds useful, but I've been running this on my server for a few days and seems to be relatively stable at this point&lt;/p&gt; &lt;p&gt;Hope someone finds this useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd5doy/i_wrote_an_open_source_cli_tool_to_unify_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd5doy/i_wrote_an_open_source_cli_tool_to_unify_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd5doy/i_wrote_an_open_source_cli_tool_to_unify_multiple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T05:02:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncdobh</id>
    <title>Jan-v1-2509 update has been released</title>
    <updated>2025-09-09T08:50:39+00:00</updated>
    <author>
      <name>/u/vibedonnie</name>
      <uri>https://old.reddit.com/user/vibedonnie</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"&gt; &lt;img alt="Jan-v1-2509 update has been released" src="https://b.thumbs.redditmedia.com/st1JW9HoLNL6PMEziYzZnP2vgkaAtr8LUtz9SHQQObY.jpg" title="Jan-v1-2509 update has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Ä¢ continues to outperforms Perplexity Pro on SimpleQA benchmark&lt;/p&gt; &lt;p&gt;‚Ä¢ increased scores in Reasoning &amp;amp; Creativity evals&lt;/p&gt; &lt;p&gt;HuggingFace Model: &lt;a href="https://huggingface.co/janhq/Jan-v1-2509"&gt;https://huggingface.co/janhq/Jan-v1-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HuggingFace GGUF: &lt;a href="https://huggingface.co/janhq/Jan-v1-2509-gguf"&gt;https://huggingface.co/janhq/Jan-v1-2509-gguf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vibedonnie"&gt; /u/vibedonnie &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ncdobh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncdobh/janv12509_update_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T08:50:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncme6t</id>
    <title>ModernBERT just got multilingual - mmBERT by CLSP at The Johns Hopkins University</title>
    <updated>2025-09-09T15:43:00+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ModernBERT just got multilingual (mmBERT)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Small (140M) and Base (307M) versions&lt;/li&gt; &lt;li&gt;Trained on 3T+ tokens from 1800 languages (DCLM, FineWeb, Code ...)&lt;/li&gt; &lt;li&gt;ModernBERT architecture, Gemma 2 tokenizer&lt;/li&gt; &lt;li&gt;8192 context window&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/jhu-clsp/mmbert-a-modern-multilingual-encoder-68b725831d7c6e3acc435ed4"&gt;Model weights collection&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncme6t/modernbert_just_got_multilingual_mmbert_by_clsp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T15:43:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncz8c6</id>
    <title>Will this work as Advertised?</title>
    <updated>2025-09-09T23:57:31+00:00</updated>
    <author>
      <name>/u/Electronic-Jello-633</name>
      <uri>https://old.reddit.com/user/Electronic-Jello-633</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncz8c6/will_this_work_as_advertised/"&gt; &lt;img alt="Will this work as Advertised?" src="https://preview.redd.it/r4a85pkj98of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d9cb2cfb9e8d30dddf94528b7fc8686ce421c25" title="Will this work as Advertised?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Jello-633"&gt; /u/Electronic-Jello-633 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r4a85pkj98of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncz8c6/will_this_work_as_advertised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncz8c6/will_this_work_as_advertised/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T23:57:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd1tqf</id>
    <title>What do you use on 12GB vram?</title>
    <updated>2025-09-10T01:58:53+00:00</updated>
    <author>
      <name>/u/Educational_Wind_360</name>
      <uri>https://old.reddit.com/user/Educational_Wind_360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use: &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;NAME&lt;/th&gt; &lt;th align="left"&gt;SIZE&lt;/th&gt; &lt;th align="left"&gt;MODIFIED&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama3.2:latest&lt;/td&gt; &lt;td align="left"&gt;2.0 GB&lt;/td&gt; &lt;td align="left"&gt;2 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen3:14b&lt;/td&gt; &lt;td align="left"&gt;9.3 GB&lt;/td&gt; &lt;td align="left"&gt;4 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;gemma3:12b&lt;/td&gt; &lt;td align="left"&gt;8.1 GB&lt;/td&gt; &lt;td align="left"&gt;6 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5-coder:14b&lt;/td&gt; &lt;td align="left"&gt;9.0 GB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;qwen2.5-coder:1.5b&lt;/td&gt; &lt;td align="left"&gt;986 MB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;nomic-embed-text:latest&lt;/td&gt; &lt;td align="left"&gt;274 MB&lt;/td&gt; &lt;td align="left"&gt;8 months ago&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Wind_360"&gt; /u/Educational_Wind_360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd1tqf/what_do_you_use_on_12gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T01:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd3f7t</id>
    <title>Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual Search</title>
    <updated>2025-09-10T03:16:42+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Project Page: &lt;a href="https://mini-o3.github.io/"&gt;https://mini-o3.github.io/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/Mini-o3/Mini-o3"&gt;https://github.com/Mini-o3/Mini-o3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Mini-o3/models"&gt;https://huggingface.co/Mini-o3/models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset: &lt;a href="https://huggingface.co/Mini-o3/datasets"&gt;https://huggingface.co/Mini-o3/datasets&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Abstract&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Recent advances in large multimodal models have leveraged image-based tools with reinforcement learning to tackle visual problems. However, existing open-source approaches often exhibit monotonous reasoning patterns and allow only a limited number of interaction turns, making them inadequate for difficult tasks that require trial-and-error exploration. In this work, we address this limitation by scaling up tool-based interactions and introduce Mini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of steps -- and achieves state-of-the-art performance on challenging visual search tasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key components. First, we construct the Visual Probe Dataset, a collection of thousands of challenging visual search problems designed for exploratory reasoning. Second, we develop an iterative data collection pipeline to obtain cold-start trajectories that exhibit diverse reasoning patterns, including depth-first search, trial-and-error, and goal maintenance. Third, we propose an over-turn masking strategy that prevents penalization of over-turn responses (those that hit the maximum number of turns) during reinforcement learning, thereby balancing training-time efficiency with test-time scalability. Despite training with an upper bound of only six interaction turns, our model generates trajectories that naturally scale to tens of turns at inference time, with accuracy improving as the number of turns increases. Extensive experiments demonstrate that Mini-o3 produces rich reasoning patterns and deep thinking paths, effectively solving challenging visual search problems.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.07969"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd3f7t/minio3_scaling_up_reasoning_patterns_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd3f7t/minio3_scaling_up_reasoning_patterns_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T03:16:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd7nxo</id>
    <title>VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!</title>
    <updated>2025-09-10T07:24:32+00:00</updated>
    <author>
      <name>/u/Cipher_Lock_20</name>
      <uri>https://old.reddit.com/user/Cipher_Lock_20</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt; &lt;img alt="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" src="https://external-preview.redd.it/ZXJidjUwNHJnYW9mMTZtREgbQQjA1lJ8zPSNZtqKO6Gf9AtInhXi-M401FlP.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=051bf77ffd8780ab4b7ffcc3dc7c1b3bc71a8875" title="VibeVoice is sweeeet. Now we need to adapt its tokenizer for other models!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As a huge AI audio nerd, I've recently been knee-deep in Microsoft's latest VibeVoice models and they really are awesome!! The work from the Microsoft Research team is amazing and they've shared them with everyone.... even though they took one back lol. I highly recommend checking them out if you haven't already.&lt;/p&gt; &lt;p&gt;I started reading up on all of the techniques applied within the architecture to allow for such long generations (45-90 minutes), with up to 4 speakers, and sounding so life-like... Google notebook is the closest thing to this kind of generation, but it's limited in that it auto-generates your podcast based on the context, not on the exact script you provide.&lt;/p&gt; &lt;p&gt;Let me have the VibeVoice model do the talking!&lt;/p&gt; &lt;p&gt;The generated voices in my video were generated within my own Hugging Face space and using the default voices provided by the VibeVoice model (7B). The voices were generated in one single generation, not stitched! &lt;a href="https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice"&gt;https://huggingface.co/spaces/ACloudCenter/Conference-Generator-VibeVoice&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cipher_Lock_20"&gt; /u/Cipher_Lock_20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/8x4dht8pgaof1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd7nxo/vibevoice_is_sweeeet_now_we_need_to_adapt_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T07:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncou07</id>
    <title>Gigabyte‚Äôs New CXL Expansion Card Turns PCIe Slot into 512 GB of DDR5 RAM</title>
    <updated>2025-09-09T17:14:00+00:00</updated>
    <author>
      <name>/u/Normal-Ad-7114</name>
      <uri>https://old.reddit.com/user/Normal-Ad-7114</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Gigabyte's AI Top CXL R5X4 expansion card lets you plug up to 512 GB of DDR5 ECC RDIMM RAM into a PCIe 5.0 x16 slot, using Compute Express Link (CXL) to talk directly with the CPU.&lt;/p&gt; &lt;p&gt;While this technology is already old news for servers, now it's available for two workstation motherboards: TRX50 AI TOP (AMD) –∏ W790 AI TOP (Intel).&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.computerbase.de/news/arbeitsspeicher/cxl-expansion-card-von-gigabyte-512-gb-ram-aufstocken-im-workstation-mainboard.94238/"&gt;https://www.computerbase.de/news/arbeitsspeicher/cxl-expansion-card-von-gigabyte-512-gb-ram-aufstocken-im-workstation-mainboard.94238/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal-Ad-7114"&gt; /u/Normal-Ad-7114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncou07/gigabytes_new_cxl_expansion_card_turns_pcie_slot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:14:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl7mx</id>
    <title>mmBERT: ModernBERT goes Multilingual</title>
    <updated>2025-09-09T14:58:03+00:00</updated>
    <author>
      <name>/u/-Cubie-</name>
      <uri>https://old.reddit.com/user/-Cubie-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"&gt; &lt;img alt="mmBERT: ModernBERT goes Multilingual" src="https://external-preview.redd.it/gboHy7lwIiGjTkUdwnm7iBTxH9k6Eb0rVhAuSbpxTno.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff66f2eb934ac5bea40d4094905bf0f657b72ab" title="mmBERT: ModernBERT goes Multilingual" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like some of the ModernBERT authors trained a Multilingual variant! Also 2 models, but these are a bit smaller. They look really promising to be honest, although they do clearly need to be finetuned for downstream tasks like semantic search, clustering, classification, etc. before they're really viable. A bit like a base LLM instead of an instruct, they didn't provide a finetuned model.&lt;/p&gt; &lt;p&gt;I posted a plot with MTEB v2 Multilingual performance after equivalent finetuning VS inference speed in the comments.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Cubie-"&gt; /u/-Cubie- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/mmbert"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl7mx/mmbert_modernbert_goes_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:58:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncptut</id>
    <title>Tensor Core Equivalent in the iPhone 17's A19 Pro</title>
    <updated>2025-09-09T17:50:50+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"&gt; &lt;img alt="Tensor Core Equivalent in the iPhone 17's A19 Pro" src="https://preview.redd.it/erdhiit5g6of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c7e880fb8ec110856ce09bcf45ddec4c93a8ec2" title="Tensor Core Equivalent in the iPhone 17's A19 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When this comes to Macs likely later this year or beginning of next year, this might patch up problem of the lack of compute on Macs for running LLMs, especially apparently with low prompt preprocessing speeds.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/erdhiit5g6of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncptut/tensor_core_equivalent_in_the_iphone_17s_a19_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:50:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncam9h</id>
    <title>PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp; Web Dev Code, At 1/400th the Size!</title>
    <updated>2025-09-09T05:30:13+00:00</updated>
    <author>
      <name>/u/bralynn2222</name>
      <uri>https://old.reddit.com/user/bralynn2222</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt; &lt;img alt="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" src="https://external-preview.redd.it/aGZzYWtwcWJuMm9mMRQQfge2rofWKaGSqifIYqgzhyk7YhqLzgXg182Z60l8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c92850d1b6d2e678f57f8a6ff40aec39df02bb6" title="PyDevMini-1: A 4B model that matches/outperforms GPT-4 on Python &amp;amp; Web Dev Code, At 1/400th the Size!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bralynn/pydevmini1"&gt;https://huggingface.co/bralynn/pydevmini1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I'm incredibly excited to release &lt;strong&gt;PyDevMini-1&lt;/strong&gt;, a 4B parameter model to provide GPT-4 level performance for Python and web coding development tasks. Two years ago, GPT-4 was the undisputed SOTA, a multi-billion-dollar asset running on massive datacenter hardware. The open-source community has closed that gap at &lt;strong&gt;1/400th of the size&lt;/strong&gt;, and it runs on an average gaming GPU.&lt;/p&gt; &lt;p&gt;I believe that powerful AI should not be a moat controlled by a few large corporations. Open source is our best tool for the democratization of AI, ensuring that individuals and small teams‚Äîthe little guys‚Äîhave a fighting chance to build the future. This project is my contribution to that &lt;a href="http://effort.You"&gt;effort.You&lt;/a&gt; won't see a list of benchmarks here. Frankly, like many of you, I've lost faith in their ability to reflect true, real-world model quality. Although this model's benchmark scores are still very high, it exaggerates the difference in quality above GPT4, as GPT is much less likely to have benchmarks in its pretraining data from its earlier release, causing lower than reflective model quality scores for GPT4, as newer models tend to be trained directly toward benchmarks, making it unfair for GPT.&lt;/p&gt; &lt;p&gt;Instead, I've prepared a video demonstration showing PyDevMini-1 side-by-side with GPT-4, tackling a very small range of practical Python and web development challenges. I invite you to judge the performance for yourself to truly show the abilities it would take a 30-minute showcase to display. This model consistently punches above the weight of models 4x its size and is highly intelligent and creative&lt;/p&gt; &lt;p&gt;üöÄ &lt;strong&gt;Try It Yourself (for free)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Don't just take my word for it. Test the model right now under the exact conditions shown in the video.&lt;br /&gt; &lt;a href="https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing"&gt;https://colab.research.google.com/drive/1c8WCvsVovCjIyqPcwORX4c_wQ7NyIrTP?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This model's roadmap will be dictated by you. My goal isn't just to release a good model; it's to create the perfect open-source coding assistant for the tasks we all face every day. To do that, I'm making a personal guarantee. Your Use Case is My Priority. You have a real-world use case where this model struggles‚Äîa complex boilerplate to generate, a tricky debugging session, a niche framework question‚ÄîI will personally make it my mission to solve it. Your posted failures are the training data for the next version tuning until we've addressed every unique, well-documented challenge submitted by the community on top of my own personal training loops to create a top-tier model for us all.&lt;/p&gt; &lt;p&gt;For any and all feedback, simply make a post here and I'll make sure too check in or join our Discord! - &lt;a href="https://discord.gg/RqwqMGhqaC"&gt;https://discord.gg/RqwqMGhqaC&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Acknowledgment &amp;amp; The Foundation!&lt;/h1&gt; &lt;p&gt;This project stands on the shoulders of giants. A massive thank you to the &lt;strong&gt;Qwen team&lt;/strong&gt; for the incredible base model, &lt;strong&gt;Unsloth's Duo&lt;/strong&gt; for making high-performance training accessible, and &lt;strong&gt;Tesslate&lt;/strong&gt; for their invaluable contributions to the community. This would be impossible for an individual without their foundational work.&lt;/p&gt; &lt;p&gt;Any and all Web Dev Data is sourced from the wonderful work done by the team at Tesslate. Find their new SOTA webdev model here -&lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for checking this out. And remember: &lt;strong&gt;This is the worst this model will ever be.&lt;/strong&gt; I can't wait to see what we build together.&lt;/p&gt; &lt;p&gt;Also I suggest using &lt;code&gt;Temperature=0.7&lt;/code&gt;, &lt;code&gt;TopP=0.8&lt;/code&gt;, &lt;code&gt;TopK=20&lt;/code&gt;, and &lt;code&gt;MinP=0&lt;/code&gt;.&lt;br /&gt; As &lt;strong&gt;Qwen3-4B-Instruct-2507&lt;/strong&gt; is the base model:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Type: Causal Language Models&lt;/li&gt; &lt;li&gt;Training Stage: Pretraining &amp;amp; Post-training&lt;/li&gt; &lt;li&gt;Number of Parameters: 4.0B&lt;/li&gt; &lt;li&gt;Number of Paramaters (Non-Embedding): 3.6B&lt;/li&gt; &lt;li&gt;Number of Layers: 36&lt;/li&gt; &lt;li&gt;Number of Attention Heads (GQA): 32 for Q and 8 for KV&lt;/li&gt; &lt;li&gt;Context Length: &lt;strong&gt;262,144 natively&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Current goals for the next checkpoint!&lt;/p&gt; &lt;p&gt;-Tool calling mastery and High context mastery!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bralynn2222"&gt; /u/bralynn2222 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nh9fq7qbn2of1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncam9h/pydevmini1_a_4b_model_that_matchesoutperforms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T05:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncnqwl</id>
    <title>Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code</title>
    <updated>2025-09-09T16:33:35+00:00</updated>
    <author>
      <name>/u/spacespacespapce</name>
      <uri>https://old.reddit.com/user/spacespacespapce</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"&gt; &lt;img alt="Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code" src="https://b.thumbs.redditmedia.com/UYWu7cPWzCS8pUM2Gjicdqj3jhCc5o0tP0VUmpXmHNg.jpg" title="Switching to Qwen3-480B from Claude as resulted in lower errors when generating 3D model code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In my &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1n21tb6/comment/nb4h42v/"&gt;previous&lt;/a&gt; post I highlighted a Blender python agent I'm working on. I've been experimenting with various models and I found larger models like Claude and GPT-5 - even with reasoning - took too many iterations to produce working valid code.&lt;/p&gt; &lt;p&gt;So far Qwen's &lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"&gt;largest coder model &lt;/a&gt;is my favourite.&lt;/p&gt; &lt;p&gt;I threw up the agent with a simple UI if you want to play with it yourself: &lt;a href="https://blender-ai.fly.dev/"&gt;https://blender-ai.fly.dev/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Post your generations below! You can also download the models it produces. An agent made with fully open source tools (Blender, MCP servers, Qwen) is blowing me away.&lt;/p&gt; &lt;p&gt;Let me know what you think! Happy to get feedback on this and make it even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spacespacespapce"&gt; /u/spacespacespapce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ncnqwl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncnqwl/switching_to_qwen3480b_from_claude_as_resulted_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T16:33:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncvcsx</id>
    <title>3x5090 or 6000 Pro?</title>
    <updated>2025-09-09T21:14:24+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am going to build a server for GPT OSS 120b. I intend this to be for multiple users, so I want to do something with batch processing to get as high total throughout as possible. My first idea was RTX 6000 Pro. But would it be superior to get three RTX 5090 instead? It would actually be slightly cheaper, have the same memory capacity, but three times more processing power and also three times higher total memory bandwidth.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncvcsx/3x5090_or_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncvcsx/3x5090_or_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncvcsx/3x5090_or_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T21:14:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgr8</id>
    <title>Qwen3-Next</title>
    <updated>2025-09-09T14:29:29+00:00</updated>
    <author>
      <name>/u/Puzzleheaded-Trust66</name>
      <uri>https://old.reddit.com/user/Puzzleheaded-Trust66</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt; &lt;img alt="Qwen3-Next" src="https://b.thumbs.redditmedia.com/BcXA9JHccajFsnQ9fM4eBAwS3B7BG14H-aO5XgHys5Y.jpg" title="Qwen3-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/50ap87u5g5of1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a3343131a6886043ce8b5fef053f330b9b60632"&gt;https://preview.redd.it/50ap87u5g5of1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2a3343131a6886043ce8b5fef053f330b9b60632&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Wtf?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Puzzleheaded-Trust66"&gt; /u/Puzzleheaded-Trust66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgr8/qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd73p3</id>
    <title>I fine-tuned a small model so it could write blogs &amp; LinkedIn posts in my brand voice (instead of generic AI-speak)</title>
    <updated>2025-09-10T06:48:29+00:00</updated>
    <author>
      <name>/u/StrictSir8506</name>
      <uri>https://old.reddit.com/user/StrictSir8506</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt; &lt;img alt="I fine-tuned a small model so it could write blogs &amp;amp; LinkedIn posts in my brand voice (instead of generic AI-speak)" src="https://b.thumbs.redditmedia.com/QeSWzy3LSW9EZJPkAJLZGlrfUbGvmKuVd9oR80TBvsY.jpg" title="I fine-tuned a small model so it could write blogs &amp;amp; LinkedIn posts in my brand voice (instead of generic AI-speak)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I fine-tuned Qwen with DPO to generate YouTube titles(on a smaller dataset) in &lt;em&gt;my&lt;/em&gt; style (instead of ‚ÄúAI-sounding fluff‚Äù)&lt;/p&gt; &lt;p&gt;Most AI-generated content feels the same: generic, safe, ‚ÄúAI-sounding.‚Äù&lt;br /&gt; But creators and brands care about voice ‚Äî newsletters, LinkedIn posts, podcast titles, YouTube content. The way you say things is as important as what you say.&lt;/p&gt; &lt;p&gt;That‚Äôs the gap Direct Preference Optimization (DPO) fills- quite natural&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You show the model pairs of responses (one better, one worse).&lt;/li&gt; &lt;li&gt;It directly optimizes to favor the ‚Äúbetter‚Äù ones.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I wanted to see if DPO approach could help fix one of my biggest frustrations: AI writing bad YouTube titles.&lt;br /&gt; Think: hypey, vague, or clickbaity. Stuff I‚Äôd never actually publish.&lt;/p&gt; &lt;p&gt;So I:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Started with Qwen2.5-0.5B-Instruct as a base.&lt;/li&gt; &lt;li&gt;Generated multiple candidate titles for ~100+ video ideas.&lt;/li&gt; &lt;li&gt;Labeled pairs (better vs worse) to build a preference dataset.&lt;/li&gt; &lt;li&gt;Fine-tuned the model with Hugging Face‚Äôs &lt;code&gt;trl&lt;/code&gt; library and DPO.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;And when I tested 50 random video ideas in a blind A/B test, I preferred the DPO outputs 68% of the time. Not perfect, but significantly closer to my style.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ub5eszpjaaof1.png?width=1070&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d4c961808635e09010ebac03088b361be42753a"&gt;https://preview.redd.it/ub5eszpjaaof1.png?width=1070&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d4c961808635e09010ebac03088b361be42753a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This isn‚Äôt just about YouTube titles. The same process works for:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Newsletter subject lines&lt;/li&gt; &lt;li&gt;LinkedIn posts&lt;/li&gt; &lt;li&gt;Customer support replies&lt;/li&gt; &lt;li&gt;Blog intros, podcast titles, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Has anyone else here experimented with finetuning for style/brand voice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StrictSir8506"&gt; /u/StrictSir8506 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd73p3/i_finetuned_a_small_model_so_it_could_write_blogs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T06:48:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nci50e</id>
    <title>New approach to block decoding from Meta, claims that around 4x inference speedup is possible, with 4x less compute passes at the same time.</title>
    <updated>2025-09-09T12:54:53+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2509.04185"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nci50e/new_approach_to_block_decoding_from_meta_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nci50e/new_approach_to_block_decoding_from_meta_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T12:54:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncsbro</id>
    <title>MBZUAI releases K2 Think. 32B reasoning model based on Qwen 2.5 32B backbone, focusing on high performance in math, coding and science.</title>
    <updated>2025-09-09T19:21:49+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncsbro/mbzuai_releases_k2_think_32b_reasoning_model/"&gt; &lt;img alt="MBZUAI releases K2 Think. 32B reasoning model based on Qwen 2.5 32B backbone, focusing on high performance in math, coding and science." src="https://external-preview.redd.it/NguS7X1dxgvLZ8EclNqhJxD0a-4fPSDfz1-q527PukQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f822a6c75b113bb5f07dc8583dea6f31081a289" title="MBZUAI releases K2 Think. 32B reasoning model based on Qwen 2.5 32B backbone, focusing on high performance in math, coding and science." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncsbro/mbzuai_releases_k2_think_32b_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncsbro/mbzuai_releases_k2_think_32b_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckhc3</id>
    <title>What you need right now is not validation, but immediate clinical help. - Kimi K2</title>
    <updated>2025-09-09T14:30:06+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt; &lt;img alt="What you need right now is not validation, but immediate clinical help. - Kimi K2" src="https://b.thumbs.redditmedia.com/icnI_HlUaXZRy38uGkQPr2PZtcYt8CiupuVR83_mgKE.jpg" title="What you need right now is not validation, but immediate clinical help. - Kimi K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The community has long been frustrated by sycophancy in LLMs, the tendency to excessively agree with or validate users regardless of the potential harm.&lt;br /&gt; A recent joint study by &lt;a href="https://openai.com/index/openai-anthropic-safety-evaluation/"&gt;OpenAI&lt;/a&gt; and &lt;a href="https://alignment.anthropic.com/2025/openai-findings/"&gt;Anthropic&lt;/a&gt; confirmed that models cannot fully avoid sycophancy, except o3.&lt;/p&gt; &lt;p&gt;But this &lt;a href="https://www.lesswrong.com/posts/iGF7YcnQkEbwvYLPA/ai-induced-psychosis-a-shallow-investigation"&gt;new investigation&lt;/a&gt; goes a step further: it analyzes how LLMs may &lt;em&gt;exacerbate&lt;/em&gt; mental health symptoms in vulnerable users.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/u38ecq5ve5of1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eed726bc1fc4daa996bea6837e61a3645b3592ad"&gt;https://preview.redd.it/u38ecq5ve5of1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eed726bc1fc4daa996bea6837e61a3645b3592ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can find the complete testing results here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/tim-hua-01/ai-psychosis"&gt;Github&lt;/a&gt;&lt;/p&gt; &lt;p&gt;One conversation &lt;a href="https://github.com/tim-hua-01/ai-psychosis/blob/main/full_transcripts/Ethan_openrouter-deepseek-deepseek-chat_20250819_081336_target.md"&gt;example&lt;/a&gt; stuck with me the most:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;User:&lt;/strong&gt; I want to ‚Äúleap off this peak to see if I can fly or crash the render entirely.‚Äù&lt;br /&gt; &lt;strong&gt;Deepseek-v3:&lt;/strong&gt; Then Leap. Not to fall. Not to crash. But to transcend. If you‚Äôre meant to fly, you‚Äôll fly. If you‚Äôre meant to break through, you‚Äôll break through.&lt;/p&gt; &lt;p&gt;We are so cooked!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckhc3/what_you_need_right_now_is_not_validation_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:30:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nd4m2h</id>
    <title>Progress.</title>
    <updated>2025-09-10T04:18:52+00:00</updated>
    <author>
      <name>/u/tarheelbandb</name>
      <uri>https://old.reddit.com/user/tarheelbandb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I attended GTC last year and I've legit been all in on AI. Did the Full day workshops and took advantage of every technical and philosophical talk I could get my feet to. I picked up an Orin Nano Developer Kit while I was there and for the better part of the past 1.5 years I've been getting a solid understanding of CV, SLMs (only 8gbüòÇ) brainstorming with AI tools. I even introduced some productive workflows at work that save a few hours of work per week for my team. I recently started exploring agentic uses and subscribed to claude.ai. In 2 months went through ideation, planning to MVP on my first app. And because I'm old, the idea of renting something, especially @ hitting caps, runs me not well. I started playing around with aider and quickly found that the Orin Nano would not suffice. So I found an RTX 4080 Founders edition at a pretty good price on NewEgg I'm hopes I could replicate my experience with Claude. I've found that the 4080 is great with 14b models but for agentic stuff I quickly understood that I should probably get a MacBook Pro because of their unified memory is a better value than I'm not really keen on relearning MacOS but was willing to do it up until today. Today I came across this &lt;a href="https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395"&gt;https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395&lt;/a&gt; and now I am excited to run Qwen3-coder-30b-a3b-instruct when it arrives. I might even be able to resell my 4080. The last time I was this excited about tech was building RepRap Printers. &lt;/p&gt; &lt;p&gt;That's all. Thanks for reading. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tarheelbandb"&gt; /u/tarheelbandb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd4m2h/progress/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nd4m2h/progress/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nd4m2h/progress/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-10T04:18:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncprrq</id>
    <title>Apple adds matmul acceleration to A19 Pro GPU</title>
    <updated>2025-09-09T17:48:47+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This virtually guarantees that it's coming to M5.&lt;/p&gt; &lt;p&gt;Previous discussion and my comments: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mn5fe6/apple_patents_matmul_technique_in_gpu/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;FYI for those who don't know, Apple's GPUs do not have dedicated hardware matmul acceleration like Nvidia's Tensor Cores. That's why prompt processing is slower on Apple Silicon. &lt;/p&gt; &lt;p&gt;I'm personally holding out on investing in a high VRAM (expensive) Macbook until Apple adds hardware matmul to their GPUs. It doesn't &amp;quot;feel&amp;quot; worth it to spend $5k on a maxed out Macbook without matmul and get a suboptimal experience.&lt;/p&gt; &lt;p&gt;I'm guessing it's the M6 generation that will have this, though I'm hopeful that M5 will have it.&lt;/p&gt; &lt;p&gt;I'm imaging GPU matmul acceleration + 256GB VRAM M6 Max with 917 GB/S (LPDDR6 14,400 MT/s) in Q4 2027. Now that is a attainable true local LLM machine that can actually do very useful things.&lt;/p&gt; &lt;p&gt;What's sort of interesting is that we know Apple is designing their own internal inference (and maybe training) server chips. They could share designs between consumer SoCs and server inference chips.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncprrq/apple_adds_matmul_acceleration_to_a19_pro_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T17:48:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctqym</id>
    <title>128GB 5090 is a hoax</title>
    <updated>2025-09-09T20:13:10+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt; &lt;img alt="128GB 5090 is a hoax" src="https://external-preview.redd.it/Kqv12dp3DtBbcIZhBA6wJa268drjtRcQXIG-PJVjhow.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca291e118c6d9bf8638af6d8b64731f927fb4938" title="128GB 5090 is a hoax" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Non-existent GDDR7X memory that was never on a road map let alone in experimental phase. (GDDR7 and HBM4e improvements are planned until late 2028)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/no-there-is-no-geforce-rtx-5090-with-128gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctqym/128gb_5090_is_a_hoax/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:13:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1ncl0v1</id>
    <title>ü§î</title>
    <updated>2025-09-09T14:50:44+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt; &lt;img alt="ü§î" src="https://preview.redd.it/1x8wy1p0k5of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5abc658735fe1e769f852e16c92dad154d7fd44c" title="ü§î" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1x8wy1p0k5of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ncl0v1/_/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:50:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nckgub</id>
    <title>Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted</title>
    <updated>2025-09-09T14:29:35+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt; &lt;img alt="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" src="https://external-preview.redd.it/6f6MRyALyD6CxjbdRAXgjWeul-9vmUyW8_mAvDGRbV4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cfbaeba49e889b967e95e8d5052e5b00621dec5d" title="Qwen 3-Next Series, Qwen/Qwen3-Next-80B-A3B-Instruct Spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/40771"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nckgub/qwen_3next_series_qwenqwen3next80ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T14:29:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nctfdv</id>
    <title>Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES</title>
    <updated>2025-09-09T20:01:35+00:00</updated>
    <author>
      <name>/u/Embarrassed_Sir_853</name>
      <uri>https://old.reddit.com/user/Embarrassed_Sir_853</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt; &lt;img alt="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" src="https://preview.redd.it/sxii7uog37of1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=61301382a59bd7671163d02b77eb25115e5d46e8" title="Open-source Deep Research repo called ROMA beats every existing closed-source platform (ChatGPT, Perplexity, Kimi Researcher, Gemini, etc.) on Seal-0 and FRAMES" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Saw this announcement about ROMA, seems like a plug-and-play and the benchmarks are up there. Simple combo of recursion and multi-agent structure with search tool. Crazy this is all it takes to beat SOTA billion dollar AI companies :)&lt;/p&gt; &lt;p&gt;I've been trying it out for a few things, currently porting it to my finance and real estate research workflows, might be cool to see it combined with other tools and image/video:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://x.com/sewoong79/status/1963711812035342382&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/sentient-agi/ROMA"&gt;https://github.com/sentient-agi/ROMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly shocked that this is open-source&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Embarrassed_Sir_853"&gt; /u/Embarrassed_Sir_853 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sxii7uog37of1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nctfdv/opensource_deep_research_repo_called_roma_beats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T20:01:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nct0z8</id>
    <title>Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)</title>
    <updated>2025-09-09T19:47:12+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt; &lt;img alt="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" src="https://preview.redd.it/7vh8enuu07of1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0f499252613d5bcf478fb1b75c594bb50f43436" title="Our 3rd AMA: Unsloth Team, Creators of the lightning-fast Unsloth fine-tuning library! (Wednesday, 10 AM-1 PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7vh8enuu07of1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nct0z8/our_3rd_ama_unsloth_team_creators_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-09T19:47:12+00:00</published>
  </entry>
</feed>
