<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-21T20:49:16+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ocfnfv</id>
    <title>NVIDIA GPU + Apple Mac via USB4?</title>
    <updated>2025-10-21T15:14:11+00:00</updated>
    <author>
      <name>/u/nuance415</name>
      <uri>https://old.reddit.com/user/nuance415</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/tiny-corp-successfully-runs-an-nvidia-gpu-on-arm-macbook-through-usb4-using-an-external-gpu-docking-station"&gt;https://www.tomshardware.com/pc-components/gpus/tiny-corp-successfully-runs-an-nvidia-gpu-on-arm-macbook-through-usb4-using-an-external-gpu-docking-station&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nuance415"&gt; /u/nuance415 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocfnfv/nvidia_gpu_apple_mac_via_usb4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocfnfv/nvidia_gpu_apple_mac_via_usb4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocfnfv/nvidia_gpu_apple_mac_via_usb4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T15:14:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oclysw</id>
    <title>Tool calling frustrations with Qwen3-30B-A3B-Instruct-GGUF</title>
    <updated>2025-10-21T19:10:08+00:00</updated>
    <author>
      <name>/u/milkipedia</name>
      <uri>https://old.reddit.com/user/milkipedia</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Roo Code and the unsloth GGUF for Qwen3-30B-A3B-Instruct, Q4_K_XL quant. It often struggles with tool calls in Roo, throwing errors when (for example) it needs to write a file but forgets to provide the file name to the tool. This seems to be &lt;a href="https://github.com/RooCodeInc/Roo-Code/issues/7406"&gt;a known problem for Qwen3&lt;/a&gt; in the Roo community and not likely to be fixed there.&lt;/p&gt; &lt;p&gt;I often hear this model extolled for its code writing capability, and I find it to be fine at that, but the tool calling failures are frequent enough to be a non-starter for me. I've taken to running against OpenRouter-hosted GLM Air but I'd rather not do that for everything all the time.&lt;/p&gt; &lt;p&gt;Are there other locally-runnable models that might work better for this? I have 24GB VRAM and 128GB SRAM, and I'm happy with offloading tensor layers for accommodating larger models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/milkipedia"&gt; /u/milkipedia &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclysw/tool_calling_frustrations_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclysw/tool_calling_frustrations_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oclysw/tool_calling_frustrations_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T19:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocgao6</id>
    <title>Noob starting advice please: I'm building a community-based RP model for a video-game character</title>
    <updated>2025-10-21T15:38:55+00:00</updated>
    <author>
      <name>/u/Pangolin_Beatdown</name>
      <uri>https://old.reddit.com/user/Pangolin_Beatdown</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think this project is pretty simple. I want to build a chatbot that speaks and behaves like a specific character (Alistair) from a specific game (Dragon Age Origins). I think the community can generate several thousand high-quality training examples to capture his specific personality, but i understand fine tuning an RP chatbot takes 50k-100k examples. &lt;/p&gt; &lt;p&gt;The model will be entirely locally-hosted, no API calls to the web, no cutting edge LLMs.&lt;/p&gt; &lt;p&gt;I want to fine-tune this model on my 3090, which runs Qwen2.5:32B very well (for example). I want the fully trained model to be able to run on gaming laptops with 8GB vram, so 7B or smaller would be best for the final deployed model (or have a small version, and then another version for people with more VRAM).&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I assume I can come up with 2000 very high quality training examples hand-written by community members from the game dialog.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can I find a general-purpose (personality-agnostic) training set for the initial fine--tune, then do a second round of fine-tuning, weighted, with our personality examples? Can anyone suggest some appropriate sets and where to find them? Most RP chatbots seem to be women and flirty in a way that doesn't suit our character.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What are the best pre-tuned models for an RP chatbot?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Has anyone done a similar project that you can point me to? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I plan to provide Knowledge base files that describe the environments in the game (Denerim city etc for you DAO nerds) so our NPC behaves appropriately in-context. Different system prompts will allow the user to start their chat at specific points in the game with a known world state, and play forward from there with original model-generated conversations and choices.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It would be cool to add conversation summary save to give continuity between sessions. Maybe update specific game plot parameters.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;It would be cool to build in some radiant-quest givers that generate plot-appropriate quests.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;I know and ennvision running this in openwebui but I know other UIs maybe better suited to this task, can you recommend?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pangolin_Beatdown"&gt; /u/Pangolin_Beatdown &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgao6/noob_starting_advice_please_im_building_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgao6/noob_starting_advice_please_im_building_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgao6/noob_starting_advice_please_im_building_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T15:38:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocj4w8</id>
    <title>Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard</title>
    <updated>2025-10-21T17:26:05+00:00</updated>
    <author>
      <name>/u/PDXcoder2000</name>
      <uri>https://old.reddit.com/user/PDXcoder2000</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"&gt; &lt;img alt="Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard" src="https://external-preview.redd.it/dMdQFElElXfyOj3dCITHMNWHT928JuIqOE8NxO-zqNU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ff4a54d8ef7e031aa7399b4c4e5d6a20fd14935" title="Llama-Embed-Nemotron-8B Takes the Top Spot on MMTEB Multilingual Retrieval Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For developers working on multilingual search or similarity tasks, Llama‑Embed‑Nemotron‑8B might be worth checking out. It’s designed to generate 4,096‑dimensional embeddings that work well across languages — especially useful for retrieval, re‑ranking, classification, and bi‑text mining projects.&lt;/p&gt; &lt;p&gt;What makes it stand out is how effectively it handles cross‑lingual and low‑resource queries, areas where many models still struggle. It was trained on a mix of 16 million query‑document pairs (half public and half synthetic), combining model merging and careful hard‑negative mining to boost accuracy.&lt;/p&gt; &lt;p&gt;Key details:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Strong performance for retrieval, re‑ranking, classification, and bi‑text mining&lt;/li&gt; &lt;li&gt;Handles low‑resource and cross‑lingual queries effectively&lt;/li&gt; &lt;li&gt;Trained on 16M query‑document pairs (8M public + 8M synthetic)&lt;/li&gt; &lt;li&gt;Combines model merging and refined hard‑negative mining for better accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model is built on meta-llama/Llama‑3.1‑8B and uses the &lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-CC-v2."&gt;Nemotron‑CC‑v2 dataset&lt;/a&gt; and it’s now ranked first on the &lt;a href="https://huggingface.co/spaces/mteb/leaderboard"&gt;MMTEB multilingual retrieval leaderboard&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;📖 Read our &lt;a href="https://huggingface.co/blog/nvidia/llama-embed-nemotron-8b"&gt;blog &lt;/a&gt;on Hugging Face to learn more about the model, architectural highlights, training methodology, performance evaluation and more.&lt;/p&gt; &lt;p&gt;💡If you’ve got suggestions or ideas, we are inviting feedback at &lt;a href="http://nemotron.ideas.nvidia.com"&gt;http://nemotron.ideas.nvidia.com&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.redd.it/oqhem2nz1iwf1.gif"&gt;https://i.redd.it/oqhem2nz1iwf1.gif&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PDXcoder2000"&gt; /u/PDXcoder2000 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocj4w8/llamaembednemotron8b_takes_the_top_spot_on_mmteb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:26:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ockagv</id>
    <title>FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems</title>
    <updated>2025-10-21T18:08:32+00:00</updated>
    <author>
      <name>/u/YiyanZ</name>
      <uri>https://old.reddit.com/user/YiyanZ</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"&gt; &lt;img alt="FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems" src="https://b.thumbs.redditmedia.com/FOo5hRCPO5Mpj116Q3BLbFjKUWelZzgfLkDrFbe9bGg.jpg" title="FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;🤔 Can AI optimize the systems it runs on?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;🚀 &lt;strong&gt;Introducing FlashInfer-Bench&lt;/strong&gt; — a workflow that makes AI systems &lt;em&gt;self-improving&lt;/em&gt; through agents.&lt;/p&gt; &lt;p&gt;It’s designed to push the boundaries of LLM serving efficiency:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Standardized signature for LLM serving kernels&lt;/li&gt; &lt;li&gt;Implement kernels in any language you like&lt;/li&gt; &lt;li&gt;Benchmark them against real-world serving workloads&lt;/li&gt; &lt;li&gt;Fastest kernels get &lt;strong&gt;day-0 integrated&lt;/strong&gt; into production&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;FlashInfer-Bench launches with first-class integration into &lt;strong&gt;FlashInfer&lt;/strong&gt;, &lt;strong&gt;SGLang&lt;/strong&gt;, and &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qc6kumc58iwf1.png?width=2178&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5e2f1a9bb2e0b338577bdbda3925c965a9876dda"&gt;Systematically Approaching AI for AI systems with FlashInfer-Bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🔗 &lt;strong&gt;Blog post:&lt;/strong&gt; &lt;a href="https://flashinfer.ai/2025/10/21/flashinfer-bench.html"&gt;flashinfer.ai/2025/10/21/flashinfer-bench.html&lt;/a&gt;&lt;br /&gt; 📊 &lt;strong&gt;Leaderboard:&lt;/strong&gt; &lt;a href="https://bench.flashinfer.ai/"&gt;bench.flashinfer.ai&lt;/a&gt;&lt;br /&gt; 💻 &lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/flashinfer-ai/flashinfer-bench"&gt;github.com/flashinfer-ai/flashinfer-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YiyanZ"&gt; /u/YiyanZ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ockagv/flashinferbench_building_the_virtuous_cycle_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T18:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc3f0i</id>
    <title>Qwen3 Omni interactive speech</title>
    <updated>2025-10-21T04:21:40+00:00</updated>
    <author>
      <name>/u/Powerful-Angel-301</name>
      <uri>https://old.reddit.com/user/Powerful-Angel-301</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 Omni is very interesting. They claim it supports real-time voice, but I couldn't find out how and there was no tutorial for this on their github. &lt;/p&gt; &lt;p&gt;Anyone having any experience with that? Basically continuously talk to the model and get voice responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Powerful-Angel-301"&gt; /u/Powerful-Angel-301 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc3f0i/qwen3_omni_interactive_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T04:21:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1occcel</id>
    <title>SmolVLM AWQ Text Quantization (4 GB → 2GB with minimal quality loss on DocVQA)</title>
    <updated>2025-10-21T13:02:06+00:00</updated>
    <author>
      <name>/u/Ok_Employee_6418</name>
      <uri>https://old.reddit.com/user/Ok_Employee_6418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"&gt; &lt;img alt="SmolVLM AWQ Text Quantization (4 GB → 2GB with minimal quality loss on DocVQA)" src="https://external-preview.redd.it/Kjegehdr73l6a0EStswJLB7yLrGnAt87gT0UjQYkxvk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9752bb52ed840b99326e2e047a618fbe01dff3c" title="SmolVLM AWQ Text Quantization (4 GB → 2GB with minimal quality loss on DocVQA)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing AWQ and GPTQ quantized versions of SmolVLM from Hugging Face. &lt;/p&gt; &lt;p&gt;These models only had their text models quantized, and had a 50% model size reduction (4GB~2GB) while keeping model degradation under 1% on the DocVQA benchmark. &lt;/p&gt; &lt;p&gt;#huggingface #smolvlm #smollm&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Employee_6418"&gt; /u/Ok_Employee_6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ronantakizawa/SmolVLM-Instruct-awq"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occcel/smolvlm_awq_text_quantization_4_gb_2gb_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:02:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1obn0q7</id>
    <title>The Innovations in DeepSeek OCR</title>
    <updated>2025-10-20T16:29:30+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DeepSeek just released a pretty shocking new paper. They really buried the lede here by referring to it simply as DeepSeek OCR. &lt;/p&gt; &lt;p&gt;While it’s a very strong OCR model, the purpose of it and the implications of their approach go far beyond what you’d expect of “yet another OCR model.”&lt;/p&gt; &lt;p&gt;Traditionally, vision LLM tokens almost seemed like an afterthought or “bolt on” to the LLM paradigm. And 10k words of English would take up far more space in a multimodal LLM when expressed as intelligible pixels than when expressed as tokens.&lt;/p&gt; &lt;p&gt;So those 10k words may have turned into 15k tokens, or 30k to 60k “visual tokens.” So vision tokens were way less efficient and really only made sense to use for data that couldn’t be effectively conveyed with words. &lt;/p&gt; &lt;p&gt;But that gets inverted now from the ideas in this paper. DeepSeek figured out how to get 10x better compression using vision tokens than with text tokens! So you could theoretically store those 10k words in just 1,500 of their special compressed visual tokens.&lt;/p&gt; &lt;p&gt;This might not be as unexpected as it sounds if you think of how your own mind works. After all, I know that when I’m looking for a part of a book that I’ve already read, I imagine it visually and always remember which side of the book it was on and approximately where on the page it was, which suggests some kind of visual memory representation at work.&lt;/p&gt; &lt;p&gt;Now, it’s not clear how exactly this interacts with the other downstream cognitive functioning of an LLM; can the model reason as intelligently over those compressed visual tokens as it can using regular text tokens? Does it make the model less articulate by forcing it into a more vision-oriented modality? &lt;/p&gt; &lt;p&gt;But you can imagine that, depending on the exact tradeoffs, it could be a very exciting new axis to greatly expand effective context sizes. Especially when combined with DeepSeek’s other recent paper from a couple weeks ago about sparse attention.&lt;/p&gt; &lt;p&gt;For all we know, Google could have already figured out something like this, which could explain why Gemini has such a huge context size and is so good and fast at OCR tasks. If they did, they probably wouldn’t say because it would be viewed as an important trade secret.&lt;/p&gt; &lt;p&gt;But the nice thing about DeepSeek is that they’ve made the entire thing open source and open weights and explained how they did it, so now everyone can try it out and explore.&lt;/p&gt; &lt;p&gt;Even if these tricks make attention more lossy, the potential of getting a frontier LLM with a 10 or 20 million token context window is pretty exciting. &lt;/p&gt; &lt;p&gt;You could basically cram all of a company’s key internal documents into a prompt preamble and cache this with OpenAI and then just add your specific query or prompt on top of that and not have to deal with search tools and still have it be fast and cost-effective. &lt;/p&gt; &lt;p&gt;Or put an entire code base into the context and cache it, and then just keep appending the equivalent of the git diffs as you make changes to the code. &lt;/p&gt; &lt;p&gt;If you’ve ever read stories about the great physicist Hans Bethe, he was known for having vast amounts of random physical facts memorized (like the entire periodic table; boiling points of various substances, etc.) so that he could seamlessly think and compute without ever having to interrupt his flow to look something up in a reference table. &lt;/p&gt; &lt;p&gt;Having vast amounts of task-specific knowledge in your working memory is extremely useful. This seems like a very clever and additive approach to potentially expanding that memory bank by 10x or more.&lt;/p&gt; &lt;p&gt;source: &lt;a href="https://x.com/doodlestein/status/1980282222893535376"&gt;https://x.com/doodlestein/status/1980282222893535376&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obn0q7/the_innovations_in_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T16:29:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc8dqx</id>
    <title>Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend.</title>
    <updated>2025-10-21T09:31:29+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"&gt; &lt;img alt="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend." src="https://external-preview.redd.it/Z24wYzZvdXdvZndmMTaXKbAxEnkCSlwbwmJDXf_lyDQzd483n4JJoFhjK3xD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae56ff1f5b7f8b918196f743f18a36cbbf044718" title="Vascura FRONT - Open Source (Apache 2.0), Bloat Free, Portable and Lightweight (288 kb) LLM Frontend." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4oaz6nuwofwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8dqx/vascura_front_open_source_apache_20_bloat_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T09:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ock0lc</id>
    <title>Qwen3-VL kinda sucks in LM Studio</title>
    <updated>2025-10-21T17:58:45+00:00</updated>
    <author>
      <name>/u/waescher</name>
      <uri>https://old.reddit.com/user/waescher</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/"&gt; &lt;img alt="Qwen3-VL kinda sucks in LM Studio" src="https://b.thumbs.redditmedia.com/FzqRtu_bjl1T0o6OQ-s1rtah6kqxUffbI4j5XO-W5vI.jpg" title="Qwen3-VL kinda sucks in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone else finding qwen3 VL absolutely terrible in LM Studio? I am using the 6bix MLX variant and even the VL 30b-a3b is really bad. Online demos like &lt;a href="https://huggingface.co/spaces/Qwen/Qwen3-VL-30B-A3B-Demo"&gt;this&lt;/a&gt; here work perfectly well.&lt;/p&gt; &lt;p&gt;Using the staff pick 30b model at up to 120k context.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waescher"&gt; /u/waescher &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ock0lc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ock0lc/qwen3vl_kinda_sucks_in_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:58:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocnohl</id>
    <title>Qwen3-VL-2B , it works very well ocr</title>
    <updated>2025-10-21T20:13:41+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocnohl/qwen3vl2b_it_works_very_well_ocr/"&gt; &lt;img alt="Qwen3-VL-2B , it works very well ocr" src="https://b.thumbs.redditmedia.com/NvNLG1hzs6RPYC_difrPtdpvyMrKLDvDAdqUHQLLbnc.jpg" title="Qwen3-VL-2B , it works very well ocr" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;our friend Maziyar did a test with good results and also left us a Google colab so that we can run it&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/MaziyarPanahi/status/1980692255414628637?t=VXwW705ixLW-rsai_37M_A&amp;amp;s=19"&gt;https://x.com/MaziyarPanahi/status/1980692255414628637?t=VXwW705ixLW-rsai_37M_A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ocnohl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocnohl/qwen3vl2b_it_works_very_well_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocnohl/qwen3vl2b_it_works_very_well_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T20:13:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc8vb4</id>
    <title>Do you have any ideas for OCR on pages of documents with very very low contrast?</title>
    <updated>2025-10-21T10:01:29+00:00</updated>
    <author>
      <name>/u/suelzsuelz</name>
      <uri>https://old.reddit.com/user/suelzsuelz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"&gt; &lt;img alt="Do you have any ideas for OCR on pages of documents with very very low contrast?" src="https://preview.redd.it/yhbgv2pztfwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ac61a3033e6e868cec48291c7feaa1ca30e5a51" title="Do you have any ideas for OCR on pages of documents with very very low contrast?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My use case is to locally extract pdf content into Markdown or JSON-structured data. The problem, as demonstrated by the example, is that the contrast between the text and background is very poor.&lt;/p&gt; &lt;p&gt;Has anyone ever processed similar documents?&lt;br /&gt; Which local models with how many parameters can do this reliably? &lt;/p&gt; &lt;p&gt;Newer cloud models don't seem to have any problems. We have already tested these:&lt;/p&gt; &lt;p&gt;- granite3.2-vision&lt;br /&gt; - minicpm-v2.6:8b&lt;br /&gt; - llama3.2-vision:11b&lt;br /&gt; - DeepSeek-OCR&lt;/p&gt; &lt;p&gt;Maybe they are just too small?&lt;/p&gt; &lt;p&gt;We are able to use a 4 x RTX 3090 Workstation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/suelzsuelz"&gt; /u/suelzsuelz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/yhbgv2pztfwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc8vb4/do_you_have_any_ideas_for_ocr_on_pages_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T10:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocjhug</id>
    <title>I built an offline-first voice AI with &lt;1 s latency on my Mac M3</title>
    <updated>2025-10-21T17:39:28+00:00</updated>
    <author>
      <name>/u/mshubham</name>
      <uri>https://old.reddit.com/user/mshubham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"&gt; &lt;img alt="I built an offline-first voice AI with &amp;lt;1 s latency on my Mac M3" src="https://external-preview.redd.it/xB_CA3iDlXtwzT5bC0DnSUQZ7myr5MTWUPuiuHH7JC8.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=41d158b450f13d467631338511219a4c0be4dd41" title="I built an offline-first voice AI with &amp;lt;1 s latency on my Mac M3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... I built an offline-first voice AI from scratch — no LiveKit, Pipecat, or any framework.&lt;/p&gt; &lt;p&gt;A perfectly blended pipeline of VAD + Turn Detection + STT + LLM + TTS.&lt;/p&gt; &lt;p&gt;Runs locally on my M3 Pro, replies in &amp;lt; 1 s, and stays under 1 K lines of code — with a minimal UI.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xmmn9fsg8iwf1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606ce0e136a49952c0878b4313e1175609657bbf"&gt;https://preview.redd.it/xmmn9fsg8iwf1.png?width=3024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=606ce0e136a49952c0878b4313e1175609657bbf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/6IEK2fXB_ok"&gt;Youtube Demo&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/shubhdotai/offline-voice-ai"&gt;Gtihub Repo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshubham"&gt; /u/mshubham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocjhug/i_built_an_offlinefirst_voice_ai_with_1_s_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T17:39:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oc7uio</id>
    <title>DeepSeek-OCR Playground — Dockerized FastAPI + React workbench (5090-ready), image → text/description, more to come</title>
    <updated>2025-10-21T08:56:32+00:00</updated>
    <author>
      <name>/u/Putrid_Passion_6916</name>
      <uri>https://old.reddit.com/user/Putrid_Passion_6916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Repo: &lt;a href="https://github.com/rdumasia303/deepseek_ocr_app"&gt;https://github.com/rdumasia303/deepseek_ocr_app&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR: A tiny web app to mess with the new DeepSeek-OCR locally. Upload an image, pick a mode (Plain OCR, Describe, Find/grounding, Freeform), and get results instantly. &lt;/p&gt; &lt;p&gt;It runs in Docker with GPU (tested on 5090/Blackwell), has a slick UI, and is “good enough” to ship &amp;amp; let the community break/fix/improve it. PRs welcome.&lt;/p&gt; &lt;p&gt;What’s inside&lt;/p&gt; &lt;p&gt;Frontend: React/Vite + glassy Tailwind UI (drag-drop, live preview, copy/download). Backend: FastAPI + Transformers, calls DeepSeek-OCR with eval_mode=True. GPU: Blackwell-friendly (bfloat16), designed to run on RTX 5090 (or any CUDA GPU).&lt;/p&gt; &lt;p&gt;Modes shipped now: Plain OCR (super strong) Describe (short freeform caption) Find (grounding) — returns boxes for a term (e.g., “Total Due”, “Signature”) Freeform — your own instruction&lt;/p&gt; &lt;p&gt;There’s groundwork laid for more modes (Markdown, Tables→CSV/MD, KV→JSON, PII, Layout map). If you add one, make a PR!&lt;/p&gt; &lt;p&gt;Quick start&lt;/p&gt; &lt;h1&gt;clone&lt;/h1&gt; &lt;p&gt;git clone &lt;a href="https://github.com/rdumasia303/deepseek_ocr_app"&gt;https://github.com/rdumasia303/deepseek_ocr_app&lt;/a&gt; cd deepseek_ocr_app&lt;/p&gt; &lt;h1&gt;run&lt;/h1&gt; &lt;p&gt;docker compose up -d --build&lt;/p&gt; &lt;h1&gt;open&lt;/h1&gt; &lt;h1&gt;frontend: http://localhost:3000 (or whatever the repo says)&lt;/h1&gt; &lt;h1&gt;backend: http://localhost:8000/docs&lt;/h1&gt; &lt;p&gt;Heads-up: First model load downloads weights + custom code (trust_remote_code). If you want reproducibility, pin a specific HF revision in the backend.&lt;/p&gt; &lt;p&gt;Sample prompts (try these) Plain OCR: (no need to type anything — just run the mode) Describe: “Describe this image concisely in 2–3 sentences.” Find: set term to Total Due, Signature, Logo, etc. Freeform: “Convert the document to markdown.” “Extract every table and output CSV only.” “Return strict JSON with fields {invoice_no, date, vendor, total:{amount,currency}}.” Known rough edges (be gentle, or better, fix them 😅)&lt;/p&gt; &lt;p&gt;Grounding (boxes) can be flaky; plain OCR and describe are rock-solid. Structured outputs (CSV/MD/JSON) need post-processing to be 100% reliable.&lt;/p&gt; &lt;p&gt;Roadmap / ideas (grab an issue &amp;amp; go wild)&lt;/p&gt; &lt;p&gt;Add Markdown / Tables / JSON / PII / Layout modes (OCR-first with deterministic fallbacks).&lt;/p&gt; &lt;p&gt;Proper box overlay scaling (processed size vs CSS pixels) — coords should snap exactly.&lt;/p&gt; &lt;p&gt;PDF ingestion (pdf2image → per-page OCR + merge).&lt;/p&gt; &lt;p&gt;Simple telemetry (mode counts, latency, GPU mem) for perf tuning.&lt;/p&gt; &lt;p&gt;One-click HuggingFace revision pin to avoid surprise code updates. If you try it, please drop feedback ) — I’ll iterate. If you make it better, I’ll take your PRs ASAP. 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid_Passion_6916"&gt; /u/Putrid_Passion_6916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oc7uio/deepseekocr_playground_dockerized_fastapi_react/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T08:56:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oclqet</id>
    <title>OpenCode Chat - a slimmer version of OC. From 20k tokens init to 5k.</title>
    <updated>2025-10-21T19:01:30+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclqet/opencode_chat_a_slimmer_version_of_oc_from_20k/"&gt; &lt;img alt="OpenCode Chat - a slimmer version of OC. From 20k tokens init to 5k." src="https://external-preview.redd.it/L3Y0gISje-yz5B28l5INg1cRAu2hY9SdiINwtHg1QYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189cca2da0e56e02b343825c7c8a8e4fa3337b7b" title="OpenCode Chat - a slimmer version of OC. From 20k tokens init to 5k." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use OpenCode &lt;strong&gt;a lot&lt;/strong&gt;… And I got so used to it, I'd rather use it over a bloatware chat client that overwhelms local models, so I forked it and slimmed it down.&lt;/p&gt; &lt;p&gt;Startup token consumption dropped from ~20K to ~5K. Will tools be less reliable? Probably. Can you now run it easier with your local models? Yeah. Should you, if you can't handle 20k context? Probably not :)&lt;/p&gt; &lt;p&gt;The entire prompt stack and tool descriptions have been rewritten around chatting instead of coding. Every file. Even &lt;code&gt;/compact&lt;/code&gt; now has persona continuity instructions instead of code-alignment language (why the hell is compacting not a thing outside of coding?!)&lt;/p&gt; &lt;p&gt;Coding might still be viable thanks to LSP, which will correct any (pun intended) mistakes made by the model.&lt;/p&gt; &lt;p&gt;This fork still uses your global config (at least on Linux), incl. MCPs and auth. Functionality is basically unchanged, it's just using slimmer descriptions and some re-engineered prompts (all changes documented in the forked repo, for the curious).&lt;/p&gt; &lt;p&gt;Linux x64 tested. Other binaries exist - try them at your own risk. I've used the standard build script, so in theory it should work. Lemme know.&lt;/p&gt; &lt;p&gt;Full details + stats + binaries are in the link. It will not always be the latest OC version, because the devs are shipping to hard :)&lt;/p&gt; &lt;p&gt;Ideas welcome. One thing I was thinking about is adding an &amp;quot;Excel&amp;quot; tool for those that want to use it in business applications without hooking it up to the cloud. I've had a go at integrating some weird stuff previously, so... happy to accept reasonable requests.&lt;/p&gt; &lt;p&gt;Much love for the OC devs &amp;lt;3 Go support them. Praise be Open Source. &lt;/p&gt; &lt;p&gt;(Funnily enough, I used CC to work on this, OC was getting confused while working on itself, and I couldn't be arsed with all the agents markdown files)&lt;br /&gt; (also, sorry, not as exciting as Qwen3VL or GPT Atlas.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IgorWarzocha/opencode-chat/releases/tag/opencode-chat-v0.1.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclqet/opencode_chat_a_slimmer_version_of_oc_from_20k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oclqet/opencode_chat_a_slimmer_version_of_oc_from_20k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T19:01:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbggm</id>
    <title>Poll on thinking/no thinking for the next open-weights Google model</title>
    <updated>2025-10-21T12:22:21+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://x.com/osanseviero/status/1980553451261292628"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbggm/poll_on_thinkingno_thinking_for_the_next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbggm/poll_on_thinkingno_thinking_for_the_next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocbkry</id>
    <title>[By GLM Team] Glyph: Scaling Context Windows via Visual-Text Compression</title>
    <updated>2025-10-21T12:27:54+00:00</updated>
    <author>
      <name>/u/NeterOster</name>
      <uri>https://old.reddit.com/user/NeterOster</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.17800"&gt;https://arxiv.org/abs/2510.17800&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at &lt;a href="https://github.com/thu-coai/Glyph"&gt;this https URL&lt;/a&gt;.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model is not yet available at the moment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeterOster"&gt; /u/NeterOster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocbkry/by_glm_team_glyph_scaling_context_windows_via/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1occ8uv</id>
    <title>Confirmed: Junk social media data makes LLMs dumber</title>
    <updated>2025-10-21T12:58:04+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt; &lt;img alt="Confirmed: Junk social media data makes LLMs dumber" src="https://a.thumbs.redditmedia.com/It6udXwo12VzdtUYx3sIoCiTZnQLhz-QLVIedLuaJN8.jpg" title="Confirmed: Junk social media data makes LLMs dumber" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new study from Texas A&amp;amp;M University and Purdue University proposes the &lt;em&gt;LLM Brain Rot Hypothesis&lt;/em&gt;: continual pretraining on “junk” social-media text (short, viral, sensational content) causes lasting declines in reasoning, long-context and safety.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wq569rzfpgwf1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7a14a98cc9682cd209918c93fa23222d2df7b23"&gt;https://preview.redd.it/wq569rzfpgwf1.png?width=2772&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e7a14a98cc9682cd209918c93fa23222d2df7b23&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ARC-Challenge with Chain Of Thoughts drops 74.9 → 57.2 and RULER-CWE 84.4 → 52.3 as junk ratio rises from 0% to 100%.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occ8uv/confirmed_junk_social_media_data_makes_llms_dumber/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T12:58:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocko1m</id>
    <title>Comparison new qwen 32b-vl vs qwen 30a3-vl</title>
    <updated>2025-10-21T18:22:10+00:00</updated>
    <author>
      <name>/u/Healthy-Nebula-3603</name>
      <uri>https://old.reddit.com/user/Healthy-Nebula-3603</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocko1m/comparison_new_qwen_32bvl_vs_qwen_30a3vl/"&gt; &lt;img alt="Comparison new qwen 32b-vl vs qwen 30a3-vl" src="https://b.thumbs.redditmedia.com/IQJxhOFgNXeP5HldjZTtWHPpItZ5QEcogwrGdma7mqE.jpg" title="Comparison new qwen 32b-vl vs qwen 30a3-vl" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Nebula-3603"&gt; /u/Healthy-Nebula-3603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ocko1m"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocko1m/comparison_new_qwen_32bvl_vs_qwen_30a3vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocko1m/comparison_new_qwen_32bvl_vs_qwen_30a3vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T18:22:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oclug7</id>
    <title>Getting most out of your local LLM setup</title>
    <updated>2025-10-21T19:05:35+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, been active LLM user since before LLama 2 weights, running my first inference of Flan-T5 with &lt;code&gt;transformers&lt;/code&gt; and later &lt;code&gt;ctranslate2&lt;/code&gt;. We regularly discuss our local setups here and I've been rocking mine for a couple of years now, so I have a few things to share. Hopefully some of them will be useful for your setup too. I'm not using an LLM to write this, so forgive me for any mistakes I made.&lt;/p&gt; &lt;h1&gt;Dependencies&lt;/h1&gt; &lt;p&gt;Hot topic. When you want to run 10-20 different OSS projects for the LLM lab - containers are almost a must. Image sizes are really unfortunate (especially with Nvidia stuff), but it's much less painful to store 40GBs of images locally than spending an entire evening on Sunday figuring out some obscure issue between Python / Node.js / Rust / Go dependencies. Setting it up is a one-time operation, but it simplifies upgrades and portability of your setup by a ton. Both Nvidia and AMD have very decent support for container runtimes, typically with a plugin for the container engine. Speaking about one - doesn't have to be Docker, but often it saves time to have the same bugs as everyone else.&lt;/p&gt; &lt;h1&gt;Choosing a Frontend&lt;/h1&gt; &lt;p&gt;The only advice I can give here is not to choose any single specific one, cause most will have their own disadvantages. I tested a lot of different ones, here is the gist:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt; - has more features than you'll ever need, but can be tricky to setup/maintain. Using containerization really helps - you set it up one time and forget about it. One of the best projects in terms of backwards compatibility, I've started using it when it was called Ollama WebUI and all my chats were preserved through all the upgrades up to now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Nio&lt;/strong&gt; - can only recommend if you want to setup an LLM marketplace for some reason.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hollama&lt;/strong&gt; - my go-to when I want a quick test of some API or model, you don't even need to install it in fact, it works perfectly fine from their GitHub pages (use it like that only if you know what you're doing though).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace ChatUI&lt;/strong&gt; - very basic, but without any feature bloat.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KoboldCpp&lt;/strong&gt; - AIO package, less polished than the other projects, but have these &amp;quot;crazy scientist&amp;quot; vibes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lobe Chat&lt;/strong&gt; - similarly countless features like Open WebUI, but less polished and coherent, UX can be confusing at times. However, has a lot going on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LibreChat&lt;/strong&gt; - another feature-rich Open WebUI alternative. Configuration can be a bit more confusing though (at least for me) due to a wierd approach to defining models and backends to connect to as well as how to fetch model lists from them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mikupad&lt;/strong&gt; - another &amp;quot;crazy scientist&amp;quot; project. Has a unique approach to generation and editing of the content. Supports a lot of lower-level config options compared to other frontends.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parllama&lt;/strong&gt; - probably most feature-rich TUI frontend out there. Has a lot of features you would only expect to see in a web-based UI. A bit heavy, can be slow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;oterm&lt;/strong&gt; - Ollama-specific, terminal-based, quite lightweight compared to some other options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;aichat&lt;/strong&gt; - Has a very generic name (in the &lt;code&gt;sigoden&lt;/code&gt;s GitHub), but is one of the simplest LLM TUIs out there. Lightweight, minimalistic, and works well for a quick chat in terminal or some shell assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gptme&lt;/strong&gt; - Even simpler than &lt;code&gt;aichat&lt;/code&gt;, with some agentic features built-in.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Interpreter&lt;/strong&gt; - one of the OG TUI agents, looked very cool then got some funding then went silent and now it's not clear what's happening with it. Based on approaches that are quite dated now, so not worth trying unless you're curious about this one specifically.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The list above is of course not exhaustive, but these are the projects I had a chance to try myself. In the end, I always return to Open WebUI as after initial setup it's fairly easy to start and it has more features than I could ever need.&lt;/p&gt; &lt;h1&gt;Choosing a Backend&lt;/h1&gt; &lt;p&gt;Once again, no single best option here, but there are some clear &amp;quot;niche&amp;quot; choices depending on your use case.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt; - not much to say, you probably know everything about it already. Great (if not only) for lightweight or CPU-only setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; - when you simply don't have time to read &lt;code&gt;llama.cpp&lt;/code&gt; docs, or compiling it from scratch. It's up to you to decide on the attribution controversy and I'm not here to judge.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vllm&lt;/strong&gt; - for a homelab, I can only recommend it if you have: a) Hardware, b) Patience, c) A specific set of models you run, d) a few other people that want to use your LLM with you. Goes one level deeper compared to &lt;code&gt;llama.cpp&lt;/code&gt; in terms of configurability and complexity, requires hunting for specific quants.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aphrodite&lt;/strong&gt; - If you chose KoboldCpp over Open WebUI, you're likely to choose Aphrodite over vllm.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KTransformers&lt;/strong&gt; - When you're trying to hunt down every last bit of performance your rig can provide. Has some very specific optimisation for specific hardware and specific LLM architectures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mistral.rs&lt;/strong&gt; - If you code in Rust, you might consider this over llama.cpp. The lead maintainer is very passionate about the project and often adds new architectures/features ahead of other backneds. At the same time, the project is insanely big, so things often take time to stabilize. Has some unique features that you won't find anywhere else: AnyMoE, ISQ quants, supports diffusion models, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular MAX&lt;/strong&gt; - inference engine from creators of Mojo language. Meant to transform ML and LLM inference in general, but work is still in early stages. Models take ~30s to compile on startup. Typically runs the original FP16 weights, so requires beefy GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexa SDK&lt;/strong&gt; - if you want something similar to Ollama, but you don't want Ollama itself. Concise CLI, supports a variety of architectures. Has bugs and usability issues due to a smaller userbase, but is actively developed. Might have some Corporate drama/controversy in the future.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt; - similar to &lt;code&gt;ktransformers&lt;/code&gt;, highly optimised for specific hardware and model architectures, but requires a lot of involvement for configuration and setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TabbyAPI&lt;/strong&gt; - wraps Exllama2 and Exllama3 with a more convenient and easy-to-use package that one would expect from an inference engine. Approximately at the same level of complexity as &lt;code&gt;vllm&lt;/code&gt; or &lt;code&gt;llama.cpp&lt;/code&gt;, but requires more specific quants.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Text Generation Inference&lt;/strong&gt; - it's like Ollama for &lt;code&gt;llama.cpp&lt;/code&gt; or TabbyAPI for Exllama3, but for &lt;code&gt;transformers&lt;/code&gt;. &amp;quot;Official&amp;quot; implementation, using same model architecture as a reference. Some common optimisations on top. Can be a more friendly alternative to &lt;code&gt;ktransformers&lt;/code&gt; or &lt;code&gt;sglang&lt;/code&gt;, but not as feature-rich.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AirLLM&lt;/strong&gt; - extremely niche use-case. You have a workload that can be slow (overnight), no API-based LLMs are acceptable, your hardware only allows for tiny models, but the task needs some of the big boys. If all these boxes are ticket - AirLLM might help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think that the key of a good homelab setup is to be able to quickly run an engine that is suitable for a specific model/feature that you want right now. Many more niche engines are moving faster than &lt;code&gt;llama.cpp&lt;/code&gt; (at the expense of stability), so having them available can allow testing new models/features earlier.&lt;/p&gt; &lt;h1&gt;TTS / STT&lt;/h1&gt; &lt;p&gt;I recommend projects that support OpenAI-compatible APIs here, that way they are more likely to integrate well with the other parts of your LLM setup. I can personally recommend Speaches (former &lt;code&gt;faster-whisper-server&lt;/code&gt;, more active) and &lt;code&gt;openedai-speech&lt;/code&gt; (less active, more hackable). Both have TTS and STT support, so you can build voice assistants with them. Containerized deployment is possible for both.&lt;/p&gt; &lt;h1&gt;Tunnels&lt;/h1&gt; &lt;p&gt;Exposing your homelab setup to the Internet can be very powerful. It's very dangerous too, so be careful. Less involved setups are based on running somethings like &lt;code&gt;cloudflared&lt;/code&gt; or &lt;code&gt;ngrok&lt;/code&gt; at the expense of some privacy and security. More involved setups are based on running your own VPN or reverse proxy with proper authentication. Tailscale is a great option.&lt;/p&gt; &lt;p&gt;A very useful/convenient add-on is to also generate a QR for your mobile device to connect to your homelab services quickly. There are some CLI tools for that too.&lt;/p&gt; &lt;h1&gt;Web RAG &amp;amp; Deep Search&lt;/h1&gt; &lt;p&gt;Almost a must for any kind of useful agentic system right now. The absolute easiest way to get one is to use &lt;a href="https://github.com/searxng/searxng"&gt;SearXNG&lt;/a&gt;. It connects nicely with a variety of frontends out of the box, including Open WebUI and LibreChat. You can run it in a container as well, so it's easy to maintain. Just make sure to configure it properly to avoid leaking your data to third parties. The quality is not great compared to paid search engines, but it's free and relatively private. If you have a budget, consider using Tavily or Jina for same purpose and every LLM will feel like a mini-Perplexity.&lt;/p&gt; &lt;p&gt;Some notable projects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Deep Research&lt;/strong&gt; - &amp;quot;Deep research at home&amp;quot;, not quite in-depth, but works decently well&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Morphic&lt;/strong&gt; - Probably most convenient to setup out of the bunch.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Perplexica&lt;/strong&gt; - Started not very developer-friendly, with some gaps/unfinished features, so haven't used actively.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SurfSense&lt;/strong&gt; - was looking quite promising in Nov 2024, but they didn't have pre-built images back then. Maybe better now.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Workflows&lt;/h1&gt; &lt;p&gt;Crazy amount of companies are building things for LLM-based automation now, most are looking like workflow engines. Pretty easy to have one locally too.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dify&lt;/strong&gt; - very well polished, great UX and designed specifically for LLM workflows (unlike &lt;code&gt;n8n&lt;/code&gt; that is more general-purpose). The biggest drawback - lack of OpenAI-compatible API for built workflows/agents, but comes with built-in UI, traceability, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flowise&lt;/strong&gt; - Similar to Dify, but more focused on LangChain functionality. Was quite buggy last time I tried, but allowed for a simpler setup of basic agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangFlow&lt;/strong&gt; - a more corporate-friendly version of Flowise/Dify, more polished, but locked on LangChain. Very turbulent development, breaking changes often introduced.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n8n&lt;/strong&gt; - Probably most well-known one, fair-code workflow automation platform with native AI capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open WebUI Pipelines&lt;/strong&gt; - Most powerful option if you firmly settled on Open WebUI and can do some Python, can do wild things for chat workflows.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Coding&lt;/h1&gt; &lt;p&gt;Very simple, current landscape is dominated by TUI agents. I tried a few personally, but unfortunately can't say that I use any of them regularly, compared to the agents based on the cloud LLMs. OpenCode + Qwen 3 Coder 480B, GLM 4.6, Kimi K2 get quite close but not close enough for me, your experience may vary.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenCode&lt;/strong&gt; - great performance, good support for a variety of local models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crush&lt;/strong&gt; - the agent seems to perform worse than OpenCode with same models, but more eye-candy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aider&lt;/strong&gt; - the OG. Being a mature well-developed project is both a pro and a con. Agentic landscape is moving fast, some solutions that were good in the past are not that great anymore (mainly talking about tool call formatting).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenHands&lt;/strong&gt; - provides a TUI agents with a WebUI, pairs nicely with Codestral, aims to be OSS version of Devin, but the quality of the agents is not quite there yet.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Extras&lt;/h1&gt; &lt;p&gt;Some other projects that can be useful for a specific use-case or just for fun. Recent smaller models suddenly became very good at agentic tasks, so surprisingly many of these tools work well enough.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agent Zero&lt;/strong&gt; - general-purpose personal assistant with Web RAG, persistent memory, tools, browser use and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Airweave&lt;/strong&gt; - ETL tool for LLM knowledge, helps to prepare data for agentic use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bolt.new&lt;/strong&gt; - Full-stack app development fully in the browser.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Browser Use&lt;/strong&gt; - LLM-powered browser automation with web UI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docling&lt;/strong&gt; - Transform documents into format ready for LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fabric&lt;/strong&gt; - LLM-driven processing of the text data in the terminal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangFuse&lt;/strong&gt; - easy LLM Observability, metrics, evals, prompt management, playground, datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latent Scope&lt;/strong&gt; - A new kind of workflow + tool for visualizing and exploring datasets through the lens of latent spaces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LibreTranslate&lt;/strong&gt; - A free and open-source machine translation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt; - LLM proxy that can aggregate multiple inference APIs together into a single endpoint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LitLytics&lt;/strong&gt; - Simple analytics platform that leverages LLMs to automate data analysis.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama-swap&lt;/strong&gt; - Runs multiple llama.cpp servers on demand for seamless switching between them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;lm-evaluation-harness&lt;/strong&gt; - A de-facto standard framework for the few-shot evaluation of language models. I can't tell that it's very user-friendly though, figuring out how to run evals for a local LLM takes some effort.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mcpo&lt;/strong&gt; - Turn MCP servers into OpenAPI REST APIs - use them anywhere.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MetaMCP&lt;/strong&gt; - Allows to manage MCPs via a WebUI, exposes multiple MCPs as a single server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OptiLLM&lt;/strong&gt; - Optimising LLM proxy that implements many advanced workflows to boost the performance of the LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Promptfoo&lt;/strong&gt; - A very nice developer-friendly way to setup evals for anything OpenAI-API compatible, including local LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repopack&lt;/strong&gt; - Packs your entire repository into a single, AI-friendly file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL Chat&lt;/strong&gt; - Chat-based SQL client, which uses natural language to communicate with the database. Be wary about connecting to the data you actually care about without proper safeguards.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SuperGateway&lt;/strong&gt; - A simple and powerful API gateway for LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TextGrad&lt;/strong&gt; - Automatic &amp;quot;Differentiation&amp;quot; via Text - using large language models to backpropagate textual gradients.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Webtop&lt;/strong&gt; - Linux in a web browser supporting popular desktop environments. Very conventient for local Computer Use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully some of this was useful! Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T19:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1och0jn</id>
    <title>Nvidia quietly released RTX Pro 5000 Blackwell 72Gb</title>
    <updated>2025-10-21T16:05:55+00:00</updated>
    <author>
      <name>/u/AleksHop</name>
      <uri>https://old.reddit.com/user/AleksHop</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/nvidia/comments/1oc76i7/nvidia_quietly_launches_rtx_pro_5000_blackwell/"&gt;https://www.reddit.com/r/nvidia/comments/1oc76i7/nvidia_quietly_launches_rtx_pro_5000_blackwell/&lt;/a&gt;&lt;br /&gt; Price will be about 5000$&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AleksHop"&gt; /u/AleksHop &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1och0jn/nvidia_quietly_released_rtx_pro_5000_blackwell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:05:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocgun0</id>
    <title>DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds...</title>
    <updated>2025-10-21T16:00:06+00:00</updated>
    <author>
      <name>/u/Xtianus21</name>
      <uri>https://old.reddit.com/user/Xtianus21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"&gt; &lt;img alt="DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds..." src="https://b.thumbs.redditmedia.com/Dsj93JBsqXXjXoOsuIY5K8HbnnE2QjHtG5dVlESoKDU.jpg" title="DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/BrianRoemmele/status/1980634806145957992"&gt;https://x.com/BrianRoemmele/status/1980634806145957992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AND &lt;/p&gt; &lt;p&gt;Have a full understanding of the text/complex drawings and their context. &lt;/p&gt; &lt;p&gt;I just changed offline data curation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xtianus21"&gt; /u/Xtianus21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ocgun0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1occan8</id>
    <title>vLLM + OpenWebUI + Tailscale = private, portable AI</title>
    <updated>2025-10-21T13:00:13+00:00</updated>
    <author>
      <name>/u/zhambe</name>
      <uri>https://old.reddit.com/user/zhambe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"&gt; &lt;img alt="vLLM + OpenWebUI + Tailscale = private, portable AI" src="https://b.thumbs.redditmedia.com/cRxS8E_CLjIUT07vC7k-8ob1jCAW2BZimcQfFKJicNg.jpg" title="vLLM + OpenWebUI + Tailscale = private, portable AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mind is positively blown... My own AI?!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zhambe"&gt; /u/zhambe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1occan8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occan8/vllm_openwebui_tailscale_private_portable_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:00:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1occyly</id>
    <title>Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs</title>
    <updated>2025-10-21T13:28:06+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"&gt; &lt;img alt="Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs" src="https://preview.redd.it/a21ouwhkvgwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=242dfbde4c1caaaa4f35057e48df50de5e9cc8f6" title="Qwen3-Next 80B-A3B llama.cpp implementation with CUDA support half-working already (up to 40k context only), also Instruct GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3424224842"&gt;Llama.cpp pull request&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;GGUFs for Instruct model&lt;/a&gt; (old news but info for the uninitiated)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a21ouwhkvgwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1occyly/qwen3next_80ba3b_llamacpp_implementation_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T13:28:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1och7m9</id>
    <title>Qwen3-VL-2B and Qwen3-VL-32B Released</title>
    <updated>2025-10-21T16:13:23+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"&gt; &lt;img alt="Qwen3-VL-2B and Qwen3-VL-32B Released" src="https://preview.redd.it/n4rx9o72phwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31c5eea069f1786b249324b0d23eca9977c6918b" title="Qwen3-VL-2B and Qwen3-VL-32B Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n4rx9o72phwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
