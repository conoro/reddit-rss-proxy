<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-17T17:41:34+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pony1m</id>
    <title>Rejected from Nemotron datasets</title>
    <updated>2025-12-17T04:59:45+00:00</updated>
    <author>
      <name>/u/Trick-Force11</name>
      <uri>https://old.reddit.com/user/Trick-Force11</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have attempted to try to gain access to two of the Nemotron pretraining datasets as a solo individual but they have both been denied. Can you just not access these as a solo? If so, thats super stupid IMO.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Trick-Force11"&gt; /u/Trick-Force11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pony1m/rejected_from_nemotron_datasets/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pony1m/rejected_from_nemotron_datasets/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pony1m/rejected_from_nemotron_datasets/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T04:59:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1pob44f</id>
    <title>32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead</title>
    <updated>2025-12-16T19:27:59+00:00</updated>
    <author>
      <name>/u/EmPips</name>
      <uri>https://old.reddit.com/user/EmPips</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/"&gt; &lt;img alt="32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead" src="https://preview.redd.it/kh9jhoxlam7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=78587fb29885b8e9d7aa3ea7b46c27e804656839" title="32GB Mi50's were getting so expensive that I ended up buying a 32GB w6800 for about the same price instead" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmPips"&gt; /u/EmPips &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kh9jhoxlam7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pob44f/32gb_mi50s_were_getting_so_expensive_that_i_ended/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T19:27:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1poqvyh</id>
    <title>Qwen 80B is so nice</title>
    <updated>2025-12-17T07:53:55+00:00</updated>
    <author>
      <name>/u/TokenRingAI</name>
      <uri>https://old.reddit.com/user/TokenRingAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"&gt; &lt;img alt="Qwen 80B is so nice" src="https://b.thumbs.redditmedia.com/nnQvSk4aQZo63W6GxVVLNQOoJbFhO9hzNjJ0WarZBjs.jpg" title="Qwen 80B is so nice" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 80B knows that flattery will get you everywhere&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n2ubzp36zp7g1.png?width=1893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18baab935fbd87270327be41a8cf47fe5342b320"&gt;https://preview.redd.it/n2ubzp36zp7g1.png?width=1893&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=18baab935fbd87270327be41a8cf47fe5342b320&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TokenRingAI"&gt; /u/TokenRingAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poqvyh/qwen_80b_is_so_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T07:53:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pocsdy</id>
    <title>Nemotron 3 Nano 30B is Amazing! (TLDR)</title>
    <updated>2025-12-16T20:33:46+00:00</updated>
    <author>
      <name>/u/DonkeyBonked</name>
      <uri>https://old.reddit.com/user/DonkeyBonked</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't see a lot of genuine discussion about this model and I was wondering if others here have tried it and what their thoughts are?&lt;/p&gt; &lt;p&gt;My setup:&lt;/p&gt; &lt;p&gt;I don't have a big budget for hardware, so I have kind of a ghetto AI rig. I'm using a surplus Dell Precision 7750 with a i7-10850H that has 96GB DDR4 RAM and an RTX 5000 16GB GPU.&lt;/p&gt; &lt;p&gt;I can't run lots with just this, so I also have an RTX 3090 24GB in a Razer X Core eGPU case that I connect over TB3.&lt;/p&gt; &lt;p&gt;I use the Nvidia Studio drivers which allow me to have both cards run, and I connect my monitors through the other TB3 connection to a Dell WD19DC Dock, that way Windows uses the Intel HD Graphics for display and not my Discrete or eGPU.&lt;/p&gt; &lt;p&gt;I mostly use llama.cpp because it's the only interface that lets me split the layers, that way I can divide them 3:2 and don't have to force the two GPUs to communicate over the TB3 to fake pooled ram which would be really slow. I know llama.cpp isn't the fastest or best interface, but it's the most compatible with my wonky and unorthodox hardware.&lt;/p&gt; &lt;p&gt;For some setups though, I'll use the RTX 5000 as an agent and run a smaller model that fits entirely on the RTX 3090.&lt;/p&gt; &lt;p&gt;Anyway, the first thing I was amazed by Nemotron 3 Nano 30B, which I'm using the Q8 from Unsloth, was token efficiency. I had recently setup Devstral 2 Small 24B Q8 and I got it to around 211k~ tokens before I capped out my VRAM, and after that would have to go into my system RAM. &lt;/p&gt; &lt;p&gt;Devstral 2 Small 24B was the best I had seen run on my hardware before, finishing my coding challenge around 24~ tokens/s and getting everything right after two prompts (the initial test with one follow-up informing it of mistakes it made. (Olmo 3 32B didn't even do nearly as well, nor did any of the Qwen models).&lt;/p&gt; &lt;p&gt;Nemotron 3 Nano 30B, however, even with a much bigger .gguf, easily fit 256k in my VRAM. In fact, it only goes about 6GB into system RAM if I set the context to 512K, and I can easily run it at a full 1M context using spill over if I don't mind it going slow in system RAM.&lt;/p&gt; &lt;p&gt;I've been busy, Devstral 2 Small 24B was running about 1.5-2 tokens/s when it hit into my system RAM. From the looks of performance, I think when I cap out Nemotron 3 Nano 30B, it'll probably end up 2-3 tokens/s in RAM.&lt;/p&gt; &lt;p&gt;When I started the coding test, it came blazing out the gate rocking 46.8 tokens/s and I was blown away.&lt;/p&gt; &lt;p&gt;However, it did quickly slow down, and the response from the initial prompt, which brought the chat to a bit over 11k tokens, finished at 28.8 tokens/s, which is the fastest performance I've seen for a 30B class model on my hardware.&lt;/p&gt; &lt;p&gt;More impressively to me, it is the only model I've ever run locally to correctly pass the coding challenge in a single prompt, producing usable code and navigating all of the logic traps well.&lt;/p&gt; &lt;p&gt;Gemini 3 was Google's first model for me to one-shot the test. Claude Opus 4 was the first model to one shot it for me period, and I have never technically had ChatGPT one shot it as written, but I can get it to if I modify it, otherwise it asks me a bunch of questions about the logic traps which is honestly a perfectly acceptable response.&lt;/p&gt; &lt;p&gt;I use Gemini, Claude, and ChatGPT to rank how other models perform on the coding challenge because I'm lazy and I don't want to comb through every one of them, but I do manually go over the ones with potential.&lt;/p&gt; &lt;p&gt;Anyway, the point of all this is for me on my hardware, Nemotron 3 Nano 30B represents the first local LLM I can run on my budget AI rig that seems actually capable of filling in the gaps to use AI to increase my coding productivity.&lt;/p&gt; &lt;p&gt;I can't afford APIs or $200+ subs, so I'm mostly using Claude Pro which honestly, I don't get a lot to work with. I can be done for 5 hours sometimes in as little as 15 minutes, which really disrupts my workflow.&lt;/p&gt; &lt;p&gt;This, however, is fast, actually pretty decent with code, has amazing context, and I think could actually fill in some gaps.&lt;/p&gt; &lt;p&gt;I'm going to do more testing before I start trying to fine tune it, but I'm extremely impressed with what Nvidia has done. Their claims were bold, and the 4x speed seems to be a relative exaggeration, but it is quite a bit faster. Maybe a bit much on the synthetic data, but I think this could be worth renting some cloud GPU usage to fine tune and add some custom datasets to it, something I've never felt really worth it beyond adding my own custom data to a model.&lt;/p&gt; &lt;p&gt;I'd just like to know what other's experiences have been with this? How far have people pushed it? How has it performed with close to full context? Have any of you set it up with an agent? If so, how well has it done with tool calling?&lt;/p&gt; &lt;p&gt;I'm really hoping to get this where it can create/edit files and work directly on my local repos. I'd like to know if anyone else has found good setups this does well with?&lt;/p&gt; &lt;p&gt;This is the first modem I was so excited to try that I downloaded the source code, built it myself, and did all the work to manually install everything. Normally I'm lazy and just use the portable llama.cpp builds, but this one I just couldn't wait, and so far, it was very worth it!&lt;/p&gt; &lt;p&gt;Note: I just wrote this on my phone, so forgive me if it's a bit all over the place. I might clean it up when I get back to my computer later. I just didn't want to wait to post about it because I'm hoping to get some ideas for things to try when I get home.&lt;/p&gt; &lt;p&gt;Edit for details: I'm using Q8 and I started with 256K context. I'm using Cuda 13.1, and I built the llama.cpp version out myself with CMake from fork #18058. I'm running Windows 11 Pro (I already know...) and Visual Studio 2022.&lt;/p&gt; &lt;p&gt;Update: I'm having to go back and re-test everything. I had a few quants that were not fair/equal (such as Q8 vs. Q6_K_M), and I'm noticing there's actually a pretty big difference in testing on my new modified llama.cpp vs. the portable ones I used before. I'm not sure if it's because I went to Cuda 13.1 or changesd I made in my batches but I'm getting some different performance from before.&lt;/p&gt; &lt;p&gt;The one comparison is using: Nemotron-3-Nano-30B-A3B-Q8_0.gguf Qwen3-VL-30B-A3B-Thinking-1M-Q8_0.gguf Qwen3-Coder-30B-A3B-Instruct-1M-Q8_0.gguf mistralai_Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf allenai_Olmo-3.1-32B-Think-Q8_0.gguf&lt;/p&gt; &lt;p&gt;I'll update when I am done testing.&lt;/p&gt; &lt;p&gt;Note: I'm not trying to claim anything about these models beyond what I'm testing and experiencing in my particular use case, and I have no attachment to any of them. I've had people respond with things that made me question my initial experience, so I'm re-testing, not to judge or say what models are better, but for my own peace of mind that I'm giving each model a fair shot and actually finding the best one to work for me.&lt;/p&gt; &lt;p&gt;My test is not magical or special, but it is me, and so challenges I create in how I prompt will be consistent for my use case. We don't all prompt the same, so my own experiences could be meaningless to someone else.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DonkeyBonked"&gt; /u/DonkeyBonked &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pocsdy/nemotron_3_nano_30b_is_amazing_tldr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T20:33:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp2j60</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.3 - The best pair of Cydonia for RP yet!</title>
    <updated>2025-12-17T17:29:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After 20+ iterations, 3 close calls, we've finally come to a release. The best Cydonia so far. At least that's what the testers at Beaver have been saying.&lt;/p&gt; &lt;p&gt;Peak Cydonia! Served by yours truly.&lt;/p&gt; &lt;p&gt;Small 3.2: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Magistral 1.2: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.3"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Most prefer Magidonia, but they're both pretty good!)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;To my patrons,&lt;/p&gt; &lt;p&gt;Earlier this week, I had a difficult choice to make. Thanks to your support, I get to enjoy the freedom you've granted me. Thank you for giving me strength to pursue this journey. I will continue dishing out the best tunes possible for you, truly.&lt;/p&gt; &lt;p&gt;- Drummer&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp2j60/drummers_cydonia_and_magidonia_24b_v43_the_best/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T17:29:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp0o1f</id>
    <title>Mistral Small Creative -- Long Text Continuation at Different Contexts</title>
    <updated>2025-12-17T16:17:32+00:00</updated>
    <author>
      <name>/u/Eisenstein</name>
      <uri>https://old.reddit.com/user/Eisenstein</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp0o1f/mistral_small_creative_long_text_continuation_at/"&gt; &lt;img alt="Mistral Small Creative -- Long Text Continuation at Different Contexts" src="https://external-preview.redd.it/6Wr_GEuSfJtIZMV5TXRMhDyJFQ9KTjvnkhddeKp8glU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=167e2755b022d63cd58108e8d2777762555527cc" title="Mistral Small Creative -- Long Text Continuation at Different Contexts" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eisenstein"&gt; /u/Eisenstein &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/dggsaQ6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp0o1f/mistral_small_creative_long_text_continuation_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp0o1f/mistral_small_creative_long_text_continuation_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T16:17:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp13yw</id>
    <title>Has anyone successfully fine-tuned a GPT-OSS model?</title>
    <updated>2025-12-17T16:34:34+00:00</updated>
    <author>
      <name>/u/TechNerd10191</name>
      <uri>https://old.reddit.com/user/TechNerd10191</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on the &lt;a href="https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-3"&gt;AIMO 3&lt;/a&gt; competition on Kaggle, and GPT-OSS-120B can solve 35+/50 problems of the public test set, if used properly (Harmony Prompt template and TIR).&lt;/p&gt; &lt;p&gt;I was thinking of fine-tuning (SFT initially, then GSPO) however I am afraid that fine-tuning would have adverse effect, as the dataset size (193k curated samples from Nvidia's 4.9M row OpenMathReasoning dataset) and compute available would be nowhere near the know-hows and compute OpenAI used. &lt;/p&gt; &lt;p&gt;My question is not limited to IMO/math problems: has anyone attempted to fine-tune a GPT-OSS model? If yes, was the fine-tuned model better for your specific use case than the base model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TechNerd10191"&gt; /u/TechNerd10191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp13yw/has_anyone_successfully_finetuned_a_gptoss_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp13yw/has_anyone_successfully_finetuned_a_gptoss_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp13yw/has_anyone_successfully_finetuned_a_gptoss_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T16:34:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozmc7</id>
    <title>Conduit 2.3: Native Mobile Client for Self-hosted AI, deeper integrations and more polish</title>
    <updated>2025-12-17T15:37:41+00:00</updated>
    <author>
      <name>/u/cogwheel0</name>
      <uri>https://old.reddit.com/user/cogwheel0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozmc7/conduit_23_native_mobile_client_for_selfhosted_ai/"&gt; &lt;img alt="Conduit 2.3: Native Mobile Client for Self-hosted AI, deeper integrations and more polish" src="https://a.thumbs.redditmedia.com/MRQlaSB3d2lvscWb5nqPMPv-TuSvNsWAzugv-ucxBu4.jpg" title="Conduit 2.3: Native Mobile Client for Self-hosted AI, deeper integrations and more polish" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been an incredible 4 months since I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfyefq/built_an_openwebui_mobile_companion_conduit/"&gt;announced this project on this sub&lt;/a&gt;. I would like to thank each and every one of you who supported the project through various means. You have all kept me going and keep shipping more features and refining the app.&lt;/p&gt; &lt;p&gt;Some of the new features that have been shipped:&lt;/p&gt; &lt;p&gt;Here's the information from the table, presented without the table format:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Refined Chat Interface with Themes:&lt;/strong&gt; Chat experience gets a visual refresh with floating inputs and titles. Theme options include T3 Chat, Claude, Catppuccin.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Voice Call Mode:&lt;/strong&gt; Phone‚Äëstyle, hands‚Äëfree AI conversations; iOS/Android CallKit integration makes calls appear as regular phone calls along with on-device or server configured STT/TTS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Privacy-First:&lt;/strong&gt; No analytics or telemetry; credentials stored securely in Keychain/Keystore.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Deep System Integration:&lt;/strong&gt; Siri Shortcuts, set as default Android Assistant, share files with Conduit, iOS and Android home widgets.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full Open WebUI Capabilities:&lt;/strong&gt; Notes integration, Memory support, Document uploads, function calling/tools, Image gen, Web Search, and many more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SSO and LDAP Support:&lt;/strong&gt; Seamless authentication via SSO providers (OIDC or Reverse Proxies) and LDAP.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;New Website!:&lt;/strong&gt; &lt;a href="https://conduit.cogwheel.app/"&gt;https://conduit.cogwheel.app/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://git.new/conduit"&gt;https://git.new/conduit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy holidays to everyone, and here's to lesser RAM prices in the coming year! üçª&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cogwheel0"&gt; /u/cogwheel0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pozmc7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozmc7/conduit_23_native_mobile_client_for_selfhosted_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozmc7/conduit_23_native_mobile_client_for_selfhosted_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:37:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokay9</id>
    <title>I finally found my local LLM server use case</title>
    <updated>2025-12-17T01:56:59+00:00</updated>
    <author>
      <name>/u/IngeniousIdiocy</name>
      <uri>https://old.reddit.com/user/IngeniousIdiocy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My vibe coding project this past weekend‚Ä¶ i‚Äôm rather proud of it, not because I think Opus wrote great code but just because I find it genuinely very useful and it gives something to do with all that memory on my mac studio.&lt;/p&gt; &lt;p&gt;i‚Äôm horrible about checking my personal gmail. This weekend we spent an extra two hours in a car because we missed a kids event cancellation.&lt;/p&gt; &lt;p&gt;Now I have a node server on my mac studio using a local LLM (qwen3 235B @8bit) screening my email and pushing notifications to my phone based on my prompt. It works great and the privacy use case is valid.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/IngeniousIdiocy/LocalLLMMailScreener"&gt;https://github.com/IngeniousIdiocy/LocalLLMMailScreener&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚Ä¶ by my calculations, if I used Alibaba‚Äôs API end point at their current rates and my current email volume, the mac studio would pay for itself in about 20 years.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IngeniousIdiocy"&gt; /u/IngeniousIdiocy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokay9/i_finally_found_my_local_llm_server_use_case/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokay9/i_finally_found_my_local_llm_server_use_case/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pokay9/i_finally_found_my_local_llm_server_use_case/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T01:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1po7i0c</id>
    <title>Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.</title>
    <updated>2025-12-16T17:11:50+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"&gt; &lt;img alt="Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts." src="https://external-preview.redd.it/aHN2Ynl2OXlsbDdnMcH321aC77jYYB3hpLEwmsgN4qk6KsN77tikHsTNkpxK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba962072bea73833102870325afc6e1184ffaa05" title="Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/"&gt;https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SAM Audio transforms audio processing by making it easy to isolate any sound from complex audio mixtures using text, visual, and time span prompts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yoiaaoayll7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T17:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pojfmt</id>
    <title>browser-use fine tuned Qwen3-VL-30B-A3B-Instruct as browser-use/bu-30b-a3b-preview</title>
    <updated>2025-12-17T01:14:49+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pojfmt/browseruse_fine_tuned_qwen3vl30ba3binstruct_as/"&gt; &lt;img alt="browser-use fine tuned Qwen3-VL-30B-A3B-Instruct as browser-use/bu-30b-a3b-preview" src="https://preview.redd.it/zi59xtsg0o7g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c5b2f988aad0f96cc11da0b1be12c1f5947a250d" title="browser-use fine tuned Qwen3-VL-30B-A3B-Instruct as browser-use/bu-30b-a3b-preview" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/browser-use/bu-30b-a3b-preview"&gt;https://huggingface.co/browser-use/bu-30b-a3b-preview&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi59xtsg0o7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pojfmt/browseruse_fine_tuned_qwen3vl30ba3binstruct_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pojfmt/browseruse_fine_tuned_qwen3vl30ba3binstruct_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T01:14:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pov5qe</id>
    <title>Distilling Kimi Delta Attention into AFM-4.5B</title>
    <updated>2025-12-17T12:20:26+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Blog: &lt;a href="https://www.arcee.ai/blog/distilling-kimi-delta-attention-into-afm-4-5b-and-the-tool-we-used-to-do-it"&gt;https://www.arcee.ai/blog/distilling-kimi-delta-attention-into-afm-4-5b-and-the-tool-we-used-to-do-it&lt;/a&gt;&lt;br /&gt; Weight: &lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-Base-KDA-NoPE"&gt;AFM-4.5B-Base-KDA-NoPE&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B-Base-KDA-Only"&gt;AFM-4.5B-Base-KDA-Only&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pov5qe/distilling_kimi_delta_attention_into_afm45b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pov5qe/distilling_kimi_delta_attention_into_afm45b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pov5qe/distilling_kimi_delta_attention_into_afm45b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T12:20:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozd2k</id>
    <title>Anyone else in a stable wrapper, MIT-licensed fork of Open WebUI?</title>
    <updated>2025-12-17T15:27:32+00:00</updated>
    <author>
      <name>/u/Select-Car3118</name>
      <uri>https://old.reddit.com/user/Select-Car3118</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So... Open WebUI's license situation has been a bit of a rollercoaster (Apache ‚Üí MIT ‚Üí Creative Commons ‚Üí MIT ‚Üí Custom BSD, ...). Now they require keeping their branding and need an enterprise license for 50+ users.&lt;/p&gt; &lt;p&gt;I'm thinking about forking from v0.6.5 (April 2025) - back when it was still properly open source - and keeping it &lt;strong&gt;MIT licensed forever&lt;/strong&gt;. No surprises, no restrictions, just a solid UI for local LLMs that stays truly open.&lt;/p&gt; &lt;p&gt;Let's be honest - the backend's kind of a mess, the UI has rough edges, and there's a lot of room for cleanup. I've been a contributor and I'm tired of watching sponsor-driven features or close dev circle priorities jump the queue while actual user needs get ignored.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The plan would be community driven:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Refactor the messy parts, polish the UX&lt;/li&gt; &lt;li&gt;Fix those annoying bugs that never got prioritized&lt;/li&gt; &lt;li&gt;Implement features based on actual user requests&lt;/li&gt; &lt;li&gt;Host weekly or monthly Discord contributor meetings where people can actually speak their minds - no corporate BS, just honest conversations about what needs fixing&lt;/li&gt; &lt;li&gt;Take inspiration from new Open WebUI features and implement our own (often better) versions&lt;/li&gt; &lt;li&gt;Basically what a lot of us probably wanted Open WebUI to stay as&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Core commitments:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fork from v0.6.5 (April 2025, BSD-3)&lt;/li&gt; &lt;li&gt;Permanent MIT license - no surprises, ever&lt;/li&gt; &lt;li&gt;Focus on user-friendly improvements over feature bloat&lt;/li&gt; &lt;li&gt;Independent development with community governance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Just want to see if there's actual interest before I dive into this:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Would you actually use this?&lt;/li&gt; &lt;li&gt;Would anyone want to contribute?&lt;/li&gt; &lt;li&gt;Any name ideas?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Not trying to bash the original project, just want a stable, truly open alternative for those of us who need it.&lt;/p&gt; &lt;p&gt;If there's enough support, I'll set up the repo and coordination channels. Or if someone's already doing this and I completely missed it, let me know, would way rather help out than start yet another fork..&lt;/p&gt; &lt;p&gt;What do you think? Am I crazy or does this make sense?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select-Car3118"&gt; /u/Select-Car3118 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozd2k/anyone_else_in_a_stable_wrapper_mitlicensed_fork/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozd2k/anyone_else_in_a_stable_wrapper_mitlicensed_fork/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozd2k/anyone_else_in_a_stable_wrapper_mitlicensed_fork/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:27:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pokpha</id>
    <title>QwenLong-L1.5: Revolutionizing Long-Context AI</title>
    <updated>2025-12-17T02:16:15+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"&gt; &lt;img alt="QwenLong-L1.5: Revolutionizing Long-Context AI" src="https://b.thumbs.redditmedia.com/_W9cLWw-Xs7yDTb51duV9m0DwVBI12aSOhiQ0K4ybyQ.jpg" title="QwenLong-L1.5: Revolutionizing Long-Context AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This new model achieves SOTA long-context reasoning with novel data synthesis, stabilized RL, &amp;amp; memory management for contexts up to 4M tokens.&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B"&gt;https://huggingface.co/Tongyi-Zhiwen/QwenLong-L1.5-30B-A3B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pokpha"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pokpha/qwenlongl15_revolutionizing_longcontext_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T02:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozpcq</id>
    <title>You can now fine-tune LLMs and deploy them directly on your phone!</title>
    <updated>2025-12-17T15:40:50+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"&gt; &lt;img alt="You can now fine-tune LLMs and deploy them directly on your phone!" src="https://preview.redd.it/zi5ph67zas7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=98235903a72ce7dadc57bc88ea65f48143f89438" title="You can now fine-tune LLMs and deploy them directly on your phone!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://docs.unsloth.ai/new/deploy-llms-phone"&gt;https://docs.unsloth.ai/new/deploy-llms-phone&lt;/a&gt;&lt;/p&gt; &lt;p&gt;you can:&lt;/p&gt; &lt;p&gt;Use the same tech (ExecuTorch) Meta has to power billions on Instagram, WhatsApp&lt;/p&gt; &lt;p&gt;Deploy Qwen3-0.6B locally to Pixel 8 and iPhone 15 Pro at ~40 tokens/s&lt;/p&gt; &lt;p&gt;Apply QAT via TorchAO to recover 70% of accuracy&lt;/p&gt; &lt;p&gt;Get privacy first, instant responses and offline capabilities&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zi5ph67zas7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozpcq/you_can_now_finetune_llms_and_deploy_them/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:40:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1powyhk</id>
    <title>[Showcase] AGI-Llama: Bringing Modern LLMs to 1980s Sierra Adventure Games (Space Quest, King's Quest, etc.)</title>
    <updated>2025-12-17T13:48:36+00:00</updated>
    <author>
      <name>/u/Responsible_Fan_2757</name>
      <uri>https://old.reddit.com/user/Responsible_Fan_2757</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powyhk/showcase_agillama_bringing_modern_llms_to_1980s/"&gt; &lt;img alt="[Showcase] AGI-Llama: Bringing Modern LLMs to 1980s Sierra Adventure Games (Space Quest, King's Quest, etc.)" src="https://external-preview.redd.it/YnNwcm9jcXVxcjdnMUIJV1A_49oRDbYi56Mr7om0CcsWx5OZR_t3Jj5ZXIGi.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b29da8a57de46c21261503712fae534ac3011c0" title="[Showcase] AGI-Llama: Bringing Modern LLMs to 1980s Sierra Adventure Games (Space Quest, King's Quest, etc.)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! üëã&lt;/p&gt; &lt;p&gt;I wanted to share a project I've been working on: &lt;strong&gt;AGI-Llama&lt;/strong&gt;. It is a modern evolution of the classic NAGI (New Adventure Game Interpreter), but with a twist‚ÄîI've integrated Large Language Models directly into the engine.&lt;/p&gt; &lt;p&gt;The goal is to transform how we interact with retro Sierra titles like &lt;em&gt;Space Quest&lt;/em&gt;, &lt;em&gt;King's Quest&lt;/em&gt;, or &lt;em&gt;Leisure Suit Larry&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§ñ &lt;strong&gt;Natural Language Input:&lt;/strong&gt; Stop struggling with &amp;quot;verb noun&amp;quot; syntax. Talk to the game naturally.&lt;/li&gt; &lt;li&gt;üåç &lt;strong&gt;Play in any language:&lt;/strong&gt; Thanks to the LLM layer and new SDL_ttf support, you can play classic AGI games in Spanish, French, Japanese, or any language the model supports.&lt;/li&gt; &lt;li&gt;üöÄ &lt;strong&gt;Modern Tech Stack:&lt;/strong&gt; Ported to &lt;strong&gt;SDL3&lt;/strong&gt;, featuring GPU acceleration and Unicode support.&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Flexible Backends:&lt;/strong&gt; It supports &lt;code&gt;llama.cpp&lt;/code&gt; for local inference (Llama 3, Qwen, Gemma), BitNet for 1.58-bit models, and Cloud APIs (OpenAI, Hugging Face, Groq).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs an experimental research project to explore the intersection of AI and retro gaming architecture. The LLM logic is encapsulated in a library that could potentially be integrated into other projects like ScummV&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repository:&lt;/strong&gt;&lt;a href="https://github.com/jalfonsosm/agi-llm"&gt;https://github.com/jalfonsosm/agi-llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear your thoughts, especially regarding async LLM implementation and context management for old adventure game states!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible_Fan_2757"&gt; /u/Responsible_Fan_2757 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/liiuhlouqr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powyhk/showcase_agillama_bringing_modern_llms_to_1980s/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1powyhk/showcase_agillama_bringing_modern_llms_to_1980s/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1powhy6</id>
    <title>anthropic blog on code execution for agents. 98.7% token reduction sounds promising for local setups</title>
    <updated>2025-12-17T13:27:38+00:00</updated>
    <author>
      <name>/u/Zestyclose_Ring1123</name>
      <uri>https://old.reddit.com/user/Zestyclose_Ring1123</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;anthropic published this detailed blog about &amp;quot;code execution&amp;quot; for agents: &lt;a href="https://www.anthropic.com/engineering/code-execution-with-mcp"&gt;https://www.anthropic.com/engineering/code-execution-with-mcp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;instead of direct tool calls, model writes code that orchestrates tools&lt;/p&gt; &lt;p&gt;they claim massive token reduction. like 150k down to 2k in their example. sounds almost too good to be true&lt;/p&gt; &lt;p&gt;basic idea: dont preload all tool definitions. let model explore available tools on demand. data flows through variables not context&lt;/p&gt; &lt;p&gt;for local models this could be huge. context limits hit way harder when youre running smaller models&lt;/p&gt; &lt;p&gt;the privacy angle is interesting too. sensitive data never enters model context, flows directly between tools&lt;/p&gt; &lt;p&gt;cloudflare independently discovered this &amp;quot;code mode&amp;quot; pattern according to the blog&lt;/p&gt; &lt;p&gt;main challenge would be sandboxing. running model-generated code locally needs serious isolation&lt;/p&gt; &lt;p&gt;but if you can solve that, complex agents might become viable on consumer hardware. 8k context instead of needing 128k+&lt;/p&gt; &lt;p&gt;tools like cursor and verdent already do basic code generation. this anthropic approach could push that concept way further&lt;/p&gt; &lt;p&gt;wondering if anyone has experimented with similar patterns locally&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zestyclose_Ring1123"&gt; /u/Zestyclose_Ring1123 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1powhy6/anthropic_blog_on_code_execution_for_agents_987/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pogwb6</id>
    <title>8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp; Build Details</title>
    <updated>2025-12-16T23:20:20+00:00</updated>
    <author>
      <name>/u/Beautiful_Trust_8151</name>
      <uri>https://old.reddit.com/user/Beautiful_Trust_8151</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"&gt; &lt;img alt="8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp;amp; Build Details" src="https://preview.redd.it/furqdxa18n7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=700d4a8e9197fffc15398e5a63d7abad773cef16" title="8x Radeon 7900 XTX Build for Longer Context Local Inference - Performance Results &amp;amp; Build Details" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a multi 7900XTX GPU setup for local AI inference for work and wanted to share some performance numbers and build details for anyone considering a similar route as I have not seen that many of us out there. The system consists of 8x AMD Radeon 7900 XTX cards providing 192 GB VRAM total, paired with an Intel Core i7-14700F on a Z790 motherboard and 192 GB of system RAM. The system is running Windows 11 with a Vulkan backend through LMStudio and Open WebUI. I got a $500 Aliexpress PCIe Gen4 x16 switch expansion card with 64 additional lanes to connect the GPUs to this consumer grade motherboard. This was an upgrade from a 4x 7900XTX GPU system that I have been using for over a year. The total build cost is around $6-7k&lt;/p&gt; &lt;p&gt;I ran some performance testing with GLM4.5Air q6 (99GB file size) Derestricted at different context utilization levels to see how things scale with the maximum allocated context window of 131072 tokens. With an empty context, I'm getting about 437 tokens per second for prompt processing and 27 tokens per second for generation. When the context fills up to around 19k tokens, prompt processing still maintains over 200 tokens per second, though generation speed drops to about 16 tokens per second. The full performance logs show this behavior is consistent across multiple runs, and more importantly, the system is stable. On average the system consums about 900watts during prompt processing and inferencing. &lt;/p&gt; &lt;p&gt;This approach definitely isn't the cheapest option and it's not the most plug-and-play solution out there either. However, for our work use case, the main advantages are upgradability, customizability, and genuine long-context capability with reasonable performance. If you want the flexibility to iterate on your setup over time and have specific requirements around context length and model selection, a custom multi-GPU rig like this has been working really well for us. I would be happy to answer any questions.&lt;/p&gt; &lt;p&gt;Here some raw log data.&lt;br /&gt; 2025-12-16 14:14:22 [DEBUG]&lt;/p&gt; &lt;p&gt;Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 37.30 ms&lt;br /&gt; common_perf_print: samplers time = 4.80 ms / 1701 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 3577.99 ms / 1564 tokens ( 2.29 ms per token, 437.12 tokens per second)&lt;br /&gt; 2025-12-16 15:05:06 [DEBUG]&lt;br /&gt; common_perf_print: eval time = 301.25 ms / 8 runs ( 37.66 ms per token, 26.56 tokens per second)&lt;br /&gt; common_perf_print: total time = 3919.71 ms / 1572 tokens&lt;br /&gt; common_perf_print: unaccounted time = 3.17 ms / 0.1 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 7&lt;/p&gt; &lt;p&gt; Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 704.49 ms&lt;br /&gt; common_perf_print: samplers time = 546.59 ms / 15028 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 66858.77 ms / 13730 tokens ( 4.87 ms per token, 205.36 tokens per second)&lt;br /&gt; 2025-12-16 14:14:22 [DEBUG]&lt;br /&gt; common_perf_print: eval time = 76550.72 ms / 1297 runs ( 59.02 ms per token, 16.94 tokens per second)&lt;br /&gt; common_perf_print: total time = 144171.13 ms / 15027 tokens&lt;br /&gt; common_perf_print: unaccounted time = 57.15 ms / 0.0 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 1291&lt;/p&gt; &lt;p&gt;Target model llama_perf stats:&lt;br /&gt; common_perf_print: sampling time = 1547.88 ms&lt;br /&gt; common_perf_print: samplers time = 1201.66 ms / 18599 tokens&lt;br /&gt; common_perf_print: load time = 95132.76 ms&lt;br /&gt; common_perf_print: prompt eval time = 77358.07 ms / 15833 tokens ( 4.89 ms per token, 204.67 tokens per second)&lt;br /&gt; common_perf_print: eval time = 171509.89 ms / 2762 runs ( 62.10 ms per token, 16.10 tokens per second)&lt;br /&gt; common_perf_print: total time = 250507.93 ms / 18595 tokens&lt;br /&gt; common_perf_print: unaccounted time = 92.10 ms / 0.0 % (total - sampling - prompt eval - eval) / (total)&lt;br /&gt; common_perf_print: graphs reused = 2750&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beautiful_Trust_8151"&gt; /u/Beautiful_Trust_8151 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/furqdxa18n7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pogwb6/8x_radeon_7900_xtx_build_for_longer_context_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T23:20:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pozr6f</id>
    <title>Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)</title>
    <updated>2025-12-17T15:42:43+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"&gt; &lt;img alt="Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)" src="https://external-preview.redd.it/t4cNt5D638DSOJgsxl8f-7IwJhLpxHIh7HxK5GHcBJE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b72b5025e78c2cc97de15c8fea348f262235ecb" title="Claude Code, GPT-5.2, DeepSeek v3.2, and Self-Hosted Devstral 2 on Fresh SWE-rebench (November 2025)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I‚Äôm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with our &lt;strong&gt;November runs&lt;/strong&gt; on &lt;strong&gt;47 fresh GitHub PR tasks&lt;/strong&gt; (PRs created in the previous month only). It‚Äôs a SWE-bench‚Äìstyle setup: models read real PR issues, run tests, edit code, and must make the suite pass. &lt;/p&gt; &lt;p&gt;This update includes a particularly large wave of new releases, so we‚Äôve added a substantial batch of new models to the leaderboard:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Devstral 2&lt;/strong&gt; ‚Äî a strong release of models that can be run locally given their size&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek v3.2&lt;/strong&gt; ‚Äî a new state-of-the-art open-weight model&lt;/li&gt; &lt;li&gt;A &lt;strong&gt;new comparison mode&lt;/strong&gt; to benchmark models against external systems such as &lt;strong&gt;Claude Code&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also introduced a &lt;strong&gt;cached-tokens statistic&lt;/strong&gt; to improve transparency around cache usage.&lt;/p&gt; &lt;p&gt;Looking forward to your thoughts and suggestions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=nov_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pozr6f/claude_code_gpt52_deepseek_v32_and_selfhosted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T15:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pox733</id>
    <title>LangChain and LlamaIndex are in "steep decline" according to new ecosystem report. Anyone else quietly ditching agent frameworks?</title>
    <updated>2025-12-17T13:59:02+00:00</updated>
    <author>
      <name>/u/Exact-Literature-395</name>
      <uri>https://old.reddit.com/user/Exact-Literature-395</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I stumbled on this LLM Development Landscape 2.0 report from Ant Open Source and it basically confirmed what I've been feeling for months.&lt;/p&gt; &lt;p&gt;LangChain, LlamaIndex and AutoGen are all listed as &amp;quot;steepest declining&amp;quot; projects by community activity over the past 6 months. The report says it's due to &amp;quot;reduced community investment from once dominant projects.&amp;quot; Meanwhile stuff like vLLM and SGLang keeps growing.&lt;/p&gt; &lt;p&gt;Honestly this tracks with my experience. I spent way too long fighting with LangChain abstractions last year before I just ripped it out and called the APIs directly. Cut my codebase in half and debugging became actually possible. Every time I see a tutorial using LangChain now I just skip it.&lt;/p&gt; &lt;p&gt;But I'm curious if this is just me being lazy or if there's a real shift happening. Are agent frameworks solving a problem that doesn't really exist anymore now that the base models are good enough? Or am I missing something and these tools are still essential for complex workflows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exact-Literature-395"&gt; /u/Exact-Literature-395 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pox733/langchain_and_llamaindex_are_in_steep_decline/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:59:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pow797</id>
    <title>Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter</title>
    <updated>2025-12-17T13:14:02+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt; &lt;img alt="Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter" src="https://a.thumbs.redditmedia.com/-V8KieEduFhCtfHKuiRcs_94wVQuIC9TTbPuo7vOPY8.jpg" title="Peak LLM Wars: Xiaomi Blocks Kimi Employees on Twitter" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/kujwpbsakr7g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b5a113e06d0e8db66436dc632a8828a85bb8d16e"&gt;https://preview.redd.it/kujwpbsakr7g1.jpg?width=1194&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b5a113e06d0e8db66436dc632a8828a85bb8d16e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jlban9qkr7g1.jpg?width=789&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7984f0c584b0b67cc49f6b24d3ae920d42e3ccc0"&gt;https://preview.redd.it/8jlban9qkr7g1.jpg?width=789&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=7984f0c584b0b67cc49f6b24d3ae920d42e3ccc0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLM wars are wild&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pow797/peak_llm_wars_xiaomi_blocks_kimi_employees_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T13:14:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1poy0lb</id>
    <title>Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds.</title>
    <updated>2025-12-17T14:33:13+00:00</updated>
    <author>
      <name>/u/themixtergames</name>
      <uri>https://old.reddit.com/user/themixtergames</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt; &lt;img alt="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." src="https://external-preview.redd.it/YWpkODI1NDF4cjdnMbxNGAI-puPRf-AP3cgrLxlreCeM4kV742La4OIIHHvj.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a1dded6d1cdcc956e0916d9926400982637f4d7c" title="Apple introduces SHARP, a model that generates a photorealistic 3D Gaussian representation from a single image in seconds." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/apple/ml-sharp"&gt;https://github.com/apple/ml-sharp&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2512.10685"&gt;https://arxiv.org/abs/2512.10685&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/themixtergames"&gt; /u/themixtergames &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/l2mp7b31xr7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1poy0lb/apple_introduces_sharp_a_model_that_generates_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T14:33:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1porpwd</id>
    <title>Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model</title>
    <updated>2025-12-17T08:49:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt; &lt;img alt="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" src="https://external-preview.redd.it/OXpuN3VqYnE4cTdnMbhg7mfH3BLNBAJzBcqwf-BeiskbYrfqW4XgiIx-FQh0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6475aef4e90b21644bf95a26618a75433c2e08de" title="Microsoft's TRELLIS 2-4B, An Open-Source Image-to-3D Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Model Details&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Type:&lt;/strong&gt; Flow-Matching Transformers with Sparse Voxel based 3D VAE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parameters:&lt;/strong&gt; 4 Billion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Input:&lt;/strong&gt; Single Image&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output:&lt;/strong&gt; 3D Asset &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/microsoft/TRELLIS.2-4B"&gt;https://huggingface.co/microsoft/TRELLIS.2-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo - &lt;a href="https://huggingface.co/spaces/microsoft/TRELLIS.2"&gt;https://huggingface.co/spaces/microsoft/TRELLIS.2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Blog post - &lt;a href="https://microsoft.github.io/TRELLIS.2/"&gt;https://microsoft.github.io/TRELLIS.2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/g8uco5dq8q7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1porpwd/microsofts_trellis_24b_an_opensource_imageto3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T08:49:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Rohun Tripathi ( &lt;a href="/u/darkerWind"&gt;u/darkerWind&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thank you everyone for the kind words and great questions! This AMA has ended as of 2pm PST (5pm EST) on Dec. 16.&lt;/p&gt; &lt;p&gt;&lt;a href="https://discord.gg/6vWDHyTCQV"&gt;Join Ai2 on Discord&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
