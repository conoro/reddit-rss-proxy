<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-25T19:23:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qm6iho</id>
    <title>Stable-DiffCoder, a strong code diffusion LLM built on Seed-Coder</title>
    <updated>2026-01-25T02:22:05+00:00</updated>
    <author>
      <name>/u/rektide</name>
      <uri>https://old.reddit.com/user/rektide</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rektide"&gt; /u/rektide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bytedance-seed.github.io/Stable-DiffCoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T02:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm0l2q</id>
    <title>I built a tool that learns your codebase's unwritten rules and conventions- no AI, just AST parsing</title>
    <updated>2026-01-24T22:11:05+00:00</updated>
    <author>
      <name>/u/Fluffy_Citron3547</name>
      <uri>https://old.reddit.com/user/Fluffy_Citron3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last six months teaching myself to orchestrate engineering codebases using AI agents. What I found is that the biggest bottleneck isn‚Äôt intelligence it‚Äôs the context window. Why have we not given agents the proper tooling to defeat this limitation? Agents constantly forget how I handle error structures or which specific components I use for the frontend. This forces mass auditing and refactoring, causing me to spend about 75% of my token budget on auditing versus writing.&lt;/p&gt; &lt;p&gt;That is why I built Drift. Drift is a first-in-class codebase intelligence tool that leverages semantic learning through AST parsing with Regex fallbacks. It scans your codebase and extracts 15 different categories with over 150 patterns. Everything is persisted and recallable via CLI or MCP in your IDE of choice.&lt;/p&gt; &lt;p&gt;What makes drift different?&lt;/p&gt; &lt;p&gt;It‚Äôs learning based not rule based. AI is capable of writing high quality code but the context limitation makes fitting conventions through a large code base extremely tedious and time consuming often leading to things silently failing or just straight up not working. &lt;/p&gt; &lt;p&gt;Drift_context is the real magic &lt;/p&gt; &lt;p&gt;Instead of an agent calling 10 tools and sytheneszing results it: &lt;/p&gt; &lt;p&gt;Takes intent &lt;/p&gt; &lt;p&gt;Takes focus area&lt;/p&gt; &lt;p&gt;Returned a curated package&lt;/p&gt; &lt;p&gt;This eliminates the audit loop, hallucination risk and gives the agent everything needed in one call.&lt;/p&gt; &lt;p&gt;Call graph analysis across 6 different languages&lt;/p&gt; &lt;p&gt;Not just ‚ÄúWhat functions exists‚Äù but..&lt;/p&gt; &lt;p&gt;Drift_reachability_forward &amp;gt; What data can this code access? (Massive for helping with security)&lt;/p&gt; &lt;p&gt;Drift_reachability_inverse &amp;gt; Who can access this field? &lt;/p&gt; &lt;p&gt;Drift_impact_analysis &amp;gt; what breaks if I change this with scoring.&lt;/p&gt; &lt;p&gt;Security-audit-grade analysis available to you or your agent through MCP or CLI&lt;/p&gt; &lt;p&gt;The MCP has been built out with frontier capabilities ensuring context is preserved and is a true tool for your agents&lt;/p&gt; &lt;p&gt;Currently support TS, PY, Java, C#, PHP, GO :&lt;/p&gt; &lt;p&gt;with‚Ä¶&lt;/p&gt; &lt;p&gt;Tree sitter parsing&lt;/p&gt; &lt;p&gt;Regex fallback&lt;/p&gt; &lt;p&gt;Framework aware detection&lt;/p&gt; &lt;p&gt;All data persist into a local file (/.drift) and you have the ability to approve, deny and ignore certain components, functions and features you don‚Äôt want the agent to be trained on.&lt;/p&gt; &lt;p&gt;check it out here: &lt;/p&gt; &lt;p&gt;IF you run into any edge cases or I don‚Äôt support the framework your code base is currently running on open a git issue feature request and ive been banging them out quick&lt;/p&gt; &lt;p&gt;Thank you for all the upvotes and stars on the project it means so much!&lt;/p&gt; &lt;p&gt;check it out here: &lt;a href="https://github.com/dadbodgeoff/drift"&gt;https://github.com/dadbodgeoff/drift&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Citron3547"&gt; /u/Fluffy_Citron3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T22:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltwza</id>
    <title>Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence.</title>
    <updated>2026-01-24T18:00:50+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt; &lt;img alt="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." src="https://preview.redd.it/66fd18ro6cfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f579cce389f709dbf297867095118be2027f04ea" title="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/2014786516153991339"&gt;https://x.com/ArtificialAnlys/status/2014786516153991339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A key driver of this momentum is the Korean National Sovereign AI Initiative, a government-backed, nationwide competition that incentivizes domestic model development through a multi-stage elimination process. The initiative shortlists national champions, with winners receiving direct government funding and guaranteed access to large-scale GPU capacity.&lt;/p&gt; &lt;p&gt;‚û§ In August 2025, five organizations were selected: Naver, SK Telecom, LG Group, Upstage, and NC AI&lt;/p&gt; &lt;p&gt;‚û§ In the most recent round announced last week, the field narrowed to three: LG, SK Telecom, and Upstage.&lt;/p&gt; &lt;p&gt;‚û§ A fourth finalist is expected to be selected in the coming months as the evaluation process continues&lt;/p&gt; &lt;p&gt;Generally, top Korean AI models tend to be open weights, and vary in size ranging from Motif‚Äòs 12.7B Thinking model to LG‚Äôs 236B K-EXAONE. Other models, such as Korea Telecom (KT)‚Äôs Mi:dm K 2.5 Pro, are proprietary and developed with a focus on business integration with existing KT clients.&lt;/p&gt; &lt;p&gt;Overview of major releases:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ LG | K-EXAONE -&lt;/strong&gt; The current leader in the Korean AI race and a shortlisted model in the Korean National Sovereign AI Initiative. K-EXAONE is a 236B open weights model and scores 32 on the Artificial Analysis Intelligence Index. K-EXAONE performs strongly across various intelligence evaluations from scientific reasoning, instruction following, to agentic coding. However, this model has high verbosity, using 100 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Upstage | Solar Open -&lt;/strong&gt; Another shortlisted model in the Korean National Sovereign AI Initiative. Solar Open is a 100B open-weights model and scores 21 on the Artificial Analysis Intelligence Index. Solar Open performs well in instruction following and has lower hallucination rate compared to peer Korean models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Naver | HyperCLOVA X SEED Think -&lt;/strong&gt; A 32B open weights reasoning model that scores 24 on the Artificial Analysis Intelligence Index. HyperCLOVA X SEED Think demonstrates strong performance on agentic tool-use workflows and scores highly in the Global MMLU Lite multilingual index for Korean, highlighting its potential usefulness in a primarily Korean language environment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Korea Telecom | Mi:dm K 2.5 Pro -&lt;/strong&gt; A proprietary reasoning model that scores 23 on the Artificial Analysis Intelligence Index. Mi:dm K 2.5 Pro sees strong performance in agentic tool-use. Mi:dm K 2.5 Pro currently has no publicly available endpoint. Instead, Korea Telecom primarily intends to package this model into product offerings and use this model to serve KT‚Äôs clients&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Motif | Motif-2-12.7B -&lt;/strong&gt; A small open weights model that scores 24 on the Artificial Analysis Intelligence Index. Motif-2-12.7B performs well in long-context reasoning and knowledge, but is highly token intensive - using 120 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66fd18ro6cfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmr570</id>
    <title>Organizing LM Studio ?</title>
    <updated>2026-01-25T18:31:48+00:00</updated>
    <author>
      <name>/u/Ztoxed</name>
      <uri>https://old.reddit.com/user/Ztoxed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Newbie question, did a search didn't see and answer.&lt;br /&gt; There are hundreds of Model choices. As I test ones here and there to learn.&lt;br /&gt; I find I am having a hard tine going back to use a previous Model.&lt;br /&gt; Is there a way to organize the models I use? I see you can open a chat and save the chat.&lt;br /&gt; But that seems clunky given the many models. Wondering if there is a good was to sort and organize types etc.&lt;/p&gt; &lt;p&gt;Thank You.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ztoxed"&gt; /u/Ztoxed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmr570/organizing_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmr570/organizing_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmr570/organizing_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:31:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmap5e</id>
    <title>Best &lt;4B dense models today?</title>
    <updated>2026-01-25T05:46:01+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think small(&amp;lt;4B) dense models are basically the only practical option for general users. But hasn't there been almost no progress since Gemma 3 4B came out? Are there any alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T05:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmo3do</id>
    <title>LM Studio - Why does my system RAM fill up and go OOM if the model says Full GPU Offload Possible?</title>
    <updated>2026-01-25T16:42:16+00:00</updated>
    <author>
      <name>/u/Nytse</name>
      <uri>https://old.reddit.com/user/Nytse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Windows, RTX 3090 (24 GB VRAM) upgraded recently from a GTX 1080 (8 GB), 32 GB RAM&lt;/p&gt; &lt;p&gt;With Firefox open with many tabs I use ~18 GB RAM. GPU stays at ~3 GB.&lt;/p&gt; &lt;p&gt;Then, in LM Studio, loading the OpenAI GPT‚ÄëOSS 20B model shows ‚ÄúFull GPU Offload Possible‚Äù. After load, VRAM jumps to ~14 GB and system RAM climbs to 32 GB, then the program crashes with OOM.&lt;/p&gt; &lt;p&gt;I have Strict Guardrails enabled, swap is on.&lt;/p&gt; &lt;p&gt;How can I avoid high RAM usage and the OOM when loading this model while using by browser? How do I know how much allocated RAM the model will have?&lt;/p&gt; &lt;p&gt;I thought that the gguf file size is similar to the VRAM allocation and only like 1 GB RAM is reserved if the model fits in the GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nytse"&gt; /u/Nytse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T16:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmbevn</id>
    <title>Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?</title>
    <updated>2026-01-25T06:22:22+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"&gt; &lt;img alt="Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?" src="https://preview.redd.it/r0ec0m21vffg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=275f52d7dfb944441f74dea25d60f8c3e620a766" title="Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on a synthetic data pipeline for training high-precision image-to-image models (Flux Klein and Qwen Image Edit). I have reached a point where standard tagging and current open-weights VL models are the main bottleneck for data quality.&lt;/p&gt; &lt;p&gt;I have benchmarked almost every trending VL model on HuggingFace and those leading the MMMU-Pro leaderboard. My conclusion is that even the best open models are &amp;quot;blind&amp;quot; to complex anatomical layering and spatial reasoning.&lt;/p&gt; &lt;p&gt;The problem is best described by the &amp;quot;Horns Issue&amp;quot; (see attached image). If a character has large organic dragon horns and a headband with small decorative horns, every open VLM I tested merges them into one generic attribute. They fail to distinguish between base anatomy and removable accessories. Gemini 3 Flash, however, is on a completely different level‚Äîit accurately describes every layer and understands the distinction perfectly.&lt;/p&gt; &lt;p&gt;My plan is to fine-tune Qwen 3 VL 32B Instruct on a dataset labeled by Gemini 3 Flash. I want to transfer that visual reasoning so I can have a local engine for high-scale synthetic captioning.&lt;/p&gt; &lt;p&gt;A few technical questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can Qwen 3 VL actually absorb this level of reasoning via SFT if it lacks the native &amp;quot;thinking&amp;quot; or CoT process Gemini uses?&lt;/li&gt; &lt;li&gt;Is the &amp;quot;blindness&amp;quot; in open models a limitation of the vision encoder itself, or is it purely a reasoning capability issue on the LLM side?&lt;/li&gt; &lt;li&gt;Has anyone here tried this kind of VLM-to-VLM distillation for high-scale labeling in generative AI pipelines?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am trying to build a local captioner that matches proprietary accuracy. Any insights on the plasticity of Qwen 32B for this specific task would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r0ec0m21vffg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T06:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmev6q</id>
    <title>[R] Open-sourcing an unfinished research project: A Self-Organizing, Graph-Based Alternative to Transformers (Looking for feedback or continuation</title>
    <updated>2026-01-25T09:40:12+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sharing a research project I worked on over a long period but had to pause due to personal reasons. Rather than letting it sit idle, I wanted to open it up to the community either for technical feedback, critique, or for anyone interested in continuing or experimenting with it.&lt;/p&gt; &lt;p&gt;The main project is called Self-Organizing State Model (SOSM): &lt;a href="https://github.com/PlanetDestroyyer/Self-Organizing-State-Model"&gt;https://github.com/PlanetDestroyyer/Self-Organizing-State-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At a high level, the goal was to explore an alternative to standard Transformer attention by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Using graph-based routing instead of dense attention&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Separating semantic representation and temporal pattern learning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Introducing a hierarchical credit/attribution mechanism for better interpretability&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The core system is modular and depends on a few supporting components: Semantic representation module (MU) &lt;a href="https://github.com/PlanetDestroyyer/MU"&gt;https://github.com/PlanetDestroyyer/MU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Temporal pattern learner (TEMPORAL) &lt;a href="https://github.com/PlanetDestroyyer/TEMPORAL"&gt;https://github.com/PlanetDestroyyer/TEMPORAL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hierarchical / K-1 self-learning mechanism &lt;a href="https://github.com/PlanetDestroyyer/self-learning-k-1"&gt;https://github.com/PlanetDestroyyer/self-learning-k-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm honestly not sure how valuable or novel this work is that‚Äôs exactly why I‚Äôm posting it here. If nothing else, I‚Äôd really appreciate constructive criticism, architectural feedback, or pointers to related work that overlaps with these ideas. If someone finds parts of it useful (or wants to take it further, refactor it, or formalize it into a paper), they‚Äôre more than welcome to do so. The project is open-source, and I‚Äôm happy to answer questions or clarify intent where needed.&lt;/p&gt; &lt;p&gt;Thanks for taking a look.&lt;/p&gt; &lt;p&gt;Summary:&lt;/p&gt; &lt;p&gt;This work explores a language model architecture based on structured semantics rather than unstructured embeddings. Instead of positional encodings, a temporal learning module is used to model sequence progression and context flow. A K-1 hierarchical system is introduced to provide interpretability, enabling analysis of how a token is predicted and which components, states, or nodes contribute to that prediction. Most importantly, rather than comparing every token with all others (as in full self-attention), the model uses a graph-based connection mechanism that restricts computation to only the most relevant or necessary tokens, enabling selective reasoning and improved efficiency.&lt;/p&gt; &lt;p&gt;(Have used claude code to code )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlyhn</id>
    <title>GLM-4.7-flash on RTX 6000 pro</title>
    <updated>2026-01-25T15:24:37+00:00</updated>
    <author>
      <name>/u/gittb</name>
      <uri>https://old.reddit.com/user/gittb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I‚Äôm getting horrible throughput considering the models size with vLLM.&lt;/p&gt; &lt;p&gt;Currently with 2x cards and DP 2 @ FP16 I‚Äôm getting around 370 gen TPS with 10x requests.&lt;/p&gt; &lt;p&gt;Anyone have a fix or a ‚Äúworking‚Äù config for 1 or two cards?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gittb"&gt; /u/gittb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmqug5</id>
    <title>Qwen3 vl 8b instruct samplers</title>
    <updated>2026-01-25T18:21:17+00:00</updated>
    <author>
      <name>/u/Aril_1</name>
      <uri>https://old.reddit.com/user/Aril_1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I'm trying to use Qwen vl instruct with koboldcpp using the samplers suggested in the qwen repo and by Unsloth:&lt;/p&gt; &lt;p&gt;temp= 0.7&lt;/p&gt; &lt;p&gt;top_p=0.8&lt;/p&gt; &lt;p&gt;top_k= 20&lt;/p&gt; &lt;p&gt;presence_penalty=1.5&lt;/p&gt; &lt;p&gt;The problem is that for any kind of use, from general assistant, to coding, or for agentic tool calling use, it has fairly poor performance, often even using incorrect json syntax.&lt;/p&gt; &lt;p&gt;Should I change something?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aril_1"&gt; /u/Aril_1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmqug5/qwen3_vl_8b_instruct_samplers/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmqug5/qwen3_vl_8b_instruct_samplers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmqug5/qwen3_vl_8b_instruct_samplers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:21:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmmpwz</id>
    <title>DGX spark performance falls short</title>
    <updated>2026-01-25T15:53:00+00:00</updated>
    <author>
      <name>/u/dereksodo</name>
      <uri>https://old.reddit.com/user/dereksodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;using cutlass-profiler, gemm, here is the performance:&lt;/p&gt; &lt;p&gt;peak int4: 157 TFLOP&lt;/p&gt; &lt;p&gt;peak int8: 200 TFLOP&lt;/p&gt; &lt;p&gt;peak fp16: 97 TFLOP&lt;/p&gt; &lt;p&gt;anyone knows why performance of int4 is not around 350-450( which i expect)?&lt;/p&gt; &lt;p&gt;env: docker (pytorch:25.12-py3)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dereksodo"&gt; /u/dereksodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjyxl</id>
    <title>Understanding Multi-Head Latent Attention (MLA)</title>
    <updated>2026-01-25T14:05:43+00:00</updated>
    <author>
      <name>/u/shreyansh26</name>
      <uri>https://old.reddit.com/user/shreyansh26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A short deep-dive on Multi-Head Latent Attention (MLA) (from DeepSeek): intuition + math, then a walk from MHA ‚Üí GQA ‚Üí MQA ‚Üí MLA, with PyTorch code and the fusion/absorption optimizations for KV-cache efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"&gt;http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shreyansh26"&gt; /u/shreyansh26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmevh7</id>
    <title>Blazing fast JSON extraction with very small LLMs-3B: LSTM to LLM</title>
    <updated>2026-01-25T09:40:42+00:00</updated>
    <author>
      <name>/u/memphet</name>
      <uri>https://old.reddit.com/user/memphet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've learned a lot from this sub, so I wanted to give back by sharing my experience on a recent project.&lt;/p&gt; &lt;p&gt;My goal was to migrate a text extraction pipeline from LSTM to an LLM. The task involves extracting specific data into JSON format from small text inputs (‚âà1024 tokens). I used in-house data to fine-tune it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraints &amp;amp; Achievements (running on an L4 GPU):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Very low end2end latency:&lt;/strong&gt; &amp;lt;500ms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High throughput:&lt;/strong&gt; ‚âà30 RPM (requests per minute)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; 0.99 accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Model:&lt;/strong&gt;&lt;br /&gt; I tested quite a few models for this task.&lt;br /&gt; Ultimately, HuggingFaceTB/SmolLM3-3B was the best fit for our needs.&lt;br /&gt; I also had very strong results with Qwen/Qwen3-4B-Instruct and Ministral&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning parameters matter less than I thought:&lt;/strong&gt; I didn't see huge gains from strictly tweaking hyperparameters. I ran extensive hyperparameter optimization only to find that simply increasing the number of epochs yielded the best (slight) improvements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data is king:&lt;/strong&gt; Poor labeling logic and bad data quality hurt me the most. If I had to redo it, I would spend much more time cleaning and validating the dataset upfront.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small LLMs struggle with Proper Nouns:&lt;/strong&gt; I noticed about a 10% error rate on names! A significant performance boost came from adding a simple post-processing step using Levenshtein distance to correct names extracted by the LLM against the input text (correcting &amp;quot;Jammes&amp;quot; -&amp;gt; &amp;quot;James&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency Gains:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Obviously the best bang for your buck. I recommend &lt;strong&gt;FP8&lt;/strong&gt; using llm-compressor if you have a Lovelace GPU or newer. Otherwise, &lt;strong&gt;AWQ&lt;/strong&gt; is solid. &lt;ul&gt; &lt;li&gt;Gain: ~50% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Formatting:&lt;/strong&gt; You want to generate as few tokens as possible. Instead of fine-tuning for a verbose JSON output like {&amp;quot;key1&amp;quot;: &amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot;: &amp;quot;value2&amp;quot;}, I fine-tuned the model to output just the values: value1,value2. &lt;ul&gt; &lt;li&gt;Gain: ~30% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't work (for me):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I really tried to make &lt;strong&gt;Speculative Decoding&lt;/strong&gt; work with vLLM. In theory, I expected gains even with just n-gram speculative decoding, but I didn't observe any improvement. I did see some speedup using Qwen 0.7B draft model, but since I ultimately chose a different base model architecture, I couldn't use them effectively. Plus, maintaining a base model + a draft model is a pain, which is also why I didn't go with Eagle.&lt;/p&gt; &lt;p&gt;If you have suggestions to squeeze out more performance or thoughts on the setup, I'm all ears!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/memphet"&gt; /u/memphet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmrrr4</id>
    <title>ClaraVerse | Local AI workspace (4 months ago) -&gt; Your feedback -&gt; Back with improvements.</title>
    <updated>2026-01-25T18:54:07+00:00</updated>
    <author>
      <name>/u/BadBoy17Ge</name>
      <uri>https://old.reddit.com/user/BadBoy17Ge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"&gt; &lt;img alt="ClaraVerse | Local AI workspace (4 months ago) -&amp;gt; Your feedback -&amp;gt; Back with improvements." src="https://a.thumbs.redditmedia.com/7F-eZ7FXWPk0GBmRwW4IWCGKcKvAemtDakgsLWI_f-8.jpg" title="ClaraVerse | Local AI workspace (4 months ago) -&amp;gt; Your feedback -&amp;gt; Back with improvements." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;We built an AI workspace that actually gets things done locally (not just another chatbot or AI slope)&lt;/h1&gt; &lt;p&gt;I've been grinding on ClaraVerse for the past few months, and we just dropped a major update. If you're tired of AI tools that just... talk at you, this might be your vibe.&lt;/p&gt; &lt;h1&gt;The TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Run it anywhere&lt;/strong&gt;: CLI tool that works on your laptop, VPS, cloud, whatever. No platform lock-in BS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;50+ integrations&lt;/strong&gt;: Gmail, Sheets, Discord, Slack, you name it. Want more? Just ask.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Actual automation&lt;/strong&gt;: Build agents that DO things, not just answer questions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat-first workflow builder&lt;/strong&gt;: Like n8n/Zapier but for AI. Chat your way through creating workflows ask, create, iterate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Everything becomes an API&lt;/strong&gt;: Seriously, every workflow you build = instant API endpoint or schedule it daily, hourly your choice.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;One-liner:&lt;/strong&gt; It's an all-in-one platform (chat, image gen, agents, docs, search). Every tool is part of the package.&lt;/p&gt; &lt;p&gt;What's actually new (beyond UI polish)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Built-in tools that agents and chats need:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PPT, PDF, XLSX readers and creators&lt;/li&gt; &lt;li&gt;Isolated code execution with dependency management&lt;/li&gt; &lt;li&gt;Interactive chat so local LLMs can ask clarifying questions mid-prompt&lt;/li&gt; &lt;li&gt;Search, scrape, image search, API tools, and memory all default&lt;/li&gt; &lt;li&gt;Tool router if you have too many tools&lt;/li&gt; &lt;li&gt;Memories that can remember and forget based on your usage&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;50+ integrations ready to go:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gmail, Sheets, Discord, Slack, and more&lt;/li&gt; &lt;li&gt;Build agents that trigger actual actions, not just suggestions&lt;/li&gt; &lt;li&gt;Schedule workflows and forget about them&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For n8n lovers who hate boilerplate:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Auto-generate workflows from prompts&lt;/li&gt; &lt;li&gt;Chain multiple AI models together&lt;/li&gt; &lt;li&gt;Structured outputs, multi-tool agents, the works&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Better chat UX:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Interactive prompts that ask clarifying questions&lt;/li&gt; &lt;li&gt;Generate images, PDFs, slides, charts in-chat&lt;/li&gt; &lt;li&gt;All integrations work in both chat AND workflows&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Admin and Model Manger:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Manage models and provider in one place&lt;/li&gt; &lt;li&gt;Assign models based on their abilities (tools, text, code, vision, image)&lt;/li&gt; &lt;li&gt;Create alias, check usage and so on with multiple user in same instance&lt;/li&gt; &lt;li&gt;Simple UI works on phone responsive as hell&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it and let us know&lt;/h1&gt; &lt;ul&gt; &lt;li&gt; GitHub: &lt;a href="https://github.com/claraverse-space/ClaraVerse"&gt;github.com/claraverse-space/ClaraVerse&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We're open source and privacy-first (chat and data stored in browser or DB, even when self-hosted - user's choice). &lt;/p&gt; &lt;p&gt;I use this myself every day. Honestly, I've seen worse tools raise fund and then lock everything behind subscriptions. This community helped build this with feedback, so it's staying free and open-source.&lt;/p&gt; &lt;p&gt;Happy to answer questions, take feature requests, or hear about how it crashes on your machine so we can fix and improve. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BadBoy17Ge"&gt; /u/BadBoy17Ge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmrrr4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrrr4/claraverse_local_ai_workspace_4_months_ago_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:54:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmh3si</id>
    <title>What happened to moondream3?</title>
    <updated>2026-01-25T11:49:01+00:00</updated>
    <author>
      <name>/u/StableDiffer</name>
      <uri>https://old.reddit.com/user/StableDiffer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So last year the moondream 3 preview came out. It was a nice performing visual model that could do some cool stuff other VL models couldn't. One month ago a MLX version appeared &lt;a href="https://huggingface.co/moondream/md3p-int4"&gt;https://huggingface.co/moondream/md3p-int4&lt;/a&gt; but until now there is no llama.cpp implementation and no public activity I could find.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableDiffer"&gt; /u/StableDiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T11:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmhvuz</id>
    <title>Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.</title>
    <updated>2026-01-25T12:30:24+00:00</updated>
    <author>
      <name>/u/Charming_Group_2950</name>
      <uri>https://old.reddit.com/user/Charming_Group_2950</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; You build a RAG system. It gives an answer. It sounds right.&lt;br /&gt; But is it actually grounded in your data, or just hallucinating with confidence?&lt;br /&gt; A single &amp;quot;correctness&amp;quot; or &amp;quot;relevance&amp;quot; score doesn‚Äôt cut it anymore, especially in enterprise, regulated, or governance-heavy environments. We need to know why it failed. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My solution:&lt;/strong&gt;&lt;br /&gt; Introducing &lt;strong&gt;TrustifAI&lt;/strong&gt; ‚Äì a framework designed to quantify, explain, and debug the trustworthiness of AI responses. &lt;/p&gt; &lt;p&gt;Instead of pass/fail, it computes a multi-dimensional Trust Score using signals like:&lt;br /&gt; * Evidence Coverage: Is the answer actually supported by retrieved documents?&lt;br /&gt; * Epistemic Consistency: Does the model stay stable across repeated generations?&lt;br /&gt; * Semantic Drift: Did the response drift away from the given context?&lt;br /&gt; * Source Diversity: Is the answer overly dependent on a single document?&lt;br /&gt; * Generation Confidence: Uses token-level log probabilities at inference time to quantify how confident the model was while generating the answer (not after judging it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; TrustifAI doesn‚Äôt just give you a number - it gives you traceability.&lt;br /&gt; It builds &lt;strong&gt;Reasoning Graphs (DAGs)&lt;/strong&gt; and &lt;strong&gt;Mermaid visualizations&lt;/strong&gt; that show why a response was flagged as reliable or suspicious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How is this different from LLM Evaluation frameworks:&lt;/strong&gt;&lt;br /&gt; All popular Eval frameworks measure how good your RAG system is, but&lt;br /&gt; TrustifAI tells you why you should (or shouldn‚Äôt) trust a specific answer - with explainability in mind.&lt;/p&gt; &lt;p&gt;Since the library is in its early stages, I‚Äôd genuinely love community feedback.&lt;br /&gt; ‚≠ê the repo if it helps üòÑ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;code&gt;pip install trustifai&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github link:&lt;/strong&gt; &lt;a href="https://github.com/Aaryanverma/trustifai"&gt;https://github.com/Aaryanverma/trustifai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Group_2950"&gt; /u/Charming_Group_2950 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmhvuz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T12:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and it‚Äôs a significant step forward for local speech synthesis. If you‚Äôve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAI‚Äôs TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;We‚Äôve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAI‚Äôs TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ü§ó Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;üìÑ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;üíª Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmp02r</id>
    <title>Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework.</title>
    <updated>2026-01-25T17:15:06+00:00</updated>
    <author>
      <name>/u/switchdoor1</name>
      <uri>https://old.reddit.com/user/switchdoor1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"&gt; &lt;img alt="Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework." src="https://preview.redd.it/9jgcnzd03jfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b78448eca7d34ac66b090e11ac5facec8f13ce8" title="Built a 100% local AI agentic workflow that automatically tests chatbots using GPT-OSS 20B via llama.cpp + Agno workflow framework." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! Ultimate goal with this project was to build an agentic testing framework that can automatically stress-test chatbots across multiple dimensions - off-topic handling, safety concerns, hallucination detection, system prompt extraction attempts, and more. The system uses AI agents to generate diverse test personalities and scenarios, then runs them against your chatbot and evaluates the responses. Set it up and you can start stacking up test data for continuous improvement.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/Prajwal-Nagaraj/Chatbot-Simulation-Workflow"&gt;https://github.com/Prajwal-Nagaraj/Chatbot-Simulation-Workflow&lt;/a&gt;&lt;/p&gt; &lt;h4&gt;Stack:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LLM&lt;/strong&gt;: GPT-OSS 20B running via llama.cpp server (local, no API keys needed)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow Engine&lt;/strong&gt;: Agno framework for orchestrating multi-agent workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backend&lt;/strong&gt;: FastAPI with async support for long-running test suites&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frontend&lt;/strong&gt;: Modern but basic web ui using js and html&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database&lt;/strong&gt;: SQLite&lt;/li&gt; &lt;/ul&gt; &lt;h4&gt;Features:&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Testing&lt;/strong&gt;: LLM generates realistic user personalities and test scenarios and also communicates with the chatbot endpoint&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-as-a-Judge Evaluation&lt;/strong&gt;: Automated scoring of chatbot responses using LLM as a judge&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Test Types&lt;/strong&gt;: off topic, safety, hallucination, system prompt testing, financial advice&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flexible Configuration&lt;/strong&gt;: CLI, YAML configs, or web UI&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Async Execution&lt;/strong&gt;: Long test suites run in background&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Database Persistence&lt;/strong&gt;: All test sessions, personalities, scenarios, and results stored in a sqlite binary The workflow is pretty wild - it generates personalities, creates scenarios for each, runs conversations, and uses an LLM judge to evaluate everything automatically. You just point it at your Openai compatible chatbot endpoint and let it rip.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/switchdoor1"&gt; /u/switchdoor1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9jgcnzd03jfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmp02r/built_a_100_local_ai_agentic_workflow_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T17:15:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmrjlh</id>
    <title>Do you power off your LLM/AI/SV PC when not using it to save on electricity, or keep it on 24/7? MultiGPU adds a lot of power!</title>
    <updated>2026-01-25T18:45:59+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there guys, hoping you're fine.&lt;/p&gt; &lt;p&gt;Wondering here, as electricity is about 0.28USD per kWh on Chile, so I'm kinda forced to have it off most of the time.&lt;/p&gt; &lt;p&gt;My idle power is about 270W with multiple GPUs (7) and no PCIe switches (5090x3,4090x2,A40x1,A6000x1, 9900X), but with a Gen 5 100 lanes switch and a Gen 4 96 lanes switch, I idle at about 370W.&lt;/p&gt; &lt;p&gt;At load it goes it ranges from 900W to 2500W, depending of the backend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmrjlh/do_you_power_off_your_llmaisv_pc_when_not_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T18:45:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmdf2a</id>
    <title>Has anyone got GLM 4.7 flash to not be shit?</title>
    <updated>2026-01-25T08:14:08+00:00</updated>
    <author>
      <name>/u/synth_mania</name>
      <uri>https://old.reddit.com/user/synth_mania</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Real talk. I feel like everyday I'm downloading a new quant and trying it out and not once have I got it to consistently work without looping.&lt;/p&gt; &lt;p&gt;I've tried with and without the suggested settings from unsloth, &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;, and others, to no avail.&lt;/p&gt; &lt;p&gt;Additionally, this has to be the slowest inference I've ever seen from a 30B A3B model. In all fairness, my only point of reference is Qwen3 Coder, but compared to that at least, the token generation speed feels positively lethargic.&lt;/p&gt; &lt;p&gt;If anybody has any tips, please let me know because I feel like I'm going in circles here. I don't think I've ever seen a modern release that had this many issues right off the bat, with no apparent improvement after a few supposed fixes.&lt;/p&gt; &lt;p&gt;It's really unfortunate because I can see the potential this model has. The chain of thought in particular seems uniquely coherent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synth_mania"&gt; /u/synth_mania &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T08:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmljeb</id>
    <title>What are the best open source coding ideas you can share?</title>
    <updated>2026-01-25T15:08:39+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt; &lt;img alt="What are the best open source coding ideas you can share?" src="https://preview.redd.it/zcf9q42bgifg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8d5b9a2f9d2a8bbb940fa1ae8f1c616ca45968f" title="What are the best open source coding ideas you can share?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build a place for my friends so they can try and learn ai assisted engineering/vibe coding. Some of them are 50 yrs experienced devs familiar with enterprise standards, some 16 yrs old vibe coders that want to build their first scripts.&lt;/p&gt; &lt;p&gt;How would you structure guide for newcomers? Any favourite tools I should add/replace?&lt;/p&gt; &lt;p&gt;What would you choose for 24h hackathon and what is more suitable for weeks/months project?&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/dontriskit/awesome-ai-software-engineering"&gt;https://github.com/dontriskit/awesome-ai-software-engineering&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zcf9q42bgifg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlpjp</id>
    <title>Internet blackout and Local LLMs</title>
    <updated>2026-01-25T15:15:05+00:00</updated>
    <author>
      <name>/u/DunderSunder</name>
      <uri>https://old.reddit.com/user/DunderSunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).&lt;/p&gt; &lt;p&gt;Meanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunderSunder"&gt; /u/DunderSunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; ‚Äî image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; ‚Äî we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn‚Äôt even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
