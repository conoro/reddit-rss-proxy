<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-28T23:06:05+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nsrrod</id>
    <title>Can crowd shape the open future, or is everything up to huge investors?</title>
    <updated>2025-09-28T15:51:24+00:00</updated>
    <author>
      <name>/u/Guardian-Spirit</name>
      <uri>https://old.reddit.com/user/Guardian-Spirit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am quite a bit concerned about the future of open-weight AI.&lt;/p&gt; &lt;p&gt;Right now, we're &lt;em&gt;mostly&lt;/em&gt; good: there is a lot of competition, a lot of open companies, but the gap between closed and open-weight is way larger than I'd like to have it. And capitalism usually means that the gap will only get larger, as commercialy successful labs will gain more power to produce their closed models, eventually leaving the competition far behind.&lt;/p&gt; &lt;p&gt;What can really be done by mortal crowd to ensure &amp;quot;utopia&amp;quot;, and not some megacorp-controlled &amp;quot;dystopia&amp;quot;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Guardian-Spirit"&gt; /u/Guardian-Spirit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsrrod/can_crowd_shape_the_open_future_or_is_everything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsrrod/can_crowd_shape_the_open_future_or_is_everything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsrrod/can_crowd_shape_the_open_future_or_is_everything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T15:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsvtq1</id>
    <title>Lmstudio tables can't be pasted</title>
    <updated>2025-09-28T18:33:16+00:00</updated>
    <author>
      <name>/u/General-Cookie6794</name>
      <uri>https://old.reddit.com/user/General-Cookie6794</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Lmstudio generates very nice tables but can't be pasted in either word or Excel.. is there a way out ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/General-Cookie6794"&gt; /u/General-Cookie6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsvtq1/lmstudio_tables_cant_be_pasted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsvtq1/lmstudio_tables_cant_be_pasted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsvtq1/lmstudio_tables_cant_be_pasted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T18:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns9jj1</id>
    <title>ChatGPT won't let you build an LLM server that passes through reasoning content</title>
    <updated>2025-09-27T23:30:33+00:00</updated>
    <author>
      <name>/u/Acceptable_Adagio_91</name>
      <uri>https://old.reddit.com/user/Acceptable_Adagio_91</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI are trying so hard to protect their special sauce now that they have added a rule in ChatGPT which disallows it from building code that will facilitate reasoning content being passed through an LLM server to a client. It doesn't care that it's an open source model, or not an OpenAI model, it will add in reasoning content filters (without being asked to) and it definitely will not remove them if asked.&lt;/p&gt; &lt;p&gt;Pretty annoying when you're just trying to work with open source models where I can see all the reasoning content anyway and for my use case, I specifically want the reasoning content to be presented to the client...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Adagio_91"&gt; /u/Acceptable_Adagio_91 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns9jj1/chatgpt_wont_let_you_build_an_llm_server_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T23:30:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsss8s</id>
    <title>4070Ti super or wait for a 5070ti</title>
    <updated>2025-09-28T16:32:25+00:00</updated>
    <author>
      <name>/u/greensmuzi</name>
      <uri>https://old.reddit.com/user/greensmuzi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Got a chance for a 4070Ti Super for 590‚Ç¨ from ebay. I am looking for a gpu for local AI tasks and gaming and was trying to get a 4070ti super, 4080 or 5070ti all 16gb. The other two usually go for around 700+‚Ç¨ used. Should I just go for it or wait for the 5070Ti? Are the 50 series architecture improvements that much better for local AI?&lt;/p&gt; &lt;p&gt;Im looking to use mostly LLMs at first but want to also try image generation and whatnot. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/greensmuzi"&gt; /u/greensmuzi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsss8s/4070ti_super_or_wait_for_a_5070ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsss8s/4070ti_super_or_wait_for_a_5070ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsss8s/4070ti_super_or_wait_for_a_5070ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T16:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns7f86</id>
    <title>Native MCP now in Open WebUI!</title>
    <updated>2025-09-27T21:52:59+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt; &lt;img alt="Native MCP now in Open WebUI!" src="https://external-preview.redd.it/M25kcGJzOW4zc3JmMUhHt6uNZXDs9ywsBLgDtMNnOeRDGUuA-xcxHHChg7dp.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=893d840d90d19c5f13b37eb84534bdf21af148f9" title="Native MCP now in Open WebUI!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4qv7zp9n3srf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns7f86/native_mcp_now_in_open_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T21:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns2fbl</id>
    <title>For llama.cpp/ggml AMD MI50s are now universally faster than NVIDIA P40s</title>
    <updated>2025-09-27T18:24:00+00:00</updated>
    <author>
      <name>/u/Remove_Ayys</name>
      <uri>https://old.reddit.com/user/Remove_Ayys</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In 2023 I implemented llama.cpp/ggml CUDA support specifically for NVIDIA P40s since they were one of the cheapest options for GPUs with 24 GB VRAM. Recently AMD MI50s became very cheap options for GPUs with 32 GB VRAM, selling for well below $150 if you order multiple of them off of Alibaba. However, the llama.cpp ROCm performance was very bad because the code was originally written for NVIDIA GPUs and simply translated to AMD via HIP. I have now optimized the CUDA FlashAttention code in particular for AMD and as a result MI50s now actually have better performance than P40s:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Test&lt;/th&gt; &lt;th&gt;Depth&lt;/th&gt; &lt;th&gt;t/s P40 (CUDA)&lt;/th&gt; &lt;th&gt;t/s P40 (Vulkan)&lt;/th&gt; &lt;th&gt;t/s MI50 (ROCm)&lt;/th&gt; &lt;th&gt;t/s MI50 (Vulkan)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;266.63&lt;/td&gt; &lt;td&gt;32.02&lt;/td&gt; &lt;td&gt;272.95&lt;/td&gt; &lt;td&gt;85.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;210.77&lt;/td&gt; &lt;td&gt;30.51&lt;/td&gt; &lt;td&gt;230.32&lt;/td&gt; &lt;td&gt;51.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;13.50&lt;/td&gt; &lt;td&gt;14.74&lt;/td&gt; &lt;td&gt;22.29&lt;/td&gt; &lt;td&gt;20.91&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Gemma 3 Instruct 27b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;12.09&lt;/td&gt; &lt;td&gt;12.76&lt;/td&gt; &lt;td&gt;19.12&lt;/td&gt; &lt;td&gt;16.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;1095.11&lt;/td&gt; &lt;td&gt;114.08&lt;/td&gt; &lt;td&gt;1140.27&lt;/td&gt; &lt;td&gt;372.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;pp512&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;249.98&lt;/td&gt; &lt;td&gt;73.54&lt;/td&gt; &lt;td&gt;420.88&lt;/td&gt; &lt;td&gt;92.10&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;td&gt;67.30&lt;/td&gt; &lt;td&gt;63.54&lt;/td&gt; &lt;td&gt;77.15&lt;/td&gt; &lt;td&gt;81.48&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen 3 30b a3b q4_K_M&lt;/td&gt; &lt;td&gt;tg128&lt;/td&gt; &lt;td&gt;16384&lt;/td&gt; &lt;td&gt;36.15&lt;/td&gt; &lt;td&gt;42.66&lt;/td&gt; &lt;td&gt;39.91&lt;/td&gt; &lt;td&gt;40.69&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I did not yet touch regular matrix multiplications so the speed on an empty context is probably still suboptimal. The Vulkan performance is in some instances better than the ROCm performance. Since I've already gone to the effort to read the AMD ISA documentation I've also purchased an MI100 and RX 9060 XT and I will optimize the ROCm performance for that hardware as well. An AMD person said they would sponsor me a Ryzen AI MAX system, I'll get my RDNA3 coverage from that.&lt;/p&gt; &lt;p&gt;Edit: looking at the numbers again there is an instance where the optimal performance of the P40 is still better than the optimal performance of the MI50 so the &amp;quot;universally&amp;quot; qualifier is not quite correct. But Reddit doesn't let me edit the post title so we'll just have to live with it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Remove_Ayys"&gt; /u/Remove_Ayys &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns2fbl/for_llamacppggml_amd_mi50s_are_now_universally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T18:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsr0vf</id>
    <title>Bring Your Own Data (BYOD)</title>
    <updated>2025-09-28T15:21:12+00:00</updated>
    <author>
      <name>/u/Long_Complex_4395</name>
      <uri>https://old.reddit.com/user/Long_Complex_4395</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The knowledge of Large Language Models sky rocketed after ChatGPT was born, everyone jumped into the trend of building and using LLMs whether its to sell to companies or companies integrating it into their system. Frequently, many models get released with new benchmarks, targeting specific tasks such as sales, code generation and reviews and the likes.&lt;/p&gt; &lt;p&gt;Last month, Harvard Business Review wrote an article on MIT Media Lab‚Äôs research which highlighted the study that 95% of investments in gen AI have produced zero returns. This is not a technical issue, but more of a business one where everybody wants to create or integrate their own AI due to the hype and FOMO. This research may or may not have put a wedge in the adoption of AI into existing systems.&lt;/p&gt; &lt;p&gt;To combat the lack of returns, Small Language Models seems to do pretty well as they are more specialized to achieve a given task. This led me to working on Otto - an end-to-end small language model builder where you build your model with your own data, its open source, still rough around the edges.&lt;/p&gt; &lt;p&gt;To demonstrate this pipeline, I got data from Huggingface - a 142MB data containing automotive customer service transcript with the following parameters&lt;/p&gt; &lt;ul&gt; &lt;li&gt;6 layers, 6 heads, 384 embedding dimensions&lt;/li&gt; &lt;li&gt;50,257 vocabulary tokens&lt;/li&gt; &lt;li&gt;128 tokens for block size.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;which gave 16.04M parameters. Its training loss improved from 9.2 to 2.2 with domain specialization where it learned automotive service conversation structure.&lt;/p&gt; &lt;p&gt;This model learned the specific patterns of automotive customer service calls, including technical vocabulary, conversation flow, and domain-specific terminology that a general-purpose model might miss or handle inefficiently.&lt;/p&gt; &lt;p&gt;There are still improvements needed for the pipeline which I am working on, you can try it out here: &lt;a href="https://github.com/Nwosu-Ihueze/otto"&gt;https://github.com/Nwosu-Ihueze/otto&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_Complex_4395"&gt; /u/Long_Complex_4395 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsr0vf/bring_your_own_data_byod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsr0vf/bring_your_own_data_byod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsr0vf/bring_your_own_data_byod/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T15:21:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt08le</id>
    <title>How do I use Higgs Audio V2 prompting for tone and emotions?</title>
    <updated>2025-09-28T21:29:20+00:00</updated>
    <author>
      <name>/u/Adept_Lawyer_4592</name>
      <uri>https://old.reddit.com/user/Adept_Lawyer_4592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I‚Äôve been experimenting with Higgs Audio V2 and I‚Äôm a bit confused about how the prompting part works.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Can I actually change the tone of the generated voice through prompting?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is it possible to add emotions (like excitement, sadness, calmness, etc.)?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Can I insert things like a laugh or specific voice effects into certain parts of the text just by using prompts?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If anyone has experience with this, I‚Äôd really appreciate some clear examples of how to structure prompts for different tones/emotions. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Lawyer_4592"&gt; /u/Adept_Lawyer_4592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt08le/how_do_i_use_higgs_audio_v2_prompting_for_tone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt08le/how_do_i_use_higgs_audio_v2_prompting_for_tone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt08le/how_do_i_use_higgs_audio_v2_prompting_for_tone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T21:29:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt0kn3</id>
    <title>Someone pinch me .! ü§£ Am I seeing this right ?.üôÑ</title>
    <updated>2025-09-28T21:43:22+00:00</updated>
    <author>
      <name>/u/sub_RedditTor</name>
      <uri>https://old.reddit.com/user/sub_RedditTor</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"&gt; &lt;img alt="Someone pinch me .! ü§£ Am I seeing this right ?.üôÑ" src="https://b.thumbs.redditmedia.com/uZSQcPB3gvUjO5vlPxt8AX_gBhOg-2Up3FDO2_kjoZE.jpg" title="Someone pinch me .! ü§£ Am I seeing this right ?.üôÑ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A what looks like 4080S with 32GB vRam ..! üßê . I just got 2X 3080 20GB üò´&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sub_RedditTor"&gt; /u/sub_RedditTor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nt0kn3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt0kn3/someone_pinch_me_am_i_seeing_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T21:43:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsoqa7</id>
    <title>What am I missing? GPT-OSS is much slower than Qwen 3 30B A3B for me!</title>
    <updated>2025-09-28T13:45:46+00:00</updated>
    <author>
      <name>/u/Final_Wheel_7486</name>
      <uri>https://old.reddit.com/user/Final_Wheel_7486</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey to y'all,&lt;/p&gt; &lt;p&gt;I'm having a slightly weird problem. For weeks now, people have been saying &amp;quot;GPT-OSS is so fast, it's so quick, it's amazing&amp;quot;, and I agree, the model is great.&lt;/p&gt; &lt;p&gt;But one thing bugs me out; Qwen 30B A3B is noticeably faster on my end. For context, I am using an RTX 4070 Ti (12 GB VRAM) and 5600 MHz 32 GB system RAM with a Ryzen 7 7700X. As for quantizations, I am using the default MFPX4 format for GPT-OSS and Q4_K_M for Qwen 3 30B A3B.&lt;/p&gt; &lt;p&gt;I am launching those with almost the same command line parameters (llama-swap in the background):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;/app/llama-server -hf unsloth/gpt-oss-20b-GGUF:F16 --jinja -ngl 19 -c 8192 -fa on -np 4 /app/llama-server -hf unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF:Q4_K_M --jinja -ngl 26 -c 8192 -fa on -np 4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;em&gt;(I just increased -ngl as long as I could until it wouldn't fit anymore - using -ngl 99 didn't work for me)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;What am I missing? GPT-OSS only hits 25 tok/s on good days, while Qwen easily hits up to 34.5 tok/s! I made sure to use the most recent releases when testing, so that can't be it... prompt processing is roughly the same speed, with a slight performance edge for GPT-OSS.&lt;/p&gt; &lt;p&gt;Anyone with the same issue?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Final_Wheel_7486"&gt; /u/Final_Wheel_7486 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsoqa7/what_am_i_missing_gptoss_is_much_slower_than_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsoqa7/what_am_i_missing_gptoss_is_much_slower_than_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsoqa7/what_am_i_missing_gptoss_is_much_slower_than_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T13:45:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsxrk6</id>
    <title>ToolNeuron Beta 4.5 Release - Feedback Wanted</title>
    <updated>2025-09-28T19:49:34+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsxrk6/toolneuron_beta_45_release_feedback_wanted/"&gt; &lt;img alt="ToolNeuron Beta 4.5 Release - Feedback Wanted" src="https://external-preview.redd.it/ODlyY3cwNW1teXJmMR13cRa5bXnAZZ9MgDHmz-pFz9c4Kk6PP4n4CLoKUFbM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2994587fc461071537d395ecb37055bd384c2ec2" title="ToolNeuron Beta 4.5 Release - Feedback Wanted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just pushed out ToolNeuron Beta 4.5 and wanted to share what‚Äôs new. This is more of a quick release focused on adding core features and stability fixes. A bigger update (5.0) will follow once things are polished.&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/Siddhesh2377/ToolNeuron/releases/tag/Beta-4.5"&gt;https://github.com/Siddhesh2377/ToolNeuron/releases/tag/Beta-4.5&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What‚Äôs New&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Code Canvas: AI responses with proper syntax highlighting instead of plain text. No execution, just cleaner code view.&lt;/li&gt; &lt;li&gt;DataHub: A plugin-and-play knowledge base for any text-based GGUF model inside ToolNeuron.&lt;/li&gt; &lt;li&gt;DataHub Store: Download and manage data-packs directly inside the app.&lt;/li&gt; &lt;li&gt;DataHub Screen: Added a dedicated screen to review memory of apps and models (Settings &amp;gt; Data Hub &amp;gt; Open).&lt;/li&gt; &lt;li&gt;Data Pack Controls: Data packs can stay loaded but only enabled when needed via the database icon near the chat send button.&lt;/li&gt; &lt;li&gt;Improved Plugin System: More stable and easier to use.&lt;/li&gt; &lt;li&gt;Web Scraping Tool: Added, but still unstable (same as Web Search plugin).&lt;/li&gt; &lt;li&gt;Fixed Chat UI &amp;amp; backend.&lt;/li&gt; &lt;li&gt;Fixed UI &amp;amp; UX for model screen.&lt;/li&gt; &lt;li&gt;Clear Chat History button now works.&lt;/li&gt; &lt;li&gt;Chat regeneration works with any model.&lt;/li&gt; &lt;li&gt;Desktop app (Mac/Linux/Windows) coming soon to help create your own data packs.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Known Issues&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Model loading may fail or stop unexpectedly.&lt;/li&gt; &lt;li&gt;Model downloading might fail if app is sent to background.&lt;/li&gt; &lt;li&gt;Some data packs may fail to load due to Android memory restrictions.&lt;/li&gt; &lt;li&gt;Web Search and Web Scrap plugins may fail on certain queries or pages.&lt;/li&gt; &lt;li&gt;Output generation can feel slow at times.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Not in This Release&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Chat context. Models will not consider previous chats for now.&lt;/li&gt; &lt;li&gt;Model tweaking is paused.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Next Steps&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Focus will be on stability for 5.0.&lt;/li&gt; &lt;li&gt;Adding proper context support.&lt;/li&gt; &lt;li&gt;Better tool stability and optimization.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Join the Discussion&lt;/h1&gt; &lt;p&gt;I‚Äôve set up a Discord server where updates, feedback, and discussions happen more actively. If you‚Äôre interested, you can join here: &lt;a href="https://discord.gg/CXaX3UHy"&gt;https://discord.gg/CXaX3UHy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is still an early build, so I‚Äôd really appreciate feedback, bug reports, or even just ideas. Thanks for checking it out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/dz64045mmyrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsxrk6/toolneuron_beta_45_release_feedback_wanted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsxrk6/toolneuron_beta_45_release_feedback_wanted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T19:49:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsqe5i</id>
    <title>I created a simple tool to manage your llama.cpp settings &amp; installation</title>
    <updated>2025-09-28T14:55:42+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsqe5i/i_created_a_simple_tool_to_manage_your_llamacpp/"&gt; &lt;img alt="I created a simple tool to manage your llama.cpp settings &amp;amp; installation" src="https://preview.redd.it/z2jl2s624xrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c627cf5e755bc96918498e6d100146f512f95e46" title="I created a simple tool to manage your llama.cpp settings &amp;amp; installation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yo! I was messing around with my configs etc and noticed it was a massive pain to keep it all in one place... So I vibecoded this thing. &lt;a href="https://github.com/IgorWarzocha/llama_cpp_manager"&gt;https://github.com/IgorWarzocha/llama_cpp_manager&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A zero-bs configuration tool for llama.cpp that runs in your terminal and keeps it all organised in one folder.&lt;/p&gt; &lt;p&gt;It starts with a wizard to configure your basic defaults, it sorts out your llama.cpp download/update - it checks the appropriate compiled binary file from the github repo, downloads it, unzips, cleans up the temp file, etc etc.&lt;/p&gt; &lt;p&gt;There's a model config management module that guides you through editing basic config, but you can also add your own parameters... All saved in json files in plain sight.&lt;/p&gt; &lt;p&gt;I also included a basic benchmarking utility that will run your saved model configs (in batch if you want) against your current server config with a pre-selected prompt and give you stats.&lt;/p&gt; &lt;p&gt;Anyway, I tested it thoroughly enough on Ubuntu/Vulkan. Can't vouch for any other situations. If you have your own compiled llama.cpp you can drop it into llama-cpp folder.&lt;/p&gt; &lt;p&gt;Let me know if it works for you (works on my machine, hah), if you would like to see any features added etc. It's hard to keep a &amp;quot;good enough&amp;quot; mindset and avoid being overwhelming or annoying lolz.&lt;/p&gt; &lt;p&gt;Cheerios.&lt;/p&gt; &lt;p&gt;edit, before you start roasting, I have now fixed hardcoded paths, hopefully all of them this time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z2jl2s624xrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsqe5i/i_created_a_simple_tool_to_manage_your_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsqe5i/i_created_a_simple_tool_to_manage_your_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T14:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsetwi</id>
    <title>LMStudio + MCP is so far the best experience I've had with models in a while.</title>
    <updated>2025-09-28T04:06:29+00:00</updated>
    <author>
      <name>/u/Komarov_d</name>
      <uri>https://old.reddit.com/user/Komarov_d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;M4 Max 128gb&lt;br /&gt; Mostly use latest gpt-oss 20b or latest mistral with thinking/vision/tools in MLX format, since a bit faster (that's the whole point of MLX I guess, since we still don't have any proper LLMs in CoreML for apple neural engine...).&lt;/p&gt; &lt;p&gt;Connected around 10 MCPs for different purposes, works just purely amazing.&lt;br /&gt; Haven't been opening chat com or claude for a couple of days. &lt;/p&gt; &lt;p&gt;Pretty happy.&lt;/p&gt; &lt;p&gt;the next step is having a proper agentic conversation/flow under the hood, being able to leave it for autonomous working sessions, like cleaning up and connecting things in my Obsidian Vault during the night while I sleep, right...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Komarov_d"&gt; /u/Komarov_d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsetwi/lmstudio_mcp_is_so_far_the_best_experience_ive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:06:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsyu7b</id>
    <title>Do you think that &lt;4B models has caught up with good old GPT3?</title>
    <updated>2025-09-28T20:32:15+00:00</updated>
    <author>
      <name>/u/Ok-Internal9317</name>
      <uri>https://old.reddit.com/user/Ok-Internal9317</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"&gt; &lt;img alt="Do you think that &amp;lt;4B models has caught up with good old GPT3?" src="https://b.thumbs.redditmedia.com/dqXrXaddxjMsBxXL3FLNfmNHICqqjQ2Qhphy8PaccHw.jpg" title="Do you think that &amp;lt;4B models has caught up with good old GPT3?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/a76qyhd1uyrf1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35fbb5e302f260d4c57ab6ad41ce0d4d770906fc"&gt;https://preview.redd.it/a76qyhd1uyrf1.png?width=807&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35fbb5e302f260d4c57ab6ad41ce0d4d770906fc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I think it was up to 3.5 that it stopped hallusinating like hell, so what do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Internal9317"&gt; /u/Ok-Internal9317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsyu7b/do_you_think_that_4b_models_has_caught_up_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:32:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsnahe</id>
    <title>September 2025 benchmarks - 3x3090</title>
    <updated>2025-09-28T12:37:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/"&gt; &lt;img alt="September 2025 benchmarks - 3x3090" src="https://a.thumbs.redditmedia.com/oowsP26bvqdNGyybdB_e9u8e_0jjWcpcYEbbz5x_2V0.jpg" title="September 2025 benchmarks - 3x3090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please enjoy the benchmarks on 3√ó3090 GPUs. &lt;/p&gt; &lt;p&gt;(If you want to reproduce my steps on your setup, you may need a fresh &lt;strong&gt;llama.cpp&lt;/strong&gt; build)&lt;/p&gt; &lt;p&gt;To run the benchmark, simply execute:&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-bench -m &amp;lt;path-to-the-model&amp;gt;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Sometimes you may need to add &lt;code&gt;--n-cpu-moe&lt;/code&gt; or &lt;code&gt;-ts&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;We‚Äôll be testing a faster ‚Äúdry run‚Äù and a run with a prefilled context (10000 tokens). So for each model, you‚Äôll see boundaries between the initial speed and later, slower speed.&lt;/p&gt; &lt;p&gt;results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;gemma3 27B Q8 - 23t/s, 26t/s&lt;/li&gt; &lt;li&gt;Llama4 Scout Q5 - 23t/s, 30t/s&lt;/li&gt; &lt;li&gt;gpt oss 120B - 95t/s, 125t/s&lt;/li&gt; &lt;li&gt;dots Q3 - 15t/s, 20t/s&lt;/li&gt; &lt;li&gt;Qwen3 30B A3B - 78t/s, 130t/s&lt;/li&gt; &lt;li&gt;Qwen3 32B - 17t/s, 23t/s&lt;/li&gt; &lt;li&gt;Magistral Q8 - 28t/s, 33t/s&lt;/li&gt; &lt;li&gt;GLM 4.5 Air Q4 - 22t/s, 36t/s&lt;/li&gt; &lt;li&gt;Nemotron 49B Q8 - 13t/s, 16t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;please share your results on your setup&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nsnahe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsnahe/september_2025_benchmarks_3x3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:37:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsghai</id>
    <title>Hunyan Image 3 Llm with image output</title>
    <updated>2025-09-28T05:42:53+00:00</updated>
    <author>
      <name>/u/ArtichokeNo2029</name>
      <uri>https://old.reddit.com/user/ArtichokeNo2029</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"&gt; &lt;img alt="Hunyan Image 3 Llm with image output" src="https://external-preview.redd.it/o3nW-C4go8YOZmYJKNZ4Y6tpE64YDvgP6ucRppFfdVQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0726b4b60205c7c2cac24ba84a82a9bbfa3680c3" title="Hunyan Image 3 Llm with image output" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pretty sure this a first of kind open sourced. They also plan a Thinking model too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArtichokeNo2029"&gt; /u/ArtichokeNo2029 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsghai/hunyan_image_3_llm_with_image_output/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T05:42:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nstg75</id>
    <title>GPT OSS 120B on 20GB VRAM - 6.61 tok/sec - RTX 2060 Super + RTX 4070 Super</title>
    <updated>2025-09-28T16:59:52+00:00</updated>
    <author>
      <name>/u/Storge2</name>
      <uri>https://old.reddit.com/user/Storge2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nstg75/gpt_oss_120b_on_20gb_vram_661_toksec_rtx_2060/"&gt; &lt;img alt="GPT OSS 120B on 20GB VRAM - 6.61 tok/sec - RTX 2060 Super + RTX 4070 Super" src="https://b.thumbs.redditmedia.com/ol8XNQyBK8l-sLamJdH4aFVCCR47eV92ZF_Tm0viPFk.jpg" title="GPT OSS 120B on 20GB VRAM - 6.61 tok/sec - RTX 2060 Super + RTX 4070 Super" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zsy2jp8irxrf1.png?width=607&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b10141f6f06f497e8c2618032dd1c3330ea9bf35"&gt;Task Manager&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c5dtvy7trxrf1.png?width=983&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=39971616dffc9938d870dbfd458f2e3178cf6b2f"&gt;Proof of the answer.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4us3kezhsxrf1.png?width=454&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b68e912e0c05f91fd3142fb3f7245eb7f1bca2a"&gt;LM Studio Settings&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System:&lt;br /&gt; Ryzen 7 5700X3D&lt;br /&gt; 2x 32GB DDR4 3600 CL18&lt;br /&gt; 512GB NVME M2 SSD&lt;br /&gt; RTX 2060 Super (8GB over PCIE 3.0X4) + RTX 4070 Super (PCIE 3.0X16)&lt;br /&gt; B450M Tommahawk Max&lt;/p&gt; &lt;p&gt;It is incredible that this can run on my machine. I think i could push context even higher maybe to 8K before running out of RAM. I just got into local running of LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Storge2"&gt; /u/Storge2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nstg75/gpt_oss_120b_on_20gb_vram_661_toksec_rtx_2060/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nstg75/gpt_oss_120b_on_20gb_vram_661_toksec_rtx_2060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nstg75/gpt_oss_120b_on_20gb_vram_661_toksec_rtx_2060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T16:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nslth7</id>
    <title>Holy moly what did those madlads at llama cpp do?!!</title>
    <updated>2025-09-28T11:20:42+00:00</updated>
    <author>
      <name>/u/Similar-Republic149</name>
      <uri>https://old.reddit.com/user/Similar-Republic149</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just ran gpt oss 20b on my mi50 32gb and im getting 90tkps !?!?!? before it was around 40 . &lt;/p&gt; &lt;p&gt;./llama-bench -m /home/server/.lmstudio/models/lmstudio-community/gpt-oss-20b-GGUF/gpt-oss-20b-MXFP4.gguf -ngl 999 -fa on -mg 1 -dev Vulkan1 &lt;/p&gt; &lt;p&gt;load_backend: loaded RPC backend from /home/server/Desktop/Llama/llama-b6615-bin-ubuntu-vulkan-x64/build/bin/libggml-rpc.so&lt;/p&gt; &lt;p&gt;ggml_vulkan: Found 2 Vulkan devices:&lt;/p&gt; &lt;p&gt;ggml_vulkan: 0 = NVIDIA GeForce RTX 2060 (NVIDIA) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 32 | shared memory: 49152 | int dot: 1 | matrix cores: KHR_coopmat&lt;/p&gt; &lt;p&gt;ggml_vulkan: 1 = AMD Instinct MI50/MI60 (RADV VEGA20) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: none&lt;/p&gt; &lt;p&gt;load_backend: loaded Vulkan backend from /home/server/Desktop/Llama/llama-b6615-bin-ubuntu-vulkan-x64/build/bin/libggml-vulkan.so&lt;/p&gt; &lt;p&gt;load_backend: loaded CPU backend from /home/server/Desktop/Llama/llama-b6615-bin-ubuntu-vulkan-x64/build/bin/libggml-cpu-haswell.so&lt;/p&gt; &lt;p&gt;| model | size | params | backend | ngl | main_gpu | dev | test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | ------------ | --------------: | -------------------: |&lt;/p&gt; &lt;p&gt;| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 999 | 1 | Vulkan1 | pp512 | 620.68 ¬± 6.62 |&lt;/p&gt; &lt;p&gt;| gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 999 | 1 | Vulkan1 | tg128 | 91.42 ¬± 1.51 |&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Similar-Republic149"&gt; /u/Similar-Republic149 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nslth7/holy_moly_what_did_those_madlads_at_llama_cpp_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nslth7/holy_moly_what_did_those_madlads_at_llama_cpp_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nslth7/holy_moly_what_did_those_madlads_at_llama_cpp_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T11:20:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsfkqd</id>
    <title>dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally</title>
    <updated>2025-09-28T04:48:30+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"&gt; &lt;img alt="dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally" src="https://b.thumbs.redditmedia.com/a-fo9Zu2i0HXnjVJmdjuGYWbgWVo6fs53xAXUEtZjsw.jpg" title="dont buy the api from the website like openrouther or groq or anyother provider they reduce the qulaity of the model to make a profit . buy the api only from official website or run the model in locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;even there is no guarantee that official will be same good as the benchmark shown us .&lt;/p&gt; &lt;p&gt;so running the model locally is the best way to use the full power of the model .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nsfkqd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsfkqd/dont_buy_the_api_from_the_website_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T04:48:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsszob</id>
    <title>The MoE tradeoff seems bad for local hosting</title>
    <updated>2025-09-28T16:40:58+00:00</updated>
    <author>
      <name>/u/upside-down-number</name>
      <uri>https://old.reddit.com/user/upside-down-number</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think I understand this right, but somebody tell me where I'm wrong here.&lt;/p&gt; &lt;p&gt;Overly simplified explanation of how an LLM works: for a dense model, you take the context, stuff it through the whole neural network, sample a token, add it to the context, and do it again. The way an MoE model works, instead of the context getting processed by the entire model, there's a router network and then the model is split into a set of &amp;quot;experts&amp;quot;, and only some subset of those get used to compute the next output token. But you need more total parameters in the model for this, there's a rough rule of thumb that an MoE model is equivalent to a dense model of size sqrt(total_params √ó active_params), all else equal. (and all else usually isn't equal, we've all seen wildly different performance from models of the same size, but never mind that).&lt;/p&gt; &lt;p&gt;So the tradeoff is, the MoE model uses more VRAM, uses less compute, and is probably more efficient at batch processing because when it's processing contexts from multiple users those are (hopefully) going to activate different experts in the model. This all works out very well if VRAM is abundant, compute (and electricity) is the big bottleneck, and you're trying to maximize throughput to a large number of users; i.e. the use case for a major AI company.&lt;/p&gt; &lt;p&gt;Now, consider the typical local LLM use case. Probably most local LLM users are in this situation: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;VRAM is not abundant, because you're using consumer grade GPUs where VRAM is kept low for market segmentation reasons&lt;/li&gt; &lt;li&gt;Compute is &lt;em&gt;relatively&lt;/em&gt; more abundant than VRAM, consider that the compute in an RTX 4090 isn't that far off from what you get from an H100; the H100's advantanges are that it has more VRAM and better memory bandwidth and so on&lt;/li&gt; &lt;li&gt;You are serving one user at a time at home, or a small number for some weird small business case&lt;/li&gt; &lt;li&gt;The incremental benefit of higher token throughput above some usability threshold of 20-30 tok/sec is not very high&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Given all that, it seems like for our use case you're going to want the best dense model you can fit in consumer-grade hardware (one or two consumer GPUs in the neighborhood of 24GB size), right? Unfortunately the major labs are going to be optimizing mostly for the largest MoE model they can fit in a 8xH100 server or similar because that's increasingly important for their own use case. Am I missing anything here? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/upside-down-number"&gt; /u/upside-down-number &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsszob/the_moe_tradeoff_seems_bad_for_local_hosting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsszob/the_moe_tradeoff_seems_bad_for_local_hosting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsszob/the_moe_tradeoff_seems_bad_for_local_hosting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T16:40:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsz6ss</id>
    <title>Local multimodal RAG: search &amp; summarize screenshots/photos fully offline</title>
    <updated>2025-09-28T20:46:24+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsz6ss/local_multimodal_rag_search_summarize/"&gt; &lt;img alt="Local multimodal RAG: search &amp;amp; summarize screenshots/photos fully offline" src="https://external-preview.redd.it/OXk5MDdtY2h3eXJmMbRJ2b6sMPmHJiOrbj5FYV3hs8t-hezd4gT3rSFztsqf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=766b7b8d1974e0c636d5623a7a7843becb301d95" title="Local multimodal RAG: search &amp;amp; summarize screenshots/photos fully offline" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;One of the strongest use cases I‚Äôve found for local LLMs + vision is turning my messy screenshot/photo library into something queryable.&lt;/p&gt; &lt;p&gt;Half my ‚Äúnotes‚Äù are just images ‚Äî slides from talks, whiteboards, book pages, receipts, chat snippets. Normally they rot in a folder. Now I can:&lt;br /&gt; ‚Äì Point a local multimodal agent (&lt;a href="https://hyperlink.nexa.ai/"&gt;Hyperlink&lt;/a&gt;) at my screenshots folder&lt;br /&gt; ‚Äì Ask in plain English ‚Üí &lt;em&gt;‚ÄúSummarize what I saved about the future of AI‚Äù&lt;/em&gt;&lt;br /&gt; ‚Äì It runs OCR + embeddings locally, pulls the right images, and gives a short summary with the source image linked&lt;/p&gt; &lt;p&gt;No cloud, no quotas. 100% on-device. My own storage is the only limit.&lt;/p&gt; &lt;p&gt;Feels like the natural extension of RAG: not just text docs, but &lt;em&gt;vision + text together&lt;/em&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Imagine querying screenshots, PDFs, and notes in one pass&lt;/li&gt; &lt;li&gt;Summaries grounded in the actual images&lt;/li&gt; &lt;li&gt;Completely private, runs on consumer hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm using Hyperlink to prototype this flow. Curious if anyone else here is building multimodal local RAG ‚Äî what have you managed to get working, and what‚Äôs been most useful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lnkwumchwyrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsz6ss/local_multimodal_rag_search_summarize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsz6ss/local_multimodal_rag_search_summarize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:46:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nt1jaa</id>
    <title>Good ol gpu heat</title>
    <updated>2025-09-28T22:24:45+00:00</updated>
    <author>
      <name>/u/animal_hoarder</name>
      <uri>https://old.reddit.com/user/animal_hoarder</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt; &lt;img alt="Good ol gpu heat" src="https://preview.redd.it/94k8168cezrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7614d55e91893f545ea06888d5bbbc047c1ec146" title="Good ol gpu heat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live at 9600ft in a basement with extremely inefficient floor heaters, so it‚Äôs usually 50-60F inside year round. I‚Äôve been fine tuning Mistral 7B for a dungeons and dragons game I‚Äôve been working on and oh boy does my 3090 pump out some heat. Popped the front cover off for some more airflow. My cat loves my new hobby, he just waits for me to run another training script so he can soak it in. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/animal_hoarder"&gt; /u/animal_hoarder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/94k8168cezrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nt1jaa/good_ol_gpu_heat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T22:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsu3og</id>
    <title>Drummer's Cydonia R1 24B v4.1 ¬∑ A less positive, less censored, better roleplay, creative finetune with reasoning!</title>
    <updated>2025-09-28T17:25:35+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"&gt; &lt;img alt="Drummer's Cydonia R1 24B v4.1 ¬∑ A less positive, less censored, better roleplay, creative finetune with reasoning!" src="https://external-preview.redd.it/pROfhPfXKnMC7ws-LUjL0JI_7ZtgvBAu9hx7L06rI9c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fc893240ec75e9bd482665fa32ded5d1badc97c0" title="Drummer's Cydonia R1 24B v4.1 ¬∑ A less positive, less censored, better roleplay, creative finetune with reasoning!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Backlog:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cydonia v4.2.0,&lt;/li&gt; &lt;li&gt;Snowpiercer 15B v3,&lt;/li&gt; &lt;li&gt;Anubis Mini 8B v1&lt;/li&gt; &lt;li&gt;Behemoth ReduX 123B v1.1 (v4.2.0 treatment)&lt;/li&gt; &lt;li&gt;RimTalk Mini (showcase)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can't wait to release v4.2.0. I think it's proof that I still have room to grow. You can test it out here: &lt;a href="https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF"&gt;https://huggingface.co/BeaverAI/Cydonia-24B-v4o-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and I went ahead and gave Largestral 2407 the same treatment here: &lt;a href="https://huggingface.co/BeaverAI/Behemoth-ReduX-123B-v1b-GGUF"&gt;https://huggingface.co/BeaverAI/Behemoth-ReduX-123B-v1b-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-R1-24B-v4.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsu3og/drummers_cydonia_r1_24b_v41_a_less_positive_less/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T17:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsy2ak</id>
    <title>GLM4.6 soon ?</title>
    <updated>2025-09-28T20:01:14+00:00</updated>
    <author>
      <name>/u/Angel-Karlsson</name>
      <uri>https://old.reddit.com/user/Angel-Karlsson</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt; &lt;img alt="GLM4.6 soon ?" src="https://b.thumbs.redditmedia.com/dskyWZciazzyTx2dXG0mytJV0Yq5yzQ46bgrpLsQ1-s.jpg" title="GLM4.6 soon ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/usggbqdmoyrf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1677630b65f1d4dee3a6776247a0a5f31be050d"&gt;https://preview.redd.it/usggbqdmoyrf1.png?width=567&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1677630b65f1d4dee3a6776247a0a5f31be050d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;While browsing the &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; website, I noticed this... maybe GLM4.6 is coming soon? Given the digital shift, I don't expect major changes... I ear some context lenght increase&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Angel-Karlsson"&gt; /u/Angel-Karlsson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsy2ak/glm46_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T20:01:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nsmksq</id>
    <title>What are Kimi devs smoking</title>
    <updated>2025-09-28T12:02:02+00:00</updated>
    <author>
      <name>/u/Thechae9</name>
      <uri>https://old.reddit.com/user/Thechae9</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt; &lt;img alt="What are Kimi devs smoking" src="https://preview.redd.it/t8wfkk09bwrf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ec5f0e9de05cf0aafc8bc507d4950ca47e8ef09" title="What are Kimi devs smoking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Strangee&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thechae9"&gt; /u/Thechae9 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8wfkk09bwrf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nsmksq/what_are_kimi_devs_smoking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-28T12:02:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
