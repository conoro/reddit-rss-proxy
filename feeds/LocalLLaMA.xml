<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-05T18:07:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nytm2a</id>
    <title>Best Practices for AI Prompting 2025?</title>
    <updated>2025-10-05T17:00:42+00:00</updated>
    <author>
      <name>/u/Party-Log-1084</name>
      <uri>https://old.reddit.com/user/Party-Log-1084</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;At this point, I‚Äôd like to know what the most effective and up-to-date techniques, strategies, prompt lists, or ready-made prompt archives are when it comes to working with AI.&lt;/p&gt; &lt;p&gt;Specifically, I‚Äôm referring to ChatGPT, Gemini, NotebookLM, and Claude. I‚Äôve been using all of these LLMs for quite some time, but I‚Äôd like to improve the overall quality and consistency of my results.&lt;/p&gt; &lt;p&gt;For example, when I want to learn about a specific topic, are there any well-structured prompt archives or proven templates to start from? What should an effective initial prompt include, how should it be structured, and what key elements or best practices should one keep in mind?&lt;/p&gt; &lt;p&gt;There‚Äôs a huge amount of material out there, but much of it isn‚Äôt very helpful. I‚Äôm looking for the methods and resources that truly work.&lt;/p&gt; &lt;p&gt;So far i only heard of that &amp;quot;awesome-ai-system-prompts&amp;quot; Github.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Party-Log-1084"&gt; /u/Party-Log-1084 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nytm2a/best_practices_for_ai_prompting_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nytm2a/best_practices_for_ai_prompting_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nytm2a/best_practices_for_ai_prompting_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T17:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyloqs</id>
    <title>Found Nemotron-9B-v2 quite underwhelming, what am I missing ?</title>
    <updated>2025-10-05T11:25:11+00:00</updated>
    <author>
      <name>/u/Professional_Row_967</name>
      <uri>https://old.reddit.com/user/Professional_Row_967</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After seeing some very positive reviews about Nvidia Nemotron-9B-v2, I downloaded the 6-bit quantized MLX flavour on my Mac Mini M4 (24GB URAM), and set a 32kB context window. After about a dozen different prompts, my opinion of the model is not very positive. It seems to also have a hard time making sense of the history of conversation, making contextually incorrect assumptions (like in AI/ML and enterprise Java framework context, expanded &amp;quot;MCP&amp;quot; to &amp;quot;Manageable Customization Platform&amp;quot;). Upon reprompting it failed to make sense of the history of the discussion so far. Note that I had switched off reasoning. I've tried several other models including &amp;quot;phi4&amp;quot;, &amp;quot;gemma 3&amp;quot;, which seem to perform far better for such prompts. Wondering if there is some setting I am missing ? It is surprising how underwhelming it felt so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Professional_Row_967"&gt; /u/Professional_Row_967 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyloqs/found_nemotron9bv2_quite_underwhelming_what_am_i/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyloqs/found_nemotron9bv2_quite_underwhelming_what_am_i/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyloqs/found_nemotron9bv2_quite_underwhelming_what_am_i/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T11:25:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyb1x2</id>
    <title>Getting 70 t/s on Qwen3-Next-80B-A3B-Instruct-exl3 4.06bpw with my 2x3090</title>
    <updated>2025-10-05T01:14:25+00:00</updated>
    <author>
      <name>/u/Aaaaaaaaaeeeee</name>
      <uri>https://old.reddit.com/user/Aaaaaaaaaeeeee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sup ‚úåÔ∏è&lt;/p&gt; &lt;p&gt;The latest exl3 0.0.7 release has seen improvements to the speed of Qwen3-Next from the last post on Qwen3-Next exl3 support. &lt;/p&gt; &lt;p&gt;I've been using 2 3090s with PCIE4X16 + PCIE3X4 lanes, they are power-limited to 200W. It's the same decoding speeds when setting them to 270W. &lt;/p&gt; &lt;p&gt;Qwen3-Next-80B-A3B 4.06bpw runs around 60-70 t/s between 0-14k context. I briefly tried extended context, 6bit k, v cache at 393,216 context: 368k in, the speed was down to 14 t/s. If you go past the context window you might get a repeating line sometimes, so for your sake set a limit on your UI. The model still writes nicely here. (368k) &lt;/p&gt; &lt;p&gt;I'm not trying to properly relay prompt processing as my setup will maintain a 200W limit, but this setup gets 370 t/s. It might become faster for someone on a different setup with tensor/expert parallel support, and more tuning with other settings. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aaaaaaaaaeeeee"&gt; /u/Aaaaaaaaaeeeee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyb1x2/getting_70_ts_on_qwen3next80ba3binstructexl3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyb1x2/getting_70_ts_on_qwen3next80ba3binstructexl3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyb1x2/getting_70_ts_on_qwen3next80ba3binstructexl3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T01:14:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nycktz</id>
    <title>vLLM - GLM-4.6 Benchmark on 8xH200 NVL: 44 token/second</title>
    <updated>2025-10-05T02:30:50+00:00</updated>
    <author>
      <name>/u/Ill_Recipe7620</name>
      <uri>https://old.reddit.com/user/Ill_Recipe7620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I booted this up with 'screen vllm serve &amp;quot;zai-org/GLM-4.6&amp;quot; --tensor-parallel-size 8&amp;quot; on 8xH200 and getting 44 token/second.&lt;/p&gt; &lt;p&gt;Does that seem slow to anyone else or is this expected?&lt;/p&gt; &lt;p&gt;No quantization just the fully dense model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ill_Recipe7620"&gt; /u/Ill_Recipe7620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nycktz/vllm_glm46_benchmark_on_8xh200_nvl_44_tokensecond/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nycktz/vllm_glm46_benchmark_on_8xh200_nvl_44_tokensecond/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nycktz/vllm_glm46_benchmark_on_8xh200_nvl_44_tokensecond/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T02:30:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykxfq</id>
    <title>BULaMU-The First Luganda Large Language Model Trained from Scratch</title>
    <updated>2025-10-05T10:41:46+00:00</updated>
    <author>
      <name>/u/AgencyInside407</name>
      <uri>https://old.reddit.com/user/AgencyInside407</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody! I hope all is well. I just wanted to share a project that I have been working on for the last several months called BULaMU. It is the first large language model that has been trained from scratch on Luganda. It has 20M parameters so it should be really easy to run on a phone, laptop, or other low powered device and does not require connecting to the internet, since inference happens in C. The details of how I trained it are &lt;a href="https://zenodo.org/records/17271688"&gt;here&lt;/a&gt;. If you would like to download it, use it, or adapt it for your own use, it is available for free on my Huggingface &lt;a href="https://huggingface.co/datasets/mwebazarick/BULaMU"&gt;account&lt;/a&gt;. I am open to any feedback that you are willing to share because I am going to continue working on improving BULaMU. I really believe that tiny language models like this decrease the high barrier to entry that AI often has by allowing people to use these models without a super powerful computer or access to the internet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencyInside407"&gt; /u/AgencyInside407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nykxfq/bulamuthe_first_luganda_large_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T10:41:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyaf4f</id>
    <title>Run Qwen3-VL-30B-A3B locally on Mac (MLX) ‚Äî one line of code</title>
    <updated>2025-10-05T00:42:55+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyaf4f/run_qwen3vl30ba3b_locally_on_mac_mlx_one_line_of/"&gt; &lt;img alt="Run Qwen3-VL-30B-A3B locally on Mac (MLX) ‚Äî one line of code" src="https://external-preview.redd.it/ZTQzdnV4a2J2NnRmMcJTQaW7wwTs59FSOInw0NP3eGSh3a1OUNFWCE2ilQKv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31ab2bf2f96532320dc7be5bb90732bb9a102f73" title="Run Qwen3-VL-30B-A3B locally on Mac (MLX) ‚Äî one line of code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! Alan from Nexa AI here üëã. Our team just pulled an all-nighter to make it easy for you to run &lt;strong&gt;Qwen3-VL-30B-A3B&lt;/strong&gt; locally on your Mac with &lt;strong&gt;MLX&lt;/strong&gt; ‚Äî no setup headaches, just one line of code&lt;/p&gt; &lt;p&gt;How to get started:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Install NexaSDK with one click: &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;https://github.com/NexaAI/nexa-sdk&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Run this in your terminal: &lt;code&gt;nexa infer NexaAI/qwen3vl-30B-A3B-mlx&lt;/code&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Note: I recommend 64GB of RAM on Mac&lt;/p&gt; &lt;p&gt;We‚Äôll keep adding &lt;strong&gt;Day-0 support&lt;/strong&gt; for any model ‚Äî if you find this useful, a star or follow really helps us keep pushing!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Question for the community:&lt;/strong&gt;&lt;br /&gt; Would you like us to support &lt;strong&gt;GGUF&lt;/strong&gt; for Qwen3-VL-30B-A3B next?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xg1noxkbv6tf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyaf4f/run_qwen3vl30ba3b_locally_on_mac_mlx_one_line_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyaf4f/run_qwen3vl30ba3b_locally_on_mac_mlx_one_line_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T00:42:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykff4</id>
    <title>Save up money or wait for the best GPUs?</title>
    <updated>2025-10-05T10:12:00+00:00</updated>
    <author>
      <name>/u/Adventurous-Nerve858</name>
      <uri>https://old.reddit.com/user/Adventurous-Nerve858</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are the best GPUs to save up money for to run the new local LLMs, TTS, AI Image Gen/Editors, Face Talking, and Video Gen models, like Wan, FantasyTalking, etc? Save up money for H100, H200, multiple RTX 6000 Pros? Or wait a few years and hope consumer grade GPUs get a lot more VRAM or the models become better and more efficient? How much money are we talking for the best, high-end AI workstation that can quickly generate and use all these tools a lot faster than a 3090, 4090 or 5090?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Nerve858"&gt; /u/Adventurous-Nerve858 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykff4/save_up_money_or_wait_for_the_best_gpus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykff4/save_up_money_or_wait_for_the_best_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nykff4/save_up_money_or_wait_for_the_best_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T10:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1ny1vrt</id>
    <title>Qwen3-VL-30B-A3B-Instruct &amp; Thinking are here!</title>
    <updated>2025-10-04T18:40:35+00:00</updated>
    <author>
      <name>/u/Full_Piano_3448</name>
      <uri>https://old.reddit.com/user/Full_Piano_3448</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ny1vrt/qwen3vl30ba3binstruct_thinking_are_here/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here!" src="https://preview.redd.it/bx7mh9pr35tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6ee3330fa6a977cbdba02a9f27a59102d055c1ed" title="Qwen3-VL-30B-A3B-Instruct &amp;amp; Thinking are here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also releasing an FP8 version, plus the FP8 of the massive Qwen3-VL-235B-A22B!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Full_Piano_3448"&gt; /u/Full_Piano_3448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bx7mh9pr35tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ny1vrt/qwen3vl30ba3binstruct_thinking_are_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ny1vrt/qwen3vl30ba3binstruct_thinking_are_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T18:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykfnt</id>
    <title>OrKa 0.9.4 release notes</title>
    <updated>2025-10-05T10:12:25+00:00</updated>
    <author>
      <name>/u/marcosomma-OrKA</name>
      <uri>https://old.reddit.com/user/marcosomma-OrKA</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;What is new&lt;/strong&gt; - Final agent is always logged with &lt;code&gt;[ORKA-FINAL]&lt;/code&gt; - ISO 8601 timestamps remove JSON serialization errors - GraphScout multi hop paths now execute fully with clean context passing - Response builder finalizes output at the end of routed sequences&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why share&lt;/strong&gt; Looking for test cases from folks running multi agent routing or memory nodes. Happy to compare traces and edge cases. - &lt;a href="https://pypi.org/project/orka-reasoning/"&gt;https://pypi.org/project/orka-reasoning/&lt;/a&gt; - &lt;a href="https://github.com/marcosomma/orka-reasoning"&gt;https://github.com/marcosomma/orka-reasoning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/marcosomma-OrKA"&gt; /u/marcosomma-OrKA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykfnt/orka_094_release_notes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykfnt/orka_094_release_notes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nykfnt/orka_094_release_notes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T10:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1ny2w2d</id>
    <title>New Build for local LLM</title>
    <updated>2025-10-04T19:20:06+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ny2w2d/new_build_for_local_llm/"&gt; &lt;img alt="New Build for local LLM" src="https://preview.redd.it/3xz2zcko95tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dae2f2880541d229aee6f421736d1ded6ea02ce3" title="New Build for local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Mac Studio M3 Ultra 512GB RAM 4TB HDD desktop&lt;/p&gt; &lt;p&gt;96core threadripper, 512GB RAM, 4x RTX Pro 6000 Max Q (all at 5.0x16), 16TB 60GBps Raid 0 NVMe LLM Server&lt;/p&gt; &lt;p&gt;Thanks for all the help getting parts selected, getting it booted, and built! It's finally together thanks to the help of the community (here and discord!)&lt;/p&gt; &lt;p&gt;Check out my cozy little AI computing paradise.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3xz2zcko95tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ny2w2d/new_build_for_local_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ny2w2d/new_build_for_local_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T19:20:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykzv3</id>
    <title>Video2X 6.x ‚Äî open-source upscaler + frame interpolation (Anime4K v4 / Real-ESRGAN / Real-CUGAN / RIFE) üöÄ</title>
    <updated>2025-10-05T10:45:43+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt; &lt;img alt="Video2X 6.x ‚Äî open-source upscaler + frame interpolation (Anime4K v4 / Real-ESRGAN / Real-CUGAN / RIFE) üöÄ" src="https://a.thumbs.redditmedia.com/1F_VojrQKMKLZEboBz7yUwIC70X-V0YN2KIWoy4oKr4.jpg" title="Video2X 6.x ‚Äî open-source upscaler + frame interpolation (Anime4K v4 / Real-ESRGAN / Real-CUGAN / RIFE) üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Big C/C++ rewrite with a faster pipeline, &lt;strong&gt;Windows &amp;amp; Linux&lt;/strong&gt; support, and a new Windows GUI installer. Upscale and/or interpolate via Vulkan-powered ncnn backends. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ku6s1j5zv9tf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f08d6adcbe29bb1dca79814ca05296dab76d11"&gt;https://preview.redd.it/ku6s1j5zv9tf1.png?width=2600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e2f08d6adcbe29bb1dca79814ca05296dab76d11&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Engines: &lt;strong&gt;Anime4K v4&lt;/strong&gt;, &lt;strong&gt;Real-ESRGAN&lt;/strong&gt;, &lt;strong&gt;Real-CUGAN&lt;/strong&gt;, &lt;strong&gt;RIFE&lt;/strong&gt;; works for both filtering (upscale) and interpolation. &lt;/li&gt; &lt;li&gt;Easy setup: Windows installer, Linux packages/AppImage, plus Docker/Podman images; Colab notebook available.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://github.com/k4yt3x/video2x"&gt;https://github.com/k4yt3x/video2x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nykzv3/video2x_6x_opensource_upscaler_frame/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T10:45:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxztlx</id>
    <title>gpt-oss 120B is running at 20t/s with $500 AMD M780 iGPU mini PC and 96GB DDR5 RAM</title>
    <updated>2025-10-04T17:20:52+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Everyone here is talking about how great AMD Ryzen AI MAX+ 395 128GB is. But mini PCs with those specs cost almost $2k. I agree the specs are amazing but the price is way high for most local LLM users. I wondered if there was any alternative. My primary purpose was to run gpt-oss 120B at readable speeds.&lt;/p&gt; &lt;p&gt;I searched for mini PCs that supported removable DDR5 sticks and had PCIE4.0 slots for future external GPU upgrades. I focused on AMD CPU/iGPU based setups since Intel specs were not as performant as AMD ones. The iGPU that came before AI MAX 395 (8060S iGPU) was AMD Radeon 890M (still RDNA3.5). Mini PCs with 890M iGPU were still expensive. The cheapest I could find was Minisforum EliteMini AI370 (32GB RAM with 1TB SSD) for $600. Otherwise, these AI 370 based mini PCs are still going for around $1000. However, that was still expensive since I would need to purchase more RAM to run gpt-oss 120B.&lt;/p&gt; &lt;p&gt;Next, I looked at previous generation of AMD iGPUs which are based on RDNA3. I found out AMD Radeon 780M iGPU based mini PC start from $300 for barebone setup (no RAM and no SSD). 780M iGPU based mini PCs are 2x times cheaper and is only 20% behind 890M performance metrics. This was perfect! I checked many online forums if there was ROCm support for 780M. Even though there is no official support for 780M, I found out there were multiple repositories that added ROCm support for 780M (gfx1103) (e.g. arch linux - &lt;a href="https://aur.archlinux.org/packages/rocwmma-gfx1103"&gt;https://aur.archlinux.org/packages/rocwmma-gfx1103&lt;/a&gt; ; Windows - &lt;a href="https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU"&gt;https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU&lt;/a&gt; ; and Ubuntu - &lt;a href="https://github.com/lamikr/rocm_sdk_builder"&gt;https://github.com/lamikr/rocm_sdk_builder&lt;/a&gt; ). Then I bought MINISFORUM UM870 Slim Mini PC barebone for $300 and 2x48GB Crucial DDR5 5600Mhz for $200. I already had 2TB SSD, so I paid $500 in total for this setup.&lt;/p&gt; &lt;p&gt;There was no guidelines on how to install ROCm or allocate most of the RAM for iGPU for 780M. So, I did the research and this is how I did it.&lt;/p&gt; &lt;p&gt;ROCm. The default ROCm 6.4.4 official installation does not work. rocm-smi does not show the iGPU. I installed 6.4.1 and it recognized the iGPU but still gfx1103 tensiles were missing. Overriding HSA_OVERRIDE_GFX_VERSION=11.0.0 did not work. Last working version that recognized this iGPU was ROCm 6.1 based on some posts. But I stopped trying here. Potentially, I could compile and build ROCM SDK Builder 6.1.2 (from lamikr's repo above) but I did not want to spend 4 hours for that.&lt;/p&gt; &lt;p&gt;Then I found out there is a repo called lemonade that ships llama cpp with rocm as release builds. Here: &lt;a href="https://github.com/aigdat/llamacpp-rocm/releases/latest"&gt;https://github.com/aigdat/llamacpp-rocm/releases/latest&lt;/a&gt; . I downloaded gfx110x version e.g. &lt;a href="http://llama-b1068-ubuntu-rocm-gfx110X-x64.zip"&gt;llama-b1068-ubuntu-rocm-gfx110X-x64.zip&lt;/a&gt; . Extracted it. Ran llama-bench with llama2-7b Q4_0 to check its speed and it was working! I was getting 20t/s for it. Not bad! But still I could load gpt-oss 120B. Ubuntu was crashing when I tried to load that model.&lt;/p&gt; &lt;p&gt;Then I searched for iGPU memory allocation. I found this amazing article about iGPU memory allocation (it is called GTT memory): &lt;a href="https://strixhalo-homelab.d7.wtf/AI/AI-Capabilities-Overview#memory-limits"&gt;https://strixhalo-homelab.d7.wtf/AI/AI-Capabilities-Overview#memory-limits&lt;/a&gt; . In short, we create a conf file in modprobe.d folder.&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nano /etc/modprobe.d/amdgpu_llm_optimized.conf&lt;/code&gt;&lt;/p&gt; &lt;p&gt;then add the following lines:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;options amdgpu gttsize=89000 ## 89GB allocated to GTT options ttm pages_limit=23330816 options ttm page_pool_size=23330816 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In grub, we need to also add edit the line that starts with GRUB_CMDLINE_LINUX_DEFAULT (add to the end if it already has some text):&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo nano /etc/default/grub&lt;/code&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;GRUB_CMDLINE_LINUX_DEFAULT=&amp;quot;quiet splash amd_iommu=off transparent_hugepage=always numa_balancing=disable amdttm.pages_limit=23330816 amdttm.page_pool_size=23330816&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then update grub with above changes.&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo update-grub&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Reboot the mini PC.&lt;/p&gt; &lt;p&gt;Also, minimize the VRAM size from the bios settings to 1GB or 512MB.&lt;/p&gt; &lt;p&gt;You can check the GTT size with this command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;sudo dmesg | egrep &amp;quot;amdgpu: .*memory&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You should see something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;[ 3.4] amdgpu 0000:c4:00.0: amdgpu: amdgpu: 1024M of VRAM memory ready [ 3.4] amdgpu 0000:c4:00.0: amdgpu: amdgpu: 89000M of GTT memory ready. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;lemonade compiled llama cpp with ROCm was giving me 18t/s TG and 270t/s PP for gpt-oss 120B in short context (pp512, tg128) but in long context TG suffered (8k context) and I was getting 6t/s. So, I continued with vulkan.&lt;/p&gt; &lt;p&gt;I installed RADV vulkan.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;sudo apt install vulkan-tools libvulkan-dev mesa-vulkan-drivers &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Downloaded the latest release build from llama cpp for vulkan in ubuntu: &lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;https://github.com/ggml-org/llama.cpp/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And finally, I was getting great numbers that aligned with dual DDR5 5600Mhz speeds (~80GB/s).&lt;/p&gt; &lt;p&gt;Enough talking. Here are some metrics.&lt;/p&gt; &lt;p&gt;ROCM with gpt-oss 120B mxfp4&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ml-ai@ai-mini-pc:/media/ml-ai/wd_2tb/llama-b1066-ubuntu-rocm-gfx110X-x64$ HSA_OVERRIDE_GFX_VERSION=11.0.0 ./llama-bench -m /media/ml-ai/wd_2tb/llm_models/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf -mmp 0 -fa 1 &amp;amp;&amp;amp; HSA_OVERRIDE_GFX_VERSION=11.0.0 ./llama-bench -m /media/ml-ai/wd_2tb/llm_models/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf -mmp 0 -fa 1 -d 8192 ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | pp512 | 269.28 ¬± 1.59 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | tg128 | 18.75 ¬± 0.01 | build: 703f9e3 (1) ggml_cuda_init: GGML_CUDA_FORCE_MMQ: no ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no ggml_cuda_init: found 1 ROCm devices: Device 0: AMD Radeon Graphics, gfx1100 (0x1100), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | pp512 @ d8192 | 169.47 ¬± 0.70 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm | 99 | 1 | 0 | tg128 @ d8192 | 6.76 ¬± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;VULKAN (RADV only) all with Flash attention enabled&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# qwen3moe 30B.A3B Q4_1 # llama cpp build: 128d522c (6686) # command used: ml-ai@ai-mini-pc:/media/ml-ai/wd_2tb/minipc/llama-b6686-bin-ubuntu-vulkan-x64$ ./build/bin/llama-bench -m /media/ml-ai/wd_2tb/llm_models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 -fa 1 &amp;amp;&amp;amp; ./build/bin/llama-bench -m /media/ml-ai/wd_2tb/llm_models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 -d 8192 -fa 1 | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | pp512 | 243.33 ¬± 0.92 | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | tg128 | 32.61 ¬± 0.07 | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | pp512 @ d8192 | 105.00 ¬± 0.14 | | qwen3moe 30B.A3B Q4_1 | 17.87 GiB | 30.53 B | RPC,Vulkan | 99 | 1 | 0 | tg128 @ d8192 | 22.29 ¬± 0.08 | # gpt-oss-20b-GGUF | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | pp512 | 355.13 ¬± 2.79 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | tg128 | 28.08 ¬± 0.09 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | pp512 @ d8192 | 234.17 ¬± 0.34 | | gpt-oss 20B MXFP4 MoE | 11.27 GiB | 20.91 B | RPC,Vulkan | 99 | 1 | 0 | tg128 @ d8192 | 24.86 ¬± 0.07 | # gpt-oss-120b-GGUF | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | pp512 | 137.60 ¬± 0.70 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | tg128 | 20.43 ¬± 0.01 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | pp512 @ d8192 | 106.22 ¬± 0.24 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | RPC,Vulkan | 99 | 1 | 0 | tg128 @ d8192 | 18.09 ¬± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;QWEN3 235B Q3_K_XL (unsloth)&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ml-ai@ai-mini-pc:/media/ml-ai/wd_2tb/minipc/llama-b6686-bin-ubuntu-vulkan-x64$ AMD_VULKAN_ICD=RADV ./build/bin/llama-bench -m /media/ml-ai/wd_2tb/llm_models/Qwen3-235B-A22B-Instruct-2507-GGUF/UD-Q3_K_XL/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf -ncmoe 20 load_backend: loaded RPC backend from /media/ml-ai/wd_2tb/minipc/llama-b6686-bin-ubuntu-vulkan-x64/build/bin/libggml-rpc.so ggml_vulkan: Found 1 Vulkan devices: ggml_vulkan: 0 = AMD Radeon Graphics (RADV PHOENIX) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat load_backend: loaded Vulkan backend from /media/ml-ai/wd_2tb/minipc/llama-b6686-bin-ubuntu-vulkan-x64/build/bin/libggml-vulkan.so load_backend: loaded CPU backend from /media/ml-ai/wd_2tb/minipc/llama-b6686-bin-ubuntu-vulkan-x64/build/bin/libggml-cpu-icelake.so | model | size | params | backend | ngl | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | --------------: | -------------------: | | qwen3moe 235B.A22B Q3_K - Medium | 96.99 GiB | 235.09 B | RPC,Vulkan | 99 | pp512 | 19.13 ¬± 0.81 | | qwen3moe 235B.A22B Q3_K - Medium | 96.99 GiB | 235.09 B | RPC,Vulkan | 99 | tg128 | 4.31 ¬± 0.28 | build: 128d522c (6686) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GLM4.5 air Q4_1 metrics&lt;/p&gt; &lt;pre&gt;&lt;code&gt;| model | size | params | backend | ngl | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: | | glm4moe 106B.A12B Q4_1 | 64.49 GiB | 110.47 B | RPC,Vulkan | 99 | 1 | pp512 | 78.32 ¬± 0.45 | | glm4moe 106B.A12B Q4_1 | 64.49 GiB | 110.47 B | RPC,Vulkan | 99 | 1 | tg128 | 9.06 ¬± 0.02 | build: 128d522c (6686) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;idle power: ~4-5W&lt;/p&gt; &lt;p&gt;peak power when generating text: ~80W&lt;/p&gt; &lt;p&gt;I know ROCm support is not great but vulkan is better at text generation for most models (even though it is 2x slower for prompt processing than ROCm).&lt;/p&gt; &lt;p&gt;Mini PCs with 780M are great value and enables us to run large MoE models at acceptable speeds. Overall, this mini PC is more than enough for my daily LLM usage (mostly asking math/CS related questions, coding and brainstorming).&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Update: added qwen3 235B and GLM AIR 4.5 metrics.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxztlx/gptoss_120b_is_running_at_20ts_with_500_amd_m780/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T17:20:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxxeu1</id>
    <title>Why are AI labs in China not focused on creating new search engines?</title>
    <updated>2025-10-04T15:45:31+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxxeu1/why_are_ai_labs_in_china_not_focused_on_creating/"&gt; &lt;img alt="Why are AI labs in China not focused on creating new search engines?" src="https://preview.redd.it/4glawt4k84tf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b93923ea011b8fdf60fa2c2f9929d68ed20ad498" title="Why are AI labs in China not focused on creating new search engines?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4glawt4k84tf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxxeu1/why_are_ai_labs_in_china_not_focused_on_creating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxxeu1/why_are_ai_labs_in_china_not_focused_on_creating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T15:45:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nykevr</id>
    <title>Sneak Preview: Ollama Bench</title>
    <updated>2025-10-05T10:11:04+00:00</updated>
    <author>
      <name>/u/phantagom</name>
      <uri>https://old.reddit.com/user/phantagom</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykevr/sneak_preview_ollama_bench/"&gt; &lt;img alt="Sneak Preview: Ollama Bench" src="https://preview.redd.it/0nec59t9p9tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1da048e9acbe9b8959f540c8d86b301f7837805c" title="Sneak Preview: Ollama Bench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A sneak preview, I need to deploy a clustered Ollama setup, needed some benchmarking, tools I found didn't do what I want, created this. When finished, we be released on github. &lt;/p&gt; &lt;p&gt;Core Benchmarking Features&lt;/p&gt; &lt;p&gt;- Parallel request execution - Launch many requests concurrently to one or more models&lt;/p&gt; &lt;p&gt;- Multiple model testing - Compare performance across different models simultaneously&lt;/p&gt; &lt;p&gt;- Request metrics - Measures per-request wall-clock time, latency percentiles (p50/p95/p99)&lt;/p&gt; &lt;p&gt;- Time-to-first-token (TTFT) - Measures streaming responsiveness when using --stream&lt;/p&gt; &lt;p&gt;- Dual endpoints - Supports both generate and chat (with --chat flag) endpoints&lt;/p&gt; &lt;p&gt;- Token counting - Tracks prompt tokens, output tokens, and calculates tokens/sec throughput&lt;/p&gt; &lt;p&gt;Workload Configuration&lt;/p&gt; &lt;p&gt;- Flexible prompts - Use inline prompt, prompt file, or JSONL file with multiple prompts&lt;/p&gt; &lt;p&gt;- Variable substitution - Template variables in prompts with --variables (supports file injection)&lt;/p&gt; &lt;p&gt;- System messages - Set system prompts for chat mode with --system&lt;/p&gt; &lt;p&gt;- Warmup requests - Optional warmup phase with --warmup to load models before measurement&lt;/p&gt; &lt;p&gt;- Shuffle mode - Randomize request order with --shuffle for load mixing&lt;/p&gt; &lt;p&gt;- Concurrency control - Set max concurrent requests with --concurrency&lt;/p&gt; &lt;p&gt;- Per-model fairness - Automatic concurrency distribution across multiple models&lt;/p&gt; &lt;p&gt;Real-time TUI Display (--tui)&lt;/p&gt; &lt;p&gt;- Live metrics dashboard - Real-time progress, throughput (req/s), latency, token stats&lt;/p&gt; &lt;p&gt;- Per-model breakdown - Individual stats table for each model with token throughput&lt;/p&gt; &lt;p&gt;- Active requests monitoring - Shows in-flight requests with elapsed time and token counts&lt;/p&gt; &lt;p&gt;- Error log panel - Displays recent errors with timestamps and details&lt;/p&gt; &lt;p&gt;- Live token preview - Press [p] to see streaming content from active requests (up to 4 requests)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phantagom"&gt; /u/phantagom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0nec59t9p9tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nykevr/sneak_preview_ollama_bench/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nykevr/sneak_preview_ollama_bench/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T10:11:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nypq6q</id>
    <title>Made the first .NET wrapper for Apple MLX - looking for feedback!</title>
    <updated>2025-10-05T14:30:45+00:00</updated>
    <author>
      <name>/u/csharp-agent</name>
      <uri>https://old.reddit.com/user/csharp-agent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Short story: I'm a .NET enthusiast and recently got excited about MLX. Thought - why not marry these two technologies?&lt;/p&gt; &lt;p&gt;That's how &lt;a href="https://github.com/managedcode/MLXSharp"&gt;&lt;strong&gt;MLXSharp&lt;/strong&gt;&lt;/a&gt; was born - the first proper .NET wrapper for MLX that also integrates with Microsoft.Extensions.AI.&lt;/p&gt; &lt;p&gt;What it can do:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Works as IChatClient and IEmbeddingGenerator&lt;/li&gt; &lt;li&gt;Dependency Injection and Semantic Kernel support&lt;/li&gt; &lt;li&gt;Ready-to-use bindings for macOS and Linux&lt;/li&gt; &lt;li&gt;.NET 9 / C# 13 friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is my first open-source project of this scale. Would really appreciate any feedback - from architecture to documentation. Especially interested in hearing from folks working with ML on .NET or those with native interop experience.&lt;/p&gt; &lt;p&gt;If anyone wants to test it on their M1/M2/M3 Mac - would love to hear your thoughts!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/managedcode/MLXSharp"&gt;https://github.com/managedcode/MLXSharp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/csharp-agent"&gt; /u/csharp-agent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nypq6q/made_the_first_net_wrapper_for_apple_mlx_looking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T14:30:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxw08b</id>
    <title>Quite accurate</title>
    <updated>2025-10-04T14:50:10+00:00</updated>
    <author>
      <name>/u/Komarov_d</name>
      <uri>https://old.reddit.com/user/Komarov_d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"&gt; &lt;img alt="Quite accurate" src="https://preview.redd.it/9lms1idpy3tf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7e75e0118f3c3580e1e97fa7a4bda42ad4520a7" title="Quite accurate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Komarov_d"&gt; /u/Komarov_d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9lms1idpy3tf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxw08b/quite_accurate/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T14:50:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nxzx6t</id>
    <title>GPT-1 Thinking 2.6m coming soon</title>
    <updated>2025-10-04T17:24:39+00:00</updated>
    <author>
      <name>/u/Creative-Ad-2112</name>
      <uri>https://old.reddit.com/user/Creative-Ad-2112</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"&gt; &lt;img alt="GPT-1 Thinking 2.6m coming soon" src="https://preview.redd.it/2ln0mw87m4tf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=409621b267ee5b6f05466d87ab6e4ace420dcc56" title="GPT-1 Thinking 2.6m coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Took my GPT-1 from past posts, gave it roughly 600 million tokens from Nemotron's pretraining dataset.&lt;br /&gt; Then finetuned it on my own private thinking dataset, used sonnet 4.5 to help me come up with it, essentially to help the model 'see' the pattern of the thinking rather than just random q/a with thinking in it.&lt;br /&gt; Used a custom tokenizer trained from scratch.&lt;/p&gt; &lt;p&gt;As for the image attached, I did it for the lolz, hope you laughed but i did test it out on some of those metrics (simple ones lol). As for the model generation, here it is lolol.&lt;/p&gt; &lt;p&gt;[PROMPT] &amp;lt;|user|&amp;gt; What are two fun facts about octopuses? &amp;lt;|think|&amp;gt; &lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; What are two fun fact##s about o##ct##op##uses? &amp;lt;|think|&amp;gt; 1. The user is asking for a log##ical question. 2. The user is asking about a user is asking for the simple and is asking for a specific, and a 'F##P##I'##. 3. I should explain the answer. 2. I should explain a small process: 3. I should have a great step is to read. 4. I should provide a specific questions: a simple, but it's specific, the user is to be able to ask. &amp;lt;|assistant|&amp;gt; 1) They have three hea##rts and blue, copper-##based blood. 2) They can insta##ntly camou##flage by changing col##or and tex##ture, and their suck##ers can ‚Äútas##te‚Äù what they touch. &amp;lt;|eot|&amp;gt; &lt;/p&gt; &lt;p&gt;--- [PROMPT] &amp;lt;|user|&amp;gt; Write a haiku about rain. &amp;lt;|think|&amp;gt;&lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; W##rite a ha##ik##u about rain. &amp;lt;|think|&amp;gt; 1. The user wants of list a specific sentence. 2. I should include the user for the ##n common. I should explain the simple, a poem, and a simple, and understand##ing. &amp;lt;|eot|&amp;gt; &amp;lt;|assistant|&amp;gt; Rain whis##pers on roofs Pud##dles bloom in soft lamp##light Earth brea##thes, rin##sed anew. &amp;lt;|eot|&amp;gt; &lt;/p&gt; &lt;p&gt;--- [PROMPT] &amp;lt;|user|&amp;gt; Explain photosynthesis in one sentence. &amp;lt;|think|&amp;gt;&lt;/p&gt; &lt;p&gt;[OUTPUT] &amp;lt;|user|&amp;gt; Expl##ain photos##ynt##hesis in one sentence. &amp;lt;|think|&amp;gt; 1. The user is the user is asking about a simple for a explanation. The user is the question. 2. The user is ask##ing a great question is a clear, it's a specific example. 4. I will explain the best way for the answer, 'a##m##aking is asking for the sound and ask about the order. 2. The user is asking for a simple, for the common question. 3. I should explain the words: ##g##ood &amp;lt;|assistant|&amp;gt; Photo##syn##thesis is the pro##cess by which plants algae, and some bac##teria use sun##light to con##vert a water and car##bon dio##xide into a glu##cose, relea##sing a oxy##gen. &amp;lt;|eot|&amp;gt;&lt;/p&gt; &lt;p&gt;As you can see its pretty good for a 2 mil parameter. Now you might be wondering that something is up, what's the catch? Well, obviously I didn't use GPT-1, I used their original implementation, converted it to pytorch, and then added differential attention, along with sparse attention.&lt;br /&gt; But that is still not enough, which is why I introduce two variants of diff_attn. &lt;/p&gt; &lt;p&gt;[model] params=2,494,574&lt;br /&gt; [model] layer_types=['dense', 'diff_sparse', 'sparse', 'diff_dense', 'sparse', 'diff_sparse', 'dense', 'sparse', 'diff_dense', 'sparse', 'diff_sparse', 'dense', 'sparse', 'diff_sparse', 'diff_dense', 'dense'] &lt;/p&gt; &lt;p&gt;I have found this to be effective. I kept the GPT-1 like core, gave it moe (but didn't use moe in this model run btw), then I introduced it to these two diff attn and intertwined it with the others. &lt;/p&gt; &lt;p&gt;So is it GPT-1? Nope, it's GPT-1 like (for clarification), abs positioning and pre-lm instead of the modern day post-lm + RoPE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Creative-Ad-2112"&gt; /u/Creative-Ad-2112 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ln0mw87m4tf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nxzx6t/gpt1_thinking_26m_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-04T17:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyhjbc</id>
    <title>Qwen3-VL-30B-A3B-Thinking GGUF with llama.cpp patch to run it</title>
    <updated>2025-10-05T07:10:16+00:00</updated>
    <author>
      <name>/u/Main-Wolverine-1042</name>
      <uri>https://old.reddit.com/user/Main-Wolverine-1042</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"&gt; &lt;img alt="Qwen3-VL-30B-A3B-Thinking GGUF with llama.cpp patch to run it" src="https://external-preview.redd.it/TAJQqYYwoN5psMtPeyiTjHyAIkVOvuAON4rUlXw9Vuc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1852409ce02e2d25ecbf839a49477903b842a7b" title="Qwen3-VL-30B-A3B-Thinking GGUF with llama.cpp patch to run it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/rsimr0s5t8tf1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78bae97847f836ea3c715504082caa5c8e93de9e"&gt;https://preview.redd.it/rsimr0s5t8tf1.png?width=1497&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=78bae97847f836ea3c715504082caa5c8e93de9e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example how to run it with vision support: &lt;strong&gt;--mmproj mmproj-Qwen3-VL-30B-A3B-F16.gguf --jinja&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF"&gt;https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF&lt;/a&gt; - First time giving this a shot‚Äîplease go easy on me!&lt;/p&gt; &lt;p&gt;here a link to llama.cpp patch &lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/blob/main/qwen3vl-implementation.patch"&gt;https://huggingface.co/yairpatch/Qwen3-VL-30B-A3B-Thinking-GGUF/blob/main/qwen3vl-implementation.patch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;how to apply the patch: &lt;strong&gt;git apply qwen3vl-implementation.patch&lt;/strong&gt; in the main llama directory.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Wolverine-1042"&gt; /u/Main-Wolverine-1042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyhjbc/qwen3vl30ba3bthinking_gguf_with_llamacpp_patch_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T07:10:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyd512</id>
    <title>vLLM + Qwen-3-VL-30B-A3B is so fast</title>
    <updated>2025-10-05T03:00:12+00:00</updated>
    <author>
      <name>/u/Striking-Warning9533</name>
      <uri>https://old.reddit.com/user/Striking-Warning9533</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am doing image captioning, and I got this speed:&lt;/p&gt; &lt;p&gt;Avg prompt throughput: 549.0 tokens/s, Avg generation throughput: 357.8 tokens/s, Running: 7 reqs, Waiting: 1 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 49.5%&lt;/p&gt; &lt;p&gt;the GPU is a H100 PCIe&lt;br /&gt; This is the model I used (AWQ) &lt;a href="https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ"&gt;https://huggingface.co/QuantTrio/Qwen3-VL-30B-A3B-Instruct-AWQ&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am processing large number of images, and most platforms will rate limit them so I have to run locally. I am running mutli process locally on single GPU&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking-Warning9533"&gt; /u/Striking-Warning9533 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyd512/vllm_qwen3vl30ba3b_is_so_fast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyd512/vllm_qwen3vl30ba3b_is_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyd512/vllm_qwen3vl30ba3b_is_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T03:00:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nynsxt</id>
    <title>Is it time to download the Deepseek/Kimi weights even if we can't run them?</title>
    <updated>2025-10-05T13:10:10+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given the uptick in articles claiming Deepseek is a threat, it's not crazy to predict that it gets banned in the near future if you live in the USA and maybe some other Western countries.&lt;/p&gt; &lt;p&gt;And yeah, there's torrents, but if it gets classified as a &lt;em&gt;THREAT&lt;/em&gt; (ridiculous ) the risk of downloading could be far different than, say, not wanting to pay for Shrek 2 and sailing the seas for it.&lt;/p&gt; &lt;p&gt;So I'm curious if there's any storage-rich preppers out there who have downloaded the weights for some of these massive models out of an abundance of caution.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nynsxt/is_it_time_to_download_the_deepseekkimi_weights/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nynsxt/is_it_time_to_download_the_deepseekkimi_weights/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nynsxt/is_it_time_to_download_the_deepseekkimi_weights/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T13:10:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyratf</id>
    <title>Hunyuan Image 3.0 Jumps to No.1 on LMArena‚Äôs Text-to-Image Leaderboard</title>
    <updated>2025-10-05T15:32:14+00:00</updated>
    <author>
      <name>/u/yogthos</name>
      <uri>https://old.reddit.com/user/yogthos</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tencent/HunyuanImage-3.0"&gt;https://huggingface.co/tencent/HunyuanImage-3.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmarena.ai/leaderboard/text-to-image"&gt;https://lmarena.ai/leaderboard/text-to-image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yogthos"&gt; /u/yogthos &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyratf/hunyuan_image_30_jumps_to_no1_on_lmarenas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:32:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyqog8</id>
    <title>Developers who use META AI lol.</title>
    <updated>2025-10-05T15:08:09+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqog8/developers_who_use_meta_ai_lol/"&gt; &lt;img alt="Developers who use META AI lol." src="https://preview.redd.it/2p7qaw5u6btf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf422c668b342ba132e44daffb0701e8cb911da5" title="Developers who use META AI lol." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And no disrespect to META AI open models. They were one of the first to make their models available publicly. &lt;/p&gt; &lt;p&gt;Can‚Äôt crosspost but here‚Äôs the OP: &lt;a href="https://www.reddit.com/r/ProgrammerHumor/s/O1tXgRqKrr"&gt;https://www.reddit.com/r/ProgrammerHumor/s/O1tXgRqKrr&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2p7qaw5u6btf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqog8/developers_who_use_meta_ai_lol/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqog8/developers_who_use_meta_ai_lol/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyopyc</id>
    <title>Did anyone try out GLM-4.5-Air-GLM-4.6-Distill ?</title>
    <updated>2025-10-05T13:49:38+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/BasedBase/GLM-4.5-Air-GLM-4.6-Distill"&gt;https://huggingface.co/BasedBase/GLM-4.5-Air-GLM-4.6-Distill&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;GLM-4.5-Air-GLM-4.6-Distill represents an advanced distillation of the GLM-4.6 model into the efficient GLM-4.5-Air architecture. Through a SVD-based knowledge transfer methodology, this model inherits the sophisticated reasoning capabilities and domain expertise of its 92-layer, 160-expert teacher while maintaining the computational efficiency of the 46-layer, 128-expert student architecture.&amp;quot;&lt;/p&gt; &lt;p&gt;Distillation scripts are public: &lt;a href="https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts"&gt;https://github.com/Basedbase-ai/LLM-SVD-distillation-scripts&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyopyc/did_anyone_try_out_glm45airglm46distill/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T13:49:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nyqkkm</id>
    <title>Apple has added significant AI-acceleration to its A19 CPU cores</title>
    <updated>2025-10-05T15:03:50+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"&gt; &lt;img alt="Apple has added significant AI-acceleration to its A19 CPU cores" src="https://preview.redd.it/ti22axwj5btf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=967e4aea50a8298df5070520a6bc78e77ecbcfb7" title="Apple has added significant AI-acceleration to its A19 CPU cores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Data source: &lt;a href="https://ai-benchmark.com/ranking_processors_detailed.html"&gt;https://ai-benchmark.com/ranking_processors_detailed.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We also might see these advances back in the M5.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ti22axwj5btf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nyqkkm/apple_has_added_significant_aiacceleration_to_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T15:03:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nylc3q</id>
    <title>NIST evaluates Deepseek as unsafe. Looks like the battle to discredit opensource is underway</title>
    <updated>2025-10-05T11:05:46+00:00</updated>
    <author>
      <name>/u/Nobby_Binks</name>
      <uri>https://old.reddit.com/user/Nobby_Binks</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nobby_Binks"&gt; /u/Nobby_Binks &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.techrepublic.com/article/news-deepseek-security-gaps-caisi-study/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nylc3q/nist_evaluates_deepseek_as_unsafe_looks_like_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nylc3q/nist_evaluates_deepseek_as_unsafe_looks_like_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-05T11:05:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect ‚Äî Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect ‚Äî Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM ‚Äì 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
