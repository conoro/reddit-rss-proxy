<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-17T21:05:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nit4v6</id>
    <title>Granite 4 release today? Collection updated with 8 private repos.</title>
    <updated>2025-09-16T20:40:36+00:00</updated>
    <author>
      <name>/u/ironwroth</name>
      <uri>https://old.reddit.com/user/ironwroth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt; &lt;img alt="Granite 4 release today? Collection updated with 8 private repos." src="https://preview.redd.it/ihwp4dy78lpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=310d508b27499694f225a40decad5893a979dfda" title="Granite 4 release today? Collection updated with 8 private repos." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ironwroth"&gt; /u/ironwroth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ihwp4dy78lpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nit4v6/granite_4_release_today_collection_updated_with_8/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T20:40:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1njcpok</id>
    <title>Cline --&gt; Qwen3-Coder tool calling fix</title>
    <updated>2025-09-17T13:09:06+00:00</updated>
    <author>
      <name>/u/jrodder</name>
      <uri>https://old.reddit.com/user/jrodder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I jumped into the AI assisted coding world about 5 weeks ago. Been doing the normal &amp;quot;download all the models and tinker&amp;quot; thing I am sure we all did. I have settled on Qwen3-Coder 30B as the best model for local use for now, as many have. Mainly it was because I use VSCode and Cline for the most part. It mostly worked, until a specific tool call and then it broke. Not the end of the world but also annoying. Did more research, and it seems like Qwen3-Coder was using it's own format, and Cline is using XML. Figured it might be worth an experiment, and I am pretty sure it works well. Hasn't failed a tool call yet although to be fair I didn't put it through the ringer. Maybe this saves someone else some time.&lt;/p&gt; &lt;p&gt;&lt;a href="https://drive.google.com/file/d/1P4B3K7Cz4rQ2TCf1XiW8ZMZbjioPIZty/view?usp=drive_link"&gt;https://drive.google.com/file/d/1P4B3K7Cz4rQ2TCf1XiW8ZMZbjioPIZty/view?usp=drive_link&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Qwen Wrapper for Cline&lt;/h1&gt; &lt;h2&gt;Overview&lt;/h2&gt; &lt;p&gt;This wrapper allows Cline, a VS Code plugin with a strong affinity for Anthropic's chat format, to work with local Qwen models. It acts as a bidirectional translator between Anthropic-style tool calls and Qwen's custom XML format, enabling seamless integration of local Qwen models with Cline.&lt;/p&gt; &lt;h2&gt;Features&lt;/h2&gt; &lt;ul&gt; &lt;li&gt; &lt;strong&gt;Request Translation:&lt;/strong&gt; Converts Anthropic-style tool definitions (XML) into the JSON format expected by Qwen.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Response Translation:&lt;/strong&gt; Translates Qwen's tool call responses (custom XML or OpenAI-style JSON) into the Anthropic-style &lt;code&gt;&amp;lt;invoke&amp;gt;&lt;/code&gt; format that Cline understands.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Local and Docker Support:&lt;/strong&gt; Can be run as a local Python script or as a self-contained Docker container.&lt;/li&gt; &lt;li&gt; &lt;strong&gt;Easy Configuration:&lt;/strong&gt; Can be configured using environment variables for easy deployment.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;How It Works&lt;/h2&gt; &lt;p&gt;The wrapper is a Flask application that sits between Cline and a local &lt;code&gt;llama-server&lt;/code&gt; instance running a Qwen model. It intercepts requests from Cline, translates them into a format that the Qwen model can understand, and then forwards them to the &lt;code&gt;llama-server&lt;/code&gt;. When the &lt;code&gt;llama-server&lt;/code&gt; responds, the wrapper translates the response back into a format that Cline can understand.&lt;/p&gt; &lt;h3&gt;Request Translation (Cline → Qwen)&lt;/h3&gt; &lt;ol&gt; &lt;li&gt; The wrapper receives a request from Cline containing an Anthropic-style &lt;code&gt;&amp;lt;tools&amp;gt;&lt;/code&gt; XML block in the system prompt.&lt;/li&gt; &lt;li&gt; It parses the XML block to extract the tool definitions.&lt;/li&gt; &lt;li&gt; It converts the tool definitions into the JSON format expected by Qwen.&lt;/li&gt; &lt;li&gt; It removes the XML block from the original prompt.&lt;/li&gt; &lt;li&gt; It forwards the translated request to the &lt;code&gt;llama-server&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Response Translation (Qwen → Cline)&lt;/h3&gt; &lt;ol&gt; &lt;li&gt; The wrapper receives a response from the &lt;code&gt;llama-server&lt;/code&gt;.&lt;/li&gt; &lt;li&gt; It detects whether the response is a standard text response, a Qwen-style tool call (&lt;code&gt;&amp;lt;tool_call&amp;gt;&lt;/code&gt;), or an OpenAI-style tool call (JSON).&lt;/li&gt; &lt;li&gt; If the response is a tool call, it translates it into the Anthropic-style &lt;code&gt;&amp;lt;invoke&amp;gt;&lt;/code&gt; XML format.&lt;/li&gt; &lt;li&gt; It returns the translated response to Cline.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Local Usage&lt;/h2&gt; &lt;p&gt;To run the wrapper locally, you need to have Python and the required dependencies installed.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Install Dependencies:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash pip install -r requirements.txt &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Configure Paths:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Edit the &lt;code&gt;qwen_wrapper.py&lt;/code&gt; file and update the following variables to point to your &lt;code&gt;llama-server&lt;/code&gt; executable and Qwen model file:&lt;/p&gt; &lt;p&gt;&lt;code&gt;python LLAMA_SERVER_EXECUTABLE = &amp;quot;/path/to/your/llama-server&amp;quot; MODEL_PATH = &amp;quot;/path/to/your/qwen/model.gguf&amp;quot; &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Run the Wrapper:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash python qwen_wrapper.py &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The wrapper will start on &lt;code&gt;http://localhost:8000&lt;/code&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Docker Usage&lt;/h2&gt; &lt;p&gt;To run the wrapper in a Docker container, you need to have Docker installed.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Place Files:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Place the following files in the same directory:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt;&lt;code&gt;* `Dockerfile` * `qwen_wrapper_docker.py` * `requirements.txt` * Your `llama-server` executable * Your Qwen model file (renamed to `model.gguf`) &lt;/code&gt;&lt;/pre&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Build the Image:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Open a terminal in the directory containing the files and run the following command to build the Docker image:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash docker build -t qwen-wrapper . &lt;/code&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Run the Container:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Once the image is built, run the following command to start the container:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash docker run -p 8000:8000 -p 8001:8001 qwen-wrapper &lt;/code&gt;&lt;/p&gt; &lt;p&gt;This will start the container and map both ports 8000 and 8001 on your host machine to the corresponding ports in the container. Port 8000 is for the wrapper API, and port 8001 is for the internal llama-server communication.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Connect Cline:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can then configure Cline to connect to &lt;code&gt;http://localhost:8000&lt;/code&gt;. The wrapper will now also accept connections from other hosts on your network using your machine's IP address.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Configuration&lt;/h2&gt; &lt;p&gt;The wrapper can be configured using the following environment variables when running in Docker:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;code&gt;LLAMA_SERVER_EXECUTABLE&lt;/code&gt;: The path to the &lt;code&gt;llama-server&lt;/code&gt; executable inside the container. Defaults to &lt;code&gt;/app/llama-server&lt;/code&gt;.&lt;/li&gt; &lt;li&gt; &lt;code&gt;MODEL_PATH&lt;/code&gt;: The path to the Qwen model file inside the container. Defaults to &lt;code&gt;/app/model.gguf&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;When running locally, these paths can be configured by editing the &lt;code&gt;qwen_wrapper.py&lt;/code&gt; file directly.&lt;/p&gt; &lt;h2&gt;Network Connectivity&lt;/h2&gt; &lt;p&gt;The wrapper now supports external connections from other hosts on your network. When running locally, the service will be accessible via: - &lt;code&gt;http://localhost:8000&lt;/code&gt; (local access) - &lt;code&gt;http://YOUR_MACHINE_IP:8000&lt;/code&gt; (external access from other hosts)&lt;/p&gt; &lt;p&gt;Make sure your firewall allows connections on port 8000 if you want to access the service from other machines.&lt;/p&gt; &lt;p&gt;flask==3.0.0 requests==2.31.0 waitress==2.1.2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jrodder"&gt; /u/jrodder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njcpok/cline_qwen3coder_tool_calling_fix/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njcpok/cline_qwen3coder_tool_calling_fix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njcpok/cline_qwen3coder_tool_calling_fix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T13:09:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkp7q</id>
    <title>A Quick Look At The AMD Instinct MI355X With ROCm 7.0</title>
    <updated>2025-09-17T18:10:09+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Instinct MI355X is coming to market. 288GB HBM3E memory, 8TB/s bandwidth, and expanded FP6 and FP4 datatype support. Phoronix had a limited hands-on:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Yesterday I was invited along with a small group of others to try out the AMD Instinct MI355X accelerator down in Austin, Texas. The AMD Instinct MI355X is fully supported with the newly-released AMD ROCm 7.0.&lt;/p&gt; &lt;p&gt;The AMD Instinct MI355X &amp;quot;hands on&amp;quot; yesterday to celebrate ROCm 7.0 and the MI350X/MI355X hardware ended up being just following a guided Jupyter Notebook for an AI demo... And one that wasn't even performance-related or anything unique to the AMD Instinct MI350 series capabilities. Not quite the hands-on time expected with originally hoping there would be enough time to tap some MI355X accelerators unconstrained and run some AI/LLM benchmarks at least with Llama.cpp and vLLM. Nevertheless via Jupyter Notebook's terminal allowed for poking at the MI355X on ROCm 7.0 during this demo session.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-Instinct-MI355X-ROCm-7.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkp7q/a_quick_look_at_the_amd_instinct_mi355x_with_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njkp7q/a_quick_look_at_the_amd_instinct_mi355x_with_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:10:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj8hee</id>
    <title>[Release] DASLab GGUF Non-Uniform Quantization Toolkit</title>
    <updated>2025-09-17T09:32:39+00:00</updated>
    <author>
      <name>/u/Loginhe</name>
      <uri>https://old.reddit.com/user/Loginhe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt; &lt;img alt="[Release] DASLab GGUF Non-Uniform Quantization Toolkit" src="https://a.thumbs.redditmedia.com/z81IhWllCbQrFrgfgKe5CFHfXTjYN85Bz1DiPDLtGE0.jpg" title="[Release] DASLab GGUF Non-Uniform Quantization Toolkit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to release the &lt;strong&gt;first open-source toolkit&lt;/strong&gt; that brings &lt;strong&gt;GPTQ + EvoPress&lt;/strong&gt; to the &lt;strong&gt;GGUF format&lt;/strong&gt;, enabling &lt;em&gt;heterogeneous quantization&lt;/em&gt; based on importance.&lt;br /&gt; &lt;strong&gt;Delivering Higher-quality models, same file size.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;What's inside&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2210.17323"&gt;&lt;strong&gt;GPTQ (ICLR '23)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;quantization with GGUF export:&lt;/strong&gt; delivers error-correcting calibration for improved performance&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/pdf/2410.14649"&gt;&lt;strong&gt;EvoPress (ICML '25)&lt;/strong&gt;&lt;/a&gt;: runs evolutionary search to automatically discover optimal per-layer quantization configs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model assembly tools:&lt;/strong&gt; package models to be fully functional with llama.cpp&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;Unlike standard uniform quantization, our toolkit &lt;strong&gt;optimizes precision where it matters most&lt;/strong&gt;.&lt;br /&gt; Critical layers (e.g. attention) can use higher precision, while others (e.g. FFN) compress more aggressively.&lt;br /&gt; With &lt;strong&gt;EvoPress search + GPTQ quantization&lt;/strong&gt;, these trade-offs are discovered automatically.&lt;/p&gt; &lt;p&gt;Our intent is providing an &lt;strong&gt;open source implementation of GGUF dynamic quantization&lt;/strong&gt; that enables non-uniform bitwidth optimization. This previously existed only in proprietary tools and fills a gap for the community, allowing lossless or near-lossless models at low bit-widths with OSS methods.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;Below are zero-shot evaluations. Full benchmark results are available in the repo.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3eg7rp0vyopf1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6590f70e8abf59f3442df57321eaa55ea85ba9c"&gt;https://preview.redd.it/3eg7rp0vyopf1.png?width=3569&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c6590f70e8abf59f3442df57321eaa55ea85ba9c&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/IST-DASLab/gptq-gguf-toolkit"&gt;DASLab GGUF Quantization Toolkit (GitHub Repo Link)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are happy to get feedback, contributions, and experiments!&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit: added clarification&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loginhe"&gt; /u/Loginhe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj8hee/release_daslab_gguf_nonuniform_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T09:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj4axf</id>
    <title>Thread for CPU-only LLM performance comparison</title>
    <updated>2025-09-17T05:08:52+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I could not find any recent posts about CPU only performance comparison of different CPUs. With recent advancements in CPUs, we are seeing incredible memory bandwidth speeds with DDR5 6400 12 channel EPYC 9005 (614.4 GB/s theoretical bw). AMD also announced that Zen 6 CPUs will have 1.6TB/s memory bw. The future of CPUs looks exciting. But for now, I wanted to test what we already have. I need your help to see where we stand with CPUs currently.&lt;/p&gt; &lt;p&gt;For this CPU only comparison, I want to use ik_llama - &lt;a href="https://github.com/ikawrakow/ik_llama.cpp"&gt;https://github.com/ikawrakow/ik_llama.cpp&lt;/a&gt; . I compiled and tested both ik_llama and llama.cpp with MoE models like Qwen3 30B3A Q4_1, gpt-oss 120B Q8 and qwen3 235B Q4_1. ik_llama is at least 2x faster prompt processing (PP) and 50% faster in text generation (TG).&lt;/p&gt; &lt;p&gt;For this benchmark, I used Qwen3 30B3A Q4_1 (19.2GB) and ran ik_llama in Ubuntu 24.04.3.&lt;/p&gt; &lt;p&gt;ik_llama installation:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/ikawrakow/ik_llama.cpp.git cd ik_llama.cpp cmake -B build cmake --build build --config Release -j $(nproc) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;llama-bench benchmark (make sure GPUs are disabled with CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; just in case if you compiled for GPUs):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; ./build/bin/llama-bench -m /media/ai-llm/wd_2t/models/Qwen3-30B-A3B-Q4_1.gguf -mmp 0 --threads 32 | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | ------------: | ---------------: | | qwen3moe ?B Q4_1 | 17.87 GiB | 30.53 B | CPU | 32 | 0 | pp512 | 263.02 ± 2.53 | | qwen3moe ?B Q4_1 | 17.87 GiB | 30.53 B | CPU | 32 | 0 | tg128 | 38.98 ± 0.16 | build: 6d2e7ca4 (3884) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GPT-OSS 120B:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=&amp;quot;&amp;quot; ./build/bin/llama-bench -m /media/ai-llm/wd_2t/models/GPT_OSS_120B_UD-Q8_K_XL/gpt-oss-120b-UD-Q8_K_XL-00001-of-00002.gguf -mmp 0 --threads 32 | model | size | params | backend | threads | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | ------: | ---: | ------------: | ---------------: | | gpt-oss ?B Q8_0 | 60.03 GiB | 116.83 B | CPU | 32 | 0 | pp512 | 163.24 ± 4.46 | | gpt-oss ?B Q8_0 | 60.03 GiB | 116.83 B | CPU | 32 | 0 | tg128 | 24.77 ± 0.42 | build: 6d2e7ca4 (3884) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So, the requirement for this benchmark is simple:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Required: list your MB, CPU, RAM size, type and channels.&lt;/li&gt; &lt;li&gt;Required: use CPU only inference (No APUs, NPUs, or build-in GPUs allowed)&lt;/li&gt; &lt;li&gt;use ik-llama (any recent version) if possible since llama.cpp will be slower for your CPU performance&lt;/li&gt; &lt;li&gt;Required model: ( &lt;a href="https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_1.gguf"&gt;https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF/blob/main/Qwen3-30B-A3B-Q4_1.gguf&lt;/a&gt; ) Run the standard llama-bench benchmark with Qwen3-30B-A3B-Q4_1.gguf (2703 version should also be fine as long as it is Q4_1) and share the command with output in the comments as I shared above.&lt;/li&gt; &lt;li&gt;Optional (not required but good to have): run CPU only benchmark with GPT-OSS 120B (file here: &lt;a href="https://huggingface.co/unsloth/gpt-oss-120b-GGUF/tree/main/UD-Q8%5C_K%5C_XL"&gt;https://huggingface.co/unsloth/gpt-oss-120b-GGUF/tree/main/UD-Q8\_K\_XL&lt;/a&gt;) and share the command with output in the comments.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I will start by adding my CPU performance in this table below.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Motherboard&lt;/th&gt; &lt;th align="left"&gt;CPU (physical cores)&lt;/th&gt; &lt;th align="left"&gt;RAM size and type&lt;/th&gt; &lt;th align="left"&gt;channels&lt;/th&gt; &lt;th align="left"&gt;Qwen3 30B3A Q4_1 TG&lt;/th&gt; &lt;th align="left"&gt;Qwen3 30B3A Q4_1 PP&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AsRock ROMED8-2T&lt;/td&gt; &lt;td align="left"&gt;AMD EPYC 7532 (32 cores)&lt;/td&gt; &lt;td align="left"&gt;8x32GB DDR4 3200Mhz&lt;/td&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;39.98&lt;/td&gt; &lt;td align="left"&gt;263.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I will check comments daily and keep updating the table.&lt;/p&gt; &lt;p&gt;This awesome community is the best place to collect such performance metrics.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj4axf/thread_for_cpuonly_llm_performance_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T05:08:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1njn72i</id>
    <title>Nvidia 5060/70 TI 16gb for FP4 training or finetuning?</title>
    <updated>2025-09-17T19:44:18+00:00</updated>
    <author>
      <name>/u/Distinct-Rain-2360</name>
      <uri>https://old.reddit.com/user/Distinct-Rain-2360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My aging 1080ti 8GB doesn't even do bf16, but finetuning 1B-3B unsloth-bnb-4bit models still works reasonably well at f16. However, we've seen deepseek with the 1.5 bit weights and gpt-oss with the fp4 weights. I get the impression that many future models will be trained on very quantized weights from the get go, especially with rocm 7 adding fp4 for their flagship instinct. With time, I assume inferencing will get faster as well, as vllm and llamacpp add native fp4 support for the whole processing pipeline. On the nvidia side, all cards with cuda capability 12+ get fp4 by default, so that means all the 5000 series. The 5090 and 5080 seem out of reach price wise, but would a cluster of 3 or 4 5060 or 5070 TIs be worth it for finetuning 30B bnb-4bit models? Either of them at 16GB configuration. The memory bandwidth is double for the 5070 (256bit vs 128bit) and about double the tensor cores as well (280 vs 144) but that commands double the price. The low power draw of the 5060 also makes it easier for people who have heat/power constraints. I feel that 6x 5060Ti 16GB with an open frame, pcie bifurcations and psu accessories beats an RTX 6000 96gb build by a long mile, but I haven't seen this brought up yet, so maybe I'm completely left field.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Rain-2360"&gt; /u/Distinct-Rain-2360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njn72i/nvidia_506070_ti_16gb_for_fp4_training_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njn72i/nvidia_506070_ti_16gb_for_fp4_training_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njn72i/nvidia_506070_ti_16gb_for_fp4_training_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T19:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nivz2n</id>
    <title>We got a 2B param model running on iPhone at ~500MB RAM — fully offline demo</title>
    <updated>2025-09-16T22:32:32+00:00</updated>
    <author>
      <name>/u/Josiahhenryus</name>
      <uri>https://old.reddit.com/user/Josiahhenryus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt; &lt;img alt="We got a 2B param model running on iPhone at ~500MB RAM — fully offline demo" src="https://external-preview.redd.it/ZDZxemk3OWFzbHBmMVMFq2pfv69EmnrpZl789HXOOBvSofKD3EML3NWxX5eD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=517811a91d95e57b2a6e0b44c23976628615830f" title="We got a 2B param model running on iPhone at ~500MB RAM — fully offline demo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ongoing research out of Derive DX Labs in Lafayette, Louisiana. We’ve been experimenting with efficiency optimizations and managed to get a 2B parameter chain-of-thought model running on iPhone with ~400–500MB RAM, fully offline.&lt;/p&gt; &lt;p&gt;I’m not super active on Reddit, so please don’t kill me if I’m slow to respond to comments — but I’ll do my best to answer questions.&lt;/p&gt; &lt;p&gt;[Correction: Meant Gemma-3N not Gemini-3B]&lt;/p&gt; &lt;p&gt;[Update on memory measurement: After running with Instruments, the total unified memory footprint is closer to ~2 GB (CPU + GPU) during inference, not just the 400–500 MB reported earlier. The earlier number reflected only CPU-side allocations. Still a big step down compared to the usual multi-GB requirements for 2B+ models.]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Josiahhenryus"&gt; /u/Josiahhenryus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6rczu79aslpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nivz2n/we_got_a_2b_param_model_running_on_iphone_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1niwb8l</id>
    <title>500,000 public datasets on Hugging Face</title>
    <updated>2025-09-16T22:46:45+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt; &lt;img alt="500,000 public datasets on Hugging Face" src="https://preview.redd.it/rokftav6vlpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26f96c62b0cfcf4ab8d9a212645ed0b0f54e16e2" title="500,000 public datasets on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rokftav6vlpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niwb8l/500000_public_datasets_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T22:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nifajh</id>
    <title>I bought a modded 4090 48GB in Shenzhen. This is my story.</title>
    <updated>2025-09-16T11:52:20+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt; &lt;img alt="I bought a modded 4090 48GB in Shenzhen. This is my story." src="https://external-preview.redd.it/1vD_R63iqu4vnM_qQf7pZNwXb9dy_UDc_Gl2j3LnTpU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5102c5612db16c04c26877a1e72e86700648e25" title="I bought a modded 4090 48GB in Shenzhen. This is my story." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe"&gt;https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few years ago, before ChatGPT became popular, I managed to score a Tesla P40 on eBay for around $150 shipped. With a few tweaks, I installed it in a Supermicro chassis. At the time, I was mostly working on video compression and simulation. It worked, but the card consistently climbed to 85°C.&lt;/p&gt; &lt;p&gt;When DeepSeek was released, I was impressed and installed Ollama in a container. With 24GB of VRAM, it worked—but slowly. After trying Stable Diffusion, it became clear that an upgrade was necessary.&lt;/p&gt; &lt;p&gt;The main issue was finding a modern GPU that could actually &lt;strong&gt;fit&lt;/strong&gt; in the server chassis. Standard 4090/5090 cards are designed for desktops: they're too large, and the power plug is inconveniently placed on top. After watching the LTT video featuring a modded 4090 with 48GB (and a follow-up from Gamers Nexus), I started searching the only place I knew might have one: Alibaba.com.&lt;/p&gt; &lt;p&gt;I contacted a seller and got a quote: &lt;strong&gt;CNY 22,900&lt;/strong&gt;. Pricey, but cheaper than expected. However, Alibaba enforces VAT collection, and I’ve had bad experiences with DHL—there was a non-zero chance I’d be charged twice for taxes. I was already over €700 in taxes and fees.&lt;/p&gt; &lt;p&gt;Just for fun, I checked &lt;a href="http://Trip.com"&gt;Trip.com&lt;/a&gt; and realized that for the same amount of money, I could fly to Hong Kong and back, with a few days to explore. After confirming with the seller that they’d meet me at their business location, I booked a flight and an Airbnb in Hong Kong.&lt;/p&gt; &lt;p&gt;For context, I don’t speak Chinese at all. Finding the place using a Chinese address was tricky. Google Maps is useless in China, Apple Maps gave some clues, and Baidu Maps was beyond my skill level. With a little help from DeepSeek, I decoded the address and located the place in an industrial estate outside the city center. Thanks to Shenzhen’s extensive metro network, I didn’t need a taxi.&lt;/p&gt; &lt;p&gt;After arriving, the manager congratulated me for being the first foreigner to find them unassisted. I was given the card from a large batch—they’re clearly producing these in volume at a factory elsewhere in town (I was proudly shown videos of the assembly line). I asked them to retest the card so I could verify its authenticity.&lt;/p&gt; &lt;p&gt;During the office tour, it was clear that their next frontier is repurposing old mining cards. I saw a large collection of NVIDIA Ampere mining GPUs. I was also told that modded 5090s with over 96GB of VRAM are in development.&lt;/p&gt; &lt;p&gt;After the test was completed, I paid in cash (a &lt;em&gt;lot&lt;/em&gt; of banknotes!) and returned to Hong Kong with my new purchase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1njk5ef</id>
    <title>I made LLaMA 1B play maze-runner… GTPO wins by a nose</title>
    <updated>2025-09-17T17:50:28+00:00</updated>
    <author>
      <name>/u/Gildarts777</name>
      <uri>https://old.reddit.com/user/Gildarts777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njk5ef/i_made_llama_1b_play_mazerunner_gtpo_wins_by_a/"&gt; &lt;img alt="I made LLaMA 1B play maze-runner… GTPO wins by a nose" src="https://external-preview.redd.it/ajhveWV0dnZpcnBmMQNEfvJVMDQ8lExFs17QC-LIJvVae-cVWFKNs-OgKDbG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6752ab5585e754bd5b5c961a2b7692eb769edf4a" title="I made LLaMA 1B play maze-runner… GTPO wins by a nose" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;I ran a little demo comparing &lt;strong&gt;GRPO&lt;/strong&gt; and &lt;strong&gt;GTPO&lt;/strong&gt; by teaching a LLaMA 1B model to solve a &lt;em&gt;tiny maze it had never seen before&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;👉 The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The model wasn’t allowed to &lt;em&gt;see&lt;/em&gt; the maze. Instead, it could only answer with moves: &lt;strong&gt;forward, right, or left&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;The video shows the &lt;strong&gt;reward signal.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;The “game” for the model was to maximize its reward, which meant navigating the maze correctly step by step.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;👉 What’s happening in the video:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We presented the &lt;strong&gt;average reward step by step with a video&lt;/strong&gt;, so that’s why the models go up and down, you’re watching the learning process in real time.&lt;/li&gt; &lt;li&gt;The “goal” was defined as the model reaching a point where it gave &lt;strong&gt;at least 50% correct answers&lt;/strong&gt; and another &lt;strong&gt;50% nearly perfect answers&lt;/strong&gt; (reward close to maximum).&lt;/li&gt; &lt;li&gt;That way, success wasn’t just about randomly guessing a few right moves out of 36 possibilities, but about actually &lt;em&gt;learning the maze logic&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;👉 GRPO vs GTPO:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We defined conflicts only on the &lt;strong&gt;first tokens&lt;/strong&gt;, using the tokens that the reward identified as correct.&lt;/li&gt; &lt;li&gt;GTPO didn’t require formula changes, just a tweak in how we defined conflicts.&lt;/li&gt; &lt;li&gt;Even on free Colab GPUs with a small Lora, &lt;strong&gt;GTPO was ~5% more efficient than GRPO&lt;/strong&gt; at reaching the goal.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The experiment wasn’t about solving mazes per se, but about testing how well these algorithms can actually &lt;em&gt;teach&lt;/em&gt; small models to do exactly what we want, in this case, a simple but strict task.&lt;/p&gt; &lt;p&gt;We’ll be releasing &lt;strong&gt;Colab friendly notebooks&lt;/strong&gt; soon so anyone can try GTPO hands on.&lt;/p&gt; &lt;p&gt;Paper &amp;amp; GitHub if you want to dive deeper:&lt;br /&gt; 📄 Paper: &lt;a href="https://arxiv.org/abs/2508.03772"&gt;https://arxiv.org/abs/2508.03772&lt;/a&gt;&lt;br /&gt; 💻 Github: &lt;a href="https://github.com/winstonsmith1897/GTPO"&gt;https://github.com/winstonsmith1897/GTPO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🙏 Huge thanks to everyone who commented on my previous post, your feedback really helped me think through this little demo, try GTPO outside of math only tasks, and even switch models.&lt;/p&gt; &lt;p&gt;Next steps:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Release more &lt;strong&gt;user-friendly notebooks&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Update the algorithm to the latest version of &lt;strong&gt;unsloth&lt;/strong&gt; and bring it to &lt;strong&gt;TRL&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Explore &lt;strong&gt;new tasks&lt;/strong&gt; to test GTPO on&lt;/li&gt; &lt;li&gt;Understand its limitations more deeply and see &lt;strong&gt;how to improve it&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Gildarts777"&gt; /u/Gildarts777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ctq3xw2tirpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njk5ef/i_made_llama_1b_play_mazerunner_gtpo_wins_by_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njk5ef/i_made_llama_1b_play_mazerunner_gtpo_wins_by_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1njm4w0</id>
    <title>How to make a small LLM from scratch?</title>
    <updated>2025-09-17T19:03:25+00:00</updated>
    <author>
      <name>/u/Charming_Barber_3317</name>
      <uri>https://old.reddit.com/user/Charming_Barber_3317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to build an llm 0.1B to 0.6B params on a less popular language. How much data will i require of that particular language? and what are the exact steps i should follow? is this a good project for my final year? I have access to rtx3090 on which i can run 20B to 40B models easily at q4_k_m.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Barber_3317"&gt; /u/Charming_Barber_3317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njm4w0/how_to_make_a_small_llm_from_scratch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njm4w0/how_to_make_a_small_llm_from_scratch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njm4w0/how_to_make_a_small_llm_from_scratch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T19:03:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7pik</id>
    <title>support for the upcoming Olmo3 model has been merged into llama.cpp</title>
    <updated>2025-09-17T08:42:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"&gt; &lt;img alt="support for the upcoming Olmo3 model has been merged into llama.cpp" src="https://external-preview.redd.it/11Q8uZ2-M8bnIL12n-O39P2wooNtmpfM5ORG4VqYvik.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fab39b6ac711c07bf4835557388faf67e2bdb807" title="support for the upcoming Olmo3 model has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16015"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7pik/support_for_the_upcoming_olmo3_model_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1njlnad</id>
    <title>LACT "indirect undervolt &amp; OC" method beats `nvidia-smi -pl 400` on 3090TI FE.</title>
    <updated>2025-09-17T18:45:23+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"&gt; &lt;img alt="LACT &amp;quot;indirect undervolt &amp;amp; OC&amp;quot; method beats `nvidia-smi -pl 400` on 3090TI FE." src="https://preview.redd.it/h4082k0frrpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b52b3a347341b760c6e962eb14813fd99117eb4" title="LACT &amp;quot;indirect undervolt &amp;amp; OC&amp;quot; method beats `nvidia-smi -pl 400` on 3090TI FE." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There have been some recent posts about using the new &amp;quot;indirect undervolt and overclock&amp;quot; method with LACT under Linux instead of simply naieve power capping your GPU(s) with &lt;code&gt;nvidia-smi -pl 300&lt;/code&gt; for example.&lt;/p&gt; &lt;p&gt;I wasn't sure if it was really any better or not, so vibe coded a small script to integrate 1Hz power measurements from my 3090TI FE 24GB GPU and run two benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Baseline &lt;code&gt;nvidia -pl 400&lt;/code&gt; naieve 400W power cap&lt;/li&gt; &lt;li&gt;LACT overclock profile with same 400W power cap&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I then ran the same ik_llama.cpp llama-sweep-bench test and sure enough the LACT overclock profile performs better/faster with less overall energy usage within the same power envelope.&lt;/p&gt; &lt;p&gt;LACT has worked on a variety of Intel/AMD/NVIDIA GPUs for a while now, but the &amp;quot;new&amp;quot; discovery to me was this &amp;quot;indirect undervolt and overclock&amp;quot; method specific to NVIDIA GPUs.&lt;/p&gt; &lt;p&gt;I have some anecdotal measurements with ComfyUI Wan2.2 i2v workflows suggesting it is faster for a given power cap as well. However, when I increased the overclocks too far it would output all dark/black videos or have occasional grey/dark square tile patches appear in the output video. I had to undo the aggressive overclock, reboot, and then it was all fine again. The values listed in the legend here seem to be working fine for now.&lt;/p&gt; &lt;p&gt;Curious what overclock profiles other folks are using for various GPU make/models. It does work headless as well and some have reported using it to reduce idle power psure. Also has anyone compared this against using nvidia-smi to set frequency cap instead of power cap or other strategies?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h4082k0frrpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njlnad/lact_indirect_undervolt_oc_method_beats_nvidiasmi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:45:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkgkf</id>
    <title>SvelteKit-based WebUI by allozaur · Pull Request #14839 · ggml-org/llama.cpp</title>
    <updated>2025-09-17T18:01:20+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkgkf/sveltekitbased_webui_by_allozaur_pull_request/"&gt; &lt;img alt="SvelteKit-based WebUI by allozaur · Pull Request #14839 · ggml-org/llama.cpp" src="https://external-preview.redd.it/lfHyUwYZ8aPaE0KlMcSCbv60pDFTFEz7jL_zRZTwdcw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=94339bd1349da02ea7d59583c915f4eb21952ba8" title="SvelteKit-based WebUI by allozaur · Pull Request #14839 · ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;This PR introduces a complete rewrite of the llama.cpp web interface, migrating from a React-based implementation to a modern SvelteKit architecture. The new implementation provides significant improvements in user experience, developer tooling, and feature capabilities while maintaining full compatibility with the llama.cpp server API.&amp;quot;&lt;/p&gt; &lt;p&gt;✨ Feature Enhancements&lt;/p&gt; &lt;h1&gt;File Handling&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dropdown Upload Menu&lt;/strong&gt;: Type-specific file selection (Images/Text/PDFs)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Universal Preview System&lt;/strong&gt;: Full-featured preview dialogs for all supported file types&lt;/li&gt; &lt;li&gt;&lt;strong&gt;PDF Dual View&lt;/strong&gt;: Text extraction + page-by-page image rendering&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Support&lt;/strong&gt;: SVG/WEBP→PNG conversion, binary detection, syntax highlighting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Model Awareness&lt;/strong&gt;: Smart UI adaptation based on model capabilities&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Graceful Failure&lt;/strong&gt;: Proper error handling and user feedback for unsupported file types&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Advanced Chat Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning Content&lt;/strong&gt;: Dedicated thinking blocks with streaming support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Conversation Branching&lt;/strong&gt;: Full tree structure with parent-child relationships&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Message Actions&lt;/strong&gt;: Edit, regenerate, delete with intelligent branch management&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Keyboard Shortcuts&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;code&gt;Ctrl+Shift+N&lt;/code&gt;: Start new conversation&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+Shift+D&lt;/code&gt;: Delete current conversation&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+K&lt;/code&gt;: Focus search conversations&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+V&lt;/code&gt;: Paste files and content to conversation&lt;/li&gt; &lt;li&gt;&lt;code&gt;Ctrl+B&lt;/code&gt;: Toggle sidebar&lt;/li&gt; &lt;li&gt;&lt;code&gt;Enter&lt;/code&gt;: Send message&lt;/li&gt; &lt;li&gt;&lt;code&gt;Shift+Enter&lt;/code&gt;: New line in message&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Paste&lt;/strong&gt;: Auto-conversion of long text to files with customizable threshold (default 2000 characters)&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Server Integration&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Slots Monitoring&lt;/strong&gt;: Real-time server resource tracking during generation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Management&lt;/strong&gt;: Advanced context error handling and recovery&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Server Status&lt;/strong&gt;: Comprehensive server state monitoring&lt;/li&gt; &lt;li&gt;&lt;strong&gt;API Integration&lt;/strong&gt;: Full &lt;code&gt;reasoning_content&lt;/code&gt; and slots endpoint support&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;🎨 User Experience Improvements&lt;/h1&gt; &lt;h1&gt;Interface Design&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Modern UI Components&lt;/strong&gt;: Consistent design system with ShadCN components&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Responsive Layout&lt;/strong&gt;: Adaptive sidebar and mobile-friendly design&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Theme System&lt;/strong&gt;: Seamless auto/light/dark mode switching&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Hierarchy&lt;/strong&gt;: Clear information architecture and content organization&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Interaction Patterns&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Keyboard Navigation&lt;/strong&gt;: Complete keyboard accessibility with shortcuts&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Drag &amp;amp; Drop&lt;/strong&gt;: Intuitive file upload with visual feedback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Defaults&lt;/strong&gt;: Context-aware UI behavior and intelligent defaults (sidebar auto-management, conversation naming)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Progressive Disclosure&lt;/strong&gt;: Advanced features available without cluttering basic interface&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Feedback &amp;amp; Communication&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Loading States&lt;/strong&gt;: Clear progress indicators during operations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Error Handling&lt;/strong&gt;: User-friendly error messages with recovery suggestions&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Status Indicators&lt;/strong&gt;: Real-time server status and resource monitoring&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Confirmation Dialogs&lt;/strong&gt;: Prevent accidental data loss with confirmation prompts&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/14839"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkgkf/sveltekitbased_webui_by_allozaur_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njkgkf/sveltekitbased_webui_by_allozaur_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj5wk4</id>
    <title>OpenAI usage breakdown released</title>
    <updated>2025-09-17T06:44:15+00:00</updated>
    <author>
      <name>/u/LeatherRub7248</name>
      <uri>https://old.reddit.com/user/LeatherRub7248</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"&gt; &lt;img alt="OpenAI usage breakdown released" src="https://preview.redd.it/njcotg7i7opf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e00350509ba0edd32cc7dfb7341451356402cd8" title="OpenAI usage breakdown released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I would have thought image generation would be higher... but this might be skewed by the fact that the 4o image (the whole ghibli craze) only came out in march 2025&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nber.org/system/files/working_papers/w34255/w34255.pdf"&gt;https://www.nber.org/system/files/working_papers/w34255/w34255.pdf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.nber.org/papers/w34255"&gt;https://www.nber.org/papers/w34255&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LeatherRub7248"&gt; /u/LeatherRub7248 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/njcotg7i7opf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj5wk4/openai_usage_breakdown_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T06:44:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgb5x</id>
    <title>Qwen3 Coder Plus</title>
    <updated>2025-09-17T15:30:00+00:00</updated>
    <author>
      <name>/u/Dependent_Factor_204</name>
      <uri>https://old.reddit.com/user/Dependent_Factor_204</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just noticed &lt;a href="https://openrouter.ai/qwen/qwen3-coder-plus"&gt;https://openrouter.ai/qwen/qwen3-coder-plus&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Not open though!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dependent_Factor_204"&gt; /u/Dependent_Factor_204 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgb5x/qwen3_coder_plus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgb5x/qwen3_coder_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgb5x/qwen3_coder_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:30:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj7mbu</id>
    <title>Big AI pushes the "we need to beat China" narrative cuz they want fat government contracts and zero democratic oversight. It's an old trick. Fear sells.</title>
    <updated>2025-09-17T08:36:32+00:00</updated>
    <author>
      <name>/u/katxwoods</name>
      <uri>https://old.reddit.com/user/katxwoods</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Throughout the Cold War, the military-industrial complex spent a fortune pushing the false narrative that the Soviet military was far more advanced than they actually were.&lt;/p&gt; &lt;p&gt;Why? To ensure the money from Congress kept flowing.&lt;/p&gt; &lt;p&gt;They lied… and lied… and lied again to get bigger and bigger defense contracts.&lt;/p&gt; &lt;p&gt;Now, obviously, there is &lt;em&gt;some&lt;/em&gt; amount of competition between the US and China, but &lt;strong&gt;Big Tech is stoking the flames beyond what is reasonable to terrify Congress into giving them whatever they want.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;What they want is fat government contracts and zero democratic oversight. Day after day we hear about another big AI company announcing a giant contract with the Department of Defense.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/katxwoods"&gt; /u/katxwoods &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj7mbu/big_ai_pushes_the_we_need_to_beat_china_narrative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T08:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgj9s</id>
    <title>Drummer's Cydonia ReduX 22B and Behemoth ReduX 123B - Throwback tunes of the good old days, now with updated tuning! Happy birthday, Cydonia v1!</title>
    <updated>2025-09-17T15:38:21+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgj9s/drummers_cydonia_redux_22b_and_behemoth_redux/"&gt; &lt;img alt="Drummer's Cydonia ReduX 22B and Behemoth ReduX 123B - Throwback tunes of the good old days, now with updated tuning! Happy birthday, Cydonia v1!" src="https://external-preview.redd.it/ZtYL1Go2LfQzSyi5MifYgd-epIPOSy-LGxJA6DFzU3k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=18777ba97955acd7508bb2f53af88f4c18be1858" title="Drummer's Cydonia ReduX 22B and Behemoth ReduX 123B - Throwback tunes of the good old days, now with updated tuning! Happy birthday, Cydonia v1!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Behemoth ReduX 123B: &lt;a href="https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1"&gt;https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They're updated finetunes of the old Mistral 22B and Mistral 123B 2407. &lt;/p&gt; &lt;p&gt;Both bases were arguably peak Mistral (aside from Nemo and &lt;span class="md-spoiler-text"&gt;Miqu&lt;/span&gt;). I decided to finetune them since the writing/creativity is just... different from what we've got today. They hold up stronger than ever, but they're still old bases so intelligence and context length isn't up there with the newer base models. Still, they both prove that these smarter, stronger models are missing out on &lt;em&gt;something&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;I figured I'd release it on Cydonia v1's one year anniversary. Can't believe it's been a year and a half since I started this journey with you all. Hope you enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-ReduX-22B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgj9s/drummers_cydonia_redux_22b_and_behemoth_redux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgj9s/drummers_cydonia_redux_22b_and_behemoth_redux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nixynv</id>
    <title>The Qwen of Pain.</title>
    <updated>2025-09-16T23:58:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt; &lt;img alt="The Qwen of Pain." src="https://preview.redd.it/0px1banw6mpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8edc833e57220e0c00a8b11ba32c881974742ef1" title="The Qwen of Pain." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0px1banw6mpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nixynv/the_qwen_of_pain/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T23:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1njkqdm</id>
    <title>Arcee going Apache 2.0!!!</title>
    <updated>2025-09-17T18:11:16+00:00</updated>
    <author>
      <name>/u/Lowgooo</name>
      <uri>https://old.reddit.com/user/Lowgooo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;CTO of Arcee just announced that their AFM-4.5B model - &lt;a href="https://huggingface.co/arcee-ai/AFM-4.5B"&gt;https://huggingface.co/arcee-ai/AFM-4.5B&lt;/a&gt;&lt;br /&gt; as well as upcoming models will all be fully open source!&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/LucasAtkins7/status/1968371293184741876"&gt;https://x.com/LucasAtkins7/status/1968371293184741876&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lowgooo"&gt; /u/Lowgooo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njkqdm/arcee_going_apache_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T18:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjn2a</id>
    <title>Kimi-K2 0905, DeepSeek V3.1, Qwen3-Next-80B-A3B, Grok 4, and others on fresh SWE-bench–style tasks collected in August 2025</title>
    <updated>2025-09-17T17:32:11+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"&gt; &lt;img alt="Kimi-K2 0905, DeepSeek V3.1, Qwen3-Next-80B-A3B, Grok 4, and others on fresh SWE-bench–style tasks collected in August 2025" src="https://b.thumbs.redditmedia.com/ZgnOCtlnsYSchww7mxS8EvEaABQb-to_49dQyMZXKqg.jpg" title="Kimi-K2 0905, DeepSeek V3.1, Qwen3-Next-80B-A3B, Grok 4, and others on fresh SWE-bench–style tasks collected in August 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/54d12kuq8rpf1.png?width=5684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6967f375cf67e45ff0fde346d6d6bb73abc997e"&gt;https://preview.redd.it/54d12kuq8rpf1.png?width=5684&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d6967f375cf67e45ff0fde346d6d6bb73abc997e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi all, I'm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We’ve updated the &lt;a href="https://swe-rebench.com"&gt;SWE-rebench leaderboard&lt;/a&gt; with model evaluations of Grok 4, Kimi K2 Instruct 0905, DeepSeek-V3.1, and Qwen3-Next-80B-A3B-Instruct on 52 fresh tasks.&lt;/p&gt; &lt;p&gt;Key takeaways from this update:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2 0915&lt;/strong&gt; has grown significantly (34.6% -&amp;gt; 42.3% increase in resolved rate) and is now in the top 3 open-source models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek V3.1&lt;/strong&gt; also improved, though less dramatically. What’s interesting is how many more tokens it now produces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct&lt;/strong&gt;, despite not being trained directly for coding, performs on par with the 30B-Coder. To reflect models speed, we’re also thinking about how best to report efficiency metrics such as tokens/sec on the leaderboard.&lt;/li&gt; &lt;li&gt;Finally, &lt;strong&gt;Grok 4&lt;/strong&gt;: the frontier model from xAI has now entered the leaderboard and is among the top performers. It’ll be fascinating to watch how it develops.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All &lt;strong&gt;52 new tasks collected in August&lt;/strong&gt; are available on the site — you can explore every problem in detail.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjn2a/kimik2_0905_deepseek_v31_qwen3next80ba3b_grok_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:32:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1njet2z</id>
    <title>IBM just released Granite Docling</title>
    <updated>2025-09-17T14:33:34+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njet2z/ibm_just_released_granite_docling/"&gt; &lt;img alt="IBM just released Granite Docling" src="https://external-preview.redd.it/9VrSOe38oy5d5NsTP4RWGmhv_WIFkf4SUZ5rRkZXUAc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c208e780cd41278802ac7163a907c2de39c8d987" title="IBM just released Granite Docling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;granite-docling-258M with Apache 2.0 license for document analysis &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-docling-682b8c766a565487bcb3ca00"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njet2z/ibm_just_released_granite_docling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njet2z/ibm_just_released_granite_docling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T14:33:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nj9601</id>
    <title>Ling Flash 2.0 released</title>
    <updated>2025-09-17T10:14:07+00:00</updated>
    <author>
      <name>/u/abskvrm</name>
      <uri>https://old.reddit.com/user/abskvrm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"&gt; &lt;img alt="Ling Flash 2.0 released" src="https://b.thumbs.redditmedia.com/Mje87GDqOP-_eCjHaD9MMo5kTGeRXnZhVYDnfTltcjY.jpg" title="Ling Flash 2.0 released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ling Flash-2.0, from InclusionAI, a language model with 100B total parameters and 6.1B activated parameters (4.8B non-embedding).&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-flash-2.0"&gt;https://huggingface.co/inclusionAI/Ling-flash-2.0&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abskvrm"&gt; /u/abskvrm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nj9601"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nj9601/ling_flash_20_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T10:14:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgicz</id>
    <title>China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D</title>
    <updated>2025-09-17T15:37:22+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"&gt; &lt;img alt="China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D" src="https://external-preview.redd.it/8TEqL7hV1nddGcswRwl5w0myECFYu5Ll4SYjP8Vx1jY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a29ca54687a9865b636eb76fe44ba0da1943af79" title="China bans its biggest tech companies from acquiring Nvidia chips, says report — Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgovj</id>
    <title>Magistral Small 2509 has been released</title>
    <updated>2025-09-17T15:44:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt; &lt;img alt="Magistral Small 2509 has been released" src="https://external-preview.redd.it/lya4BYVSdGKwEDIK4epA43BL60WtN4IIIDfpTgnEljc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14e03904bf3f936ad1691d3e1bcf8b07536b90d5" title="Magistral Small 2509 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509-GGUF"&gt;https://huggingface.co/mistralai/Magistral-Small-2509-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509"&gt;https://huggingface.co/mistralai/Magistral-Small-2509&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Magistral Small 1.2&lt;/h1&gt; &lt;p&gt;Building upon &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;Mistral Small 3.2 (2506)&lt;/a&gt;, &lt;strong&gt;with added reasoning capabilities&lt;/strong&gt;, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.&lt;/p&gt; &lt;p&gt;Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.&lt;/p&gt; &lt;p&gt;Learn more about Magistral in our &lt;a href="https://mistral.ai/news/magistral/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The model was presented in the paper &lt;a href="https://huggingface.co/papers/2506.10910"&gt;Magistral&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Updates compared with &lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;Magistral Small 1.1&lt;/a&gt;&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodality&lt;/strong&gt;: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance upgrade&lt;/strong&gt;: Magistral Small 1.2 should give you significatively better performance than Magistral Small 1.1 as seen in the &lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509#benchmark-results"&gt;benchmark results&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better tone and persona&lt;/strong&gt;: You should experiment better LaTeX and Markdown formatting, and shorter answers on easy general prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finite generation&lt;/strong&gt;: The model is less likely to enter infinite generation loops.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Special think tokens&lt;/strong&gt;: [THINK] and [/THINK] special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning prompt&lt;/strong&gt;: The reasoning prompt is given in the system prompt.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning:&lt;/strong&gt; Capable of long chains of reasoning traces before providing an answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: Vision capabilities enable the model to analyze images and reason based on visual content in addition to text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Open license allowing usage and modification for both commercial and non-commercial purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Window:&lt;/strong&gt; A 128k context window. Performance &lt;em&gt;might&lt;/em&gt; degrade past &lt;strong&gt;40k&lt;/strong&gt; but Magistral should still give good results. Hence we recommend to leave the maximum model length to 128k and only lower if you encounter low performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0vo5ev3xqpf1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f81d6fa64a262e991112d1c8011e18d1d75b2774"&gt;https://preview.redd.it/d0vo5ev3xqpf1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f81d6fa64a262e991112d1c8011e18d1d75b2774&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
</feed>
