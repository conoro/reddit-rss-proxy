<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-16T19:24:22+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pn8upp</id>
    <title>NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!</title>
    <updated>2025-12-15T14:34:28+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt; &lt;img alt="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" src="https://preview.redd.it/sic85bvhpd7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d01067e3a3899e680b913799c37c8ef9b609ff4c" title="NVIDIA releases Nemotron 3 Nano, a new 30B hybrid reasoning model!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth GGUF: &lt;a href="https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF"&gt;https://huggingface.co/unsloth/Nemotron-3-Nano-30B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Nemotron 3 has a 1M context window and the best in class performance for SWE-Bench, reasoning and chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/sic85bvhpd7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pn8upp/nvidia_releases_nemotron_3_nano_a_new_30b_hybrid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T14:34:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1po6we5</id>
    <title>llama.cpp recent updates - gpt120 = 20t/s</title>
    <updated>2025-12-16T16:49:26+00:00</updated>
    <author>
      <name>/u/Aggressive-Bother470</name>
      <uri>https://old.reddit.com/user/Aggressive-Bother470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama-bench is fine.&lt;/p&gt; &lt;p&gt;Actual text generation is now hideous @ 20t/s. Was previously 130~ with llama-bench still claiming 160.&lt;/p&gt; &lt;p&gt;Build 7389 was fine. Happened some time after that?&lt;/p&gt; &lt;p&gt;Nobody else seeing this?!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Bother470"&gt; /u/Aggressive-Bother470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po6we5/llamacpp_recent_updates_gpt120_20ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po6we5/llamacpp_recent_updates_gpt120_20ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po6we5/llamacpp_recent_updates_gpt120_20ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T16:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnslcb</id>
    <title>My Local coding agent worked 2 hours unsupervised and here is my setup</title>
    <updated>2025-12-16T04:14:38+00:00</updated>
    <author>
      <name>/u/Express_Quail_1493</name>
      <uri>https://old.reddit.com/user/Express_Quail_1493</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Setup&lt;/p&gt; &lt;p&gt;--- Model&lt;br /&gt; devstral-small-2 from bartowski IQ3_xxs version.&lt;br /&gt; Run with lm studio &amp;amp; intentionally limit the context at 40960 which should't take more than (14gb ram even when context is full)&lt;/p&gt; &lt;p&gt;---Tool&lt;br /&gt; kilo code (set file limit to 500 lines) it will read in chunks&lt;br /&gt; 40960 ctx limit is actually a strength not weakness (more ctx = easier confusion)&lt;br /&gt; Paired with qdrant in the kilo code UI.&lt;br /&gt; Setup the indexing with qdrant (the little database icon) use model &lt;a href="https://ollama.com/toshk0/nomic-embed-text-v2-moe"&gt;https://ollama.com/toshk0/nomic-embed-text-v2-moe&lt;/a&gt; in ollama (i choose ollama to keep indexing and seperate from Lm studio to allow lm studio to focus on the heavy lifting)&lt;/p&gt; &lt;p&gt;--Result&lt;br /&gt; minimal drift on tasks&lt;br /&gt; slight errors on tool call but the model quickly realign itself. A oneshot prompt implimentation of a new feature in my codebase in architect mode resulted in 2 hours of coding unsupervised kilo code auto switches to code mode to impliment after planning in architect mode which is amazing. Thats been my lived experience&lt;/p&gt; &lt;p&gt;EDIT: ministral 3 3b also works okayISH if you are desprate on hardware resources (3.5gb laptop GPU) but it will want to frequently pause and ask you some questions at the slightest hint of anythings it might be unclear on&lt;/p&gt; &lt;p&gt;Feel free to also share your fully localhost setup that also solved long running tasks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Quail_1493"&gt; /u/Express_Quail_1493 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnslcb/my_local_coding_agent_worked_2_hours_unsupervised/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T04:14:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnb824</id>
    <title>Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face</title>
    <updated>2025-12-15T16:08:45+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt; &lt;img alt="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" src="https://external-preview.redd.it/Mno1ZHBiNTg0ZTdnMetROQBwb-dMzbNK88p-4KlSnzkAfcO7Jy5xOmtEL7Fy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea250dde0c1f1556ba5b404754e09373d2c88623" title="Chatterbox Turbo, new open-source voice AI model, just released on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Links:&lt;br /&gt; - Model (PyTorch): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo&lt;/a&gt;&lt;br /&gt; - Model (ONNX): &lt;a href="https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX"&gt;https://huggingface.co/ResembleAI/chatterbox-turbo-ONNX&lt;/a&gt;&lt;br /&gt; - GitHub: &lt;a href="https://github.com/resemble-ai/chatterbox"&gt;https://github.com/resemble-ai/chatterbox&lt;/a&gt;&lt;br /&gt; - Demo: &lt;a href="https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo"&gt;https://huggingface.co/spaces/ResembleAI/chatterbox-turbo-demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6v5yql484e7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnb824/chatterbox_turbo_new_opensource_voice_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T16:08:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1po7p11</id>
    <title>Built a local image hub to organize my 30k+ PNG chaos ‚Äî v0.10 integrates with A1111, handles ComfyUI workflows &amp; runs 100% offline (v0.10.5 perf update)</title>
    <updated>2025-12-16T17:19:03+00:00</updated>
    <author>
      <name>/u/SunTzuManyPuppies</name>
      <uri>https://old.reddit.com/user/SunTzuManyPuppies</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7p11/built_a_local_image_hub_to_organize_my_30k_png/"&gt; &lt;img alt="Built a local image hub to organize my 30k+ PNG chaos ‚Äî v0.10 integrates with A1111, handles ComfyUI workflows &amp;amp; runs 100% offline (v0.10.5 perf update)" src="https://b.thumbs.redditmedia.com/VFn-uCHPoxLGXJNZRNMjTYRk1oZO7X06HZO5dPbxrWE.jpg" title="Built a local image hub to organize my 30k+ PNG chaos ‚Äî v0.10 integrates with A1111, handles ComfyUI workflows &amp;amp; runs 100% offline (v0.10.5 perf update)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I posted a while ago on other subs about a tool I built to manage my own mess of AI images, and wanted to share the latest update here since I know this community appreciates local-first software.&lt;/p&gt; &lt;p&gt;Quick context: I have over 30k images generated across Invoke, A1111, SwarmUI, etc. My folder was a disaster. Windows Explorer is useless for searching metadata, and existing tools either wanted cloud access or were too clunky.&lt;/p&gt; &lt;p&gt;So I built Image MetaHub. It‚Äôs a desktop app that indexes your local folders and lets you search by prompt, model, LoRA, seed, sampler, etc. Everything runs locally, no cloud, no account, no telemetry ‚Äî it‚Äôs just your folders and your PNGs.&lt;/p&gt; &lt;p&gt;Image MetaHub parses metadata from:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stable Diffusion / Automatic1111 images (PNG info, etc.)&lt;/li&gt; &lt;li&gt;ComfyUI (partial coverage; parser is actively being extended)&lt;/li&gt; &lt;li&gt;InvokeAI&lt;/li&gt; &lt;li&gt;Fooocus&lt;/li&gt; &lt;li&gt;&lt;a href="http://SD.Next"&gt;SD.Next&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Forge&lt;/li&gt; &lt;li&gt;SwarmUI&lt;/li&gt; &lt;li&gt;DrawThings&lt;/li&gt; &lt;li&gt;Online services like Midjourney / Nijijourney (when prompts/settings are saved into the downloaded files)&lt;/li&gt; &lt;li&gt;&lt;p&gt;Other tools that store generation parameters in PNG/JPG metadata&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Note: ComfyUI support is still evolving and may not cover every custom node or complex workflow yet.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(sorry just copied this last part from the Readme, its a lot to remember lol)&lt;/p&gt; &lt;p&gt;Anyway, I pushed a big update recently, v0.10.x -- the change is moving from &amp;quot;just viewing&amp;quot; to actually integrating the app into your workflow. I added an integration with Automatic1111, so you can open an image from your library and send the metadata back to your local A1111 instance - or even trigger variations directly from a simple modal in the app. The options are still basic, but its functional and it is being improved every day. Will be able to integrate with other tools soon as well.&lt;/p&gt; &lt;p&gt;I also spent a lot of time rewriting the parser for ComfyUI. Instead of just scraping text, it uses a node registry to traverse the workflow graph embedded in the image. It handles complex custom nodes pretty well.&lt;/p&gt; &lt;p&gt;Today I just pushed a dedicated performance update specifically for large libraries. Switched from full-image decoding to direct header reading during metadata enrichment and optimized IPC batches. Indexing overhead is now down to ~13ms per file on average on an SSD, so it stays snappy even if you dump 50k images into it.&lt;/p&gt; &lt;p&gt;Regarding license, the project is open-source based. The core functionality ‚Äî browsing, indexing, reading metadata/prompts, filtering ‚Äî is free and always will be. I recently added a Pro tier for some of the advanced workflow tools (like the A1111 generation bridge and analytics) to help me sustain development as a solo dev, but it‚Äôs a one-time license, no subscriptions. You can use the free version forever to organize your library without hitting a paywall.&lt;/p&gt; &lt;p&gt;If you‚Äôre drowning in unorganized local generations and want to keep your library private, give it a shot.&lt;/p&gt; &lt;p&gt;Repo/Download: &lt;a href="https://github.com/LuqP2/Image-MetaHub"&gt;https://github.com/LuqP2/Image-MetaHub&lt;/a&gt;&lt;br /&gt; Website: &lt;a href="https://imagemetahub.com"&gt;https://imagemetahub.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SunTzuManyPuppies"&gt; /u/SunTzuManyPuppies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1po7p11"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7p11/built_a_local_image_hub_to_organize_my_30k_png/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po7p11/built_a_local_image_hub_to_organize_my_30k_png/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T17:19:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3v2l</id>
    <title>XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face</title>
    <updated>2025-12-16T14:51:15+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3v2l/xiaomimimomimov2flash_hugging_face/"&gt; &lt;img alt="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" src="https://external-preview.redd.it/pEssBYcofSIqxenRV_1O2yb3vr7ekZdMtNbDln2iEbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413fe05449bcb79ceb4c3c13d870125113113e50" title="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;MiMo-V2-Flash&lt;/strong&gt; is a Mixture-of-Experts (MoE) language model with &lt;strong&gt;309B total parameters&lt;/strong&gt; and &lt;strong&gt;15B active parameters&lt;/strong&gt;. Designed for high-speed reasoning and agentic workflows, it utilizes a novel hybrid attention architecture and Multi-Token Prediction (MTP) to achieve state-of-the-art performance while significantly reducing inference costs.&lt;/p&gt; &lt;p&gt;MiMo-V2-Flash creates a new balance between long-context modeling capability and inference efficiency. Key features include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hybrid Attention Architecture&lt;/strong&gt;: Interleaves Sliding Window Attention (SWA) and Global Attention (GA) with a 5:1 ratio and an aggressive 128-token window. This reduces KV-cache storage by nearly 6x while maintaining long-context performance via learnable &lt;strong&gt;attention sink bias&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;: Equipped with a lightweight MTP module (0.33B params/block) using dense FFNs. This triples output speed during inference and will be good to accelerates rollout in RL training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Pre-Training&lt;/strong&gt;: Trained on 27T tokens using FP8 mixed precision and native 32k seq length. The context window supports up to 256k length.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities&lt;/strong&gt;: Post-training utilizes Multi-Teacher On-Policy Distillation (MOPD) and large-scale agentic RL, achieving superior performance on &lt;strong&gt;SWE-Bench&lt;/strong&gt; and complex reasoning tasks.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3v2l/xiaomimimomimov2flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3v2l/xiaomimimomimov2flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:51:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1po49p3</id>
    <title>Full AI Voice Agent (Whisper + 700M LLM + NeuTTS) running entirely on an Nvidia Jetson Orin Nano ($250 hardware) with no internet access</title>
    <updated>2025-12-16T15:07:21+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"&gt; &lt;img alt="Full AI Voice Agent (Whisper + 700M LLM + NeuTTS) running entirely on an Nvidia Jetson Orin Nano ($250 hardware) with no internet access" src="https://external-preview.redd.it/9UWsVs2hpMeho4on4Wi5M0sA3g8yJ-Upoj2dNWPAG_M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e9c82e009084fe605bea0791ebccc366c5869a5" title="Full AI Voice Agent (Whisper + 700M LLM + NeuTTS) running entirely on an Nvidia Jetson Orin Nano ($250 hardware) with no internet access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We‚Äôve been playing with what's truly possible for low-latency, privacy-first voice agents, and just released a demo: Agent Santa.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1po49p3/video/s8sca29xzk7g1/player"&gt;https://reddit.com/link/1po49p3/video/s8sca29xzk7g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The entire voice-to-text-to-speech loop runs &lt;em&gt;locally&lt;/em&gt; on a sub-$250 Nvidia Jetson Orin Nano.&lt;/p&gt; &lt;p&gt;The ML Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;STT: OpenAI Whisper EN tiny&lt;/li&gt; &lt;li&gt;LLM: LiquidAI‚Äôs 700M-parameter LFM2&lt;/li&gt; &lt;li&gt;TTS: Our NeuTTS (zero-cost cloning, high quality)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The whole thing consumes under 4GB RAM and 2GB VRAM. This showcases that complex, multi-model AI can be fully deployed on edge devices today.&lt;/p&gt; &lt;p&gt;We'd love to hear your feedback on the latency and potential applications for this level of extreme on-device efficiency.&lt;/p&gt; &lt;p&gt;Git Repo: &lt;a href="https://github.com/neuphonic/neutts-air"&gt;https://github.com/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;https://huggingface.co/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po49p3/full_ai_voice_agent_whisper_700m_llm_neutts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T15:07:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz1je</id>
    <title>support for GLM4V vision encoder has been merged into llama.cpp</title>
    <updated>2025-12-16T10:53:34+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt; &lt;img alt="support for GLM4V vision encoder has been merged into llama.cpp" src="https://external-preview.redd.it/i0ktGuORgovwZVXClbj98qHky3ndOw6pJOFp0qnTifE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d399641787a216379ff3d6b42189093d66157b6" title="support for GLM4V vision encoder has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18042"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T10:53:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pny30h</id>
    <title>The Attention Hybrid MoE Architecture is the Future. Now, AI Labs Should Dedicate Resources to Improve Long Context Recall Capabilities.</title>
    <updated>2025-12-16T09:52:11+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Qwen3-Next-80B-A30 since it was fully supported in Llama.cpp, and I found it to be the best open-weight model I've ever ran locally ((Unsloth)_Qwen3-Next-80B-A3B-Instruct-GGUF-Q6_K_XL). It's also the first model I could run at full context size (256K) on a single RTX3090 (forcing model expert weights onto CPU, obviously) at around 12t/s.&lt;/p&gt; &lt;p&gt;Before, you say &amp;quot;oh, that's so slow&amp;quot;, let me clarify that a 12t/s speed is twice as fast as I can ever read. Also, just last year, people were happy to run llama3-70B at an average speed of 5t/s, and 2 years ago, people were happy to run llama2-7B (8K context size ü§¶‚Äç‚ôÄÔ∏è) at 12t/s.&lt;/p&gt; &lt;p&gt;Today, I tried (Unsloth)_Nemotron-3-Nano-30B-A3B-GGUF-Q8_K_XL at full context size (1M ü§Ø), and the speed is around 12.5t/s (again, forcing model expert weights onto CPU, obviously). The full context uses 12.6GB of VRAM, leaving me with about 11GB of free VRAM üåãü§Ø. I tested it's recall capability up to 80K, and the model is solid, with almost no context degradation that I can tell.&lt;/p&gt; &lt;p&gt;So, if it's not obvious to some already, this Mamba2-Transformer Hybrid MoE architecture is here so stay. AI Labs must now improve models recall capabilities to truly benefit from in-context learning. I am no expert in the field, and please feel free to interject and correct me if I am wrong, but I think if a smaller model is well trained to fully utilize long context to draw conclusions or discover knowledge it was not trained on, if will allow for the shipping of smaller yet capable models.&lt;/p&gt; &lt;p&gt;My point is, we don't need a model that holds all the human knowledge in its weights, but one that is trained to derive or rediscover unseen knowledge and build upon that to solve novel problems. In other words, I think if a model can reason about novel data, it would reuse the same parameters for many domains, dramatically reducing the size of the training corpus needed to reach a given capability ceiling.&lt;/p&gt; &lt;p&gt;I think if this is achieved, we can expect a decrease in training costs and an increase in model intelligence. We might even see a better model generalization very soon.&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pny30h/the_attention_hybrid_moe_architecture_is_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:52:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3ln2</id>
    <title>Key Highlights of NVIDIA‚Äôs New Model: Nemotron-Cascade-8B</title>
    <updated>2025-12-16T14:40:35+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3ln2/key_highlights_of_nvidias_new_model/"&gt; &lt;img alt="Key Highlights of NVIDIA‚Äôs New Model: Nemotron-Cascade-8B" src="https://external-preview.redd.it/2QEDEkegLrJTJtx6HLSiu0oL0Rwu2mfV0busJyR6xa4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=931815c93c1175064ff4b44e489163b0700b3b19" title="Key Highlights of NVIDIA‚Äôs New Model: Nemotron-Cascade-8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] General-Purpose Reinforcement-Learned Model&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Trained through a sequential and domain-wise reinforcement learning pipeline built on top of a base Qwen3-8B model, enhancing performance across diverse task domains&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Dual Reasoning &amp;amp; Instruction Modes&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports both &lt;em&gt;thinking&lt;/em&gt; (reasoning) and &lt;em&gt;instruct&lt;/em&gt; (non-reasoning) modes, allowing flexible use cases within the same model architecture.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] Strong Benchmark Performance&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Achieves competitive results on knowledge, reasoning, alignment, math, and code benchmarks, with metrics comparable to much larger models in several evaluations.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Open Model Release &amp;amp; License&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Released with the NVIDIA Open Model License and openly available for community use, research, and customization.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Nemotron-Cascade-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3ln2/key_highlights_of_nvidias_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3ln2/key_highlights_of_nvidias_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxkvw</id>
    <title>llama.cpp support for Nemotron 3 Nano merged!</title>
    <updated>2025-12-16T09:17:48+00:00</updated>
    <author>
      <name>/u/QuackerEnte</name>
      <uri>https://old.reddit.com/user/QuackerEnte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7418"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7418&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Details&lt;/p&gt; &lt;p&gt;llama : add support for NVIDIA Nemotron 3 Nano (#18058)&lt;/p&gt; &lt;p&gt;llama : add support for NVIDIA Nemotron Nano 3 This commit adds support for the NVIDIA Nemotron Nano 3 model, enabling the conversion and running of this model.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuackerEnte"&gt; /u/QuackerEnte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxkvw/llamacpp_support_for_nemotron_3_nano_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:17:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1po97ad</id>
    <title>Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)</title>
    <updated>2025-12-16T18:15:16+00:00</updated>
    <author>
      <name>/u/HuseyinKama</name>
      <uri>https://old.reddit.com/user/HuseyinKama</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"&gt; &lt;img alt="Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)" src="https://b.thumbs.redditmedia.com/aSafmamtZw_FO6TPvBeRD2HjUbX9rDJK6GJUvvHUwxo.jpg" title="Finally managed to run Qwen-2.5-7B on a 4GB GTX 1050 without CPU offloading (Surgical Memory Alignment)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a weekend project that grew into something bigger. Like many of you, I'm stuck with low-end hardware (a glorious &lt;strong&gt;GTX 1050 with 4GB VRAM&lt;/strong&gt;).&lt;/p&gt; &lt;p&gt;Every time I tried to load a modern 7B model (like Llama-3 or Qwen-2.5), I hit the dreaded OOM wall. The files were technically small enough (~3.9GB), but the fragmentation and padding overhead during inference always pushed usage just over 4GB, forcing me to offload layers to the CPU (which kills speed).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; I realized that standard GGUF quantization tools often prioritize block size uniformity over memory efficiency. They add &amp;quot;zero-padding&amp;quot; to tensors to make them fit standard block sizes. On a 24GB card, you don't care. On a 4GB card, that 50-100MB of wasted padding is fatal.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Solution (QKV Core):&lt;/strong&gt; I wrote a custom framework to handle what I call &lt;strong&gt;&amp;quot;Surgical Alignment.&amp;quot;&lt;/strong&gt; Instead of blindly padding, it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Analyzes the entropy of each layer.&lt;/li&gt; &lt;li&gt;Switches between Dictionary Coding and Raw Storage.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crucially:&lt;/strong&gt; It trims and realigns memory blocks to strictly adhere to &lt;code&gt;llama.cpp&lt;/code&gt;'s block boundaries (e.g., 110-byte alignment for Q3_K) without the usual padding waste.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;VRAM:&lt;/strong&gt; Saved about &lt;strong&gt;44MB&lt;/strong&gt; per model, which was enough to keep the entire Qwen-2.5-7B purely on GPU. No more crashes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speed:&lt;/strong&gt; Because the blocks are cache-aligned, I saw a &lt;strong&gt;~34% improvement in I/O load times&lt;/strong&gt; (8.2s vs 12.5s) using Numba-accelerated kernels.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm open-sourcing this as &lt;strong&gt;QKV Core&lt;/strong&gt;. It‚Äôs still early/experimental, but if you have a 4GB/6GB card and are struggling with OOMs, this might save you.&lt;/p&gt; &lt;p&gt;Here are the benchmarks comparing standard vs. surgical alignment: &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hpytxtcbxl7g1.png?width=2961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=554e38ec8de4f5af5cd33f6535b7e3d2aa67651e"&gt;https://preview.redd.it/hpytxtcbxl7g1.png?width=2961&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=554e38ec8de4f5af5cd33f6535b7e3d2aa67651e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/QKV-Core/QKV-Core"&gt;https://github.com/QKV-Core/QKV-Core&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love to hear your feedback on the quantization logic!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HuseyinKama"&gt; /u/HuseyinKama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po97ad/finally_managed_to_run_qwen257b_on_a_4gb_gtx_1050/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T18:15:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnfaqo</id>
    <title>I'm strong enough to admit that this bugs the hell out of me</title>
    <updated>2025-12-15T18:40:57+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt; &lt;img alt="I'm strong enough to admit that this bugs the hell out of me" src="https://preview.redd.it/9xkz6sfcxe7g1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=55149229a153c6c87d61ae1aa53e61a1b3a65df8" title="I'm strong enough to admit that this bugs the hell out of me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9xkz6sfcxe7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnfaqo/im_strong_enough_to_admit_that_this_bugs_the_hell/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T18:40:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnusp9</id>
    <title>Alibaba Open-Sources CosyVoice 3, a New TTS Model</title>
    <updated>2025-12-16T06:16:09+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Coverage&lt;/strong&gt;: Covers 9 common languages (Chinese, English, Japanese, Korean, German, Spanish, French, Italian, Russian), 18+ Chinese dialects/accents and meanwhile supports both multi-lingual/cross-lingual zero-shot voice cloning.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Content Consistency &amp;amp; Naturalness&lt;/strong&gt;: Achieves state-of-the-art performance in content consistency, speaker similarity, and prosody naturalness.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pronunciation Inpainting&lt;/strong&gt;: Supports pronunciation inpainting of Chinese Pinyin and English CMU phonemes, providing more controllability and thus suitable for production use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Normalization&lt;/strong&gt;: Supports reading of numbers, special symbols and various text formats without a traditional frontend module.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bi-Streaming&lt;/strong&gt;: Support both text-in streaming and audio-out streaming, and achieves latency as low as 150ms while maintaining high-quality audio output.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Instruct Support&lt;/strong&gt;: Supports various instructions such as languages, dialects, emotions, speed, volume, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weight: &lt;a href="https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512"&gt;https://huggingface.co/FunAudioLLM/Fun-CosyVoice3-0.5B-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.17589"&gt;https://arxiv.org/abs/2505.17589&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnusp9/alibaba_opensources_cosyvoice_3_a_new_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T06:16:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1po2slg</id>
    <title>My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)</title>
    <updated>2025-12-16T14:06:42+00:00</updated>
    <author>
      <name>/u/Outrageous-Yak8298</name>
      <uri>https://old.reddit.com/user/Outrageous-Yak8298</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt; &lt;img alt="My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)" src="https://b.thumbs.redditmedia.com/DaBXg_p6QRIuS4sCb73zScs5SsGoLmqoDUfn34hBixE.jpg" title="My professor lent me an A6000, so I tried to build a coding model. Here is Anni! (Qwen3-14B Fine-tune)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Feedback and suggestions are welcomed! &lt;a href="https://hanstan.link/how-i-trained-a-sota-coding-model-on-a-single-gpu/"&gt;Full Technical Write-up&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm a 2nd year undergrad AI student and just finished training my very first LLM. Like many of you, I wanted to train a capable coding model but didn't have a cluster of H100s‚Äîjust a single &lt;strong&gt;Nvidia A6000 (48GB) thanks to my professor :)&lt;/strong&gt; and a dream!&lt;/p&gt; &lt;p&gt;I spent the last few months building &lt;strong&gt;Anni&lt;/strong&gt; &lt;a href="https://github.com/CoderUni/Anni"&gt;&lt;strong&gt;https://github.com/CoderUni/Anni&lt;/strong&gt;&lt;/a&gt;, a 14B Qwen3-based model fine-tuned on the &lt;strong&gt;Nvidia OpenCodeReasoning-2&lt;/strong&gt; dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model:&lt;/strong&gt; Qwen3-14B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; Single A6000 (48GB VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Time:&lt;/strong&gt; Reduced from ~1.6 months (projected) to &lt;strong&gt;~2 weeks&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Score:&lt;/strong&gt; &lt;strong&gt;41.7% Pass@1&lt;/strong&gt; on LiveCodeBench (v6), theoretically matching &lt;strong&gt;Claude 3.5 Sonnet (Thinking)&lt;/strong&gt; and beating &lt;strong&gt;GPT-4o&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The &amp;quot;SOTA&amp;quot; Benchmark Reality Check (Please Read)&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qwbv16c4pk7g1.jpg?width=1740&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1975e04ca21c0dfa9d746a3ab479a4e2c8d93a2c"&gt;https://preview.redd.it/qwbv16c4pk7g1.jpg?width=1740&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=1975e04ca21c0dfa9d746a3ab479a4e2c8d93a2c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Before anyone calls it out, I want to be 100% transparent: &lt;strong&gt;This benchmark score is likely contaminated.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After seeing the crazy numbers, I couldn't believe I beat last year's SOTA models and investigated. I then found out that the LiveCodeBench (v6) questions are from &lt;strong&gt;April‚ÄìMay 2025&lt;/strong&gt;. My training dataset (OpenCodeReasoning-2) was curated between &lt;strong&gt;March‚ÄìMay 2025&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I would love to test it on problems released &lt;strong&gt;after June 2025&lt;/strong&gt; once LCB v7 comes out!&lt;/p&gt; &lt;p&gt;Despite my best efforts to deduplicate the data using content-based hashing, there is a high probability the model &amp;quot;saw&amp;quot; the test questions during training.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Did I beat Nvidia's Nemotron 1.1 model?&lt;/strong&gt; Unlikely.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Does it demonstrate that a student can realistically train a model that comes close to SOTA models?&lt;/strong&gt; Absolutely.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How I decreased training times and fit this in one GPU&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;I initially thought I could simply blindly follow tutorials without understanding the fundamentals.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DO NOT DO IT! Take your time to learn and understand the fundamentals! It's the best decision you will ever make! It helped me in the long run.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After going through many research reports and &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; posts, I learned how to optimize everything to get this done in 2 weeks instead of 2 months. Here is what worked:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Progressive Training:&lt;/strong&gt; I didn't train on 32k context immediately. I split training into 4 stages, starting with &amp;quot;easy&amp;quot; short samples (0-4k tokens) and progressively scaling to &amp;quot;hard&amp;quot; long contexts (up to 32k). This stabilized loss and sped up convergence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early Stopping:&lt;/strong&gt; I realized convergence happened way faster than expected on high-quality synthetic data, saving weeks of compute.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;&amp;quot;Hacky&amp;quot; Deployment:&lt;/strong&gt; Since I can't afford a permanent GPU instance, I served the model using &lt;strong&gt;vLLM&lt;/strong&gt; inside a Colab instance, tunneled out via &lt;strong&gt;Ngrok&lt;/strong&gt; to a custom Next.js frontend. It‚Äôs janky, but it works for free.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Blog post&lt;/h1&gt; &lt;p&gt;&lt;a href="https://hanstan.link/how-i-trained-a-high-performance-coding-model-on-a-single-gpu/"&gt;https://hanstan.link/how-i-trained-a-high-performance-coding-model-on-a-single-gpu/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I took a long time writing a deep dive into how I built Anni and the challenges I faced (Unsloth bugs, GGUF export issues, and the exact curriculum schedule). I hope that someone would be able to find it useful!&lt;/p&gt; &lt;h1&gt;Links&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Hugging Face:&lt;/strong&gt; &lt;a href="https://huggingface.co/BigJuicyData/Anni"&gt;https://huggingface.co/BigJuicyData/Anni&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GGUF:&lt;/strong&gt; &lt;a href="https://huggingface.co/BigJuicyData/Anni-Q4_K_M-GGUF"&gt;https://huggingface.co/BigJuicyData/Anni-Q4_K_M-GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to roast the model or training process! I would greatly appreciate it since I would really like to learn!&lt;/p&gt; &lt;p&gt;Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous-Yak8298"&gt; /u/Outrageous-Yak8298 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po2slg/my_professor_lent_me_an_a6000_so_i_tried_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:06:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz80z</id>
    <title>I may have over-quantized this little guy.</title>
    <updated>2025-12-16T11:04:26+00:00</updated>
    <author>
      <name>/u/AllergicToTeeth</name>
      <uri>https://old.reddit.com/user/AllergicToTeeth</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"&gt; &lt;img alt="I may have over-quantized this little guy." src="https://preview.redd.it/35p9o4zosj7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f20379a29c30a291fbfb4ccd8cb2c67757d7a55" title="I may have over-quantized this little guy." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllergicToTeeth"&gt; /u/AllergicToTeeth &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/35p9o4zosj7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz80z/i_may_have_overquantized_this_little_guy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:04:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1po18y9</id>
    <title>GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)</title>
    <updated>2025-12-16T12:56:27+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt; &lt;img alt="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" src="https://external-preview.redd.it/bLrsVXDvN3_NMKaZkcGBPVdeuTpEZp7rVIyw-KAF9KY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc4cc8f4e345c1545572514e6454ad7fa760089d" title="GLM-4.5V, GLM-4.6V and GLM_4.6V-Flash are now supported by llama.cpp (GGUFs)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you need this&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1pnz1je/support_for_glm4v_vision_encoder_has_been_merged/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ggml-org/glm-4v"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po18y9/glm45v_glm46v_and_glm_46vflash_are_now_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T12:56:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1po8yt0</id>
    <title>I was bored</title>
    <updated>2025-12-16T18:06:24+00:00</updated>
    <author>
      <name>/u/MyLovelyAngelKirino</name>
      <uri>https://old.reddit.com/user/MyLovelyAngelKirino</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/"&gt; &lt;img alt="I was bored" src="https://preview.redd.it/nhl4dnk9wl7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=729557fb960d4bec26e17dfb24132426e8a0ca3a" title="I was bored" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Being unemployed and having to much hardware and too much time on my hands I built this.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MyLovelyAngelKirino"&gt; /u/MyLovelyAngelKirino &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nhl4dnk9wl7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po8yt0/i_was_bored/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T18:06:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnz9xu</id>
    <title>Qwen3 Next speed optimization has been merged into llama.cpp</title>
    <updated>2025-12-16T11:07:37+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt; &lt;img alt="Qwen3 Next speed optimization has been merged into llama.cpp" src="https://external-preview.redd.it/DvlPrtOQd3Cfpjgulr94g-6gX7cbuY0-dqBY_cGanOg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=51f4d927a593a1d76b03526eda2d2fe2ba251bc9" title="Qwen3 Next speed optimization has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17996"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnz9xu/qwen3_next_speed_optimization_has_been_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T11:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1po78bl</id>
    <title>Allen Institute for AI introduces Molmo 2</title>
    <updated>2025-12-16T17:01:44+00:00</updated>
    <author>
      <name>/u/Agitated_Camel1886</name>
      <uri>https://old.reddit.com/user/Agitated_Camel1886</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"&gt; &lt;img alt="Allen Institute for AI introduces Molmo 2" src="https://external-preview.redd.it/rfRzO8US_wiYh7h-OdrK9nkP4gi6Ae-_y-DBzDUMAug.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5e9a124c28a948901092f449c8c7cbba0b01ac87" title="Allen Institute for AI introduces Molmo 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1po78bl/video/v5jtc9a7wl7g1/player"&gt;https://reddit.com/link/1po78bl/video/v5jtc9a7wl7g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Allen Institute for AI (Ai2)'s website: &lt;a href="https://allenai.org/molmo"&gt;https://allenai.org/molmo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am super impressed by the ability to analyze videos (Video QA, Counting and pointing, Dense captioning), and it's only 8B!!&lt;/p&gt; &lt;p&gt;HuggingFace: &lt;a href="https://huggingface.co/allenai/Molmo2-8B"&gt;https://huggingface.co/allenai/Molmo2-8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agitated_Camel1886"&gt; /u/Agitated_Camel1886 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po78bl/allen_institute_for_ai_introduces_molmo_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T17:01:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1po7i0c</id>
    <title>Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts.</title>
    <updated>2025-12-16T17:11:50+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"&gt; &lt;img alt="Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts." src="https://external-preview.redd.it/aHN2Ynl2OXlsbDdnMcH321aC77jYYB3hpLEwmsgN4qk6KsN77tikHsTNkpxK.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ba962072bea73833102870325afc6e1184ffaa05" title="Meta announced a new SAM Audio Model for audio editing that can segment sound from complex audio mixtures using text, visual, and time span prompts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/"&gt;https://about.fb.com/news/2025/12/our-new-sam-audio-model-transforms-audio-editing/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SAM Audio transforms audio processing by making it easy to isolate any sound from complex audio mixtures using text, visual, and time span prompts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/yoiaaoayll7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po7i0c/meta_announced_a_new_sam_audio_model_for_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T17:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1po3bn4</id>
    <title>XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face</title>
    <updated>2025-12-16T14:29:04+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"&gt; &lt;img alt="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" src="https://external-preview.redd.it/pEssBYcofSIqxenRV_1O2yb3vr7ekZdMtNbDln2iEbQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=413fe05449bcb79ceb4c3c13d870125113113e50" title="XiaomiMiMo/MiMo-V2-Flash ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/XiaomiMiMo/MiMo-V2-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1po3bn4/xiaomimimomimov2flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T14:29:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pnxekt</id>
    <title>It was Ilya who "closed" OpenAI</title>
    <updated>2025-12-16T09:05:33+00:00</updated>
    <author>
      <name>/u/licuphand</name>
      <uri>https://old.reddit.com/user/licuphand</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt; &lt;img alt="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" src="https://preview.redd.it/rn6rsl7p7j7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9fd882c08aa9fff702ae363b643c6636cc846267" title="It was Ilya who &amp;quot;closed&amp;quot; OpenAI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/licuphand"&gt; /u/licuphand &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rn6rsl7p7j7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pnxekt/it_was_ilya_who_closed_openai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-16T09:05:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pniwfj</id>
    <title>Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams.</title>
    <updated>2025-12-15T21:02:55+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt; &lt;img alt="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." src="https://b.thumbs.redditmedia.com/bsv34WIHZXC49Az9mFES5lSIAtaQ2CuLZJ4dCaLxsEY.jpg" title="Ai2 Open Modeling AMA ft researchers from the Molmo and Olmo teams." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre researchers and engineers from Ai2, the nonprofit AI lab. We recently announced:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2&lt;/strong&gt;‚Äîopen multimodal models for video + images that can return grounded answers (pixel coordinates + timestamps), trained with open datasets&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3&lt;/strong&gt;‚Äîa family of fully open language models (7B‚Äì32B) with Base/Instruct/Thinking variants, long‚Äëcontext support, open training recipes &amp;amp; checkpoints&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Ask us anything about local inference, training mixes &amp;amp; our truly open approach, long‚Äëcontext, grounded video QA/tracking, and real‚Äëworld deployment.&lt;/p&gt; &lt;p&gt;Participating in the AMA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Molmo 2 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Ranjay Krishna ( &lt;a href="/u/ranjaykrishna"&gt;u/ranjaykrishna&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Zixian Ma ( &lt;a href="/u/Frequent_Rooster2980"&gt;u/Frequent_Rooster2980&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Chris Clark ( &lt;a href="/u/mostly_reasonable"&gt;u/mostly_reasonable&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Jieyu Zhang ( &lt;a href="/u/Jealous_Programmer51"&gt;u/Jealous_Programmer51&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Olmo 3 researchers:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Kyle Lo ( &lt;a href="/u/klstats"&gt;u/klstats&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Allyson Ettinger ( &lt;a href="/u/aeclang"&gt;u/aeclang&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Finbarr Timbers ( &lt;a href="/u/fnbr"&gt;u/fnbr&lt;/a&gt; )&lt;/li&gt; &lt;li&gt;Faeze Brahman ( &lt;a href="/u/faebrhn"&gt;u/faebrhn&lt;/a&gt; )&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôll be live from &lt;strong&gt;1pm&lt;/strong&gt; to &lt;strong&gt;2pm PST.&lt;/strong&gt; Read up on our latest releases below, and feel welcome to jump in anytime!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚ñ∂Ô∏è &lt;strong&gt;Try in the Playground:&lt;/strong&gt; &lt;a href="https://playground.allenai.org"&gt;https://playground.allenai.org&lt;/a&gt;&lt;/li&gt; &lt;li&gt;‚¨áÔ∏è &lt;strong&gt;Download&lt;/strong&gt;: &lt;a href="https://huggingface.co/collections/allenai/molmo2"&gt;https://huggingface.co/collections/allenai/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìù &lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href="https://allenai.org/blog/molmo2"&gt;https://allenai.org/blog/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑReport: &lt;a href="https://allenai.org/papers/molmo2"&gt;https://allenai.org/papers/molmo2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª &lt;strong&gt;API coming soon&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;ü´Ü PROOF:&lt;/strong&gt; &lt;a href="https://x.com/allen_ai/status/2000692253606514828"&gt;https://x.com/allen_ai/status/2000692253606514828&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Join us on Reddit&lt;/strong&gt; &lt;a href="/r/allenai"&gt;r/allenai&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;Join Ai2 on Discord:&lt;/strong&gt; &lt;a href="https://discord.gg/6vWDHyTCQV"&gt;https://discord.gg/6vWDHyTCQV&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8"&gt;https://preview.redd.it/fxw1g2fcmf7g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=009a9377edfefefc5efd52db0af81b807b9971b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pniwfj/ai2_open_modeling_ama_ft_researchers_from_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-15T21:02:55+00:00</published>
  </entry>
</feed>
