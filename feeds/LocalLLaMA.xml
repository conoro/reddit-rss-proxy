<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-22T16:53:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oct4ug</id>
    <title>AI developers can now run LLMs or other AI workloads on ARM-based MacBooks with the power of Nvidia RTX GPUs.</title>
    <updated>2025-10-21T23:55:44+00:00</updated>
    <author>
      <name>/u/ANR2ME</name>
      <uri>https://old.reddit.com/user/ANR2ME</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/tiny-corp-successfully-runs-an-nvidia-gpu-on-arm-macbook-through-usb4-using-an-external-gpu-docking-station"&gt;https://www.tomshardware.com/pc-components/gpus/tiny-corp-successfully-runs-an-nvidia-gpu-on-arm-macbook-through-usb4-using-an-external-gpu-docking-station&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;The main issue is that TinyCorp's drivers only work with Nvidia GPUs featuring a GPU system processor, which is why no GTX-series graphics cards are supported. AMD GPUs based on RDNA 2, 3, and 4 reportedly work as well.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANR2ME"&gt; /u/ANR2ME &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oct4ug/ai_developers_can_now_run_llms_or_other_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oct4ug/ai_developers_can_now_run_llms_or_other_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oct4ug/ai_developers_can_now_run_llms_or_other_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T23:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1od6gg8</id>
    <title>When a realization hits after listening to Andrej Karpathy</title>
    <updated>2025-10-22T12:08:32+00:00</updated>
    <author>
      <name>/u/martinerous</name>
      <uri>https://old.reddit.com/user/martinerous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od6gg8/when_a_realization_hits_after_listening_to_andrej/"&gt; &lt;img alt="When a realization hits after listening to Andrej Karpathy" src="https://b.thumbs.redditmedia.com/rt6F15aTvLpbOALS9I2VjePP1Gwfk9hp87XfuN1oG_Q.jpg" title="When a realization hits after listening to Andrej Karpathy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gwm5p912lnwf1.jpg?width=602&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80c6e8c3de2768bb0483431b293735cb8422129"&gt;https://preview.redd.it/gwm5p912lnwf1.jpg?width=602&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c80c6e8c3de2768bb0483431b293735cb8422129&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For context: &lt;a href="https://www.dwarkesh.com/p/andrej-karpathy"&gt;https://www.dwarkesh.com/p/andrej-karpathy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? Is there any solution possible to not reward messy or totally irrelevant chains of thought even when LLM somehow ends up with a correct answer? Is any company actually doing something about it already?&lt;/p&gt; &lt;p&gt;Without such mechanisms, it smells a bit like cargo cult. &amp;quot;Thinking is good, I'll think tralalala trololo.... The answer to 1+1 is 2.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martinerous"&gt; /u/martinerous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od6gg8/when_a_realization_hits_after_listening_to_andrej/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od6gg8/when_a_realization_hits_after_listening_to_andrej/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od6gg8/when_a_realization_hits_after_listening_to_andrej/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T12:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1odasbj</id>
    <title>GPT-OSS-20b TAKE THE HELM! Further experiments in autopilot.</title>
    <updated>2025-10-22T15:04:54+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odasbj/gptoss20b_take_the_helm_further_experiments_in/"&gt; &lt;img alt="GPT-OSS-20b TAKE THE HELM! Further experiments in autopilot." src="https://external-preview.redd.it/ZspUdJpzHTOkiEa4knHQwTZYSVuilpI-6KVKwszZkLU.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5807a68162444fb98dc8bc5d6f30eaf44af008c3" title="GPT-OSS-20b TAKE THE HELM! Further experiments in autopilot." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/Deveraux-Parker/GPT-OSS-20b-TAKE-THE-WHEEL"&gt;Github...&lt;/a&gt;&lt;/p&gt; &lt;p&gt;After fiddling around the other day I did a little more messing with gpt-oss-20b and prompting to get it to be a bit more reliable at flying/shooting/controlling the spaceship.&lt;/p&gt; &lt;p&gt;The basic idea is that the system calculates bad and good control choices and feeds the AI a list of options with pre-filled &amp;quot;thinking&amp;quot; on the choices that encourage it to make correct choices. It is still given agency and does deviate from perfect flight from time to time (and will eventually crash as you see here).&lt;/p&gt; &lt;p&gt;To allow fast-paced decision making, this whole stack is running gpt-oss-20b in VLLM on a 4090, and since each generation is only looking to output a single token (that represents a single control input), it allows the system to run in near-realtime. The look-ahead code tries to predict and mitigate the already low latency and the result is an autopilot that is actually reasonably good at flying the ship.&lt;/p&gt; &lt;p&gt;I went ahead and collapsed everything into a single HTML file if you feel like messing with it, and tossed it at the github link above. You'll need an openAI spec API to use it with gpt-oss-20b running on port 8005 (or have to edit the file appropriately to match your own system).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=Yo7GWnGtpoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odasbj/gptoss20b_take_the_helm_further_experiments_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odasbj/gptoss20b_take_the_helm_further_experiments_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:04:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocubgr</id>
    <title>AlphaXiv,Compare the Deepseek-OCR and Mistral-OCR OCR models</title>
    <updated>2025-10-22T00:50:12+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocubgr/alphaxivcompare_the_deepseekocr_and_mistralocr/"&gt; &lt;img alt="AlphaXiv,Compare the Deepseek-OCR and Mistral-OCR OCR models" src="https://preview.redd.it/cad0dcl99kwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=92a74808e6f96bf30624a7c3deeabbfd027cf384" title="AlphaXiv,Compare the Deepseek-OCR and Mistral-OCR OCR models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cad0dcl99kwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocubgr/alphaxivcompare_the_deepseekocr_and_mistralocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocubgr/alphaxivcompare_the_deepseekocr_and_mistralocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T00:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocgun0</id>
    <title>DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds...</title>
    <updated>2025-10-21T16:00:06+00:00</updated>
    <author>
      <name>/u/Xtianus21</name>
      <uri>https://old.reddit.com/user/Xtianus21</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"&gt; &lt;img alt="DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds..." src="https://b.thumbs.redditmedia.com/Dsj93JBsqXXjXoOsuIY5K8HbnnE2QjHtG5dVlESoKDU.jpg" title="DeepSeek-OCR AI can scan an entire microfiche sheet and not just cells and retain 100% of the data in seconds..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/BrianRoemmele/status/1980634806145957992"&gt;https://x.com/BrianRoemmele/status/1980634806145957992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;AND &lt;/p&gt; &lt;p&gt;Have a full understanding of the text/complex drawings and their context. &lt;/p&gt; &lt;p&gt;I just changed offline data curation!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xtianus21"&gt; /u/Xtianus21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ocgun0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocgun0/deepseekocr_ai_can_scan_an_entire_microfiche/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:00:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1od7jll</id>
    <title>DeepSeek-OCR encoder as a tiny Python package (encoder-only tokens, CUDA/BF16, 1-liner install)</title>
    <updated>2025-10-22T12:57:19+00:00</updated>
    <author>
      <name>/u/Exciting_Traffic_667</name>
      <uri>https://old.reddit.com/user/Exciting_Traffic_667</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If youâ€™re benchmarking the new &lt;strong&gt;DeepSeek-OCR&lt;/strong&gt; on local stacks, this package (that I made) exposes the &lt;strong&gt;encoder&lt;/strong&gt; directlyâ€”skip the decoder and just get the vision tokens.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Encoder-only: returns [1, N, 1024] tokens for your downstream OCR/doc pipelines.&lt;/li&gt; &lt;li&gt;Speed/VRAM: BF16 + optional CUDA Graphs; avoids full VLM runtime.&lt;/li&gt; &lt;li&gt;Install:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;``` pip install deepseek-ocr-encoder&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Minimal example (HF Transformers):&lt;/p&gt; &lt;p&gt;``` from transformers import AutoModel from deepseek_ocr_encoder import DeepSeekOCREncoder import torch&lt;/p&gt; &lt;p&gt;m = AutoModel.from_pretrained(&amp;quot;deepseek-ai/DeepSeek-OCR&amp;quot;, trust_remote_code=True, use_safetensors=True, torch_dtype=torch.bfloat16, attn_implementation=&amp;quot;eager&amp;quot;).eval().to(&amp;quot;cuda&amp;quot;, dtype=torch.bfloat16) enc = DeepSeekOCREncoder(m, device=&amp;quot;cuda&amp;quot;, dtype=torch.bfloat16, freeze=True) print(enc(&amp;quot;page.png&amp;quot;).shape) ```&lt;/p&gt; &lt;p&gt;Links: &lt;a href="https://pypi.org/project/deepseek-ocr-encoder/"&gt;https://pypi.org/project/deepseek-ocr-encoder/&lt;/a&gt; &lt;a href="https://github.com/dwojcik92/deepseek-ocr-encoder"&gt;https://github.com/dwojcik92/deepseek-ocr-encoder&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Exciting_Traffic_667"&gt; /u/Exciting_Traffic_667 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7jll/deepseekocr_encoder_as_a_tiny_python_package/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7jll/deepseekocr_encoder_as_a_tiny_python_package/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od7jll/deepseekocr_encoder_as_a_tiny_python_package/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T12:57:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oclug7</id>
    <title>Getting most out of your local LLM setup</title>
    <updated>2025-10-21T19:05:35+00:00</updated>
    <author>
      <name>/u/Everlier</name>
      <uri>https://old.reddit.com/user/Everlier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, been active LLM user since before LLama 2 weights, running my first inference of Flan-T5 with &lt;code&gt;transformers&lt;/code&gt; and later &lt;code&gt;ctranslate2&lt;/code&gt;. We regularly discuss our local setups here and I've been rocking mine for a couple of years now, so I have a few things to share. Hopefully some of them will be useful for your setup too. I'm not using an LLM to write this, so forgive me for any mistakes I made.&lt;/p&gt; &lt;h1&gt;Dependencies&lt;/h1&gt; &lt;p&gt;Hot topic. When you want to run 10-20 different OSS projects for the LLM lab - containers are almost a must. Image sizes are really unfortunate (especially with Nvidia stuff), but it's much less painful to store 40GBs of images locally than spending an entire evening on Sunday figuring out some obscure issue between Python / Node.js / Rust / Go dependencies. Setting it up is a one-time operation, but it simplifies upgrades and portability of your setup by a ton. Both Nvidia and AMD have very decent support for container runtimes, typically with a plugin for the container engine. Speaking about one - doesn't have to be Docker, but often it saves time to have the same bugs as everyone else.&lt;/p&gt; &lt;h1&gt;Choosing a Frontend&lt;/h1&gt; &lt;p&gt;The only advice I can give here is not to choose any single specific one, cause most will have their own disadvantages. I tested a lot of different ones, here is the gist:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Open WebUI&lt;/strong&gt; - has more features than you'll ever need, but can be tricky to setup/maintain. Using containerization really helps - you set it up one time and forget about it. One of the best projects in terms of backwards compatibility, I've started using it when it was called Ollama WebUI and all my chats were preserved through all the upgrades up to now.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Chat Nio&lt;/strong&gt; - can only recommend if you want to setup an LLM marketplace for some reason.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hollama&lt;/strong&gt; - my go-to when I want a quick test of some API or model, you don't even need to install it in fact, it works perfectly fine from their GitHub pages (use it like that only if you know what you're doing though).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace ChatUI&lt;/strong&gt; - very basic, but without any feature bloat.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KoboldCpp&lt;/strong&gt; - AIO package, less polished than the other projects, but have these &amp;quot;crazy scientist&amp;quot; vibes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lobe Chat&lt;/strong&gt; - similarly countless features like Open WebUI, but less polished and coherent, UX can be confusing at times. However, has a lot going on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LibreChat&lt;/strong&gt; - another feature-rich Open WebUI alternative. Configuration can be a bit more confusing though (at least for me) due to a wierd approach to defining models and backends to connect to as well as how to fetch model lists from them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mikupad&lt;/strong&gt; - another &amp;quot;crazy scientist&amp;quot; project. Has a unique approach to generation and editing of the content. Supports a lot of lower-level config options compared to other frontends.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parllama&lt;/strong&gt; - probably most feature-rich TUI frontend out there. Has a lot of features you would only expect to see in a web-based UI. A bit heavy, can be slow.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;oterm&lt;/strong&gt; - Ollama-specific, terminal-based, quite lightweight compared to some other options.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;aichat&lt;/strong&gt; - Has a very generic name (in the &lt;code&gt;sigoden&lt;/code&gt;s GitHub), but is one of the simplest LLM TUIs out there. Lightweight, minimalistic, and works well for a quick chat in terminal or some shell assistance.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;gptme&lt;/strong&gt; - Even simpler than &lt;code&gt;aichat&lt;/code&gt;, with some agentic features built-in.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open Interpreter&lt;/strong&gt; - one of the OG TUI agents, looked very cool then got some funding then went silent and now it's not clear what's happening with it. Based on approaches that are quite dated now, so not worth trying unless you're curious about this one specifically.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The list above is of course not exhaustive, but these are the projects I had a chance to try myself. In the end, I always return to Open WebUI as after initial setup it's fairly easy to start and it has more features than I could ever need.&lt;/p&gt; &lt;h1&gt;Choosing a Backend&lt;/h1&gt; &lt;p&gt;Once again, no single best option here, but there are some clear &amp;quot;niche&amp;quot; choices depending on your use case.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;llama.cpp&lt;/strong&gt; - not much to say, you probably know everything about it already. Great (if not only) for lightweight or CPU-only setups.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ollama&lt;/strong&gt; - when you simply don't have time to read &lt;code&gt;llama.cpp&lt;/code&gt; docs, or compiling it from scratch. It's up to you to decide on the attribution controversy and I'm not here to judge.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;vllm&lt;/strong&gt; - for a homelab, I can only recommend it if you have: a) Hardware, b) Patience, c) A specific set of models you run, d) a few other people that want to use your LLM with you. Goes one level deeper compared to &lt;code&gt;llama.cpp&lt;/code&gt; in terms of configurability and complexity, requires hunting for specific quants.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aphrodite&lt;/strong&gt; - If you chose KoboldCpp over Open WebUI, you're likely to choose Aphrodite over vllm.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;KTransformers&lt;/strong&gt; - When you're trying to hunt down every last bit of performance your rig can provide. Has some very specific optimisation for specific hardware and specific LLM architectures.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mistral.rs&lt;/strong&gt; - If you code in Rust, you might consider this over llama.cpp. The lead maintainer is very passionate about the project and often adds new architectures/features ahead of other backneds. At the same time, the project is insanely big, so things often take time to stabilize. Has some unique features that you won't find anywhere else: AnyMoE, ISQ quants, supports diffusion models, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modular MAX&lt;/strong&gt; - inference engine from creators of Mojo language. Meant to transform ML and LLM inference in general, but work is still in early stages. Models take ~30s to compile on startup. Typically runs the original FP16 weights, so requires beefy GPUs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexa SDK&lt;/strong&gt; - if you want something similar to Ollama, but you don't want Ollama itself. Concise CLI, supports a variety of architectures. Has bugs and usability issues due to a smaller userbase, but is actively developed. Recently been noted in some sneaky self-promotion.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SGLang&lt;/strong&gt; - similar to &lt;code&gt;ktransformers&lt;/code&gt;, highly optimised for specific hardware and model architectures, but requires a lot of involvement for configuration and setup.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TabbyAPI&lt;/strong&gt; - wraps Exllama2 and Exllama3 with a more convenient and easy-to-use package that one would expect from an inference engine. Approximately at the same level of complexity as &lt;code&gt;vllm&lt;/code&gt; or &lt;code&gt;llama.cpp&lt;/code&gt;, but requires more specific quants.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;HuggingFace Text Generation Inference&lt;/strong&gt; - it's like Ollama for &lt;code&gt;llama.cpp&lt;/code&gt; or TabbyAPI for Exllama3, but for &lt;code&gt;transformers&lt;/code&gt;. &amp;quot;Official&amp;quot; implementation, using same model architecture as a reference. Some common optimisations on top. Can be a more friendly alternative to &lt;code&gt;ktransformers&lt;/code&gt; or &lt;code&gt;sglang&lt;/code&gt;, but not as feature-rich.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AirLLM&lt;/strong&gt; - extremely niche use-case. You have a workload that can be slow (overnight), no API-based LLMs are acceptable, your hardware only allows for tiny models, but the task needs some of the big boys. If all these boxes are ticket - AirLLM might help.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think that the key of a good homelab setup is to be able to quickly run an engine that is suitable for a specific model/feature that you want right now. Many more niche engines are moving faster than &lt;code&gt;llama.cpp&lt;/code&gt; (at the expense of stability), so having them available can allow testing new models/features earlier.&lt;/p&gt; &lt;h1&gt;TTS / STT&lt;/h1&gt; &lt;p&gt;I recommend projects that support OpenAI-compatible APIs here, that way they are more likely to integrate well with the other parts of your LLM setup. I can personally recommend Speaches (former &lt;code&gt;faster-whisper-server&lt;/code&gt;, more active) and &lt;code&gt;openedai-speech&lt;/code&gt; (less active, more hackable). Both have TTS and STT support, so you can build voice assistants with them. Containerized deployment is possible for both.&lt;/p&gt; &lt;h1&gt;Tunnels&lt;/h1&gt; &lt;p&gt;Exposing your homelab setup to the Internet can be very powerful. It's very dangerous too, so be careful. Less involved setups are based on running somethings like &lt;code&gt;cloudflared&lt;/code&gt; or &lt;code&gt;ngrok&lt;/code&gt; at the expense of some privacy and security. More involved setups are based on running your own VPN or reverse proxy with proper authentication. Tailscale is a great option.&lt;/p&gt; &lt;p&gt;A very useful/convenient add-on is to also generate a QR for your mobile device to connect to your homelab services quickly. There are some CLI tools for that too.&lt;/p&gt; &lt;h1&gt;Web RAG &amp;amp; Deep Search&lt;/h1&gt; &lt;p&gt;Almost a must for any kind of useful agentic system right now. The absolute easiest way to get one is to use &lt;a href="https://github.com/searxng/searxng"&gt;SearXNG&lt;/a&gt;. It connects nicely with a variety of frontends out of the box, including Open WebUI and LibreChat. You can run it in a container as well, so it's easy to maintain. Just make sure to configure it properly to avoid leaking your data to third parties. The quality is not great compared to paid search engines, but it's free and relatively private. If you have a budget, consider using Tavily or Jina for same purpose and every LLM will feel like a mini-Perplexity.&lt;/p&gt; &lt;p&gt;Some notable projects:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local Deep Research&lt;/strong&gt; - &amp;quot;Deep research at home&amp;quot;, not quite in-depth, but works decently well&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Morphic&lt;/strong&gt; - Probably most convenient to setup out of the bunch.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Perplexica&lt;/strong&gt; - Started not very developer-friendly, with some gaps/unfinished features, so haven't used actively.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SurfSense&lt;/strong&gt; - was looking quite promising in Nov 2024, but they didn't have pre-built images back then. Maybe better now.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Workflows&lt;/h1&gt; &lt;p&gt;Crazy amount of companies are building things for LLM-based automation now, most are looking like workflow engines. Pretty easy to have one locally too.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Dify&lt;/strong&gt; - very well polished, great UX and designed specifically for LLM workflows (unlike &lt;code&gt;n8n&lt;/code&gt; that is more general-purpose). The biggest drawback - lack of OpenAI-compatible API for built workflows/agents, but comes with built-in UI, traceability, and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flowise&lt;/strong&gt; - Similar to Dify, but more focused on LangChain functionality. Was quite buggy last time I tried, but allowed for a simpler setup of basic agents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangFlow&lt;/strong&gt; - a more corporate-friendly version of Flowise/Dify, more polished, but locked on LangChain. Very turbulent development, breaking changes often introduced.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;n8n&lt;/strong&gt; - Probably most well-known one, fair-code workflow automation platform with native AI capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open WebUI Pipelines&lt;/strong&gt; - Most powerful option if you firmly settled on Open WebUI and can do some Python, can do wild things for chat workflows.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Coding&lt;/h1&gt; &lt;p&gt;Very simple, current landscape is dominated by TUI agents. I tried a few personally, but unfortunately can't say that I use any of them regularly, compared to the agents based on the cloud LLMs. OpenCode + Qwen 3 Coder 480B, GLM 4.6, Kimi K2 get quite close but not close enough for me, your experience may vary.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;OpenCode&lt;/strong&gt; - great performance, good support for a variety of local models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Crush&lt;/strong&gt; - the agent seems to perform worse than OpenCode with same models, but more eye-candy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aider&lt;/strong&gt; - the OG. Being a mature well-developed project is both a pro and a con. Agentic landscape is moving fast, some solutions that were good in the past are not that great anymore (mainly talking about tool call formatting).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenHands&lt;/strong&gt; - provides a TUI agents with a WebUI, pairs nicely with Codestral, aims to be OSS version of Devin, but the quality of the agents is not quite there yet.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Extras&lt;/h1&gt; &lt;p&gt;Some other projects that can be useful for a specific use-case or just for fun. Recent smaller models suddenly became very good at agentic tasks, so surprisingly many of these tools work well enough.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Agent Zero&lt;/strong&gt; - general-purpose personal assistant with Web RAG, persistent memory, tools, browser use and more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Airweave&lt;/strong&gt; - ETL tool for LLM knowledge, helps to prepare data for agentic use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bolt.new&lt;/strong&gt; - Full-stack app development fully in the browser.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Browser Use&lt;/strong&gt; - LLM-powered browser automation with web UI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Docling&lt;/strong&gt; - Transform documents into format ready for LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fabric&lt;/strong&gt; - LLM-driven processing of the text data in the terminal.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LangFuse&lt;/strong&gt; - easy LLM Observability, metrics, evals, prompt management, playground, datasets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latent Scope&lt;/strong&gt; - A new kind of workflow + tool for visualizing and exploring datasets through the lens of latent spaces.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LibreTranslate&lt;/strong&gt; - A free and open-source machine translation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LiteLLM&lt;/strong&gt; - LLM proxy that can aggregate multiple inference APIs together into a single endpoint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LitLytics&lt;/strong&gt; - Simple analytics platform that leverages LLMs to automate data analysis.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;llama-swap&lt;/strong&gt; - Runs multiple llama.cpp servers on demand for seamless switching between them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;lm-evaluation-harness&lt;/strong&gt; - A de-facto standard framework for the few-shot evaluation of language models. I can't tell that it's very user-friendly though, figuring out how to run evals for a local LLM takes some effort.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mcpo&lt;/strong&gt; - Turn MCP servers into OpenAPI REST APIs - use them anywhere.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MetaMCP&lt;/strong&gt; - Allows to manage MCPs via a WebUI, exposes multiple MCPs as a single server.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OptiLLM&lt;/strong&gt; - Optimising LLM proxy that implements many advanced workflows to boost the performance of the LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Promptfoo&lt;/strong&gt; - A very nice developer-friendly way to setup evals for anything OpenAI-API compatible, including local LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Repopack&lt;/strong&gt; - Packs your entire repository into a single, AI-friendly file.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SQL Chat&lt;/strong&gt; - Chat-based SQL client, which uses natural language to communicate with the database. Be wary about connecting to the data you actually care about without proper safeguards.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SuperGateway&lt;/strong&gt; - A simple and powerful API gateway for LLMs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TextGrad&lt;/strong&gt; - Automatic &amp;quot;Differentiation&amp;quot; via Text - using large language models to backpropagate textual gradients.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Webtop&lt;/strong&gt; - Linux in a web browser supporting popular desktop environments. Very conventient for local Computer Use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hopefully some of this was useful! Thanks.&lt;/p&gt; &lt;p&gt;Edit 1: Mention Nexa SDK drama Edit 2: Adding recommendations from comments&lt;/p&gt; &lt;h1&gt;Community Recommendations&lt;/h1&gt; &lt;p&gt;Other tools/projects from the comments in this post.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;transformers serve&lt;/strong&gt; - easy button for native inference for model architectures not supported by more optimised inference engines with OpenAI-compatible API (not all modalities though). For evals, small-scale inference, etc. Mentioned by &lt;a href="/u/kryptkpr"&gt;u/kryptkpr&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Silly Tavern&lt;/strong&gt; - text, image, text-to-speech, character cards, great for enterprise resource planning. Mentioned by &lt;a href="/u/IrisColt"&gt;u/IrisColt&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;onnx-asr&lt;/strong&gt; - lightweight runtime (no PyTorch or transformers, CPU-friendly) for speech recognition. Excellent support for Parakeet models. Mentioned by &lt;a href="/u/jwpbe"&gt;u/jwpbe&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;shepta-onnx&lt;/strong&gt; - a very comprehensive TTS/SST solution with support for a lot of extra tasks and runtimes. Mentioned by &lt;a href="/u/jwpbe"&gt;u/jwpbe&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;headscale&lt;/strong&gt; - self-hosted control server for Tailscale aimed at homelab use-case. Mentioned by &lt;a href="/u/spaceman3000"&gt;u/spaceman3000&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;netbird&lt;/strong&gt; - a more user-friendly alternative to Tailscale, self-hostable. Mentioned by &lt;a href="/u/spaceman3000"&gt;u/spaceman3000&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;mcpo&lt;/strong&gt; - developed by Open WebUI org, converts MCP to OpenAPI tools. Mentioned by &lt;a href="/u/RealLordMathis"&gt;u/RealLordMathis&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Oobabooga&lt;/strong&gt; - the OG all-in-one solution for local text generation. Mentioned by &lt;a href="/u/Nrgte"&gt;u/Nrgte&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;tmuxai&lt;/strong&gt; - tmux-enabled assistant, reads visible content from opened panes, can execute commands. Have some interesting features like Observe/Prepare/Watch modes. Mentioned by &lt;a href="/u/el95149"&gt;u/el95149&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Cherry Studio&lt;/strong&gt; - desktop all-in-one app for inference, alternative to LM Studio with some neat features. Mentioned by &lt;a href="/u/Dentuam"&gt;u/Dentuam&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;olla&lt;/strong&gt; - OpenAI-compatible routing proxy. Mentioned and developed by &lt;a href="/u/2shanigans"&gt;u/2shanigans&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt; - desktop all-in-one app for inference. Very beginner-friendly, supports MLX natively. Mentioned by &lt;a href="/u/2shanigans"&gt;u/2shanigans&lt;/a&gt; and &lt;a href="/u/Predatedtomcat"&gt;u/Predatedtomcat&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Everlier"&gt; /u/Everlier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oclug7/getting_most_out_of_your_local_llm_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T19:05:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1odax0g</id>
    <title>Feasibility Check: Modifying DeepSeek-OCR (2510.18234) into an Instruction-Following Document VLM?</title>
    <updated>2025-10-22T15:09:45+00:00</updated>
    <author>
      <name>/u/hiiamtin</name>
      <uri>https://old.reddit.com/user/hiiamtin</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone &lt;/p&gt; &lt;p&gt;I've been digging into the new DeepSeek-OCR paper (arXiv: 2510.18234), and its DeepEncoder looks like a game-changer for handling high-resolution, dense documents with its high-compression ratio.&lt;/p&gt; &lt;p&gt;As I understand it, the model in its current form is a pure OCR engine, with a workflow of:&lt;/p&gt; &lt;p&gt;Image -&amp;gt; [Encoder -&amp;gt; Decoder] -&amp;gt; Full Text (It seems it's not designed to take text instructions, only image inputs).&lt;/p&gt; &lt;p&gt;I'm wondering about the feasibility of modifying this to become an instruction-following Visual Language Model (VLM) for documents.&lt;/p&gt; &lt;p&gt;The Core Idea: To change the workflow to: Image + Text Instruction -&amp;gt; Specific Answer&lt;/p&gt; &lt;p&gt;For example: * Input: (Image of an invoice) + &amp;quot;Extract the final total.&amp;quot; * Output: &amp;quot;$450.72&amp;quot; * Input: (Image of a paper) + &amp;quot;Summarize the abstract.&amp;quot; * Output: &amp;quot;The paper introduces a novel optical compression engine...&amp;quot;&lt;/p&gt; &lt;p&gt;Proposed High-Level Approach:&lt;/p&gt; &lt;p&gt;Since the base model only accepts images, a modification would be necessary:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keep the DeepEncoder: Leverage the pre-trained DeepEncoder as the powerful, high-resolution vision backbone.&lt;/li&gt; &lt;li&gt;Modify the Architecture: This is the key step. We would need to adapt the model (likely the DeepSeek3B-MoE decoder part) to accept two types of input simultaneously: &lt;ul&gt; &lt;li&gt;The vision_tokens (from the document via the Encoder/Projector).&lt;/li&gt; &lt;li&gt;The text_tokens (from the user's new instruction).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Instruction Fine-Tune: Re-train (SFT) this modified model on a new dataset of (image, instruction, answer) pairs. This would teach the LLM decoder to reason based on the combined inputs, rather than just transcribe the visual input.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My Questions: * Is this a sound approach? Does this architectural modification make sense? * Has anyone tried this? I know of models like LLaVA, Donut, etc., but the appeal here is starting with DeepSeek's SOTA document-specific encoder, rather than a general-purpose one like CLIP. * What are the biggest challenges? I assume preventing &amp;quot;catastrophic forgetting&amp;quot; (i.e., making sure it can still do basic OCR) would be one. How hard is it to get the model to properly attend to both the image and text instructions?&lt;/p&gt; &lt;p&gt;Would love to hear any thoughts or see if I'm missing a more obvious path. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hiiamtin"&gt; /u/hiiamtin &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odax0g/feasibility_check_modifying_deepseekocr_251018234/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odax0g/feasibility_check_modifying_deepseekocr_251018234/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odax0g/feasibility_check_modifying_deepseekocr_251018234/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:09:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1odddyg</id>
    <title>Free GPU memory during local LLM inference without KV cache hogging VRAM</title>
    <updated>2025-10-22T16:40:37+00:00</updated>
    <author>
      <name>/u/ivaniumr</name>
      <uri>https://old.reddit.com/user/ivaniumr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt; &lt;img alt="Free GPU memory during local LLM inference without KV cache hogging VRAM" src="https://external-preview.redd.it/Qexkfi7FQ3mBNXUsI349OiBIZqFoa4py3iuRmtBXCE0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a119b694af4a1e62532c3a6e7da37245e136c66" title="Free GPU memory during local LLM inference without KV cache hogging VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building &lt;a href="https://github.com/ovg-project/kvcached"&gt;kvcached&lt;/a&gt;, a library that lets local LLM inference engines such as &lt;strong&gt;SGLang&lt;/strong&gt; and &lt;strong&gt;vLLM&lt;/strong&gt; free idle KV cache memory instead of occupying the entire GPU. This allows you to run a model locally without using all available VRAM, so other applications can still run or even share the GPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;âœ… Works out of the box with SGLang and vLLM&lt;/li&gt; &lt;li&gt;ðŸ”§ Support for Ollama and LM Studio is in progress&lt;/li&gt; &lt;li&gt;ðŸ§© No changes to your model or prompts required&lt;/li&gt; &lt;li&gt;ðŸš€ Install with pip and it runs out of the box&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our code is open source: &lt;a href="https://github.com/ovg-project/kvcached"&gt;https://github.com/ovg-project/kvcached&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deep dive blog for those interested in the techniques behind it: &lt;a href="https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141"&gt;https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We would love feedback from the local LLM community. If you want to run multiple models on one GPU, combine LLMs with other GPU applications, or simply reduce memory usage, feel free to try it out and ask questions. Happy to discuss and improve together ðŸ™Œ&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/co5zu9swyowf1.jpg?width=3217&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f6fd3f6976033f9a3c4a23fb1743fa7f5dd0a59f"&gt;https://preview.redd.it/co5zu9swyowf1.jpg?width=3217&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f6fd3f6976033f9a3c4a23fb1743fa7f5dd0a59f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivaniumr"&gt; /u/ivaniumr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T16:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1och7m9</id>
    <title>Qwen3-VL-2B and Qwen3-VL-32B Released</title>
    <updated>2025-10-21T16:13:23+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"&gt; &lt;img alt="Qwen3-VL-2B and Qwen3-VL-32B Released" src="https://preview.redd.it/n4rx9o72phwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31c5eea069f1786b249324b0d23eca9977c6918b" title="Qwen3-VL-2B and Qwen3-VL-32B Released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n4rx9o72phwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1och7m9/qwen3vl2b_and_qwen3vl32b_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T16:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbco8</id>
    <title>LMStudio - Now has GLM 4.6 Support (CUDA)</title>
    <updated>2025-10-22T15:26:02+00:00</updated>
    <author>
      <name>/u/YouAreRight007</name>
      <uri>https://old.reddit.com/user/YouAreRight007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, just so you know, LMStudio seems to now have GLM 4.6 support. Yay.&lt;/p&gt; &lt;p&gt;I'm getting 2.99 tokens a second when generating 3000 tokens using 1 3090 and PC RAM.&lt;/p&gt; &lt;p&gt;Model: Unsloth GLM 4.6 UD - Q3_K_XL (147.22GB)&lt;/p&gt; &lt;p&gt;Hardware setup: single 3090 + 14700K with 192GB RAM DDR5333. (14700K limited to 250Watts)&lt;/p&gt; &lt;p&gt;NOTE: Getting a buffer related error when trying to offload layers onto 2x 3090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouAreRight007"&gt; /u/YouAreRight007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1odcib3</id>
    <title>Qwen3-VL-2B GGUF is here</title>
    <updated>2025-10-22T16:08:01+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"&gt; &lt;img alt="Qwen3-VL-2B GGUF is here" src="https://external-preview.redd.it/W7AGVvstE9pre0GHVfitXRaqvaJcdswOTBT90OSCLds.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0b8a9fdf290f4fb4c4b047a45249827638e70e25" title="Qwen3-VL-2B GGUF is here" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GGUFs are available (Note currently only NexaSDK supports Qwen3-VL-2B GGUF model)&lt;br /&gt; &lt;a href="https://huggingface.co/NexaAI/Qwen3-VL-2B-Thinking-GGUF"&gt;https://huggingface.co/NexaAI/Qwen3-VL-2B-Thinking-GGUF&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/NexaAI/Qwen3-VL-2B-Instruct-GGUF"&gt;https://huggingface.co/NexaAI/Qwen3-VL-2B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a quick demo of it counting circles: 155 t/s on M4 Max&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1odcib3/video/y3bwkg6psowf1/player"&gt;https://reddit.com/link/1odcib3/video/y3bwkg6psowf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quickstart in 2 steps&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 1: Download &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;NexaSDK&lt;/a&gt; with one click&lt;/li&gt; &lt;li&gt;Step 2: one line of code to run in your terminal: &lt;ul&gt; &lt;li&gt;&lt;code&gt;nexa infer NexaAI/Qwen3-VL-2B-Instruct-GGUF&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;nexa infer NexaAI/Qwen3-VL-2B-Thinking-GGUF&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What would you use this model for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odcib3/qwen3vl2b_gguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T16:08:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1odavba</id>
    <title>Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzenâ„¢ AI NPU</title>
    <updated>2025-10-22T15:07:57+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"&gt; &lt;img alt="Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzenâ„¢ AI NPU" src="https://external-preview.redd.it/y7g9bbQ0RPFXfYX-nbtW899i8fH3DJk9OyQ8tRM7MG4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b115fd1754530c90df1effac911d91b79c4dfb2b" title="Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzenâ„¢ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;About the Demo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Workflow:&lt;/strong&gt; &lt;code&gt;whisper-large-v3-turbo&lt;/code&gt; transcribes audio; &lt;code&gt;gpt-oss:20b&lt;/code&gt; generates the summary. Both models are &lt;strong&gt;pre-loaded&lt;/strong&gt; on the NPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Settings:&lt;/strong&gt; &lt;code&gt;gpt-oss:20b&lt;/code&gt; reasoning effort = &lt;strong&gt;High&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test system:&lt;/strong&gt; ASRock 4X4 BOX-AI340 Mini PC (Kraken Point), 96 GB RAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; FastFlowLM (CLI mode).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;About FLM&lt;/h1&gt; &lt;p&gt;Weâ€™re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; â€” a fast runtime for running &lt;strong&gt;Whisper (Audio)&lt;/strong&gt;, &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama (maybe llama.cpp since we have our own backend?)&lt;/strong&gt;, but deeply optimized for AMD NPUs â€” with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;âœ¨ &lt;strong&gt;From Idle Silicon to Instant Power â€” FastFlowLM (FLM) Makes Ryzenâ„¢ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10Ã— more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (16 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo â†’ Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Weâ€™re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;ðŸ™&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0t8ijUPg4A0?si=539G5mrICJNOwe6Z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1od59hx</id>
    <title>Qwen3-VL-32B-Instruct GGUF with unofficial llama.cpp release to run it (Pre-release build)</title>
    <updated>2025-10-22T11:08:02+00:00</updated>
    <author>
      <name>/u/Main-Wolverine-1042</name>
      <uri>https://old.reddit.com/user/Main-Wolverine-1042</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"&gt; &lt;img alt="Qwen3-VL-32B-Instruct GGUF with unofficial llama.cpp release to run it (Pre-release build)" src="https://external-preview.redd.it/PKrFTdIjOXXO8Una8BkhYkAJCuw5_mrHyizXt_acZpM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=11da2e1a8abf95953a3114c228cd807881642de2" title="Qwen3-VL-32B-Instruct GGUF with unofficial llama.cpp release to run it (Pre-release build)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/c0glmg5dzmwf1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63b57f9aded62db68cf7c75a9f0108f4131e1b4"&gt;https://preview.redd.it/c0glmg5dzmwf1.jpg?width=2048&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a63b57f9aded62db68cf7c75a9f0108f4131e1b4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/yairpatch/llama.cpp"&gt;https://github.com/yairpatch/llama.cpp&lt;/a&gt; - Clone this repository and build it.&lt;/p&gt; &lt;p&gt;Or use this prebuilt release - &lt;a href="https://github.com/yairpatch/llama.cpp/releases"&gt;https://github.com/yairpatch/llama.cpp/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;32B Model page - &lt;a href="https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF"&gt;https://huggingface.co/yairpatch/Qwen3-VL-32B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4B Model page - &lt;a href="https://huggingface.co/yairzar/Qwen3-VL-4B-Instruct-GGUF"&gt;https://huggingface.co/yairzar/Qwen3-VL-4B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Uploading in progress of more QWEN3VL variants.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Wolverine-1042"&gt; /u/Main-Wolverine-1042 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od59hx/qwen3vl32binstruct_gguf_with_unofficial_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T11:08:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocx27p</id>
    <title>A quickly put together a GUI for the DeepSeek-OCR model that makes it a bit easier to use</title>
    <updated>2025-10-22T03:01:37+00:00</updated>
    <author>
      <name>/u/SmashShock</name>
      <uri>https://old.reddit.com/user/SmashShock</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocx27p/a_quickly_put_together_a_gui_for_the_deepseekocr/"&gt; &lt;img alt="A quickly put together a GUI for the DeepSeek-OCR model that makes it a bit easier to use" src="https://external-preview.redd.it/wAx0-554iQTzQwf91BRGvJShhLon54aeVFhskNqXubo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b92062881d606dc5d0bb4a9e9814b152550e5f6" title="A quickly put together a GUI for the DeepSeek-OCR model that makes it a bit easier to use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;EDIT: this should now work with newer Nvidia cards. Please try the setup instructions again (with a fresh zip) if it failed for you previously.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;I put together a GUI for DeepSeek's new OCR model. The model seems quite good at document understanding and structured text extraction so I figured it deserved the start of a proper interface.&lt;/p&gt; &lt;p&gt;The various OCR types available correspond in-order to the first 5 entries in &lt;a href="https://github.com/deepseek-ai/DeepSeek-OCR/blob/8cf003d38821fa1b19c73da3bd1b0dc262ea8136/README.md#prompts-examples"&gt;this list&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Flask backend manages the model, Electron frontend for the UI. The model downloads automatically from HuggingFace on first load, about 6.7 GB.&lt;/p&gt; &lt;p&gt;Runs on Windows, with untested support for Linux. Currently requires an Nvidia card. If you'd like to help test it out or fix issues on Linux or other platforms, or you would like to contribute in any other way, please feel free to make a PR!&lt;/p&gt; &lt;p&gt;Download and repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ihatecsv/deepseek-ocr-client"&gt;https://github.com/ihatecsv/deepseek-ocr-client&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmashShock"&gt; /u/SmashShock &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/klnlh8omskwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocx27p/a_quickly_put_together_a_gui_for_the_deepseekocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocx27p/a_quickly_put_together_a_gui_for_the_deepseekocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T03:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbwjj</id>
    <title>LFM2-VL 3B released today</title>
    <updated>2025-10-22T15:46:03+00:00</updated>
    <author>
      <name>/u/cruncherv</name>
      <uri>https://old.reddit.com/user/cruncherv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New &lt;strong&gt;LFM2-VL 3B&lt;/strong&gt; version released by LiquidAI today.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;HuggingFace &lt;/a&gt;page&lt;/li&gt; &lt;li&gt;Available quant: &lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF"&gt;GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;Average&lt;/th&gt; &lt;th align="left"&gt;MMStar&lt;/th&gt; &lt;th align="left"&gt;MMMU (val)&lt;/th&gt; &lt;th align="left"&gt;MathVista&lt;/th&gt; &lt;th align="left"&gt;BLINK&lt;/th&gt; &lt;th align="left"&gt;InfoVQA (val)&lt;/th&gt; &lt;th align="left"&gt;MMBench (dev en)&lt;/th&gt; &lt;th align="left"&gt;OCRBench&lt;/th&gt; &lt;th align="left"&gt;POPE&lt;/th&gt; &lt;th align="left"&gt;RealWorldQA&lt;/th&gt; &lt;th align="left"&gt;MME&lt;/th&gt; &lt;th align="left"&gt;MM-IFEval&lt;/th&gt; &lt;th align="left"&gt;SEEDBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;InternVL3_5-2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.63&lt;/td&gt; &lt;td align="left"&gt;57.67&lt;/td&gt; &lt;td align="left"&gt;51.78&lt;/td&gt; &lt;td align="left"&gt;61.6&lt;/td&gt; &lt;td align="left"&gt;50.97&lt;/td&gt; &lt;td align="left"&gt;69.29&lt;/td&gt; &lt;td align="left"&gt;78.18&lt;/td&gt; &lt;td align="left"&gt;834&lt;/td&gt; &lt;td align="left"&gt;87.17&lt;/td&gt; &lt;td align="left"&gt;60.78&lt;/td&gt; &lt;td align="left"&gt;2,128.83&lt;/td&gt; &lt;td align="left"&gt;47.31&lt;/td&gt; &lt;td align="left"&gt;75.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen2.5-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.61&lt;/td&gt; &lt;td align="left"&gt;56.13&lt;/td&gt; &lt;td align="left"&gt;51.67&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;td align="left"&gt;48.97&lt;/td&gt; &lt;td align="left"&gt;76.12&lt;/td&gt; &lt;td align="left"&gt;80.41&lt;/td&gt; &lt;td align="left"&gt;824&lt;/td&gt; &lt;td align="left"&gt;86.17&lt;/td&gt; &lt;td align="left"&gt;65.23&lt;/td&gt; &lt;td align="left"&gt;2,163.29&lt;/td&gt; &lt;td align="left"&gt;38.62&lt;/td&gt; &lt;td align="left"&gt;73.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;InternVL3-2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.46&lt;/td&gt; &lt;td align="left"&gt;61.1&lt;/td&gt; &lt;td align="left"&gt;48.7&lt;/td&gt; &lt;td align="left"&gt;57.6&lt;/td&gt; &lt;td align="left"&gt;53.1&lt;/td&gt; &lt;td align="left"&gt;66.1&lt;/td&gt; &lt;td align="left"&gt;81.1&lt;/td&gt; &lt;td align="left"&gt;831&lt;/td&gt; &lt;td align="left"&gt;90.1&lt;/td&gt; &lt;td align="left"&gt;65.1&lt;/td&gt; &lt;td align="left"&gt;2,186.40&lt;/td&gt; &lt;td align="left"&gt;38.49&lt;/td&gt; &lt;td align="left"&gt;74.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SmolVLM2-2.2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;54.85&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;51.5&lt;/td&gt; &lt;td align="left"&gt;42.3&lt;/td&gt; &lt;td align="left"&gt;37.75&lt;/td&gt; &lt;td align="left"&gt;69.24&lt;/td&gt; &lt;td align="left"&gt;725&lt;/td&gt; &lt;td align="left"&gt;85.1&lt;/td&gt; &lt;td align="left"&gt;57.5&lt;/td&gt; &lt;td align="left"&gt;1792.5&lt;/td&gt; &lt;td align="left"&gt;19.42&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.31&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;57.73&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;45.33&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;62.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.03&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.37&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.81&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;822&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89.01&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.37&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2,050.90&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.83&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;76.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Table from: &lt;a href="http://liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge"&gt;liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cruncherv"&gt; /u/cruncherv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1od7hyu</id>
    <title>M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)</title>
    <updated>2025-10-22T12:55:20+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"&gt; &lt;img alt="M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)" src="https://preview.redd.it/2r0ue3k6unwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b506f746bde959cb1cf422094c3babaa4c4113e" title="M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[&lt;a href="https://www.youtube.com/watch?v=BKQggt9blGo"&gt;Geekerwan&lt;/a&gt;]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2r0ue3k6unwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1od35w1</id>
    <title>New model from Tencent, HunyuanWorld-Mirror</title>
    <updated>2025-10-22T09:01:59+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"&gt; &lt;img alt="New model from Tencent, HunyuanWorld-Mirror" src="https://external-preview.redd.it/4mzrgM79cCe_QE-8XZM35Vw90_ckM3tR76mq1apuGAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d7d696f5cccceda23331d47643538ea7a5dce0c" title="New model from Tencent, HunyuanWorld-Mirror" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HunyuanWorld-Mirror is a versatile feed-forward model for comprehensive 3D geometric prediction. It integrates diverse geometric priors (camera poses, calibrated intrinsics, depth maps) and simultaneously generates various 3D representations (point clouds, multi-view depths, camera parameters, surface normals, 3D Gaussians) in a single forward pass.&lt;/p&gt; &lt;p&gt;Really interesting for folks into 3D...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-Mirror"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T09:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocrocy</id>
    <title>DeepSeek-OCR - Lives up to the hype</title>
    <updated>2025-10-21T22:51:20+00:00</updated>
    <author>
      <name>/u/Bohdanowicz</name>
      <uri>https://old.reddit.com/user/Bohdanowicz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt; &lt;img alt="DeepSeek-OCR - Lives up to the hype" src="https://external-preview.redd.it/DGhh7DsESjWeCrgHI7E8he7jk8ACLXaitOluBNwi630.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86af002276d4a89cdea0ff0abd7fac0d455b8d9a" title="DeepSeek-OCR - Lives up to the hype" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to try this out. Dockerized the model with fastapi in a wsl environment. Gave it 10000 pdfs to convert to markdown.&lt;/p&gt; &lt;p&gt;Hardware - 1 x A6000 ADA on a Ryzen 1700 /w 32gb ram&lt;/p&gt; &lt;p&gt;Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00&amp;lt;00:00, 3.29it/s, est. speed input: 3000.81 toks/s, output: 220.20 toks/s]&lt;/p&gt; &lt;p&gt;I'm averaging less than 1 second per page.&lt;/p&gt; &lt;p&gt;This is the real deal.&lt;/p&gt; &lt;p&gt;EDIT: Decided to share the docker build if anyone is interested. It wraps the model up nicely so you can try it out directly with the api. it uses the vllm-openapi 0.8.5 public docker image.&lt;/p&gt; &lt;p&gt;Also included a pdf to markdown utility which will process anything in the /data subfolder to .md just by running it since there is an issue using the batch processor directly via the api.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/se9r9dsnyjwf1.png?width=1458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcd8118c3e1c167cc13d159579527a802e55fd84"&gt;https://preview.redd.it/se9r9dsnyjwf1.png?width=1458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcd8118c3e1c167cc13d159579527a802e55fd84&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API"&gt;https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: Making major improvements/fix to the dockerized api. I was in a rush yesterday and there are some major issues that I plan to resolve at lunch.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bohdanowicz"&gt; /u/Bohdanowicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T22:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1od5dxw</id>
    <title>I created a corporate-level chat UI with advanced features</title>
    <updated>2025-10-22T11:14:42+00:00</updated>
    <author>
      <name>/u/BlueLemonPixel</name>
      <uri>https://old.reddit.com/user/BlueLemonPixel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"&gt; &lt;img alt="I created a corporate-level chat UI with advanced features" src="https://external-preview.redd.it/dPYWmiXWuS054Q7CG9UBLqSEshctiMSWamOBef2b2Gs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31fcc76b887198eea07c204089b8d5f4a93bc082" title="I created a corporate-level chat UI with advanced features" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueLemonPixel"&gt; /u/BlueLemonPixel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/25jaz4q9cnwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T11:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1od0jw1</id>
    <title>[R] We figured out how to predict 32B model reasoning performance with a 1B model. 100x cheaper. Paper inside.</title>
    <updated>2025-10-22T06:13:40+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/?sort=new"&gt;70B intermediate checkpoints release&lt;/a&gt;? We said we wanted to enable real research on training dynamics. Well, here's exactly the kind of work we hoped would happen.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;rBridge:&lt;/strong&gt; Use 1B models to predict whether your 32B model will be good at reasoning. Actually works.&lt;/p&gt; &lt;p&gt;The problem: Small models can't do reasoning (emergence happens at 7B+), so how do you know if your training recipe works without spending $200k?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our solution:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Align evaluation with both pre-training objective AND target task&lt;/li&gt; &lt;li&gt;Use frontier model reasoning traces as gold labels&lt;/li&gt; &lt;li&gt;Weight tokens by task importance automatically&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100x compute reduction vs baselines&lt;/li&gt; &lt;li&gt;Accurately predict which datasets are worth training on&lt;/li&gt; &lt;li&gt;RÂ² = 0.826 predicting 32B performance from 1B proxy&lt;/li&gt; &lt;li&gt;Works zero-shot on new datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tested on: GSM8K, MATH500, ARC-C, MMLU Pro, CQA, HumanEval&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.arxiv.org/abs/2509.21013"&gt;https://www.arxiv.org/abs/2509.21013&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what open research looks like - building on each other's work to make LLM development accessible to everyone, not just companies with infinite compute.&lt;/p&gt; &lt;p&gt;Code coming soon. Apache 2.0 as always.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T06:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1od8fz0</id>
    <title>YES! Super 80b for 8gb VRAM - Qwen3-Next-80B-A3B-Instruct-GGUF</title>
    <updated>2025-10-22T13:34:41+00:00</updated>
    <author>
      <name>/u/Mangleus</name>
      <uri>https://old.reddit.com/user/Mangleus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So amazing to be able to run this beast on a 8GB VRAM laptop &lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note that this is not yet supported by latest llama.cpp so you need to compile the non-official version as shown in the link above. (Do not forget to add GPU support when compiling). &lt;/p&gt; &lt;p&gt;Have fun! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mangleus"&gt; /u/Mangleus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T13:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1od4wj4</id>
    <title>2025 Skynet is released in beta version</title>
    <updated>2025-10-22T10:48:00+00:00</updated>
    <author>
      <name>/u/Max-HWN</name>
      <uri>https://old.reddit.com/user/Max-HWN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"&gt; &lt;img alt="2025 Skynet is released in beta version" src="https://preview.redd.it/nstd6t1x7nwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe7be5c4c7050b73bdeb33c732a3875526215c72" title="2025 Skynet is released in beta version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, if you are afraid of AI taking over, we still have a lot of time ðŸ˜‚&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Max-HWN"&gt; /u/Max-HWN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nstd6t1x7nwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T10:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1od1hw4</id>
    <title>hey Z.ai, two weeks was yesterday</title>
    <updated>2025-10-22T07:13:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"&gt; &lt;img alt="hey Z.ai, two weeks was yesterday" src="https://preview.redd.it/lg6u60lj5mwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb37b472c5a42bbe348ff5652a5ce811e269f95d" title="hey Z.ai, two weeks was yesterday" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lg6u60lj5mwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T07:13:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oda8mk</id>
    <title>Qwen team is helping llama.cpp again</title>
    <updated>2025-10-22T14:44:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt; &lt;img alt="Qwen team is helping llama.cpp again" src="https://preview.redd.it/dh1iaky2eowf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=addcf456730d4f5ec04b561980fa9d74dfb18d96" title="Qwen team is helping llama.cpp again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh1iaky2eowf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T14:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
