<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-09T15:35:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r06157</id>
    <title>We built a P2P WebGPU runner (DeepSeek R1 / Qwen 2.5 /Lama in browser). Roast our architecture.</title>
    <updated>2026-02-09T14:44:40+00:00</updated>
    <author>
      <name>/u/Healthy-Art9086</name>
      <uri>https://old.reddit.com/user/Healthy-Art9086</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;We got tired of the &amp;quot;Cloud vs. Local&amp;quot; trade-off. So we built Agentical.net—a LLM runner that executes entirely in the browser and uses P2P connections for inference requests.&lt;/p&gt; &lt;p&gt;The Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Inference: WebGPU (Zero install). We are currently testing support for DeepSeek-R1 (Distill) and Qwen 2.5, alongside Llama-3.&lt;/li&gt; &lt;li&gt;Network: WebRTC (P2P). Data is end-to-end encrypted; no central server sees your tokens.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The &amp;quot;TBD&amp;quot; Architecture Question (Need Feedback):&lt;/p&gt; &lt;p&gt;We are currently debating how to handle Local RAG. Our white paper has this marked as &amp;quot;TBD,&amp;quot; but we are leaning toward using IndexDB. Or should we use local RAG - web platform pings localhost server. Is using local RAG a viable architecture, or is that adding unnecessary latency/complexity and we should just use browser-based setup (indexDB)?&lt;/p&gt; &lt;p&gt;Why I'm posting:&lt;/p&gt; &lt;p&gt;WebGPU is still the Wild West. We need to know where it breaks.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Does the LLM load to your GPU?&lt;/li&gt; &lt;li&gt;What t/s are you getting?&lt;/li&gt; &lt;li&gt;Is the P2P connection holding up?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Link: &lt;a href="http://agentical.net"&gt;agentical.net&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://agentical.net/assets/Agentical-01.pdf"&gt;Whitepapper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Roast away. We want to know if the indexDB approach for RAG is genius or wouldn't work? If not why?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Healthy-Art9086"&gt; /u/Healthy-Art9086 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r06157/we_built_a_p2p_webgpu_runner_deepseek_r1_qwen_25/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r06157/we_built_a_p2p_webgpu_runner_deepseek_r1_qwen_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r06157/we_built_a_p2p_webgpu_runner_deepseek_r1_qwen_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T14:44:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r06m26</id>
    <title>Lance/LanceDB users can now easily share multimodal datasets on Hugging Face Hub</title>
    <updated>2026-02-09T15:07:37+00:00</updated>
    <author>
      <name>/u/laminarflow027</name>
      <uri>https://old.reddit.com/user/laminarflow027</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, Lance became an &lt;a href="https://lancedb.com/blog/lance-x-huggingface-a-new-era-of-sharing-multimodal-data/"&gt;officially supported format&lt;/a&gt; on the Hugging Face Hub. Lance is an open source modern, columnar lakehouse format for AI/ML datasets that include multimodal data, embeddings, nested fields, and more. LanceDB is an open source, embedded library that exposes convenient APIs on top of the Lance format to manage embeddings and indices.&lt;/p&gt; &lt;p&gt;Check out the latest Lance datasets uploaded by the awesome OSS community here: &lt;a href="https://huggingface.co/datasets?library=library%3Alance"&gt;https://huggingface.co/datasets?library=library%3Alance&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What the Hugging Face integration means in practice for Lance format and LanceDB users on the Hub: - Binary assets (images, audio, videos) stored inline as blobs: No external files and pointers to manage - Efficient columnar access: Directly stream metadata from the Hub without touching heavier data (like videos) for fast exploration - Prebuilt indices can be shared alongside the data: Vector/FTS/scalar indices are packaged with the dataset, so no need to redo the work already done by others - Fast random access and scans: Lance format specializes in blazing fast random access (helps with vector search and data shuffles for training). It does so without compromising scan performance, so your large analytical queries can be run on traditional tabular data using engines like DuckDB, Spark, Ray, Trino, etc.&lt;/p&gt; &lt;p&gt;Earlier, to share large multimodal datasets, you had to store multiple directories with binary assets + pointer URLs to the large blobs in your Parquet tables on the Hub. Once downloaded, as a user, you'd have had to recreate any vector/FTS indices on your local machine, which can be an expensive process.&lt;/p&gt; &lt;p&gt;Now, with Lance officially supported as a format on the Hub, you can package all your datasets along with their indices as a single, shareable artifact, with familiar table semantics that work with your favourite query engine. Reuse others' work, and prepare your models for training, search and analytics/RAG with ease!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Disclaimer: I work at LanceDB and have been a member of Lance's and Hugging Face's open source communities for several years.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;It's very exciting to see the variety of Lance datasets that people &lt;a href="https://huggingface.co/datasets?library=library%3Alance"&gt;have uploaded&lt;/a&gt; already on the HF Hub, feel free to share your own, and spread the word!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laminarflow027"&gt; /u/laminarflow027 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r06m26/lancelancedb_users_can_now_easily_share/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r06m26/lancelancedb_users_can_now_easily_share/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r06m26/lancelancedb_users_can_now_easily_share/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T15:07:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02xqc</id>
    <title>Bitnet.cpp - Inference framework for 1-bit (ternary) LLM's</title>
    <updated>2026-02-09T12:29:57+00:00</updated>
    <author>
      <name>/u/Academic_Wallaby7135</name>
      <uri>https://old.reddit.com/user/Academic_Wallaby7135</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;bitnet.cpp&lt;/strong&gt; is Microsoft’s official C++ inference framework for &lt;strong&gt;1-bit Large Language Models (LLMs)&lt;/strong&gt;, optimized for &lt;strong&gt;BitNet b1.58&lt;/strong&gt; and similar architectures. It supports &lt;strong&gt;fast, lossless inference&lt;/strong&gt; on both &lt;strong&gt;CPU&lt;/strong&gt; and &lt;strong&gt;GPU&lt;/strong&gt; (with NPU support planned), using highly optimized kernels for &lt;strong&gt;ternary quantized models&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Officially Supported Models&lt;/strong&gt; (available on Hugging Face):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;BitNet-b1.58-2B-4T&lt;/strong&gt; (~2.4B params) – Optimized GGUF format for CPU/GPU inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bitnet_b1_58-large&lt;/strong&gt; (~0.7B params) – Lightweight variant for edge devices.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bitnet_b1_58-3B&lt;/strong&gt; (~3.3B params) – Larger model for higher accuracy tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Llama3-8B-1.58-100B-tokens&lt;/strong&gt; (~8B params) – LLaMA 3 adapted to 1.58-bit quantization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Falcon3 Family&lt;/strong&gt; (1B–10B params) – Instruction-tuned Falcon models in 1.58-bit format.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Falcon-E Family&lt;/strong&gt; (1B–3B params) – Energy-efficient Falcon variants.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Academic_Wallaby7135"&gt; /u/Academic_Wallaby7135 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02xqc/bitnetcpp_inference_framework_for_1bit_ternary/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02xqc/bitnetcpp_inference_framework_for_1bit_ternary/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02xqc/bitnetcpp_inference_framework_for_1bit_ternary/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:29:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzrl2g</id>
    <title>Are there any alternatives to Open WebUI that don't have terrible UX?</title>
    <updated>2026-02-09T02:05:01+00:00</updated>
    <author>
      <name>/u/lostmsu</name>
      <uri>https://old.reddit.com/user/lostmsu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Configuring Open WebUI is a nightmare.&lt;/p&gt; &lt;p&gt;Even if you managed to add a tool server and got tools to show up in UI (which is comparable to completing dark brotherhood quest in Skyrim in complexity), you have to enable it every fucking time you start a new chat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lostmsu"&gt; /u/lostmsu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzrl2g/are_there_any_alternatives_to_open_webui_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:05:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r05z1u</id>
    <title>NeKot - a terminal UI for chatting with LLMs</title>
    <updated>2026-02-09T14:42:21+00:00</updated>
    <author>
      <name>/u/Balanceballs</name>
      <uri>https://old.reddit.com/user/Balanceballs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r05z1u/nekot_a_terminal_ui_for_chatting_with_llms/"&gt; &lt;img alt="NeKot - a terminal UI for chatting with LLMs" src="https://external-preview.redd.it/cGU5aXcxMXFkaGlnMUDgoY8iZvuD8-qvPg3YBDQPSGFwPqN3mJnlo1Z7vJep.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57d698948b15c3d29c66f86ff3d9e4f5300aa5f0" title="NeKot - a terminal UI for chatting with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p9zoiw/nekot_a_terminal_interface_for_interacting_with"&gt;posted about the app&lt;/a&gt; some time ago and received really useful feedback. Almost all suggested things have now been implemented/improved, specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Web search tool added&lt;/li&gt; &lt;li&gt;Stdin piping now supported&lt;/li&gt; &lt;li&gt;Mouse text selection implemented(in general mouse support across the app)&lt;/li&gt; &lt;li&gt;Removed API keys requirement for local backends&lt;/li&gt; &lt;li&gt;Koboldcpp and other single model backends support&lt;/li&gt; &lt;li&gt;Many UI improvements like Shift+Tab support and light backgrounds support&lt;/li&gt; &lt;li&gt;A bunch of bugs fixed &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope this makes living in the terminal a little more pleasant and fun :D&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/BalanceBalls/nekot"&gt;https://github.com/BalanceBalls/nekot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balanceballs"&gt; /u/Balanceballs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/uf0k8r0qdhig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r05z1u/nekot_a_terminal_ui_for_chatting_with_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r05z1u/nekot_a_terminal_ui_for_chatting_with_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T14:42:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzo77z</id>
    <title>MiniMax M2.2 Coming Soon!</title>
    <updated>2026-02-08T23:22:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt; &lt;img alt="MiniMax M2.2 Coming Soon!" src="https://preview.redd.it/cj2as13ttcig1.png?width=140&amp;amp;height=19&amp;amp;auto=webp&amp;amp;s=0c0420fddf7b3160e28c4a7e5bea9abb03314341" title="MiniMax M2.2 Coming Soon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It found on their website code&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cj2as13ttcig1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9492b73dd14c581e30b35a5e64062f4ac7356a3f"&gt;https://preview.redd.it/cj2as13ttcig1.png?width=825&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9492b73dd14c581e30b35a5e64062f4ac7356a3f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://cdn.hailuo.ai/mmx-agent/prod-web-va-0.1.746/_next/static/chunks/app/(pages"&gt;https://cdn.hailuo.ai/mmx-agent/prod-web-va-0.1.746/_next/static/chunks/app/(pages)/(base)/page-0cfae9566c3e528b.js&lt;/a&gt;/(base)/page-0cfae9566c3e528b.js)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzo77z/minimax_m22_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T23:22:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzytme</id>
    <title>Caret – A terminal tool to inspect and clean massive LLM datasets</title>
    <updated>2026-02-09T08:26:32+00:00</updated>
    <author>
      <name>/u/Mental_Figure_1130</name>
      <uri>https://old.reddit.com/user/Mental_Figure_1130</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt; &lt;img alt="Caret – A terminal tool to inspect and clean massive LLM datasets" src="https://preview.redd.it/ip091tcnifig1.png?width=140&amp;amp;height=63&amp;amp;auto=webp&amp;amp;s=7526987c073820123909e1af40dafb10c9ef19d9" title="Caret – A terminal tool to inspect and clean massive LLM datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I’ve been working on a CLI tool called &lt;a href="https://github.com/rouapps/caret"&gt;Caret&lt;/a&gt; because I was struggling to inspect large pre-training datasets efficiently.&lt;/p&gt; &lt;p&gt;The main issue I had was that opening 10GB+ JSONL or Parquet files usually crashed my editor (VS Code) or used too much RAM. I wanted something that felt like &lt;code&gt;less&lt;/code&gt; but understood the structure of LLM data, specifically for visualizing tokenization and finding bad data.&lt;/p&gt; &lt;p&gt;It’s written in Rust and uses memory-mapped I/O, so it opens files of basically any size instantly without loading them fully into RAM.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Zero-Copy Open:&lt;/strong&gt; Uses &lt;code&gt;mmap&lt;/code&gt; to handle massive files. You can scroll through a 100GB dataset instantly.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token X-Ray:&lt;/strong&gt; Toggles a view that visualizes exactly how your tokenizer (Tiktoken, Llama 3, GPT-2...) is splitting the text (see screenshot).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SimHash Deduplication:&lt;/strong&gt; Uses parallelized SimHash (with hardware &lt;code&gt;POPCNT&lt;/code&gt;) to find near-duplicates in your training data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Parquet &amp;amp; CSV Support:&lt;/strong&gt; Handles binary formats natively without needing to convert them to JSONL first.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MCP Server:&lt;/strong&gt; I added an experimental MCP (Model Context Protocol) server. If you use Claude Desktop or Cursor, you can connect it to Caret to &amp;quot;chat&amp;quot; with your local dataset (e.g., &amp;quot;Find me 5 examples of bad JSON formatting in this file&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;How it works under the hood:&lt;/strong&gt; Instead of reading the whole file, it builds a lightweight index of line offsets and maps the file into virtual memory. When you scroll, it slices the bytes directly from the OS page cache. For remote HuggingFace datasets, it fetches only the parquet metadata footer first and streams row groups on demand, so you don't have to download the full repo to check the data quality.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Installation:&lt;/strong&gt; If you have Rust installed:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/rouapps/caret.git cd caret &amp;amp;&amp;amp; cargo run --release -- path/to/data.jsonl &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It’s still early days, so I’d appreciate any feedback or issue reports if you try it on your datasets!&lt;/p&gt; &lt;p&gt;Github link: &lt;a href="https://github.com/rouapps/caret"&gt;https://github.com/rouapps/caret&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ip091tcnifig1.png?width=1778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff35eda5fa5628659c5b0c7abf2f4903644419b"&gt;https://preview.redd.it/ip091tcnifig1.png?width=1778&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cff35eda5fa5628659c5b0c7abf2f4903644419b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mental_Figure_1130"&gt; /u/Mental_Figure_1130 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzytme/caret_a_terminal_tool_to_inspect_and_clean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:26:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1r05p7o</id>
    <title>What I've Learned From Digitizing 20 Million Historical Documents</title>
    <updated>2026-02-09T14:31:22+00:00</updated>
    <author>
      <name>/u/noahdasanaike</name>
      <uri>https://old.reddit.com/user/noahdasanaike</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noahdasanaike"&gt; /u/noahdasanaike &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://noahdasanaike.github.io/posts/digitizing-census.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r05p7o/what_ive_learned_from_digitizing_20_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r05p7o/what_ive_learned_from_digitizing_20_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02j1i</id>
    <title>Ported from-scratch Inference Engine based on LFM2-350M to pure C!</title>
    <updated>2026-02-09T12:09:02+00:00</updated>
    <author>
      <name>/u/Des_goes_Brrr</name>
      <uri>https://old.reddit.com/user/Des_goes_Brrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Previously implemented Batched Inference Engine built from first principles with focus on correctness, not optimizations. Achieved single batch CPU speeds of 50 tokens/second on M2-Pro 16 GB CPU, but only 4 tokens/second on my old Intel Core i5 laptop. &lt;/p&gt; &lt;p&gt;Previous post link: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qb4ydw/batched_inference_engine_with_lfms_dense_model/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qb4ydw/batched_inference_engine_with_lfms_dense_model/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The old laptop speeds disappointed me, hence reimplementing the single-batch inference part in pure C, achieving 3x speedups (from 4 tokens/second to 12 tokens/second) with no other optimizations than hybrid caching and CBLAS GEMM APIs for Intel (OneMKL) and Arm (ArmPL). Again, building from first principles, used bin files and not gguf files and no other optimizations used!&lt;/p&gt; &lt;p&gt;GitHub Link: &lt;a href="https://github.com/marvinmboya/LFMs-Continuous-Batching-in-C"&gt;https://github.com/marvinmboya/LFMs-Continuous-Batching-in-C&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Big Thanks to:&lt;br /&gt; Kay Lack's &amp;quot;Just enough C to have fun!&amp;quot; , &lt;a href="https://www.youtube.com/watch?v=5aZiRjgSGQU"&gt;https://www.youtube.com/watch?v=5aZiRjgSGQU&lt;/a&gt; . The best C crash course video by far! Jacob Sorber's C programming videos, &lt;a href="https://www.youtube.com/@JacobSorber"&gt;https://www.youtube.com/@JacobSorber&lt;/a&gt; . Used to remind myself of C tooling and capabilities. Also adopted RoPE implementation from antirez's C repo on Flux.2-Klein, with minor tweaks! &lt;/p&gt; &lt;p&gt;This project was not initially planned, just birthed out of disappointment in my old laptop's single-batch decoding speeds! Enjoyed it though! &lt;/p&gt; &lt;p&gt;I am currently in &lt;strong&gt;Massachusetts, USA&lt;/strong&gt;, &lt;strong&gt;#OpenToWork&lt;/strong&gt; for &lt;strong&gt;intern&lt;/strong&gt; and &lt;strong&gt;full time&lt;/strong&gt; roles, &lt;strong&gt;willing to relocate&lt;/strong&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Des_goes_Brrr"&gt; /u/Des_goes_Brrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02j1i/ported_fromscratch_inference_engine_based_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02j1i/ported_fromscratch_inference_engine_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02j1i/ported_fromscratch_inference_engine_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:09:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzsbn9</id>
    <title>StepFun is preparing a "bigger surprise" for Chinese New Year, and will also release Step-3.5-Flash-Base.</title>
    <updated>2026-02-09T02:40:54+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"&gt; &lt;img alt="StepFun is preparing a &amp;quot;bigger surprise&amp;quot; for Chinese New Year, and will also release Step-3.5-Flash-Base." src="https://preview.redd.it/zytph079tdig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c50a154f20b8f8f0e56f8e7c6353847588c73a1" title="StepFun is preparing a &amp;quot;bigger surprise&amp;quot; for Chinese New Year, and will also release Step-3.5-Flash-Base." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/21#698941a597b7256a083f94b6"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash/discussions/21#698941a597b7256a083f94b6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;They also mentioned discussions with Nvidia regarding NVFP4 and responded to questions about excessive token usage by stating they are working on it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zytph079tdig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzsbn9/stepfun_is_preparing_a_bigger_surprise_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T02:40:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzxgxp</id>
    <title>Qwen3.5 dense and MoE support on llama.cpp</title>
    <updated>2026-02-09T07:02:39+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Spotted &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b7973"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b7973&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxgxp/qwen35_dense_and_moe_support_on_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxgxp/qwen35_dense_and_moe_support_on_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzxgxp/qwen35_dense_and_moe_support_on_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T07:02:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03xow</id>
    <title>I tested Kimi k2.5 against Opus. I was hopeful and Kimi didn’t let me down</title>
    <updated>2026-02-09T13:16:07+00:00</updated>
    <author>
      <name>/u/LimpComedian1317</name>
      <uri>https://old.reddit.com/user/LimpComedian1317</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been using Opus for almost all code-related work and Kimi for anything and everything else, from writing to brain dumping. It’s honestly the model with the highest EQ.&lt;/p&gt; &lt;p&gt;Their announcement early this month was a pretty big bang. It was beating frontier models on several tasks while being much cheaper. So, I was wondering if I could just replace Opus with Kimi K2.5, which would save me a lot of money lol. I don’t do hardcore stuff; anything that can solve mid-tier coding tasks at a much lower cost than Opus is welcome.&lt;/p&gt; &lt;p&gt;I have tried Deepseek v3 special, it’s good, but it wasn’t there yet.&lt;/p&gt; &lt;p&gt;So, here’s what I found out.&lt;/p&gt; &lt;h1&gt;The repo + tasks&lt;/h1&gt; &lt;p&gt;I made a Next.js web app, a Google Earth-style globe viewer using Cesium. Both models started from the same clean commit and received the same prompts.&lt;/p&gt; &lt;p&gt;Task 1 was building the actual globe app (Cesium globe, pan/zoom/rotate, base layers, and basic UI). Task 2 was the real test: add auth, wire PostHog via Composio (wanted to dogfood our new PostHog integration), capture user location after sign-in, then show active users as markers on the globe with name/email on click.&lt;/p&gt; &lt;p&gt;Both the models were in Claude Code.&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Task 1 (Globe build):&lt;/strong&gt; Both got close; both needed a fix pass.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2.5:&lt;/strong&gt; ~29m + 9m 43s fix, &lt;strong&gt;15.9k output tokens&lt;/strong&gt;, &lt;strong&gt;429 files changed&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opus 4.5:&lt;/strong&gt; ~23m + ~7m fix, &lt;strong&gt;22 files changed&lt;/strong&gt; (token breakdown wasn’t available for this run)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Task 2 (Auth + Composio + PostHog):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kimi first tried to run a server-only package in the browser, auth broke. Then it tried NextAuth, and that was busted too. The fix loop just kept making things worse and fumbling the output. Meanwhile, Opus just did the full flow end-to-end, and it worked. It was expected.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Kimi-K2.5:&lt;/strong&gt; ~18m + 5m 2s + 1m 3s fixes, &lt;strong&gt;24.3k output tokens&lt;/strong&gt;, &lt;strong&gt;21 files changed&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opus 4.5:&lt;/strong&gt; ~40+ min, &lt;strong&gt;21.6k output tokens&lt;/strong&gt;, &lt;strong&gt;6 files changed&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’ve got demos + prompts + &lt;code&gt;.patch&lt;/code&gt; files in the blog so you can apply the exact changes locally and judge it yourself: &lt;a href="https://composio.dev/blog/kimi-k2.5-vs-opus-4.6"&gt;Kimi K2.5 vs. Opus 4.5: David vs. Goliath&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As far as code quality and output go, I knew the answer; it’s even a bit unfair to put these two together. But Kimi k2.5 would actually be sufficient for a lot of tasks. And it’s definitely better than Sonnet and would be ideal for other non-coding tasks where cost is a concern. I am pretty sure this is currently the best model for building agentic products.&lt;/p&gt; &lt;p&gt;Would love your experience building with Kimi K2.5, any tips and tricks to get the best out of it are welcome. I want to cancel my max sub lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LimpComedian1317"&gt; /u/LimpComedian1317 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03xow/i_tested_kimi_k25_against_opus_i_was_hopeful_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03xow/i_tested_kimi_k25_against_opus_i_was_hopeful_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r03xow/i_tested_kimi_k25_against_opus_i_was_hopeful_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T13:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzwzqj</id>
    <title>ministral-3-3b is great model, give it a shot!</title>
    <updated>2026-02-09T06:35:18+00:00</updated>
    <author>
      <name>/u/FeiX7</name>
      <uri>https://old.reddit.com/user/FeiX7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently I was experimenting the small models that can do tool calls effectively and can fit in 6GB Vram and I found ministral-3-3b.&lt;/p&gt; &lt;p&gt;Currently using it's instruct version with Q8 and it's accuracy to run tools written in skills md is generous.&lt;/p&gt; &lt;p&gt;I am curious about your use cases of this model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FeiX7"&gt; /u/FeiX7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzwzqj/ministral33b_is_great_model_give_it_a_shot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T06:35:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01sek</id>
    <title>POV: You left repetition_penalty at 1.0</title>
    <updated>2026-02-09T11:28:46+00:00</updated>
    <author>
      <name>/u/AurumDaemonHD</name>
      <uri>https://old.reddit.com/user/AurumDaemonHD</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01sek/pov_you_left_repetition_penalty_at_10/"&gt; &lt;img alt="POV: You left repetition_penalty at 1.0" src="https://external-preview.redd.it/dXk0YW5vcW1lZ2lnMetm2yxWlv74oo2KCat6XpxnSQj55CMNYXwNFsRMpvok.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f734f7997c12fe223c17899a35ea1fb934c5f75" title="POV: You left repetition_penalty at 1.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AurumDaemonHD"&gt; /u/AurumDaemonHD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/fuvcpoqmegig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01sek/pov_you_left_repetition_penalty_at_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r01sek/pov_you_left_repetition_penalty_at_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T11:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r01u46</id>
    <title>Ryzen + RTX: you might be wasting VRAM without knowing it (LLama Server)</title>
    <updated>2026-02-09T11:31:23+00:00</updated>
    <author>
      <name>/u/Medium-Technology-79</name>
      <uri>https://old.reddit.com/user/Medium-Technology-79</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a pretty stupid mistake, but it’s &lt;em&gt;so&lt;/em&gt; easy to fall into it that I wanted to share it, hoping it might help someone else.&lt;/p&gt; &lt;p&gt;The workstation I use has a Ryzen 9 CPU with an integrated GPU, which I think is a very common setup.&lt;br /&gt; I also have an Nvidia RTX GPU installed in a PCIe slot.&lt;/p&gt; &lt;p&gt;My monitor was connected directly to the Nvidia GPU, which means Windows 11 uses it as the primary GPU (for example when opening a browser, watching YouTube, etc.).&lt;/p&gt; &lt;p&gt;In this configuration, Llama-Server does &lt;strong&gt;not&lt;/strong&gt; have access to the full VRAM of the Nvidia GPU, because part of it is already being used by the operating system for graphics. And when you’re close to the VRAM limit, this makes a &lt;em&gt;huge&lt;/em&gt; difference.&lt;/p&gt; &lt;p&gt;I discovered this completely by accident... I'm VRAM addicted!&lt;/p&gt; &lt;p&gt;After connecting the monitor to the motherboard and rebooting the PC, I was able to confirm that Llama-Server had access to &lt;strong&gt;all&lt;/strong&gt; of the precious VRAM.&lt;br /&gt; Using Windows Task Manager, you can see that the Nvidia GPU VRAM is completely free, while the integrated GPU VRAM is being used instead.&lt;/p&gt; &lt;p&gt;I know this isn’t anything revolutionary, but maybe someone else is making the same mistake without realizing it.&lt;/p&gt; &lt;p&gt;Just it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medium-Technology-79"&gt; /u/Medium-Technology-79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01u46/ryzen_rtx_you_might_be_wasting_vram_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r01u46/ryzen_rtx_you_might_be_wasting_vram_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r01u46/ryzen_rtx_you_might_be_wasting_vram_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T11:31:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzppr7</id>
    <title>Qwen3.5 Support Merged in llama.cpp</title>
    <updated>2026-02-09T00:32:33+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"&gt; &lt;img alt="Qwen3.5 Support Merged in llama.cpp" src="https://external-preview.redd.it/LP9lWJIkvOFwEJy7i2edxqBM2iBmROue3pUEdiXyxYg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a45fd4b46acdf1a22c62c7c684471a43354c1397" title="Qwen3.5 Support Merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19435"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzppr7/qwen35_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T00:32:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzjbw2</id>
    <title>I built a rough .gguf LLM visualizer</title>
    <updated>2026-02-08T20:08:31+00:00</updated>
    <author>
      <name>/u/sultan_papagani</name>
      <uri>https://old.reddit.com/user/sultan_papagani</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"&gt; &lt;img alt="I built a rough .gguf LLM visualizer" src="https://b.thumbs.redditmedia.com/kcxBxykQQ15O2Oz4xDuJc0i9OygqR7aRSLKKBTm5a5E.jpg" title="I built a rough .gguf LLM visualizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hacked together a small tool that lets you upload a .gguf file and visualize its internals in a 3D-ish way (layers / neurons / connections). The original goal was just to see what’s inside these models instead of treating them like a black box. &lt;/p&gt; &lt;p&gt;That said, my version is pretty rough, and I’m very aware that someone who actually knows what they’re doing could’ve built something way better :p &lt;/p&gt; &lt;p&gt;So I figured I’d ask here: Does something like this already exist, but done properly? If yes, I’d much rather use that For reference, this is really good: &lt;a href="https://bbycroft.net/llm"&gt;https://bbycroft.net/llm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;…but you can’t upload new LLMs.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sultan_papagani"&gt; /u/sultan_papagani &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qzjbw2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzjbw2/i_built_a_rough_gguf_llm_visualizer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-08T20:08:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r015z4</id>
    <title>I managed to jailbreak 43 of 52 recent models</title>
    <updated>2026-02-09T10:52:45+00:00</updated>
    <author>
      <name>/u/sirjoaco</name>
      <uri>https://old.reddit.com/user/sirjoaco</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r015z4/i_managed_to_jailbreak_43_of_52_recent_models/"&gt; &lt;img alt="I managed to jailbreak 43 of 52 recent models" src="https://external-preview.redd.it/YTA3NHl0dmhyZGlnMUNU3vkEOynofhKg3zLh75rLSPZOaY5MGdNqMt8faW6e.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=326b5aa6d703c3059a3c89ab8668c2775aa7efc8" title="I managed to jailbreak 43 of 52 recent models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-5 broke at level 2,&lt;/p&gt; &lt;p&gt;Full report here: &lt;a href="http://rival.tips/jailbreak"&gt;rival.tips/jailbreak&lt;/a&gt; I'll be adding more models to this benchmark soon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sirjoaco"&gt; /u/sirjoaco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xmbxf1vhrdig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r015z4/i_managed_to_jailbreak_43_of_52_recent_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r015z4/i_managed_to_jailbreak_43_of_52_recent_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T10:52:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0519a</id>
    <title>Strix Halo, Step-3.5-Flash-Q4_K_S imatrix, llama.cpp/ROCm/Vulkan Power &amp; Efficiency test</title>
    <updated>2026-02-09T14:04:09+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0519a/strix_halo_step35flashq4_k_s_imatrix/"&gt; &lt;img alt="Strix Halo, Step-3.5-Flash-Q4_K_S imatrix, llama.cpp/ROCm/Vulkan Power &amp;amp; Efficiency test" src="https://preview.redd.it/lf6f8di34hig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f94c8e2dab63321d47133ed77312fadc06ceb5e6" title="Strix Halo, Step-3.5-Flash-Q4_K_S imatrix, llama.cpp/ROCm/Vulkan Power &amp;amp; Efficiency test" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, i did recently some quants to test best fit for strix halo, and i settled with custom imatrix &lt;code&gt;Q4_K_S&lt;/code&gt; quant, builded with &lt;code&gt;wikitext-103-raw-v1&lt;/code&gt;. Model has sligtly better PPL than Q4_K_M without imatrix, but it's few GB smaller. I tested it with ROCm/Vulkan backend, and &lt;code&gt;llama.cpp build 7966 (8872ad212)&lt;/code&gt;, so with Step-3.5-Flash support already merged to the main branch. There are some issues with toolcalling with that (and few others) models at the moment but seems it's not related to quants itself.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Quantization&lt;/th&gt; &lt;th&gt;Size (Binary GiB)&lt;/th&gt; &lt;th&gt;Size (Decimal GB)&lt;/th&gt; &lt;th&gt;PPL (Perplexity)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Q4_K_S (imatrix) THIS VERSION&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;104 GiB&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;111 GB&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;2.4130&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q4_K_M (standard)&lt;/td&gt; &lt;td&gt;111 GiB&lt;/td&gt; &lt;td&gt;119 GB&lt;/td&gt; &lt;td&gt;2.4177&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;ROCm is more efficient: For a full benchmark run, &lt;strong&gt;ROCm was 4.7x faster&lt;/strong&gt; and &lt;strong&gt;consumed 65% less energy&lt;/strong&gt; than Vulkan. Prompt Processing: ROCm dominates in prompt ingestion speed, reaching over 350 t/s for short contexts and maintaining much higher throughput as context grows. Token Generation: Vulkan shows slightly higher raw generation speeds (T/s) for small contexts, but at a significantly higher energy cost. Not efficient with CTX &amp;gt;= 8k. Context Scaling: The model remains usable and tested up to 131k context, though energy costs scale exponentially on the Vulkan backend compared to a more linear progression on ROCm.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mixer3d/step-3.5-flash-imatrix-gguf"&gt;Link to this quant on HF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Outcome from comparison between ROCm/Vulkan is simalar to that one i performed few months ago with Qwen3-Coder, so from now on i will test only ROCm for bigger context, and probably will use Vulkan only as a failover on strix-halo. &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p48d7f/strix_halo_debian_13616126178_qwen3coderq8/"&gt;Link on r/LocalLLaMa for Qwen3coder older benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lf6f8di34hig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0519a/strix_halo_step35flashq4_k_s_imatrix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0519a/strix_halo_step35flashq4_k_s_imatrix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T14:04:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03nyq</id>
    <title>New PR for GLM 5.Show more details for the architecture and parameters</title>
    <updated>2026-02-09T13:03:45+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03nyq/new_pr_for_glm_5show_more_details_for_the/"&gt; &lt;img alt="New PR for GLM 5.Show more details for the architecture and parameters" src="https://preview.redd.it/xbntmqm9wgig1.jpg?width=140&amp;amp;height=88&amp;amp;auto=webp&amp;amp;s=57f90442cdd4687102ce6eb308c88cf7ef31ebf1" title="New PR for GLM 5.Show more details for the architecture and parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43858"&gt;https://github.com/huggingface/transformers/pull/43858&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xbntmqm9wgig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=da75a8dd1887ada367c9152cdeb13ad50fc6796c"&gt;https://preview.redd.it/xbntmqm9wgig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=da75a8dd1887ada367c9152cdeb13ad50fc6796c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wng50ssdwgig1.png?width=1323&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65b30b4b03dc5c4ce8c63d4729121b22c56382dc"&gt;https://preview.redd.it/wng50ssdwgig1.png?width=1323&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65b30b4b03dc5c4ce8c63d4729121b22c56382dc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03nyq/new_pr_for_glm_5show_more_details_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03nyq/new_pr_for_glm_5show_more_details_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r03nyq/new_pr_for_glm_5show_more_details_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T13:03:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1r02o7o</id>
    <title>GLM 5 Support Is On It's Way For Transformers</title>
    <updated>2026-02-09T12:16:36+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"&gt; &lt;img alt="GLM 5 Support Is On It's Way For Transformers" src="https://external-preview.redd.it/_RA8pRu79eov51fP28AH3ibXc2RY_CG7SQQVryJy9WU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=810b321415879975e3408c463a34398fefd38bf5" title="GLM 5 Support Is On It's Way For Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This probably means the model launch is imminent, and all evidence points to Pony Alpha on OpenRouter being a stealth deployment of GLM 5&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/43858"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r02o7o/glm_5_support_is_on_its_way_for_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T12:16:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qzz0vr</id>
    <title>GLM 5 is coming! spotted on vllm PR</title>
    <updated>2026-02-09T08:39:31+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt; &lt;img alt="GLM 5 is coming! spotted on vllm PR" src="https://preview.redd.it/285aias7lfig1.jpg?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=5a644c4fce313f2c4b8643b1d8a7931145a54db1" title="GLM 5 is coming! spotted on vllm PR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023"&gt;https://preview.redd.it/285aias7lfig1.jpg?width=680&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=5287959d193fad4f96c5c80ec8b7546a7dcbe023&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/34124"&gt;https://github.com/vllm-project/vllm/pull/34124&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qzz0vr/glm_5_is_coming_spotted_on_vllm_pr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T08:39:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r00yxq</id>
    <title>A Modest Proposal: A 1% Income Tax on Every Python Library a Developer includes</title>
    <updated>2026-02-09T10:41:15+00:00</updated>
    <author>
      <name>/u/crantob</name>
      <uri>https://old.reddit.com/user/crantob</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r00yxq/a_modest_proposal_a_1_income_tax_on_every_python/"&gt; &lt;img alt="A Modest Proposal: A 1% Income Tax on Every Python Library a Developer includes" src="https://preview.redd.it/8zegrsmr6gig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b57e54c1bdb067aeff6da154784be1fe3279cd14" title="A Modest Proposal: A 1% Income Tax on Every Python Library a Developer includes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crantob"&gt; /u/crantob &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8zegrsmr6gig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r00yxq/a_modest_proposal_a_1_income_tax_on_every_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r00yxq/a_modest_proposal_a_1_income_tax_on_every_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T10:41:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03wfq</id>
    <title>Bad news for local bros</title>
    <updated>2026-02-09T13:14:31+00:00</updated>
    <author>
      <name>/u/FireGuy324</name>
      <uri>https://old.reddit.com/user/FireGuy324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"&gt; &lt;img alt="Bad news for local bros" src="https://preview.redd.it/ui5ovstbygig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaeb40f2ac5a09ac1ba2fe03e433877561acb20" title="Bad news for local bros" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FireGuy324"&gt; /u/FireGuy324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ui5ovstbygig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T13:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
