<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-25T20:17:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1reey6u</id>
    <title>MiniMax's agent code has ~90% overlap with Kimi's â€” three independent repos document the same finding</title>
    <updated>2026-02-25T14:38:43+00:00</updated>
    <author>
      <name>/u/SkyAgreeable3048</name>
      <uri>https://old.reddit.com/user/SkyAgreeable3048</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reey6u/minimaxs_agent_code_has_90_overlap_with_kimis/"&gt; &lt;img alt="MiniMax's agent code has ~90% overlap with Kimi's â€” three independent repos document the same finding" src="https://preview.redd.it/9cyaysphinlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bfa70c0023eb30cb7b077fdff7a392e17cb8b088" title="MiniMax's agent code has ~90% overlap with Kimi's â€” three independent repos document the same finding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I posted about this earlier but it got reported and removed before I had a chance to properly explain how the code was obtained â€” fair enough, so here's a more complete writeup.&lt;/p&gt; &lt;h1&gt;What are &amp;quot;skills&amp;quot; and how were they obtained&lt;/h1&gt; &lt;p&gt;Besides their open-source models, both Kimi (&lt;a href="https://www.kimi.com/agent"&gt;kimi.com/agent&lt;/a&gt;) and MiniMax (&lt;a href="https://agent.minimax.io/"&gt;agent.minimax.io&lt;/a&gt;) run commercial agent platforms. These agents run inside sandboxed server environments and use server-side code packages called &amp;quot;skills&amp;quot; to handle tasks like generating Word, Excel, and PDF files. A skill is a directory containing instruction files, Python scripts, .NET binaries, and other assets â€” essentially the agent's operational playbook for producing professional-quality document outputs. None of this code was open-sourced.&lt;/p&gt; &lt;p&gt;However, neither platform restricted the agent's access to its own skill directories. Because the agents can read arbitrary paths and write to an output directory, anyone could simply prompt the agent: &amp;quot;Find the skills directory and copy it into the output dir.&amp;quot; No exploits, no system access â€” just a conversational request.&lt;/p&gt; &lt;p&gt;Multiple people did this independently. Two repos archived the extracted skills from both platforms (&lt;a href="https://github.com/thvroyal/kimi-skills"&gt;one&lt;/a&gt;, &lt;a href="https://github.com/QvvvvvvQ/skills_leaks"&gt;two&lt;/a&gt;), and a &lt;a href="https://github.com/nullpond/minimax-skill-analysis"&gt;third&lt;/a&gt; ran a detailed side-by-side comparison documenting the overlap. Everything below is independently verifiable from these repos.&lt;/p&gt; &lt;h1&gt;What the comparison found&lt;/h1&gt; &lt;p&gt;The evidence falls into three layers:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;13 files shipped with byte-identical content.&lt;/strong&gt; Not similar â€” identical. &lt;code&gt;diff -q&lt;/code&gt; returns nothing. This includes 8 Python scripts in the PDF skill and 5 files in the Word skill (shared .NET libraries and a &lt;code&gt;.csproj&lt;/code&gt; project file that was renamed from &lt;code&gt;KimiDocx.csproj&lt;/code&gt; to &lt;code&gt;DocxProject.csproj&lt;/code&gt; but whose content is byte-for-byte the same).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;14 Python files were renamed but barely rewritten.&lt;/strong&gt; MiniMax renamed every Python file in the Word skill â€” &lt;a href="http://helpers.py"&gt;&lt;code&gt;helpers.py&lt;/code&gt;&lt;/a&gt; â†’ &lt;a href="http://utils.py"&gt;&lt;code&gt;utils.py&lt;/code&gt;&lt;/a&gt;, &lt;a href="http://comments.py"&gt;&lt;code&gt;comments.py&lt;/code&gt;&lt;/a&gt; â†’ &lt;a href="http://annotations.py"&gt;&lt;code&gt;annotations.py&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;business_rules.py&lt;/code&gt; â†’ &lt;a href="http://integrity.py"&gt;&lt;code&gt;integrity.py&lt;/code&gt;&lt;/a&gt; â€” but the logic was left untouched. A 727-line file had 6 lines changed, all import renames. A 593-line file had 4 lines changed. The XML manipulation, validation algorithms, and element ordering are character-for-character identical.&lt;/p&gt; &lt;p&gt;On top of all this, MiniMax left provenance markers in their own code. A compiled binary (&lt;code&gt;DocxChecker.dll&lt;/code&gt;) still contained the build path &lt;code&gt;kimiagent/.kimi/skills/&lt;/code&gt; in its metadata â€” a build artifact from Kimi's dev environment, shipped inside MiniMax's product. And &lt;code&gt;browser_helper.js&lt;/code&gt; had &lt;code&gt;'kimi'&lt;/code&gt; hardcoded in a username list for scanning Chromium installations.&lt;/p&gt; &lt;h1&gt;MiniMax's response&lt;/h1&gt; &lt;p&gt;MiniMax has since pushed multiple rounds of rewrites. The DLL was deleted, the entire PDF skill was removed, directory structures were reorganized, and the C# project was renamed again. But the early versions are all archived in the repos above, and the core logic and algorithms remain the same.&lt;/p&gt; &lt;h1&gt;Why this matters&lt;/h1&gt; &lt;p&gt;The fact that this code was obtainable via prompt doesn't make it fair game â€” these are proprietary, in-house codebases powering commercial products. Kimi never open-sourced any of it. Shipping someone else's proprietary code in your own commercial product without attribution or permission, then scrambling to rewrite it once it's discovered, goes well beyond what we've been debating with model distillation. That discussion is about gray areas. This one isn't.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyAgreeable3048"&gt; /u/SkyAgreeable3048 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9cyaysphinlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reey6u/minimaxs_agent_code_has_90_overlap_with_kimis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reey6u/minimaxs_agent_code_has_90_overlap_with_kimis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T14:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6fud</id>
    <title>Your coding agent sessions are sitting on your machine right now. Big labs use this data internally. We could build an open equivalent.</title>
    <updated>2026-02-25T07:11:25+00:00</updated>
    <author>
      <name>/u/No-Point1424</name>
      <uri>https://old.reddit.com/user/No-Point1424</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Every time you use Claude Code or Codex CLI in agent mode, it logs everything locally. The full loop: your task, the model's reasoning, every tool call, every environment response, every error and retry. Complete (state â†’ action â†’ reward â†’ next state) tuples. The exact data format RL researchers dream about.&lt;/p&gt; &lt;p&gt;I checked all my machines today.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Mac Mini: ~/.claude/projects/ 3.1GB 1103 files 574 agentic sessions MacBook: ~/.codex/sessions/ 2.4GB 3530 files 79 agentic sessions ~/.claude/projects/ 652MB 316 files 99 agentic sessions &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;775 sessions with real tool calls. 41 million tokens.&lt;/p&gt; &lt;p&gt;Extrapolate to thousands developers and we would have hundreds of billions tokens of real agentic trajectory data. No Pile equivalent exists for this. It's just sitting on people's hard drives, being silently deleted.&lt;/p&gt; &lt;p&gt;Claude Code deletes logs after 30 days by default. Fix it now:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;echo '{&amp;quot;cleanupPeriodDays&amp;quot;: 36500}' &amp;gt; ~/.claude/settings.json &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Why this data matters&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The environment always tells you if it worked. Exit code 0 or not. Tests pass or not. This is the missing training signal , causal reasoning, error recovery, long-horizon planning. Things current models are genuinely bad at.&lt;/p&gt; &lt;p&gt;Big labs already collect this. Every Claude Code,codex session trains proprietary models. There's no open equivalent, not because the data doesn't exist, but because it's fragmented across developer machines.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The proposal&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Federated learning. Your data never leaves your machine. You train a small LoRA adapter locally, share only the weights with differential privacy noise, and get an improved global model back. Everyone contributes compute and signal. Nobody exposes their data or we can anonymize the data and create a dataset finetune a model.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Check your own machines&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;du -sh ~/.codex/sessions/ 2&amp;gt;/dev/null du -sh ~/.claude/projects/ 2&amp;gt;/dev/null find ~/.codex/sessions/ -name &amp;quot;*.jsonl&amp;quot; | wc -l find ~/.claude/projects/ -name &amp;quot;*.jsonl&amp;quot; | wc -l &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Drop your numbers in the comments. I want to know the actual scale sitting unused across this community.&lt;/p&gt; &lt;p&gt;If there's enough interest we can build this out.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Point1424"&gt; /u/No-Point1424 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6fud/your_coding_agent_sessions_are_sitting_on_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6fud/your_coding_agent_sessions_are_sitting_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re6fud/your_coding_agent_sessions_are_sitting_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:11:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1reo5bv</id>
    <title>The Qwen 3.5 A3B model at 4 bit k_xl works better with 8 bit KV cache...</title>
    <updated>2026-02-25T20:02:47+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'll probably toss up some examples later, but I've got some things to do today. I just wanted to mention that I did a whole mess of personal benchmark/testing on that new qwen 3.5 A3b. That thing is amazing.&lt;/p&gt; &lt;p&gt;Interestingly, when I re-ran everything at Q8_0 KV Cache, it improved across the board. Normally, kicking KV cache to 8 bit gives me a bit more headroom but has a measurable drop in performance, so this was a weird result I thought I'd share.&lt;/p&gt; &lt;p&gt;Anyone else mess with this?&lt;/p&gt; &lt;p&gt;Remarkable model all around. I can't wait to mess with this a bit more later. Going to set up some wild stuff :).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reo5bv/the_qwen_35_a3b_model_at_4_bit_k_xl_works_better/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reo5bv/the_qwen_35_a3b_model_at_4_bit_k_xl_works_better/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reo5bv/the_qwen_35_a3b_model_at_4_bit_k_xl_works_better/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T20:02:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1recm21</id>
    <title>H-Neurons: On The Existence, Impact, And Origin Of Hallucination-Associated Neurons In Llms | "Tsinghua Researchers Found The Exact Neurons That Make Llms Hallucinate"</title>
    <updated>2026-02-25T13:02:42+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1recm21/hneurons_on_the_existence_impact_and_origin_of/"&gt; &lt;img alt="H-Neurons: On The Existence, Impact, And Origin Of Hallucination-Associated Neurons In Llms | &amp;quot;Tsinghua Researchers Found The Exact Neurons That Make Llms Hallucinate&amp;quot;" src="https://preview.redd.it/5818iy1t2nlg1.jpg?width=140&amp;amp;height=127&amp;amp;auto=webp&amp;amp;s=ab1d912de94144cc6092ea0aae445be06c2d2577" title="H-Neurons: On The Existence, Impact, And Origin Of Hallucination-Associated Neurons In Llms | &amp;quot;Tsinghua Researchers Found The Exact Neurons That Make Llms Hallucinate&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;Abstract:&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Large language models (LLMs) frequently generate hallucinations â€“ plausible but factually incorrect outputs â€“ undermining their reliability. While prior work has examined hallucinations from macroscopic perspectives such as training data and objectives, the underlying neuron-level mechanisms remain largely unexplored. In this paper, we conduct a systematic investigation into hallucination-associated neurons (H-Neurons) in LLMs from three perspectives: identification, behavioral impact, and origins. Regarding their identification, we demonstrate that a remarkably sparse subset of neurons (less than 0.1% of total neurons) can reliably predict hallucination occurrences, with strong generalization across diverse scenarios. In terms of behavioral impact, controlled interventions reveal that these neurons are causally linked to over-compliance behaviors. Concerning their origins, we trace these neurons back to the pre-trained base models and find that these neurons remain predictive for hallucination detection, indicating they emerge during pre-training. Our findings bridge macroscopic behavioral patterns with microscopic neural mechanisms, offering insights for developing more reliable LLMs.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;Layman's Explanation:&lt;/h2&gt; &lt;p&gt;When an LLM makes something up like says Sydney is the capital of Australia with total confidence, that's a hallucination, and until now nobody really knew where inside the model that behavior comes from. &lt;strong&gt;This paper found it.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;There's a tiny group of neurons, less than one tenth of one percent of all the neurons in the model, that light up specifically when the model is about to hallucinate. The researchers call them &lt;strong&gt;H-Neurons&lt;/strong&gt;. They found them by giving models thousands of trivia questions, collecting cases where the model consistently got things right and consistently got things wrong, and then looking at which neurons were doing more work during the wrong answers. &lt;/p&gt; &lt;p&gt;The part that matters most is what these neurons actually do. These neurons encode something the authors call over-compliance: a general willingness to give you what you want even when what you want is wrong, dangerous, or nonsensical. Hallucination is just one way that tendency expresses itself. The model fabricates an answer because the alternative of saying &amp;quot;I don't know&amp;quot; feels like not doing its job. It's the same impulse that makes it agree when you challenge a correct answer, or follow a jailbreak prompt. Same neurons, same circuit, different symptoms, all suppressable. &lt;/p&gt; &lt;hr /&gt; &lt;h5&gt;Link to the Paper: &lt;a href="https://arxiv.org/html/2512.01797"&gt;https://arxiv.org/html/2512.01797&lt;/a&gt;&lt;/h5&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1recm21"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1recm21/hneurons_on_the_existence_impact_and_origin_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1recm21/hneurons_on_the_existence_impact_and_origin_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T13:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1reluol</id>
    <title>Run LFM2.5-1.2B-Thinking at over 200 tokens per second in your browser on WebGPU</title>
    <updated>2026-02-25T18:42:15+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reluol/run_lfm2512bthinking_at_over_200_tokens_per/"&gt; &lt;img alt="Run LFM2.5-1.2B-Thinking at over 200 tokens per second in your browser on WebGPU" src="https://external-preview.redd.it/Z2wwNDduMXhtb2xnMY80GElZp8bY1WsZbcTEiuAuQg5uYjbmxYx9iXSM6pgO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a795bb5bad440b62a643cf67e0fd34a0168d5101" title="Run LFM2.5-1.2B-Thinking at over 200 tokens per second in your browser on WebGPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The model runs 100% locally in the browser on WebGPU with Transformers.js. This video was recorded on an M4 Max, but do let me know what speed you get on your hardware so we can continue improving performance across all hardware.&lt;/p&gt; &lt;p&gt;Try it out yourself! &lt;a href="https://huggingface.co/spaces/LiquidAI/LFM2.5-1.2B-Thinking-WebGPU"&gt;https://huggingface.co/spaces/LiquidAI/LFM2.5-1.2B-Thinking-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qrapad1xmolg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reluol/run_lfm2512bthinking_at_over_200_tokens_per/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reluol/run_lfm2512bthinking_at_over_200_tokens_per/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T18:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1relj66</id>
    <title>Qwen dropped Qwen3.5-FP8 versions on HF</title>
    <updated>2026-02-25T18:31:05+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yay! I really wanted the 122b-a10b FP8 - excited to test it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen35"&gt;https://huggingface.co/collections/Qwen/qwen35&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1relj66/qwen_dropped_qwen35fp8_versions_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1relj66/qwen_dropped_qwen35fp8_versions_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1relj66/qwen_dropped_qwen35fp8_versions_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T18:31:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1re4uoh</id>
    <title>Qwen 3.5 122b/35b/27b/397b ðŸ“Š benchmark comparison WEBSITE with More models like GPT 5.2, GPT OSS, etc</title>
    <updated>2026-02-25T05:43:59+00:00</updated>
    <author>
      <name>/u/9r4n4y</name>
      <uri>https://old.reddit.com/user/9r4n4y</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re4uoh/qwen_35_122b35b27b397b_benchmark_comparison/"&gt; &lt;img alt="Qwen 3.5 122b/35b/27b/397b ðŸ“Š benchmark comparison WEBSITE with More models like GPT 5.2, GPT OSS, etc" src="https://preview.redd.it/w0mwcw1iwklg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=84a8b126d4eeefb54e018aeae9c7f736eeb2d94e" title="Qwen 3.5 122b/35b/27b/397b ðŸ“Š benchmark comparison WEBSITE with More models like GPT 5.2, GPT OSS, etc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full comparison for GPT-5.2, Claude 4.5 Opus, Gemini-3 Pro, Qwen3-Max-Thinking, K2.5-1T-A32B, Qwen3.5-397B, GPT-5-mini, GPT-OSS-120B, Qwen3-235B, Qwen3.5-122B, Qwen3.5-27B, and Qwen3.5-35B.&lt;/p&gt; &lt;p&gt;â€‹Includes all verified scores and head-to-head infographics here: ðŸ‘‰ &lt;a href="https://compareqwen35.tiiny.site"&gt;https://compareqwen35.tiiny.site&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For test i also made the website with 122B --&amp;gt; &lt;a href="https://9r4n4y.github.io/files-Compare/"&gt;https://9r4n4y.github.io/files-Compare/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ðŸ‘†ðŸ‘†ðŸ‘†&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/9r4n4y"&gt; /u/9r4n4y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1re4uoh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re4uoh/qwen_35_122b35b27b397b_benchmark_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re4uoh/qwen_35_122b35b27b397b_benchmark_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T05:43:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1re76g6</id>
    <title>This benchmark from shows Unsolth Q3 quantization beats both Q4 and MXFP4</title>
    <updated>2026-02-25T07:55:49+00:00</updated>
    <author>
      <name>/u/Oatilis</name>
      <uri>https://old.reddit.com/user/Oatilis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re76g6/this_benchmark_from_shows_unsolth_q3_quantization/"&gt; &lt;img alt="This benchmark from shows Unsolth Q3 quantization beats both Q4 and MXFP4" src="https://preview.redd.it/5wtmzjgvillg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d4c18e0d43199c66837a33ca093dde5739ad022" title="This benchmark from shows Unsolth Q3 quantization beats both Q4 and MXFP4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I thought this was interesting, especially since at first glance both Q4 and Q3 here are K_XL, and it doesn't make sense a Q3 will beat Q4 in any scenario. &lt;/p&gt; &lt;p&gt;However it's worth mentioning this is:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Not a standard benchmark&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;These are not straight-forward quantizations, it's a &amp;quot;dynamic quantization&amp;quot; which affects weights differently across the model. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My money is on one of these two factors leading to this results, however, if by any chance a smaller quantization does beat a larger one, this is super interesting in terms research.&lt;/p&gt; &lt;p&gt;&lt;a href="https://unsloth.ai/docs/models/qwen3.5#qwen3.5-397b-a17b-benchmarks"&gt;Source&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Oatilis"&gt; /u/Oatilis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5wtmzjgvillg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re76g6/this_benchmark_from_shows_unsolth_q3_quantization/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re76g6/this_benchmark_from_shows_unsolth_q3_quantization/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:55:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1re3l3r</id>
    <title>Qwen3-30B-A3B vs Qwen3.5-35B-A3B on RTX 5090</title>
    <updated>2026-02-25T04:39:52+00:00</updated>
    <author>
      <name>/u/3spky5u-oss</name>
      <uri>https://old.reddit.com/user/3spky5u-oss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Qwen3-30B-A3B vs Qwen3.5-35B-A3B on RTX 5090 â€” Day-1 Extended Benchmark (Q4_K_M, llama.cpp)&lt;/h1&gt; &lt;p&gt;Qwen3.5-35B-A3B dropped today. Same MoE architecture as the 30B (3B active params), 5B more total parameters, and ships with a vision projector. Grabbed the Q4_K_M, ran it head-to-head against my daily driver Qwen3-30B-A3B through 7 test sections. All automated, same prompts, same hardware, same server config.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR: The 3.5 is ~32% slower in raw generation but handles long context significantly better â€” flat tok/s scaling vs the 30B's 21% degradation. Thinking mode is where it gets interesting. Quality is a wash with slight 3.5 edge in structure/formatting.&lt;/strong&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Hardware &amp;amp; Setup&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;NVIDIA RTX 5090 (32 GB VRAM, Blackwell)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Server&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;llama.cpp b8115 (Docker: ghcr.io/ggml-org/llama.cpp:server-cuda)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Quant&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Q4_K_M for both models&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;KV Cache&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Q8_0 (-ctk q8_0 -ctv q8_0)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;32,768 tokens (-c 32768)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Params&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;-ngl 999 -np 4 --flash-attn on -t 12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Model A&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Qwen3-30B-A3B-Q4_K_M (17 GB on disk)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Model B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Qwen3.5-35B-A3B-Q4_K_M (21 GB on disk)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Both models warmed up with a throwaway request before timing. Server-side timings from the API response (not wall-clock).&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 1: Raw Inference Speed&lt;/h2&gt; &lt;p&gt;Direct to llama.cpp /v1/chat/completions. No middleware.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="right"&gt;30B tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 tok/s&lt;/th&gt; &lt;th align="right"&gt;30B prompt t/s&lt;/th&gt; &lt;th align="right"&gt;3.5 prompt t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Short (8-9 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;248.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;169.5&lt;/td&gt; &lt;td align="right"&gt;59.1&lt;/td&gt; &lt;td align="right"&gt;62.9&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Medium (73-78 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;236.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;163.5&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;751.4&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;495.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Long-form (800 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;232.6&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;116.3&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1,015.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;651.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Code gen (298-400 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;233.9&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;161.6&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;905.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;656.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Reasoning (200 tok)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;234.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;158.2&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;1,136.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;724.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Average&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;237.1&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;153.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;773.5&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;518.1&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 30B is 35% faster in generation on average. The 3.5 drops to 116 tok/s on long outputs (800 tokens) â€” interesting regression vs its ~160 tok/s on shorter outputs. Prompt processing also notably slower on the 3.5 (larger vocab: 248K vs 152K tokens).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VRAM&lt;/strong&gt;: 30B uses 27.3 GB idle, 3.5 uses 29.0 GB idle. Both fit comfortably on the 5090.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 2: Response Quality (Side-by-Side)&lt;/h2&gt; &lt;p&gt;Same prompts, temperature=0.7. Both models produce competent output across all categories. A few standouts:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Creative: &amp;quot;Short story about an engineer at a construction site&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;30B:&lt;/strong&gt; Engineer Mara arrived at the construction site to find a strange metallic structure buried beneath the earth. Curious, she ordered the crew to stop and excavate carefully...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5:&lt;/strong&gt; Engineer Elias adjusted his hard hat, scanning the foundation of the new city library. The soil samples were pristine, yet his drone feed showed a strange, rhythmic vibration beneath the concrete pour...&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Both solid. The 3.5 has slightly more atmospheric prose.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Haiku: &amp;quot;Write a haiku about concrete curing&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;30B:&lt;/strong&gt; Hard and gray, / slowly it gains strength in silence â€” / concrete breathes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5:&lt;/strong&gt; Gray slurry turns hard / Sunlight warms the drying set / Stronger with each day&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Both valid 5-7-5. Matter of taste.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Coding: LRU Cache with O(1) get/put&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Both models correctly implement an LRU cache using OrderedDict or a doubly-linked list + hashmap. The 3.5 generates more code (800 tokens vs 644) with more verbose docstrings and explanations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Reasoning: Terzaghi bearing capacity calculation&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;30B (254 tokens):&lt;/strong&gt; Gets to the answer quickly with clear step-by-step.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5 (500 tokens):&lt;/strong&gt; More structured with numbered sections, parameter identification, and explicit Terzaghi equation for undrained clay (qu = cu * Nc + q * Nq). More thorough.&lt;/p&gt; &lt;p&gt;Both arrive at the correct answer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Domain: USCS soil classification (LL=45, PL=22, 60% passing #200)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Both correctly classify as &lt;strong&gt;CL (Lean Clay)&lt;/strong&gt;. Both show PI = 45 - 22 = 23, check the Casagrande plasticity chart, and arrive at CL. The 3.5 explicitly references ASTM D2487 and formats as a decision flowchart. 30B is more conversational but equally correct.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 3: RAG Pipeline&lt;/h2&gt; &lt;p&gt;Both models tested through a full RAG system (hybrid vector + BM25 retrieval with reranking, geotechnical knowledge base). This tests how well the model grounds its answers in retrieved context.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="center"&gt;30B RAG&lt;/th&gt; &lt;th align="center"&gt;3.5 RAG&lt;/th&gt; &lt;th align="right"&gt;30B Cites&lt;/th&gt; &lt;th align="right"&gt;3.5 Cites&lt;/th&gt; &lt;th align="center"&gt;30B Frame&lt;/th&gt; &lt;th align="center"&gt;3.5 Frame&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&amp;quot;CBR&amp;quot; (3 chars)&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&amp;quot;Define permafrost&amp;quot;&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;2&lt;/td&gt; &lt;td align="right"&gt;2&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Freeze-thaw on glaciolacustrine clay&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Atterberg limits for glacial till&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="center"&gt;BAD&lt;/td&gt; &lt;td align="center"&gt;BAD&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Schmertmann method&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPT vs SPT comparison&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;td align="center"&gt;OK&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Both trigger RAG on all 6 queries. Both have exactly 1 &amp;quot;document framing&amp;quot; issue (the model says &amp;quot;the documents indicate...&amp;quot; instead of speaking as the expert). The 3.5 generates wordier responses (183 words on &amp;quot;CBR&amp;quot; vs 101).&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 4: Context Length Scaling&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;This is the most interesting result.&lt;/strong&gt; Generation tok/s as context size grows:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;Context Tokens&lt;/th&gt; &lt;th align="right"&gt;30B gen tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 gen tok/s&lt;/th&gt; &lt;th align="right"&gt;30B prompt t/s&lt;/th&gt; &lt;th align="right"&gt;3.5 prompt t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;512&lt;/td&gt; &lt;td align="right"&gt;237.9&lt;/td&gt; &lt;td align="right"&gt;160.1&lt;/td&gt; &lt;td align="right"&gt;1,219&lt;/td&gt; &lt;td align="right"&gt;3,253&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;1,024&lt;/td&gt; &lt;td align="right"&gt;232.8&lt;/td&gt; &lt;td align="right"&gt;159.5&lt;/td&gt; &lt;td align="right"&gt;4,884&lt;/td&gt; &lt;td align="right"&gt;3,695&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;2,048&lt;/td&gt; &lt;td align="right"&gt;224.1&lt;/td&gt; &lt;td align="right"&gt;161.3&lt;/td&gt; &lt;td align="right"&gt;6,375&lt;/td&gt; &lt;td align="right"&gt;3,716&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;4,096&lt;/td&gt; &lt;td align="right"&gt;205.9&lt;/td&gt; &lt;td align="right"&gt;161.4&lt;/td&gt; &lt;td align="right"&gt;6,025&lt;/td&gt; &lt;td align="right"&gt;3,832&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;8,192&lt;/td&gt; &lt;td align="right"&gt;186.6&lt;/td&gt; &lt;td align="right"&gt;158.6&lt;/td&gt; &lt;td align="right"&gt;5,712&lt;/td&gt; &lt;td align="right"&gt;3,877&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;30B degrades 21.5% from 512 to 8K context&lt;/strong&gt; (238 -&amp;gt; 187 tok/s). The 3.5 stays &lt;strong&gt;essentially flat&lt;/strong&gt; â€” 160.1 to 158.6, only -0.9% degradation.&lt;/p&gt; &lt;p&gt;The 3.5 also shows flat prompt processing speed as context grows (3.2K -&amp;gt; 3.9K, slight increase), while the 30B peaks at 2K context then slowly declines.&lt;/p&gt; &lt;p&gt;If you're running long conversations or RAG with big context windows, the 3.5 will hold its speed better.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 5: Structured Output (JSON)&lt;/h2&gt; &lt;p&gt;Both models asked to return raw JSON (no markdown wrappers, no explanation). Four tests of increasing complexity.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="center"&gt;30B Valid&lt;/th&gt; &lt;th align="center"&gt;3.5 Valid&lt;/th&gt; &lt;th align="center"&gt;30B Clean&lt;/th&gt; &lt;th align="center"&gt;3.5 Clean&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Simple object (Tokyo)&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Array of 5 planets&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nested soil report&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Schema-following project&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;td align="center"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Both: 4/4 valid JSON, 4/4 clean&lt;/strong&gt; (no markdown code fences when asked not to use them). Perfect scores. No difference here.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 6: Multi-Turn Conversation&lt;/h2&gt; &lt;p&gt;5-turn conversation about foundation design, building up conversation history each turn.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;Turn&lt;/th&gt; &lt;th align="right"&gt;30B tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 tok/s&lt;/th&gt; &lt;th align="right"&gt;30B prompt tokens&lt;/th&gt; &lt;th align="right"&gt;3.5 prompt tokens&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;234.4&lt;/td&gt; &lt;td align="right"&gt;161.0&lt;/td&gt; &lt;td align="right"&gt;35&lt;/td&gt; &lt;td align="right"&gt;34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;2&lt;/td&gt; &lt;td align="right"&gt;230.6&lt;/td&gt; &lt;td align="right"&gt;160.6&lt;/td&gt; &lt;td align="right"&gt;458&lt;/td&gt; &lt;td align="right"&gt;456&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;3&lt;/td&gt; &lt;td align="right"&gt;228.5&lt;/td&gt; &lt;td align="right"&gt;160.8&lt;/td&gt; &lt;td align="right"&gt;892&lt;/td&gt; &lt;td align="right"&gt;889&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;4&lt;/td&gt; &lt;td align="right"&gt;221.5&lt;/td&gt; &lt;td align="right"&gt;161.0&lt;/td&gt; &lt;td align="right"&gt;1,321&lt;/td&gt; &lt;td align="right"&gt;1,317&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;5&lt;/td&gt; &lt;td align="right"&gt;215.8&lt;/td&gt; &lt;td align="right"&gt;160.0&lt;/td&gt; &lt;td align="right"&gt;1,501&lt;/td&gt; &lt;td align="right"&gt;1,534&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;30B: -7.9% degradation&lt;/strong&gt; over 5 turns (234 -&amp;gt; 216 tok/s).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3.5: -0.6% degradation&lt;/strong&gt; over 5 turns (161 -&amp;gt; 160 tok/s).&lt;/p&gt; &lt;p&gt;Same story as context scaling â€” the 3.5 holds steady. The 30B is always faster in absolute terms, but loses more ground as the conversation grows.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Section 7: Thinking Mode&lt;/h2&gt; &lt;p&gt;Server restarted with --reasoning-budget -1 (unlimited thinking). The llama.cpp API returns thinking in a reasoning_content field, final answer in content.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="right"&gt;30B think wds&lt;/th&gt; &lt;th align="right"&gt;30B answer wds&lt;/th&gt; &lt;th align="right"&gt;3.5 think wds&lt;/th&gt; &lt;th align="right"&gt;3.5 answer wds&lt;/th&gt; &lt;th align="right"&gt;30B tok/s&lt;/th&gt; &lt;th align="right"&gt;3.5 tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Sheep riddle&lt;/td&gt; &lt;td align="right"&gt;585&lt;/td&gt; &lt;td align="right"&gt;94&lt;/td&gt; &lt;td align="right"&gt;223&lt;/td&gt; &lt;td align="right"&gt;16&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;229.5&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;95.6&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Bearing capacity calc&lt;/td&gt; &lt;td align="right"&gt;2,100&lt;/td&gt; &lt;td align="right"&gt;0*&lt;/td&gt; &lt;td align="right"&gt;1,240&lt;/td&gt; &lt;td align="right"&gt;236&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;222.8&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;161.4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Logic puzzle (boxes)&lt;/td&gt; &lt;td align="right"&gt;943&lt;/td&gt; &lt;td align="right"&gt;315&lt;/td&gt; &lt;td align="right"&gt;691&lt;/td&gt; &lt;td align="right"&gt;153&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;226.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;161.2&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;USCS classification&lt;/td&gt; &lt;td align="right"&gt;1,949&lt;/td&gt; &lt;td align="right"&gt;0*&lt;/td&gt; &lt;td align="right"&gt;1,563&lt;/td&gt; &lt;td align="right"&gt;0*&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;221.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;160.7&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;*Hit the 3,000 token limit while still thinking â€” no answer generated.&lt;/p&gt; &lt;p&gt;Key observations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The 30B thinks at full speed&lt;/strong&gt; â€” 222-230 tok/s during thinking, same as regular generation. Thinking is basically free in terms of throughput.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The 3.5 takes a thinking speed hit&lt;/strong&gt; â€” 95-161 tok/s vs its normal 160 tok/s. On the sheep riddle it drops to 95 tok/s.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The 3.5 is more concise in thinking&lt;/strong&gt; â€” 223 words vs 585 for the sheep riddle, 1,240 vs 2,100 for bearing capacity. It thinks less but reaches the answer more efficiently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The 3.5 reaches the answer more often&lt;/strong&gt; â€” on the bearing capacity problem, the 3.5 produced 236 answer words within the token budget while the 30B burned all 3,000 tokens on thinking alone.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both models correctly answer the sheep riddle (9) and logic puzzle. Both correctly apply Terzaghi's equation when they get to the answer.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Summary Table&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="right"&gt;Qwen3-30B-A3B&lt;/th&gt; &lt;th align="right"&gt;Qwen3.5-35B-A3B&lt;/th&gt; &lt;th align="left"&gt;Winner&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Generation tok/s&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;235.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;159.0&lt;/td&gt; &lt;td align="left"&gt;30B (+48%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt processing tok/s&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;953.7&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;649.0&lt;/td&gt; &lt;td align="left"&gt;30B (+47%)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;TTFT (avg)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;100.5 ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;119.2 ms&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;VRAM (idle)&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;27.3 GB&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;29.0 GB&lt;/td&gt; &lt;td align="left"&gt;30B (-1.7 GB)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Context scaling (512-&amp;gt;8K)&lt;/td&gt; &lt;td align="right"&gt;-21.5%&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;-0.9%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Multi-turn degradation&lt;/td&gt; &lt;td align="right"&gt;-7.9%&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;-0.6%&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAG accuracy&lt;/td&gt; &lt;td align="right"&gt;6/6&lt;/td&gt; &lt;td align="right"&gt;6/6&lt;/td&gt; &lt;td align="left"&gt;Tie&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;JSON accuracy&lt;/td&gt; &lt;td align="right"&gt;4/4&lt;/td&gt; &lt;td align="right"&gt;4/4&lt;/td&gt; &lt;td align="left"&gt;Tie&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Thinking efficiency&lt;/td&gt; &lt;td align="right"&gt;Verbose&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;Concise&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3.5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Thinking speed&lt;/td&gt; &lt;td align="right"&gt;&lt;strong&gt;225 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="right"&gt;145 tok/s&lt;/td&gt; &lt;td align="left"&gt;30B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Quality&lt;/td&gt; &lt;td align="right"&gt;Good&lt;/td&gt; &lt;td align="right"&gt;Slightly better&lt;/td&gt; &lt;td align="left"&gt;3.5 (marginal)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Verdict&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;For raw speed and short interactions&lt;/strong&gt;: Stick with the 30B. It's 48% faster and the quality difference is negligible for quick queries.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For long conversations, big context windows, or RAG-heavy workloads&lt;/strong&gt;: The 3.5 has a real architectural advantage. Its flat context scaling curve means it'll hold 160 tok/s at 8K context while the 30B drops to 187 tok/s â€” and that gap likely widens further at 16K+.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For thinking/reasoning tasks&lt;/strong&gt;: It's a tradeoff. The 30B thinks faster but burns more tokens on verbose reasoning. The 3.5 thinks more concisely and reaches the answer within budget more reliably, but at lower throughput.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My plan&lt;/strong&gt;: Keeping the 30B as my daily driver for now. The speed advantage matters for interactive use. But I'll be watching the 3.5 closely â€” once llama.cpp optimizations land for the new architecture, that context scaling advantage could be a killer feature.&lt;/p&gt; &lt;p&gt;Also worth noting: the 3.5 ships with a vision projector (mmproj-BF16.gguf) â€” the A3B architecture now supports multimodal. Didn't benchmark it here but it's there.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;em&gt;Benchmark script, raw results JSONs, and full response texts available on request. All tests automated â€” zero cherry-picking.&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/3spky5u-oss"&gt; /u/3spky5u-oss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re3l3r/qwen330ba3b_vs_qwen3535ba3b_on_rtx_5090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re3l3r/qwen330ba3b_vs_qwen3535ba3b_on_rtx_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re3l3r/qwen330ba3b_vs_qwen3535ba3b_on_rtx_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T04:39:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1reemt6</id>
    <title>LLM Architectures of 10 Open-Weight Model Releases in Spring 2026</title>
    <updated>2026-02-25T14:26:29+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reemt6/llm_architectures_of_10_openweight_model_releases/"&gt; &lt;img alt="LLM Architectures of 10 Open-Weight Model Releases in Spring 2026" src="https://external-preview.redd.it/gjYGra9uiPqNhq19miCEGfawOof-NQvHX3DFMod_FxY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=514bf4bc79b719264d88bffb4439e93bd77a5710" title="LLM Architectures of 10 Open-Weight Model Releases in Spring 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://magazine.sebastianraschka.com/p/a-dream-of-spring-for-open-weight"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reemt6/llm_architectures_of_10_openweight_model_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reemt6/llm_architectures_of_10_openweight_model_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T14:26:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1re894z</id>
    <title>The FIRST local vision model to get this right!</title>
    <updated>2026-02-25T09:02:26+00:00</updated>
    <author>
      <name>/u/po_stulate</name>
      <uri>https://old.reddit.com/user/po_stulate</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re894z/the_first_local_vision_model_to_get_this_right/"&gt; &lt;img alt="The FIRST local vision model to get this right!" src="https://preview.redd.it/lz1m6nnetllg1.png?width=140&amp;amp;height=95&amp;amp;auto=webp&amp;amp;s=76a479a04722956d7dce2d4e7bf120f210e8e04b" title="The FIRST local vision model to get this right!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I decided to give qwen3.5-35b-a3b a try on this once very popular question in this sub. I've tried literally every popular local vision models in the past including bigger ones like glm-4.6v (106B) and qwen3-vl-235b-a22b and none of them got it even remotely correct. So I was thinking after it failed I will try qwen3.5-122b-a10b on this and hopefully it can get it after a few tries.&lt;/p&gt; &lt;p&gt;And to my surprise, 35b-a3b got it the first try! It came to the correct answer multiple times in the thinking process using different methods but didn't believe itself that 102 is the correct answer. After like the 5th time it calculated 102, it quoted &amp;quot;Not drawn accurately&amp;quot; and decided that it's probably actually the correct answer. Took over 30k thinking tokens for this.&lt;/p&gt; &lt;p&gt;I'm so amazed my these new qwen3.5 models, gonna test 122b on this now.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/po_stulate"&gt; /u/po_stulate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1re894z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re894z/the_first_local_vision_model_to_get_this_right/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re894z/the_first_local_vision_model_to_get_this_right/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T09:02:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1renq5y</id>
    <title>Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090</title>
    <updated>2026-02-25T19:47:47+00:00</updated>
    <author>
      <name>/u/jaigouk</name>
      <uri>https://old.reddit.com/user/jaigouk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"&gt; &lt;img alt="Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090" src="https://preview.redd.it/4k6v6oaf2plg1.png?width=140&amp;amp;height=84&amp;amp;auto=webp&amp;amp;s=0731983ae56b9cb55abd8cab74a5bcbbd706b15c" title="Qwen3.5 Model Comparison: 27B vs 35B on RTX 4090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to check qwen3.5 models that can be run on my GPU. So I compared 3 GGUF options.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; RTX 4090 (24GB VRAM)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Test:&lt;/strong&gt; Multi-agent Tetris development (Planner â†’ Developer â†’ QA)&lt;/p&gt; &lt;h1&gt;Models Under Test&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Preset&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Port&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;th align="left"&gt;Parallel&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen35-27b-multi&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;7082&lt;/td&gt; &lt;td align="left"&gt;17 GB&lt;/td&gt; &lt;td align="left"&gt;3 slots&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen35-35b-q3-multi&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Q3_K_XL&lt;/td&gt; &lt;td align="left"&gt;7081&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;3 slots&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;qwen35-35b-multi&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;7080&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;3 slots&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Architecture comparison:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;27B&lt;/strong&gt;: Dense model, 27B total / 27B active params&lt;/li&gt; &lt;li&gt;&lt;strong&gt;35B&lt;/strong&gt;: Sparse MoE, 35B total / 3B active params&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Charts&lt;/h1&gt; &lt;h1&gt;Total Time Comparison&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4k6v6oaf2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1387a394caa912a388f96eae8e8405a020a298"&gt;https://preview.redd.it/4k6v6oaf2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fc1387a394caa912a388f96eae8e8405a020a298&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Phase Breakdown&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/763vc0vi2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a4fb7acd8c22a8ba97a5c40cf1596c569dfeb4cb"&gt;https://preview.redd.it/763vc0vi2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a4fb7acd8c22a8ba97a5c40cf1596c569dfeb4cb&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;VRAM Efficiency&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6lpoqssk2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d4de5cb2326247fc7b0b321d64955ffbf627fe7"&gt;https://preview.redd.it/6lpoqssk2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2d4de5cb2326247fc7b0b321d64955ffbf627fe7&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Code Output Comparison&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/31c5ptpm2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3564dd47cc5a0a98ce8a4afcaac240f00b94d438"&gt;https://preview.redd.it/31c5ptpm2plg1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3564dd47cc5a0a98ce8a4afcaac240f00b94d438&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;th align="left"&gt;Total Time&lt;/th&gt; &lt;th align="left"&gt;Plan&lt;/th&gt; &lt;th align="left"&gt;Dev&lt;/th&gt; &lt;th align="left"&gt;QA&lt;/th&gt; &lt;th align="left"&gt;Lines&lt;/th&gt; &lt;th align="left"&gt;Valid&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-27B Q4&lt;/td&gt; &lt;td align="left"&gt;17 GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;134.0s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;36.3s&lt;/td&gt; &lt;td align="left"&gt;72.1s&lt;/td&gt; &lt;td align="left"&gt;25.6s&lt;/td&gt; &lt;td align="left"&gt;312&lt;/td&gt; &lt;td align="left"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen3.5-35B Q3&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;34.8s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;7.3s&lt;/td&gt; &lt;td align="left"&gt;20.1s&lt;/td&gt; &lt;td align="left"&gt;7.5s&lt;/td&gt; &lt;td align="left"&gt;322&lt;/td&gt; &lt;td align="left"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3.5-35B Q4&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;37.8s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;8.2s&lt;/td&gt; &lt;td align="left"&gt;22.0s&lt;/td&gt; &lt;td align="left"&gt;7.6s&lt;/td&gt; &lt;td align="left"&gt;311&lt;/td&gt; &lt;td align="left"&gt;YES&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Key Findings&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;35B models are dramatically faster than 27B&lt;/strong&gt; â€” 35s vs 134s (3.8x faster!)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;35B Q3 is fastest overall&lt;/strong&gt; â€” 34.8s total, uses only 16GB VRAM&lt;/li&gt; &lt;li&gt;&lt;strong&gt;35B Q4 slightly slower than Q3&lt;/strong&gt; â€” 37.8s vs 34.8s (8% slower, 4GB more VRAM)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;27B is surprisingly slow&lt;/strong&gt; â€” Dense architecture less efficient than sparse MoE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;All models produced valid, runnable code&lt;/strong&gt; â€” 311-322 lines each&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Speed Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Phase&lt;/th&gt; &lt;th align="left"&gt;27B Q4&lt;/th&gt; &lt;th align="left"&gt;35B Q3&lt;/th&gt; &lt;th align="left"&gt;35B Q4&lt;/th&gt; &lt;th align="left"&gt;35B Q3 vs 27B&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Planning&lt;/td&gt; &lt;td align="left"&gt;36.3s&lt;/td&gt; &lt;td align="left"&gt;7.3s&lt;/td&gt; &lt;td align="left"&gt;8.2s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;5.0x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Development&lt;/td&gt; &lt;td align="left"&gt;72.1s&lt;/td&gt; &lt;td align="left"&gt;20.1s&lt;/td&gt; &lt;td align="left"&gt;22.0s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.6x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;QA Review&lt;/td&gt; &lt;td align="left"&gt;25.6s&lt;/td&gt; &lt;td align="left"&gt;7.5s&lt;/td&gt; &lt;td align="left"&gt;7.6s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.4x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;134.0s&lt;/td&gt; &lt;td align="left"&gt;34.8s&lt;/td&gt; &lt;td align="left"&gt;37.8s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3.8x faster&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;VRAM Efficiency&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;th align="left"&gt;Time&lt;/th&gt; &lt;th align="left"&gt;VRAM Efficiency&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;35B Q3&lt;/td&gt; &lt;td align="left"&gt;16 GB&lt;/td&gt; &lt;td align="left"&gt;34.8s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;Best&lt;/strong&gt; (fastest, lowest VRAM)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;27B Q4&lt;/td&gt; &lt;td align="left"&gt;17 GB&lt;/td&gt; &lt;td align="left"&gt;134.0s&lt;/td&gt; &lt;td align="left"&gt;Worst (slow, mid VRAM)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;35B Q4&lt;/td&gt; &lt;td align="left"&gt;20 GB&lt;/td&gt; &lt;td align="left"&gt;37.8s&lt;/td&gt; &lt;td align="left"&gt;Good (fast, highest VRAM)&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Generated Code &amp;amp; QA Analysis&lt;/h1&gt; &lt;p&gt;All three models produced functional Tetris games with similar structure:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Lines&lt;/th&gt; &lt;th align="left"&gt;Chars&lt;/th&gt; &lt;th align="left"&gt;Syntax&lt;/th&gt; &lt;th align="left"&gt;QA Verdict&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;27B Q4&lt;/td&gt; &lt;td align="left"&gt;312&lt;/td&gt; &lt;td align="left"&gt;11,279&lt;/td&gt; &lt;td align="left"&gt;VALID&lt;/td&gt; &lt;td align="left"&gt;Issues noted&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;35B Q3&lt;/td&gt; &lt;td align="left"&gt;322&lt;/td&gt; &lt;td align="left"&gt;11,260&lt;/td&gt; &lt;td align="left"&gt;VALID&lt;/td&gt; &lt;td align="left"&gt;Issues noted&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;35B Q4&lt;/td&gt; &lt;td align="left"&gt;311&lt;/td&gt; &lt;td align="left"&gt;10,260&lt;/td&gt; &lt;td align="left"&gt;VALID&lt;/td&gt; &lt;td align="left"&gt;Issues noted&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;QA Review Summary&lt;/h1&gt; &lt;p&gt;All three QA agents identified similar potential issues in the generated code:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Common observations across models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Collision detection edge cases (pieces near board edges)&lt;/li&gt; &lt;li&gt;Rotation wall-kick not fully implemented&lt;/li&gt; &lt;li&gt;Score calculation could have edge cases with &amp;gt;4 lines&lt;/li&gt; &lt;li&gt;Game over detection timing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Verdict:&lt;/strong&gt; All three games compile and run correctly. The QA agents were thorough in identifying &lt;em&gt;potential&lt;/em&gt; edge cases, but the core gameplay functions properly. The issues noted are improvements rather than bugs blocking playability.&lt;/p&gt; &lt;h1&gt;Code Quality Comparison&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Aspect&lt;/th&gt; &lt;th align="left"&gt;27B Q4&lt;/th&gt; &lt;th align="left"&gt;35B Q3&lt;/th&gt; &lt;th align="left"&gt;35B Q4&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Class structure&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;td align="left"&gt;Good&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;All 7 pieces&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Rotation states&lt;/td&gt; &lt;td align="left"&gt;4 each&lt;/td&gt; &lt;td align="left"&gt;4 each&lt;/td&gt; &lt;td align="left"&gt;4 each&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Line clearing&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Scoring&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Game over&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Controls help&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;td align="left"&gt;Yes&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;All three models produced structurally similar, fully-featured implementations.&lt;/p&gt; &lt;h1&gt;Recommendation&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Qwen3.5-35B Q3_K_XL as the daily driver.&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3.8x faster than Qwen3.5-27B&lt;/li&gt; &lt;li&gt;Uses less VRAM (16GB vs 17GB)&lt;/li&gt; &lt;li&gt;Produces equivalent quality code&lt;/li&gt; &lt;li&gt;Best VRAM efficiency of all tested models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full benchmark with generated code: &lt;a href="https://jaigouk.com/gpumod/benchmarks/20260225_qwen35_comparison/"&gt;https://jaigouk.com/gpumod/benchmarks/20260225_qwen35_comparison/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jaigouk"&gt; /u/jaigouk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1renq5y/qwen35_model_comparison_27b_vs_35b_on_rtx_4090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T19:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1rechcr</id>
    <title>Qwen just published the vision language benchmarks of qwen3.5 medium and I have compared Qwen3.5-35b-a3b with Qwen3-VL-235b-a22b, They actually perform close to each other which is insane!</title>
    <updated>2026-02-25T12:57:00+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rechcr/qwen_just_published_the_vision_language/"&gt; &lt;img alt="Qwen just published the vision language benchmarks of qwen3.5 medium and I have compared Qwen3.5-35b-a3b with Qwen3-VL-235b-a22b, They actually perform close to each other which is insane!" src="https://preview.redd.it/5yfl6ics1nlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0be51e6ebac5d994265103624a292df8b2510163" title="Qwen just published the vision language benchmarks of qwen3.5 medium and I have compared Qwen3.5-35b-a3b with Qwen3-VL-235b-a22b, They actually perform close to each other which is insane!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5yfl6ics1nlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rechcr/qwen_just_published_the_vision_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rechcr/qwen_just_published_the_vision_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T12:57:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1remcej</id>
    <title>Anthropic Drops Flagship Safety Pledge</title>
    <updated>2026-02-25T18:59:11+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1remcej/anthropic_drops_flagship_safety_pledge/"&gt; &lt;img alt="Anthropic Drops Flagship Safety Pledge" src="https://external-preview.redd.it/PTr_0OK3p9e9gnNDPqpmy0xkmssi7-vtV1HQVArmozc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=72d457d9874072e6e8ff3a231754e7000271be9e" title="Anthropic Drops Flagship Safety Pledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://time.com/7380854/exclusive-anthropic-drops-flagship-safety-pledge/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1remcej/anthropic_drops_flagship_safety_pledge/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1remcej/anthropic_drops_flagship_safety_pledge/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T18:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1regq10</id>
    <title>Qwen 3.5 27-35-122B - Jinja Template Modification (Based on Bartowski's Jinja) - No thinking by default - straight quick answers, need thinking? simple activation with "/think" command anywhere in the system prompt.</title>
    <updated>2026-02-25T15:44:52+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1regq10/qwen_35_2735122b_jinja_template_modification/"&gt; &lt;img alt="Qwen 3.5 27-35-122B - Jinja Template Modification (Based on Bartowski's Jinja) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." src="https://preview.redd.it/jmql2sb6vnlg1.png?width=140&amp;amp;height=76&amp;amp;auto=webp&amp;amp;s=cb531eea03e0b4d0b404092bc8cee85cd83648ce" title="Qwen 3.5 27-35-122B - Jinja Template Modification (Based on Bartowski's Jinja) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kinda didn't like how Qwen 3.5 thinking activation / deactivation work.&lt;br /&gt; For me the best solution is OFF by default and activated when needed.&lt;/p&gt; &lt;p&gt;This small mod is based on &lt;a href="https://huggingface.co/bartowski"&gt;Bartowski&lt;/a&gt;'s Jinja template: Qwen 3.5 model will answer without any thinking by default, but if you add &amp;quot;/think&amp;quot; tag anywhere in system prompt, model with start thinking as usual, quick and simple solution for llama.cpp, LM Studio etc.&lt;/p&gt; &lt;p&gt;For llama.cpp: `--chat-template-file D:\QWEN3.5.MOD.jinja`&lt;br /&gt; For LM Studio: Just paste this template as shown on screenshot 3, into &amp;quot;Template (Jinja)&amp;quot; section.&lt;/p&gt; &lt;p&gt;Link to Template - &lt;a href="https://pastebin.com/vPDSY9b8"&gt;https://pastebin.com/vPDSY9b8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1regq10"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1regq10/qwen_35_2735122b_jinja_template_modification/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1regq10/qwen_35_2735122b_jinja_template_modification/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T15:44:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1rehykx</id>
    <title>Qwen3.5 "Low Reasoning Effort" trick in llama-server</title>
    <updated>2026-02-25T16:28:28+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With a logit bias adjustment for the &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; token and a grammar to defend against the bias forcing additional &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tokens into the response, you can effectively adjust the average length of reasoning.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;curl -sS http://127.0.0.1:8083/v1/chat/completions \ -H 'content-type: application/json' \ -d '{ &amp;quot;model&amp;quot;: &amp;quot;qwen3.5-35b-a3b&amp;quot;, &amp;quot;stream&amp;quot;: false, &amp;quot;logit_bias&amp;quot;: { &amp;quot;248069&amp;quot;: 11.8 }, &amp;quot;grammar&amp;quot;: &amp;quot;root ::= pre &amp;lt;[248069]&amp;gt; post\npre ::= !&amp;lt;[248069]&amp;gt;*\npost ::= !&amp;lt;[248069]&amp;gt;*&amp;quot;, &amp;quot;messages&amp;quot;: [ { &amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;hello world&amp;quot; } ] }' &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A few logit biases to consider:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;code&gt;11.8&lt;/code&gt; is a nice balance that favors reasoning when it is helpful, while often skipping or short circuiting reasoning for easy prompts.&lt;/li&gt; &lt;li&gt;&lt;code&gt;12.5&lt;/code&gt; more strongly favors less reasoning.&lt;/li&gt; &lt;li&gt;&lt;code&gt;13.3&lt;/code&gt; essentially disables reasoning.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You can try any value you want, of course.&lt;/p&gt; &lt;p&gt;Even 11.8 is obviously going to cause the model to be less intelligent, but probably still smarter than disabling thinking entirely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rehykx/qwen35_low_reasoning_effort_trick_in_llamaserver/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rehykx/qwen35_low_reasoning_effort_trick_in_llamaserver/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rehykx/qwen35_low_reasoning_effort_trick_in_llamaserver/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T16:28:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1red6sv</id>
    <title>update your llama.cpp for Qwen 3.5</title>
    <updated>2026-02-25T13:27:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 3.5 27B multi-GPU crash fix&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19866"&gt;https://github.com/ggml-org/llama.cpp/pull/19866&lt;/a&gt;&lt;/p&gt; &lt;p&gt;prompt caching on multi-modal models&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19849"&gt;https://github.com/ggml-org/llama.cpp/pull/19849&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19877"&gt;https://github.com/ggml-org/llama.cpp/pull/19877&lt;/a&gt;&lt;/p&gt; &lt;p&gt;for the reference, If you think your GPU is too small, compare it with my results on potato (12GB VRAM) Windows:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;PS C:\Users\jacek\git\llama.cpp&amp;gt; .\2026.02.25\bin\Release\llama-bench.exe -fa 1 -m J:\llm\models\Qwen3.5-35B-A3B-Q4_K_M.gguf --n-cpu-moe 21,22,23 ggml_cuda_init: found 1 CUDA devices: Device 0: NVIDIA GeForce RTX 5070, compute capability 12.0, VMM: yes | model | size | params | backend | ngl | n_cpu_moe | fa | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | -: | --------------: | -------------------: | | qwen35moe ?B Q4_K - Medium | 19.74 GiB | 34.66 B | CUDA | 99 | 21 | 1 | pp512 | 1453.20 + 6.78 | | qwen35moe ?B Q4_K - Medium | 19.74 GiB | 34.66 B | CUDA | 99 | 21 | 1 | tg128 | 62.33 + 0.31 | | qwen35moe ?B Q4_K - Medium | 19.74 GiB | 34.66 B | CUDA | 99 | 22 | 1 | pp512 | 1438.74 + 20.48 | | qwen35moe ?B Q4_K - Medium | 19.74 GiB | 34.66 B | CUDA | 99 | 22 | 1 | tg128 | 61.39 + 0.28 | | qwen35moe ?B Q4_K - Medium | 19.74 GiB | 34.66 B | CUDA | 99 | 23 | 1 | pp512 | 1410.17 + 11.95 | | qwen35moe ?B Q4_K - Medium | 19.74 GiB | 34.66 B | CUDA | 99 | 23 | 1 | tg128 | 61.94 + 0.20 | build: f20469d91 (8153) &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1red6sv/update_your_llamacpp_for_qwen_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1red6sv/update_your_llamacpp_for_qwen_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1red6sv/update_your_llamacpp_for_qwen_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T13:27:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1rdxfdu</id>
    <title>Qwen3.5-35B-A3B is a gamechanger for agentic coding.</title>
    <updated>2026-02-25T00:04:44+00:00</updated>
    <author>
      <name>/u/jslominski</name>
      <uri>https://old.reddit.com/user/jslominski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"&gt; &lt;img alt="Qwen3.5-35B-A3B is a gamechanger for agentic coding." src="https://external-preview.redd.it/PKLapkmPqiug1xgPt5ocaUvPuWOhjG0WSxGrmcOrq3A.png?width=140&amp;amp;height=73&amp;amp;auto=webp&amp;amp;s=50ab068aed10118ee05f1a934121fbe2a05fb971" title="Qwen3.5-35B-A3B is a gamechanger for agentic coding." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/m4v951sv5jlg1.jpg?width=2367&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bec61ca20f08bb766987147287c7d6664308fa2f"&gt;Qwen3.5-35B-A3B with Opencode&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just tested this badboy with Opencode &lt;strong&gt;cause frankly I couldn't believe those benchmarks.&lt;/strong&gt; Running it on a single RTX 3090 on a headless Linux box. Freshly compiled Llama.cpp and those are my settings after some tweaking, still not fully tuned: &lt;/p&gt; &lt;p&gt;./llama.cpp/llama-server \&lt;/p&gt; &lt;p&gt;-m /models/&lt;strong&gt;Qwen3.5-35B-A3B-MXFP4_MOE.gguf&lt;/strong&gt; \&lt;/p&gt; &lt;p&gt;-a &amp;quot;DrQwen&amp;quot; \&lt;/p&gt; &lt;p&gt;-c 131072 \&lt;/p&gt; &lt;p&gt;-ngl all \&lt;/p&gt; &lt;p&gt;-ctk q8_0 \&lt;/p&gt; &lt;p&gt;-ctv q8_0 \&lt;/p&gt; &lt;p&gt;-sm none \&lt;/p&gt; &lt;p&gt;-mg 0 \&lt;/p&gt; &lt;p&gt;-np 1 \&lt;/p&gt; &lt;p&gt;-fa on&lt;/p&gt; &lt;p&gt;Around 22 gigs of vram used.&lt;/p&gt; &lt;p&gt;Now the fun part:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;I'm getting over 100t/s on it&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;This is the first open weights model I was able to utilise on my home hardware to successfully complete my own &amp;quot;coding test&amp;quot; I used for years for recruitment (mid lvl mobile dev, around 5h to complete &amp;quot;pre AI&amp;quot; ;)). It did it in around 10 minutes, strong pass. First agentic tool that I was able to &amp;quot;crack&amp;quot; it with was &lt;a href="http://Kodu.AI"&gt;Kodu.AI&lt;/a&gt; with some early sonnet roughly 14 months ago.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For fun I wanted to recreate this dashboard OpenAI used during Cursor demo last summer, I did a recreation of it with Claude Code back then and posted it on Reddit: &lt;a href="https://www.reddit.com/r/ClaudeAI/comments/1mk7plb/just_recreated_that_gpt5_cursor_demo_in_claude/"&gt;https://www.reddit.com/r/ClaudeAI/comments/1mk7plb/just_recreated_that_gpt5_cursor_demo_in_claude/&lt;/a&gt; So... Qwen3.5 was able to do it in around 5 minutes. &lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;I think we got something special here...&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jslominski"&gt; /u/jslominski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rdxfdu/qwen3535ba3b_is_a_gamechanger_for_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T00:04:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rei65v</id>
    <title>Qwen3.5-35B-A3B quantization quality + speed benchmarks on RTX 5080 16GB (Q8_0 vs Q4_K_M vs UD-Q4_K_XL)</title>
    <updated>2026-02-25T16:35:49+00:00</updated>
    <author>
      <name>/u/gaztrab</name>
      <uri>https://old.reddit.com/user/gaztrab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ran some benchmarks on Qwen3.5-35B-A3B with llama.cpp on a single-GPU consumer workstation. Model doesn't fit in VRAM so this is a CPU/GPU offloading setup over PCIe 5.0.&lt;/p&gt; &lt;h1&gt;System Specs&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Component&lt;/th&gt; &lt;th align="left"&gt;Spec&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU&lt;/td&gt; &lt;td align="left"&gt;NVIDIA GeForce RTX 5080 16GB GDDR7 (Blackwell, sm_120, 960 GB/s bandwidth)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CPU&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen 9 9950X (32 threads)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;RAM&lt;/td&gt; &lt;td align="left"&gt;128 GB DDR5-4800 (dual channel, ~77 GB/s)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PCIe&lt;/td&gt; &lt;td align="left"&gt;5.0 x16 (~64 GB/s bidirectional)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;OS&lt;/td&gt; &lt;td align="left"&gt;Ubuntu 24.04.3 LTS, kernel 6.17.0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;CUDA&lt;/td&gt; &lt;td align="left"&gt;13.1, driver 590.48.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp&lt;/td&gt; &lt;td align="left"&gt;b1-9051663 (main benchmarks), b1-a96a112 (for --fit on tests). Built with -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=120 -DGGML_CUDA_FA_ALL_QUANTS=ON&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Quantization Quality (WikiText-2 Perplexity)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;PPL&lt;/th&gt; &lt;th align="left"&gt;vs Q8_0&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;36.9 GB&lt;/td&gt; &lt;td align="left"&gt;6.5342&lt;/td&gt; &lt;td align="left"&gt;baseline&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;~20 GB&lt;/td&gt; &lt;td align="left"&gt;6.6688&lt;/td&gt; &lt;td align="left"&gt;+2.1%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;~19 GB&lt;/td&gt; &lt;td align="left"&gt;7.1702&lt;/td&gt; &lt;td align="left"&gt;+9.7%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;UD-Q4_K_XL is significantly worse than standard Q4_K_M on this model&lt;/strong&gt; â€” both larger file size and nearly 10% higher perplexity. This is consistent with other reports of Unsloth Dynamic quants underperforming on MoE architectures (&lt;a href="/u/ubergarm"&gt;u/ubergarm&lt;/a&gt;'s KLD data on Qwen3-30B-A3B showed the same pattern). &lt;strong&gt;If you're running Qwen3.5-35B-A3B at Q4, use standard Q4_K_M.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Speed Benchmarks&lt;/h1&gt; &lt;p&gt;All configs: 20 threads, 65K context, flash attention, &lt;code&gt;--no-mmap&lt;/code&gt;, KV cache q8_0, llama.cpp built from source.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Config&lt;/th&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Strategy&lt;/th&gt; &lt;th align="left"&gt;tok/s (short)&lt;/th&gt; &lt;th align="left"&gt;tok/s (medium)&lt;/th&gt; &lt;th align="left"&gt;tok/s (long)&lt;/th&gt; &lt;th align="left"&gt;VRAM&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Full offload&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-ot &amp;quot;exps=CPU&amp;quot;&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;35.7&lt;/td&gt; &lt;td align="left"&gt;32.8&lt;/td&gt; &lt;td align="left"&gt;33.2&lt;/td&gt; &lt;td align="left"&gt;8064 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Auto-fit&lt;/td&gt; &lt;td align="left"&gt;Q8_0&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;--fit on (b8149)&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;40.5&lt;/td&gt; &lt;td align="left"&gt;40.3&lt;/td&gt; &lt;td align="left"&gt;39.6&lt;/td&gt; &lt;td align="left"&gt;14660 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Full offload&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;-ot &amp;quot;exps=CPU&amp;quot;&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;51.0&lt;/td&gt; &lt;td align="left"&gt;49.8&lt;/td&gt; &lt;td align="left"&gt;49.4&lt;/td&gt; &lt;td align="left"&gt;7217 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Partial offload&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;--n-cpu-moe 24&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;69.6&lt;/td&gt; &lt;td align="left"&gt;67.0&lt;/td&gt; &lt;td align="left"&gt;65.7&lt;/td&gt; &lt;td align="left"&gt;14874 MB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Auto-fit&lt;/td&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;--fit on&lt;/code&gt;&lt;/td&gt; &lt;td align="left"&gt;67.4&lt;/td&gt; &lt;td align="left"&gt;62.3&lt;/td&gt; &lt;td align="left"&gt;64.1&lt;/td&gt; &lt;td align="left"&gt;14551 MB&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;em&gt;Note: The&lt;/em&gt; &lt;strong&gt;&lt;em&gt;--fit&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;on configs (auto-fit rows) were tested on a newer llama.cpp build (&lt;/em&gt;&lt;strong&gt;&lt;em&gt;a96a112&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;) since the older build didn't support the flag. All other configs used build&lt;/em&gt; &lt;strong&gt;&lt;em&gt;9051663&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Each workload ran 5 times (first discarded as warmup). Standard deviations were generally &amp;lt; 1 tok/s except for configs close to VRAM limits.&lt;/p&gt; &lt;h1&gt;Key Takeaways&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Best config for 16GB VRAM:&lt;/strong&gt; Q4_K_M with &lt;code&gt;--n-cpu-moe 24&lt;/code&gt; (keeps 16/40 MoE layers on GPU, offloads 24 to CPU). ~70 tok/s with only 2.1% PPL loss vs Q8_0.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KV cache q8_0 is a free lunch:&lt;/strong&gt; Compared to f16 KV cache, q8_0 gives +12-38% throughput AND uses less VRAM. No reason not to use &lt;code&gt;-ctk q8_0 -ctv q8_0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--fit on works but manual tuning beats it:&lt;/strong&gt; The new auto-fit flag in b8149 is convenient and gets you ~90-95% of the way there, but hand-tuning &lt;code&gt;--n-cpu-moe&lt;/code&gt; gets another 7% on top.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;--n-cpu-moe sweet spot matters:&lt;/strong&gt; For Q4_K_M on 16GB, &lt;code&gt;--n-cpu-moe 16&lt;/code&gt; OOMs and &lt;code&gt;--n-cpu-moe 32&lt;/code&gt; is too conservative. 24 is the sweet spot. For Q8_0, even &lt;code&gt;--n-cpu-moe 32&lt;/code&gt; barely fits.&lt;/p&gt; &lt;h1&gt;Launch Command&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;./llama-server \ -m ./Qwen3.5-35B-A3B-Q4_K_M.gguf \ -c 65536 \ -ngl 999 \ --n-cpu-moe 24 \ -fa on \ -t 20 \ -b 4096 \ -ub 4096 \ --no-mmap \ --jinja \ -ctk q8_0 \ -ctv q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Happy to answer questions about the setup. Previous model was Qwen3-Next-80B-A3B at ~22 tok/s on the same hardware, so this is a 3.2x speedup with a much more capable model.Qwen3.5-35B-A3B Benchmarks on RTX 5080 16GB&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gaztrab"&gt; /u/gaztrab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rei65v/qwen3535ba3b_quantization_quality_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T16:35:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1re72h4</id>
    <title>Qwen3.5 27B better than 35B-A3B?</title>
    <updated>2026-02-25T07:49:05+00:00</updated>
    <author>
      <name>/u/-OpenSourcer</name>
      <uri>https://old.reddit.com/user/-OpenSourcer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re72h4/qwen35_27b_better_than_35ba3b/"&gt; &lt;img alt="Qwen3.5 27B better than 35B-A3B?" src="https://preview.redd.it/f9x0emmuillg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bee689086672602801cb1e88155d725c01342793" title="Qwen3.5 27B better than 35B-A3B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which model would be better with 16 GB of VRAM and 32 GB of RAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-OpenSourcer"&gt; /u/-OpenSourcer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f9x0emmuillg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re72h4/qwen35_27b_better_than_35ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re72h4/qwen35_27b_better_than_35ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:49:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1re6ifz</id>
    <title>Anthropic is the leading contributor to open weight models</title>
    <updated>2026-02-25T07:15:29+00:00</updated>
    <author>
      <name>/u/DealingWithIt202s</name>
      <uri>https://old.reddit.com/user/DealingWithIt202s</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It just happens to be entirely against their will and TOS. I say: Distill Baby Distill!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DealingWithIt202s"&gt; /u/DealingWithIt202s &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1re6ifz/anthropic_is_the_leading_contributor_to_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T07:15:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1refvmr</id>
    <title>Qwen 3 27b is... impressive</title>
    <updated>2026-02-25T15:13:40+00:00</updated>
    <author>
      <name>/u/-dysangel-</name>
      <uri>https://old.reddit.com/user/-dysangel-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"&gt; &lt;img alt="Qwen 3 27b is... impressive" src="https://preview.redd.it/5uje69y1pnlg1.gif?frame=1&amp;amp;width=140&amp;amp;height=115&amp;amp;auto=webp&amp;amp;s=756267272f48b42047d0e902abf95d86a1940bda" title="Qwen 3 27b is... impressive" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/5uje69y1pnlg1.gif"&gt;https://i.redd.it/5uje69y1pnlg1.gif&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;All Prompts&lt;/strong&gt;&lt;br /&gt; &amp;quot;Task: create a GTA-like 3D game where you can walk around, get in and drive cars&amp;quot;&lt;br /&gt; &amp;quot;walking forward and backward is working, but I cannot turn or strafe??&amp;quot;&lt;br /&gt; &amp;quot;this is pretty fun! Iâ€™m noticing that the camera is facing backward though, for both walking and car?&amp;quot;&lt;br /&gt; &amp;quot;yes, it works! What could we do to enhance the experience now?&amp;quot;&lt;br /&gt; &amp;quot;Iâ€™m not too fussed about a HUD, and the physics are not bad as they are already - adding building and obstacles definitely feels like the highest priority!&amp;quot; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-dysangel-"&gt; /u/-dysangel- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1refvmr/qwen_3_27b_is_impressive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T15:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1reds0p</id>
    <title>Qwen 3.5 craters on hard coding tasks â€” tested all Qwen3.5 models (And Codex 5.3) on 70 real repos so you don't have to.</title>
    <updated>2026-02-25T13:52:13+00:00</updated>
    <author>
      <name>/u/hauhau901</name>
      <uri>https://old.reddit.com/user/hauhau901</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reds0p/qwen_35_craters_on_hard_coding_tasks_tested_all/"&gt; &lt;img alt="Qwen 3.5 craters on hard coding tasks â€” tested all Qwen3.5 models (And Codex 5.3) on 70 real repos so you don't have to." src="https://preview.redd.it/5g4ostqlbnlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea4807a66237a7f8bf87e955618494b8fe058e3f" title="Qwen 3.5 craters on hard coding tasks â€” tested all Qwen3.5 models (And Codex 5.3) on 70 real repos so you don't have to." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, some of you might remember &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1r7shtv/i_built_a_benchmark_that_tests_coding_llms_on/&lt;/a&gt; where I shared APEX Testing â€” my benchmark that tests coding models on real codebases with real problems.&lt;/p&gt; &lt;p&gt;Since then I've added 5 more tasks (now 70 total), and more importantly tested a bunch of new models people were asking about: all the Qwen 3.5 variants, GPT-5.3 Codex, and several local quantized models running on LM Studio.&lt;/p&gt; &lt;p&gt;I also built a proper agentic tool-use system for the local models now â€” instead of dumping the entire repo into one prompt, models get all required tools and they explore + implement on their own, just like the cloud agentic models do. Way fairer comparison. Heavy anti-benchmaxxing focus is in place as well so GL to companies who try to take that approach and promise the moon and the stars :)&lt;/p&gt; &lt;p&gt;What caught me off guard:&lt;/p&gt; &lt;p&gt;- Codex 5.3 is basically tied with GPT-5.2 at #4 overall. barely drops across difficulty levels â€” super consistent from easy to master tasks -&amp;gt; &lt;strong&gt;Recommended&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;- Qwen 3.5 397B craters on master tasks. holds ~1550 ELO on hard/expert which is respectable, but drops to 1194 on master. when it needs to coordinate across many files over many steps, it just loses track of what it's doing&lt;/p&gt; &lt;p&gt;- GLM-4.7 quantized is still the local GOAT. 1572 ELO, beats every single Qwen 3.5 model including the full 397B cloud version. if you're picking one local model for coding, this is still it (better than GLM-5 even!)&lt;/p&gt; &lt;p&gt;- Qwen 3.5 27B is genuinely decent on a single GPU though. 1384 ELO, beats DeepSeek V3.2 and all the qwen3-coder models. for &amp;quot;fix this bug&amp;quot; / &amp;quot;add this endpoint&amp;quot; type work it holds up&lt;/p&gt; &lt;p&gt;- The 35B MoE (3B active) is rough. 1256, worse than the 27B dense on almost everything. the tiny active param count really shows on multi-step agentic work&lt;/p&gt; &lt;p&gt;- One qwen model found a loophole lol â€” qwen3.5-27b ran the test suite on a master task, saw existing tests passing, declared everything &amp;quot;already implemented&amp;quot; and quit without writing a single line of code. it was the only model out of 25+ that tried this. had to patch my system after that one ðŸ˜…&lt;/p&gt; &lt;p&gt;Still running: Qwen 3.5 122B only has 3/70 tasks done so take that ranking with a grain of salt. &lt;strong&gt;Also planning BF16 and Q8_K_XL runs&lt;/strong&gt; for the Qwen3.5 models to show the real quantization tax â€” should have those up in a day or two.&lt;/p&gt; &lt;p&gt;Methodology in brief: 70 tasks across real GitHub repos â€” bug fixes, refactors, from-scratch builds, debugging race conditions, building CLI tools, you name it. All models get the same starting point, agentic tool-use, scored on&lt;/p&gt; &lt;p&gt;Correctness/completeness/quality/efficiency, ELO calculated pairwise with difficulty adjustments. task titles are public on the site, prompts/diffs kept private to avoid contamination. solo project, self-funded ($3000 and counting lol).&lt;/p&gt; &lt;p&gt;Full leaderboard with filters by category, difficulty, per-model breakdowns, and individual run data:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.apex-testing.org"&gt;https://www.apex-testing.org&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions, and if you want a specific model tested let me know and I might add it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hauhau901"&gt; /u/hauhau901 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5g4ostqlbnlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1reds0p/qwen_35_craters_on_hard_coding_tasks_tested_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1reds0p/qwen_35_craters_on_hard_coding_tasks_tested_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-25T13:52:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
