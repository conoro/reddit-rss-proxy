<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-19T21:22:29+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oa29de</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.2.0</title>
    <updated>2025-10-18T17:47:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt; &lt;img alt="Drummer's Cydonia and Magidonia 24B v4.2.0" src="https://external-preview.redd.it/texRxv_iJ0Ni14pBUMNg-YEbpRERebh0ufaJ753mjSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab199da434abf76101631f6569f8bb80838d47f9" title="Drummer's Cydonia and Magidonia 24B v4.2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magidonia is Cydonia using Magistral 2509 base.&lt;/p&gt; &lt;p&gt;Magidonia variant: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cydonia (Small 3.2) variant: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4.2.0 is an upgrade from 4.1 in regards to creativity. Enjoy!&lt;/p&gt; &lt;p&gt;Does anyone have a base to recommend for finetuning? Waiting for GLM Air 4.6 to come out :^)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;By the way, Huggingface has restricted storage in my account and I'm having a harder time doing my open-source work for the community. I'll be all out of space after a few days of work thanks to their storage restriction. &lt;/p&gt; &lt;p&gt;I tried contacting them via [&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;) but they told me to make my case to [&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;) . I haven't received a response from &lt;em&gt;that&lt;/em&gt; team yet. Other employees I've reached out to recommended that I pay around $200 / mo to get the storage I need, I think.&lt;/p&gt; &lt;p&gt;At this point I believe they're not interested in giving me an exception. I got bundled up with those who upload 1T models, I guess? I'm not sure what to do next, but I might have to start deleting models. Let me know if you guys have any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaafyq</id>
    <title>3 3090's, room for one more?</title>
    <updated>2025-10-18T23:16:49+00:00</updated>
    <author>
      <name>/u/BusinessBookkeeper63</name>
      <uri>https://old.reddit.com/user/BusinessBookkeeper63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt; &lt;img alt="3 3090's, room for one more?" src="https://preview.redd.it/4wabpxvjcyvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcb88de7be3c5396eac5805691a1be8d3309133" title="3 3090's, room for one more?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently running 3 3090's and was thinking of adding one more. But as you can see, my case Thermaltake CTE750 Air has some free space, but not sure if it can fit another 3090.&lt;/p&gt; &lt;p&gt;I know, I know, I should have had a server rack but I was looking for a Local AI + relatively decent looking case, so this is what I landed on. The CTE 750 is big enough for 3 3090's, but not sure if I should be doing 4 given temps inside a closed case is probably going to rise quick. The third 3090 needs a custom mount and sits on the side of the case in this picture, but it rests on the intake fans and I have screwed the standing with 3 screws. I have no idea, where I could fit the 4th.&lt;/p&gt; &lt;p&gt;Any suggestions on how I could do 4 3090;s in this case or if anyone has done this before?&lt;/p&gt; &lt;p&gt;Also looking for suggestions on my cooling. Currently it has intake from bottom, front, back and sides and outtake on top only. This is somewhat based on the CTE design, but open to other suggestions. Another option, is to eventually do water cooling to save on some space and keep things cooler, but that's a project kept for December.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BusinessBookkeeper63"&gt; /u/BusinessBookkeeper63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4wabpxvjcyvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T23:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oayijj</id>
    <title>Turn any dataset into a reasoning dataset easily and cheaply</title>
    <updated>2025-10-19T19:22:43+00:00</updated>
    <author>
      <name>/u/ApprehensiveTart3158</name>
      <uri>https://old.reddit.com/user/ApprehensiveTart3158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tldr; this model is tiny but meant for recreating grounded reasoning generation without changing your datasets too much (scroll down for link)&lt;/p&gt; &lt;p&gt;I woke up one day and thought if it is possible to make an LLM (a tiny one, 0.6b!) turn those old but gold chat datasets into reasoning chat datasets, turns out yes it is possible and the results were quite good. &lt;/p&gt; &lt;p&gt;Which allows you to then fine tune a model on those same older but hq datasets but your model would also learn to reason like those big SOTA's. &lt;/p&gt; &lt;p&gt;Tried multiple llms, gemma3 1b, gemma3 270m and qwen3 0.6b, qwen3 0.6b gave me by far the best results and good interference / training speeds. &lt;/p&gt; &lt;p&gt;Tried both the instruct and base variants of this model, yes the base model performed significantly better and did not seem to overfit, it was fine-tuned on 1 epoch of a mixed half gpt OSS half deepseek r1 dataset with the special format the model uses and needs (about 200k rows total)&lt;/p&gt; &lt;p&gt;The model replicates how deepseeek r1 or gpt OSS would think about answering, you provide it the assistant output and user input (exact format on model page) and it would generate plausible grounded reasoning, keep in mind I've decided to almost completely eliminate reasoning about policies (gpt OSS stuff) and censorship biased reasoning while filtering, so it can think about spicy content, but due to limited data in that field you should check how it performs at that, generally deepseek r1 styled reasoning works better at NSFW, but obviously yes if you make it think about a rejection it would reject in the reasoning. &lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://huggingface.co/Pinkstack/syngen-reasoning-0.6b"&gt;https://huggingface.co/Pinkstack/syngen-reasoning-0.6b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also I made a very quick example dataset for you to evaluate how well it replicates reasoning: &lt;a href="https://huggingface.co/datasets/Pinkstack/syngen-reasoning-example-80-smoltalk1"&gt;https://huggingface.co/datasets/Pinkstack/syngen-reasoning-example-80-smoltalk1&lt;/a&gt; usually it does pretty good but as a rule of thumb, if you give it nonsense it would think poorly, feel free to test that though could be funny. &lt;/p&gt; &lt;p&gt;Hopefully this is useful to somebody! 🎉&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveTart3158"&gt; /u/ApprehensiveTart3158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayijj/turn_any_dataset_into_a_reasoning_dataset_easily/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayijj/turn_any_dataset_into_a_reasoning_dataset_easily/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oayijj/turn_any_dataset_into_a_reasoning_dataset_easily/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T19:22:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob0n3g</id>
    <title>Benchmarking different CLI parameters for VRAM vs. tk/s</title>
    <updated>2025-10-19T20:45:55+00:00</updated>
    <author>
      <name>/u/jumpingcross</name>
      <uri>https://old.reddit.com/user/jumpingcross</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm getting more speed than I need out of my GPU and wanted to explore tradeoffs of token generation speed vs. VRAM in llama.cpp since I'm sharing the GPU with other tools. What I'm seeing is that n-gpu-layers and n-cpu-moe can do that, but the decrease in VRAM is comparatively modest vs. the decrease in speed, with n-gpu-layers having a much stronger effect than n-cpu-moe. In particular, n-gpu-layers and n-cpu-moe to a lesser extent drop performance by a whole ton the moment you set them away from their defaults, while the VRAM remains almost entirely the same. no-kv-offload on the other hand drops VRAM usage by a fair amount while not impacting speed too heavily (20 GB -&amp;gt; 17 GB; 614 tk/s -&amp;gt; 550 tk/s), so I might consider using this in the future. &lt;/p&gt; &lt;p&gt;The results are probably YMMV and dependent on the specific setup being used (my system is a 5090 + DDR5-6400 RAM running llama.cpp version 6697 and the unsloth 4_K_M quant of Qwen 3 Coder 30B). I also didn't use too many Monte Carlo runs since I just wanted something quick and dirty, so there's probably some variation in the results. I uploaded the python script I used to automate the testing here (&lt;a href="https://pastebin.com/q6hTfMkq"&gt;https://pastebin.com/q6hTfMkq&lt;/a&gt;) along with raw results from my system in case it's of interest to anyone else. It's not the most efficient thing in the world (wastes time tearing down/starting up llama.cpp even when it's not necessary) but does the job for my needs. The code has some other arguments I played around with, but from what I saw, they didn't seem to decrease VRAM by any significant amount and some even increased it. Disclaimer, I used AI to generate a simple base for this script since I didn't want to waste time going through documentation trying to figure out how to properly query/manage the server, but modified the resulting script manually for my needs.&lt;/p&gt; &lt;p&gt;tl;dr no-kv-offload seems like an interesting option for a modest reduction in VRAM while not hurting performance too much. n-cpu-moe and n-gpu-layers can also reduce by a lot but costs quite a bit of speed.&lt;/p&gt; &lt;p&gt;Curious to know what other people think about the results or if there are any other parameters that might be interesting to look at.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jumpingcross"&gt; /u/jumpingcross &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob0n3g/benchmarking_different_cli_parameters_for_vram_vs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob0n3g/benchmarking_different_cli_parameters_for_vram_vs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob0n3g/benchmarking_different_cli_parameters_for_vram_vs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T20:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8klx</id>
    <title>Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models</title>
    <updated>2025-10-18T21:54:34+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt; &lt;img alt="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" src="https://external-preview.redd.it/ZnUwMzR3NXV5eHZmMTNcUS_6HPKChleMDpJ0qQU2p3V675TK2MrxaOnhSll8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=568275d60bed24ec215f8d24a426e506eab3f5d7" title="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jak8lx5uyxvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:54:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oat6fh</id>
    <title>Best Current Model for Programming?</title>
    <updated>2025-10-19T15:55:20+00:00</updated>
    <author>
      <name>/u/MurazakiUsagi</name>
      <uri>https://old.reddit.com/user/MurazakiUsagi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title says it all. I'm looking to work with Rust, C/C++, Python and Assembly. &lt;/p&gt; &lt;p&gt;Thank you in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MurazakiUsagi"&gt; /u/MurazakiUsagi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oat6fh/best_current_model_for_programming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T15:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar69q</id>
    <title>Any resource to understand LLM fine tuning/inference at a medium level to learn about temperature, quanitzation, loss functions, gpu setup?</title>
    <updated>2025-10-19T14:34:15+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any resource you found helpful to learn LLM fine tuning at a medium level so. i can start tinkering by knowing what's happening behind the scenes? Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob17go</id>
    <title>I made a mod of Qwen Code for working with local models in LM Studio</title>
    <updated>2025-10-19T21:08:22+00:00</updated>
    <author>
      <name>/u/feverdream</name>
      <uri>https://old.reddit.com/user/feverdream</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"&gt; &lt;img alt="I made a mod of Qwen Code for working with local models in LM Studio" src="https://external-preview.redd.it/YRVmeTTENyTnVFkxiv6fyP_vZo98Ij9IpjhsuYhArJo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=83641b9a64a38f5187a89aa2294d825e59a7f28c" title="I made a mod of Qwen Code for working with local models in LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/sip3pvr0v4wf1.png?width=1691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9eb322340ffed7a42020ed91ea4f3520b2125ac"&gt;https://preview.redd.it/sip3pvr0v4wf1.png?width=1691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e9eb322340ffed7a42020ed91ea4f3520b2125ac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I made &lt;a href="https://github.com/dkowitz/LowCal-Code"&gt;LowCal Code&lt;/a&gt; specifically to work with my locally hosted models in LM Studio, and also with the option to use online models through OpenRouter - that's it, those are the only two options with /auth, LM Studio or OpenRouter.&lt;/p&gt; &lt;p&gt;When you use /model&lt;/p&gt; &lt;ul&gt; &lt;li&gt;With LM Studio, it shows you available models to choose from, along with their configured and maximum context sizes (you have to manually configure a model in LM Studio once and set it's context size before it's available in LowCal).&lt;/li&gt; &lt;li&gt;With OpenRouter, it shows available models (hundreds), along with context size and price, and you can filter them. You need an api key.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Other local model enhancements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;/promptmode set &amp;lt;full/concise/auto&amp;gt;&lt;/code&gt; &lt;ul&gt; &lt;li&gt;full: full, long system prompt with verbose instructions and lots of examples&lt;/li&gt; &lt;li&gt;concise: short, abbreviated prompt for conserving context space and decreasing latency, particularly for local models. Dynamically constructed to only include instructions/examples for tools from the currently activated /toolset.&lt;/li&gt; &lt;li&gt;auto: automatically uses concise prompt when using LM Studio endpoint and full prompt when using OpenRouter endpoint&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;/toolset (list, show, activate/use, create, add, remove)&lt;/code&gt; - use custom tool collections to exclude tools from being used and saving context space and decreasing latency, particularly with local models. Using the shell tool is often more efficient than using file tools. &lt;ul&gt; &lt;li&gt;list: list available preset tool collections&lt;/li&gt; &lt;li&gt;show : shows which tools are in a collection&lt;/li&gt; &lt;li&gt;activate/use: Use a selected tool collection&lt;/li&gt; &lt;li&gt;create: Create a new tool collection&lt;code&gt;/toolset create &amp;lt;name&amp;gt; [tool1, tool2, ...]&lt;/code&gt; (Use tool names from /tools)&lt;/li&gt; &lt;li&gt;add/remove: add/remove tool to/from a tool collection &lt;code&gt;/toolset add[remove] &amp;lt;name&amp;gt; tool&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;/promptinfo&lt;/code&gt; - Show the current system prompt in a /view window (↑↓ to scroll, 'q' to quit viewer).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's made to run efficiently and autonomously with local models, gpt-oss-120, 20, Qwen3-coder-30b, glm-45-air, and others work really well! Honestly I don't see a huge difference in effectiveness between the concise prompt and the huge full system prompt, and often using just the shell tool, or in combination with WebSearch or Edit can be much faster and more effective than many of the other tools.&lt;/p&gt; &lt;p&gt;I developed it to use on my 128gb Strix Halo system on Ubuntu, so I'm not sure it won't be buggy on other platforms (especially Windows).&lt;/p&gt; &lt;p&gt;Let me know what you think! &lt;a href="https://github.com/dkowitz/LowCal-Code"&gt;https://github.com/dkowitz/LowCal-Code&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/feverdream"&gt; /u/feverdream &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob17go/i_made_a_mod_of_qwen_code_for_working_with_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T21:08:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oayrh9</id>
    <title>Got new 5070ti gpu, have access to 16gb vram. What things can I do with it for AI?</title>
    <updated>2025-10-19T19:32:21+00:00</updated>
    <author>
      <name>/u/AdOver7835</name>
      <uri>https://old.reddit.com/user/AdOver7835</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Had 2050 earlier with 4gb. Curious what new superpowers do I get with this new vram access?&lt;br /&gt; So far&lt;br /&gt; 1. ran gpt-oss 20b in lmstuio\. with upto 30k context window it gives around 40 tok/sec output.&lt;br /&gt; 2. ran gemma-27b. runs around 17 tok/sec&lt;br /&gt; 3. ran qwen3 coder 30b -- rund around 30 tok/sec&lt;/p&gt; &lt;p&gt;Apart from running models locally, I want to do things which earlier I didn't think of. &lt;/p&gt; &lt;p&gt;Planned :&lt;br /&gt; 1. Image generation with flux and automatic1111&lt;br /&gt; 2. want to try openai whisper&lt;br /&gt; 3. want to build ai agents which runs 24*7&lt;/p&gt; &lt;p&gt;last but not the least, complete spiderman 2 on this :)&lt;/p&gt; &lt;p&gt;Please help me with ideas and experimentations, I want to utilize this precious thing as much as possible and upskill myself in AI world.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdOver7835"&gt; /u/AdOver7835 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oayrh9/got_new_5070ti_gpu_have_access_to_16gb_vram_what/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T19:32:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavy6f</id>
    <title>Reverse Engineering and Tracing internal thoughts of LLM</title>
    <updated>2025-10-19T17:43:57+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks I did following experiments to understand inner working of LLM&lt;br /&gt; Index of experiments I did in this article (I used LLama 3 1B)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Token Prediction Trace&lt;/li&gt; &lt;li&gt;Attribution Analysis&lt;/li&gt; &lt;li&gt;Layer Emergence (knowledge tracing)&lt;/li&gt; &lt;li&gt;Weight Matrix Analyis (How knowledge encoded in weights)&lt;/li&gt; &lt;li&gt;Dimension Tokens Analysis (which Dimension stored encoded token for “paris”)&lt;/li&gt; &lt;li&gt;Prediction Chain (How does each dimension contribute to final output)&lt;/li&gt; &lt;li&gt;Token→Neuron Map (Which neurons encode token)&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;a href="https://medium.com/@harishhacker3010/reverse-engineering-and-tracing-internal-thoughts-of-llm-3017b5f72008"&gt;https://medium.com/@harishhacker3010/reverse-engineering-and-tracing-internal-thoughts-of-llm-3017b5f72008&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavy6f/reverse_engineering_and_tracing_internal_thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oatip1</id>
    <title>If the bubble really pops how can that affect local AI models?</title>
    <updated>2025-10-19T16:08:41+00:00</updated>
    <author>
      <name>/u/WEREWOLF_BX13</name>
      <uri>https://old.reddit.com/user/WEREWOLF_BX13</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If all this AI bubble talk really comes to an popa after all, how might this affect the development of more local AI models? From what I've seen MoE models still outperforms most models easily, but creating models is still expensive as shit, rather for the planet than their pocket, donation exists anyways.&lt;/p&gt; &lt;p&gt;But the servers these models use to be trained consumes a shitton of load, and I could imagine most big company servers not allowing AI to be trained on their servers anymore considering the massive amounts of models being released every week. Do you think AI would immediately freeze in advancement upon a bubble pop making us have to wait more 80 years for an actual AGI?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WEREWOLF_BX13"&gt; /u/WEREWOLF_BX13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oatip1/if_the_bubble_really_pops_how_can_that_affect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T16:08:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oauxgg</id>
    <title>lazylms - TUI for LM Studio</title>
    <updated>2025-10-19T17:04:04+00:00</updated>
    <author>
      <name>/u/Rugs007</name>
      <uri>https://old.reddit.com/user/Rugs007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"&gt; &lt;img alt="lazylms - TUI for LM Studio" src="https://b.thumbs.redditmedia.com/HpahzCgyFc5ZYpuDsGsueGlJc11EMlLdwC_J2jGKOvc.jpg" title="lazylms - TUI for LM Studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys! I made a TUI for using LM Studio by staying in the terminal. This is a hobby side project, MIT licensed and uses the CLI and REST API. Feel free to give it a try. This is inspired by lazygit and lazydocker. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Rugz007/lazylms"&gt;https://github.com/Rugz007/lazylms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rugs007"&gt; /u/Rugs007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oauxgg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oauxgg/lazylms_tui_for_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:04:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1oayl4j</id>
    <title>Free API Key for GLM 4.6</title>
    <updated>2025-10-19T19:25:35+00:00</updated>
    <author>
      <name>/u/avianio</name>
      <uri>https://old.reddit.com/user/avianio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi guys, providing a free API for GLM 4.6 for the next 48 hours as part of a load test. Enjoy.&lt;/p&gt; &lt;p&gt;Here are the credentials:&lt;/p&gt; &lt;p&gt;Model Name: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;z-ai/glm-4.6 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Base URL: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;https://api.avian.io/v1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;API Key: &lt;/p&gt; &lt;pre&gt;&lt;code&gt;avian-8z-5Qb5tLGS6q_A2j6Z2-iZxD78XnKCuvisEQQswZXw &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/avianio"&gt; /u/avianio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oayl4j/free_api_key_for_glm_46/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T19:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav3r1</id>
    <title>Quantized some MoE models with MXFP4</title>
    <updated>2025-10-19T17:10:57+00:00</updated>
    <author>
      <name>/u/noctrex</name>
      <uri>https://old.reddit.com/user/noctrex</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So as I was sitting and trying out some MXFP4_MOE quants from &lt;a href="https://huggingface.co/Face314"&gt;Face314&lt;/a&gt; &amp;amp; &lt;a href="https://huggingface.co/sm54"&gt;sm54&lt;/a&gt;, I can say that liked them very much.&lt;/p&gt; &lt;p&gt;So I thought why not quantize some more this weekend.&lt;/p&gt; &lt;p&gt;Well, here they are:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/noctrex"&gt;https://huggingface.co/noctrex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Any suggestions or critique welcome.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noctrex"&gt; /u/noctrex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav3r1/quantized_some_moe_models_with_mxfp4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:10:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oav4hi</id>
    <title>Two new Google models, "lithiumflow" and "orionmist", have been added to LMArena. This is Google's naming scheme and "orion" has been used internally with Gemini 3 codenames, so these are likely Gemini 3 models</title>
    <updated>2025-10-19T17:11:45+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist"&gt;https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakrdm</id>
    <title>Drop your underrated models you run LOCALLY</title>
    <updated>2025-10-19T08:52:22+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably within the 0.2b -32b range, or MoEs up to 140b&lt;/p&gt; &lt;p&gt;I’m on a LLM downloading spree, and wanna fill up a 2tb SSD with them. &lt;/p&gt; &lt;p&gt;Can be any use case. Just make sure to mention the use case too &lt;/p&gt; &lt;p&gt;Thank you ✌️&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oamo0k</id>
    <title>Gemma 4</title>
    <updated>2025-10-19T10:53:39+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are very very excited about the release of gemini 3.0 including me, but im more excited in the gemma family of models since they are based on gemini models and on top of that are open-sourced. And simce Gemini 3.0 is groundbreaking (apparently, like the pelican svg, robot svg, xbox svg, os etc tests), I am very curious about how will the gemma 4 models perform. And also, gemma 4 is going to be a big leap compared to gemma 3 coz It was based on gemini 2.0, not 2.5. So we are getting 2 genarational leaps!&lt;/p&gt; &lt;p&gt;When it will be released??&lt;/p&gt; &lt;p&gt;Gemma 1 was based on gemini 1 and was released ~1-2 months after gemini&lt;/p&gt; &lt;p&gt;Gemma 2 was based on gemini 1.5 and was released ~4 months after gemini 1.5&lt;/p&gt; &lt;p&gt;Gemma 3 was based on gemini 2 and was released ~1-2 months after gemini 2.0&lt;/p&gt; &lt;p&gt;So Gemma 4 might be released ~1-2 months after gemini 3??? Maybe???&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T10:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavxt8</id>
    <title>I built a 1B CAD generator model</title>
    <updated>2025-10-19T17:43:33+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt; &lt;img alt="I built a 1B CAD generator model" src="https://external-preview.redd.it/ZGFhNmE0bzJ2M3dmMdhv6U5XLy0vFYTB3BWLA3H-O3YDxkmUtGbojZ8LN3lz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a21d6d0c153a39bacb389fe42d52137134b86925" title="I built a 1B CAD generator model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a weekend, I decided to build a small language model to generate me 3d files. No reason except for pure curiosity. Here's what I did:&lt;/p&gt; &lt;p&gt;- Gather dataset on OpenSCAD: This turns out to be quite bad because people's code quality is low &amp;amp; in-consistent.&lt;/p&gt; &lt;p&gt;- Generate synthetic data (prompt -&amp;gt; openscad): This was the most wasteful per dollar part. I spent 150$+ on Claude API (70% are on reasoning token). Ended up using Gemma3-12b running in 48 hours continuously.&lt;/p&gt; &lt;p&gt;- Finetune Gemma3-270M, 1B &amp;amp; 4B: 270M lacks fundamental code &amp;amp; object understanding and failed badly. 1B is a good balance between render-ability rate &amp;amp; speed.&lt;/p&gt; &lt;p&gt;Overall, I spent 150$ on Claude (totally wasted) &amp;amp; 25$ on GPU. Both given as credits and grants.&lt;/p&gt; &lt;p&gt;I also made a CLI app if you wanna try on Mac, Linux or Raspberry Pi 4/5: &lt;a href="https://github.com/ThomasVuNguyen/MakeMe"&gt;https://github.com/ThomasVuNguyen/MakeMe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models, dataset &amp;amp; code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasVuNguyen/K"&gt;https://github.com/ThomasVuNguyen/K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b"&gt;https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pn0yo3o2v3wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready 🎉</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready 🎉" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;/p&gt; &lt;p&gt;1: &lt;a href="https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV"&gt;https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV&lt;/a&gt;&lt;br /&gt; 2: &lt;a href="https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi"&gt;https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi&lt;/a&gt;&lt;br /&gt; 3: &lt;a href="https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1"&gt;https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
