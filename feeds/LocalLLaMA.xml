<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-21T03:54:06+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nlxq55</id>
    <title>1K+ schemas of agentic projects visualized</title>
    <updated>2025-09-20T13:07:26+00:00</updated>
    <author>
      <name>/u/altsoph</name>
      <uri>https://old.reddit.com/user/altsoph</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I analyzed 1K+ Reddit posts about AI agent projects, processed them automatically into graphical schemas, and studied them. You can play with them interactively: &lt;a href="https://altsoph.com/pp/aps/"&gt;https://altsoph.com/pp/aps/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Besides many really strange constructions, I found three dominant patterns: chat-with-data (50%), business process automation (25%), and tool-assisted planning (15%). Each has specific requirements and pain points, and these patterns seem remarkably consistent with my own experience building agent systems.&lt;/p&gt; &lt;p&gt; I'd love to discuss if others see different patterns in this data.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/altsoph"&gt; /u/altsoph &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxq55/1k_schemas_of_agentic_projects_visualized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxq55/1k_schemas_of_agentic_projects_visualized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxq55/1k_schemas_of_agentic_projects_visualized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:07:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm75ku</id>
    <title>I just downloaded LM Studio. What models do you suggest for multiple purposes (mentioned below)? Multiple models for different tasks are welcomed too.</title>
    <updated>2025-09-20T19:26:02+00:00</updated>
    <author>
      <name>/u/abdullahmnsr2</name>
      <uri>https://old.reddit.com/user/abdullahmnsr2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use the free version of ChatGPT, and I use it for many things. Here are the uses that I want the models for:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Creative writing / Blog posts / general stories / random suggestions and ideas on multiple topics.&lt;/li&gt; &lt;li&gt;Social media content suggestion. For example, the title and description for YouTube, along with hashtags for YouTube and Instagram. I also like generating ideas for my next video.&lt;/li&gt; &lt;li&gt;Coding random things, usually something small to make things easier for me in daily life. Although, I am interested in creating a complete website using a model.&lt;/li&gt; &lt;li&gt;If possible, a model or LM Studio setting where I can search the web.&lt;/li&gt; &lt;li&gt;I also want a model where I can upload images, txt files, PDFs and more and extract information out of them.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Right now, I have a model suggested by LM Studio called &amp;quot;openai/gpt-oss-20b&amp;quot;.&lt;/p&gt; &lt;p&gt;I don't mind multiple models for a specific task.&lt;/p&gt; &lt;p&gt;Here are my laptop specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Lenovo Legion 5&lt;/li&gt; &lt;li&gt;Core i7, 12th Gen&lt;/li&gt; &lt;li&gt;16GB RAM&lt;/li&gt; &lt;li&gt;Nvidia RTX 3060&lt;/li&gt; &lt;li&gt;1.5TB SSD&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdullahmnsr2"&gt; /u/abdullahmnsr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm75ku/i_just_downloaded_lm_studio_what_models_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm75ku/i_just_downloaded_lm_studio_what_models_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm75ku/i_just_downloaded_lm_studio_what_models_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T19:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlwhmk</id>
    <title>CodeRabbit commits $1 million to open source</title>
    <updated>2025-09-20T12:07:58+00:00</updated>
    <author>
      <name>/u/Motor_Cycle7600</name>
      <uri>https://old.reddit.com/user/Motor_Cycle7600</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlwhmk/coderabbit_commits_1_million_to_open_source/"&gt; &lt;img alt="CodeRabbit commits $1 million to open source" src="https://external-preview.redd.it/4kvDM7gPrl0ixzXFp7sXBnwp2EZyHR1-9DufiPVEqAE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aeda450f719cb582ac0b17b8b6737424b7b57034" title="CodeRabbit commits $1 million to open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Motor_Cycle7600"&gt; /u/Motor_Cycle7600 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.coderabbit.ai/blog/coderabbit-commits-1-million-to-open-source"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlwhmk/coderabbit_commits_1_million_to_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlwhmk/coderabbit_commits_1_million_to_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T12:07:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlxchg</id>
    <title>How to think about GPUs (by Google)</title>
    <updated>2025-09-20T12:50:05+00:00</updated>
    <author>
      <name>/u/notdl</name>
      <uri>https://old.reddit.com/user/notdl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxchg/how_to_think_about_gpus_by_google/"&gt; &lt;img alt="How to think about GPUs (by Google)" src="https://preview.redd.it/dtyx6xrfgbqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46161052c3a29fd8459abfcce3ffb4aa7283efca" title="How to think about GPUs (by Google)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notdl"&gt; /u/notdl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dtyx6xrfgbqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxchg/how_to_think_about_gpus_by_google/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlxchg/how_to_think_about_gpus_by_google/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T12:50:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm8bvz</id>
    <title>Automated high quality manga translations?</title>
    <updated>2025-09-20T20:13:41+00:00</updated>
    <author>
      <name>/u/Shadow-Amulet-Ambush</name>
      <uri>https://old.reddit.com/user/Shadow-Amulet-Ambush</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Some time ago I created and open sourced LLocle coMics to automate translating manga. It's a python script that uses Olama to translate a set of manga pages after the user uses Mokuro to OCR the pages and combine them in 1 html file.&lt;/p&gt; &lt;p&gt;Over-all I'm happy with the quality that I typically get out of the project using the Xortron Criminal Computing model. The main drawbacks are the astronomical time it takes to do a translation (I leave it running over night or while I'm at work) and the fact that I'm just a hobbyist so 10% of the time a textbox will just get some kind of weird error or garbled translation.&lt;/p&gt; &lt;p&gt;Does anyone have any alternatives to suggest? I figure someone here must have thought of something that may be helpful. I couldn't find a way to make use of Ooba with DeepThink&lt;/p&gt; &lt;p&gt;I'm also fine with suggestions that speed up manual translation process.&lt;/p&gt; &lt;p&gt;EDIT:&lt;/p&gt; &lt;p&gt;It looks like &lt;a href="https://github.com/zyddnys/manga-image-translator"&gt;https://github.com/zyddnys/manga-image-translator&lt;/a&gt; is really good, but needs a very thorough guide to be usable. Like its instructions are BAD. I don't understand how to use the config or any of the options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Shadow-Amulet-Ambush"&gt; /u/Shadow-Amulet-Ambush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm8bvz/automated_high_quality_manga_translations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm8bvz/automated_high_quality_manga_translations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm8bvz/automated_high_quality_manga_translations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T20:13:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm7emt</id>
    <title>Built LLM Colosseum - models battle each other in a kingdom system</title>
    <updated>2025-09-20T19:36:15+00:00</updated>
    <author>
      <name>/u/Rude-Worry4747</name>
      <uri>https://old.reddit.com/user/Rude-Worry4747</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm7emt/built_llm_colosseum_models_battle_each_other_in_a/"&gt; &lt;img alt="Built LLM Colosseum - models battle each other in a kingdom system" src="https://external-preview.redd.it/ZzZibmJvOTBnZHFmMXDN2m6-iaYzARiSYaB3uMzH_TOvUDGCWgAPkhd3bhyy.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=13cf2bc3afc351071a7bf60e4d18a0d12ff79188" title="Built LLM Colosseum - models battle each other in a kingdom system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally shipped this project I've been working on. It's basically an LLM evaluation platform but as a competitive ladder system.&lt;/p&gt; &lt;p&gt;The problem: Human voting (like LLM Arena) doesn't scale, and standard benchmarks feel stale. So I built something where models fight their way up ranks: Novice â†’ Expert â†’ Master â†’ King.&lt;/p&gt; &lt;p&gt;How it works:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Models judge each other (randomly selected from the pool)&lt;/li&gt; &lt;li&gt;Winners get promoted, losers get demoted&lt;/li&gt; &lt;li&gt;Multi-turn debates where they actually argue back and forth&lt;/li&gt; &lt;li&gt;Problems come from AIME, MMLU Pro, community submissions, and models generating challenges for each other&lt;/li&gt; &lt;li&gt;Runs 24/7, you can watch live battles from anyone who spins it up&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The self-judging thing creates weird dynamics. Good models become judges for others, and you get this whole competitive ecosystem. Watching GPT-5 and Claude 4 debate ethics in real-time is pretty entertaining.&lt;/p&gt; &lt;p&gt;Still rough around the edges but the core idea seems to work. Built with FastAPI/Next.js, integrates with OpenRouter for multiple models.&lt;/p&gt; &lt;p&gt;It's all open source. Would love people to try it!&lt;/p&gt; &lt;p&gt;Link : &lt;a href="https://llmcolosseum.vercel.app/"&gt;https://llmcolosseum.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rude-Worry4747"&gt; /u/Rude-Worry4747 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/q2lj2o90gdqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm7emt/built_llm_colosseum_models_battle_each_other_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm7emt/built_llm_colosseum_models_battle_each_other_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T19:36:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlfm4p</id>
    <title>Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast</title>
    <updated>2025-09-19T21:11:21+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/"&gt; &lt;img alt="Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast" src="https://external-preview.redd.it/YzFwanVkZnpzNnFmMbLrEG3LS8K9xI7Zo9NLFNWl_BVRzdP5tkFGVRvYzADE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01aa65a0a1dadcdaa683d6c6c12e54de616964d9" title="Matthew McConaughey says he wants a private LLM on Joe Rogan Podcast" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Matthew McConaughey says he wants a private LLM, fed only with his books, notes, journals, and aspirations, so he can ask it questions and get answers based solely on that information, without any outside influence.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://x.com/JonhernandezIA/status/1969054219647803765"&gt;https://x.com/JonhernandezIA/status/1969054219647803765&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey Matthew, what you described already exists. It's called &lt;a href="https://hyperlink.nexa.ai/"&gt;Hyperlink&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n2vmpefzs6qf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlfm4p/matthew_mcconaughey_says_he_wants_a_private_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-19T21:11:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm51eq</id>
    <title>In-depth on SM Threading in Cuda, Cublas/Cudnn</title>
    <updated>2025-09-20T18:02:09+00:00</updated>
    <author>
      <name>/u/Freonr2</name>
      <uri>https://old.reddit.com/user/Freonr2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm51eq/indepth_on_sm_threading_in_cuda_cublascudnn/"&gt; &lt;img alt="In-depth on SM Threading in Cuda, Cublas/Cudnn" src="https://external-preview.redd.it/GxatAI4cgl8Z6EnMUen6jv88kc56GcXox3oS99t_ODc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de4a9d9a9728e8d418b1640dd51d4a75142969e3" title="In-depth on SM Threading in Cuda, Cublas/Cudnn" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Freonr2"&gt; /u/Freonr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://modal.com/gpu-glossary/readme"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm51eq/indepth_on_sm_threading_in_cuda_cublascudnn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm51eq/indepth_on_sm_threading_in_cuda_cublascudnn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T18:02:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmf0hw</id>
    <title>New E-commerce encoders in town: RexBERT</title>
    <updated>2025-09-21T01:16:54+00:00</updated>
    <author>
      <name>/u/Minute_Smile5698</name>
      <uri>https://old.reddit.com/user/Minute_Smile5698</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HF blog published: &lt;a href="https://huggingface.co/blog/thebajajra/rexbert-encoders"&gt;https://huggingface.co/blog/thebajajra/rexbert-encoders&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Outperforms ModernBERT&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Minute_Smile5698"&gt; /u/Minute_Smile5698 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmf0hw/new_ecommerce_encoders_in_town_rexbert/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:16:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nltfwx</id>
    <title>AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right.</title>
    <updated>2025-09-20T09:12:06+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"&gt; &lt;img alt="AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right." src="https://external-preview.redd.it/MHdnZnppa2VkYXFmMfboDEJV_8E07yibCTC4f2dErk0sK7LfErgP63h2qGj9.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=322dc704f600fd94bc23fadf748b6bdace23594e" title="AI CEOs: only I am good and wise enough to build ASI (artificial superintelligence). Everybody else is evil or won't do it right." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kdoptgkedaqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nltfwx/ai_ceos_only_i_am_good_and_wise_enough_to_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmdh7a</id>
    <title>Adding Brave search to LM Studio via MCPs</title>
    <updated>2025-09-20T23:59:51+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I found these directions easy and clear. &lt;a href="https://medium.com/@anojrs/adding-web-search-to-lm-studio-via-mcp-d4b257fbd589"&gt;https://medium.com/@anojrs/adding-web-search-to-lm-studio-via-mcp-d4b257fbd589&lt;/a&gt;. Note you'll need to get a free Brave search api. Too, there are other search tools you can use. YMMV.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmdh7a/adding_brave_search_to_lm_studio_via_mcps/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmdh7a/adding_brave_search_to_lm_studio_via_mcps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmdh7a/adding_brave_search_to_lm_studio_via_mcps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T23:59:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm31u5</id>
    <title>What's the next model you are really excited to see?</title>
    <updated>2025-09-20T16:43:21+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have had so many new models in the last few months I have lost track on what is to come. What's the next model you are really excited to see coming?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm31u5/whats_the_next_model_you_are_really_excited_to_see/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm31u5/whats_the_next_model_you_are_really_excited_to_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm31u5/whats_the_next_model_you_are_really_excited_to_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T16:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeq9w</id>
    <title>Llama.cpp support for Ling Mini 2.0 is probably coming next week</title>
    <updated>2025-09-21T01:02:01+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"&gt; &lt;img alt="Llama.cpp support for Ling Mini 2.0 is probably coming next week" src="https://external-preview.redd.it/2pTMNMbI2akSWow2DVQcK_a-oWX8FigInIZ74WH_NyQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2f02e74afa0abefa42b4330fc05577e733ff328" title="Llama.cpp support for Ling Mini 2.0 is probably coming next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Llama.cpp support for Ling Mini 2.0 is coming in the following days, it seems thereâ€™s already a PR waiting to be merged and some GGUFs already out.&lt;/p&gt; &lt;p&gt;An interesting thing about this model is that it has 16B total parameters, but only 1.4B are activated per input token, and it outperforms Ernie 4.5 21B A3B, which is a tad bigger and uses more active parameters. Quite a nice addition for the GPU-poor folks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16036"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeq9w/llamacpp_support_for_ling_mini_20_is_probably/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:02:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlkwr3</id>
    <title>OpenWebUI is the most bloated piece of s**t on earth, not only that but it's not even truly open source anymore, now it just pretends it is because you can't remove their branding from a single part of their UI. Suggestions for new front end?</title>
    <updated>2025-09-20T01:08:06+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Honestly, I'm better off straight up using SillyTavern, I can even have some fun with a cute anime girl as my assistant helping me code or goof off instead of whatever dumb stuff they're pulling.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlkwr3/openwebui_is_the_most_bloated_piece_of_st_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T01:08:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlufzx</id>
    <title>llama.ui: new updates!</title>
    <updated>2025-09-20T10:14:45+00:00</updated>
    <author>
      <name>/u/COBECT</name>
      <uri>https://old.reddit.com/user/COBECT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt; &lt;img alt="llama.ui: new updates!" src="https://preview.redd.it/mjwmirusoaqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b3160b178b7387b2186e3b81e5c0b04c1d83fe5" title="llama.ui: new updates!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm excited to announce an update to &lt;strong&gt;llama.ui&lt;/strong&gt;, a privacy focused web interface for interacting with Large Language Models! We bring some awesome new features and performance improvements: - Configuration Presets: Save and load your favorite configurations for different models and use cases. - Text-to-Speech: Listen to the AI's responses! Supports multiple voices and languages. - Database Export/Import: Backup your chat history or transfer to a new device! - Conversation Branching: Experiment with different paths in your conversations.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/COBECT"&gt; /u/COBECT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mjwmirusoaqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlufzx/llamaui_new_updates/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T10:14:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm4b0q</id>
    <title>Efficient 4B parameter gpt OSS distillation without the over-censorship</title>
    <updated>2025-09-20T17:32:58+00:00</updated>
    <author>
      <name>/u/ApprehensiveTart3158</name>
      <uri>https://old.reddit.com/user/ApprehensiveTart3158</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've personally loved using gpt oss, but it wasn't very fast locally and was totally over censored. &lt;/p&gt; &lt;p&gt;So I've thought about it and made a fine tune of qwen3 4B thinking on GPT OSS outputs, with MOST of the &amp;quot;I can't comply with that&amp;quot; removed from the fine tuning dataset. &lt;/p&gt; &lt;p&gt;You can find it here: &lt;a href="https://huggingface.co/Pinkstack/DistilGPT-OSS-qwen3-4B"&gt;https://huggingface.co/Pinkstack/DistilGPT-OSS-qwen3-4B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yes, it is small and no it cannot be properly used for speculative decoding but it is pretty cool to play around with and it is very fast. &lt;/p&gt; &lt;p&gt;From my personal testing (note, not benchmarked yet as that does take quite a bit of compute that I don't have right now): Reasoning efforts (low, high, medium) all works as intended and absolutely do change how long the model thinks which is huge. It thinks almost exactly like gpt oss and yes it does think about &amp;quot;policies&amp;quot; but from what I've seen with high reasoning it may start thinking about rejecting then convince itself to answer.. Lol(for example if you ask it to let's say swear at you, it would most of the time comply), unless what you asked is really unsafe it would probably comply, and it feels exactly like gpt oss, same style of code, almost identical output styles just not as much general knowledge as it is just 4b parameters!!&lt;/p&gt; &lt;p&gt;If you have questions or want to share something please comment and let me know, would live to hear what you think! :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveTart3158"&gt; /u/ApprehensiveTart3158 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm4b0q/efficient_4b_parameter_gpt_oss_distillation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T17:32:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm9uye</id>
    <title>My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)</title>
    <updated>2025-09-20T21:17:42+00:00</updated>
    <author>
      <name>/u/picturpoet</name>
      <uri>https://old.reddit.com/user/picturpoet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"&gt; &lt;img alt="My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)" src="https://preview.redd.it/0dhjzmgzydqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c79b9409fbebd0a546d3ef854d3b29ce2460c94e" title="My first local run using Magistral 1.2 - 4 bit and I'm thrilled to bits (no pun intended)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My Mac Studio M4 Max base model just came through and I was so excited to run something locally having always depended on cloud based models.&lt;/p&gt; &lt;p&gt;I don't know what use cases I will build yet but just so exciting that there's a new fun model available to try the moment I began.&lt;/p&gt; &lt;p&gt;Any ideas of what I should do next on my Local Llama roadmap and how I can get to being an intermediate localllm user from my current noob status is fully appreciated. ðŸ˜„&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/picturpoet"&gt; /u/picturpoet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0dhjzmgzydqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm9uye/my_first_local_run_using_magistral_12_4_bit_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T21:17:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nly3w1</id>
    <title>Qwen 3 VL next week</title>
    <updated>2025-09-20T13:24:32+00:00</updated>
    <author>
      <name>/u/Long_Bluejay_5368</name>
      <uri>https://old.reddit.com/user/Long_Bluejay_5368</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt; &lt;img alt="Qwen 3 VL next week" src="https://a.thumbs.redditmedia.com/fLB-QxQX_aAn0F5HXNaiy2dlb5JbWlBjS-VuT3q3TC0.jpg" title="Qwen 3 VL next week" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3"&gt;https://preview.redd.it/19dxif2kmbqf1.png?width=788&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=56a8c11d753f68cacd685640484117a43de99ce3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;what do you think about it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Long_Bluejay_5368"&gt; /u/Long_Bluejay_5368 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nly3w1/qwen_3_vl_next_week/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T13:24:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nme5xy</id>
    <title>4x MI50 32GB reach 22 t/s with Qwen3 235B-A22B and 36 t/s with Qwen2.5 72B in vllm</title>
    <updated>2025-09-21T00:33:31+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;It is exciting to see AMD is finally fixing their software stack. I recently updated my MI50 GPU drivers and ROCm stack to 6.4.3. AMD officially deprecated support for MI50 (gfx906). But ROCm 6.4.3 works with one simple fix. You need to copy tensile library of MI50 from a package and paste it in rocm folder (details: &lt;a href="https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977"&gt;https://github.com/ROCm/ROCm/issues/4625#issuecomment-2899838977&lt;/a&gt; ).&lt;/p&gt; &lt;p&gt;For performance tests, I used vllm backend - &lt;a href="https://github.com/nlzy/vllm-gfx906"&gt;https://github.com/nlzy/vllm-gfx906&lt;/a&gt; . Thank you &lt;a href="/u/NaLanZeYu"&gt;u/NaLanZeYu&lt;/a&gt; for supporting gfx906 in a separate vllm fork!&lt;/p&gt; &lt;p&gt;In my venv, I installed pytorch 2.8. I kept the original triton 3.3 but I earlier checked and triton 3.5 was also working with MI50. For single GPU, there were no package issues. For multi-GPU, there was an issue - rccl was compiled without gfx906 support. What I did was I compiled rccl with gfx906 support.&lt;/p&gt; &lt;p&gt;Downloaded rccl 2.22.3 (for ROCm 6.4.3) from &lt;a href="https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3"&gt;https://github.com/ROCm/rccl/releases/tag/rocm-6.4.3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;extracted the zip file.&lt;/p&gt; &lt;p&gt;installed in ubuntu terminal:&lt;/p&gt; &lt;p&gt;```sudo ./install.sh --amdgpu_targets gfx906 -i -j 32 -p -r```&lt;/p&gt; &lt;p&gt;in vllmenv installation folder find &lt;a href="http://lbrccl.so"&gt;lbrccl.so&lt;/a&gt; and rename or delete it so that pytorch cannot use it. e.g. _librccl.so&lt;/p&gt; &lt;p&gt;in vllmenv, import the new rccl library location:&lt;/p&gt; &lt;p&gt;VLLM_NCCL_SO_PATH=/opt/rocm/lib&lt;/p&gt; &lt;p&gt;(or LD_LIBRARY_PATH=/opt/rocm/lib:$LD_LIBRARY_PATH)&lt;/p&gt; &lt;p&gt;now, vllm supports multi-GPU properly for MI50 with ROCm 6.4.3.&lt;/p&gt; &lt;p&gt;Some metrics:&lt;/p&gt; &lt;p&gt;single MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama-3.1-8B-AWQ-4bit - TG 93t/s; PP 945t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;four MI50 - single requests in vllm bench serve:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5 72B gptq int4 (TP 4) - TG 36/s; PP 500t/s&lt;/li&gt; &lt;li&gt;Qwen3-235B-A22B-AWQ (TP 4) - TG 22t/s; PP 290t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;All of them are connected to my MB with PCIE4.0 16x speed. CPU: AMD EPYC 7532 with 8x32GB DDR4 3200Mhz ECC RAM.&lt;/p&gt; &lt;p&gt;Overall, there is a great performance uplift (up to 25%) when we use ROCm 6.4.3 with gfx906.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nme5xy/4x_mi50_32gb_reach_22_ts_with_qwen3_235ba22b_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T00:33:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm0mzw</id>
    <title>Whisper Large v3 running in real-time on a M2 Macbook Pro</title>
    <updated>2025-09-20T15:08:21+00:00</updated>
    <author>
      <name>/u/rruk01</name>
      <uri>https://old.reddit.com/user/rruk01</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt; &lt;img alt="Whisper Large v3 running in real-time on a M2 Macbook Pro" src="https://external-preview.redd.it/NnkxeHk1bTIxY3FmMbdFe5hFZkGFnrWFqBq5GQzhAAe-tezJH5BHnp8SS6Dh.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c4c3798d766be6b03e6447b1663fb9590cdfcffe" title="Whisper Large v3 running in real-time on a M2 Macbook Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been working on using the Whisper models on device for 2-3 years now and wanted to share my progress. &lt;/p&gt; &lt;p&gt;I've figured out several optimisations which combined together means I can run the Whisper Large v3 (not turbo) model on a macbook with about 350-600ms latency for live (hypothesis/cyan) requests and 900-1200ms for completed (white) requests. It can also run on an iPhone 14 Pro with about 650-850ms latency for live requests and 1900ms for completed requests. The optimisations work for all the Whisper models and would probably work for the NVIDIA Parakeet / Canary models too. &lt;/p&gt; &lt;p&gt;The optimisations include speeding up the encoder on Apple Neural Engine so it runs at &lt;strong&gt;150ms&lt;/strong&gt; per run, this is compared to a naive 'ANE-optimised' encoder which runs at about &lt;strong&gt;500ms&lt;/strong&gt;. This does not require significant quantisation. The model running in the demo is quantised at Q8, but mainly so it takes up less hard-disk space, FP16 runs at similar speed. I've also optimised hypothesis requests so the output is much more stable. &lt;/p&gt; &lt;p&gt;If there's interest I'd be happy to write up a blog post on these optimisations, I'm also considering making an open source SDK so people can run this themselves, again if there's interest. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rruk01"&gt; /u/rruk01 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2ibrz4m21cqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm0mzw/whisper_large_v3_running_in_realtime_on_a_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T15:08:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlu3cd</id>
    <title>The iPhone 17 Pro can run LLMs fast!</title>
    <updated>2025-09-20T09:53:52+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt; &lt;img alt="The iPhone 17 Pro can run LLMs fast!" src="https://a.thumbs.redditmedia.com/lazvh4ZugenSKXRU1IYFLEO6hichFPkV7Tw3LqJ6h_8.jpg" title="The iPhone 17 Pro can run LLMs fast!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The new A19 Pro finally integrates neural accelerators into the GPU cores themselves, essentially Appleâ€™s version of Nvidiaâ€™s Tensor cores which are used for accelerating matrix multiplication that is prevalent in the transformers models we love so much. So I thought it would be interesting to test out running our smallest finetuned models on it!&lt;/p&gt; &lt;p&gt;Boy does the GPU fly compared to running the model only on CPU. The token generation is only about double but the prompt processing is over 10x faster! Itâ€™s so much faster that itâ€™s actually usable even on longer context as the prompt processing doesnâ€™t quickly become too long and the token generation speed is still high.&lt;/p&gt; &lt;p&gt;I tested using the Pocket Pal app on IOS which runs regular llamacpp with MLX Metal optimizations as far as I know. Shown are the comparison of the model running on GPU fully offloaded with Metal API and flash attention enabled vs running on CPU only. &lt;/p&gt; &lt;p&gt;Judging by the token generation speed, the A19 Pro must have about 70-80GB/s of memory bandwidth to the GPU and the CPU can access only about half of that bandwidth. &lt;/p&gt; &lt;p&gt;Anyhow the new GPU with the integrated tensor cores now look very interesting for running LLMs. Perhaps when new Mac Studios with updated M chips comes out with a big version of this new GPU architecture, I might even be able to use them to serve models for our low cost API. ðŸ¤”&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nlu3cd"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlu3cd/the_iphone_17_pro_can_run_llms_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T09:53:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nm6v83</id>
    <title>Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b</title>
    <updated>2025-09-20T19:14:37+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt; &lt;img alt="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" src="https://b.thumbs.redditmedia.com/Ol0Pxbro6vazPUaSzrEtZA_JvTzjW_-by2F4t8qWnuU.jpg" title="Qwen Next 80b q4 vs q8 vs GPT 120b vs Qwen Coder 30b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran this test on my M4 Max MacBook Pro 128 GB laptop. The interesting find is how prompt processing speed stays relatively flat as context grows. This is completely different behavior from Qwen3 Coder.&lt;/p&gt; &lt;p&gt;GPT 120b starts out faster but then becomes slower as context fills. However only the 4 bit quant of Qwen Next manages to overtake it when looking at total elapsed time. And that first happens at 80k context length. For most cases the GPT model stays the fastest then.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nm6v83"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nm6v83/qwen_next_80b_q4_vs_q8_vs_gpt_120b_vs_qwen_coder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T19:14:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nlyy6n</id>
    <title>Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping</title>
    <updated>2025-09-20T14:00:49+00:00</updated>
    <author>
      <name>/u/PhantomWolf83</name>
      <uri>https://old.reddit.com/user/PhantomWolf83</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt; &lt;img alt="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" src="https://external-preview.redd.it/942g63AteF3sF5KI6YzwLlHNUjooze5_uZcUA7PiVqQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=259bd6663f4a689dc50651317dca845a29e37f3f" title="Intel Arc Pro B60 24GB professional GPU listed at $599, in stock and shipping" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PhantomWolf83"&gt; /u/PhantomWolf83 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/intel-arc-pro-b60-24gb-professional-gpu-listed-at-599-in-stock-and-shipping"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nlyy6n/intel_arc_pro_b60_24gb_professional_gpu_listed_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-20T14:00:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmeu5s</id>
    <title>Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted</title>
    <updated>2025-09-21T01:07:48+00:00</updated>
    <author>
      <name>/u/TKGaming_11</name>
      <uri>https://old.reddit.com/user/TKGaming_11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt; &lt;img alt="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" src="https://external-preview.redd.it/VoGpbOIxrqAHEzxUbIOFVzMNSL9glnfyk27odhpB_Jk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d87cfdb2cbad767672c45769d597618162abb80" title="Qwen3-Omni, Qwen/Qwen3-Omni-7B spotted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TKGaming_11"&gt; /u/TKGaming_11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/41025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmeu5s/qwen3omni_qwenqwen3omni7b_spotted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T01:07:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmg185</id>
    <title>Qwen3Omni</title>
    <updated>2025-09-21T02:08:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt; &lt;img alt="Qwen3Omni" src="https://preview.redd.it/wcxu5ypyefqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b0e169e57d635253c780f31d6542861df594c98" title="Qwen3Omni" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcxu5ypyefqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T02:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building ðŸ”¨&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio ðŸ‘¾&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
