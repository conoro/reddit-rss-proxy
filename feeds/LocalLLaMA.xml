<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-18T21:34:43+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1njt6ut</id>
    <title>DeepSeek-R1 on Nature: How Pure Reinforcement Learning Unlocks LLM Reasoning</title>
    <updated>2025-09-17T23:50:20+00:00</updated>
    <author>
      <name>/u/First_Ground_9849</name>
      <uri>https://old.reddit.com/user/First_Ground_9849</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, Big news in the AI world today‚Äî&lt;strong&gt;DeepSeek-R1&lt;/strong&gt; is featured on the cover of &lt;em&gt;Nature&lt;/em&gt;! This is a significant milestone for reinforcement learning and reasoning in large language models. Here‚Äôs what makes this groundbreaking:&lt;/p&gt; &lt;h3&gt;üß† Pure Reinforcement Learning Breakthrough&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;DeepSeek-R1 is the &lt;strong&gt;first model&lt;/strong&gt; to achieve state-of-the-art reasoning &lt;strong&gt;without any supervised fine-tuning (SFT)&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;It uses &lt;strong&gt;Group Relative Policy Optimization (GRPO)&lt;/strong&gt;, a novel RL method that reduces computational cost while maintaining high performance.&lt;/li&gt; &lt;li&gt;The model &lt;strong&gt;autonomously developed&lt;/strong&gt; advanced reasoning strategies like self-reflection, verification, and dynamic adaptation‚Äîall through RL, &lt;strong&gt;without human demonstrations&lt;/strong&gt;. ### üèÜ Top-Tier Performance&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AIME 2024&lt;/strong&gt;:&lt;/li&gt; &lt;li&gt;&lt;code&gt;pass@1&lt;/code&gt;: &lt;strong&gt;77.9%&lt;/strong&gt; ‚Üí with self-consistency: &lt;strong&gt;86.7%&lt;/strong&gt; (surpassing human average)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;MATH-500&lt;/strong&gt;: &lt;strong&gt;97.3%&lt;/strong&gt; (pass@1)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Codeforces Rating&lt;/strong&gt;: &lt;strong&gt;2029&lt;/strong&gt; (Top 5% globally)&lt;/li&gt; &lt;li&gt;Also excels in biology, physics, chemistry, and broader benchmarks like MMLU-Pro (&lt;strong&gt;84.0%&lt;/strong&gt;), AlpacaEval 2.0 (&lt;strong&gt;87.6%&lt;/strong&gt;), and Arena-Hard (&lt;strong&gt;92.3%&lt;/strong&gt;) ### üîç Emergent Reasoning Behaviors During training, the model showed:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Self-correction&lt;/strong&gt;: ‚ÄúAha moments‚Äù where it reevaluated its reasoning (e.g., sudden increase in the word ‚Äúwait‚Äù)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-chain reasoning&lt;/strong&gt;: Generating hundreds to thousands of tokens to solve complex problems&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive token usage&lt;/strong&gt;: Using more tokens for hard problems, fewer for easy ones ### üåç Open Research &amp;amp; Model Release DeepSeek has released:&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek-R1-Zero&lt;/strong&gt; (pure RL version)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek-R1&lt;/strong&gt; (multistage RL + SFT for alignment)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Distilled smaller models&lt;/strong&gt; for broader accessibility&lt;/li&gt; &lt;li&gt;All &lt;strong&gt;code, weights, and data&lt;/strong&gt; under MIT license ### üìå Limitations &amp;amp; Future Work The model still has room for improvement in:&lt;/li&gt; &lt;li&gt;Tool use (e.g., calculators, search)&lt;/li&gt; &lt;li&gt;Token efficiency (sometimes overthinks)&lt;/li&gt; &lt;li&gt;Language mixing (optimized for EN/ZH only)&lt;/li&gt; &lt;li&gt;Prompt sensitivity (works best zero-shot) But the work proves that &lt;strong&gt;pure RL can unlock reasoning&lt;/strong&gt; without human data‚Äîpaving the way for more autonomous, self-improving AI. &lt;strong&gt;Paper &amp;amp; Resources:&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.nature.com/articles/s41586-025-09422-z"&gt;Nature Article&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-R1"&gt;GitHub Repo&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/DeepSeek-ai"&gt;Hugging Face&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What do you think? Is pure RL the future of LLM training?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/First_Ground_9849"&gt; /u/First_Ground_9849 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njt6ut/deepseekr1_on_nature_how_pure_reinforcement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njt6ut/deepseekr1_on_nature_how_pure_reinforcement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njt6ut/deepseekr1_on_nature_how_pure_reinforcement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T23:50:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkc6tx</id>
    <title>How are you using computer-use agents?</title>
    <updated>2025-09-18T15:58:15+00:00</updated>
    <author>
      <name>/u/New-Strain-7472</name>
      <uri>https://old.reddit.com/user/New-Strain-7472</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to understand how people are using computer-use agents in practice. If you are using computer-use agents today, what's your use-case?&lt;/p&gt; &lt;p&gt;To clarify, I'm not looking for folks building these agents. I'd love to hear from you if you are / know of individuals, teams, or companies actually using them in their workflows, products, or internal processes.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Strain-7472"&gt; /u/New-Strain-7472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkc6tx/how_are_you_using_computeruse_agents/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkc6tx/how_are_you_using_computeruse_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkc6tx/how_are_you_using_computeruse_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T15:58:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1njwmtl</id>
    <title>Google's paper, SLED, seems to improve factuality with (all? Most?) LLMs at only a 4% speed penalty</title>
    <updated>2025-09-18T02:33:14+00:00</updated>
    <author>
      <name>/u/laser_man6</name>
      <uri>https://old.reddit.com/user/laser_man6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/"&gt;https://research.google/blog/making-llms-more-accurate-by-using-all-of-their-layers/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This paper put out a year or so ago, and referenced by today's blog post, shows a method for decoding using the weighted average of every layer's logits. It improves factuality over DoLa (which itself improves over just standard sampling?) by anywhere from 2-16%with only a 4% hit to speed! I'm surprised I haven't seen this here since it seems like it shouldn't be too bad to implement into something like VLLM or llama.cpp, and it seems to work for many different models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/laser_man6"&gt; /u/laser_man6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njwmtl/googles_paper_sled_seems_to_improve_factuality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njwmtl/googles_paper_sled_seems_to_improve_factuality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njwmtl/googles_paper_sled_seems_to_improve_factuality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T02:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgovj</id>
    <title>Magistral Small 2509 has been released</title>
    <updated>2025-09-17T15:44:12+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt; &lt;img alt="Magistral Small 2509 has been released" src="https://external-preview.redd.it/lya4BYVSdGKwEDIK4epA43BL60WtN4IIIDfpTgnEljc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=14e03904bf3f936ad1691d3e1bcf8b07536b90d5" title="Magistral Small 2509 has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509-GGUF"&gt;https://huggingface.co/mistralai/Magistral-Small-2509-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509"&gt;https://huggingface.co/mistralai/Magistral-Small-2509&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Magistral Small 1.2&lt;/h1&gt; &lt;p&gt;Building upon &lt;a href="https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506"&gt;Mistral Small 3.2 (2506)&lt;/a&gt;, &lt;strong&gt;with added reasoning capabilities&lt;/strong&gt;, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters.&lt;/p&gt; &lt;p&gt;Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.&lt;/p&gt; &lt;p&gt;Learn more about Magistral in our &lt;a href="https://mistral.ai/news/magistral/"&gt;blog post&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The model was presented in the paper &lt;a href="https://huggingface.co/papers/2506.10910"&gt;Magistral&lt;/a&gt;.&lt;/p&gt; &lt;h1&gt;Updates compared with &lt;a href="https://huggingface.co/mistralai/Magistral-Small-2507"&gt;Magistral Small 1.1&lt;/a&gt;&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Multimodality&lt;/strong&gt;: The model now has a vision encoder and can take multimodal inputs, extending its reasoning capabilities to vision.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance upgrade&lt;/strong&gt;: Magistral Small 1.2 should give you significatively better performance than Magistral Small 1.1 as seen in the &lt;a href="https://huggingface.co/mistralai/Magistral-Small-2509#benchmark-results"&gt;benchmark results&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Better tone and persona&lt;/strong&gt;: You should experiment better LaTeX and Markdown formatting, and shorter answers on easy general prompts.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Finite generation&lt;/strong&gt;: The model is less likely to enter infinite generation loops.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Special think tokens&lt;/strong&gt;: [THINK] and [/THINK] special tokens encapsulate the reasoning content in a thinking chunk. This makes it easier to parse the reasoning trace and prevents confusion when the '[THINK]' token is given as a string in the prompt.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning prompt&lt;/strong&gt;: The reasoning prompt is given in the system prompt.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Reasoning:&lt;/strong&gt; Capable of long chains of reasoning traces before providing an answer.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports dozens of languages, including English, French, German, Greek, Hindi, Indonesian, Italian, Japanese, Korean, Malay, Nepali, Polish, Portuguese, Romanian, Russian, Serbian, Spanish, Turkish, Ukrainian, Vietnamese, Arabic, Bengali, Chinese, and Farsi.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: Vision capabilities enable the model to analyze images and reason based on visual content in addition to text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Apache 2.0 License:&lt;/strong&gt; Open license allowing usage and modification for both commercial and non-commercial purposes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Window:&lt;/strong&gt; A 128k context window. Performance &lt;em&gt;might&lt;/em&gt; degrade past &lt;strong&gt;40k&lt;/strong&gt; but Magistral should still give good results. Hence we recommend to leave the maximum model length to 128k and only lower if you encounter low performance.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d0vo5ev3xqpf1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f81d6fa64a262e991112d1c8011e18d1d75b2774"&gt;https://preview.redd.it/d0vo5ev3xqpf1.png?width=1342&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f81d6fa64a262e991112d1c8011e18d1d75b2774&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgovj/magistral_small_2509_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:44:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkhn3t</id>
    <title>Problem with glm air in LMStudio</title>
    <updated>2025-09-18T19:21:33+00:00</updated>
    <author>
      <name>/u/Magnus114</name>
      <uri>https://old.reddit.com/user/Magnus114</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkhn3t/problem_with_glm_air_in_lmstudio/"&gt; &lt;img alt="Problem with glm air in LMStudio" src="https://preview.redd.it/xvt021tj4zpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cecfe08ace7af193f16c61bd5dfdcc67c200c03" title="Problem with glm air in LMStudio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I have tried to get glm 4.5 air to work with opencode. Works great when I use it via openrouter, but when I run same model locally (LMStudio) all tool call fails. Have tried different quants, but so far nothing works. &lt;/p&gt; &lt;p&gt;Anyone who have a clue? Would really appreciate suggestions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Magnus114"&gt; /u/Magnus114 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xvt021tj4zpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkhn3t/problem_with_glm_air_in_lmstudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkhn3t/problem_with_glm_air_in_lmstudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:21:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkk5tv</id>
    <title>I built APM ‚Äì a package manager for agent workflows and context (works with local LLaMA too)</title>
    <updated>2025-09-18T20:57:53+00:00</updated>
    <author>
      <name>/u/Amazing_Midnight_813</name>
      <uri>https://old.reddit.com/user/Amazing_Midnight_813</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been hacking on a side project I‚Äôm calling &lt;strong&gt;APM (Agent Package Manager)&lt;/strong&gt;. The idea is simple: treat agent prompts + workflows like code packages you can version, share, and reuse.&lt;/p&gt; &lt;p&gt;My pain point was always the same:&lt;br /&gt; ‚Äì I‚Äôd write complex prompt/rule files, but couldn‚Äôt easily reuse them in another project.&lt;br /&gt; ‚Äì Teammates would reinvent the wheel with their own context configs.&lt;br /&gt; ‚Äì There wasn‚Äôt a clean way to compose/share agent workflows across different models.&lt;/p&gt; &lt;p&gt;With APM you can:&lt;br /&gt; ‚Äì &lt;code&gt;apm init&lt;/code&gt; to create a package of rules/prompts&lt;br /&gt; ‚Äì Publish it to GitHub&lt;br /&gt; ‚Äì Pull it into any project with &lt;code&gt;apm install&lt;/code&gt;&lt;br /&gt; ‚Äì Compile everything into optimized context with &lt;code&gt;apm compile&lt;/code&gt;&lt;/p&gt; &lt;p&gt;‚Äì And then run workflows against compatible Agent CLIs (Codex today) with &lt;code&gt;apm run&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Local angle&lt;/strong&gt;: it‚Äôs built on Codex CLI under the hood `apm runtime setup codex`, which you can configure to point to your own LLaMA instance. So you can spin up workflows locally without touching cloud APIs. The architecture supports extending to other CLIs.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/danielmeppiel/apm"&gt;github.com/danielmeppiel/apm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love if someone here with a local LLaMA setup could try it out and tell me if it actually makes context management/sharing easier. Early days, so any feedback (bugs, feature ideas) is gold.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Midnight_813"&gt; /u/Amazing_Midnight_813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkk5tv/i_built_apm_a_package_manager_for_agent_workflows/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkk5tv/i_built_apm_a_package_manager_for_agent_workflows/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkk5tv/i_built_apm_a_package_manager_for_agent_workflows/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T20:57:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkigg6</id>
    <title>New stealth model Golden Capybara?</title>
    <updated>2025-09-18T19:52:29+00:00</updated>
    <author>
      <name>/u/Adept_Photograph_796</name>
      <uri>https://old.reddit.com/user/Adept_Photograph_796</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"&gt; &lt;img alt="New stealth model Golden Capybara?" src="https://b.thumbs.redditmedia.com/iMYu_qRqMZ8AGfGw81KMO1yjddk0Rr7udWRjyvqT90k.jpg" title="New stealth model Golden Capybara?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/2yjkolxa9zpf1.png?width=2806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bea303a1394a6c099d644ecb55f33b9e30da15aa"&gt;https://preview.redd.it/2yjkolxa9zpf1.png?width=2806&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bea303a1394a6c099d644ecb55f33b9e30da15aa&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Golden Capybara has been popping up in a lot of tournaments but I can't find anything about it online... thinking it's another stealth model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adept_Photograph_796"&gt; /u/Adept_Photograph_796 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkigg6/new_stealth_model_golden_capybara/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1njgicz</id>
    <title>China bans its biggest tech companies from acquiring Nvidia chips, says report ‚Äî Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D</title>
    <updated>2025-09-17T15:37:22+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"&gt; &lt;img alt="China bans its biggest tech companies from acquiring Nvidia chips, says report ‚Äî Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D" src="https://external-preview.redd.it/8TEqL7hV1nddGcswRwl5w0myECFYu5Ll4SYjP8Vx1jY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a29ca54687a9865b636eb76fe44ba0da1943af79" title="China bans its biggest tech companies from acquiring Nvidia chips, says report ‚Äî Beijing claims its homegrown AI processors now match H20 and RTX Pro 6000D" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/tech-industry/artificial-intelligence/china-bans-its-biggest-tech-companies-from-acquiring-nvidia-chips-says-report-beijing-claims-its-homegrown-ai-processors-now-match-h20-and-rtx-pro-6000d"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njgicz/china_bans_its_biggest_tech_companies_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T15:37:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nka2cl</id>
    <title>Am I the first one to run a full multi-agent workflow on an edge device?</title>
    <updated>2025-09-18T14:38:09+00:00</updated>
    <author>
      <name>/u/Abit_Anonymous</name>
      <uri>https://old.reddit.com/user/Abit_Anonymous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nka2cl/am_i_the_first_one_to_run_a_full_multiagent/"&gt; &lt;img alt="Am I the first one to run a full multi-agent workflow on an edge device?" src="https://external-preview.redd.it/cnpxNWU2MmRueHBmMYp0QtsqZN4801amkhsrr7Zpcr6-upwl98kHrYEp8oGI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43b8d55e77ae6ba3f231b599f323a6bef27f6908" title="Am I the first one to run a full multi-agent workflow on an edge device?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Discussion&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôve been messing with Jetson boards for quiet a while, but this was my first time trying to push a real multi-agent stack onto one. Instead of cloud or desktop, I wanted to see if I could get a Multi Agent AI Workflow to run end-to-end on a Jetson Orin Nano 8GB.&lt;/p&gt; &lt;p&gt;The goal: talk to the device, have it generate a PowerPoint, all locally.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup&lt;/strong&gt; ‚Ä¢ Jetson Orin Nano 8GB ‚Ä¢ CAMEL-AI framework for agent orchestration ‚Ä¢ Whisper for STT ‚Ä¢ CAMEL PPTXToolkit for slide generation ‚Ä¢ Models tested: Mistral 7B Q4, Llama 3.1 8B Q4, Qwen 2.5 7B Q4&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What actually happened&lt;/strong&gt; ‚Ä¢ Whisper crushed it. 95%+ accuracy even with noise. ‚Ä¢ CAMEL‚Äôs agent split made sense. One agent handled chat, another handled slide creation. Felt natural, no duct tape. ‚Ä¢ Jetson held up way better than I expected. 7B inference + Whisper at the same time on 8GB is wild. ‚Ä¢ The slides? Actually useful, not just generic bullets.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What broke my flow (Learnings for future too.)&lt;/strong&gt; ‚Ä¢ TTS was slooow. 15‚Äì25s per reply ‚Ä¢ Totally ruins the convo feel. ‚Ä¢ Mistral kept breaking function calls with bad JSON. ‚Ä¢ Llama 3.1 was too chunky for 8GB, constant OOM. ‚Ä¢ Qwen 2.5 7B ended up being the sweet spot.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Takeaways&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Model fit &amp;gt; model hype.&lt;/li&gt; &lt;li&gt;TTS on edge is the real bottleneck.&lt;/li&gt; &lt;li&gt;8GB is just enough, but you‚Äôre cutting it close.&lt;/li&gt; &lt;li&gt;Edge optimization is very different from cloud.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;So yeah, it worked. Multi-agent on edge is possible. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full pipeline&lt;/strong&gt; Whisper ‚Üí CAMEL agents ‚Üí PPTXToolkit ‚Üí TTS. &lt;/p&gt; &lt;p&gt;Curious if anyone else here has tried running Agentic Workflows or any other multi-agent frameworks on edge hardware? Or am I actually the first to get this running?‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã‚Äã&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abit_Anonymous"&gt; /u/Abit_Anonymous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/af1mqlrenxpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nka2cl/am_i_the_first_one_to_run_a_full_multiagent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nka2cl/am_i_the_first_one_to_run_a_full_multiagent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T14:38:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1njqt5s</id>
    <title>once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?</title>
    <updated>2025-09-17T22:07:52+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"&gt; &lt;img alt="once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?" src="https://preview.redd.it/s4cols18tspf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1ff443b6756ab190d26356690b7694db6efda4f6" title="once China is able to produce its own GPU for datacenters (which they are forced to due to both import and export bans by both China and USA), there will be less reason to release their models open weight?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s4cols18tspf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njqt5s/once_china_is_able_to_produce_its_own_gpu_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T22:07:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk1jbc</id>
    <title>I just made VRAM approximation tool for LLM</title>
    <updated>2025-09-18T07:09:12+00:00</updated>
    <author>
      <name>/u/SmilingGen</name>
      <uri>https://old.reddit.com/user/SmilingGen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a simple tool to estimate how much memory is needed to run GGUF models locally, based on your desired maximum context size.&lt;/p&gt; &lt;p&gt;You just paste the direct download URL of a GGUF model (for example, from Hugging Face), enter the context length you plan to use, and it will give you an approximate memory requirement.&lt;/p&gt; &lt;p&gt;It‚Äôs especially useful if you're trying to figure out whether a model will fit in your available VRAM or RAM, or when comparing different quantization levels like Q4_K_M vs Q8_0.&lt;/p&gt; &lt;p&gt;The tool is completely free and open-source. You can try it here: &lt;a href="https://www.kolosal.ai/memory-calculator"&gt;https://www.kolosal.ai/memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And check out the code on GitHub: &lt;a href="https://github.com/KolosalAI/model-memory-calculator"&gt;https://github.com/KolosalAI/model-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'd really appreciate any feedback, suggestions, or bug reports if you decide to give it a try.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SmilingGen"&gt; /u/SmilingGen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk1jbc/i_just_made_vram_approximation_tool_for_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T07:09:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkfvrl</id>
    <title>Local LLM Coding Stack (24GB minimum, ideal 36GB)</title>
    <updated>2025-09-18T18:14:56+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"&gt; &lt;img alt="Local LLM Coding Stack (24GB minimum, ideal 36GB)" src="https://preview.redd.it/ia5muohupypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1148b1bf986af2a5825964a50e7d6bdf8dc5dc16" title="Local LLM Coding Stack (24GB minimum, ideal 36GB)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perhaps this could be useful to someone trying to get his/her own local AI coding stack. I do scientific coding stuff, not web or application development related stuff, so the needs might be different. &lt;/p&gt; &lt;p&gt;Deployed on a 48gb Mac, but this should work on 32GB, and maybe even 24GB setups:&lt;/p&gt; &lt;p&gt;General Tasks, used 90% of the time: Cline on top of Qwen3Coder-30b-a3b. Served by LM Studio in MLX format for maximum speed. This is the backbone of everything else...&lt;/p&gt; &lt;p&gt;Difficult single script tasks, 5% of the time: QwenCode on top of GPT-OSS 20b (Reasoning effort: High). Served by LM Studio. This cannot be served at the same time of Qwen3Coder due to lack of RAM. The problem cracker. GPT-OSS can be swept with other reasoning models with tool use capabilities (Magistral, DeepSeek, ERNIE-thinking, EXAONE, etc... lot of options here)&lt;/p&gt; &lt;p&gt;Experimental, hand-made prototyping: Continue doing auto-complete work on top of Qwen2.5-Coder 7b. Served by Ollama to be always available together with the model served by LM Studio. When you need to be in the loop of creativity this is the one.&lt;/p&gt; &lt;p&gt;IDE for data exploration: Spyder&lt;/p&gt; &lt;p&gt;Long Live to Local LLM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ia5muohupypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkfvrl/local_llm_coding_stack_24gb_minimum_ideal_36gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk5df9</id>
    <title>Ryzen 6800H iGPU 680M Vulkan benchmarks llama.cpp</title>
    <updated>2025-09-18T11:11:32+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I continue to be impressed on how well iGPU perform. Here are some updated LLM benchmarks. &lt;/p&gt; &lt;p&gt;Llama.cpp with Vulkan for Ubuntu is running pretty fast especially when you throw a &lt;a href="https://www.tensorops.ai/post/what-is-mixture-of-experts-llm"&gt;MoE&lt;/a&gt; model at it.&lt;/p&gt; &lt;p&gt;AMD Ryzen 7 &lt;a href="https://www.techpowerup.com/cpu-specs/ryzen-7-6800h.c2527"&gt;6800H&lt;/a&gt; CPU with Radeon Graphics &lt;a href="https://www.techpowerup.com/gpu-specs/radeon-680m.c3871"&gt;680M&lt;/a&gt; with 64GB &lt;a href="https://en.wikipedia.org/wiki/DDR5_SDRAM"&gt;DDR5&lt;/a&gt; 4800 system RAM and &lt;a href="https://www.reddit.com/r/ollama/comments/1lbpqln/minipc_ryzen_7_6800h_cpu_and_igpu_680m/"&gt;16GB for iGPU&lt;/a&gt;. System running &lt;a href="https://kubuntu.org/"&gt;Kubuntu&lt;/a&gt; 25.10 and &lt;a href="https://docs.mesa3d.org/relnotes/25.1.7.html"&gt;Mesa 25.1.7&lt;/a&gt;-1ubuntu1.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases"&gt;Release&lt;/a&gt; llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/releases/download/b6501/llama-b6478-bin-ubuntu-vulkan-x64.zip"&gt;Vulkan&lt;/a&gt; build: 28c39da7 (6478)&lt;/p&gt; &lt;p&gt;Using &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/7195"&gt;llama-bench&lt;/a&gt; sorted by &lt;a href="https://web.dev/articles/llm-sizes"&gt;Parameter size&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size GiB&lt;/th&gt; &lt;th align="left"&gt;Params B&lt;/th&gt; &lt;th align="left"&gt;pp512 t/s&lt;/th&gt; &lt;th align="left"&gt;tg128 t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-3.5-MoE-instruct-IQ4_NL.gguf&lt;/td&gt; &lt;td align="left"&gt;21.99&lt;/td&gt; &lt;td align="left"&gt;41.87&lt;/td&gt; &lt;td align="left"&gt;95.58&lt;/td&gt; &lt;td align="left"&gt;16.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EXAONE-4.0-32B-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;18.01&lt;/td&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;30.4&lt;/td&gt; &lt;td align="left"&gt;2.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-IQ4_NL.gguf&lt;/td&gt; &lt;td align="left"&gt;16.12&lt;/td&gt; &lt;td align="left"&gt;30.53&lt;/td&gt; &lt;td align="left"&gt;150.73&lt;/td&gt; &lt;td align="left"&gt;30.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;15.25&lt;/td&gt; &lt;td align="left"&gt;30.53&lt;/td&gt; &lt;td align="left"&gt;140.24&lt;/td&gt; &lt;td align="left"&gt;28.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL.gguf&lt;/td&gt; &lt;td align="left"&gt;20.24&lt;/td&gt; &lt;td align="left"&gt;30.53&lt;/td&gt; &lt;td align="left"&gt;120.68&lt;/td&gt; &lt;td align="left"&gt;25.55&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;M-MOE-4X7B-Dark-MultiVerse-UC-E32-24B-D_AU-Q4_k_m.gguf&lt;/td&gt; &lt;td align="left"&gt;13.65&lt;/td&gt; &lt;td align="left"&gt;24.15&lt;/td&gt; &lt;td align="left"&gt;35.81&lt;/td&gt; &lt;td align="left"&gt;4.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ERNIE-4.5-21B-A3B-PT.i1-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;10.89&lt;/td&gt; &lt;td align="left"&gt;21.83&lt;/td&gt; &lt;td align="left"&gt;176.99&lt;/td&gt; &lt;td align="left"&gt;30.29&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ERNIE-4.5-21B-A3B-PT-IQ4_NL.gguf&lt;/td&gt; &lt;td align="left"&gt;11.52&lt;/td&gt; &lt;td align="left"&gt;21.83&lt;/td&gt; &lt;td align="left"&gt;196.39&lt;/td&gt; &lt;td align="left"&gt;29.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;SmallThinker-21B-A3B-Instruct.IQ4_XS.imatrix.gguf&lt;/td&gt; &lt;td align="left"&gt;10.78&lt;/td&gt; &lt;td align="left"&gt;21.51&lt;/td&gt; &lt;td align="left"&gt;155.94&lt;/td&gt; &lt;td align="left"&gt;26.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EuroLLM-9B-Instruct-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;4.7&lt;/td&gt; &lt;td align="left"&gt;9.15&lt;/td&gt; &lt;td align="left"&gt;116.78&lt;/td&gt; &lt;td align="left"&gt;12.94&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EuroLLM-9B-Instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;5.2&lt;/td&gt; &lt;td align="left"&gt;9.15&lt;/td&gt; &lt;td align="left"&gt;113.45&lt;/td&gt; &lt;td align="left"&gt;12.06&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;EuroLLM-9B-Instruct-Q6_K_L.gguf&lt;/td&gt; &lt;td align="left"&gt;7.23&lt;/td&gt; &lt;td align="left"&gt;9.15&lt;/td&gt; &lt;td align="left"&gt;110.87&lt;/td&gt; &lt;td align="left"&gt;9.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek-R1-0528-Qwen3-8B-IQ4_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;4.26&lt;/td&gt; &lt;td align="left"&gt;8.19&lt;/td&gt; &lt;td align="left"&gt;136.77&lt;/td&gt; &lt;td align="left"&gt;14.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-mini-MoE-instruct-IQ2_XS.gguf&lt;/td&gt; &lt;td align="left"&gt;2.67&lt;/td&gt; &lt;td align="left"&gt;7.65&lt;/td&gt; &lt;td align="left"&gt;347.45&lt;/td&gt; &lt;td align="left"&gt;61.27&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-mini-MoE-instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;4.65&lt;/td&gt; &lt;td align="left"&gt;7.65&lt;/td&gt; &lt;td align="left"&gt;294.85&lt;/td&gt; &lt;td align="left"&gt;40.51&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2.5-7B-Instruct.Q8_0.gguf&lt;/td&gt; &lt;td align="left"&gt;7.54&lt;/td&gt; &lt;td align="left"&gt;7.62&lt;/td&gt; &lt;td align="left"&gt;256.57&lt;/td&gt; &lt;td align="left"&gt;8.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;llama-2-7b.Q4_0.gguf&lt;/td&gt; &lt;td align="left"&gt;3.56&lt;/td&gt; &lt;td align="left"&gt;6.74&lt;/td&gt; &lt;td align="left"&gt;279.81&lt;/td&gt; &lt;td align="left"&gt;16.72&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Phi-4-mini-instruct-Q4_K_M.gguf&lt;/td&gt; &lt;td align="left"&gt;2.31&lt;/td&gt; &lt;td align="left"&gt;3.84&lt;/td&gt; &lt;td align="left"&gt;275.75&lt;/td&gt; &lt;td align="left"&gt;25.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granite-3.1-3b-a800m-instruct_f16.gguf&lt;/td&gt; &lt;td align="left"&gt;6.15&lt;/td&gt; &lt;td align="left"&gt;3.3&lt;/td&gt; &lt;td align="left"&gt;654.88&lt;/td&gt; &lt;td align="left"&gt;34.39&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk5df9/ryzen_6800h_igpu_680m_vulkan_benchmarks_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk5df9/ryzen_6800h_igpu_680m_vulkan_benchmarks_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk5df9/ryzen_6800h_igpu_680m_vulkan_benchmarks_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T11:11:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk87rk</id>
    <title>Qwen3-next-80b-a3b hits 1400 elo (also longcat-flash)</title>
    <updated>2025-09-18T13:24:45+00:00</updated>
    <author>
      <name>/u/GabryIta</name>
      <uri>https://old.reddit.com/user/GabryIta</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"&gt; &lt;img alt="Qwen3-next-80b-a3b hits 1400 elo (also longcat-flash)" src="https://a.thumbs.redditmedia.com/nVKYZwK_64TwYIgHazAhHI7N38wg6aI4LwIqXOVqdq4.jpg" title="Qwen3-next-80b-a3b hits 1400 elo (also longcat-flash)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/48ne8xrrcxpf1.png?width=2244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1e166212d7f9a2f5361e5a8ee35fead714d1a7a"&gt;https://preview.redd.it/48ne8xrrcxpf1.png?width=2244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d1e166212d7f9a2f5361e5a8ee35fead714d1a7a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I just noticed the Lmarena leaderboard has been updated, even though there‚Äôs been no announcement on social media. (lately they only post updates for major models. kind of a shame)&lt;/p&gt; &lt;p&gt;The new Qwen3-next-80b-a3b reaches 1400 ELO with just 3B active parameters&lt;br /&gt; According to the benchmark, its performance is on par with qwen3-235b-a22b and qwen3-235b-a22b-thinking-2507&lt;/p&gt; &lt;p&gt;Anyone tried it yet? Is it actually that good in real-world use?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GabryIta"&gt; /u/GabryIta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk87rk/qwen3next80ba3b_hits_1400_elo_also_longcatflash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T13:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkb0yq</id>
    <title>Latest Open-Source AMD Improvements Allowing For Better Llama.cpp AI Performance Against Windows 11</title>
    <updated>2025-09-18T15:14:48+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I was checking out the recent llama.cpp benchmarks and the data in this link shows that llama.cpp runs significantly faster on Windows 11 (25H2) than on Ubuntu for AMD GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/llama-cpp-windows-linux/3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkb0yq/latest_opensource_amd_improvements_allowing_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkb0yq/latest_opensource_amd_improvements_allowing_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T15:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkdnf2</id>
    <title>A dialogue where god tries (and fails) to prove to satan that humans can reason</title>
    <updated>2025-09-18T16:52:06+00:00</updated>
    <author>
      <name>/u/FinnFarrow</name>
      <uri>https://old.reddit.com/user/FinnFarrow</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"&gt; &lt;img alt="A dialogue where god tries (and fails) to prove to satan that humans can reason" src="https://preview.redd.it/fqm6nmw8dypf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d353499d3cd5e9c56a4af4ceac147c01524c7fe1" title="A dialogue where god tries (and fails) to prove to satan that humans can reason" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.astralcodexten.com/p/what-is-man-that-thou-art-mindful"&gt;Full article here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FinnFarrow"&gt; /u/FinnFarrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fqm6nmw8dypf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkdnf2/a_dialogue_where_god_tries_and_fails_to_prove_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:52:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkkghp</id>
    <title>Decart-AI releases ‚ÄúOpen Source Nano Banana for Video‚Äù</title>
    <updated>2025-09-18T21:09:20+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt; &lt;img alt="Decart-AI releases ‚ÄúOpen Source Nano Banana for Video‚Äù" src="https://preview.redd.it/eisyod0snzpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b42197a4268b3f80790f0506f12d7d6cfc5a4bb" title="Decart-AI releases ‚ÄúOpen Source Nano Banana for Video‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building ‚ÄúOpen Source Nano Banana for Video‚Äù - here is open source demo v0.1&lt;/p&gt; &lt;p&gt;We are open sourcing Lucy Edit, the first foundation model for text-guided video editing!&lt;/p&gt; &lt;p&gt;Lucy Edit lets you prompt to try on uniforms or costumes - with motion, face, and identity staying perfectly preserved&lt;/p&gt; &lt;p&gt;Get the model on @huggingface ü§ó, API on @FAL, and nodes on @ComfyUI üßµ&lt;/p&gt; &lt;p&gt;X post: &lt;a href="https://x.com/decartai/status/1968769793567207528?s=46"&gt;https://x.com/decartai/status/1968769793567207528?s=46&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face: &lt;a href="https://huggingface.co/decart-ai/Lucy-Edit-Dev"&gt;https://huggingface.co/decart-ai/Lucy-Edit-Dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Lucy Edit Node on ComfyUI: &lt;a href="https://github.com/decartAI/lucy-edit-comfyui"&gt;https://github.com/decartAI/lucy-edit-comfyui&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/eisyod0snzpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkkghp/decartai_releases_open_source_nano_banana_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T21:09:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkgh1u</id>
    <title>RX 7700 launched with 2560 cores (relatively few) and 16GB memory with 624 GB/s bandwidth (relatively high)</title>
    <updated>2025-09-18T18:37:07+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkgh1u/rx_7700_launched_with_2560_cores_relatively_few/"&gt; &lt;img alt="RX 7700 launched with 2560 cores (relatively few) and 16GB memory with 624 GB/s bandwidth (relatively high)" src="https://external-preview.redd.it/ZtmeYdNQChVt_XKulmUOe_WPIvyPIS1JhHlC4A0Lp38.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb90e99fc8917d6295abd4272b2623d002b47934" title="RX 7700 launched with 2560 cores (relatively few) and 16GB memory with 624 GB/s bandwidth (relatively high)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This seems like an LLM GPU. Lot‚Äôs of bandwidth compared to compute.&lt;/p&gt; &lt;p&gt;See &lt;a href="https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7700.html"&gt;https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7700.html&lt;/a&gt; for the full specs&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://videocardz.com/newz/amd-launches-radeon-rx-7700-with-2560-cores-and-16gb-memory"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkgh1u/rx_7700_launched_with_2560_cores_relatively_few/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkgh1u/rx_7700_launched_with_2560_cores_relatively_few/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:37:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkcbwp</id>
    <title>GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with "/think" command anywhere in the system prompt.</title>
    <updated>2025-09-18T16:03:16+00:00</updated>
    <author>
      <name>/u/-Ellary-</name>
      <uri>https://old.reddit.com/user/-Ellary-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"&gt; &lt;img alt="GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." src="https://b.thumbs.redditmedia.com/-CO-B8aIjArUbhh-kSj2q3zmOt03NEOLl8XW32plQVQ.jpg" title="GLM 4.5 Air - Jinja Template Modification (Based on Unsloth's) - No thinking by default - straight quick answers, need thinking? simple activation with &amp;quot;/think&amp;quot; command anywhere in the system prompt." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-Ellary-"&gt; /u/-Ellary- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nkcbwp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkcbwp/glm_45_air_jinja_template_modification_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T16:03:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkh88k</id>
    <title>Can you guess what model you're talking to in 5 prompts?</title>
    <updated>2025-09-18T19:05:38+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"&gt; &lt;img alt="Can you guess what model you're talking to in 5 prompts?" src="https://external-preview.redd.it/NTh4aG80cW8xenBmMfQ6ULqGkcZNtZeiwHOodBaY1uWCovO-Ocod72xeRKh_.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bb846ec47ccac331a0bed323c431ac8221cc38d8" title="Can you guess what model you're talking to in 5 prompts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I made a &lt;a href="https://whichllama.com"&gt;web version&lt;/a&gt; of the WhichLlama? bot in our Discord server (you should &lt;a href="https://discord.gg/bNQP7DcQ"&gt;join&lt;/a&gt;!) to share here. I think my own &amp;quot;LLM palate&amp;quot; isn't refined enough to tell models apart (drawing an analogy to coffee and wine tasting).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/y7dajeso1zpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkh88k/can_you_guess_what_model_youre_talking_to_in_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:05:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk706v</id>
    <title>Qwen Next is my new go to model</title>
    <updated>2025-09-18T12:32:30+00:00</updated>
    <author>
      <name>/u/Miserable-Dare5090</name>
      <uri>https://old.reddit.com/user/Miserable-Dare5090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It is blazing fast, made 25 back to back tool calls with no errors, both as mxfp4 and qx86hi quants. I had been unable to test until now, and previously OSS-120B had become my main model due to speed/tool calling efficiency. Qwen delivered! &lt;/p&gt; &lt;p&gt;Have not tested coding, or RP (I am not interested in RP, my use is as a true assistant, running tasks). what are the issues that people have found? i prefer it to Qwen 235 which I can run at 6 bits atm. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Miserable-Dare5090"&gt; /u/Miserable-Dare5090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk706v/qwen_next_is_my_new_go_to_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T12:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkjpu3</id>
    <title>Model: Qwen3 Next Pull Request llama.cpp</title>
    <updated>2025-09-18T20:40:40+00:00</updated>
    <author>
      <name>/u/Loskas2025</name>
      <uri>https://old.reddit.com/user/Loskas2025</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt; &lt;img alt="Model: Qwen3 Next Pull Request llama.cpp" src="https://a.thumbs.redditmedia.com/_7Yokv2mOxidJMZ0PGNDJ03EhjWToQTSVgNpJKxH3R0.jpg" title="Model: Qwen3 Next Pull Request llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/yy1fvsujizpf1.png?width=424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabb8a9874c6f2d8ac968673d1a4d84bf1f4eec0"&gt;https://preview.redd.it/yy1fvsujizpf1.png?width=424&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fabb8a9874c6f2d8ac968673d1a4d84bf1f4eec0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're fighting with you guys! Maximum support!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Loskas2025"&gt; /u/Loskas2025 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkjpu3/model_qwen3_next_pull_request_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T20:40:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkbrk1</id>
    <title>Local Suno just dropped</title>
    <updated>2025-09-18T15:42:25+00:00</updated>
    <author>
      <name>/u/Different_Fix_2217</name>
      <uri>https://old.reddit.com/user/Different_Fix_2217</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors"&gt;https://huggingface.co/fredconex/SongBloom-Safetensors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/fredconex/ComfyUI-SongBloom"&gt;https://github.com/fredconex/ComfyUI-SongBloom&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Examples:&lt;br /&gt; &lt;a href="https://files.catbox.moe/i0iple.flac"&gt;https://files.catbox.moe/i0iple.flac&lt;/a&gt;&lt;br /&gt; &lt;a href="https://files.catbox.moe/96i90x.flac"&gt;https://files.catbox.moe/96i90x.flac&lt;/a&gt;&lt;br /&gt; &lt;a href="https://files.catbox.moe/zot9nu.flac"&gt;https://files.catbox.moe/zot9nu.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a DPO trained one that just came out &lt;a href="https://huggingface.co/fredconex/SongBloom-Safetensors/blob/main/songbloom_full_150s_dpo.safetensors"&gt;https://huggingface.co/fredconex/SongBloom-Safetensors/blob/main/songbloom_full_150s_dpo.safetensors&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Using the DPO one this was feeding it the start of Metallica fade to black and some claude generated lyrics&lt;br /&gt; &lt;a href="https://files.catbox.moe/sopv2f.flac"&gt;https://files.catbox.moe/sopv2f.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This was higher cfg / lower temp / another seed: &lt;a href="https://files.catbox.moe/olajtj.flac"&gt;https://files.catbox.moe/olajtj.flac&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Crazy leap for local&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different_Fix_2217"&gt; /u/Different_Fix_2217 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkbrk1/local_suno_just_dropped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T15:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nk7jbi</id>
    <title>NVIDIA invests 5 billions $ into Intel</title>
    <updated>2025-09-18T12:56:04+00:00</updated>
    <author>
      <name>/u/Kooky-Somewhere-2883</name>
      <uri>https://old.reddit.com/user/Kooky-Somewhere-2883</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"&gt; &lt;img alt="NVIDIA invests 5 billions $ into Intel" src="https://external-preview.redd.it/n5kP5NRletQy7r254iQxj6sHk25NybFeHBeqLvZxjz8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5ddf7ad07021e40c3004f38a19f49697e1cd4cc6" title="NVIDIA invests 5 billions $ into Intel" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bizarre news, so NVIDIA is like 99% of the market now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooky-Somewhere-2883"&gt; /u/Kooky-Somewhere-2883 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/09/18/intel-nvidia-investment.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nk7jbi/nvidia_invests_5_billions_into_intel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T12:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkieo3</id>
    <title>PSA it costs authors $12,690 to make a Nature article Open Access</title>
    <updated>2025-09-18T19:50:42+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"&gt; &lt;img alt="PSA it costs authors $12,690 to make a Nature article Open Access" src="https://preview.redd.it/xkcal9zq9zpf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07dcfaf4df77e0f86644296480de4064b0f6ca22" title="PSA it costs authors $12,690 to make a Nature article Open Access" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;And the DeepSeek folks paid up so we can read their work without hitting a paywall. Massive respect for absorbing the costs so the public benefits.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xkcal9zq9zpf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkieo3/psa_it_costs_authors_12690_to_make_a_nature/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T19:50:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building üî®&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio üëæ&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
