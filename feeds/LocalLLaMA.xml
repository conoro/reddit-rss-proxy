<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-18T19:48:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p039e3</id>
    <title>Epstein emails graph relationship extraction and visualizer</title>
    <updated>2025-11-18T05:04:27+00:00</updated>
    <author>
      <name>/u/madmax_br5</name>
      <uri>https://old.reddit.com/user/madmax_br5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"&gt; &lt;img alt="Epstein emails graph relationship extraction and visualizer" src="https://b.thumbs.redditmedia.com/5MRfJ06E2zDuBlR4SudLQG-kry1JtdebITN8KKt2jKc.jpg" title="Epstein emails graph relationship extraction and visualizer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o5pig6lx5y1g1.png?width=3434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85d3e0204d9eb558e49791cc41b6789a62c6296"&gt;https://preview.redd.it/o5pig6lx5y1g1.png?width=3434&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c85d3e0204d9eb558e49791cc41b6789a62c6296&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I built this visualizer with the help of claude code: &lt;a href="https://github.com/maxandrews/Epstein-doc-explorer"&gt;https://github.com/maxandrews/Epstein-doc-explorer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There is a hosted version linked in the repo, I can't paste it here because reddit inexplicably banned the link sitewide (see my post history for details if you're interested). &lt;/p&gt; &lt;p&gt;It uses the claude agents framework (so you can use your MAX plan inference budget if you have one) to extract relationships triple, tags, and other metadata from the documents, then clusters tags with qwen instruct embeddings, dedupes actor names into an alias table, and serves it all in a nice UI. If you don't have a max plan, you can fork and refactor to use any other capable LLM. &lt;/p&gt; &lt;h1&gt;Analysis Pipeline Features&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/maxandrews/Epstein-doc-explorer#analysis-pipeline-features"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Powered Extraction:&lt;/strong&gt; Uses Claude to extract entities, relationships, and events from documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Tagging:&lt;/strong&gt; Automatically tags triples with contextual metadata (legal, financial, travel, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tag Clustering:&lt;/strong&gt; Groups 28,000+ tags into 30 semantic clusters using K-means for better filtering&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Entity Deduplication:&lt;/strong&gt; Merges duplicate entities using LLM-based similarity detection&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Incremental Processing:&lt;/strong&gt; Supports analyzing new documents without reprocessing everything&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Top-3 Cluster Assignment:&lt;/strong&gt; Each relationship is assigned to its 3 most relevant tag clusters&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Visualization Features&lt;/h1&gt; &lt;p&gt;&lt;a href="https://github.com/maxandrews/Epstein-doc-explorer#visualization-features"&gt;&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Interactive Network Graph:&lt;/strong&gt; Force-directed graph with 15,000+ relationships&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Actor-Centric Views:&lt;/strong&gt; Click any actor to see their specific relationships&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Filtering:&lt;/strong&gt; Filter by 30 content categories (Legal, Financial, Travel, etc.)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Timeline View:&lt;/strong&gt; Chronological relationship browser with document links&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Viewer:&lt;/strong&gt; Full-text document display with highlighting&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Responsive Design:&lt;/strong&gt; Works on desktop and mobile devices&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance Optimized:&lt;/strong&gt; Uses materialized database columns for fast filtering&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madmax_br5"&gt; /u/madmax_br5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p039e3/epstein_emails_graph_relationship_extraction_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T05:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozo2v8</id>
    <title>Do we rely too much on huggingface? Do you think they‚Äôll eventually regulate open source models? Is there any way to distribute them elsewhere?</title>
    <updated>2025-11-17T18:27:54+00:00</updated>
    <author>
      <name>/u/Borkato</name>
      <uri>https://old.reddit.com/user/Borkato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know torrenting may be a thing, but I‚Äôm also just curious if anyone knows anything or has any insight.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Borkato"&gt; /u/Borkato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozo2v8/do_we_rely_too_much_on_huggingface_do_you_think/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozo2v8/do_we_rely_too_much_on_huggingface_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozo2v8/do_we_rely_too_much_on_huggingface_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:27:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0iqgw</id>
    <title>LibreChat first impressions</title>
    <updated>2025-11-18T17:51:38+00:00</updated>
    <author>
      <name>/u/DHasselhoff77</name>
      <uri>https://old.reddit.com/user/DHasselhoff77</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm setting up an instance for about five users on a cheap virtual private server. I'm using Mistral's API but from the point of view of the app it's a &amp;quot;custom endpoint&amp;quot; so I suppose this will apply to other non-OpenAI vendors as well. &lt;/p&gt; &lt;p&gt;First of all, LibreChat was easy to get running. Their guide on &lt;code&gt;docker compose&lt;/code&gt; worked perfectly and it was quick to test things both locally and on a Ubuntu server. They ship an example config and docker compose override file, which is great. The documentation also had clear examples how to add a user from the command line.&lt;/p&gt; &lt;p&gt;The configuration process itself was a confusing experience because the contents are spread between environment variables and &lt;code&gt;librechat.yaml&lt;/code&gt;. For example, I wanted to configure a custom model. I had to add an element to &lt;code&gt;endpoints: custom&lt;/code&gt; list in the YAML, which was nicely signposted with commented-out sections. But to configure which models are shown in the UI (I wanted to hide unused ones), it's a list stored in a string in the &lt;code&gt;ENDPOINTS&lt;/code&gt; env var. Took almost an hour to figure that out... Also, the app starts even with an invalid YAML in the config.&lt;/p&gt; &lt;p&gt;Once I got the Mistral models running, I could chat and also upload images. Both work fine. Image upload was a bit clunky because the web UI always asks if you'd like to locally OCR the image or &amp;quot;send it to the provider&amp;quot;. Speaking of the web UI, it works fine. It's model selector has a nice search, side panels can be opened and closed. There's support for temporary chats but they can't be made the default though (Kagi Assistant does this).&lt;/p&gt; &lt;p&gt;Custom system prompts and sampling parameters must be added via &amp;quot;agents&amp;quot;. In fact, I had to go back and set that same env var to &lt;code&gt;ENDPOINTS=custom,agents&lt;/code&gt;, to be able to even change the system prompt. This seemed to work OK and apparently you can also share prompts between users.&lt;/p&gt; &lt;p&gt;I had a quick test with the built-in RAG but couldn't get it to work. The docs helpfully showed how to change compose to run a different image, but I had to piece together myself that another env var (&lt;code&gt;OLLAMA_BASE_URL=http://host.docker.internal:11434&lt;/code&gt;) had to be added for it actually run. This resulted in &amp;quot;400 status code (no body)&amp;quot; errors somewhere in the stack, an unresolved issue mentioned already four months ago: &lt;a href="https://github.com/danny-avila/LibreChat/discussions/8389"&gt;https://github.com/danny-avila/LibreChat/discussions/8389&lt;/a&gt; &lt;a href="https://github.com/danny-avila/LibreChat/discussions/7847"&gt;https://github.com/danny-avila/LibreChat/discussions/7847&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not 100% convinced in the quality of the engineering in this project (it uses MongoDB, after all) but I'll continue to try to get the RAG work before making my final judgement.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DHasselhoff77"&gt; /u/DHasselhoff77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iqgw/librechat_first_impressions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iqgw/librechat_first_impressions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iqgw/librechat_first_impressions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T17:51:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0b5d5</id>
    <title>Qwen is the winner</title>
    <updated>2025-11-18T12:53:40+00:00</updated>
    <author>
      <name>/u/rogerrabbit29</name>
      <uri>https://old.reddit.com/user/rogerrabbit29</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0b5d5/qwen_is_the_winner/"&gt; &lt;img alt="Qwen is the winner" src="https://b.thumbs.redditmedia.com/traImpcKdT9dKEEYKavcTI2l3tpfEjZ5v66fWlO-RaU.jpg" title="Qwen is the winner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran GPT 5, Qwen 3, Gemini 2.5, and Claude Sonnet 4.5 all at once through MGX's race mode, to simulate and predict the COMEX gold futures trend for the past month.&lt;/p&gt; &lt;p&gt;Here's how it went: Qwen actually came out on top, with predictions closest to the actual market data. Gemini kind of missed the mark though, I think it misinterpreted the prompt and just gave a single daily prediction instead of the full trend. As for GPT 5, it ran for about half an hour and never actually finished. Not sure if it's a stability issue with GPT 5 in race mode, or maybe just network problems.&lt;/p&gt; &lt;p&gt;I'll probably test each model separately when I have more time. This was just a quick experiment, so I took a shortcut with MGX since running all four models simultaneously seemed like a time saver. This result is just for fun, no need to take it too seriously, lol.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/e4tsi2nui02g1.jpg?width=2190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=968697a04e44582ffd89368e798fbac2f7fda92f"&gt;https://preview.redd.it/e4tsi2nui02g1.jpg?width=2190&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=968697a04e44582ffd89368e798fbac2f7fda92f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/murxs68vi02g1.jpg?width=1693&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=06dd3e33746b577842ef081063b3cc3b332715d4"&gt;https://preview.redd.it/murxs68vi02g1.jpg?width=1693&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=06dd3e33746b577842ef081063b3cc3b332715d4&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rogerrabbit29"&gt; /u/rogerrabbit29 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0b5d5/qwen_is_the_winner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0b5d5/qwen_is_the_winner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0b5d5/qwen_is_the_winner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T12:53:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0e5a6</id>
    <title>Long Term Memory - Mem0/Zep/LangMem - what made you choose it?</title>
    <updated>2025-11-18T15:00:19+00:00</updated>
    <author>
      <name>/u/nicoloboschi</name>
      <uri>https://old.reddit.com/user/nicoloboschi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm evaluating memory solutions for AI agents and curious about real-world experiences.&lt;/p&gt; &lt;p&gt;For those using Mem0, Zep, or similar tools:&lt;/p&gt; &lt;p&gt;- What initially attracted you to it?&lt;/p&gt; &lt;p&gt;- What's working well?&lt;/p&gt; &lt;p&gt;- What pain points remain?&lt;/p&gt; &lt;p&gt;- What would make you switch to something else?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicoloboschi"&gt; /u/nicoloboschi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0e5a6/long_term_memory_mem0zeplangmem_what_made_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0e5a6/long_term_memory_mem0zeplangmem_what_made_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0e5a6/long_term_memory_mem0zeplangmem_what_made_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:00:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozy72c</id>
    <title>Baguettotron, a 321 million parameters generalist Small Reasoning Model (80-layers deep)</title>
    <updated>2025-11-18T01:02:19+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozy72c/baguettotron_a_321_million_parameters_generalist/"&gt; &lt;img alt="Baguettotron, a 321 million parameters generalist Small Reasoning Model (80-layers deep)" src="https://external-preview.redd.it/Y2tKjEjozUln8VtzJeRPc_zDKjaxfbVqC-L3XOBXmQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=153a2353c4c45555147bd60bb8b6ef7ddf0c6d9d" title="Baguettotron, a 321 million parameters generalist Small Reasoning Model (80-layers deep)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Baguettotron is a 321 million parameters generalist Small Reasoning Model, trained on 200 billions tokens from SYNTH, a fully open generalist dataset.&lt;/p&gt; &lt;p&gt;Despite being trained on consideraly less data, Baguettotron outperforms most SLM of the same size range on non-code industry benchmarks, providing an unprecedented balance between memory, general reasoning, math and retrieval performance.&lt;/p&gt; &lt;p&gt;The name is both a nod to French origins and to the unusual shape of the model: with 80 layers, Baguettotron is currently the deepest SLM in its size range.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/PleIAs/Baguettotron"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozy72c/baguettotron_a_321_million_parameters_generalist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozy72c/baguettotron_a_321_million_parameters_generalist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T01:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0fd8i</id>
    <title>Curiosity is All You Need</title>
    <updated>2025-11-18T15:47:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.10395"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0fd8i/curiosity_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0fd8i/curiosity_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:47:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozre2i</id>
    <title>NanoGPT 124m from scratch using a 4090 and a billion tokens of Fineweb in a cave with a box of scraps.</title>
    <updated>2025-11-17T20:29:18+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozre2i/nanogpt_124m_from_scratch_using_a_4090_and_a/"&gt; &lt;img alt="NanoGPT 124m from scratch using a 4090 and a billion tokens of Fineweb in a cave with a box of scraps." src="https://external-preview.redd.it/Xkp-uBD1eaeELSsY4T0RZUFyVZGTUIdapJjtKGQFbjY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=19349af1d39c198a50fecbc1f1e139ec105a188f" title="NanoGPT 124m from scratch using a 4090 and a billion tokens of Fineweb in a cave with a box of scraps." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Need a buddy and only have a few hours to make one? &lt;/p&gt; &lt;p&gt;I was recently doing some digging into NanoGPT, Karpathy's couple years old &lt;a href="https://github.com/karpathy/nanoGPT"&gt;repo&lt;/a&gt; to recreate GPT-2 124m using 10 billion tokens of fineweb and 8xA100 40gb over the course of four days.&lt;/p&gt; &lt;p&gt;More recently, I saw that they've started &lt;a href="https://github.com/KellerJordan/modded-nanogpt"&gt;speedrunning efforts&lt;/a&gt; to train the same model to 3.28 loss as fast as possible with 8xH100, and currently the speed record on that setup is less than 3 minutes to train from scratch.&lt;/p&gt; &lt;p&gt;That led me to think... with all of the advancements that have been made in the last few years, how fast could I train the same model to that 3.28 loss range on a single 4090?&lt;/p&gt; &lt;p&gt;The answer? 115 minutes flat. It ran through 0.92 billion tokens in the process, with 130-140k t/s speeds during training.&lt;/p&gt; &lt;p&gt;What does this mean?&lt;/p&gt; &lt;p&gt;If you ever find yourself lonely in a cave with a box of scraps, a 4090, and a billion fineweb tokens... you can build your own teeny-jarvis in a couple hours flat then chat with it. I've provided training code and inference code, and the trained model if you want to mess with it for some odd reason. I set up a little github repo as well, so if you feel like trying your hands at modifying my training run and beating it, drop a PR with your results/log/training run and I'll add it to the speedrun chart:&lt;br /&gt; &lt;a href="https://github.com/Deveraux-Parker/nanoGPT_1GPU_SPEEDRUN"&gt;https://github.com/Deveraux-Parker/nanoGPT_1GPU_SPEEDRUN&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I haven't bothered with any posttraining/finetuning/etc etc etc, this is just the base model trained up from nothing. I might go through and add a little instruct tune on top of it so that I can create a teeny little chatgpt. &lt;/p&gt; &lt;p&gt;Here's the list of things it's implementing:&lt;br /&gt; &lt;strong&gt;Computation &amp;amp; Precision Optimizations&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;FP8 Quantization&lt;/strong&gt; - 8-bit floating-point numbers (float8) for matrix multiplications instead of the usual 16 or 32-bit. This cuts memory use and speeds up math operations dramatically.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mixed Precision Training (bfloat16)&lt;/strong&gt; - Most computations happen in bfloat16, which is faster than float32 while maintaining good numerical stability.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Triton Kernels&lt;/strong&gt; - Hand-written GPU kernels for specific operations like symmetric matrix multiplication (X¬∑X^T), which are faster than PyTorch's default implementations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;torch.compile&lt;/strong&gt; - PyTorch 2.0's JIT compilation that fuses operations and optimizes the computational graph.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flash Attention&lt;/strong&gt; - Ultra-fast attention implementation that reduces memory usage and speeds up the attention mechanism.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Novel Optimizer &amp;amp; Training Techniques&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Muon Optimizer&lt;/strong&gt; - A custom momentum-based optimizer that uses orthogonalization (keeping gradient directions independent) for better convergence.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Polar Express Orthogonalization&lt;/strong&gt; - A specific algorithm to maintain orthogonality in the Muon optimizer's updates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;NorMuon Variance Estimator&lt;/strong&gt; - Adaptive second moment estimation that helps Muon scale gradients appropriately.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multiple Optimizers&lt;/strong&gt; - Using Adam for embeddings/scalars and Muon for weight matrices, each optimized for their parameter type.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Alternating Optimizer Steps&lt;/strong&gt; - Muon runs every other step, both optimizers on odd steps, reducing computational overhead.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gradient Accumulation&lt;/strong&gt; - Accumulating gradients over 32 micro-batches to simulate larger batch sizes without running out of memory.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Architecture Innovations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;YaRN (Yet another RoPE extensioN)&lt;/strong&gt; - Extends the context length capability of Rotary Position Embeddings beyond what the model was trained on.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RoPE (Rotary Position Embeddings)&lt;/strong&gt; - More efficient positional encoding than absolute positions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RMS Normalization&lt;/strong&gt; - Simpler and faster than LayerNorm while being equally effective.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Squared ReLU Activation&lt;/strong&gt; - Using ReLU(x)¬≤ instead of GELU, which is faster and works well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Skip Connections with Learnable Gates&lt;/strong&gt; - U-Net-style architecture where early layers connect to later layers through learned gates.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Value Embeddings&lt;/strong&gt; - Separate embedding tables that inject information directly into attention values.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smear Gating&lt;/strong&gt; - Mixes each token with the previous token using a learned gate.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Backout Connections&lt;/strong&gt; - Subtracts certain layer outputs to prevent feature redundancy.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Attention Gating&lt;/strong&gt; - Per-head gates that learn to selectively use attention outputs.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Learning Rate &amp;amp; Schedule Optimizations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Custom LR Multipliers&lt;/strong&gt; - Different learning rates for embeddings (75x), scalars (5x), etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Custom Weight Decay Multipliers&lt;/strong&gt; - Different regularization strength for different parameter types.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Warmup-Stable-Decay Schedule&lt;/strong&gt; - Linear warmup (100 steps), stable plateau (80% of training), then cosine decay.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dynamic Muon Momentum&lt;/strong&gt; - Momentum coefficient that changes during training (0.85‚Üí0.95‚Üí0.85).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Adaptive Hyperparameter Tuning&lt;/strong&gt; - Automatically adjusts learning rate and weight decay based on train/val loss dynamics.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Memory &amp;amp; Data Optimizations&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Expandable Memory Segments&lt;/strong&gt; - PyTorch memory allocator setting that reduces fragmentation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Kernel Warmup&lt;/strong&gt; - Pre-compiling and warming up kernels before actual training to avoid first-step slowdown.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asynchronous Data Loading&lt;/strong&gt; - Background threads preload the next data shard while training continues.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;BOS-Aligned Batching&lt;/strong&gt; - Sequences are aligned to document boundaries (BOS tokens) for more natural training.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pin Memory&lt;/strong&gt; - Keeps data in page-locked memory for faster CPU‚ÜíGPU transfers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Non-Blocking Transfers&lt;/strong&gt; - Async GPU transfers that overlap with computation.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;set_to_none=True&lt;/strong&gt; - More efficient way to zero gradients than setting them to zero tensors.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Training Efficiency Tricks&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Variable Attention Window Sizes&lt;/strong&gt; - Different layers use different block masking sizes (some see more context, some less).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logit Capping&lt;/strong&gt; - Applies 30¬∑sigmoid(logits/7.5) to prevent extreme values.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vocabulary Size Rounding&lt;/strong&gt; - Rounds vocab to multiples of 128 for better GPU utilization.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strategic Initialization&lt;/strong&gt; - Zero initialization for output projections, uniform bounded for inputs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Checkpoint Resumption&lt;/strong&gt; - Can pause and resume training without losing progress.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Early Stopping&lt;/strong&gt; - Automatically stops when target validation loss is reached.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Frequent Checkpointing&lt;/strong&gt; - Saves model every validation step to prevent data loss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Gradient Zeroing&lt;/strong&gt; - Only zeroes gradients after they're used, not before.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/DevParker/NanoGPT-124m-In-A-Cave-With-A-Box-Of-Scraps/blob/main/README.md"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozre2i/nanogpt_124m_from_scratch_using_a_4090_and_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozre2i/nanogpt_124m_from_scratch_using_a_4090_and_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T20:29:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kdcc</id>
    <title>DR Tulu: An open, end-to-end training recipe for long-form deep research</title>
    <updated>2025-11-18T18:51:23+00:00</updated>
    <author>
      <name>/u/ai2_official</name>
      <uri>https://old.reddit.com/user/ai2_official</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt; &lt;img alt="DR Tulu: An open, end-to-end training recipe for long-form deep research" src="https://preview.redd.it/6z12rgxba22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44824e149eda9e20a1c7b45b09ec52f394824e96" title="DR Tulu: An open, end-to-end training recipe for long-form deep research" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;What Ai2 is releasing&lt;/h1&gt; &lt;p&gt;We‚Äôre making available the entirety of our DR Tulu research and training stack under a permissive license.&lt;/p&gt; &lt;p&gt;Releasing all of DR Tulu‚Äôs components serves three goals. First, it enables reproducibility and transparency: we release our curated prompt datasets, training and evaluation code (including our RLER implementation), and our 8B model checkpoint so others can replicate our results and study how reward functions and tool configurations shape behavior. Second, it provides deployment flexibility‚Äîyou can run the agent with your own MCP tool stack, infrastructure, and privacy constraints. Third, it supports extensibility: the dr-agent-lib agent library lets you plug in domain-specific tools and retrieval systems without retraining by simply describing new tools to the model. Taken together, these artifacts make DR Tulu the first fully open, end-to-end deep research framework.&lt;/p&gt; &lt;p&gt;We encourage you to experiment with different tool configurations, audit the agent‚Äôs research steps, and test how DR Tulu handles your domain's research questions. If you find issues or ways to improve the approach, we'd love to hear about them.&lt;/p&gt; &lt;p&gt;üìö Blog: &lt;a href="https://allenai.org/blog/dr-tulu"&gt;https://allenai.org/blog/dr-tulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚úèÔ∏è Paper: &lt;a href="http://allenai.org/papers/drtulu"&gt;http://allenai.org/papers/drtulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Models: &lt;a href="https://huggingface.co/collections/rl-research/dr-tulu"&gt;https://huggingface.co/collections/rl-research/dr-tulu&lt;/a&gt;&lt;/p&gt; &lt;p&gt;‚å®Ô∏è Code: &lt;a href="https://github.com/rlresearch/DR-Tulu"&gt;https://github.com/rlresearch/DR-Tulu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ai2_official"&gt; /u/ai2_official &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/6z12rgxba22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdcc/dr_tulu_an_open_endtoend_training_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:51:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0lnlo</id>
    <title>Make your AI talk like a caveman and decrease token usage</title>
    <updated>2025-11-18T19:39:38+00:00</updated>
    <author>
      <name>/u/RegionCareful7282</name>
      <uri>https://old.reddit.com/user/RegionCareful7282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt; &lt;img alt="Make your AI talk like a caveman and decrease token usage" src="https://preview.redd.it/7g67ftgti22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7d9207d83386575ef61218ed4c0a30301826b10" title="Make your AI talk like a caveman and decrease token usage" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been working on a little side project to help LLMs talk like‚Ä¶ cavemen.&lt;br /&gt; Why? To save tokens, of course. &lt;/p&gt; &lt;p&gt;It works because LLMs can easily fill in grammar and connectives on their own. So we strip what‚Äôs predictable, keep what‚Äôs meaningful, and the model still understands everything perfectly. &lt;/p&gt; &lt;p&gt;Store RAG documents in caveman-compressed form so each chunk carries more valuable data, fits more context, and gives better retrieval quality.&lt;/p&gt; &lt;p&gt;Thought I'd share it here as it might be beneficial in order to not waste tokens on unnecessary words :)&lt;/p&gt; &lt;p&gt;Feel free to contribute if you have any additions!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/wilpel/caveman-compression"&gt;https://github.com/wilpel/caveman-compression&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RegionCareful7282"&gt; /u/RegionCareful7282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/7g67ftgti22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0lnlo/make_your_ai_talk_like_a_caveman_and_decrease/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T19:39:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p07cva</id>
    <title>Kimi is the best open-source AI with the least hallucinations</title>
    <updated>2025-11-18T09:15:39+00:00</updated>
    <author>
      <name>/u/xiaoruhao</name>
      <uri>https://old.reddit.com/user/xiaoruhao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p07cva/kimi_is_the_best_opensource_ai_with_the_least/"&gt; &lt;img alt="Kimi is the best open-source AI with the least hallucinations" src="https://b.thumbs.redditmedia.com/XGTp9yxVUYXfqqGs0K_Q2pcDH-ORhYULZEYo9Lnv0-M.jpg" title="Kimi is the best open-source AI with the least hallucinations" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/i7bzp350gz1g1.jpg?width=4096&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=dde01936ef6e44c5e9679d5fafd9f4e8e7c3d300"&gt;https://preview.redd.it/i7bzp350gz1g1.jpg?width=4096&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=dde01936ef6e44c5e9679d5fafd9f4e8e7c3d300&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Bigger is better?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xiaoruhao"&gt; /u/xiaoruhao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p07cva/kimi_is_the_best_opensource_ai_with_the_least/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p07cva/kimi_is_the_best_opensource_ai_with_the_least/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p07cva/kimi_is_the_best_opensource_ai_with_the_least/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T09:15:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kdqf</id>
    <title>That jump in ARC-AGI-2 score from Gemini 3</title>
    <updated>2025-11-18T18:51:48+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"&gt; &lt;img alt="That jump in ARC-AGI-2 score from Gemini 3" src="https://b.thumbs.redditmedia.com/nZ1Acy54HFuRvJailXsBX9aW7Ms7ZFdoAHCltnW5c0Y.jpg" title="That jump in ARC-AGI-2 score from Gemini 3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p0kdqf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kdqf/that_jump_in_arcagi2_score_from_gemini_3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:51:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0f4r8</id>
    <title>Gemini 3 Pro vs Kimi K2 Thinking</title>
    <updated>2025-11-18T15:38:27+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone done some initial comparisons between the new Gemini 3 Pro and Kimi K2 Thinking?&lt;/p&gt; &lt;p&gt;What are their strengths/weaknesses relative to each other?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0f4r8/gemini_3_pro_vs_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:38:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0d18g</id>
    <title>Cloudfare down = ChatGPT down. Local LLM gang for the win!</title>
    <updated>2025-11-18T14:14:53+00:00</updated>
    <author>
      <name>/u/satireplusplus</name>
      <uri>https://old.reddit.com/user/satireplusplus</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"&gt; &lt;img alt="Cloudfare down = ChatGPT down. Local LLM gang for the win!" src="https://external-preview.redd.it/AGL421fF8rguq6HfntHEktFb_6D8E61a63BOf9nljqw.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0b9fcd248f726ab4f10022b9235b7e02a4c61c6" title="Cloudfare down = ChatGPT down. Local LLM gang for the win!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/satireplusplus"&gt; /u/satireplusplus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://imgur.com/a/B1K8M3f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0d18g/cloudfare_down_chatgpt_down_local_llm_gang_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T14:14:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1p06byo</id>
    <title>Another Reflection 70B Movement: "Momentum" model at movementlabs.ai is just GLM 4.6</title>
    <updated>2025-11-18T08:08:32+00:00</updated>
    <author>
      <name>/u/Broad_Travel_1825</name>
      <uri>https://old.reddit.com/user/Broad_Travel_1825</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"&gt; &lt;img alt="Another Reflection 70B Movement: &amp;quot;Momentum&amp;quot; model at movementlabs.ai is just GLM 4.6" src="https://b.thumbs.redditmedia.com/X42mpw8E4fXegIsm9hPHq-_TInjxRF4CfSnEz0VYIsQ.jpg" title="Another Reflection 70B Movement: &amp;quot;Momentum&amp;quot; model at movementlabs.ai is just GLM 4.6" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/445ltlss1z1g1.png?width=794&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=824b68302441151b9f84af3cc4916af115268a77"&gt;Front-end token substitution&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qe9um1fe3z1g1.png?width=731&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a3587782dc490d5eebd96cae0e5107a7efa3349a"&gt;A glitch token specific to GLM 4.6&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Well, well, well... What are you trying to hide?&lt;/p&gt; &lt;p&gt;Also, someone &lt;a href="https://linux.do/t/topic/1182920"&gt;here&lt;/a&gt; observed&lt;code&gt;{&amp;quot;chat&amp;quot;:&amp;quot;Celebras Error : 403&amp;quot;}&lt;/code&gt; response. The super-fast MPU+Momentum model is actually a router to cerebras/glm-4.6.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Broad_Travel_1825"&gt; /u/Broad_Travel_1825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p06byo/another_reflection_70b_movement_momentum_model_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T08:08:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0jr1f</id>
    <title>Mistral removing ton of old models from API (preparing for a new launch?)</title>
    <updated>2025-11-18T18:28:50+00:00</updated>
    <author>
      <name>/u/mpasila</name>
      <uri>https://old.reddit.com/user/mpasila</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt; &lt;img alt="Mistral removing ton of old models from API (preparing for a new launch?)" src="https://preview.redd.it/tg4zaa7b622g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=879c9f3922693c16a694f6bce7604bb1dd61da54" title="Mistral removing ton of old models from API (preparing for a new launch?)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They are going to be removing 9 (screenshot is missing one) models from their API at the end of this month. So I wonder if that means they are preparing to release something early December? I sure hope I finally get Nemo 2.0 or something... (it's been over a year since that released).&lt;br /&gt; Source: &lt;a href="https://docs.mistral.ai/getting-started/models#legacy-models"&gt;https://docs.mistral.ai/getting-started/models#legacy-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mpasila"&gt; /u/mpasila &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tg4zaa7b622g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0jr1f/mistral_removing_ton_of_old_models_from_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:28:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0euvd</id>
    <title>The world‚Äôs fastest open-source TTS: Supertonic</title>
    <updated>2025-11-18T15:27:47+00:00</updated>
    <author>
      <name>/u/ANLGBOY</name>
      <uri>https://old.reddit.com/user/ANLGBOY</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"&gt; &lt;img alt="The world‚Äôs fastest open-source TTS: Supertonic" src="https://external-preview.redd.it/YTdlbmtuc2FhMTJnMeUni0jQysE8S8tC5OeTL5WYLlemOmlkeCkLZq86D7UU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35f3083d0ba442d2daf2de8bc05be89e611809d9" title="The world‚Äôs fastest open-source TTS: Supertonic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Demo &lt;a href="https://huggingface.co/spaces/Supertone/supertonic#interactive-demo"&gt;https://huggingface.co/spaces/Supertone/supertonic#interactive-demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code &lt;a href="https://github.com/supertone-inc/supertonic"&gt;https://github.com/supertone-inc/supertonic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hello!&lt;/p&gt; &lt;p&gt;I want to share Supertonic, a newly open-sourced TTS engine that focuses on extreme speed, lightweight deployment, and real-world text understanding.&lt;/p&gt; &lt;p&gt;It‚Äôs available in 8+ programming languages: C++, C#, Java, JavaScript, Rust, Go, Swift, and Python, so you can plug it almost anywhere ‚Äî from native apps to browsers to embedded/edge devices.&lt;/p&gt; &lt;p&gt;Technical highlights are&lt;/p&gt; &lt;p&gt;(1) Lightning-speed ‚Äî Real-time factor:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; 0.001 on RTX4090&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; 0.006 on M4 Pro&lt;/p&gt; &lt;p&gt;(2) Ultra lightweight ‚Äî 66M parameters&lt;/p&gt; &lt;p&gt;(3) On-device TTS ‚Äî Complete privacy and zero network latency&lt;/p&gt; &lt;p&gt;(4) Advanced text understanding ‚Äî Handles complex, real-world inputs naturally&lt;/p&gt; &lt;p&gt;(5) Flexible deployment ‚Äî Works in browsers, mobile apps, and small edge devices&lt;/p&gt; &lt;p&gt;Regarding (4), one of my favorite test sentences is: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚Ä¢&lt;/strong&gt; He spent 10,000 JPY to buy tickets for a JYP concert.&lt;/p&gt; &lt;p&gt;Here, ‚ÄúJPY‚Äù refers to Japanese yen, while ‚ÄúJYP‚Äù refers to a name ‚Äî Supertonic handles the difference seamlessly.&lt;/p&gt; &lt;p&gt;Hope it's useful for you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ANLGBOY"&gt; /u/ANLGBOY &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/w8c1bnsaa12g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0euvd/the_worlds_fastest_opensource_tts_supertonic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T15:27:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0iayb</id>
    <title>Google Antigravity is a cursor clone</title>
    <updated>2025-11-18T17:36:03+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you love vibe coding: &lt;a href="https://antigravity.google/"&gt;https://antigravity.google/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Supports models other than gemini such as GPT-OSS. Hopefully we will get instructions for running local models soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0iayb/google_antigravity_is_a_cursor_clone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T17:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0dxns</id>
    <title>If the bubble bursts, what's gonna happen to all those chips?</title>
    <updated>2025-11-18T14:51:51+00:00</updated>
    <author>
      <name>/u/freecodeio</name>
      <uri>https://old.reddit.com/user/freecodeio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Will they become cheap? Here's hoping I can have an H200 in my garage for $1500. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freecodeio"&gt; /u/freecodeio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0dxns/if_the_bubble_bursts_whats_gonna_happen_to_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T14:51:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozu5v4</id>
    <title>20,000 Epstein Files in a single text file available to download (~100 MB)</title>
    <updated>2025-11-17T22:14:12+00:00</updated>
    <author>
      <name>/u/tensonaut</name>
      <uri>https://old.reddit.com/user/tensonaut</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've processed all the text and image files (~25,000 document pages/emails) within individual folders released last friday into a two column text file. I used Googles tesseract OCR library to convert jpg to text. &lt;/p&gt; &lt;p&gt;You can download it here: &lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I uploaded it yesterday, but some of files were incomplete. This version is full. For each document, I've included the full path to the original google drive folder from House oversight committee so you can link and verify contents.&lt;/p&gt; &lt;p&gt;I used mistral 7b to extract entities and relationships and build a basic Graph RAG. There are some new &amp;quot;associations&amp;quot; that have not been reported in the news but couldn't find any breakthrough content. Also my entity/relationship extraction was quick and dirty. Sharing this dataset for people interested in getting into RAG and digging deeper to get more insight that what meets the eye.&lt;/p&gt; &lt;p&gt;In using this dataset, please be sensitive to the privacy of the people involved (and remember that many of these people were certainly not involved in any of the actions which precipitated the investigation.) - Quoted from Enron Email Dataset release&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tensonaut"&gt; /u/tensonaut &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozu5v4/20000_epstein_files_in_a_single_text_file/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T22:14:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0kikj</id>
    <title>Gemma 4!!!</title>
    <updated>2025-11-18T18:56:50+00:00</updated>
    <author>
      <name>/u/Namra_7</name>
      <uri>https://old.reddit.com/user/Namra_7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kikj/gemma_4/"&gt; &lt;img alt="Gemma 4!!!" src="https://preview.redd.it/p1tbzwhqb22g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c5d31a16c68548f0586cb57f320d60a00e9e043" title="Gemma 4!!!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Namra_7"&gt; /u/Namra_7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p1tbzwhqb22g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kikj/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0kikj/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T18:56:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0bql2</id>
    <title>My local AI server is up and running, while ChatGPT and Claude are down due to Cloudflare's outage. Take that, big tech corps!</title>
    <updated>2025-11-18T13:20:14+00:00</updated>
    <author>
      <name>/u/alex_bit_</name>
      <uri>https://old.reddit.com/user/alex_bit_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Local servers for the win!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alex_bit_"&gt; /u/alex_bit_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0bql2/my_local_ai_server_is_up_and_running_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T13:20:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p0gjcu</id>
    <title>Gemini 3 is launched</title>
    <updated>2025-11-18T16:31:01+00:00</updated>
    <author>
      <name>/u/Several-Republic-609</name>
      <uri>https://old.reddit.com/user/Several-Republic-609</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt; &lt;img alt="Gemini 3 is launched" src="https://external-preview.redd.it/Jcgyato32sPSUDLsqQhcsyfnhHKEryk97hJ_EjIMDyU.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dc3edcd8902e26525ff2ad02160747ab3d46316e" title="Gemini 3 is launched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Several-Republic-609"&gt; /u/Several-Republic-609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.google/products/gemini/gemini-3/#note-from-ceo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p0gjcu/gemini_3_is_launched/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-18T16:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ozopxx</id>
    <title>AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)</title>
    <updated>2025-11-17T18:50:44+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/e3scgr4j5v1g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ca0bf85cb4cf2a2b7e12e8d99d515186275c15e3" title="AMA Announcement: MiniMax, The Opensource Lab Behind MiniMax-M2 + Gifts to Our Community (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/e3scgr4j5v1g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ozopxx/ama_announcement_minimax_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-17T18:50:44+00:00</published>
  </entry>
</feed>
