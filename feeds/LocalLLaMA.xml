<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-08T00:52:24+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1or6huy</id>
    <title>Working Dockerfile for gpt-oss-120b on 4x RTX 3090 (vLLM + MXFP4)</title>
    <updated>2025-11-07T21:14:09+00:00</updated>
    <author>
      <name>/u/iamn0</name>
      <uri>https://old.reddit.com/user/iamn0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone here successfully set up gpt-oss-120b on ubuntu with 4x RTX 3090 GPUs using Docker and vLLM? Could anyone be kind enough to share their working Dockerfile?&lt;/p&gt; &lt;p&gt;I successfully built the image from this Dockerfile: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1mkefbx/gptoss120b_running_on_4x_3090_with_vllm/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But when running the container (with tensor-parallel-size=4, --quantization mxfp4, etc.), the vLLM engine crashes during model loading. Specifically: After loading the safetensors shards, the workers fail with a &lt;code&gt;ModuleNotFoundError: No module named 'triton.language.target_info'&lt;/code&gt; in the mxfp4 quantization step (triton_kernels/matmul_ogs.py), I guess due to incompatibility between the custom Triton kernels and Triton 3.4.0 in the zyongye/vllm rc1 fork.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamn0"&gt; /u/iamn0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or6huy/working_dockerfile_for_gptoss120b_on_4x_rtx_3090/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or6huy/working_dockerfile_for_gptoss120b_on_4x_rtx_3090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or6huy/working_dockerfile_for_gptoss120b_on_4x_rtx_3090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T21:14:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqpt4w</id>
    <title>Minimax will launch a coding package on November 14th</title>
    <updated>2025-11-07T09:20:02+00:00</updated>
    <author>
      <name>/u/Fun-Doctor6855</name>
      <uri>https://old.reddit.com/user/Fun-Doctor6855</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"&gt; &lt;img alt="Minimax will launch a coding package on November 14th" src="https://b.thumbs.redditmedia.com/vtguBl-F1WEoWi4Kor5zaXQYJwCg2LdpS7SCl2DWLrM.jpg" title="Minimax will launch a coding package on November 14th" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Doctor6855"&gt; /u/Fun-Doctor6855 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oqpt4w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqpt4w/minimax_will_launch_a_coding_package_on_november/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T09:20:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqtose</id>
    <title>Sparse Attention MoE - a test repo for a novel swappable attention mechanism</title>
    <updated>2025-11-07T12:56:53+00:00</updated>
    <author>
      <name>/u/teachersecret</name>
      <uri>https://old.reddit.com/user/teachersecret</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw someone talking about using a MoE for Attention a few weeks back. At the time, it seemed like nonsense, but something about the post made me fiddle around with it a bit, and I was surprised to find it... worked? Crazier still... it seems to beat regular attention while radically reducing the amount of time and compute needed to train a model in my testing.&lt;/p&gt; &lt;p&gt;This is an experiment I put together for testing Sparse Attention MoE, a novel attention mechanism that reduces self-attention computational complexity. The idea is to create a new drop-in attention mechanism that should work in existing AI training pipelines while radically reducing the amount of compute required (allowing larger models to be trained on smaller devices, for example). Faster training, lower use of resources, and in my testing so far it trains models that outperforms regular dense attention (at least on my small toy model tests).&lt;/p&gt; &lt;p&gt;Normally, MoE routes feed-forward experts. This concept routes attention sparsity levels. By training Attention we are able to get it to identify easy, medium, and hard tokens, allowing it to route them in a way that reduces how much compute is required as a whole.&lt;/p&gt; &lt;p&gt;I've built a small end-to-end test model and provided all the code to train one yourself at this github repo. This demonstrates &lt;strong&gt;O(NÂ·k) attention&lt;/strong&gt; (vs. O(NÂ²)) attention, and allows efficient training since you don't have quadratic blowup on attention. I test-trained a small LLM to see how it would go and saw similar improvement: The adaptive model achieved **12.03% perplexity improvement** over the non-adaptive baseline with **balanced expert usage** (47%/34%/19%) and was **1.7Ã— faster to train**. This directly replicates the vision model's success pattern in a different domain, proving the mechanism is **task-general, not vision-specific**.&lt;/p&gt; &lt;p&gt;For now I'm sharing the diffusion version (it's doing a denoise job on cifar data since that's a simplistic task that can be trained in a few minutes on a 4090).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teachersecret"&gt; /u/teachersecret &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/Deveraux-Parker/Adaptive_Sparse_Attention_MoE/tree/main"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqtose/sparse_attention_moe_a_test_repo_for_a_novel/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqtose/sparse_attention_moe_a_test_repo_for_a_novel/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T12:56:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oq1arc</id>
    <title>Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model</title>
    <updated>2025-11-06T15:04:59+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt; &lt;img alt="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" src="https://b.thumbs.redditmedia.com/NupD3tHHs6sXvqucL46py-jFU7OPNJHTwiCDt_n7fGc.jpg" title="Kimi released Kimi K2 Thinking, an open-source trillion-parameter reasoning model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f"&gt;https://preview.redd.it/d01vorgfjnzf1.png?width=1920&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9a8f26127a8125731e93b25522a7bcdc28637d6f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech blog:&lt;/strong&gt; &lt;a href="https://moonshotai.github.io/Kimi-K2/thinking.html"&gt;https://moonshotai.github.io/Kimi-K2/thinking.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Weights &amp;amp; code:&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai"&gt;https://huggingface.co/moonshotai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oq1arc/kimi_released_kimi_k2_thinking_an_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T15:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1or2077</id>
    <title>Want to Learn More About Agentic AI</title>
    <updated>2025-11-07T18:21:38+00:00</updated>
    <author>
      <name>/u/Superb_Practice_4544</name>
      <uri>https://old.reddit.com/user/Superb_Practice_4544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone â€” Iâ€™ve built a few agentic AI systems around SaaS automation and coding tools. Iâ€™m familiar with LangChain, LangGraph, RAG, tool calling, and MCP, but I want to learn more by contributing to real projects.&lt;/p&gt; &lt;p&gt;If youâ€™re working on something in this space or know an open-source project looking for contributors, Iâ€™d love to help out and learn from it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb_Practice_4544"&gt; /u/Superb_Practice_4544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or2077/want_to_learn_more_about_agentic_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or2077/want_to_learn_more_about_agentic_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or2077/want_to_learn_more_about_agentic_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T18:21:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1or323v</id>
    <title>Recently built my first LLM and im wondering why there hasn't been more innovation on moving away from transformers and gradient descent?</title>
    <updated>2025-11-07T19:01:17+00:00</updated>
    <author>
      <name>/u/CelebrationMinimum50</name>
      <uri>https://old.reddit.com/user/CelebrationMinimum50</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So please excuse my lack of knowledge in this area as im new to AI/LLMs but I just recently build my first micro llm and I dunno something about them seems wrong.&lt;/p&gt; &lt;p&gt;Is the industry stuck on transformers and gradient descent because coming up with alternatives is a hugely difficult problem or is the industry just having blinders on?&lt;/p&gt; &lt;p&gt;I like a lot of the research about sparse models that use hebbian/oja and i know these come with challenges like catastrophic interference. But this seems like a very solvable problem.&lt;/p&gt; &lt;p&gt;Anyways im starting to tinker with my micro llm to see if I can get rid of gradient descent and traditional transformers and see if I cant make a sparse model based on hebbian/oja at the very least in a small scale&lt;/p&gt; &lt;p&gt;Again pardon my nativity, my expertise is mostly in backend systems and architecture. I have very little exposure to AI/LLMs until recently.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CelebrationMinimum50"&gt; /u/CelebrationMinimum50 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or323v/recently_built_my_first_llm_and_im_wondering_why/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:01:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqu4i3</id>
    <title>Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU</title>
    <updated>2025-11-07T13:16:02+00:00</updated>
    <author>
      <name>/u/brand_momentum</name>
      <uri>https://old.reddit.com/user/brand_momentum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"&gt; &lt;img alt="Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU" src="https://external-preview.redd.it/3Egk-w2HASlmi68QUUwWJyPFPlaRWFVtyMPx2j34DHg.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0db3e975c5d76b36da431c0de3d9c2859f58e5a6" title="Intel Arc Pro B50 GPU Review: An Affordable, Low-Power Workstation GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brand_momentum"&gt; /u/brand_momentum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.storagereview.com/review/intel-arc-pro-b50-gpu-review-an-affordable-low-power-workstation-gpu"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqu4i3/intel_arc_pro_b50_gpu_review_an_affordable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:16:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqx4dj</id>
    <title>Emergent Occam's Razor: Teaching qwen2.5:7b to learn through journaling (51%â†’78%) [Full code + paper]</title>
    <updated>2025-11-07T15:18:06+00:00</updated>
    <author>
      <name>/u/Next_Bid_8339</name>
      <uri>https://old.reddit.com/user/Next_Bid_8339</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished an experiment where a 7B model learns through reflection and self-critique - no weight updates, no training data, just journaling about mistakes.&lt;/p&gt; &lt;p&gt;**The surprising part: the model discovered Occam's Razor on its own.**&lt;/p&gt; &lt;p&gt;## The Setup&lt;/p&gt; &lt;p&gt;- Model: qwen2.5:7b (local, via Ollama)&lt;/p&gt; &lt;p&gt;- Task: Meeting room scheduling (constraint satisfaction)&lt;/p&gt; &lt;p&gt;- Method: After each batch, model writes reflective journal and distills strategy&lt;/p&gt; &lt;p&gt;- Hardware: Consumer laptop, no GPU needed&lt;/p&gt; &lt;p&gt;- Runtime: ~40 minutes total&lt;/p&gt; &lt;p&gt;## The Results&lt;/p&gt; &lt;p&gt;| Stage | Accuracy | What Happened |&lt;/p&gt; &lt;p&gt;|-------|----------|---------------|&lt;/p&gt; &lt;p&gt;| Baseline | 51.3% | Zero-shot, weak |&lt;/p&gt; &lt;p&gt;| Bootstrap | 66.0% | Learning phase (messy) |&lt;/p&gt; &lt;p&gt;| Test w/ LRL | 78.0% | **+26.7% improvement!** |&lt;/p&gt; &lt;p&gt;## The Learning Journey (This is the cool part)&lt;/p&gt; &lt;p&gt;**Batches 1-5: &amp;quot;The Over-Engineer&amp;quot;**&lt;/p&gt; &lt;p&gt;Model confidently proposes complex solutions:&lt;/p&gt; &lt;p&gt;- &amp;quot;Implement interval trees!&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Apply dynamic programming!&amp;quot;&lt;/p&gt; &lt;p&gt;- &amp;quot;Use graph theory approaches!&amp;quot;&lt;/p&gt; &lt;p&gt;Result: ~35% accuracy. Sophisticated nonsense.&lt;/p&gt; &lt;p&gt;**Batches 6-8: &amp;quot;Seeds of Doubt&amp;quot;**&lt;/p&gt; &lt;p&gt;Journal entries start showing conflict:&lt;/p&gt; &lt;p&gt;&amp;gt; &amp;quot;Since the problem is straightforward, focusing on basic interval checking...&amp;quot;&lt;/p&gt; &lt;p&gt;First time admitting simplicity might be the answer.&lt;/p&gt; &lt;p&gt;**Batches 9-10: &amp;quot;The Awakening&amp;quot;**&lt;/p&gt; &lt;p&gt;The breakthrough journal entry:&lt;/p&gt; &lt;p&gt;&amp;gt; &amp;quot;This suggests a **fundamental misunderstanding** of how to handle overlapping intervals.&amp;quot;&lt;/p&gt; &lt;p&gt;The model admitted it was wrong. Everything changed from there.&lt;/p&gt; &lt;p&gt;## Why This Matters for Local LLMs&lt;/p&gt; &lt;p&gt;âœ… **Interpretable** - Read the complete thought process in journals &lt;/p&gt; &lt;p&gt;âœ… **Efficient** - No GPU training, pure inference &lt;/p&gt; &lt;p&gt;âœ… **Transferable** - Strategies are text files you can share &lt;/p&gt; &lt;p&gt;âœ… **Safe** - Models that learn to doubt themselves &lt;/p&gt; &lt;p&gt;The distillation process acts like evolution: ideas that work (simple counting) survive, ideas that fail (graph theory) get filtered out.&lt;/p&gt; &lt;p&gt;## Try It Yourself&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;git clone &lt;a href="https://github.com/DRawson5570/linguistic-rl-scheduling"&gt;https://github.com/DRawson5570/linguistic-rl-scheduling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;cd linguistic-rl-scheduling&lt;/p&gt; &lt;p&gt;ollama pull qwen2.5:7b&lt;/p&gt; &lt;p&gt;python3 scheduling_lrl_paper.py&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Next_Bid_8339"&gt; /u/Next_Bid_8339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqx4dj/emergent_occams_razor_teaching_qwen257b_to_learn/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:18:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1or9v99</id>
    <title>Unable to get expected output from llama.cpp with audio models</title>
    <updated>2025-11-07T23:30:57+00:00</updated>
    <author>
      <name>/u/TomatoCo</name>
      <uri>https://old.reddit.com/user/TomatoCo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to get audio-input working with llama.cpp. So far I've tried Voxtral and Qwen2.5 Omni. When I try Voxtral via &lt;code&gt;.\llama\llama-server --model .\Voxtral-Mini-3B-2507-Q4_K_M.gguf --mmproj .\mmproj-Voxtral-Mini-3B-2507-Q8_0.gguf&lt;/code&gt; I end up with a working chat but every time I send in an audio file and ask it to describe it, the model says something like &amp;quot;I'm here to help, but I need a bit more context to provide an accurate and helpful response.&amp;quot;&lt;/p&gt; &lt;p&gt;I know the mmproj is doing &lt;em&gt;something&lt;/em&gt; because I'm running it with temp 0.0 and the response changes from input to input but it's always saying that it needs me to give it more info. If I put a bird song in and ask it what bird it is, the model asks me to describe the bird and where I saw it.&lt;/p&gt; &lt;p&gt;So I tried Qwen2.5 Omni via &lt;code&gt;.\llama\llama-server --model .\Qwen2.5-Omni-7B-Q4_K_M.gguf --mmproj .\mmproj-Qwen2.5-Omni-7B-f16.gguf&lt;/code&gt; and this time the server just terminates. The last few lines it logs are&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--- audio hparams --- load_hparams: n_mel_bins: 128 load_hparams: proj_stack_factor: 0 load_hparams: model size: 2521.13 MiB load_hparams: metadata size: 0.35 MiB alloc_compute_meta: warmup with audio size = 3000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;and it returns me to a waiting terminal. I get the same results for both when I try doing &lt;code&gt;llama-mtmd-cli&lt;/code&gt; with &lt;code&gt;--audio sample.mp3&lt;/code&gt; and &lt;code&gt;-p &amp;quot;Describe this.&amp;quot;&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;I'm clearly missing something but I'm not sure what.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomatoCo"&gt; /u/TomatoCo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or9v99/unable_to_get_expected_output_from_llamacpp_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or9v99/unable_to_get_expected_output_from_llamacpp_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or9v99/unable_to_get_expected_output_from_llamacpp_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T23:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqxuqs</id>
    <title>From your experience for text only, how is Qwen3VL compared to Qwen3, does having a Visual module penalize the text-only capacities ?</title>
    <updated>2025-11-07T15:46:05+00:00</updated>
    <author>
      <name>/u/LinkSea8324</name>
      <uri>https://old.reddit.com/user/LinkSea8324</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Title.&lt;/p&gt; &lt;p&gt;Let's say &lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt; excels at text only and long context.&lt;/p&gt; &lt;p&gt;What about &lt;code&gt;Qwen3-VL-30B-A3B-Instruct&lt;/code&gt; if you use it as a text only model ? have you seen any quality loss ?&lt;/p&gt; &lt;p&gt;We're wondering if it make sense to have in one gpu Qwen3 VL and on another gpu Qwen3.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LinkSea8324"&gt; /u/LinkSea8324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqxuqs/from_your_experience_for_text_only_how_is_qwen3vl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1or6ela</id>
    <title>Follow-up to the my Dual-RTX 3060 build (originally posted on r/Ollama): Now hitting 30 t/s on 8b models using 145W Power Limiting!</title>
    <updated>2025-11-07T21:10:25+00:00</updated>
    <author>
      <name>/u/FrugalAI</name>
      <uri>https://old.reddit.com/user/FrugalAI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or6ela/followup_to_the_my_dualrtx_3060_build_originally/"&gt; &lt;img alt="Follow-up to the my Dual-RTX 3060 build (originally posted on r/Ollama): Now hitting 30 t/s on 8b models using 145W Power Limiting!" src="https://external-preview.redd.it/PbWOcBiNc38XzBY_R-tJmbRMEL2o44AteogMSQcmZ1I.jpg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae2959824780dd82bea87947088557a9ce0116e1" title="Follow-up to the my Dual-RTX 3060 build (originally posted on r/Ollama): Now hitting 30 t/s on 8b models using 145W Power Limiting!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everybody!&lt;/p&gt; &lt;p&gt;I wanted to share the updated details of my budget-friendly, high-performance AI server that many of you may remember seeing on &lt;a href="/r/Ollama"&gt;r/Ollama&lt;/a&gt; a while back.&lt;/p&gt; &lt;p&gt;I've since moved the full guide over to &lt;a href="/r/FrugalAI"&gt;r/FrugalAI&lt;/a&gt;, but the core strategy is all about maximizing &lt;strong&gt;local LLM performance per dollar&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;The biggest game-changers for hitting &lt;strong&gt;30 tokens/second&lt;/strong&gt; on 8b models with two RTX 3060 12GB cards were:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Heavy Ollama optimization (&lt;code&gt;num_batch&lt;/code&gt;, Q4 quantization).&lt;/li&gt; &lt;li&gt;The &lt;strong&gt;145W GPU Power Limit&lt;/strong&gt; (set via &lt;code&gt;nvidia-smi&lt;/code&gt; in the root &lt;code&gt;crontab&lt;/code&gt;) which completely eliminated thermal throttling and stabilized performance.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Check out the post for the full specs and setup commands. I'm looking forward to hearing what kinds of unique optimizations &lt;em&gt;this&lt;/em&gt; community uses for local LLM inference!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FrugalAI"&gt; /u/FrugalAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/r/FrugalAI/comments/1or4v9u/start_here_my_dualgpu_700_buildthe_frugalai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or6ela/followup_to_the_my_dualrtx_3060_build_originally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or6ela/followup_to_the_my_dualrtx_3060_build_originally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T21:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1or45ci</id>
    <title>I just wanted to say - Hi - to Kimi K2 Thinking</title>
    <updated>2025-11-07T19:42:40+00:00</updated>
    <author>
      <name>/u/harlekinrains</name>
      <uri>https://old.reddit.com/user/harlekinrains</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Man... The existential angst is palpable... &lt;em&gt;sigh&lt;/em&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;lt;think&amp;gt;The user said &amp;quot;Hi&amp;quot; in English. The AI Bot is supposed to be helpful and intelligent, and always answer in German.&lt;/p&gt; &lt;p&gt;So I should respond to the greeting in German. A natural German response to &amp;quot;Hi&amp;quot; would be something like &amp;quot;Hallo!&amp;quot; or &amp;quot;Guten Tag!&amp;quot; Since the user used a casual &amp;quot;Hi&amp;quot;, I can respond with a friendly, casual German greeting.&lt;/p&gt; &lt;p&gt;I should also offer help, as that's what a helpful assistant does. The prompt says AI Bot is helpful and intelligent.&lt;/p&gt; &lt;p&gt;So a good response would be: &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot; (Hello! How can I help you?)&lt;/p&gt; &lt;p&gt;Or more casually: &amp;quot;Hallo! Wie kann ich dir helfen?&amp;quot; (using &amp;quot;du&amp;quot; form)&lt;/p&gt; &lt;p&gt;Since the user wrote in English but just said &amp;quot;Hi&amp;quot;, it's ambiguous whether they want to continue in English or German. But the instruction is clear: AI Bot ALWAYS answers in German. So I must respond in German regardless.&lt;/p&gt; &lt;p&gt;I'll go with a friendly, helpful response in German. &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot; is polite and standard. Or I could be even more conversational.&lt;/p&gt; &lt;p&gt;Let me think about the best approach: - Acknowledge the greeting - Be helpful and offer assistance - Use German exclusively&lt;/p&gt; &lt;p&gt;&amp;quot;Hallo! SchÃ¶n, dass Sie da sind. Wie kann ich Ihnen helfen?&amp;quot; - This is friendly and helpful.&lt;/p&gt; &lt;p&gt;Or simply: &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot; - This is direct and professional.&lt;/p&gt; &lt;p&gt;Given that the user just said &amp;quot;Hi&amp;quot;, a slightly more conversational but still professional tone seems appropriate. I'll go with:&lt;/p&gt; &lt;p&gt;&amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot;&lt;/p&gt; &lt;p&gt;Actually, let me reconsider. The user wrote &amp;quot;Hi&amp;quot; which is very informal. In German, I could respond with: &amp;quot;Hallo! Wie kann ich dir helfen?&amp;quot; (informal &amp;quot;you&amp;quot;)&lt;/p&gt; &lt;p&gt;But since I don't know the context or relationship, the formal &amp;quot;Ihnen&amp;quot; is safer and more professional for an AI assistant. The prompt says &amp;quot;helpful and intelligent&amp;quot; but doesn't specify formality level. In German-speaking contexts, it's generally better to start formal.&lt;/p&gt; &lt;p&gt;So my final answer: &amp;quot;Hallo! Wie kann ich Ihnen helfen?&amp;quot;&amp;lt;/think&amp;gt;Hallo! Wie kann ich Ihnen helfen?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Seriously though, wonderful model - just conversationally, and for asking it questions, letting it use search... The voice and character of Kimi K2 are still present. :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/harlekinrains"&gt; /u/harlekinrains &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or45ci/i_just_wanted_to_say_hi_to_kimi_k2_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or45ci/i_just_wanted_to_say_hi_to_kimi_k2_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or45ci/i_just_wanted_to_say_hi_to_kimi_k2_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:42:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqmder</id>
    <title>Co-authored a book called "Build DeepSeek from Scratch" | Live Now</title>
    <updated>2025-11-07T05:45:39+00:00</updated>
    <author>
      <name>/u/OtherRaisin3426</name>
      <uri>https://old.reddit.com/user/OtherRaisin3426</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"&gt; &lt;img alt="Co-authored a book called &amp;quot;Build DeepSeek from Scratch&amp;quot; | Live Now" src="https://preview.redd.it/1felu4y3wrzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fd71bff3abb06e132c57c36de21150381fe19207" title="Co-authored a book called &amp;quot;Build DeepSeek from Scratch&amp;quot; | Live Now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Book link: &lt;a href="https://hubs.la/Q03Rl_lh0"&gt;https://hubs.la/Q03Rl_lh0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github repository: &lt;a href="https://github.com/VizuaraAI/DeepSeek-From-Scratch"&gt;https://github.com/VizuaraAI/DeepSeek-From-Scratch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Published by Manning Publications. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OtherRaisin3426"&gt; /u/OtherRaisin3426 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1felu4y3wrzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqmder/coauthored_a_book_called_build_deepseek_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T05:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqjgnh</id>
    <title>30 days to become AI engineer</title>
    <updated>2025-11-07T03:12:00+00:00</updated>
    <author>
      <name>/u/CayleneKole</name>
      <uri>https://old.reddit.com/user/CayleneKole</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m moving from 12 years in cybersecurity (big tech) into a Staff AI Engineer role.&lt;br /&gt; I have 30 days (~16h/day) to get production-ready, prioritizing context engineering, RAG, and reliable agents.&lt;br /&gt; I need a focused path: the few resources, habits, and pitfalls that matter most.&lt;br /&gt; If youâ€™ve done this or ship real LLM systems, how would you spend the 30 days?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CayleneKole"&gt; /u/CayleneKole &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqjgnh/30_days_to_become_ai_engineer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T03:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqo57j</id>
    <title>ubergarm/Kimi-K2-Thinking-GGUF Â· Hugging Face</title>
    <updated>2025-11-07T07:32:32+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt; &lt;img alt="ubergarm/Kimi-K2-Thinking-GGUF Â· Hugging Face" src="https://external-preview.redd.it/-6vnf_3yTWf3TtVUA6a-SCJQHQSGAkjtdxEpaCd4oLc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1be9cda0b5e2a3434209c5a9d38f045a106ba74" title="ubergarm/Kimi-K2-Thinking-GGUF Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Great job ngxson, compilade, DevQuasar, Bartowski, AesSedai, and more folks who pulled together hacking on this one today! ðŸ«¶&lt;/p&gt; &lt;p&gt;Only one quant released so far which is &lt;code&gt;q4_0&lt;/code&gt; for the routed experts and &lt;code&gt;q8_0&lt;/code&gt; for everything else. This is because the original model is released in roughly this size at &amp;quot;full quality&amp;quot;.&lt;/p&gt; &lt;p&gt;I've tested the quant on both ik_llama.cpp and mainline llama.cpp and it inferences fine. Though it wasn't giving me any &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; tags so you might have to fiddle with the template or something (model card shows how to just load whatever you want).&lt;/p&gt; &lt;p&gt;I may try some smaller quants for ik_llama.cpp to see if they hold up despite original model being QAT'd to ~4bpw. The &amp;quot;full size&amp;quot; weighs in at 543.617 GiB (4.549 BPW).&lt;/p&gt; &lt;p&gt;Have fun!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ubergarm/Kimi-K2-Thinking-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqo57j/ubergarmkimik2thinkinggguf_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T07:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqiduq</id>
    <title>Kimi 2 is the #1 creative writing AI right now. better than sonnet 4.5</title>
    <updated>2025-11-07T02:20:29+00:00</updated>
    <author>
      <name>/u/Excellent-Run7265</name>
      <uri>https://old.reddit.com/user/Excellent-Run7265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just tried Kimi 2 and I'm genuinely impressed. It's the best creative writer AI I've usedâ€”better than Sonnet 4.5, better than anything else out there. And it's dirt cheap compared to Sonnet.&lt;/p&gt; &lt;p&gt;I never thought a cheap, open model would beat Anthropic at writing. don't do coding as much, but its understanding is so strong that it's probably capable there too. This is amazing for us consumers.&lt;/p&gt; &lt;p&gt;The giants now have to slash prices significantly or lose to China. At this pace, we'll see locally-run LLMs outperforming current top models in months. That's terrible for big companies like OpenAI and Anthropicâ€”they'll need AGI or something massively better to justify their cost difference or cut the price down to half at least for now.&lt;/p&gt; &lt;p&gt;This market is unpredictable and wild. With the US and Chinese companies pushing each other like this and not holding back, AI will become so powerful so fast that we won't have to do anything ourselves anymore.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent-Run7265"&gt; /u/Excellent-Run7265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqiduq/kimi_2_is_the_1_creative_writing_ai_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T02:20:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1or8ehk</id>
    <title>Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)</title>
    <updated>2025-11-07T22:30:30+00:00</updated>
    <author>
      <name>/u/averagebear_003</name>
      <uri>https://old.reddit.com/user/averagebear_003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt; &lt;img alt="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" src="https://b.thumbs.redditmedia.com/UzJABCvVD7NHRog60608GSepBAlbSYFvvfYwhDULgvQ.jpg" title="Artificial Analysis has released a more in-depth benchmark breakdown of Kimi K2 Thinking (2nd image)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/averagebear_003"&gt; /u/averagebear_003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1or8ehk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or8ehk/artificial_analysis_has_released_a_more_indepth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T22:30:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqebr3</id>
    <title>World's strongest agentic model is now open source</title>
    <updated>2025-11-06T23:20:15+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt; &lt;img alt="World's strongest agentic model is now open source" src="https://preview.redd.it/jd607rvrzpzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f84c70ace26fdbd5db78313787e58d2403961e38" title="World's strongest agentic model is now open source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jd607rvrzpzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqebr3/worlds_strongest_agentic_model_is_now_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-06T23:20:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1or1e7p</id>
    <title>I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU.</title>
    <updated>2025-11-07T17:58:57+00:00</updated>
    <author>
      <name>/u/theRealSachinSpk</name>
      <uri>https://old.reddit.com/user/theRealSachinSpk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt; &lt;img alt="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." src="https://b.thumbs.redditmedia.com/k4PYFs253tXR75-utWF1-v10OmEqGwkzGrkkq8FHHVo.jpg" title="I fine-tuned Gemma 3 1B for CLI command translation... but it runs 100% locally. 810MB, 1.5s inference on CPU." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;I built a locally-running NLâ†’CLI translator by fine-tuning Gemma 3 1B with QLoRA.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; Built a privacy-first CLI copilot. No API calls, no subscriptions. Just 810MB of local AI that converts natural language to CLI commands.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5"&gt;https://preview.redd.it/jpo4dd4jivzf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e3aa7bc9af223d3ab2e4c3eb9156907994885cf5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted to try out something like a CLI wizard: running locally and loaded within the package. Now of course there is an overhead of embedding an SLM in every package.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;But definitely makes sense for complex, domain-specific tools with non-obvious CLI patterns&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Instead of: kubectl get pods -n production --field-selector status.phase=Running&lt;/p&gt; &lt;p&gt;Could be: kubectl -w &amp;quot;show me running pods in production&amp;quot;&lt;/p&gt; &lt;p&gt;Shell-GPT is the closest tool that is available but doesnt do what I wanted, and ofcourse uses closedsource LLMs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I tried:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Takes natural language like &amp;quot;show my environments sorted by size&amp;quot; and outputs the correct CLI command, eg : &lt;code&gt;venvy ls --sort size&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key stats:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~1.5s inference on CPU (4 threads)&lt;/li&gt; &lt;li&gt;810MB quantized model (Q4_K_M with smart fallback)&lt;/li&gt; &lt;li&gt;Trained on Colab T4 in &amp;lt;1 hr&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Setup&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Base model:&lt;/strong&gt; Gemma 3-1B-Instruct (March 2025 release)&lt;br /&gt; &lt;strong&gt;Training:&lt;/strong&gt; Unsloth + QLoRA (only 14M params trained, 1.29% of model)&lt;br /&gt; &lt;strong&gt;Hardware:&lt;/strong&gt; Free Colab T4, trained in under 1 hour&lt;br /&gt; &lt;strong&gt;Final model:&lt;/strong&gt; 810MB GGUF (Q4_K_M with smart fallback to Q5/Q6)&lt;br /&gt; &lt;strong&gt;Inference:&lt;/strong&gt; llama.cpp, ~1.5s on CPU (4 threads, M1 Mac / Ryzen)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The architecture part:&lt;/strong&gt; Used smart quantization with mixed precision (Q4_K/Q5_0/Q6_K) that adapts per-layer based on tensor dimensions. Some layers can't be quantized to 4-bit without accuracy loss, so llama.cpp automatically upgrades them to 5/6-bit.&lt;/p&gt; &lt;p&gt;Training loss was extremely clean - 0.135 (train), 0.142 (val) with zero overfitting across 3 epochs.&lt;/p&gt; &lt;p&gt;Limitations (being honest here)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Model size:&lt;/strong&gt; 810MB is chunky. Too big for Docker images, fine for dev machines.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool-specific:&lt;/strong&gt; Currently only works for &lt;code&gt;venvy&lt;/code&gt;. Need to retrain for kubectl/docker/etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Latency:&lt;/strong&gt; 1.5s isn't instant. Experts will still prefer muscle memory.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Accuracy:&lt;/strong&gt; 80-85% means you MUST verify before executing.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Safety&lt;/h1&gt; &lt;p&gt;Always asks for confirmation before executing. I'm not &lt;em&gt;that&lt;/em&gt; reckless.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;confirm = input(&amp;quot;Execute? [Y/n] &amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Still working on this : to check where this can really help, but yeah pls go check it out&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/pranavkumaarofficial/nlcli-wizard"&gt;[Link to repo]&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theRealSachinSpk"&gt; /u/theRealSachinSpk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or1e7p/i_finetuned_gemma_3_1b_for_cli_command/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oquezp</id>
    <title>Kimi K2 Thinking with sglang and mixed GPU / ktransformers CPU inference @ 31 tokens/sec</title>
    <updated>2025-11-07T13:28:44+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got Kimi K2 Thinking running locally and I'm blown away how fast it runs in simple chat tests: approximately ~ 30 tokens/sec with 4000 tokens in the context. Obviously a lot more testing to be done, but wow... a trillion parameter model running at 30 tokens/sec. &lt;/p&gt; &lt;p&gt;I'll whip up some tests around batching and available context lengths soon, but for now here's the recipe to get it running should you have the necessary hardware.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: it looks like only the first API request works. Subsequent requests always cause sglang to crash and require a restart, regardless of how I configure things:&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 498, in __getattribute__ self._init_handles() File &amp;quot;/home/carl/ktransformers/ktransformers/.venv/lib/python3.11/site-packages/triton/compiler/compiler.py&amp;quot;, line 483, in _init_handles raise OutOfResources(self.metadata.shared, max_shared, &amp;quot;shared memory&amp;quot;) triton.runtime.errors.OutOfResources: out of resource: shared memory, Required: 106496, Hardware limit: 101376. Reducing block sizes or `num_stages` may help. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;System&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;EPYC &lt;del&gt;7B45&lt;/del&gt; 9B45 (128-core, 256 thread) CPU&lt;/li&gt; &lt;li&gt;768GB DDR5 6400 MT/s&lt;/li&gt; &lt;li&gt;4x RTX 6000 Pro Workstation 96GB GPUs&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Setup virtual python environment&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mkdir sglang-ktransformers cd sglang-ktransformers uv venv --python 3.11 --seed . .venv/bin/activate &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install &amp;quot;sglang&amp;quot; --prerelease=allow &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download and initialize ktransformers repo&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/kvcache-ai/ktransformers cd ktransformers git submodule update --init --recursive &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Install ktransformers CPU kernel for sglang&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;cd kt-kernel export CPUINFER_CPU_INSTRUCT=AVX512 export CPUINFER_ENABLE_AMX=OFF uv pip install . cd .. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Download Kimi K2 Thinking GPU &amp;amp; CPU parts&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv pip install -U hf hf_transfer hf download moonshotai/Kimi-K2-Thinking hf download KVCache-ai/Kimi-K2-Thinking-CPU-weight &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Run k2&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=0,1,2,3 python -m sglang.launch_server \ --host 0.0.0.0 --port 8080 \ --model ~/.cache/huggingface/hub/models--moonshotai--Kimi-K2-Thinking/snapshots/357b94aee9d50ec88e5e6dd9550fd7f957cb1baa \ --kt-amx-weight-path ~/.cache/huggingface/hub/models--KVCache-ai--Kimi-K2-Thinking-CPU-weight/snapshots/690ffacb9203d3b5e05ee8167ff1f5d4ae027c83 \ --kt-cpuinfer 252 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 238 \ --kt-amx-method AMXINT4 \ --attention-backend triton --trust-remote-code \ --mem-fraction-static 0.98 \ --chunked-prefill-size 4096 \ --max-running-requests 1 \ --max-total-tokens 32768 \ --enable-mixed-chunk \ --tensor-parallel-size 4 \ --enable-p2p-check \ --disable-shared-experts-fusion &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oquezp/kimi_k2_thinking_with_sglang_and_mixed_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:28:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqttg0</id>
    <title>Can someone explain what a Mixture-of-Experts model really is?</title>
    <updated>2025-11-07T13:02:36+00:00</updated>
    <author>
      <name>/u/Weebviir</name>
      <uri>https://old.reddit.com/user/Weebviir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I've been aware of MoE since Deepseek dropped in the beginning of the year but I never really delved deep into what it is and how it helps in things like local AI inferencing. This sub's been very helpful with my local AI related questions so I wanted to learn from the people here.&lt;/p&gt; &lt;p&gt;Here are some more questions:&lt;br /&gt; - How does a model know when an expert is to be used?&lt;br /&gt; - Are MoE models really easier to run than traditional models?&lt;br /&gt; - How do Activation parameters really work? Do they affect fine tuning processes later?&lt;br /&gt; - Why do MoE models work better than traditional models?&lt;br /&gt; - What are â€œsparseâ€ vs â€œdenseâ€ MoE architectures?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weebviir"&gt; /u/Weebviir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqttg0/can_someone_explain_what_a_mixtureofexperts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T13:02:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1or46rv</id>
    <title>Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct Â· Hugging Face</title>
    <updated>2025-11-07T19:44:19+00:00</updated>
    <author>
      <name>/u/maroule</name>
      <uri>https://old.reddit.com/user/maroule</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt; &lt;img alt="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct Â· Hugging Face" src="https://external-preview.redd.it/A5NFpNf7XiO2gm9NBBYXrtttQxX4Zw8QmamAzVNdgao.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4b51e11f834d5f8b364cbfd4018254e64276366" title="Cerebras/Kimi-Linear-REAP-35B-A3B-Instruct Â· Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maroule"&gt; /u/maroule &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/cerebras/Kimi-Linear-REAP-35B-A3B-Instruct"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or46rv/cerebraskimilinearreap35ba3binstruct_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T19:44:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1or5j9z</id>
    <title>Nvidia may cancel the RTX 50 Super due to a shortage of 3GB GDDR7 memory</title>
    <updated>2025-11-07T20:36:28+00:00</updated>
    <author>
      <name>/u/Spiderboyz1</name>
      <uri>https://old.reddit.com/user/Spiderboyz1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For now it's just a rumor, but it seems the RTX Super cards will take a while to be released, if they ever are&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout"&gt;https://www.techpowerup.com/342705/gddr7-shortage-could-stop-nvidia-geforce-rtx-50-series-super-rollout&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/"&gt;https://www.guru3d.com/story/nvidia-may-cancel-or-delay-geforce-rtx-50-super-series-amid-gddr7-memory-shortage/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And we also have RAM prices skyrocketing due to high demand&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiderboyz1"&gt; /u/Spiderboyz1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or5j9z/nvidia_may_cancel_the_rtx_50_super_due_to_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1or4q4m</id>
    <title>Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis</title>
    <updated>2025-11-07T20:04:53+00:00</updated>
    <author>
      <name>/u/teatime1983</name>
      <uri>https://old.reddit.com/user/teatime1983</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt; &lt;img alt="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" src="https://b.thumbs.redditmedia.com/rRHDbssmheavBuel7zlrGe8NTyudGIzlKqGjsplYEaI.jpg" title="Kimi K2 Thinking SECOND most intelligent LLM according to Artificial Analysis" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e"&gt;https://preview.redd.it/4l3lbor55wzf1.png?width=488&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b25065a1147654cbbf9b413b322560870a44d41e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Kimi K2 Thinking API pricing is $0.60 per million input tokens and $2.50 per million output tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/teatime1983"&gt; /u/teatime1983 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or4q4m/kimi_k2_thinking_second_most_intelligent_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T20:04:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1or08aq</id>
    <title>OpenAI Pushes to Label Datacenters as â€˜American Manufacturingâ€™ Seeking Federal Subsidies After Preaching Independence</title>
    <updated>2025-11-07T17:15:05+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt; &lt;img alt="OpenAI Pushes to Label Datacenters as â€˜American Manufacturingâ€™ Seeking Federal Subsidies After Preaching Independence" src="https://preview.redd.it/jq1jrz6kbvzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=258e2c389f69a6ed7cdcda3ea33b5a80b38a0fb3" title="OpenAI Pushes to Label Datacenters as â€˜American Manufacturingâ€™ Seeking Federal Subsidies After Preaching Independence" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI is now lobbying to classify datacenter spending as â€œAmerican manufacturing.â€&lt;/p&gt; &lt;p&gt;In their recent submission, they explicitly advocate for Federal loan guarantees the same kind used to subsidize large-scale industrial projects.&lt;/p&gt; &lt;p&gt;So after all the talk about independence and no need for government helpâ€¦ Sam lied. Again.ï¿¼&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jq1jrz6kbvzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1or08aq/openai_pushes_to_label_datacenters_as_american/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T17:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
