<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-03T13:48:39+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nwywyn</id>
    <title>Any way to use 2 laptops (2x64gb ram 1x6gb vram 1x16gb together to run local LLMs) somehow?</title>
    <updated>2025-10-03T13:18:14+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone know if anything to somehow make this possible or is it even possible &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwywyn/any_way_to_use_2_laptops_2x64gb_ram_1x6gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwywyn/any_way_to_use_2_laptops_2x64gb_ram_1x6gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwywyn/any_way_to_use_2_laptops_2x64gb_ram_1x6gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:18:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nww2m7</id>
    <title>Deep dive: Optimizing LLM inference for speed &amp; efficiency — lessons learned from real-world experiments</title>
    <updated>2025-10-03T11:06:15+00:00</updated>
    <author>
      <name>/u/tony_silkworm</name>
      <uri>https://old.reddit.com/user/tony_silkworm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="http://trungtranthanh.medium.com/the-art-of-llm-inference-fast-fit-and-free-c9faf1190d78"&gt;trungtranthanh.medium.com/the-art-of-llm-inference-fast-fit-and-free-c9faf1190d78&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tony_silkworm"&gt; /u/tony_silkworm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww2m7/deep_dive_optimizing_llm_inference_for_speed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww2m7/deep_dive_optimizing_llm_inference_for_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nww2m7/deep_dive_optimizing_llm_inference_for_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:06:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwpshs</id>
    <title>Granite 4 H Tiny Q8 in RTX 3090, It's a context king.</title>
    <updated>2025-10-03T04:39:05+00:00</updated>
    <author>
      <name>/u/Plotozoario</name>
      <uri>https://old.reddit.com/user/Plotozoario</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm testing the Granite 4 H Tiny Q8 in the LM Studio, and holy moly, you can set the context window up to 1M and keep solid 50-60 tokens/s using a single RTX 3090 24Gb + 48GB RAM DDR4 3200mhz with Flash attention enabled. How far we come!! &lt;/p&gt; &lt;p&gt;Unfortunately i didn't tested yet the degradation of the model after the 100k tokens.&lt;/p&gt; &lt;p&gt;What is your vision about this new model and its new context management?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Plotozoario"&gt; /u/Plotozoario &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwpshs/granite_4_h_tiny_q8_in_rtx_3090_its_a_context_king/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T04:39:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2ghd</id>
    <title>GLM 4.6 is nice</title>
    <updated>2025-10-02T12:31:10+00:00</updated>
    <author>
      <name>/u/theodordiaconu</name>
      <uri>https://old.reddit.com/user/theodordiaconu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bit the bullet and sacrificed 3$ (lol) for a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; subscription as I can't run this behemoth locally. And because I'm a very generous dude I wanted them to keep the full margin instead of going through routers.&lt;/p&gt; &lt;p&gt;For convenience, I created a simple 'glm' bash script that starts claude with env variables (that point to z.ai). I type glm and I'm locked in.&lt;/p&gt; &lt;p&gt;Previously I experimented a lot with OW models with GPT-OSS-120B, GLM 4.5, KIMI K2 0905, Qwen3 Coder 480B (and their latest variant included which is only through 'qwen' I think) honestly they were making silly mistakes on the project or had trouble using agentic tools (many failed edits) and abandoned their use quickly in favor of the king: gpt-5-high. I couldn't even work with Sonnet 4 unless it was frontend.&lt;/p&gt; &lt;p&gt;This specific project I tested it on is an open-source framework I'm working on, and it's not very trivial to work on a framework that wants to adhere to 100% code coverage for every change, every little addition/change has impacts on tests, on documentation on lots of stuff. Before starting any task I have to feed the whole documentation.&lt;/p&gt; &lt;p&gt;GLM 4.6 is in another class for OW models. I felt like it's an equal to GPT-5-high and Claude 4.5 Sonnet. Ofcourse this is an early vibe-based assessment, so take it with a grain of sea salt.&lt;/p&gt; &lt;p&gt;Today I challenged them (Sonnet 4.5, GLM 4.6) to refactor a class that had 600+ lines. And I usually have bad experiences when asking for refactors with all models.&lt;/p&gt; &lt;p&gt;Sonnet 4.5 could not make it reach 100% on its own after refactor, started modifying existing tests and sort-of found a silly excuse for not reaching 100% it stopped at 99.87% and said that it's the testing's fault (lmao).&lt;/p&gt; &lt;p&gt;Now on the other hand, GLM 4.6, it worked for 10 mins I think?, ended up with a perfect result. It understood the assessment. They both had interestingly similar solutions to refactoring, so planning wise, both were good and looked like they really understood the task. I never leave an agent run without reading its plan first.&lt;/p&gt; &lt;p&gt;I'm not saying it's better than Sonnet 4.5 or GPT-5-High, I just tried it today, all I can say for a fact is that it's a different league for open weight, perceived on this particular project.&lt;/p&gt; &lt;p&gt;Congrats &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;&lt;br /&gt; What OW models do you use for coding?&lt;/p&gt; &lt;p&gt;LATER_EDIT: the 'bash' script since a few asked in ~/.local/bin on Mac: &lt;a href="https://pastebin.com/g9a4rtXn"&gt;https://pastebin.com/g9a4rtXn&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodordiaconu"&gt; /u/theodordiaconu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwogkl</id>
    <title>Sloppiest model!?</title>
    <updated>2025-10-03T03:27:03+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Odd request, but can anyone share the sloppiest models they have tried? I'm trying to generate data with as much AI slop (it's not this–its that / shivers-down-spines / emojis / bulleted lists / testaments &amp;amp; tapestries /etc) as possible.&lt;/p&gt; &lt;p&gt;EDIT: Thanks for the input guys! I think I found the model (Original versions of Qwen3 14B / 30BA3B with /no_think seems to do a great job :D)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwogkl/sloppiest_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:27:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwzg4h</id>
    <title>What LLMs don't sugarcoat things? I don't want an always positive take.</title>
    <updated>2025-10-03T13:39:53+00:00</updated>
    <author>
      <name>/u/read_too_many_books</name>
      <uri>https://old.reddit.com/user/read_too_many_books</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ChatGPT will clearly warp things to make you feel good.&lt;/p&gt; &lt;p&gt;I believe this has been noted by some people on the inside via Twitter as well.&lt;/p&gt; &lt;p&gt;I'd like a LLM that is more of just a transformer, than one that was neutered to promote a specific viewpoint. &lt;/p&gt; &lt;p&gt;Any suggestions appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/read_too_many_books"&gt; /u/read_too_many_books &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzg4h/what_llms_dont_sugarcoat_things_i_dont_want_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzg4h/what_llms_dont_sugarcoat_things_i_dont_want_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwzg4h/what_llms_dont_sugarcoat_things_i_dont_want_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T13:39:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwgma3</id>
    <title>A Summary of Key AI Events from September 2025</title>
    <updated>2025-10-02T21:27:57+00:00</updated>
    <author>
      <name>/u/nh_local</name>
      <uri>https://old.reddit.com/user/nh_local</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;ByteDance released &lt;strong&gt;Seedream 4.0&lt;/strong&gt;, a next-generation image model unifying high-quality text-to-image generation and natural-language image editing.&lt;/li&gt; &lt;li&gt;An advanced Gemini variant, reported as &lt;strong&gt;Gemini 2.5 - Deep Think&lt;/strong&gt;, achieved gold-medal-level performance at the ICPC World Finals programming contest.&lt;/li&gt; &lt;li&gt;OpenAI reported a reasoning and code model achieved a perfect score (12/12) in ICPC testing.&lt;/li&gt; &lt;li&gt;Suno released &lt;strong&gt;Suno v5&lt;/strong&gt;, an upgrade in music generation with studio-grade fidelity and more natural-sounding vocals.&lt;/li&gt; &lt;li&gt;Alibaba unveiled &lt;strong&gt;Qwen-3-Max&lt;/strong&gt;, its flagship model with over a trillion parameters, focusing on long context and agent capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wan 2.5&lt;/strong&gt; was released, a generative video model focused on multi-shot consistency and character animation.&lt;/li&gt; &lt;li&gt;Anthropic announced &lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt;, a model optimized for coding, agent construction, and improved reasoning.&lt;/li&gt; &lt;li&gt;OpenAI released &lt;strong&gt;Sora 2&lt;/strong&gt;, a flagship video and audio generation model with improved physical modeling and synchronized sound.&lt;/li&gt; &lt;li&gt;DeepSeek released &lt;strong&gt;DeepSeek-V3.2-Exp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;OpenAI and NVIDIA announced a strategic partnership for NVIDIA to supply at least &lt;strong&gt;10 gigawatts&lt;/strong&gt; of AI systems for OpenAI's infrastructure.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nh_local"&gt; /u/nh_local &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T21:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxitg</id>
    <title>Local LLM project ideas to appease management (with only a V100)</title>
    <updated>2025-10-03T12:17:40+00:00</updated>
    <author>
      <name>/u/IllllIIlIllIllllIIIl</name>
      <uri>https://old.reddit.com/user/IllllIIlIllIllllIIIl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Powers That Be (middle management) at my workplace have issued a vague and nebulous directive: &amp;quot;Do &lt;em&gt;something&lt;/em&gt; with LLMs so that we can say we're on board with leaderships new AI initiative.&amp;quot; They have given me no further directions and have allocated a budget of exactly $0 to do this. All I've got to work with is a single Dell R730 with dual Intel Xeon golds, 768gb of DDR4 RAM, and one ancient 16GB Nvidia Tesla V100. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Realistically, is there &lt;em&gt;anything&lt;/em&gt; remotely useful or impressive that I might be able to do with this limited hardware?&lt;/strong&gt; I was thinking I could use llama.cpp + OpenWebUI with a Confluence connector to do RAG over our internal documentation, but I suspect 16gb of VRAM might not allow a large enough context window to be all that useful. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;And suggestions for models too?&lt;/strong&gt; Without a specific idea in mind, I was looking at gpt-oss-20b (which I've tested at Q4_K_M and got about 150 tk/s, so potentially usable) or maybe Qwen3 7B or 14B. But I know a &lt;em&gt;ton&lt;/em&gt; of models have come out lately, and I'm a bit overwhelmed by the options. &lt;/p&gt; &lt;p&gt;For reference, I'm a systems engineer within a research organization that does genomics and public health. I doubt there's anything genomics related I could do, so I guess I'm more looking for generic project ideas that might sound impressive to non technical management. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Any suggestions on project ideas or models would be greatly appreciated!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllllIIlIllIllllIIIl"&gt; /u/IllllIIlIllIllllIIIl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxitg/local_llm_project_ideas_to_appease_management/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxitg/local_llm_project_ideas_to_appease_management/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxitg/local_llm_project_ideas_to_appease_management/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:17:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1nww6jy</id>
    <title>vllm setup for nvidia (can use llama)</title>
    <updated>2025-10-03T11:12:06+00:00</updated>
    <author>
      <name>/u/Superb-Security-578</name>
      <uri>https://old.reddit.com/user/Superb-Security-578</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww6jy/vllm_setup_for_nvidia_can_use_llama/"&gt; &lt;img alt="vllm setup for nvidia (can use llama)" src="https://external-preview.redd.it/g-dAVuhM7CACDLn6hnyYgfALNmbnOxqfo8XgA0k50eE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50cc15d083f6554b68138a570e8dd46add3e9da4" title="vllm setup for nvidia (can use llama)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Having recently nabbed 2x 3090 second hand and playing around with ollama, I wanted to make better use of both cards. I created this setup (based on a few blog posts) for prepping Ubuntu 24.04 and then running vllm with single or multiple GPU.&lt;/p&gt; &lt;p&gt;I thought it might make it easier for those with less technically ability. Note that I am still learning all this myself (Quantization, Context size), but it works!&lt;/p&gt; &lt;p&gt;On a clean machine this worked perfectly to then get up and running.&lt;/p&gt; &lt;p&gt;You can provide other models via flags or edit the api_server.py to change my defaults (&amp;quot;model&amp;quot;: &amp;quot;RedHatAI/gemma-3-27b-it-quantized.w4a16&amp;quot;).&lt;/p&gt; &lt;p&gt;I then use roocode in vscode to access the openAI compatible API, but other plugins should work.&lt;/p&gt; &lt;p&gt;Now back to playing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Superb-Security-578"&gt; /u/Superb-Security-578 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/aloonj/vllm-nvidia"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nww6jy/vllm_setup_for_nvidia_can_use_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nww6jy/vllm_setup_for_nvidia_can_use_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwibsb</id>
    <title>Ming V2 is out</title>
    <updated>2025-10-02T22:37:08+00:00</updated>
    <author>
      <name>/u/Chance_Camp3720</name>
      <uri>https://old.reddit.com/user/Chance_Camp3720</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ming V2 is already out&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance_Camp3720"&gt; /u/Chance_Camp3720 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T22:37:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwimej</id>
    <title>GLM 4.6 Local Gaming Rig Performance</title>
    <updated>2025-10-02T22:50:00+00:00</updated>
    <author>
      <name>/u/VoidAlchemy</name>
      <uri>https://old.reddit.com/user/VoidAlchemy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"&gt; &lt;img alt="GLM 4.6 Local Gaming Rig Performance" src="https://preview.redd.it/0peifhs11ssf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9c59d73d412700f4d4389356e00f3193eb466bc1" title="GLM 4.6 Local Gaming Rig Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sad there is no GLM-4.6-Air (seems unlikely it will be released, but who knows). So instead I cooked the &lt;code&gt;ubergarm/GLM-4.6-GGUF&lt;/code&gt; &lt;code&gt;smol-IQ2_KS&lt;/code&gt; 97.990 GiB (2.359 BPW) quant which is just a little bigger than full Q8_0 Air.&lt;/p&gt; &lt;p&gt;It is running well on my local gaming rig with 96GB RAM + 24 GB VRAM. I can get up to 32k context, or can do some trade-offs between PP and TG speeds and context length. &lt;/p&gt; &lt;p&gt;The graph is &lt;code&gt;llama-sweep-bench&lt;/code&gt; showing how quantizing kv-cache gives a steeper drop off on TG for this architecture which I observed similarly in the older GLM-4.5.&lt;/p&gt; &lt;p&gt;Have fun running quants of these big models at home on your gaming rig! The huggingface repo has some metrics comparing quality vs size trade-offs and folks over on AI Beavers Discord have a lot of KLD metrics comparing various available quants from different quant cookers so pick the right size for your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VoidAlchemy"&gt; /u/VoidAlchemy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0peifhs11ssf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwimej/glm_46_local_gaming_rig_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T22:50:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8c6y</id>
    <title>Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration</title>
    <updated>2025-10-02T16:20:56+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt; &lt;img alt="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" src="https://external-preview.redd.it/aG1yZ2k0M3Y0cXNmMTjBkk0zpHe1cUKuUpjTdKuc-czjYGWzckCtqtrm-IdD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd6b6a33eda5421f6a81ae5b65f2f068b49e13c" title="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/14cmif4v4qsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw52ad</id>
    <title>Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP</title>
    <updated>2025-10-02T14:18:05+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt; &lt;img alt="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" src="https://external-preview.redd.it/ODh3bjRsOWJpcHNmMcggpjsEMzF-IE1l8vJahmQmeeToARZwc_P-uEOcis7p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491d98e181b37d6e6d0003c442bfc14ebbed0594" title="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2wd6</id>
    <title>Granite 4.0 Language Models - a ibm-granite Collection</title>
    <updated>2025-10-02T12:51:10+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt; &lt;img alt="Granite 4.0 Language Models - a ibm-granite Collection" src="https://external-preview.redd.it/dG6nrEEPIkS2YfUpzm-ii0PPK1xkTA3ZMcynqcTCXQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374e83fed526e7600d653259e65b30be13801c21" title="Granite 4.0 Language Models - a ibm-granite Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite 4, &lt;strong&gt;32B-A9B, 7B-A1B, and 3B&lt;/strong&gt; dense models available.&lt;/p&gt; &lt;p&gt;GGUF's are in the same repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-language-models-6811a18b820ef362d9e5a82c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwuslg</id>
    <title>Qwen2.5 VL for OCR</title>
    <updated>2025-10-03T09:52:37+00:00</updated>
    <author>
      <name>/u/Jastibute</name>
      <uri>https://old.reddit.com/user/Jastibute</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been living in the dark ages up until today. I've asked ChatGPT maybe 50 questions over the years but overall I've not used AI past this. But today I discovered Qwen for OCR which sounds very interesting to me because I've had the need to scan thousands of pages of various books for a number of years now and I think finally this is becoming a possibility cheaply. I was initially looking at Tesseract and I might yet go down this route because it means not needing to buy expensive hardware or paying cloud services and it might be good enough for my needs but I would like to entertain the idea of Qwen. I would like to self host it. The only problem is video cards. I can justify one new 16GB or maybe a 20GB video card but that's it. Don't want to go into video card farming. Once I finish scanning a dozen or so books, I don't see a need for AI for me for the foreseeable future. Will continue living in the dark ages unless another use case surfaces for me.&lt;/p&gt; &lt;p&gt;Q is: I don't care about speed. I don't know how AI works but if it needs to offload to RAM and move slowly, I don't care as long as the quality is the same and it gets there eventually. I've currently got an 8GB video card. Is this capable of running say Qwen3-VL albeit slowly or does this model have a minimum requirement? I'm taking about this in the context of OCR with good quality images.&lt;/p&gt; &lt;p&gt;I have 2.5 in the heading, but found that 3 is out already while typing this up and forgot to change the heading.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jastibute"&gt; /u/Jastibute &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwuslg/qwen25_vl_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:52:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxje9</id>
    <title>SDLM 32B/4B from OpenGVLab</title>
    <updated>2025-10-03T12:18:24+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4#introduction"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4"&gt;https://huggingface.co/OpenGVLab/SDLM-32B-D4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B-D8"&gt;https://huggingface.co/OpenGVLab/SDLM-3B-D8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-3B-D4"&gt;https://huggingface.co/OpenGVLab/SDLM-3B-D4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Qwen 2.5 finetunes)&lt;/p&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We propose a &lt;strong&gt;S&lt;/strong&gt;equential &lt;strong&gt;D&lt;/strong&gt;iffusion &lt;strong&gt;L&lt;/strong&gt;anguage &lt;strong&gt;M&lt;/strong&gt;odel (&lt;strong&gt;SDLM&lt;/strong&gt;), to cheaply stimulate the parallel prediction capabilities of diffusion models. Specifically, SDLM reduces distribution shift by limiting the prediction range to a fixed block length and enforces decoding order through the longest prefix decoding method, thereby significantly improving prediction efficiency while ensuring generation quality. Our method can be viewed as a further generalization of the autoregressive (AR) paradigm. Therefore, it is possible to use pre-trained AR weights and quickly migrate to the diffusion framework with only minimal instruction fine-tuning.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/OpenGVLab/SDLM-32B-D4#overall-concept"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Overall Concept&lt;/h1&gt; &lt;p&gt;SDLM delivers strong performance with significantly faster decoding speed. It operates approximately 2x faster than comparable autoregressive models while matching their accuracy, and achieves up to 5x speedup over other diffusion language models, as evidenced by results on the MATH-500 benchmark.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxje9/sdlm_32b4b_from_opengvlab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwnlp8</id>
    <title>How has everyone been liking Granite 4?</title>
    <updated>2025-10-03T02:43:24+00:00</updated>
    <author>
      <name>/u/SpicyWangz</name>
      <uri>https://old.reddit.com/user/SpicyWangz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How does it compare to similar models for you?&lt;/p&gt; &lt;p&gt;So far I've been testing out the 7b model and it's been performing really well on my benchmarks for a model of that size. I think I've found a new go-to model for that class.&lt;/p&gt; &lt;p&gt;The output looks fairly plaintext without much formatting or markdown. I'd probably like to see a little more structure and variation from it, but I prefer plain to the table hell that I've gotten from gpt-oss-20b.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SpicyWangz"&gt; /u/SpicyWangz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwnlp8/how_has_everyone_been_liking_granite_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T02:43:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwovv8</id>
    <title>Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements</title>
    <updated>2025-10-03T03:49:33+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt; &lt;img alt="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" src="https://preview.redd.it/q7lat3zxjtsf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=db27566f8b03ab0a4d8599a0ebfc454ee0ea0790" title="Granite-4.0-H-Tiny vs. OLMoE: Rapid AI improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, just looking at some of the new model releases and wanted to share a quick comparison I made that really shows how fast things are moving in the world of open-source LLMs.&lt;/p&gt; &lt;p&gt;I've been tracking and comparing a couple of Mixture of Experts models that have a similar dense and active parameters, in this case a 7B total parameter count with 1B active parameters. With today's Granite release we can compare OLMoE, which came out in January, and the new Granite-4.0-H-Tiny model that just dropped today.&lt;/p&gt; &lt;p&gt;The side-by-side results are pretty wild for just a 10-month difference. The new Granite model is straight-up better on every single metric we can compare. It's not just a small improvement, either. We're talking huge jumps in areas like math, coding, and general knowledge.&lt;/p&gt; &lt;p&gt;Things are advancing really fast, just to give a little more perspective, the new Granite-4.0-H-Tiny has a similar MMLU score to Llama 2 70B that came out on January 2024 but the granite model can run at reasonable speeds even on a potato PC with CPU inference, I still remember the old days when people were happy that Llama 2 70B could run at 2tk/s on their machines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q7lat3zxjtsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwovv8/granite40htiny_vs_olmoe_rapid_ai_improvements/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T03:49:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwr6sb</id>
    <title>How's granite 4 small 32B going for you?</title>
    <updated>2025-10-03T06:01:18+00:00</updated>
    <author>
      <name>/u/mr_zerolith</name>
      <uri>https://old.reddit.com/user/mr_zerolith</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I notice that it's almost twice as fast as my current favorite, SEED OSS 36B. 79 tokens/sec starting from a blank context, but this speed doesn't seem to degrade as you fill up the context. &lt;/p&gt; &lt;p&gt;Accuracy on some hard questions is a little challenging ( less smart than SEED OSS ) but it does good with clarifications.&lt;br /&gt; Output length is short and to the point, doesn't spam you with emojis, fancy formatting or tables ( i like this ) &lt;/p&gt; &lt;p&gt;Memory consumption is extremely low per K of context, I don't understand how i can jack the context up to 512k and run it on a 5090. Memory usage doesn't seem to climb as i fill up the context either.&lt;/p&gt; &lt;p&gt;First impressions are good. There may be something special here. Let me know what your experiences look like.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_zerolith"&gt; /u/mr_zerolith &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwr6sb/hows_granite_4_small_32b_going_for_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T06:01:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwwoab</id>
    <title>LoRA without regrets implemented in Hugging Face TRL [colab, and python scripts]</title>
    <updated>2025-10-03T11:36:54+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;LoRA Without Regret&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;[!WARNING] I wrote this page for the TRL docs, but thought it's just drop it here in advance for anyone who can't wait. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I also made a &lt;a href="https://colab.research.google.com/drive/1wd8qYg3qWwp81N6LAJmlXASdV5GRzWcn?usp=sharing"&gt;colab notebook&lt;/a&gt; of this guide.&lt;/p&gt; &lt;p&gt;Recent research from the team at &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;Thinking Machines Lab&lt;/a&gt; (Schulman et al., 2025) shows that &lt;strong&gt;LoRA can match full fine-tuning performance&lt;/strong&gt; when configured correctly, while using only ~67% of the compute. These findings are exciting to TRL users because they're straightforward to implement and can improve model performance on smaller budgets.&lt;/p&gt; &lt;p&gt;This guide provides simple instructions to reproduce the results of the blog post in TRL.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;[!TIP] It is recommended to read the blog post before following this guide, or to consult both resources in parallel for best results.&lt;/p&gt; &lt;/blockquote&gt; &lt;h2&gt;Benefits of LoRA over full fine-tuning&lt;/h2&gt; &lt;p&gt;First of all, let's remind ourselves of the benefits of &lt;a href="https://huggingface.co/docs/trl/en/peft_integration"&gt;LoRA over full fine-tuning&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;LoRA adds adapter layers on top of the base model, which contains significantly fewer parameters than the base model itself. This design reduces GPU memory requirements and enables more efficient training. As described in the &lt;a href="https://thinkingmachines.ai/blog/lora/"&gt;blog&lt;/a&gt;, this approach was originally thought to involve a performance trade-off, although careful configuration can overcome this trade-off and match full fine-tuning performance. &lt;/p&gt; &lt;h2&gt;Examples with TRL&lt;/h2&gt; &lt;p&gt;Let's implement and train LoRA adapters in TRL scripts based on the core findings of the blog post. Afterwards, we'll revisit each finding in light of the TRL results.&lt;/p&gt; &lt;h3&gt;Supervised Fine-Tuning (SFT)&lt;/h3&gt; &lt;p&gt;The blog post performs SFT on a range of models and datasets from the Hub, which we can reproduce in TRL.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.2-1B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;allenai/tulu-3-sft-mixture&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.2-1B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;open-thoughts/OpenThoughts-114k&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;allenai/tulu-3-sft-mixture&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.1-8B"&gt;Llama-3.1-8B-Instruct&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k"&gt;open-thoughts/OpenThoughts-114k&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;uv run &amp;quot;&lt;a href="https://raw.githubusercontent.com/huggingface/trl/main/trl/scripts/sft.py"&gt;https://raw.githubusercontent.com/huggingface/trl/main/trl/scripts/sft.py&lt;/a&gt;&amp;quot; \ --model_name_or_path Qwen/Qwen2.5-3B-Instruct \ --dataset_name open-thoughts/OpenThoughts-114k \ --learning_rate 2.0e-5 \ --num_train_epochs 1 \ --packing \ --per_device_train_batch_size 2 \ --gradient_accumulation_steps 16 \ --gradient_checkpointing \ --eval_strategy no \ --use_peft \ --lora_r 256 \ --lora_alpha 16 \ --lora_target_modules all-linear \ --output_dir Qwen2.5-3B-OpenThoughts-LoRA \ --report_to trackio \ --push_to_hub&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;To run the script locally, you will need to have &lt;code&gt;uv&lt;/code&gt; installed. Check out the &lt;a href="https://docs.astral.sh/uv/"&gt;uv documentation&lt;/a&gt; for more details.&lt;/p&gt; &lt;p&gt;Once training starts, you can monitor the progress in &lt;a href="https://huggingface.co/trackio"&gt;Trackio&lt;/a&gt;, which will log the URL.&lt;/p&gt; &lt;h3&gt;Reinforcement Learning (GRPO)&lt;/h3&gt; &lt;p&gt;The blog post performs GRPO on a range of models and datasets from the Hub, and once again we can reproduce the results in TRL. &lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Dataset&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.1-8B-Base&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/openai/gsm8k"&gt;GSM8k&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama/Llama-3.2-1B"&gt;Llama-3.1-8B-Base&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/zwhe99/DeepMath-103K"&gt;DeepMath-103K&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-8b-base"&gt;Qwen3-8b-base&lt;/a&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/datasets/zwhe99/DeepMath-103K"&gt;DeepMath-103K&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;For reinforcement learning, the blog uses a math reasoning task that we can reproduce as a Python function.&lt;/p&gt; &lt;p&gt;&amp;lt;details&amp;gt; &amp;lt;summary&amp;gt;Reward function&amp;lt;/summary&amp;gt;&lt;/p&gt; &lt;p&gt;```python def strip_reasoning_accuracy_reward( completions: list[list[dict[str, str]]], solution: list[str], **kwargs ) -&amp;gt; list[Optional[float]]: &amp;quot;&amp;quot;&amp;quot;Reward function that strips reasoning tags and checks mathematical accuracy.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;This function: 1. Extracts the content from completions 2. Removes &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags (for reasoning that shouldn't be evaluated) 3. Parses both the gold solution and the predicted answer 4. Uses math_verify to check if they are mathematically equivalent Args: completions: List of model completions, each containing a list of messages solution: List of ground truth solutions **kwargs: Additional arguments (ignored but required for trainer compatibility) Returns: List of rewards where: - 1.0 if the answer is correct - 0.0 if the answer is incorrect - None if the solution is not parseable (skips this example) &amp;quot;&amp;quot;&amp;quot; contents = [completion[0][&amp;quot;content&amp;quot;] for completion in completions] rewards = [] for content, sol in zip(contents, solution): # Strip reasoning tags from completion while &amp;quot;&amp;lt;think&amp;gt;&amp;quot; in content and &amp;quot;&amp;lt;/think&amp;gt;&amp;quot; in content: start = content.find(&amp;quot;&amp;lt;think&amp;gt;&amp;quot;) end = content.find(&amp;quot;&amp;lt;/think&amp;gt;&amp;quot;, start) if start != -1 and end != -1: content = content[:start] + content[end + len(&amp;quot;&amp;lt;/think&amp;gt;&amp;quot;) :] else: break # Parse gold solution gold_parsed = parse( f&amp;quot;${sol}$&amp;quot;, extraction_config=[ LatexExtractionConfig( boxed_match_priority=0, try_extract_without_anchor=True ) ], ) if len(gold_parsed) != 0: # We require the answer to be provided in correct latex (no malformed operators) answer_parsed = parse( content, extraction_config=[ LatexExtractionConfig( boxed_match_priority=0, normalization_config=NormalizationConfig( basic_latex=True, units=True, malformed_operators=False, nits=False, boxed=True, ), try_extract_without_anchor=False, ) ], extraction_mode=&amp;quot;first_match&amp;quot;, ) # Compute binary rewards if verifiable, `None` otherwise to skip this example try: reward = float(verify(gold_parsed, answer_parsed)) except Exception as e: print( f&amp;quot;verify failed: {e}, answer: {answer_parsed}, gold: {gold_parsed}&amp;quot; ) reward = None else: # If the gold solution is not parseable, we assign `None` to skip this example reward = None rewards.append(reward) return rewards &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;&amp;lt;/details&amp;gt;&lt;/p&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;p&gt;uv run &amp;quot;&lt;a href="https://huggingface.co/datasets/burtenshaw/lora-without-regrets/resolve/main/grpo.py"&gt;https://huggingface.co/datasets/burtenshaw/lora-without-regrets/resolve/main/grpo.py&lt;/a&gt;&amp;quot; \ --model_name_or_path Qwen/Qwen3-0.6B \ --dataset_name HuggingFaceH4/OpenR1-Math-220k-default-verified \ --output_dir grpo-full-qwen3-0.6b \ --learning_rate 1.0e-6 \ --lr_scheduler_type cosine \ --warmup_ratio 0.0 \ --max_grad_norm 1.0 \ --beta 0.0 \ --max_prompt_length 1024 \ --max_completion_length 4096 \ --num_generations 16 \ --generation_batch_size 16 \ --gradient_accumulation_steps 8 \ --per_device_train_batch_size 1 \ --num_train_epochs 1 \ --lora_r 1 \ --lora_alpha 32 \ --lora_dropout 0.0 \ --lora_target_modules all-linear \ --vllm_mode colocate \ --save_strategy steps \ --save_steps 50 \ --save_total_limit 1 \ --logging_steps 1 \ --max_steps 200 \ --report_to trackio ```&lt;/p&gt; &lt;p&gt;The reinforcement learning script with GRPO is implemented as a custom script in TRL, which uses the reward function shown above. You can review it at &lt;a href="https://huggingface.co/datasets/burtenshaw/lora-without-regrets/blob/main/grpo.py"&gt;&lt;code&gt;grpo.py&lt;/code&gt;&lt;/a&gt; - Reinforcement learning with LoRA best practices&lt;/p&gt; &lt;h2&gt;Key findings in optimizing LoRA&lt;/h2&gt; &lt;p&gt;The authors recommend applying LoRA to all weight matrices rather than limiting it to attention layers, as increasing the rank does not compensate for this restriction. In TRL, this can be configured using &lt;code&gt;--lora_target_modules all-linear&lt;/code&gt; to apply LoRA to all weight matrices.&lt;/p&gt; &lt;p&gt;We were able to reproduce the results of the blog post using TRL and the SmolLM3 model. We trained the model for 500 steps on the &lt;a href="https://huggingface.co/datasets/HuggingFaceH4/OpenR1-Math-220k-default-verified"&gt;Math 220k dataset&lt;/a&gt; with the reward function and configuration above. As you can see in the figure below, the LoRA model's average train reward curve matches the full fine-tuning curve.&lt;/p&gt; &lt;p&gt;![train reward](&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/5.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/5.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;And most importantly, the LoRA model uses significantly less memory than the full fine-tuning model, as we can see in the figure below.&lt;/p&gt; &lt;p&gt;![memory usage](&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/6.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/6.png&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Here are the parameters we used to train the above models&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Parameter&lt;/th&gt; &lt;th&gt;LoRA&lt;/th&gt; &lt;th&gt;Full FT&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--model_name_or_path&lt;/code&gt;&lt;/td&gt; &lt;td&gt;HuggingFaceTB/SmolLM3-3B&lt;/td&gt; &lt;td&gt;HuggingFaceTB/SmolLM3-3B&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--dataset_name&lt;/code&gt;&lt;/td&gt; &lt;td&gt;HuggingFaceH4/OpenR1-Math-220k-default-verified&lt;/td&gt; &lt;td&gt;HuggingFaceH4/OpenR1-Math-220k-default-verified&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--learning_rate&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1.0e-6&lt;/td&gt; &lt;td&gt;1.0e-5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--max_prompt_length&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1024&lt;/td&gt; &lt;td&gt;1024&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--max_completion_length&lt;/code&gt;&lt;/td&gt; &lt;td&gt;4096&lt;/td&gt; &lt;td&gt;4096&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_r&lt;/code&gt;&lt;/td&gt; &lt;td&gt;1&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_alpha&lt;/code&gt;&lt;/td&gt; &lt;td&gt;32&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_dropout&lt;/code&gt;&lt;/td&gt; &lt;td&gt;0.0&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;code&gt;--lora_target_modules&lt;/code&gt;&lt;/td&gt; &lt;td&gt;all-linear&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Let's break down the key findings of the blog post and how we were able to reproduce them.&lt;/p&gt; &lt;h3&gt;1. &lt;em&gt;LoRA performs better when applied to all weight matrices&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The authors recommend applying LoRA to all weight matrices rather than limiting it to attention layers, as increasing the rank does not compensate for this restriction. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/1.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/1.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Attention-only LoRA underperforms even when using a higher rank to match parameter count. In TRL, this can be configured using &lt;code&gt;--lora_target_modules all-linear&lt;/code&gt; to apply LoRA to all weight matrices. In Python, we can do this like so:&lt;/p&gt; &lt;p&gt;```python from peft import LoraConfig &lt;/p&gt; &lt;p&gt;peft_config = LoraConfig(target_modules=&amp;quot;all-linear&amp;quot;)&lt;br /&gt; ```&lt;/p&gt; &lt;h3&gt;2. &lt;em&gt;The adapter needs sufficient capacity to learn from the dataset&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The blog post recommends using a sufficient LoRA rank to learn from the dataset. The rank determines the number of trainable parameters in the LoRA adapter. Therefore, &amp;quot;For datasets that exceed LoRA capacity, LoRA underperforms FullFT&amp;quot;. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/3.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/3.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the TRL script, we could use &lt;code&gt;--lora_r&lt;/code&gt; to set the rank and adapt it based on the task and dataset we're training on. The blog post recommends the following ranks based on the task and dataset size:&lt;/p&gt; &lt;p&gt;Reinforcement learning tasks typically require lower capacity, so smaller LoRA ranks can be used. This is because policy gradient algorithms extract roughly ~1 bit of information per episode, demanding minimal parameter capacity. &lt;/p&gt; &lt;p&gt;The blog post defines the ideal dataset size for LoRA to match full fine-tuning as &amp;quot;Post-training scale&amp;quot;. Which we can use to determine the recommended rank for SFT and RL LoRAs as:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Task Type&lt;/th&gt; &lt;th&gt;Dataset Size&lt;/th&gt; &lt;th&gt;Recommended Rank&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;SFT&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Post-training scale&lt;/td&gt; &lt;td&gt;256&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;RL&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Any size&lt;/td&gt; &lt;td&gt;1-32&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;3. &lt;em&gt;&amp;quot;FullFT and high-rank LoRAs have similar learning curves&amp;quot;&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;Counterintuitively, the blog post recommends using similar learning rates to full fine-tuning. In the TRL script, we could use &lt;code&gt;--learning_rate&lt;/code&gt; to set the learning rate. The \( \frac{1}{r} \) scaling in LoRA makes the optimal learning rate approximately rank-independent.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/2.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/2.png&lt;/a&gt;&lt;/p&gt; &lt;h3&gt;4. &lt;em&gt;&amp;quot;In some scenarios, LoRA is less tolerant of large batch sizes than full fine-tuning.&amp;quot;&lt;/em&gt;&lt;/h3&gt; &lt;p&gt;The blog post recommends using an effective batch size &amp;lt; 32 because the authors found LoRA to be less tolerant of large batch sizes. This could not be mitigated by increasing the LoRA rank. In the TRL script, we could use &lt;code&gt;--per_device_train_batch_size&lt;/code&gt; and &lt;code&gt;--gradient_accumulation_steps&lt;/code&gt; to set the batch size.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/4.png"&gt;https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/lora_without_regret/4.png&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Takeaways&lt;/h2&gt; &lt;p&gt;Using TRL, you can efficiently implement LoRA adapters to match full fine-tuning performance, applying the core insights (targeting all weight matrices, choosing the right rank, and managing batch size and learning rate) without the heavy compute cost of FullFT.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwwoab/lora_without_regrets_implemented_in_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwwoab/lora_without_regrets_implemented_in_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwwoab/lora_without_regrets_implemented_in_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwkzq7</id>
    <title>Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data</title>
    <updated>2025-10-03T00:36:48+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt; &lt;img alt="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" src="https://external-preview.redd.it/nBFUIJw0Ejvd09O6shC9aA8_DA1taNSIvE_cak2wtlo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac886e4b9ca714a3746fa6d670ba959d7721d3a2" title="Huawei Develop New LLM Quantization Method (SINQ) that's 30x Faster than AWQ and Beats Calibrated Methods Without Needing Any Calibration Data" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2509.22944"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwkzq7/huawei_develop_new_llm_quantization_method_sinq/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T00:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwukd5</id>
    <title>Granite4 -1M context window, and no one even noticed?</title>
    <updated>2025-10-03T09:38:28+00:00</updated>
    <author>
      <name>/u/Western_Courage_6563</name>
      <uri>https://old.reddit.com/user/Western_Courage_6563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How is it, when IBM drops a model, no one notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Western_Courage_6563"&gt; /u/Western_Courage_6563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwukd5/granite4_1m_context_window_and_no_one_even_noticed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T09:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwxxya</id>
    <title>Bought a used 5090 only to find out it was tampered with</title>
    <updated>2025-10-03T12:36:48+00:00</updated>
    <author>
      <name>/u/a201905</name>
      <uri>https://old.reddit.com/user/a201905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a angry/disappointment/frustration post from someone who was very excited at the opportunity to upgrade from 3080 to a 5090 at a discount to run local LLM.&lt;/p&gt; &lt;p&gt;A MSI rtx 5090 came up at my local, trustworthy auction house and I won it for around $2k. It was a stretch on my budget but it was too good of an opportunity so I jumped on it. I was extremely excited and upgraded the PSU but when I tried to put everything together, the system would not boot. I tried everything for hours until I remembered reading the article about people stealing GPU cores. &lt;/p&gt; &lt;p&gt;So I looked at the back and noticed the warranty tamper sticker was voided. i looked back at the auction site and I can see the image they posted with the screw tampered. I was blinded by the potential happiness this was going to bring me and I just didn't pay attention.&lt;/p&gt; &lt;p&gt;What a disappointment. Why do people do this garbage to others. I hope karma bites you in the ass. &lt;/p&gt; &lt;p&gt;Edit: I should have been clearer, i opened it and it's missing the core. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a201905"&gt; /u/a201905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwxxya/bought_a_used_5090_only_to_find_out_it_was/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T12:36:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwv4q0</id>
    <title>A list of models released or udpated last week on this sub, in case you missed any (3rd Oct)</title>
    <updated>2025-10-03T10:12:59+00:00</updated>
    <author>
      <name>/u/aifeed-fyi</name>
      <uri>https://old.reddit.com/user/aifeed-fyi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We had an interesting week in releases this week (Open &amp;amp; Closed).&lt;/p&gt; &lt;p&gt;Here is the weekly list of models, I found discussed on LocalLlama &lt;em&gt;this week.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Please update or let me know in the comments if there are any mistakes or misses. Good Friday!&lt;/p&gt; &lt;h1&gt;Model Releases &amp;amp; Updates&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Description&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;HF / GH&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;GLM-4.6&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 200k ctx&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu75l3/glm_46_is_out_and_its_going_against_claude_45/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.6"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;DeepSeek-V3.2-Exp&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM exp/base&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nte4j1/deepseekaideepseekv32exp_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Granite 4.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;IBM LLM collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Ming V2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Multimodal collection&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nwibsb/ming_v2_is_out/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/inclusionAI/ming-v2-68ddea4954413c128d706630"&gt;HF Collection&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;LFM2-Audio-1.5&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Audio&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/gallery/1nvltym"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-Audio-1.5B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.liquid.ai/blog/introducing-liquid-nanos-frontier-grade-performance-on-everyday-devices"&gt;LiquidAI&lt;/a&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt; &lt;/a&gt;nanos&lt;/td&gt; &lt;td align="left"&gt;Small task LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuyjp9/liquidai_bet_on_small_but_mighty_model/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/collections/LiquidAI/liquid-nanos-68b98d898414dd94d4d5f99a"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Qwen3 Omni AWQ&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;30B 4bit AWQ&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nt2l57/qwen3_omni_awq_released/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/cpatonn/Qwen3-Omni-30B-A3B-Instruct-AWQ-4bit"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Ring-1T-preview&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;1T reasoning 50B Active&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu44n4/1t_open_source_reasoning_model_with_50b_activation/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T-preview"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;RingFlash linea r 2&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;LLM 104B MOE&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Ling-mini-2.0&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;16B LLM&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu1rul/lingmini20_finally_almost_here_lets_push_context/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;InternVL3_5 Flash&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Vision-language&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;K2-Think 32B&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;32B reasoning&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/LLM360/K2-Think-32B"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Apriel-1.5-15b-Thinker&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;15B multimodal&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nv5uw8/dont_sleep_on_apriel1515bthinker_and_snowpiercer/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/ServiceNow-AI/Apriel-1.5-15b-Thinker"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;VibeVoice 1.8.0 (8-bit)&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;8-bit speech&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nuxdd4/release_finally_a_working_8bit_quantized/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/FabioSarracino/VibeVoice-Large-Q8"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;N&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;eutts-air&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;TTS model&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;HF&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;🧰 Resources &amp;amp; Tools&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Name&lt;/th&gt; &lt;th align="left"&gt;Type&lt;/th&gt; &lt;th align="left"&gt;Reddit&lt;/th&gt; &lt;th align="left"&gt;Link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Onyx&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Open-source Chat UI&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;–&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Kroko ASR&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Speech recognition&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ntiua9/we_just_opensourced_kroko_asr_a_fast_streaming/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://kroko.ai"&gt;kroko.ai&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;MGM-Omni&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Omni chatbot&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nu3slg/an_opensource_omni_chatbot_for_long_speech_and/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://github.com/dvlab-research/MGM-Omni"&gt;GitHub&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;monkeSearch Report&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;Research/benchmark&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;Reddit&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;a href="https://monkesearch.github.io/"&gt;monkesearch.github.io&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aifeed-fyi"&gt; /u/aifeed-fyi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwv4q0/a_list_of_models_released_or_udpated_last_week_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T10:12:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwx1rx</id>
    <title>The most important AI paper of the decade. No debate</title>
    <updated>2025-10-03T11:55:32+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt; &lt;img alt="The most important AI paper of the decade. No debate" src="https://preview.redd.it/d2rcvb6nyvsf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0051a67e9886e507e2b0a35679f4d469050fda91" title="The most important AI paper of the decade. No debate" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/d2rcvb6nyvsf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwx1rx/the_most_important_ai_paper_of_the_decade_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-03T11:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect — The Open‑Source Distributed Training Lab (Thu, Oct 2 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect — Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect — Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We’re excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I’m Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM – 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
