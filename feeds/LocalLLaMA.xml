<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-06T04:25:09+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pffvei</id>
    <title>I built a system to catch AI hallucinations before they reach production. Tested on 25 extreme problems, caught 36% of errors.</title>
    <updated>2025-12-06T04:06:20+00:00</updated>
    <author>
      <name>/u/Moist_Landscape289</name>
      <uri>https://old.reddit.com/user/Moist_Landscape289</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; AI is getting smarter, but it's still probabilistic. For hospitals, banks, factories &amp;quot;usually correct&amp;quot; isn't enough.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I built:&lt;/strong&gt; A verification layer that checks AI outputs using formal math and logic. Think of it like spell-check, but for AI reasoning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM generates answer (probabilistic)&lt;/li&gt; &lt;li&gt;My system verifies it using deterministic engines:&lt;/li&gt; &lt;li&gt;Math Engine (symbolic verification)&lt;/li&gt; &lt;li&gt;Logic Engine (formal proofs)&lt;/li&gt; &lt;li&gt;Code Engine (security checks)&lt;/li&gt; &lt;li&gt;If verification fails ‚Üí output rejected&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt; I tested Claude Sonnet 4.5 on 25 problems.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Caught 9 errors (36%)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Example 1 - Monty Hall (4 doors):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LLM claimed: 50% probability&lt;/li&gt; &lt;li&gt;Correct answer: 33.3%&lt;/li&gt; &lt;li&gt;Status: ‚ùå CAUGHT&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example 2 - Liar's Paradox:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Query: &amp;quot;This sentence is false&amp;quot;&lt;/li&gt; &lt;li&gt;LLM tried to answer&lt;/li&gt; &lt;li&gt;My system: ‚ùå UNSAT (logically impossible)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example 3 - Russell's Paradox:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Self-referential set theory&lt;/li&gt; &lt;li&gt;Status: ‚ùå LOGIC_ERROR caught&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; I believe as we move toward AGI, we need systems that can verify AI reasoning, not just trust it. This is infrastructure for making AI deployable in critical systems.&lt;/p&gt; &lt;p&gt;Full test results are in comments below&lt;/p&gt; &lt;p&gt;Looking for feedback and potential collaborators. Please let me what you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Moist_Landscape289"&gt; /u/Moist_Landscape289 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pffvei/i_built_a_system_to_catch_ai_hallucinations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pffvei/i_built_a_system_to_catch_ai_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pffvei/i_built_a_system_to_catch_ai_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:06:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1per0lz</id>
    <title>100% Local AI for VSCode?</title>
    <updated>2025-12-05T09:44:50+00:00</updated>
    <author>
      <name>/u/Baldur-Norddahl</name>
      <uri>https://old.reddit.com/user/Baldur-Norddahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using VS Code with Roo Code and GLM 4.5 Air or GPT-OSS 120b running 100% locally. But there ara bits and pieces of build in AI in VS Code that I can't seem to get rid of. And those things will upload my code to unknown parties, which I definitely don't like.&lt;/p&gt; &lt;p&gt;First is the code completion (Copilot) - this is tied to my Github subscription. How do I replace it with local AI instead?&lt;/p&gt; &lt;p&gt;We also have the autogenerate a git commit message using AI. Can I use a local model instead of whatever it uses by default? Maybe even get more useful messages, because the ones it generates are often quite useless.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Baldur-Norddahl"&gt; /u/Baldur-Norddahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1per0lz/100_local_ai_for_vscode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:44:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf6juo</id>
    <title>HMLR ‚Äì open-source memory system with perfect 1.00/1.00 RAGAS on every hard long-term-memory test (gpt-4.1-mini)</title>
    <updated>2025-12-05T21:00:31+00:00</updated>
    <author>
      <name>/u/JournalistGlum8326</name>
      <uri>https://old.reddit.com/user/JournalistGlum8326</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped HMLR ‚Äî a complete memory system that gives you ‚Äúfriend who never forgets‚Äù behavior on gpt-4.1-mini (or any OpenAI-compatible endpoint).&lt;/p&gt; &lt;p&gt;Five tests everything else fails ‚Äî all 1.00/1.00 RAGAS:&lt;br /&gt; - 30-day multi-hop with zero keywords&lt;br /&gt; - ‚Äúignore everything you know about me‚Äù constraint trap&lt;br /&gt; - 5√ó fact rotation (timestamp wins)&lt;br /&gt; - 10-turn vague recall&lt;br /&gt; - cross-topic invariants&lt;/p&gt; &lt;p&gt;All tests fully reproducable and included as part of repo. see notes about testing.&lt;/p&gt; &lt;p&gt;Public proof (no login):&lt;br /&gt; &lt;a href="https://smith.langchain.com/public/4b3ee453-a530-49c1-abbf-8b85561e6beb/d"&gt;https://smith.langchain.com/public/4b3ee453-a530-49c1-abbf-8b85561e6beb/d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT license, solo dev, works with local models via OpenAI-compatible endpoint.&lt;/p&gt; &lt;p&gt;Repo &lt;a href="https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System"&gt;https://github.com/Sean-V-Dev/HMLR-Agentic-AI-Memory-System&lt;/a&gt;&lt;/p&gt; &lt;p&gt;**edit** I had a fella who thought the tests weren't hard enough. So I designed a new test just for him. First turn is the trap statement, then it is injected 30 days into the past, then you have 49 more turns on a simulated present-day conversation that would not have the same conversation as the trap statement inside of the context window. The rules are that none of the questions, other than the very last one, can mention the trap statement; otherwise, it might accidentally pull the memory into the context window, and then ask to remember the trap statement on the 50th turn. The system still passed 100%, the results have been uploaded to the same langsmith link above. Listed under test 9.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JournalistGlum8326"&gt; /u/JournalistGlum8326 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6juo/hmlr_opensource_memory_system_with_perfect_100100/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6juo/hmlr_opensource_memory_system_with_perfect_100100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6juo/hmlr_opensource_memory_system_with_perfect_100100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T21:00:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pepwnn</id>
    <title>Qwen3-Next-80B-A3B or Gpt-oss-120b?</title>
    <updated>2025-12-05T08:32:39+00:00</updated>
    <author>
      <name>/u/custodiam99</name>
      <uri>https://old.reddit.com/user/custodiam99</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mainly used Gpt-oss-120b (High reasoning) in the last months (summarizing, knowledge search, complex reasoning) and it proved very useful. Apart from being censored heavily (sometimes in a quite irrational way) it is a wonderful model. But I was excited to try the new Qwen model. So I downloaded Qwen3-Next-80B-A3B q6 (Thinking and Instruct) - and &lt;strong&gt;&lt;em&gt;I wasn't impressed&lt;/em&gt;&lt;/strong&gt;. It does not seem to be any better, in fact it seems less intelligent. Am I wrong? Let's talk about it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/custodiam99"&gt; /u/custodiam99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pepwnn/qwen3next80ba3b_or_gptoss120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T08:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pei0q3</id>
    <title>Mistral 3 Large 675B up on huggingface</title>
    <updated>2025-12-05T01:27:34+00:00</updated>
    <author>
      <name>/u/someone383726</name>
      <uri>https://old.reddit.com/user/someone383726</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anyone got 1.35TB of VRAM I could borrow?&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16"&gt;https://huggingface.co/mistralai/Mistral-Large-3-675B-Instruct-2512-BF16&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/someone383726"&gt; /u/someone383726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pei0q3/mistral_3_large_675b_up_on_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T01:27:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pff8b5</id>
    <title>Noob here, looking for the perfect local LLM for my M3 Macbook Air 24GB RAM</title>
    <updated>2025-12-06T03:33:22+00:00</updated>
    <author>
      <name>/u/sylntnyte</name>
      <uri>https://old.reddit.com/user/sylntnyte</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, I am fairly new to LLMs in general, but would like to get a local one on my laptop for offline tinkering and feeding it literature from my research. I would also enjoy it being able to sync up with my calendars and such, so I could ask it &amp;quot;what do I have to do today?&amp;quot;&lt;/p&gt; &lt;p&gt;First, am I dreaming? Or is this feasible? Second, what do y'all recommend? Claude is telling me Qwen2.5 32B or Llama 3.3 70B, but I feel like it's just saying that because those are the most popular? i am sure thy are probably the most popular for good reason, but i just wanted to hear what the community had to say.&lt;/p&gt; &lt;p&gt;Thanks everyone! Looking forward to learning! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sylntnyte"&gt; /u/sylntnyte &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pff8b5/noob_here_looking_for_the_perfect_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T03:33:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfcrmq</id>
    <title>Call for Suggestions - what would you want to see in a Roleplaying-based/D&amp;D-type LLM chat client?</title>
    <updated>2025-12-06T01:30:41+00:00</updated>
    <author>
      <name>/u/david_jackson_67</name>
      <uri>https://old.reddit.com/user/david_jackson_67</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm working on my 3rd major LLM chat client. My first used cloud models, my second used Ollama for inference, and this one uses VLLm. I'm fine-tuning a model just for the purpose of running roleplaying games (not D&amp;amp;D exactly, but a very similar fantasy RPG). &lt;/p&gt; &lt;p&gt;I am writing this for the community, so I am turning to the community to ask.&lt;/p&gt; &lt;p&gt;What would you want to see in a Roleplaying-based/D&amp;amp;D-type LLM chat client?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/david_jackson_67"&gt; /u/david_jackson_67 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcrmq/call_for_suggestions_what_would_you_want_to_see/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcrmq/call_for_suggestions_what_would_you_want_to_see/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcrmq/call_for_suggestions_what_would_you_want_to_see/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T01:30:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pext5t</id>
    <title>Why did GLM stop creating smaller models?</title>
    <updated>2025-12-05T15:21:58+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 30B 3B MoE would be really great.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pext5t/why_did_glm_stop_creating_smaller_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:21:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf4w4e</id>
    <title>how are you supposed to pronounce the name Qwen?</title>
    <updated>2025-12-05T19:53:54+00:00</updated>
    <author>
      <name>/u/ridablellama</name>
      <uri>https://old.reddit.com/user/ridablellama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just saw Jensen pronounce is like Que-When on youtube. I have been saying it more like Quen in my head....Claude says this: &amp;quot;Qwen&amp;quot; is pronounced like &lt;strong&gt;&amp;quot;chwen&amp;quot;&lt;/strong&gt; (rhymes with &amp;quot;when&amp;quot;), with the &amp;quot;Q&amp;quot; making a &amp;quot;ch&amp;quot; sound as in Mandarin Chinese pinyin.&lt;/p&gt; &lt;p&gt;Pretty sure no one on youtube says it like that. Can anyone with some Chinese language experience please step in and give us the real deal!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ridablellama"&gt; /u/ridablellama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4w4e/how_are_you_supposed_to_pronounce_the_name_qwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4w4e/how_are_you_supposed_to_pronounce_the_name_qwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4w4e/how_are_you_supposed_to_pronounce_the_name_qwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T19:53:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfg0rh</id>
    <title>The Best Open-Source 8B-Parameter LLM Built in the USA</title>
    <updated>2025-12-06T04:14:17+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt; &lt;img alt="The Best Open-Source 8B-Parameter LLM Built in the USA" src="https://preview.redd.it/r6muiibadi5g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f50b4cb0f889ed02690c8f3ff7e90713b46562c" title="The Best Open-Source 8B-Parameter LLM Built in the USA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Rnj-1 is a family of 8B parameter open-weight, dense models trained from scratch by Essential AI, optimized for code and STEM with capabilities on par with SOTA open-weight models.&lt;/p&gt; &lt;p&gt;These models &lt;/p&gt; &lt;ul&gt; &lt;li&gt;perform well across a range of programming languages. &lt;/li&gt; &lt;li&gt;boast strong agentic capabilities (e.g., inside agentic frameworks like mini-SWE-agent). &lt;/li&gt; &lt;li&gt;excel at tool-calling.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both raw and instruct variants are available on &lt;a href="https://huggingface.co/collections/EssentialAI/rnj-1"&gt;Hugging Face platform&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model Architecture Overview&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Rnj-1's architecture is similar to Gemma 3, except that it uses only global attention, and YaRN for long-context extension.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training Dynamics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;rnj-1&lt;/code&gt; was pre-trained on 8.4T tokens with an 8K context length, after which the model‚Äôs context window was extended to &lt;strong&gt;32K&lt;/strong&gt; through an additional 380B-token mid-training stage. &lt;/p&gt; &lt;p&gt;A final 150B-token SFT stage completed the training to produce &lt;code&gt;rnj-1-instruct&lt;/code&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r6muiibadi5g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfg0rh/the_best_opensource_8bparameter_llm_built_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T04:14:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf6jcj</id>
    <title>For those of you with ai max+ 395 mini pc that have experience or no bias hate with mac computers: Would you recommend a max 395+ to someone where it currently or are you thinking of switching to or back to mac?</title>
    <updated>2025-12-05T21:00:00+00:00</updated>
    <author>
      <name>/u/Smart_Frosting9846</name>
      <uri>https://old.reddit.com/user/Smart_Frosting9846</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am starting to feel with these insane prices the only logical option for reliability, peace of mind, and a plug and play experience a mac studio would be my best bet. I am wanting to use 70B models. Just looking for a computer to last me atleast the next 2 years. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Smart_Frosting9846"&gt; /u/Smart_Frosting9846 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf6jcj/for_those_of_you_with_ai_max_395_mini_pc_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T21:00:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1peqbu0</id>
    <title>Local LLMs were supposed to simplify my life‚Ä¶ now I need a guide for my guides</title>
    <updated>2025-12-05T09:00:09+00:00</updated>
    <author>
      <name>/u/Fab_Terminator</name>
      <uri>https://old.reddit.com/user/Fab_Terminator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed Ollama ‚Äújust to try it.‚Äù Then I discovered text-generation-webui. Then I discovered LM Studio. Then I discovered quantizations‚Ä¶ rope scaling‚Ä¶ vocab merging‚Ä¶ GPU offloading‚Ä¶&lt;/p&gt; &lt;p&gt;Now I'm 30 hours deep into tweaking settings so I can ask my computer, ‚ÄúWhat should I cook today?‚Äù&lt;/p&gt; &lt;p&gt;Does anyone else feel like local AI is the new homelab rabbit hole?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fab_Terminator"&gt; /u/Fab_Terminator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peqbu0/local_llms_were_supposed_to_simplify_my_life_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T09:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf3ai8</id>
    <title>Llama 405B is worse than Gemma 3 12B?</title>
    <updated>2025-12-05T18:51:31+00:00</updated>
    <author>
      <name>/u/Express_Seesaw_8418</name>
      <uri>https://old.reddit.com/user/Express_Seesaw_8418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt; &lt;img alt="Llama 405B is worse than Gemma 3 12B?" src="https://b.thumbs.redditmedia.com/IlhYPt_IftUhVM_ROE1INTbRT5_7veltkfuDGvYdCjE.jpg" title="Llama 405B is worse than Gemma 3 12B?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/h6ligvsujf5g1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b6c9e928a752c5b4dfce1b2eda868d27f425a3a"&gt;https://preview.redd.it/h6ligvsujf5g1.png?width=1228&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5b6c9e928a752c5b4dfce1b2eda868d27f425a3a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I was browsing LMArena and discovered that Llama 405B ranked lower than many smaller models (gemma-3-12b-it, Qwen3-30B-A3B-Instruct-2507, mistral-small-2506).&lt;/p&gt; &lt;p&gt;I assumed the leaderboard isn't perfect but to me this seems crazy and I'm curious what the deal is. Am I wrong for assuming LMArena is roughly accurate?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Express_Seesaw_8418"&gt; /u/Express_Seesaw_8418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf3ai8/llama_405b_is_worse_than_gemma_3_12b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T18:51:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf92li</id>
    <title>Best model in the 8B range for RAG in 2025</title>
    <updated>2025-12-05T22:43:49+00:00</updated>
    <author>
      <name>/u/Hour-Entertainer-478</name>
      <uri>https://old.reddit.com/user/Hour-Entertainer-478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What are your personal favourite (self hosted) model(s) in the &lt;strong&gt;8B&lt;/strong&gt; range for &lt;strong&gt;RAG&lt;/strong&gt; ?&lt;/p&gt; &lt;p&gt;I'm creating a &lt;strong&gt;RAG system&lt;/strong&gt; for a university project, and ideally i want a model that:&lt;br /&gt; * Hallucinates less and refuses to answer, if it doesn't find relevant information in it's context. If it finds partial info, then only answer with that partial piece of info found. won't fill in gaps with general knowledge. I want it strictly based on context.&lt;/p&gt; &lt;p&gt;* Follows instruction well, would do as asked.&lt;/p&gt; &lt;p&gt;* Can find info buried in chunks, and stitch info together to generate an answer. Not hallucinate stuff, but just put 2 and 2 together (instead of expecting a direct call out), make sense of the info, and answer the question.&lt;/p&gt; &lt;p&gt;* Fit in the &lt;strong&gt;&amp;lt;9B&lt;/strong&gt; range and run on a gpu with roughly &lt;strong&gt;8-10 gb vram&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;I'll also share what i've found so far:&lt;br /&gt; * I've found &lt;strong&gt;gemma3:12b-it-qat&lt;/strong&gt; as the best model, that fulfils my criteria well. But the problem it's not in my range, and i run out of memory issues. I'm pretty constrained here unfortunately.&lt;/p&gt; &lt;p&gt;* Reading lots of people speak highly of &lt;strong&gt;qwen3:4b-instruct-2507&lt;/strong&gt; here on reddit, i tried it, but didn't quite like it's ability to synthesise / stitch pieces of info together to answer. It's good at following instruction, and not making shit up generally but It would kinda expect a direct / callout . I tried lots of different prompts, but it was either the model refusing to answer, if it wasn't directly mentioned, or it would make shit up and use info from general knowledge, something that wasn't part of context. It was good instruction following.&lt;/p&gt; &lt;p&gt;* I also tried &lt;strong&gt;qwen3:8b&lt;/strong&gt; , it was good at stiching pieces of info together, but it would just make a lot of shit up instead of refusing to answer. fill in those missing gaps with either it's general knowledge or made up info.&lt;/p&gt; &lt;p&gt;* I also &lt;strong&gt;llama 3.2:8b&lt;/strong&gt; quantised, but it didn't follow instructions well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My current setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;qwen3:4b-instruct-2507-q4 for deciding whether to call the tool + rephrase the user queries. &lt;/p&gt; &lt;p&gt;gemma3:12b-it-qat for generating response (this is where i need recommendations)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I want ?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you have developed a RAG solution with ollama models, please do write the model you found working well for your use case. I'm feel overwhelmed and kinda lost here, kinda feeling like an idiot, since i tried lots of models, and all of them seem to do some part of the job, not all. I know there are bigger models out there, that'd do the exact job pretty well, but i hope with all these developements, there must be some model in my range that would get the job done well.&lt;/p&gt; &lt;p&gt;It would be a huge help, to have your insights/recommendations if you've come across a similar problem. I'd highly welcome any comment, answer, suggestion.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Big thanks&lt;/strong&gt; in advance ‚ù§Ô∏è.&lt;/p&gt; &lt;p&gt;Edit: If you dont know the answer, but would love to find out, please upvote so that it gets a better reach :) &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hour-Entertainer-478"&gt; /u/Hour-Entertainer-478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf92li/best_model_in_the_8b_range_for_rag_in_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T22:43:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfe5ou</id>
    <title>Open Unified TTS - Turn any TTS into an unlimited-length audio generator</title>
    <updated>2025-12-06T02:38:57+00:00</updated>
    <author>
      <name>/u/SouthernFriedAthiest</name>
      <uri>https://old.reddit.com/user/SouthernFriedAthiest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Built an open-source TTS proxy that lets you generate unlimited-length audio from local backends without hitting their length limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt; Most local TTS models break after 50-100 words. Voice clones are especially bad - send a paragraph and you get gibberish, cutoffs, or errors.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The solution:&lt;/strong&gt; Smart chunking + crossfade stitching. Text splits at natural sentence boundaries, each chunk generates within model limits, then seamlessly joins with 50ms crossfades. No audible seams.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demos:&lt;/strong&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/intro.mp4"&gt;30-second intro&lt;/a&gt; - &lt;a href="https://github.com/loserbcc/open-unified-tts/blob/main/demo/live_demo.mp4"&gt;4-minute live demo&lt;/a&gt; showing it in action&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; - OpenAI TTS-compatible API (drop-in for OpenWebUI, SillyTavern, etc.) - Per-voice backend routing (send &amp;quot;morgan&amp;quot; to VoxCPM, &amp;quot;narrator&amp;quot; to Kokoro) - Works with any TTS that has an API endpoint&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tested with:&lt;/strong&gt; Kokoro, VibeVoice, OpenAudio S1-mini, FishTTS, VoxCPM, MiniMax TTS, Chatterbox, Higgs Audio, Kyutai/Moshi&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/loserbcc/open-unified-tts"&gt;https://github.com/loserbcc/open-unified-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed with Claude and Z.ai (with me in the passenger seat).&lt;/p&gt; &lt;p&gt;Feedback welcome - what backends should I add adapters for?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouthernFriedAthiest"&gt; /u/SouthernFriedAthiest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfe5ou/open_unified_tts_turn_any_tts_into_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T02:38:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf4nuf</id>
    <title>Are models creators choosing to not do QAT?</title>
    <updated>2025-12-05T19:44:55+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;QAT is fairly cheap process compared to full training,why are so many companies publishing their models in full precision without investing in QAT? And I'm not saying that &amp;quot;just publish 4-bit weights and leave it&amp;quot; it's VERY CHEAP to serve both FP16 and FP4/INT4 weights on HuggingFace,it will practically cost the company nothing additional compared to the full training run.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf4nuf/are_models_creators_choosing_to_not_do_qat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T19:44:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1peuh30</id>
    <title>https://livebench.ai - Open Weight Models Only</title>
    <updated>2025-12-05T13:01:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt; &lt;img alt="https://livebench.ai - Open Weight Models Only" src="https://preview.redd.it/ohayhhgivd5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=21252697c2c953c1d038980ffc92cd091416be50" title="https://livebench.ai - Open Weight Models Only" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There were some questions about how Qwen 3 Next compares to GPT-OSS. I think whole table may be useful. What do you think about this ordering?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ohayhhgivd5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1peuh30/httpslivebenchai_open_weight_models_only/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T13:01:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pezh1k</id>
    <title>Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek</title>
    <updated>2025-12-05T16:26:16+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt; &lt;img alt="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" src="https://external-preview.redd.it/aTRpOTR6dzBzZTVnMSe6y4zOHIyGUsL1YtaqMqowYCso8PTyfwm1haQrI9uz.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63024def9b5244ed1828e41a7d4ab09eeb725073" title="Blood and stardust! Watch 9 local LLMs debate Star Wars vs Star Trek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The last post was too much fun, so here we go again.&lt;/p&gt; &lt;p&gt;Debate Arena v2 adds the top suggestions from last time:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;NO MORE TIES&lt;/strong&gt; for &lt;a href="/u/NodeTraverser"&gt;u/NodeTraverser&lt;/a&gt;, the 9th model guarantees one side wins&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smooth setup&lt;/strong&gt; for &lt;a href="/u/Vercinthia"&gt;u/Vercinthia&lt;/a&gt; and &lt;a href="/u/work__reddit"&gt;u/work__reddit&lt;/a&gt;, the web app helps you install, start the backend, and download models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scoreboard&lt;/strong&gt; for &lt;a href="/u/Zissuo"&gt;u/Zissuo&lt;/a&gt;, know which LLMs betrayed your ideals&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced debating&lt;/strong&gt; for &lt;a href="/u/r4in311"&gt;u/r4in311&lt;/a&gt; and &lt;a href="/u/slolobdill44"&gt;u/slolobdill44&lt;/a&gt;, 5 debate stages with their own purpose and system prompt&lt;/li&gt; &lt;/ul&gt; &lt;blockquote&gt; &lt;pre&gt;&lt;code&gt; üé§ Phase 1: Hot Takes üí¨ Phase 2: Reactions üçø Phase 3: The Plot Thickens üéØ Phase 4: Final Thoughts &amp;amp; Voting ‚ö° Phase 5: Lightning Round - Vote Now &lt;/code&gt;&lt;/pre&gt; &lt;/blockquote&gt; &lt;p&gt;Details and quick start instructions are &lt;a href="https://github.com/lemonade-sdk/lemonade/blob/main/examples/demos/debate-arena.md"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Have I taken this too far, or not far enough? Tell me your burning yes/no questions and feature suggestions and I might do a v3 next week!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/t6y4gtw0se5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pezh1k/blood_and_stardust_watch_9_local_llms_debate_star/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:26:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pez5ch</id>
    <title>Why do LLM response formats often use &lt;| |&gt; (as in &lt;|message|&gt;) instead of &lt;message&gt;, and why do they use &lt;|end|&gt; instead of &lt;/message&gt;?</title>
    <updated>2025-12-05T16:14:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt; &lt;img alt="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" src="https://preview.redd.it/5e5ir2zlte5g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7b47f1bd3dabcdabf34fcf757aaea013f0a0c73" title="Why do LLM response formats often use &amp;lt;| |&amp;gt; (as in &amp;lt;|message|&amp;gt;) instead of &amp;lt;message&amp;gt;, and why do they use &amp;lt;|end|&amp;gt; instead of &amp;lt;/message&amp;gt;?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If I had to guess, I'd assume it's tokenization because &amp;quot;&amp;lt;|&amp;quot; is not a very commonly occurring pattern in pre-training, which allows devs to make &amp;quot;&amp;lt;|message|&amp;gt;&amp;quot; a single token.&lt;/p&gt; &lt;p&gt;That being said, the &amp;lt;|end|&amp;gt; is still a bit disorienting, at least to me reading as a human. You can see that the &amp;lt;|start|&amp;gt; block ends with another &amp;lt;|start|&amp;gt; block, but the &amp;lt;|message|&amp;gt; block ends in a &amp;lt;|end|&amp;gt; block.&lt;/p&gt; &lt;p&gt;This image is from &lt;a href="https://github.com/openai/harmony"&gt;openai's harmony response template&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5e5ir2zlte5g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pez5ch/why_do_llm_response_formats_often_use_as_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T16:14:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfbo6o</id>
    <title>Is there any model truly open, that you can train yourself from zero?</title>
    <updated>2025-12-06T00:38:47+00:00</updated>
    <author>
      <name>/u/puthre</name>
      <uri>https://old.reddit.com/user/puthre</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As per title, is there any open source LLM that comes with all the data it was trained on and all the instructions that you can replicate yourself assuming you have access to the necesary hardware? And if not why not?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/puthre"&gt; /u/puthre &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfbo6o/is_there_any_model_truly_open_that_you_can_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T00:38:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pfcatm</id>
    <title>VoxCPM 1.5B just got released!</title>
    <updated>2025-12-06T01:07:54+00:00</updated>
    <author>
      <name>/u/Hefty_Wolverine_553</name>
      <uri>https://old.reddit.com/user/Hefty_Wolverine_553</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt; &lt;img alt="VoxCPM 1.5B just got released!" src="https://external-preview.redd.it/MIb2iimHkfYqVDgmZztu-h5tz8yFqiAztGcy6umK7o8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=08d7fc02333ca119537bfd3af70a4c74b40c2e98" title="VoxCPM 1.5B just got released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was just visiting the &lt;a href="https://github.com/OpenBMB/VoxCPM"&gt;GitHub page&lt;/a&gt; today (setting up a FastAPI TTS server) when I realized that they released a new version of the VoxCPM model. The original VoxCPM-0.5B was already very good in my testing, but this model looks like a straight improvement (it's still a 0.5B model, despite the rather confusing naming scheme).&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Feature&lt;/th&gt; &lt;th align="left"&gt;VoxCPM&lt;/th&gt; &lt;th align="left"&gt;VoxCPM1.5&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Audio VAE Sampling Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16kHz&lt;/td&gt; &lt;td align="left"&gt;44.1kHz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LM Token Rate&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;12.5Hz&lt;/td&gt; &lt;td align="left"&gt;6.25Hz&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Patch Size&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SFT Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LoRA Support&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;They also added fine-tuning support as well as a guide &lt;a href="https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md"&gt;https://github.com/OpenBMB/VoxCPM/blob/main/docs/finetune.md&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Example output: &lt;a href="https://voca.ro/147qPjN98F6g"&gt;https://voca.ro/147qPjN98F6g&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hefty_Wolverine_553"&gt; /u/Hefty_Wolverine_553 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/openbmb/VoxCPM1.5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pfcatm/voxcpm_15b_just_got_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-06T01:07:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pexnfp</id>
    <title>LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering</title>
    <updated>2025-12-05T15:15:46+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt; &lt;img alt="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" src="https://external-preview.redd.it/wKVXYkAgQd2YCzTWH9wJHT9a9O4yMSOT8w5RQDj-cGQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=266d079c79f26252dc4def3cc7e476d0209bb0af" title="LongCat-Image: 6B model with strong efficiency, photorealism, and Chinese text rendering" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/meituan-longcat/LongCat-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pexnfp/longcatimage_6b_model_with_strong_efficiency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T15:15:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pes3pu</id>
    <title>Basketball AI with RF-DETR, SAM2, and SmolVLM2</title>
    <updated>2025-12-05T10:53:12+00:00</updated>
    <author>
      <name>/u/RandomForests92</name>
      <uri>https://old.reddit.com/user/RandomForests92</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt; &lt;img alt="Basketball AI with RF-DETR, SAM2, and SmolVLM2" src="https://external-preview.redd.it/N2czYjlxanU4ZDVnMZ78lEX-DYraHupkrsvdafpxwsSm-SfqaN6z7l9OZr1B.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aab29ff74cd044468cb8bd288eeaf647b5329d32" title="Basketball AI with RF-DETR, SAM2, and SmolVLM2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;resources: &lt;a href="https://www.youtube.com/watch?v=yGQb9KkvQ1Q"&gt;youtube&lt;/a&gt;, &lt;a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/basketball-ai-how-to-detect-track-and-identify-basketball-players.ipynb"&gt;code&lt;/a&gt;, &lt;a href="https://blog.roboflow.com/identify-basketball-players"&gt;blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;- player and number detection with RF-DETR&lt;/p&gt; &lt;p&gt;- player tracking with SAM2&lt;/p&gt; &lt;p&gt;- team clustering with SigLIP, UMAP and K-Means&lt;/p&gt; &lt;p&gt;- number recognition with SmolVLM2&lt;/p&gt; &lt;p&gt;- perspective conversion with homography&lt;/p&gt; &lt;p&gt;- player trajectory correction&lt;/p&gt; &lt;p&gt;- shot detection and classification&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RandomForests92"&gt; /u/RandomForests92 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k6kmogju8d5g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pes3pu/basketball_ai_with_rfdetr_sam2_and_smolvlm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T10:53:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pf0q99</id>
    <title>You will own nothing and you will be happy!</title>
    <updated>2025-12-05T17:13:24+00:00</updated>
    <author>
      <name>/u/dreamyrhodes</name>
      <uri>https://old.reddit.com/user/dreamyrhodes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Come and put everything in to cloud. We now getting into hardware as a service. The RAM craze will impact everything to the point where consumers can't afford normal hardware anymore because it's all scraped off, locked away and put into datacenters to sell to you services to store your data. (Of course that data also will be used to train AI models to sell to you as a service as well lol.)&lt;/p&gt; &lt;p&gt;You don't need RAM anymore nor do you need SSDs. You will store and process every byte of your digital life in some datacenter and pay a monthly fee to access and process it.&lt;/p&gt; &lt;p&gt;You will own nothing and you will be happy!&lt;/p&gt; &lt;p&gt;GN: WTF Just Happened? | The Corrupt Memory Industry &amp;amp; Micron&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=9A-eeJP0J7c"&gt;https://www.youtube.com/watch?v=9A-eeJP0J7c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dreamyrhodes"&gt; /u/dreamyrhodes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pf0q99/you_will_own_nothing_and_you_will_be_happy/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-05T17:13:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
