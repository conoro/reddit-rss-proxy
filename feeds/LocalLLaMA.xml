<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-14T09:36:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o5xkka</id>
    <title>RTX 5090 + FP4 + Open WebUI via TensorRT-LLM (because VLLM made me cry at 2am)</title>
    <updated>2025-10-13T21:56:30+00:00</updated>
    <author>
      <name>/u/Putrid_Passion_6916</name>
      <uri>https://old.reddit.com/user/Putrid_Passion_6916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So… after a late-night slap fight with VLLM on Blackwell and FP4, I did the unthinkable: I got GPT5 to read the docs and tried NVIDIA’s own TensorRT-LLM. Turns out the fix was hiding in plain sight (right next to my empty coffee mug).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/rdumasia303/tensorrt-llm_with_open-webui"&gt;https://github.com/rdumasia303/tensorrt-llm_with_open-webui&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why you might care&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;5090 / Blackwell friendly:&lt;/strong&gt; Built to run cleanly on RTX 5090 and friends.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FP4 works:&lt;/strong&gt; Runs FP4 models that can be grumpy in other stacks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI-compatible:&lt;/strong&gt; Drop-in for Open WebUI or anything that speaks &lt;code&gt;/v1&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One compose file:&lt;/strong&gt; Nothing too magical required.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I haven't got multimodal models working, but&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvidia/Qwen3-30B-A3B-FP4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Works, and it's fast - so that's me done for tonight.&lt;/p&gt; &lt;h1&gt;Apologies if this has been done before - but all I could find were folks saying 'Can it be done?' So I made it.&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid_Passion_6916"&gt; /u/Putrid_Passion_6916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xkka/rtx_5090_fp4_open_webui_via_tensorrtllm_because/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xkka/rtx_5090_fp4_open_webui_via_tensorrtllm_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xkka/rtx_5090_fp4_open_webui_via_tensorrtllm_because/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T21:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o60ymh</id>
    <title>Pretraining with hierarchical memories</title>
    <updated>2025-10-14T00:23:37+00:00</updated>
    <author>
      <name>/u/Zc5Gwu</name>
      <uri>https://old.reddit.com/user/Zc5Gwu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.arxiv.org/abs/2510.02375"&gt;https://www.arxiv.org/abs/2510.02375&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Apple researchers discovered a way to add “slow” knowledge-memory post-training while using a smaller set of parameters for reasoning. Their ablation studies find that the approach outperforms RAG in both processing flops and storage.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zc5Gwu"&gt; /u/Zc5Gwu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o60ymh/pretraining_with_hierarchical_memories/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o60ymh/pretraining_with_hierarchical_memories/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o60ymh/pretraining_with_hierarchical_memories/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5h18a</id>
    <title>Has anyone gotten hold of DGX Spark for running local LLMs?</title>
    <updated>2025-10-13T11:24:16+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt; &lt;img alt="Has anyone gotten hold of DGX Spark for running local LLMs?" src="https://preview.redd.it/ombg19hz5vuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0724eec16512ad1132ec21657234daac0040c74" title="Has anyone gotten hold of DGX Spark for running local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark is apparently one of the Time's Best Invention of 2025!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ombg19hz5vuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T11:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5o388</id>
    <title>Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart</title>
    <updated>2025-10-13T16:12:58+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o388/geoffrey_hinton_explains_neural_netsllms_to_jon/"&gt; &lt;img alt="Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart" src="https://external-preview.redd.it/bNgP_VTVxX2BsZJEDcXLtnXMLl1zl3HlPzbOEzNfKJA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cba0db700ee1e06c7624b8f25db999e589ae4843" title="Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even if you've worked extensively with neural nets and LLMs before, you might get some intuition about them fron Hinton. I've watched a bunch of Hinton's videos over the years and this discussion with Jon Stewart was unusually good. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=jrK3PsD3APk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o388/geoffrey_hinton_explains_neural_netsllms_to_jon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o388/geoffrey_hinton_explains_neural_netsllms_to_jon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:12:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o695bm</id>
    <title>WhatsApp food ordering AI Agent example with source code</title>
    <updated>2025-10-14T07:39:59+00:00</updated>
    <author>
      <name>/u/necati-ozmen</name>
      <uri>https://old.reddit.com/user/necati-ozmen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;We’ve been making minimal AI agent examples with full source code.&lt;/p&gt; &lt;p&gt;Here’s one that lets you order food on WhatsApp, it shows a menu, takes your order, and checks the status through chat. Using Supabase, Whatsapp cloud API, OpenAI and Voltagent.&lt;/p&gt; &lt;p&gt;It uses tools and memory to keep context and handle actions.&lt;/p&gt; &lt;p&gt;The project is simple on purpose and feel free to fork it and build your own version. Feedback and PRs are welcome:)&lt;/p&gt; &lt;p&gt;Disclaimer: I’m one of the maintainers of VoltAgent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/necati-ozmen"&gt; /u/necati-ozmen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/VoltAgent/voltagent/tree/main/examples/with-whatsapp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o695bm/whatsapp_food_ordering_ai_agent_example_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o695bm/whatsapp_food_ordering_ai_agent_example_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T07:39:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o65di4</id>
    <title>Nvidia DGX Spark reviews started</title>
    <updated>2025-10-14T03:54:37+00:00</updated>
    <author>
      <name>/u/raphaelamorim</name>
      <uri>https://old.reddit.com/user/raphaelamorim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"&gt; &lt;img alt="Nvidia DGX Spark reviews started" src="https://external-preview.redd.it/uAv19XEYpnCDKkb7y0-bzGB8la4s2d7ck6vl-XJBRac.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=576e994d52db2badc52382731998578b9ff595e5" title="Nvidia DGX Spark reviews started" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably start selling on October 15th&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/raphaelamorim"&gt; /u/raphaelamorim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/zs-J9sKxvoM?si=237f_mBVyLH7QBOE"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o65di4/nvidia_dgx_spark_reviews_started/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T03:54:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5u0rr</id>
    <title>Significant speedup for local models</title>
    <updated>2025-10-13T19:44:45+00:00</updated>
    <author>
      <name>/u/MikeBeezzz</name>
      <uri>https://old.reddit.com/user/MikeBeezzz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MikeyBeez/hybrid-transformer-experiment"&gt;https://github.com/MikeyBeez/hybrid-transformer-experiment&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeBeezzz"&gt; /u/MikeBeezzz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5u0rr/significant_speedup_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5u0rr/significant_speedup_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5u0rr/significant_speedup_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T19:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5mlng</id>
    <title>Anyone think openAI will create a sequel of GPT-OSS?</title>
    <updated>2025-10-13T15:19:12+00:00</updated>
    <author>
      <name>/u/BothYou243</name>
      <uri>https://old.reddit.com/user/BothYou243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean they should right? because gpt-oss (not biased or just have some grudge) is a nice model, and the rprobelm is it's just nice, so creating somethign better is still needed, anyone got any leaks about it?&lt;/p&gt; &lt;p&gt;what about anthropic, wont they drop something open, and xAI?&lt;br /&gt; xAI have poteential to outpace everyone, i am not. a fan of open sorucing some 1 year old model trend, but if they create soemthign from scracth to open source just like openAI did, it will be Absolutely Incredible! (yes taken from tim cook)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BothYou243"&gt; /u/BothYou243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5n4fu</id>
    <title>Fully functional native FP4 training finally released</title>
    <updated>2025-10-13T15:37:51+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been eagerly watching the development of FP4 training, as it would enable anyone with a Blackwell device to train models with 2x the parameters that we can currently fit with FP8, and 4x BF16, which most people are still training in (get with the times people).&lt;/p&gt; &lt;p&gt;There have been many papers previously showing that FP4 is effective:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.19115"&gt;https://arxiv.org/abs/2505.19115&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2501.17116"&gt;https://arxiv.org/abs/2501.17116&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14669"&gt;https://arxiv.org/abs/2505.14669&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.20586"&gt;https://arxiv.org/abs/2502.20586&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And one of them has also been working on public versions of the training kernels... but they have only released the forward pass kernels: &lt;a href="https://github.com/huggingface/transformers/pull/38696"&gt;https://github.com/huggingface/transformers/pull/38696&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a comparison of the 4 papers by Gemini, if you're interested in the details: &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565"&gt;https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT-OSS was also trained in FP4, but released no code, though I would bet that NVidia's in house solution was used.&lt;/p&gt; &lt;p&gt;Now, finally, NVidia has published their own FP4 training recipe. It's not well documented or tested yet, and apparently one of the techniques required for stable quantization (stochastic rounding) &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/2255#issuecomment-3387759788"&gt;simply doesn't work on the consumer RTX 50 series&lt;/a&gt;, only the datacenter cards, but still, it's here and we can use it. The use of Hadamard transforms should still allow consumer cards to train with some stability.&lt;/p&gt; &lt;p&gt;Here's some documentation which touches on their FP4 recipe: &lt;a href="https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb"&gt;https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here's their paper which goes into detail: &lt;a href="https://arxiv.org/abs/2509.25149v1"&gt;https://arxiv.org/abs/2509.25149v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6870y</id>
    <title>GitHub - RagView/RagView : Validate RAG route on your dataset</title>
    <updated>2025-10-14T06:37:21+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6870y/github_ragviewragview_validate_rag_route_on_your/"&gt; &lt;img alt="GitHub - RagView/RagView : Validate RAG route on your dataset" src="https://external-preview.redd.it/CoNyHXb1E7NtiQZE7vuXWMXL6Jkf78hPpk0RKlCSlZ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e774eb1e33f85868b8ceca67abc63be011e9ecd6" title="GitHub - RagView/RagView : Validate RAG route on your dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/RagView/RagView"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6870y/github_ragviewragview_validate_rag_route_on_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6870y/github_ragviewragview_validate_rag_route_on_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T06:37:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5o8z1</id>
    <title>Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!</title>
    <updated>2025-10-13T16:18:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt; &lt;img alt="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" src="https://external-preview.redd.it/TovPswR4pl93bt0GT2q9uuik1XMY41ZSblXtMnDzdsU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed07f9c93dd6fdbb3800e12511f49c15a31923c3" title="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hot Take: Many models today are 'too smart' in a creative sense - trying too hard to be sensible and end up limiting their imagination to the user's prompt. Rerolls don't usually lead to different outcomes, and every gen seems catered to the user's expectations. Worst of all, there's an assistant bias that focuses on serving you (the user) instead of the story. All of these stifle their ability to express characters in a lively way. (inb4 skill issue)&lt;/p&gt; &lt;p&gt;Given the success of 22B and 123B ReduX v1.0, I revisited the old models and brought out a flavorful fusion of creativity and smarts through my latest tuning. 22B may not be as smart and sensible as the newer 24B, but ReduX makes it (more than) serviceable for users hoping for broader imagination and better immersion in their creative uses.&lt;/p&gt; &lt;h1&gt;Cydonia ReduX 22B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Behemoth ReduX 123B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1"&gt;https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;Enjoy! (Please note that this is a dual release: 123B and 22B. Notice the two links in this post.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5xp19</id>
    <title>What's a good uncensored model for general use?</title>
    <updated>2025-10-13T22:01:21+00:00</updated>
    <author>
      <name>/u/BankbusterMagic</name>
      <uri>https://old.reddit.com/user/BankbusterMagic</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried the Satyr model and it can't output anything except porn. If I ask it to write a story about three people going grocery shopping the wind up doing it in the dairy aisle. What's an uncensored model good for general use? 16GB RAM and 16GB VRAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BankbusterMagic"&gt; /u/BankbusterMagic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xp19/whats_a_good_uncensored_model_for_general_use/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xp19/whats_a_good_uncensored_model_for_general_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xp19/whats_a_good_uncensored_model_for_general_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T22:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o67m80</id>
    <title>How would you rate this 2x RTX 5090 build ?</title>
    <updated>2025-10-14T06:01:02+00:00</updated>
    <author>
      <name>/u/icybergenome</name>
      <uri>https://old.reddit.com/user/icybergenome</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering I am expecting it to run following tasks comfortably:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stable Diffusion XL,&lt;/li&gt; &lt;li&gt;InstantMesh,&lt;/li&gt; &lt;li&gt;ComfyUI Workflows,&lt;/li&gt; &lt;li&gt;LLM Inference (70B, Quant 4, 60-80 token/s, 32K Context),&lt;/li&gt; &lt;li&gt;Fine Tuning 30B using LoRA. 70B using QLoRA&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Component&lt;/th&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Price&lt;/th&gt; &lt;th&gt;Key Specs&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;2x NVIDIA RTX 5090 32GB&lt;/td&gt; &lt;td&gt;$4,800&lt;/td&gt; &lt;td&gt;64GB VRAM total • Blackwell FP8/FP4 • 1,792 GB/s each&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;AMD Ryzen 9 7950X&lt;/td&gt; &lt;td&gt;$420&lt;/td&gt; &lt;td&gt;16C/32T • 5.7GHz boost • PCIe 5.0 • 170W TDP&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;ASRock X870E Taichi&lt;/td&gt; &lt;td&gt;$480&lt;/td&gt; &lt;td&gt;2x PCIe 5.0 x16 • 4x DDR5 slots • 5x M.2 • WiFi 7&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;256GB DDR5 6000MHz CL30&lt;/td&gt; &lt;td&gt;$700&lt;/td&gt; &lt;td&gt;4x64GB • G.SKILL • EXPO certified • 1.35V&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Storage (OS)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Samsung 990 PRO 2TB&lt;/td&gt; &lt;td&gt;$170&lt;/td&gt; &lt;td&gt;PCIe 4.0 • 7,450 MB/s read • 5yr warranty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Storage (Data)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Silicon Power UD90 8TB&lt;/td&gt; &lt;td&gt;$310&lt;/td&gt; &lt;td&gt;PCIe 4.0 • 5,000 MB/s • Models + datasets&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Corsair HX1500i 1500W&lt;/td&gt; &lt;td&gt;$400&lt;/td&gt; &lt;td&gt;80+ Platinum • 4x 12VHPWR • 10yr warranty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Fractal Meshify 2 Compact&lt;/td&gt; &lt;td&gt;$110&lt;/td&gt; &lt;td&gt;ATX • Mesh front • 315mm GPU clearance&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;Arctic Liquid Freezer III 360&lt;/td&gt; &lt;td&gt;$130&lt;/td&gt; &lt;td&gt;360mm AIO • 350W TDP • 6yr warranty&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Fans&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;3x Noctua NF-A14 PWM&lt;/td&gt; &lt;td&gt;$90&lt;/td&gt; &lt;td&gt;140mm • 1,500 RPM • Ultra-quiet&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Option&lt;/th&gt; &lt;th&gt;Cost&lt;/th&gt; &lt;th&gt;VRAM&lt;/th&gt; &lt;th&gt;Training Speed&lt;/th&gt; &lt;th&gt;Decision&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4x RTX 3090 (used)&lt;/td&gt; &lt;td&gt;$2,800&lt;/td&gt; &lt;td&gt;96GB&lt;/td&gt; &lt;td&gt;Baseline (no FP8)&lt;/td&gt; &lt;td&gt;❌ Outdated architecture&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;2x RTX 5090&lt;/strong&gt; ⭐&lt;/td&gt; &lt;td&gt;$4,800&lt;/td&gt; &lt;td&gt;64GB&lt;/td&gt; &lt;td&gt;&lt;strong&gt;2.5x faster&lt;/strong&gt; (FP8)&lt;/td&gt; &lt;td&gt;✅ &lt;strong&gt;BEST VALUE&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1x RTX 6000 Pro&lt;/td&gt; &lt;td&gt;$7,200&lt;/td&gt; &lt;td&gt;96GB&lt;/td&gt; &lt;td&gt;2x faster&lt;/td&gt; &lt;td&gt;⚠️ Better as 2nd card later&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;3x RTX 5090&lt;/td&gt; &lt;td&gt;$7,200&lt;/td&gt; &lt;td&gt;96GB&lt;/td&gt; &lt;td&gt;3x faster&lt;/td&gt; &lt;td&gt;✅ Ideal upgrade path&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;What's more valuable: More VRAM (96GB) or modern architecture (64GB)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/icybergenome"&gt; /u/icybergenome &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o67m80/how_would_you_rate_this_2x_rtx_5090_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o67m80/how_would_you_rate_this_2x_rtx_5090_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o67m80/how_would_you_rate_this_2x_rtx_5090_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T06:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5qx6p</id>
    <title>4x4090 build running gpt-oss:20b locally - full specs</title>
    <updated>2025-10-13T17:53:32+00:00</updated>
    <author>
      <name>/u/RentEquivalent1671</name>
      <uri>https://old.reddit.com/user/RentEquivalent1671</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt; &lt;img alt="4x4090 build running gpt-oss:20b locally - full specs" src="https://external-preview.redd.it/oLekl_ORR7Cm_gsrJon__vT598RBB5Hxp4VkS8gKBSU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c8926f5bd6382bf4684a66eaf41cb4337b53990" title="4x4090 build running gpt-oss:20b locally - full specs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4j5t70ot0xuf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fce49b840afd6f046d783920b7425c7627c7cbe8"&gt;https://preview.redd.it/4j5t70ot0xuf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fce49b840afd6f046d783920b7425c7627c7cbe8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made this monster by myself. &lt;/p&gt; &lt;p&gt;Configuration: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Processor:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; AMD Threadripper PRO 5975WX&lt;/p&gt; &lt;p&gt; -32 cores / 64 threads&lt;/p&gt; &lt;p&gt; -Base/Boost clock: varies by workload&lt;/p&gt; &lt;p&gt; -Av temp: 44°C&lt;/p&gt; &lt;p&gt; -Power draw: 116-117W at 7% load&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Motherboard:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; ASUS Pro WS WRX80E-SAGE SE WIFI&lt;/p&gt; &lt;p&gt; -Chipset: WRX80E&lt;/p&gt; &lt;p&gt; -Form factor: E-ATX workstation&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Memory:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Total: 256GB DDR4-3200 ECC&lt;/p&gt; &lt;p&gt; Configuration: 8x 32GB Samsung modules&lt;/p&gt; &lt;p&gt; Type: Multi-bit ECC registered&lt;/p&gt; &lt;p&gt; Av Temperature: 32-41°C across modules&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Graphics Cards:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 4x NVIDIA GeForce RTX 4090&lt;/p&gt; &lt;p&gt; VRAM: 24GB per card (96GB total)&lt;/p&gt; &lt;p&gt; Power: 318W per card (450W limit each)&lt;/p&gt; &lt;p&gt; Temperature: 29-37°C under load&lt;/p&gt; &lt;p&gt; Utilization: 81-99%&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Storage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Samsung SSD 990 PRO 2TB NVMe&lt;/p&gt; &lt;p&gt; -Temperature: 32-37°C&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Power Supply:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 2x XPG Fusion 1600W Platinum&lt;/p&gt; &lt;p&gt; Total capacity: 3200W&lt;/p&gt; &lt;p&gt; Configuration: Dual PSU redundant&lt;/p&gt; &lt;p&gt; Current load: 1693W (53% utilization)&lt;/p&gt; &lt;p&gt; Headroom: 1507W available &lt;/p&gt; &lt;p&gt;I run &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gptoss-20b&lt;/a&gt; on each GPU and have on average 107 tokens per second. So, in total, I have like 430 t/s with 4 threads. &lt;/p&gt; &lt;p&gt;Disadvantage is, 4090 is quite old, and I would recommend to use 5090. This is my first build, this is why mistakes can happen :) &lt;/p&gt; &lt;p&gt;Advantage is, the amount of T/S. And quite good model. Of course It is not ideal and you have to make additional requests to have certain format, but my personal opinion is that gptoss-20b is the real balance between quality and quantity. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RentEquivalent1671"&gt; /u/RentEquivalent1671 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:53:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o681ap</id>
    <title>Hello, everyone.</title>
    <updated>2025-10-14T06:27:07+00:00</updated>
    <author>
      <name>/u/Particular-Honey-137</name>
      <uri>https://old.reddit.com/user/Particular-Honey-137</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o681ap/hello_everyone/"&gt; &lt;img alt="Hello, everyone." src="https://b.thumbs.redditmedia.com/Ch_sKvv0esNrS9Ms9dUcD4DxmELRZqbZTPnWFYipmRo.jpg" title="Hello, everyone." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/8hg692c2u0vf1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b672f36e2d00b2d8fdce3aa5536e97837cbebd63"&gt;https://preview.redd.it/8hg692c2u0vf1.jpg?width=1024&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b672f36e2d00b2d8fdce3aa5536e97837cbebd63&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm just a regular person who's been really into Llama lately, trying out various things. I found this place while looking for information, and this is my first time posting. I look forward to being a part of this community.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular-Honey-137"&gt; /u/Particular-Honey-137 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o681ap/hello_everyone/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o681ap/hello_everyone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o681ap/hello_everyone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T06:27:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69vm5</id>
    <title>What’s the point of a DGX Spark for inference if a Mac Studio M1 Ultra beats it at TG and equals it at PP at half the price?</title>
    <updated>2025-10-14T08:28:09+00:00</updated>
    <author>
      <name>/u/Valuable-Run2129</name>
      <uri>https://old.reddit.com/user/Valuable-Run2129</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I might be missing something here, but with the results I’ve seen, the DGX does what Apple did 3 years ago (actually worse token generation). &lt;/p&gt; &lt;p&gt;Is the DGX as bad as it seems for inference? We all knew that TG would have been shit with that bandwidth, but even prompt processing doesn’t seem great. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Valuable-Run2129"&gt; /u/Valuable-Run2129 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69vm5/whats_the_point_of_a_dgx_spark_for_inference_if_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:28:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1o69jfe</id>
    <title>Still no qwen3 next 80b gguf?</title>
    <updated>2025-10-14T08:05:39+00:00</updated>
    <author>
      <name>/u/LebiaseD</name>
      <uri>https://old.reddit.com/user/LebiaseD</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is it coming will it come?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LebiaseD"&gt; /u/LebiaseD &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o69jfe/still_no_qwen3_next_80b_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T08:05:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5qo0r</id>
    <title>It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase</title>
    <updated>2025-10-13T17:44:28+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"&gt; &lt;img alt="It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase" src="https://external-preview.redd.it/gbZbO_XMemMwlSTTx1mACM7p7UtdxxgpOKoIWv4akso.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5312d01181eaa8e15816fcf71e260612f9b1af" title="It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/karpathy/nanochat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o68jt0</id>
    <title>qwen3 coder 4b and 8b, please</title>
    <updated>2025-10-14T07:00:21+00:00</updated>
    <author>
      <name>/u/madaradess007</name>
      <uri>https://old.reddit.com/user/madaradess007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;why did qwen stop releasing small models?&lt;br /&gt; can we do it on our own? i'm on 8gb macbook air, so 8b is max for me&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/madaradess007"&gt; /u/madaradess007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o68jt0/qwen3_coder_4b_and_8b_please/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o68jt0/qwen3_coder_4b_and_8b_please/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o68jt0/qwen3_coder_4b_and_8b_please/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T07:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5nlli</id>
    <title>Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp; More</title>
    <updated>2025-10-13T15:55:32+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt; &lt;img alt="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" src="https://b.thumbs.redditmedia.com/53soxOPlO99qPaIqPE4FAHhMXBhvv7QgMGAh0UoqSHU.jpg" title="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to share &lt;strong&gt;Nanonets-OCR2&lt;/strong&gt;, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).&lt;/p&gt; &lt;p&gt;🔍 &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LaTeX Equation Recognition:&lt;/strong&gt; Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (&lt;code&gt;$...$&lt;/code&gt;) and display (&lt;code&gt;$$...$$&lt;/code&gt;) equations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Image Description:&lt;/strong&gt; Describes images within documents using structured &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Signature Detection &amp;amp; Isolation:&lt;/strong&gt; Identifies and isolates signatures from other text, outputting them within a &lt;code&gt;&amp;lt;signature&amp;gt;&lt;/code&gt; tag. This is crucial for processing legal and business documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watermark Extraction:&lt;/strong&gt; Detects and extracts watermark text from documents, placing it within a &lt;code&gt;&amp;lt;watermark&amp;gt;&lt;/code&gt; tag.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Checkbox Handling:&lt;/strong&gt; Converts form checkboxes and radio buttons into standardized Unicode symbols (&lt;code&gt;☐&lt;/code&gt;, &lt;code&gt;☑&lt;/code&gt;, &lt;code&gt;☒&lt;/code&gt;) for consistent and reliable processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Table Extraction:&lt;/strong&gt; Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flow charts &amp;amp; Organisational charts:&lt;/strong&gt; Extracts flow charts and organisational as &lt;a href="https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org"&gt;mermaid&lt;/a&gt; code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handwritten Documents:&lt;/strong&gt; The model is trained on handwritten documents across multiple languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Question Answering (VQA):&lt;/strong&gt; The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with &amp;quot;Not mentioned.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://docstrange.nanonets.com/"&gt;🖥️ Live Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://nanonets.com/research/nanonets-ocr-2"&gt;📢 Blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NanoNets/docstrange"&gt;⌨️ GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 &lt;a href="https://huggingface.co/collections/nanonets/nanonets-ocr2-68ed207f17ee6c31d226319e"&gt;Huggingface models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea00f9623db4529514533820223b2fb53be4767d"&gt;Document with equation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4a1316e250f7f244f6e253d66c8ebf1ba105313"&gt;Document with complex checkboxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8bcc88b138a553c7760d6e46319b864802339913"&gt;Quarterly Report (Please use the Markdown(Financial Docs) for best result in docstrange demo)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9"&gt;Signatures&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=008fce272c2979b00e0033c34ffcd2b0d69cb24c"&gt;mermaid code for flowchart&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jytsym6eiwuf1.png?width=2462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c"&gt;Visual Question Answering&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try it out and share your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5ptit</id>
    <title>Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.</title>
    <updated>2025-10-13T17:14:18+00:00</updated>
    <author>
      <name>/u/Dentuam</name>
      <uri>https://old.reddit.com/user/Dentuam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"&gt; &lt;img alt="Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture." src="https://external-preview.redd.it/IjppR-RE-RkBB_gQduyqs52uBDc0W1Hhz7wl-iWhgJ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bde18ad695deb84f2f7185a79fef6c32828efb7f" title="Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.&lt;/p&gt; &lt;p&gt;Ring-1T achieves silver-level IMO reasoning through pure natural language reasoning.&lt;/p&gt; &lt;p&gt;→ 1 T total / 50 B active params · 128 K context window → Reinforced by Icepop RL + ASystem (Trillion-Scale RL Engine) → Open-source SOTA in natural language reasoning — AIME 25 / HMMT 25 / ARC-AGI-1 / CodeForce&lt;/p&gt; &lt;p&gt;Deep thinking · Open weights · FP8 version available&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AntLingAGI/status/1977767599657345027?t=jx-D236A8RTnQyzLh-sC6g&amp;amp;s=19"&gt;https://x.com/AntLingAGI/status/1977767599657345027?t=jx-D236A8RTnQyzLh-sC6g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dentuam"&gt; /u/Dentuam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o6163l</id>
    <title>DGX Spark review with benchmark</title>
    <updated>2025-10-14T00:33:01+00:00</updated>
    <author>
      <name>/u/alew3</name>
      <uri>https://old.reddit.com/user/alew3</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"&gt; &lt;img alt="DGX Spark review with benchmark" src="https://external-preview.redd.it/WNdw4kTz_uFbrszyWcTmBGBzFo8R71Bs5ZxJc5c0h-o.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a6226efb1a534fbfbdcc59966a365bcdb316c259" title="DGX Spark review with benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As expected, not the best performer.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alew3"&gt; /u/alew3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/-3r2woTQjec?si=PruuNNLJVTwCYvC7"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o6163l/dgx_spark_review_with_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:33:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o623qi</id>
    <title>I tested if tiny LLMs can self-improve through memory: Qwen3-1.7B gained +8% accuracy on MATH problems</title>
    <updated>2025-10-14T01:16:29+00:00</updated>
    <author>
      <name>/u/MariusNocturnum</name>
      <uri>https://old.reddit.com/user/MariusNocturnum</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;TL;DR&lt;/h2&gt; &lt;p&gt;Implemented Google's ReasoningBank paper on small models (1.7B params). Built a memory system that extracts reasoning strategies from successful solutions and retrieves them for similar problems. &lt;strong&gt;Result: 1.7B model went from 40% → 48% accuracy on MATH Level 3-4 problems (+20% relative improvement).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Smaller models benefited MORE than larger ones.&lt;/strong&gt; Afer phase 1 is finished tuning phase 2 will attempt to answer, &amp;quot;can the model recursively improve by fine-tuning on its own successful traces?&amp;quot;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What I Built&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;reasoning-bank-slm&lt;/strong&gt; - Testing if small language models can bootstrap their reasoning ability through: 1. &lt;strong&gt;Memory extraction&lt;/strong&gt;: When the model solves a problem, extract generalizable strategies 2. &lt;strong&gt;Semantic retrieval&lt;/strong&gt;: For new problems, retrieve relevant strategies from memory 3. &lt;strong&gt;Guided solving&lt;/strong&gt;: Inject retrieved strategies as hints into the prompt 4. &lt;strong&gt;Recursive loop&lt;/strong&gt; (Phase 2): Fine-tune the model on successful reasoning traces, repeat&lt;/p&gt; &lt;p&gt;Full code on GitHub: &lt;a href="https://github.com/Lanerra/reasoning-bank-slm"&gt;https://github.com/Lanerra/reasoning-bank-slm&lt;/a&gt;&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Experimental Setup&lt;/h2&gt; &lt;p&gt;&lt;strong&gt;Hardware:&lt;/strong&gt; - Ryzen 9 7950X, 128GB RAM - RTX 4090 + RTX 3090 - Running llama-server locally&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models tested:&lt;/strong&gt; - Qwen3-1.7B-Instruct (primary) - Qwen3-4B-Instruct (comparison) - Qwen3-Embedding-0.6B (retrieval)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt; MATH Level 3-4 (harder than GSM8K) - 100 training problems → build memory bank - 100 test problems → baseline vs memory-augmented&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Design features:&lt;/strong&gt; - Answer leak prevention (filters memories containing expected answer) - Wilson confidence intervals for statistical rigor - Deterministic seeding for reproducibility&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Phase 1 Results (Qwen3-1.7B)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;With Memory&lt;/th&gt; &lt;th&gt;Change&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;Accuracy&lt;/td&gt; &lt;td&gt;40.0%&lt;/td&gt; &lt;td&gt;48.0%&lt;/td&gt; &lt;td&gt;&lt;strong&gt;+8.0%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Problems solved&lt;/td&gt; &lt;td&gt;40/100&lt;/td&gt; &lt;td&gt;48/100&lt;/td&gt; &lt;td&gt;+8&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Improvements&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;16&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Regressions&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;td&gt;-&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Net effect: +8 problems (2:1 improvement ratio)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Memory bank: 223 strategies extracted from training set&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What Actually Improved&lt;/h2&gt; &lt;p&gt;Sample problems where memory helped:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Complex plane geometry:&lt;/strong&gt; - Baseline: Failed (wrong format) - Retrieved: &amp;quot;Vector Magnitude Method&amp;quot; - Result: ✓ Correct (25π)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Polynomial analysis:&lt;/strong&gt; - Baseline: Failed (no answer) - Retrieved: &amp;quot;Equate Target Value to Function&amp;quot; - Result: ✓ Correct (5)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Fibonacci series summation:&lt;/strong&gt; - Baseline: Failed - Retrieved: &amp;quot;Coefficient Multiplication and Summation&amp;quot; - Result: ✓ Correct (1)&lt;/p&gt; &lt;p&gt;These aren't edge cases - the retrieved strategies were genuinely applicable.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Regressions (The Honest Part)&lt;/h2&gt; &lt;p&gt;8 problems got worse with memory. All showed the same pattern: model failed to produce an answer (not wrong answer, but no answer at all).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; 223 memories is too many. Retrieval pulls less-relevant strategies → context bloat → model confusion.&lt;/p&gt; &lt;p&gt;Supporting evidence: Runs with fewer memories (10, 40) had zero regressions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Fix for Phase 2:&lt;/strong&gt; Better retrieval filtering, quality thresholds, or reduce k.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Comparison: Model Size Matters&lt;/h2&gt; &lt;p&gt;Tested both 1.7B and 4B on same problems:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Baseline&lt;/th&gt; &lt;th&gt;With Memory&lt;/th&gt; &lt;th&gt;Improvement&lt;/th&gt; &lt;th&gt;Regressions&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;4B&lt;/td&gt; &lt;td&gt;76%&lt;/td&gt; &lt;td&gt;80%&lt;/td&gt; &lt;td&gt;+4%&lt;/td&gt; &lt;td&gt;0&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;1.7B&lt;/td&gt; &lt;td&gt;40%&lt;/td&gt; &lt;td&gt;48%&lt;/td&gt; &lt;td&gt;+8%&lt;/td&gt; &lt;td&gt;8&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;Key insight:&lt;/strong&gt; Smaller models benefit more from memory but are more fragile. The 4B already knows most strategies; the 1.7B needs the hints.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Why This Might Matter&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Small models can punch above their weight&lt;/strong&gt; with the right scaffolding&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory &amp;gt; parameters&lt;/strong&gt; for certain reasoning tasks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Opens path to recursive self-improvement&lt;/strong&gt;: If Phase 2 works (fine-tuning on successful traces), models could bootstrap capability without human supervision&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;h2&gt;Phase 2 Preview&lt;/h2&gt; &lt;p&gt;Next up: Can the model improve by learning from its own successes?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Loop:&lt;/strong&gt; 1. Harvest successful reasoning traces from memory bank 2. Fine-tune via LoRA on these traces 3. Test on problems the original model failed 4. Measure differential improvement 5. Hot-swap improved model, repeat&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hypothesis:&lt;/strong&gt; The 16 improvements from Phase 1 suggest the model can apply better strategies. If we fine-tune on those successful traces, can we bake the improvements in?&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Reproducibility&lt;/h2&gt; &lt;p&gt;Everything is open source. The repo includes: - Full code with fixes and improvements - Dataset preparation scripts (GSM8K and MATH) - Statistical analysis tools - Diagnostic scripts for debugging - Instructions for running locally&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hardware requirements (All models used for testing are quantized to Q8):&lt;/strong&gt; - 4.3GB+ VRAM for 4B model - 1.7GB+ VRAM for 1.7B model&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Limitations &amp;amp; Honesty&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Not statistically significant&lt;/strong&gt; (95% CI overlap) - need larger n&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Regressions exist&lt;/strong&gt; - memory can confuse small models&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extraction variance&lt;/strong&gt; - same training set produces 29-223 memories depending on run&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset ceiling&lt;/strong&gt; - 4B at 76% baseline doesn't have much room to improve&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Phase 2 unproven&lt;/strong&gt; - recursive loop might amplify errors instead of improvements&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is early research. I'm sharing to get feedback and replication attempts.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Why I'm Posting&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: Want others to check my work&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Collaboration&lt;/strong&gt;: Ideas for improving retrieval/extraction?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Curiosity&lt;/strong&gt;: Has anyone else tried this with small models?&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: This could fail spectacularly in Phase 2 - documenting either way&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If you replicate this and get different results, please let me know. Science requires replication.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Lanerra/reasoning-bank-slm"&gt;https://github.com/Lanerra/reasoning-bank-slm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback, criticisms, and replication attempts welcome. Especially interested if anyone has ideas for: - Better memory extraction methods - Smarter retrieval filtering - Handling the regression problem - Phase 2 design approaches&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MariusNocturnum"&gt; /u/MariusNocturnum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o623qi/i_tested_if_tiny_llms_can_selfimprove_through/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T01:16:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1o61gzs</id>
    <title>Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8</title>
    <updated>2025-10-14T00:47:06+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt; &lt;img alt="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" src="https://preview.redd.it/fjr53w0m4zuf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b1986eb8662405e67e0522e5d8d37f03ea577ffc" title="Nvidia breakthrough gives 4-bit pretraining technique the accuracy of FP8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;-NVFP4 is a way to store numbers for training large models using just 4 bits instead of 8 or 16. This makes training faster and use less memory&lt;/p&gt; &lt;p&gt;-NVFP4 shows 4-bit pretraining of a 12B Mamba Transformer on 10T tokens can match FP8 accuracy while cutting compute and memory.&lt;/p&gt; &lt;p&gt;-The validation loss stays within 1% of FP8 for most of training and grows to about 1.5% late during learning rate decay. &lt;/p&gt; &lt;p&gt;-Task scores stay close, for example MMLU Pro 62.58% vs 62.62%, while coding dips a bit like MBPP+ 55.91% vs 59.11%.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/godofprompt/status/1977678347879714912"&gt;X thread&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="http://arxiv.org/abs/2509.25149"&gt;Arxiv paper&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fjr53w0m4zuf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o61gzs/nvidia_breakthrough_gives_4bit_pretraining/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-14T00:47:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5v78n</id>
    <title>The top open models on are now all by Chinese companies</title>
    <updated>2025-10-13T20:27:10+00:00</updated>
    <author>
      <name>/u/k_schaul</name>
      <uri>https://old.reddit.com/user/k_schaul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt; &lt;img alt="The top open models on are now all by Chinese companies" src="https://preview.redd.it/xhsv9ilkuxuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f3ce5e0a0548bdb8546f46e0f43b1b008af719" title="The top open models on are now all by Chinese companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full analysis here (🎁 gift link): &lt;a href="https://wapo.st/4nPUBud"&gt;wapo.st/4nPUBud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_schaul"&gt; /u/k_schaul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhsv9ilkuxuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
