<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-01T15:52:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qs27hf</id>
    <title>g-HOOT in the Machine</title>
    <updated>2026-01-31T13:21:43+00:00</updated>
    <author>
      <name>/u/TheVeryNearFuture</name>
      <uri>https://old.reddit.com/user/TheVeryNearFuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt; &lt;img alt="g-HOOT in the Machine" src="https://preview.redd.it/z78lvao9rogg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae89cf23b154560e4ea34ce2fff5ea8a457a781b" title="g-HOOT in the Machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.14805"&gt;https://arxiv.org/abs/2507.14805&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheVeryNearFuture"&gt; /u/TheVeryNearFuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z78lvao9rogg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsxlol</id>
    <title>Best local opensource LLM to translate large bodies of text?</title>
    <updated>2026-02-01T12:48:11+00:00</updated>
    <author>
      <name>/u/brazilianmonkey1</name>
      <uri>https://old.reddit.com/user/brazilianmonkey1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have ChatGPT but when I try to translate transcripts from videos with 1h~2h+ or 300 page documents or books, etc. the model is really inconsistent even if you ask it to &amp;quot;continue translating from where you stopped&amp;quot;. Maybe it's a skill issue, maybe you're supposed to send it in clunks of texts, but then it becomes a boring manual process of ctrl c + v.&lt;/p&gt; &lt;p&gt;So is there a free alternative (since I don't want to end up paying twice as I don't plan on unsubbing to ChatGPT) that I can download and use on my PC?&lt;/p&gt; &lt;p&gt;Please have in mind I'm a noob and don't understand much how to set up these things, I tried ComfyUI once for image models but didn't manage to get it running and I need it to be light prob under 8gb of ram since I have 16gb in theory but like if I open a web browser it goes to 12gb of use it's kinda crazy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brazilianmonkey1"&gt; /u/brazilianmonkey1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxlol/best_local_opensource_llm_to_translate_large/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxlol/best_local_opensource_llm_to_translate_large/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxlol/best_local_opensource_llm_to_translate_large/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:48:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsxpa3</id>
    <title>MC62-G40 Mainboard for multi-GPU setup?</title>
    <updated>2026-02-01T12:53:06+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So my trajectory is a classical one:&lt;/p&gt; &lt;p&gt;Mini-PC with eGPU -&amp;gt; PC with two GPUs (x) -&amp;gt; Multi-GPU in former miner frame.&lt;/p&gt; &lt;p&gt;I was thinking about using an acceptable priced MC62-G40 mobo that seems to have all bells and whistles that I may need and I was wondering if someone else uses it and if they have advice for the best CPU and generally for the best performance and possible issues.&lt;/p&gt; &lt;p&gt;Any advice is appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxpa3/mc62g40_mainboard_for_multigpu_setup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxpa3/mc62g40_mainboard_for_multigpu_setup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxpa3/mc62g40_mainboard_for_multigpu_setup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:53:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsyje8</id>
    <title>chatllm.cpp supports Qwen3-ASR and ForcedAligner</title>
    <updated>2026-02-01T13:31:54+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;chatllm.cpp supports Qwen3-ASR and ForcedAligner.&lt;/p&gt; &lt;h2&gt;1. speech recognition with Qwen3-ASR&lt;/h2&gt; &lt;p&gt;``&lt;code&gt; main.exe --multimedia-file-tags {{ }} -i -m ...\qwen3-asr-1.7b.bin ________ __ __ __ __ ___ / ____/ /_ ____ _/ /_/ / / / / |/ /_________ ____ / / / __ \/ __&lt;/code&gt;/ &lt;strong&gt;/ / / / / /|&lt;em&gt;/ // _&lt;/em&gt;&lt;em&gt;/ _&lt;/em&gt; / __ \ / /&lt;/strong&gt;&lt;em&gt;/ / / / /&lt;/em&gt;/ / /&lt;em&gt;/ /&lt;/em&gt;&lt;strong&gt;/ /&lt;/strong&gt;&lt;em&gt;/ / / // /&lt;/em&gt;&lt;em&gt;/ /&lt;/em&gt;/ / /&lt;em&gt;/ / \&lt;/em&gt;&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;/ /_/\&lt;/strong&gt;,&lt;em&gt;/\&lt;/em&gt;&lt;em&gt;/&lt;/em&gt;_&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;/&lt;em&gt;/ /&lt;/em&gt;(_)&lt;/strong&gt;&lt;em&gt;/ .&lt;/em&gt;&lt;strong&gt;/ .&lt;/strong&gt;&lt;em&gt;/ You are served by Qwen3-ASR, /&lt;/em&gt;/ /_/ with 2031739904 (2.0B) parameters.&lt;/p&gt; &lt;p&gt;File &amp;gt; ...\obama.mp3 language English&amp;lt;asr_text&amp;gt;This week, I travel to Chicago to deliver my final farewell address to the nation. Following in the tradition of presidents before me, it was an opportunity to say thank you. ... ```&lt;/p&gt; &lt;h2&gt;2. add time stamps (align text &amp;amp; audio)&lt;/h2&gt; &lt;p&gt;``&lt;code&gt; main.exe --multimedia-file-tags {{ }} -i -m ..\qwen3-focedaligner-0.6b.bin --set delimiter &amp;quot;|&amp;quot; --set language english ________ __ __ __ __ ___ / ____/ /_ ____ _/ /_/ / / / / |/ /_________ ____ / / / __ \/ __&lt;/code&gt;/ &lt;strong&gt;/ / / / / /|&lt;em&gt;/ // _&lt;/em&gt;&lt;em&gt;/ _&lt;/em&gt; / __ \ / /&lt;/strong&gt;&lt;em&gt;/ / / / /&lt;/em&gt;/ / /&lt;em&gt;/ /&lt;/em&gt;&lt;strong&gt;/ /&lt;/strong&gt;&lt;em&gt;/ / / // /&lt;/em&gt;&lt;em&gt;/ /&lt;/em&gt;/ / /&lt;em&gt;/ / \&lt;/em&gt;&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;/ /_/\&lt;/strong&gt;,&lt;em&gt;/\&lt;/em&gt;&lt;em&gt;/&lt;/em&gt;_&lt;strong&gt;&lt;em&gt;/&lt;/em&gt;&lt;/strong&gt;&lt;strong&gt;/&lt;em&gt;/ /&lt;/em&gt;(_)&lt;/strong&gt;&lt;em&gt;/ .&lt;/em&gt;&lt;strong&gt;/ .&lt;/strong&gt;&lt;em&gt;/ You are served by Qwen3-ForcedAligner, /&lt;/em&gt;/ /_/ with 601300992 (0.6B) parameters.&lt;/p&gt; &lt;p&gt;You &amp;gt; {{audio:...\obama.mp3}}This week, I travel to Chicago to deliver my final farewell address to the nation.| Following in the tradition of presidents before me, it was an opportunity to say thank you.| ...&lt;/p&gt; &lt;p&gt;A.I. &amp;gt; 0 00:00:00,800 --&amp;gt; 00:00:05,360 This week, I travel to Chicago to deliver my final farewell address to the nation.&lt;/p&gt; &lt;p&gt;1 00:00:06,000 --&amp;gt; 00:00:10,880 Following in the tradition of presidents before me, it was an opportunity to say thank you.&lt;/p&gt; &lt;p&gt;.... ``` &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsyje8/chatllmcpp_supports_qwen3asr_and_forcedaligner/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsyje8/chatllmcpp_supports_qwen3asr_and_forcedaligner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsyje8/chatllmcpp_supports_qwen3asr_and_forcedaligner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T13:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsub12</id>
    <title>[OSS] Kakveda ‚Äì Failure intelligence &amp; pre-flight warnings for LLM systems</title>
    <updated>2026-02-01T09:47:40+00:00</updated>
    <author>
      <name>/u/Street_Pop9758</name>
      <uri>https://old.reddit.com/user/Street_Pop9758</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sharing Kakveda, an open-source project that explores failure intelligence&lt;/p&gt; &lt;p&gt;for LLM and agent-based systems.&lt;/p&gt; &lt;p&gt;It focuses on remembering recurring failure modes and providing pre-flight&lt;/p&gt; &lt;p&gt;‚Äúthis failed before‚Äù warnings instead of treating failures as logs.&lt;/p&gt; &lt;p&gt;Runs locally via Docker Compose.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/prateekdevisingh/kakveda"&gt;https://github.com/prateekdevisingh/kakveda&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs: &lt;a href="https://kakveda.com"&gt;https://kakveda.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback on the idea and architecture.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Street_Pop9758"&gt; /u/Street_Pop9758 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsub12/oss_kakveda_failure_intelligence_preflight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsub12/oss_kakveda_failure_intelligence_preflight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsub12/oss_kakveda_failure_intelligence_preflight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T09:47:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qszphr</id>
    <title>What are the best collection of small models to run on 8gb ram?</title>
    <updated>2026-02-01T14:22:04+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably different models for different use cases.&lt;/p&gt; &lt;p&gt;Coding (python, Java, html, js, css)&lt;/p&gt; &lt;p&gt;Math &lt;/p&gt; &lt;p&gt;Language (translation / learning)&lt;/p&gt; &lt;p&gt;Emotional support / therapy- like &lt;/p&gt; &lt;p&gt;Conversational&lt;/p&gt; &lt;p&gt;General knowledge&lt;/p&gt; &lt;p&gt;Instruction following &lt;/p&gt; &lt;p&gt;Image analysis/ vision &lt;/p&gt; &lt;p&gt;Creative writing / world building &lt;/p&gt; &lt;p&gt;RAG&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qszphr/what_are_the_best_collection_of_small_models_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qszphr/what_are_the_best_collection_of_small_models_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qszphr/what_are_the_best_collection_of_small_models_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T14:22:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt0qvj</id>
    <title>Running a SHA-256 Hash-Chained Multi-Agent LLM Discourse locally on Android (Termux + llama3.2:3b)</title>
    <updated>2026-02-01T15:03:33+00:00</updated>
    <author>
      <name>/u/NeoLogic_Dev</name>
      <uri>https://old.reddit.com/user/NeoLogic_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt0qvj/running_a_sha256_hashchained_multiagent_llm/"&gt; &lt;img alt="Running a SHA-256 Hash-Chained Multi-Agent LLM Discourse locally on Android (Termux + llama3.2:3b)" src="https://preview.redd.it/wi8v8rlhewgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4e2cf7d113222f8a0be057a0b4acda289a8f83a2" title="Running a SHA-256 Hash-Chained Multi-Agent LLM Discourse locally on Android (Termux + llama3.2:3b)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While most discussions around local LLMs focus on benchmarks or fine-tuning, I wanted to explore something different: auditability, epistemic boundaries, and refusal as a measurable property ‚Äî fully offline. Setup Device: Android smartphone Environment: Termux Runtime: Ollama Model: llama3.2:3b (local, no network access) Architecture: Multi-agent discourse with strict role separation One anchoring agent (‚ÄúDominus‚Äù) Multiple debating agents Integrity layer: SHA-256 hash chaining Every agent response includes the hash of the previous state Creates a tamper-evident, append-only discourse log Why hash-chaining? Most AI ‚Äúdebates‚Äù collapse into unverifiable text streams. Here, each turn cryptographically commits to the prior one, producing raw, auditable data instead of summaries or interpretations. This allows: Post-hoc verification External analysis Detection of retroactive manipulation Reproducible discourse states Observation Under these constraints, something interesting happens: The agents systematically refuse to speculate beyond defined premises. They explicitly acknowledge missing context and halt rather than hallucinate ‚Äî as long as the ‚Äúvirtual space‚Äù they operate in remains undefined. No claims about consciousness here. But very clear evidence of algorithmic boundary recognition under integrity pressure. Why on a phone? Because local sovereignty matters. This runs entirely offline, on commodity hardware, without cloud inference, APIs, or hidden system prompts.&lt;/p&gt; &lt;p&gt;I‚Äôm curious how others in this community would interpret refusal, boundary signaling, and integrity constraints in local models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeoLogic_Dev"&gt; /u/NeoLogic_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wi8v8rlhewgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt0qvj/running_a_sha256_hashchained_multiagent_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt0qvj/running_a_sha256_hashchained_multiagent_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T15:03:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qt1oni</id>
    <title>From JSON rules to an AI governance execution layer: making LLM behavior observable (not prompt engineering)</title>
    <updated>2026-02-01T15:39:20+00:00</updated>
    <author>
      <name>/u/Sad_Perception3670</name>
      <uri>https://old.reddit.com/user/Sad_Perception3670</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/"&gt; &lt;img alt="From JSON rules to an AI governance execution layer: making LLM behavior observable (not prompt engineering)" src="https://a.thumbs.redditmedia.com/wdx9uVdei2cPzqEX3tGTvnEFL_BsvP6YRB2Whk98yo4.jpg" title="From JSON rules to an AI governance execution layer: making LLM behavior observable (not prompt engineering)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In a previous post, I shared a JSON-defined rule system to make LLM behavior explicit in teaching and model comparison. &lt;/p&gt; &lt;p&gt;Since then, I‚Äôve taken the next step:&lt;br /&gt; I built a thin execution layer (‚Äúwrapper‚Äù) around the rules to make them &lt;strong&gt;operational&lt;/strong&gt;, &lt;strong&gt;testable&lt;/strong&gt;, and &lt;strong&gt;stable across sessions&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;This is not about better prompts.&lt;br /&gt; It is about separating &lt;strong&gt;interaction rules&lt;/strong&gt; from &lt;strong&gt;task content&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;What changed compared to the pure JSON approach&lt;/strong&gt;&lt;br /&gt; - the rules are now &lt;strong&gt;actively enforced&lt;/strong&gt;, not just described&lt;br /&gt; - state (profiles, overlays, reasoning mode) is explicit and visible&lt;br /&gt; - violations and drift are surfaced instead of silently absorbed&lt;br /&gt; - the same rules can be applied across different providers and models &lt;/p&gt; &lt;p&gt;The goal is not convenience, but &lt;strong&gt;observability&lt;/strong&gt;:&lt;br /&gt; you can see &lt;strong&gt;when&lt;/strong&gt; a model complies, deviates, or fails under the same rules. &lt;/p&gt; &lt;p&gt;Why this is not prompt engineering&lt;br /&gt; Prompts address the &lt;strong&gt;content level&lt;/strong&gt;.&lt;br /&gt; This layer operates on the workflow and control level:&lt;br /&gt; - standalone commands instead of implicit mode switches&lt;br /&gt; - explicit profiles instead of stylistic guessing&lt;br /&gt; - structured reasoning paths that can be switched, audited, or disabled&lt;br /&gt; - quality signals and self-debunking triggered by rules, not wording &lt;/p&gt; &lt;p&gt;Below are three screenshots that illustrate this separation&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sz5za5rgjwgg1.png?width=2966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8581619a17a7a3031e446d337dfdbfab97add850"&gt;Image 1 ‚Äî Explicit system state - All interaction parameters are visible and inspectable.Nothing is inferred from wording or conversation history.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/4kvjo1whjwgg1.png?width=2966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf10bec42cb221689d29aae8ae9cb05ed6cd053a"&gt;Image 2 ‚Äî Reasoning as a selectable workflow - Reasoning is chosen explicitly (or disabled).Different reasoning paths become a variable that can be compared.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/emom9ouijwgg1.png?width=2966&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ac46dd274af71314014e92a3774a5ebf89932fe5"&gt;Image 3 ‚Äî Rule enforcement instead of silent drift - The system flags uncertainty, missing markers, and structural violations.Weaknesses are made visible instead of hidden behind fluent text.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This wrapper does not make models ‚Äúcorrect‚Äù or ‚Äúsafe‚Äù.&lt;br /&gt; It makes their behavior &lt;strong&gt;explicit&lt;/strong&gt;, &lt;strong&gt;comparable&lt;/strong&gt;, and &lt;strong&gt;discussable&lt;/strong&gt;. &lt;/p&gt; &lt;p&gt;Repository (rules + wrapper + tests):&lt;br /&gt; &lt;a href="https://github.com/vfi64/wrapper"&gt;https://github.com/vfi64/wrapper&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I‚Äôm especially interested in feedback from:&lt;br /&gt; - people comparing models&lt;br /&gt; - educators working on AI literacy&lt;br /&gt; - anyone who has hit the limits of prompt-based control&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sad_Perception3670"&gt; /u/Sad_Perception3670 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qt1oni/from_json_rules_to_an_ai_governance_execution/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T15:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjya0</id>
    <title>Just wanted to post about a cool project, the internet is sleeping on.</title>
    <updated>2026-02-01T00:59:08+00:00</updated>
    <author>
      <name>/u/daLazyModder</name>
      <uri>https://old.reddit.com/user/daLazyModder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/frothywater/kanade-tokenizer"&gt;https://github.com/frothywater/kanade-tokenizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is a audio tokenizer that has been optimized and can do really fast voice cloning. With super fast realtime factor. Can even run on cpu faster then realtime. I vibecoded a fork with gui for gradio and a tkinter realtime gui for it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dalazymodder/kanade-tokenizer"&gt;https://github.com/dalazymodder/kanade-tokenizer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Honestly I think it blows rvc out of the water for real time factor and one shotting it.&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/1G1YU3SvGFsf"&gt;https://vocaroo.com/1G1YU3SvGFsf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://vocaroo.com/1j630aDND3d8"&gt;https://vocaroo.com/1j630aDND3d8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;example of ljspeech to kokoro voice&lt;/p&gt; &lt;p&gt;the cloning could be better but the rtf is crazy fast considering the quality.&lt;/p&gt; &lt;p&gt;Minor Update: Updated the gui with more clear instructions on the fork and the streaming for realtime works better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daLazyModder"&gt; /u/daLazyModder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjya0/just_wanted_to_post_about_a_cool_project_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjya0/just_wanted_to_post_about_a_cool_project_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjya0/just_wanted_to_post_about_a_cool_project_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:59:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsh7dz</id>
    <title>Analyzed 5,357 ICLR 2026 accepted papers - here's what the research community is actually working on</title>
    <updated>2026-01-31T23:03:24+00:00</updated>
    <author>
      <name>/u/dippatel21</name>
      <uri>https://old.reddit.com/user/dippatel21</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Went through the accepted papers at ICLR 2026 and counted what the research community is actually focusing on. Some findings that seem relevant for people doing local training and fine-tuning:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Alignment methods&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GRPO appears in 157 papers, DPO in only 55&lt;/li&gt; &lt;li&gt;The academic community seems to have largely moved past DPO toward Group Relative Policy Optimization&lt;/li&gt; &lt;li&gt;If you're still using DPO for post-training, might be worth looking into GRPO&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RLVR over RLHF&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;125 papers on Reinforcement Learning with Verifiable Rewards vs 54 for RLHF&lt;/li&gt; &lt;li&gt;The shift is toward domains where correctness is programmatically checkable (math, code, logic) rather than relying on human preference data&lt;/li&gt; &lt;li&gt;Makes sense for local work since you don't need expensive human annotation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Data efficiency finding&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper called &amp;quot;Nait&amp;quot; (Neuron-Aware Instruction Tuning) shows training on 10% of Alpaca-GPT4, selected by neuron activation patterns, outperforms training on 100%&lt;/li&gt; &lt;li&gt;Implication: most instruction tuning data is redundant. Smart selection &amp;gt; more data&lt;/li&gt; &lt;li&gt;Could matter a lot for compute-constrained local training&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Test-time compute&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;257 papers on test-time training/adaptation/scaling&lt;/li&gt; &lt;li&gt;This is now mainstream, not experimental&lt;/li&gt; &lt;li&gt;Relevant for inference optimization on local hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Mamba/SSMs&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;202 papers mention Mamba or state space models&lt;/li&gt; &lt;li&gt;Not dead, still an active research direction&lt;/li&gt; &lt;li&gt;Worth watching for potential attention alternatives that run better on consumer hardware&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Security concern for agents&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;MCP Security Bench shows models with better instruction-following are MORE vulnerable to prompt injection via tool outputs&lt;/li&gt; &lt;li&gt;The &amp;quot;capability-vulnerability paradox&amp;quot; - something to consider if you're building local agents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hallucination&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;123 papers on hallucination, 125 on factuality&lt;/li&gt; &lt;li&gt;Still unsolved but heavily researched&lt;/li&gt; &lt;li&gt;One interesting approach treats it as retrieval grounding rather than generation problem&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What are your thoughts on the trend? Noticed anything interesting?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dippatel21"&gt; /u/dippatel21 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsh7dz/analyzed_5357_iclr_2026_accepted_papers_heres/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsh7dz/analyzed_5357_iclr_2026_accepted_papers_heres/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsh7dz/analyzed_5357_iclr_2026_accepted_papers_heres/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T23:03:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsj8x4</id>
    <title>Beating GPT-2 for &lt;&lt;$100: the nanochat journey ¬∑ karpathy nanochat ¬∑ Discussion #481</title>
    <updated>2026-02-01T00:28:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsj8x4/beating_gpt2_for_100_the_nanochat_journey/"&gt; &lt;img alt="Beating GPT-2 for &amp;lt;&amp;lt;$100: the nanochat journey ¬∑ karpathy nanochat ¬∑ Discussion #481" src="https://external-preview.redd.it/tRLTF88fq1bbkxeX1rd0tIqyBPPusWm9EVZ_4pC6axI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a46384d561fdba570fc99a8b65c323f9f2075d5" title="Beating GPT-2 for &amp;lt;&amp;lt;$100: the nanochat journey ¬∑ karpathy nanochat ¬∑ Discussion #481" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seven years after GPT-2, you can now beat it for &amp;lt;$100.&lt;br /&gt; Andrej Karpathy shows a 3-hour training run on 8√óH100 that edges past GPT-2 on the CORE benchmark.&lt;br /&gt; He shares the architecture/optimizer tweaks, the data setup, and a simple script to reproduce it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/karpathy/nanochat/discussions/481"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsj8x4/beating_gpt2_for_100_the_nanochat_journey/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsj8x4/beating_gpt2_for_100_the_nanochat_journey/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:28:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsjqdl</id>
    <title>Are small models actually getting more efficient?</title>
    <updated>2026-02-01T00:49:26+00:00</updated>
    <author>
      <name>/u/estebansaa</name>
      <uri>https://old.reddit.com/user/estebansaa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;‚Äôm trying to understand whether small models (say, sub-1 GB or around that range) are genuinely getting &lt;em&gt;smarter&lt;/em&gt;, or if hard size limits mean they‚Äôll always hit a ceiling.&lt;/p&gt; &lt;p&gt;My long-term hope is that we eventually see a small local model reach something close to &lt;strong&gt;Gemini 2.5‚Äìlevel reasoning&lt;/strong&gt;, at least for constrained tasks. The use case I care about is games: I‚Äôd love to run an LLM locally inside a game to handle logic, dialogue, and structured outputs.&lt;/p&gt; &lt;p&gt;Right now my game depends on an API model (Gemini 3 Flash). It works great, but obviously that‚Äôs not viable for selling a game long-term if it requires an external API.&lt;/p&gt; &lt;p&gt;So my question is:&lt;br /&gt; Do you think we‚Äôll see, in the not-too-distant future, a &lt;strong&gt;small local model&lt;/strong&gt; that can reliably:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate strict JSON&lt;/li&gt; &lt;li&gt;Reason at roughly Gemini 3 Flash levels (or close)&lt;/li&gt; &lt;li&gt;Handle large contexts (ideally 50k‚Äì100k tokens)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Or are we fundamentally constrained by model size here, with improvements mostly coming from scale rather than efficiency?&lt;/p&gt; &lt;p&gt;Curious to hear thoughts from people following quantization, distillation, MoE, and architectural advances closely.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/estebansaa"&gt; /u/estebansaa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjqdl/are_small_models_actually_getting_more_efficient/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjqdl/are_small_models_actually_getting_more_efficient/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsjqdl/are_small_models_actually_getting_more_efficient/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T00:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qswba2</id>
    <title>Llama 3.2 3B on Snapdragon 8 Elite: CPU is fast, but how do we unlock the NPU/GPU in Termux? üöÄ</title>
    <updated>2026-02-01T11:41:49+00:00</updated>
    <author>
      <name>/u/NeoLogic_Dev</name>
      <uri>https://old.reddit.com/user/NeoLogic_Dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qswba2/llama_32_3b_on_snapdragon_8_elite_cpu_is_fast_but/"&gt; &lt;img alt="Llama 3.2 3B on Snapdragon 8 Elite: CPU is fast, but how do we unlock the NPU/GPU in Termux? üöÄ" src="https://preview.redd.it/8hdxiuxhevgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dab5759cbcfe82848a7c42507951b94d2d2acc2e" title="Llama 3.2 3B on Snapdragon 8 Elite: CPU is fast, but how do we unlock the NPU/GPU in Termux? üöÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve spent the last few hours optimizing Llama 3.2 3B on the new Snapdragon 8 Elite via Termux. After some environment tuning, the setup is rock solid‚Äîmemory management is no longer an issue, and the Oryon cores are absolutely ripping through tokens. However, running purely on CPU feels like owning a Ferrari and never leaving second gear. I want to tap into the Adreno 830 GPU or the Hexagon NPU to see what this silicon can really do. The Challenge: Standard Ollama/llama.cpp builds in Termux default to CPU. I‚Äôm looking for anyone who has successfully bridged the gap to the hardware accelerators on this specific chip. Current leads I'm investigating: OpenCL/Vulkan Backends: Qualcomm recently introduced a new OpenCL GPU backend for llama.cpp specifically for Adreno. Has anyone successfully compiled this in Termux with the correct libOpenCL.so links from /system/vendor/lib64?.&lt;br /&gt; QNN (Qualcomm AI Engine Direct): There are experimental GGML_HTP (Hexagon Tensor Processor) backends appearing in some research forks. Has anyone managed to get the QNN SDK libraries working natively in Termux to offload the KV cache?. Vulkan via Turnip: With the Adreno 8-series being so new, are the current Turnip drivers stable enough for llama-cpp-backend-vulkan?. If you‚Äôve moved past CPU-only inference on the 8 Elite, how did you handle the library dependencies? Let‚Äôs figure out how to make neobild the fastest mobile LLM implementation out there. üõ†Ô∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeoLogic_Dev"&gt; /u/NeoLogic_Dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8hdxiuxhevgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qswba2/llama_32_3b_on_snapdragon_8_elite_cpu_is_fast_but/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qswba2/llama_32_3b_on_snapdragon_8_elite_cpu_is_fast_but/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T11:41:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsx9r0</id>
    <title>Ultra-Sparse MoEs are the future</title>
    <updated>2026-02-01T12:31:51+00:00</updated>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPT-OSS-120B,Qwen3-Next-80B-A3B etc.. we need more of the ultra-sparse MoEs! Like we can create a 120B that uses fine-grained expert system ‚Üí distill it into a 30B A3B ‚Üí again into 7B A1B all trained in MXFP4?&lt;/p&gt; &lt;p&gt;That would be perfect because it solves the issue of direct distillation (model can't approximate the much larger teacher internal representations due to high complexity) while allowing to run models on actual consumer hardware from 96-128GB of ram ‚Üí 24GB GPUs ‚Üí 8GB GPUs.&lt;/p&gt; &lt;p&gt;A more efficient reasoning would be also a great idea! I noticed that specifically in GPT-OSS-120B (low) where it thinks in 1 or 2 words and follows a specific structure we had a great advancement for spec decoding for that model because it's predictable so it's faster.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx9r0/ultrasparse_moes_are_the_future/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsxvt8</id>
    <title>Am I crazy for wanting a model that's intentionally smaller and more human-like instead of chasing max performance?</title>
    <updated>2026-02-01T13:01:52+00:00</updated>
    <author>
      <name>/u/t0x3e8</name>
      <uri>https://old.reddit.com/user/t0x3e8</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anyone else want a model that's intentionally smaller and more human-like?&lt;/p&gt; &lt;p&gt;I'm looking for something that talks like a normal person, not trying to sound super smart, just good at having a conversation. A model that knows when it doesn't know something and just says so.&lt;/p&gt; &lt;p&gt;Everyone's chasing the biggest, smartest models, but I want something balanced and conversational. Something that runs on regular hardware and feels more like talking to a person than a computer trying too hard to impress you.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Does something like this exist, or is everyone just focused on making models as powerful as possible?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/t0x3e8"&gt; /u/t0x3e8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxvt8/am_i_crazy_for_wanting_a_model_thats/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxvt8/am_i_crazy_for_wanting_a_model_thats/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxvt8/am_i_crazy_for_wanting_a_model_thats/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T13:01:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsenpy</id>
    <title>Don‚Äôt buy b60 for LLMs</title>
    <updated>2026-01-31T21:21:10+00:00</updated>
    <author>
      <name>/u/damirca</name>
      <uri>https://old.reddit.com/user/damirca</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I kinda regret buying b60. I thought that 24gb for 700 eur is a great deal, but the reality is completely different.&lt;/p&gt; &lt;p&gt;For starters, I live with a custom compiled kernel with the patch from an Intel dev to solve ffmpeg crashes.&lt;/p&gt; &lt;p&gt;Then I had to install the card into a windows machine in order to get GPU firmware updated (under Linux one need v2.0.19 of fwupd which is not available in Ubuntu yet) to solve the crazy fan speed on the b60 even when the temp of the gpu is 30 degrees Celsius.&lt;/p&gt; &lt;p&gt;But even after solving all of this, the actual experience doing local LLM on b60 is meh.&lt;/p&gt; &lt;p&gt;On llama.cpp the card goes crazy every time it does inference: fans go super high then low, the high again. The speed is about 10-15tks at best in models like mistral 14b. The noise level is just unbearable.&lt;/p&gt; &lt;p&gt;So the only reliable way is intel‚Äôs llm-scaler, but as of now it‚Äôs based on vllm 0.11.1 whereas latest version of vllm is 0.15. So Intel is like 6 months behind which is an eternity in this AI bubble times. For example any of new mistral models are not supported and one cannot run them on vanilla vllm too.&lt;/p&gt; &lt;p&gt;With llm-scaler the behavior of the card is ok: when it‚Äôs doing inference the fan goes louder and stays louder as long is it‚Äôs needed. The speed is like 20-25 tks on qwen3 VL 8b. However there are only some models that work with llm-scaler and most of them only with fp8, so for example qwen3 VL 8b after some requests processed with 16k length takes 20gb. That kinda bad: you have 24gb of vram but you cannot run normally 30b model with q4 quant and has to stick with 8b model with fp8.&lt;/p&gt; &lt;p&gt;Overall I think XFX 7900XTX would have been much better deal: same 24gb, 2x faster, in Dec the price was only 50 eur more than b60, it can run newest models with newest llama.cpp versions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/damirca"&gt; /u/damirca &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsenpy/dont_buy_b60_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T21:21:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1qssxhx</id>
    <title>Research: vllm-mlx on Apple Silicon achieves 21% to 87% higher throughput than llama.cpp</title>
    <updated>2026-02-01T08:26:21+00:00</updated>
    <author>
      <name>/u/Synor</name>
      <uri>https://old.reddit.com/user/Synor</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Synor"&gt; /u/Synor &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2601.19139v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qssxhx/research_vllmmlx_on_apple_silicon_achieves_21_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qssxhx/research_vllmmlx_on_apple_silicon_achieves_21_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T08:26:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsy0gg</id>
    <title>Deepseek v4/3.5 is probably coming out tomorrow or in the next 5 days?</title>
    <updated>2026-02-01T13:07:49+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Are you ready for an llm with engrams? Perhaps it has even vision? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsy0gg/deepseek_v435_is_probably_coming_out_tomorrow_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T13:07:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsvgsh</id>
    <title>some uncensored models</title>
    <updated>2026-02-01T10:53:41+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since there haven‚Äôt been any (major) new local model releases lately, let‚Äôs check what uncensored models are available on Hugging Face. There are different abliteration methods, so varioud models can behave quite differently. Unfortunately, I can‚Äôt find any Nemotron-3 Nano variants.&lt;/p&gt; &lt;p&gt;Which one do you use?&lt;/p&gt; &lt;p&gt;GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF"&gt;https://huggingface.co/DavidAU/GLM-4.7-Flash-Uncensored-Heretic-NEO-CODE-Imatrix-MAX-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Huihui-GLM-4.7-Flash-abliterated-GGUF"&gt;https://huggingface.co/mradermacher/Huihui-GLM-4.7-Flash-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Olafangensan/GLM-4.7-Flash-heretic-GGUF"&gt;https://huggingface.co/Olafangensan/GLM-4.7-Flash-heretic-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT OSS 20B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf"&gt;https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-abliterated-uncensored-NEO-Imatrix-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf"&gt;https://huggingface.co/DavidAU/OpenAi-GPT-oss-20b-HERETIC-uncensored-NEO-Imatrix-gguf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-20b-BF16-abliterated-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/p-e-w_gpt-oss-20b-heretic-GGUF"&gt;https://huggingface.co/bartowski/p-e-w_gpt-oss-20b-heretic-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT OSS 120B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-gpt-oss-120b-BF16-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF"&gt;https://huggingface.co/bartowski/kldzj_gpt-oss-120b-heretic-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma 12B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DreamFast/gemma-3-12b-it-heretic"&gt;https://huggingface.co/DreamFast/gemma-3-12b-it-heretic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-12b-it-abliterated-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Gemma 27B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF"&gt;https://huggingface.co/mlabonne/gemma-3-27b-it-abliterated-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-i1-GGUF"&gt;https://huggingface.co/mradermacher/gemma-3-27b-it-heretic-v2-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 30B A3B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-30B-A3B-Instruct-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-30B-A3B-abliterated-v2"&gt;https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen3-30B-A3B-abliterated-v2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 8B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/DavidAU/Qwen3-8B-Hivemind-Instruct-Heretic-Abliterated-Uncensored-NEO-Imatrix-GGUF"&gt;https://huggingface.co/DavidAU/Qwen3-8B-Hivemind-Instruct-Heretic-Abliterated-Uncensored-NEO-Imatrix-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated"&gt;https://huggingface.co/huihui-ai/Huihui-Qwen3-VL-8B-Instruct-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen 32B&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3-VL-32B-Instruct-heretic-v2-GGUF"&gt;https://huggingface.co/mradermacher/Qwen3-VL-32B-Instruct-heretic-v2-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/huihui-ai/Qwen3-32B-abliterated"&gt;https://huggingface.co/huihui-ai/Qwen3-32B-abliterated&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsvgsh/some_uncensored_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T10:53:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsn78m</id>
    <title>Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site</title>
    <updated>2026-02-01T03:25:12+00:00</updated>
    <author>
      <name>/u/georgemoore13</name>
      <uri>https://old.reddit.com/user/georgemoore13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"&gt; &lt;img alt="Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site" src="https://external-preview.redd.it/bfjA2IIU81Nyg02gojI5OSsrZO__DkXhWMj0JjL44MA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b3c6a4dfe6f5b662bd4fcb95ee7ed9af6ee839f" title="Exposed Moltbook Database Let Anyone Take Control of Any AI Agent on the Site" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/georgemoore13"&gt; /u/georgemoore13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.404media.co/exposed-moltbook-database-let-anyone-take-control-of-any-ai-agent-on-the-site/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsn78m/exposed_moltbook_database_let_anyone_take_control/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T03:25:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsxowq</id>
    <title>OLMO 3.5 Is Around The Corner</title>
    <updated>2026-02-01T12:52:34+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"&gt; &lt;img alt="OLMO 3.5 Is Around The Corner" src="https://preview.redd.it/bfhk9qzqpvgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=63c2040c8dfb4a24d40bb5ca076c537bef194d77" title="OLMO 3.5 Is Around The Corner" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The OLMO series is seriously under-appreciated. Yes they may not perform the best compared to other openweight models, but OLMO models are fully open sourced, from their datasets to training recipes. So it's nice to see them experiment with more niche techniques.&lt;/p&gt; &lt;p&gt;It seems like for 3.5, they'll be using some of the techniques that Qwen3-Next introduced, so long context tasks should take less memory.&lt;/p&gt; &lt;p&gt;Though this series seems to be a set of Dense models, with the smallest being a 1B model.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;OLMo 3.5 Hybrid is a hybrid architecture model from Ai2 that combines standard transformer attention layers with linear attention layers using the Gated Deltanet. This hybrid approach aims to improve efficiency while maintaining model quality by interleaving full attention layers with linear attention layers.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/bfhk9qzqpvgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsxowq/olmo_35_is_around_the_corner/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:52:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsrscu</id>
    <title>Can 4chan data REALLY improve a model? TURNS OUT IT CAN!</title>
    <updated>2026-02-01T07:20:46+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt; &lt;img alt="Can 4chan data REALLY improve a model? TURNS OUT IT CAN!" src="https://a.thumbs.redditmedia.com/kH2rOREckIffGxPYl8Dxt7JwePnVvjX39wCJzfAooO0.jpg" title="Can 4chan data REALLY improve a model? TURNS OUT IT CAN!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hear me out, no one (really) knows how these things work.&lt;/p&gt; &lt;p&gt;A few days ago, I released &lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B"&gt;Assistant_Pepe_8B&lt;/a&gt;, you can read the discussion in &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;this thread&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;I trained it on an extended &lt;strong&gt;4chan dataset&lt;/strong&gt;, on an abliterated base, but what I didn't expect was to get this:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457"&gt;https://preview.redd.it/lrqwx8ca1ugg1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4dcfcfb9c107fa3d417e5ff623c4952e5e2ab457&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a3bby1yd1ugg1.png?width=2980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f050bbd512a12a359626af79ccebcd2d2445877"&gt;https://preview.redd.it/a3bby1yd1ugg1.png?width=2980&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8f050bbd512a12a359626af79ccebcd2d2445877&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Somehow, &lt;strong&gt;against all common sense&lt;/strong&gt;, the model &lt;strong&gt;outperformed&lt;/strong&gt; nvidia's nemotron, the base it was trained on. This is usually the other way around. You take a smart base, tune a model on it, and accept the sacrifice of some intelligence to give it flavor.&lt;/p&gt; &lt;p&gt;At first I thought &amp;quot;OK nice, a coincidence, who cares?&amp;quot;&lt;/p&gt; &lt;p&gt;But then I looked more closely at the scores:&lt;/p&gt; &lt;p&gt;1) The abliterated base &lt;strong&gt;scored higher&lt;/strong&gt; than the base.&lt;br /&gt; 2) The finetune scored even &lt;strong&gt;higher than both&lt;/strong&gt;.&lt;br /&gt; 3) The finetune was literally on an extremely noise 4chan dataset, it should have eaten glue.&lt;/p&gt; &lt;p&gt;And then I remembered something: the original, gpt4chan (by Yannic Kilcher) scored especially high in truthfulness (that was b4 benchmaxxing).&lt;/p&gt; &lt;p&gt;So I took a closer look on recent models I released; the abliterated Impish_LLAMA_4B not only outperformed the base tune (the unabliterated one), it also changed its political alignment (you can check for yourself the UGI stats, I feel like I spammed enough images). &lt;/p&gt; &lt;p&gt;People were initially joking about the &amp;quot;alignment tax&amp;quot;, I think there's a none trivial substance in all of this. It seems to me just above a marginal error or statistical noise.&lt;/p&gt; &lt;p&gt;Oh, and the KL divergence for Impish_LLAMA_4B was :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;&amp;lt;0.01 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsrscu/can_4chan_data_really_improve_a_model_turns_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T07:20:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qsx51z</id>
    <title>Falcon-H1-Tiny (90M) is out - specialized micro-models that actually work</title>
    <updated>2026-02-01T12:25:04+00:00</updated>
    <author>
      <name>/u/United-Manner-7</name>
      <uri>https://old.reddit.com/user/United-Manner-7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TII just dropped Falcon-H1-Tiny - a series of sub-100M models that quietly challenge the scaling dogma. We've all suspected that narrow, specialized smal models tend to hallucinate less than giant generalists. After all, a 90M parameter model has far less internal &amp;quot;room&amp;quot; to drift off-topic or invent facts outside its training scope. But this release &lt;em&gt;proves&lt;/em&gt; it with numbers - and flips the script on how we think about capability at tiny scales.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's actually new&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Anti-curriculum training&lt;/strong&gt;: Instead of pretraining on web junk then fine-tuning, they inject target-domain data (SFT, reasoning traces, tool calls) from token #1. For 90M models with ~5 GT memorization windows, this works - no overfitting even after 100+ epochs on high-quality data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid Mamba+Attention blocks&lt;/strong&gt; inherited from Falcon-H1, plus Learnable Multipliers + Muon optimizer (up to 20% relative gain over AdamW).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Specialized variants that punch above weight&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;90M tool-caller hits 94.44% relevance detection (knows &lt;em&gt;when&lt;/em&gt; to call a function) matches 270M Function Gemma globally despite weaker AST accuracy&lt;/li&gt; &lt;li&gt;600M reasoning model (R-0.6B) post-GRPO solves 75% of AIME24 problems pass@1 - competitive with 7B-class models when scaled at inference&lt;/li&gt; &lt;li&gt;90M coder with native FIM support runs autocomplete inside VS Code via Continue plugin&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters for local deployment&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Models this size (~90 MB quantized Q8_0) run on any modern phone or Raspberry Pi without breaking a sweat. They're not trying to replace your 7B daily driver they're purpose-built for constrained environments where footprint and latency dominate. And if you scaled these designs to ~1B parameters (11√ó), the'd likely cover 90% of everyday local use cases: chat, tool calling, light coding, reasoning traces - all while staying under 500 MB even quantized.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Base 90M instruct model: &lt;a href="https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M"&gt;https://huggingface.co/tiiuae/Falcon-H1-Tiny-R-90M&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Full model collection: &lt;a href="https://huggingface.co/tiiuae/models"&gt;https://huggingface.co/tiiuae/models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Technical blogpost with experiments: &lt;a href="https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost"&gt;https://huggingface.co/spaces/tiiuae/tiny-h1-blogpost&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/United-Manner-7"&gt; /u/United-Manner-7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qsx51z/falconh1tiny_90m_is_out_specialized_micromodels/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-01T12:25:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
