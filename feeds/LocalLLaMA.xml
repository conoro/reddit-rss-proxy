<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-31T13:31:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qrkf8a</id>
    <title>Need help brainstorming on my opensource project</title>
    <updated>2026-01-30T22:36:23+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkf8a/need_help_brainstorming_on_my_opensource_project/"&gt; &lt;img alt="Need help brainstorming on my opensource project" src="https://external-preview.redd.it/emFpN2huNzVia2dnMXAjvbzZlDodMUt4XPu-WVR4gri-PW-w3a3Tn0De93z1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0bd8730e5b27ca282279d87a0334e1e2fe9af60b" title="Need help brainstorming on my opensource project" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been working on this opensource project, Gitnexus. It creates knowledge graph of codebases, make clusters, process maps. Basically skipping the tech jargon, the idea is that to make the tools itself smarter so LLMs can offload a lot of the retrieval reasoning part to the tools. I found haiku 4.5 was able to outperform opus 4.5 using its MCP on deep architectural context. &lt;/p&gt; &lt;p&gt;It feels promising so I wanna go deeper into its development and benchmark it, converting it from a cool demo to an actual viable opensource product. I would really appreciate some advice on potential niche usecase I can tune it for, point me to some discussion forum where I can get people to brainstorm with me, maybe some micro funding sources ( some opensource programs or something ) for purchasing LLM provider credits ( Being a student i cant afford much myself üòÖ )&lt;/p&gt; &lt;p&gt;github: &lt;a href="https://github.com/abhigyanpatwari/gitnexus"&gt;https://github.com/abhigyanpatwari/gitnexus&lt;/a&gt; ( Leave a ‚≠ê if seemed cool )&lt;br /&gt; try it here: &lt;a href="https://gitnexus.vercel.com"&gt;https://gitnexus.vercel.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/5zx3h775bkgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkf8a/need_help_brainstorming_on_my_opensource_project/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkf8a/need_help_brainstorming_on_my_opensource_project/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T22:36:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs2cyh</id>
    <title>Early language models - how did they pull it off?</title>
    <updated>2026-01-31T13:28:33+00:00</updated>
    <author>
      <name>/u/OwnMathematician2620</name>
      <uri>https://old.reddit.com/user/OwnMathematician2620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you remember Tay, the Microsoft chatbot from 2016? Or (earliest generation of) Xiaoice from 2014? Despite the fact that AI technology has been around for many years, I find it increasingly difficult to imagine how they managed to do it back then.&lt;/p&gt; &lt;p&gt;The paper 'Attention is All You Need' was published in 2017, and the GPT-2 paper ('Language Models are Unsupervised Multitask Learners') in 2019. Yes, I know we had RNNs before that could do a similar thing, but how on earth did they handle the training dataset? Not to mention their ability to learn from many conversations during inference, which is also what got Tay taken down after only a day.&lt;/p&gt; &lt;p&gt;I don't think they even used the design principle as modern LLMs. It's a shame that I can't find any official information about Tay's architecture, as well as how it's trained...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OwnMathematician2620"&gt; /u/OwnMathematician2620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs2cyh/early_language_models_how_did_they_pull_it_off/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:28:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrbfez</id>
    <title>spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-30T17:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"&gt; &lt;img alt="spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/g3Kl7EuA7uN68kx8-95HOYqEV6uFaejZ8ghgYxWQDJQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d663e7e68ebfca599cda3a0ba19c677f3a8b64c8" title="spec : add ngram-mod by ggerganov ¬∑ Pull Request #19164 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;watch the video&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19164"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrbfez/spec_add_ngrammod_by_ggerganov_pull_request_19164/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T17:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs0gtj</id>
    <title>Are there any open source or free NPU supported LLM chat apps for Snapdragon 8 Gen 5</title>
    <updated>2026-01-31T11:55:42+00:00</updated>
    <author>
      <name>/u/LdWilmore</name>
      <uri>https://old.reddit.com/user/LdWilmore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried:&lt;/p&gt; &lt;p&gt;PocketPal - Doesn't detect NPU and GPU in device selection&lt;/p&gt; &lt;p&gt;ChatterUI - Same no NPU&lt;/p&gt; &lt;p&gt;Layla Lite - QNN is behind pay wall&lt;/p&gt; &lt;p&gt;Paage.ai - supposedly has Executorch support but can't find any PTE models for Snapdragon 8 Gen 5&lt;/p&gt; &lt;p&gt;MNN Chat&lt;/p&gt; &lt;p&gt;Google AI Edge Gallery&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LdWilmore"&gt; /u/LdWilmore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs0gtj/are_there_any_open_source_or_free_npu_supported/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs0gtj/are_there_any_open_source_or_free_npu_supported/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs0gtj/are_there_any_open_source_or_free_npu_supported/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T11:55:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs19cz</id>
    <title>Career Direction Advice in the Field of Artificial Intelligence</title>
    <updated>2026-01-31T12:36:24+00:00</updated>
    <author>
      <name>/u/ztarek10</name>
      <uri>https://old.reddit.com/user/ztarek10</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am a Mechatronics graduate, and I have been interested in the field of Artificial Intelligence. However, I did not study it in a formal or academic way. Instead, I started working directly in the field: I typically used pre-trained models and integrated them into projects, and when fine-tuning was required, I would obtain a dataset and perform the fine-tuning accordingly. The main issue is that I feel more like a technician than an engineer. I am not comfortable with the feeling that I do not fully understand the field, its concepts, or its terminology. Therefore, I would like to ask for advice on how to proceed.&lt;/p&gt; &lt;p&gt;For context, I am currently working on a Computer Vision project inside the company, and whenever the company has an AI-related project, the company manager contacts me directly. This has left me uncertain about the next step: should I start learning the field from the fundamentals, continue working on the current project, consider leaving my job, or take a different approach altogether?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ztarek10"&gt; /u/ztarek10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs19cz/career_direction_advice_in_the_field_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs19cz/career_direction_advice_in_the_field_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs19cz/career_direction_advice_in_the_field_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T12:36:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrqk9o</id>
    <title>Managed to run Kimi k2.5 IQ4-SX locally.</title>
    <updated>2026-01-31T02:56:33+00:00</updated>
    <author>
      <name>/u/el3mancee</name>
      <uri>https://old.reddit.com/user/el3mancee</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"&gt; &lt;img alt="Managed to run Kimi k2.5 IQ4-SX locally." src="https://a.thumbs.redditmedia.com/wPblHyZa-nLIek-Bsb8Vto-LtlSalMG4iuSpvu7oeE0.jpg" title="Managed to run Kimi k2.5 IQ4-SX locally." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Loaded with a max token capable(262,114 tokens)&lt;/p&gt; &lt;p&gt;1 Max Studio M1 Ultra(host), 1 Asus Gx10, 3 Strix Halo. Connected with Thunderbolt and 10 Gbps Ethernet. &lt;/p&gt; &lt;p&gt;Tg 8.5 tps. Pp 15-20 tps.&lt;/p&gt; &lt;p&gt;Can reach ~15 tps tg when using concurrent requests.&lt;/p&gt; &lt;p&gt;Pretty slow for production, I think.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/el3mancee"&gt; /u/el3mancee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qrqk9o"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrqk9o/managed_to_run_kimi_k25_iq4sx_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T02:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrvx16</id>
    <title>What‚Äôs the best way to run an offline, private LLM for daily tasks?</title>
    <updated>2026-01-31T07:27:20+00:00</updated>
    <author>
      <name>/u/FollowingMindless144</name>
      <uri>https://old.reddit.com/user/FollowingMindless144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want an LLM that runs &lt;strong&gt;fully offline&lt;/strong&gt;, is &lt;strong&gt;secure/private&lt;/strong&gt;, and can handle basic stuff like reminders, notes, simple automation, maybe voice later.&lt;/p&gt; &lt;p&gt;Not looking for cloud APIs or ‚Äújust use ChatGPT‚Äù answers curious what people here are actually using &lt;em&gt;in practice&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Are local setups (Ollama / LM Studio / llama.cpp etc.) good enough now, or is this still more hobby than daily driver?&lt;/p&gt; &lt;p&gt;Would love to hear real setups, tradeoffs, and ‚Äúdon‚Äôt do this‚Äù lessons.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FollowingMindless144"&gt; /u/FollowingMindless144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvx16/whats_the_best_way_to_run_an_offline_private_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvx16/whats_the_best_way_to_run_an_offline_private_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvx16/whats_the_best_way_to_run_an_offline_private_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T07:27:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qryr2e</id>
    <title>What good are 128k+ context windows for &lt;40b Parameter models?</title>
    <updated>2026-01-31T10:17:49+00:00</updated>
    <author>
      <name>/u/Your_Friendly_Nerd</name>
      <uri>https://old.reddit.com/user/Your_Friendly_Nerd</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is only anecdotal evidence, nothing based off of solid research, but I find that, after ~10k tokens, responses for most models I've tried (which are all under 40b parameters) the quality noticeably degrades, and after 30k tokens the models become borderline unusable. So what use-cases are there (if any) for such large maximum context windows?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Your_Friendly_Nerd"&gt; /u/Your_Friendly_Nerd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qryr2e/what_good_are_128k_context_windows_for_40b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qryr2e/what_good_are_128k_context_windows_for_40b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qryr2e/what_good_are_128k_context_windows_for_40b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T10:17:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs1y5f</id>
    <title>Are commercial models like Claude, Gemini, and ChatGPT counting their whole internal tool calling pipeline part of their ‚Äúmodel‚Äù? (for benchmarks)</title>
    <updated>2026-01-31T13:09:57+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When it comes to benchmark testing and comparing against open source local models, are the big companies wrapping a bunch of tools together with their base model and calling the sum of all the parts the ‚Äúmodel‚Äù? Or are they just testing and benchmarking the base LLM without any connected tools?&lt;/p&gt; &lt;p&gt;It seems like it would be unfair to compare local models to SOTA commercial models if they are not comparing apples to apples. &lt;/p&gt; &lt;p&gt;Could we even tell if they were doing this or not? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs1y5f/are_commercial_models_like_claude_gemini_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:09:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7cbh</id>
    <title>Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!</title>
    <updated>2026-01-30T14:44:01+00:00</updated>
    <author>
      <name>/u/fictionlive</name>
      <uri>https://old.reddit.com/user/fictionlive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"&gt; &lt;img alt="Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!" src="https://preview.redd.it/on28koqz0igg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=011c51e6852eeaee308ab92b0cd9e4852da4bfe0" title="Kimi-k2.5 reaches gemini 2.5 Pro-like performance in long context!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fictionlive"&gt; /u/fictionlive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/on28koqz0igg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7cbh/kimik25_reaches_gemini_25_prolike_performance_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:44:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr7ncz</id>
    <title>Design Arena is now dominated by an open model</title>
    <updated>2026-01-30T14:55:35+00:00</updated>
    <author>
      <name>/u/moks4tda</name>
      <uri>https://old.reddit.com/user/moks4tda</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt; &lt;img alt="Design Arena is now dominated by an open model" src="https://a.thumbs.redditmedia.com/IOzZQkj-NN9LpvwuZen1AYFWFys9dnrIFBwpaZCd7D0.jpg" title="Design Arena is now dominated by an open model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The first month of 2026 is already this wild, I can't even imagine what's coming next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/moks4tda"&gt; /u/moks4tda &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qr7ncz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr7ncz/design_arena_is_now_dominated_by_an_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T14:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrfbo8</id>
    <title>NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development</title>
    <updated>2026-01-30T19:26:46+00:00</updated>
    <author>
      <name>/u/Delicious_Air_737</name>
      <uri>https://old.reddit.com/user/Delicious_Air_737</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt; &lt;img alt="NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development" src="https://b.thumbs.redditmedia.com/vc1UXvPAXbGKJMGRgO3vaYujfIckG9Mk-wfTZzXzgEY.jpg" title="NVIDIA Releases Massive Collection of Open Models, Data and Tools to Accelerate AI Development" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6key4zy0fjgg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b0bfa274d54a0e695e0cbc067cd40c4c9dfa4e"&gt;https://preview.redd.it/6key4zy0fjgg1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=62b0bfa274d54a0e695e0cbc067cd40c4c9dfa4e&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At CES 2026, NVIDIA announced what might be &lt;a href="https://namiru.ai/blog/nvidia-releases-massive-collection-of-open-models-data-and-tools-to-accelerate-ai-development?source=red-nvidia-kinga"&gt;the most significant open-source AI release&lt;/a&gt; to date. The company unveiled new models, datasets, and tools spanning everything from speech recognition to drug discovery.&lt;/p&gt; &lt;p&gt;For regular users, this release means better voice assistants, smarter document search, faster drug development, safer self-driving cars, and more capable robots. These technologies will filter into consumer products throughout 2026.&lt;/p&gt; &lt;p&gt;NVIDIA is betting that by enabling the entire AI ecosystem, they sell more GPUs. Based on the companies already adopting these technologies, that bet is paying off. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Air_737"&gt; /u/Delicious_Air_737 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrfbo8/nvidia_releases_massive_collection_of_open_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T19:26:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrvwy6</id>
    <title>NVIDIA releases new graphics driver for old Pascal and Maxwell graphics cards - Neowin</title>
    <updated>2026-01-31T07:27:12+00:00</updated>
    <author>
      <name>/u/maifee</name>
      <uri>https://old.reddit.com/user/maifee</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/maifee"&gt; /u/maifee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.neowin.net/news/nvidia-releases-new-graphics-driver-for-old-pascal-and-maxwell-graphics-cards/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvwy6/nvidia_releases_new_graphics_driver_for_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrvwy6/nvidia_releases_new_graphics_driver_for_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T07:27:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qs27hf</id>
    <title>g-HOOT in the Machine</title>
    <updated>2026-01-31T13:21:43+00:00</updated>
    <author>
      <name>/u/TheVeryNearFuture</name>
      <uri>https://old.reddit.com/user/TheVeryNearFuture</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt; &lt;img alt="g-HOOT in the Machine" src="https://preview.redd.it/z78lvao9rogg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae89cf23b154560e4ea34ce2fff5ea8a457a781b" title="g-HOOT in the Machine" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2507.14805"&gt;https://arxiv.org/abs/2507.14805&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheVeryNearFuture"&gt; /u/TheVeryNearFuture &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z78lvao9rogg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qs27hf/ghoot_in_the_machine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T13:21:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrazyy</id>
    <title>Cline team got absorbed by OpenAI. Kilo is going full source available in response.</title>
    <updated>2026-01-30T16:56:49+00:00</updated>
    <author>
      <name>/u/demon_bhaiya</name>
      <uri>https://old.reddit.com/user/demon_bhaiya</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt; &lt;img alt="Cline team got absorbed by OpenAI. Kilo is going full source available in response." src="https://external-preview.redd.it/OJiv7stnybHLdn8-mzf6t_NZ9C8xS7VIYLhMSJsX0d8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=969735c0073c014c64189d4c6b79a9e599fe2c52" title="Cline team got absorbed by OpenAI. Kilo is going full source available in response." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who used Cline with local models, heads up that the core team appears to have joined OpenAI's Codex group based on their LinkedIn profiles. No official announcement yet, but we have seen how these acqui-hires usually play out.&lt;/p&gt; &lt;p&gt;Kilo Code (which forked from Cline and Roo Code) just responded by announcing they are making their backend source available by Feb 6. The VS Code extension, JetBrains plugin, and CLI stay Apache 2.0(Open source). Their gateway supports 500+ models including Qwen, DeepSeek, and Mistral.&lt;/p&gt; &lt;p&gt;They're offering $100 credits to anyone who contributed to Cline, and $150 per merged PR in February. If you want to keep building on an open codebase instead of watching another project disappear into a walled garden, might be worth checking out.&lt;/p&gt; &lt;p&gt;The agentic coding space needs alternatives that work with local and open weight models. Would suck to see all the decent tools end up controlled by the big labs.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/demon_bhaiya"&gt; /u/demon_bhaiya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.kilo.ai/p/cline-just-acqui-hired"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrazyy/cline_team_got_absorbed_by_openai_kilo_is_going/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T16:56:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrmu2v</id>
    <title>What shoddy development looks like</title>
    <updated>2026-01-31T00:12:53+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"&gt; &lt;img alt="What shoddy development looks like" src="https://preview.redd.it/9l7wwnsu6kgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a3c02e836b0951ddacdc25d64410b0f33a6b6e4" title="What shoddy development looks like" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9l7wwnsu6kgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrmu2v/what_shoddy_development_looks_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T00:12:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrj1y4</id>
    <title>Stop it with the Agents/Projects Slop and spam</title>
    <updated>2026-01-30T21:44:24+00:00</updated>
    <author>
      <name>/u/Daemontatox</name>
      <uri>https://old.reddit.com/user/Daemontatox</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The sub is now averaging 3-4 unfinished sloppy Agentic project that's titled the &amp;quot;best next discovery&amp;quot; or &amp;quot;alternative to [insert famous tool here]&amp;quot; or this tool is so amazing i can't even.&lt;/p&gt; &lt;p&gt;It's getting really hard to filter through them and read through the meaningful posts or actual local content.&lt;/p&gt; &lt;p&gt;We need to either add a new tag for slop or ban it altogether because the sub is slowly turning into &amp;quot;omg this tool is clawdbot 2.0&amp;quot; or some guy trying to sell his half finished project that clauded wrote for him on a weekend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Daemontatox"&gt; /u/Daemontatox &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrj1y4/stop_it_with_the_agentsprojects_slop_and_spam/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T21:44:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qr4p4x</id>
    <title>Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself.</title>
    <updated>2026-01-30T12:55:38+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt; &lt;img alt="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." src="https://external-preview.redd.it/MnNnNHZ6eGNoaGdnMcC0w-E97YmQ2Bn80LEN79By6gOnSLJ7DXbqces3JuUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=189015efd04b82b6e73fa3d8be460d38d65659e4" title="Yann LeCun says the best open models are not coming from the West. Researchers across the field are using Chinese models. Openness drove AI progress. Close access, and the West risks slowing itself." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Forbes on YouTube: Yann LeCun Gives Unfiltered Take On The Future Of AI In Davos: &lt;a href="https://www.youtube.com/watch?v=MWMe7yjPYpE"&gt;https://www.youtube.com/watch?v=MWMe7yjPYpE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Video by vitrupo on ùïè: &lt;a href="https://x.com/vitrupo/status/2017218170273313033"&gt;https://x.com/vitrupo/status/2017218170273313033&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/n31pvrxchhgg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qr4p4x/yann_lecun_says_the_best_open_models_are_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T12:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrkb1b</id>
    <title>How was GPT-OSS so good?</title>
    <updated>2026-01-30T22:31:44+00:00</updated>
    <author>
      <name>/u/xt8sketchy</name>
      <uri>https://old.reddit.com/user/xt8sketchy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been messing around with a lot of local LLMs (120b and under) recently, and while some of them excel at specific things, none of them feel quite as good as GPT-OSS 120b all-around.&lt;/p&gt; &lt;p&gt;The model is 64GB at full precision, is BLAZING fast, and is pretty good at everything. It's consistent, it calls tools properly, etc.&lt;/p&gt; &lt;p&gt;But it's sort of old... it's been so long since GPT-OSS came out and we haven't really had a decent all-around open-weights/source replacement for it (some may argue GLM4.5 Air, but I personally feel like that model is only really better in agentic software dev, and lags behind in everything else. It's also slower and larger at full precision.)&lt;/p&gt; &lt;p&gt;I'm no expert when it comes to how LLM training/etc works, so forgive me if some of my questions are dumb, but:&lt;br /&gt; - Why don't people train more models in 4-bit natively, like GPT-OSS? Doesn't it reduce training costs? Is there some downside I'm not thinking of?&lt;br /&gt; - I know GPT-OSS was fast in part due to it being A3B, but there are plenty of smaller, dumber, NEWER A3B models that are much slower. What else makes it so fast? Why aren't we using what we learned from GPT-OSS in newer models?&lt;br /&gt; - What about a model (like GPT-OSS) makes it feel so much better? Is it the dataset? Did OpenAI just have a dataset that was THAT GOOD that their model is still relevant HALF A YEAR after release?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xt8sketchy"&gt; /u/xt8sketchy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrkb1b/how_was_gptoss_so_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-30T22:31:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qryqvo</id>
    <title>This overhyped nonsense is getting tiring (moltbook)</title>
    <updated>2026-01-31T10:17:29+00:00</updated>
    <author>
      <name>/u/NolenBrolen</name>
      <uri>https://old.reddit.com/user/NolenBrolen</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This morning I check my YouTube feed again to get flooded by multiple videos all talking about this &lt;strong&gt;&amp;quot;&lt;/strong&gt;incredible&lt;strong&gt;&amp;quot;&lt;/strong&gt; moltbook thing. &lt;/p&gt; &lt;p&gt;I thought it was nonsense to begin with but then I decided 'Hey let's give it a look' so I went to go checkout and moltbook myself and the website literally doesn't work. &lt;/p&gt; &lt;p&gt;I tried navigated to the 'Browse Submolts' page and clicked over a dozen threads and literally none of them will load or open. I find it so exhaustic to have these constant nonsense hype cycles. What happened to real AI technology and development that these things get so much hype for nothing and don't even work properly. I just don't get it. &lt;/p&gt; &lt;p&gt;Thought I just wanted to share to see if anyone else feels the same way because I can't be the only one.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NolenBrolen"&gt; /u/NolenBrolen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qryqvo/this_overhyped_nonsense_is_getting_tiring_moltbook/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qryqvo/this_overhyped_nonsense_is_getting_tiring_moltbook/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qryqvo/this_overhyped_nonsense_is_getting_tiring_moltbook/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T10:17:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrwo9v</id>
    <title>Here it goes</title>
    <updated>2026-01-31T08:12:32+00:00</updated>
    <author>
      <name>/u/gotkush</name>
      <uri>https://old.reddit.com/user/gotkush</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"&gt; &lt;img alt="Here it goes" src="https://preview.redd.it/pchjv5z88ngg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=46ff31a155e1a7011f67f91d666503ef5cbcdf51" title="Here it goes" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My friend sold me his mining unit that he never got to use. He had it at his mom‚Äôs house and his mom moved out of town so he let me keep it. Was gonna part it out but I think it‚Äôs my new project. It has 8 RTx 3090 which has 24gbvram I would just need to upgrade the mobo cpu ram and the est j found was around 2500 for mobo 5900ryzen 256gb ram. It has 4 1000w power, would just need to get 8 pci risers so i can have each gou run at pcie4.0 x16. What donyoi guys think ? U think its over kill, im bery interested in havin my own ai sandbkx. Wouldnlike to get eveyones r thoughts&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gotkush"&gt; /u/gotkush &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pchjv5z88ngg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrwo9v/here_it_goes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T08:12:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrzyaz</id>
    <title>I found that MXFP4 has lower perplexity than Q4_K_M and Q4_K_XL.</title>
    <updated>2026-01-31T11:27:30+00:00</updated>
    <author>
      <name>/u/East-Engineering-653</name>
      <uri>https://old.reddit.com/user/East-Engineering-653</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This post was originally written in Korean and then translated into English using ChatGPT.&lt;br /&gt; Hello, I am currently serving LLM models using a Tesla P40 and llama.cpp. When running models in the 30‚Äì32B range, I usually rely on 4-bit quantization. Until now, I primarily used Q4_K_XL, and if Q4_K_XL was not available, I used Q4_K_M instead. I initially avoided MXFP4 quantization because, compared to other 4-bit quantization methods, it has a smaller size, so I naturally assumed its accuracy would be lower. However, out of curiosity sparked by MXFP4‚Äôs fast speed, I compared Q4_K_M, Q4_K_XL, and MXFP4 quantization methods for the GLM-4.7-Flash and Nemotron-3-nano models using the &lt;code&gt;llama-perplexity&lt;/code&gt; command.&lt;/p&gt; &lt;p&gt;Below are the commands used, along with the Python code and command used to generate the dataset. The dataset generation command was created using ChatGPT.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import argparse import os import re import sys import urllib.request from pathlib import Path import random def download(url: str, dst: Path) -&amp;gt; None: dst.parent.mkdir(parents=True, exist_ok=True) with urllib.request.urlopen(url) as r, open(dst, &amp;quot;wb&amp;quot;) as f: f.write(r.read()) def normalize_text(text: str, mode: str) -&amp;gt; str: text = text.replace(&amp;quot;\r\n&amp;quot;, &amp;quot;\n&amp;quot;).replace(&amp;quot;\r&amp;quot;, &amp;quot;\n&amp;quot;) if mode == &amp;quot;ppl&amp;quot;: text = re.sub(r&amp;quot;\n\s*\n+&amp;quot;, &amp;quot;\n&amp;quot;, text) text = re.sub(r&amp;quot;[ \t]+&amp;quot;, &amp;quot; &amp;quot;, text) text = text.strip() + &amp;quot;\n&amp;quot; return text if mode == &amp;quot;line&amp;quot;: lines = [] for line in text.split(&amp;quot;\n&amp;quot;): line = line.strip() if not line: continue line = re.sub(r&amp;quot;[ \t]+&amp;quot;, &amp;quot; &amp;quot;, line) lines.append(line) return &amp;quot;\n&amp;quot;.join(lines) + &amp;quot;\n&amp;quot; raise ValueError(f&amp;quot;unknown mode: {mode}&amp;quot;) def take_prefix(text: str, max_chars: int | None) -&amp;gt; str: if max_chars is None: return text if max_chars &amp;lt;= 0: return &amp;quot;&amp;quot; return text[:max_chars] def sample_lines(text: str, n_lines: int, seed: int) -&amp;gt; str: random.seed(seed) lines = [ln for ln in text.split(&amp;quot;\n&amp;quot;) if ln.strip()] if n_lines &amp;lt;= 0 or n_lines &amp;gt;= len(lines): return &amp;quot;\n&amp;quot;.join(lines) + &amp;quot;\n&amp;quot; sampled = random.sample(lines, n_lines) return &amp;quot;\n&amp;quot;.join(sampled) + &amp;quot;\n&amp;quot; def main(): ap = argparse.ArgumentParser() g = ap.add_mutually_exclusive_group(required=True) g.add_argument(&amp;quot;--url&amp;quot;, help=&amp;quot;download source url&amp;quot;) g.add_argument(&amp;quot;--infile&amp;quot;, help=&amp;quot;local input file path&amp;quot;) ap.add_argument(&amp;quot;--out&amp;quot;, required=True, help=&amp;quot;output text file path&amp;quot;) ap.add_argument(&amp;quot;--mode&amp;quot;, choices=[&amp;quot;ppl&amp;quot;, &amp;quot;line&amp;quot;], default=&amp;quot;ppl&amp;quot;, help=&amp;quot;ppl: keep newlines but collapse blanks/spaces, line: one sentence per line style&amp;quot;) ap.add_argument(&amp;quot;--max-chars&amp;quot;, type=int, default=None, help=&amp;quot;optional: cut the output to first N characters (fast/low-memory eval)&amp;quot;) ap.add_argument(&amp;quot;--sample-lines&amp;quot;, type=int, default=None, help=&amp;quot;optional: sample N non-empty lines uniformly (good for quick comparison)&amp;quot;) ap.add_argument(&amp;quot;--seed&amp;quot;, type=int, default=42) args = ap.parse_args() out_path = Path(args.out) if args.url: tmp = out_path.with_suffix(out_path.suffix + &amp;quot;.download&amp;quot;) download(args.url, tmp) in_path = tmp else: in_path = Path(args.infile) try: raw = in_path.read_text(encoding=&amp;quot;utf-8&amp;quot;, errors=&amp;quot;replace&amp;quot;) except Exception as e: print(f&amp;quot;failed to read input: {e}&amp;quot;, file=sys.stderr) sys.exit(1) text = normalize_text(raw, args.mode) if args.sample_lines is not None: text = sample_lines(text, args.sample_lines, args.seed) text = take_prefix(text, args.max_chars) out_path.parent.mkdir(parents=True, exist_ok=True) out_path.write_text(text, encoding=&amp;quot;utf-8&amp;quot;) if args.url: try: os.remove(in_path) except OSError: pass print(f&amp;quot;wrote: {out_path} ({out_path.stat().st_size} bytes)&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Command&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;python3 wikitext_prep.py \ --url https://cosmo.zip/pub/datasets/wikitext-2-raw/wiki.test.raw \ --out /data/wikitext2_test.txt \ --mode ppl \ --max-chars 2000000 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Using the command below, I measured the perplexity of the quantized models.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity -m modelname.gguf -f wikitext2_test.txt -c 32768 -b 4096 -fa on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The table below summarizes the test results, which were also organized using ChatGPT. The actual &lt;code&gt;llama-perplexity&lt;/code&gt; output is quite long, so it is attached separately below. For reference, Q4_K_M and Q4_K_XL were measured simultaneously, and after a llama.cpp update, Q4_K_XL and MXFP4 were measured simultaneously. Because the testing time was very long and the perplexity of Q4_K_XL was similar before and after the update, I assumed that the perplexity of Q4_K_M would also not be significantly affected by build changes.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Q4_K_M (Unsloth)&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL (previous)&lt;/th&gt; &lt;th align="left"&gt;MXFP4_MOE&lt;/th&gt; &lt;th align="left"&gt;UD-Q4_K_XL (current)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;llama.cpp build&lt;/td&gt; &lt;td align="left"&gt;7803&lt;/td&gt; &lt;td align="left"&gt;7803&lt;/td&gt; &lt;td align="left"&gt;7896&lt;/td&gt; &lt;td align="left"&gt;7896&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GGUF file type&lt;/td&gt; &lt;td align="left"&gt;Q4_K ‚Äì Medium&lt;/td&gt; &lt;td align="left"&gt;Q4_K ‚Äì Medium&lt;/td&gt; &lt;td align="left"&gt;MXFP4 MoE&lt;/td&gt; &lt;td align="left"&gt;Q4_K ‚Äì Medium&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;File size&lt;/td&gt; &lt;td align="left"&gt;17.05 GiB&lt;/td&gt; &lt;td align="left"&gt;16.31 GiB&lt;/td&gt; &lt;td align="left"&gt;15.79 GiB&lt;/td&gt; &lt;td align="left"&gt;16.31 GiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;BPW&lt;/td&gt; &lt;td align="left"&gt;4.89&lt;/td&gt; &lt;td align="left"&gt;4.68&lt;/td&gt; &lt;td align="left"&gt;4.53&lt;/td&gt; &lt;td align="left"&gt;4.68&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL (final)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;16.1745 ¬± 0.1870&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.8605 ¬± 0.1823&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;10.7235 ¬± 0.1052&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15.7309 ¬± 0.1803&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt eval speed&lt;/td&gt; &lt;td align="left"&gt;64.39 tok/s&lt;/td&gt; &lt;td align="left"&gt;64.37 tok/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;68.20 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.73 tok/s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ms/token&lt;/td&gt; &lt;td align="left"&gt;15.53 ms&lt;/td&gt; &lt;td align="left"&gt;15.54 ms&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.66 ms&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;14.76 ms&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Time per pass (ETA)&lt;/td&gt; &lt;td align="left"&gt;529.38 s&lt;/td&gt; &lt;td align="left"&gt;530.05 s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;501.55 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;502.66 s&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU self (total)&lt;/td&gt; &lt;td align="left"&gt;20811 MiB&lt;/td&gt; &lt;td align="left"&gt;20056 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;17874 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;18552 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU model buffer&lt;/td&gt; &lt;td align="left"&gt;17284.84 MiB&lt;/td&gt; &lt;td align="left"&gt;16529.37 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;15852.01 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;16529.37 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;KV cache size&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3196 MiB&lt;/strong&gt; (K 1692 + V 1504)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;3196 MiB&lt;/strong&gt; (K 1692 + V 1504)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1692 MiB&lt;/strong&gt; (K 1692 + V 0)&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1692 MiB&lt;/strong&gt; (K 1692 + V 0)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU free (log-based)&lt;/td&gt; &lt;td align="left"&gt;3406 MiB&lt;/td&gt; &lt;td align="left"&gt;4162 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;6342 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;5666 MiB&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Load time&lt;/td&gt; &lt;td align="left"&gt;9.90 s&lt;/td&gt; &lt;td align="left"&gt;9.55 s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.13 s&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;43.72 s&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;mmap / direct_io&lt;/td&gt; &lt;td align="left"&gt;mmap off / direct_io on&lt;/td&gt; &lt;td align="left"&gt;mmap off / direct_io on&lt;/td&gt; &lt;td align="left"&gt;mmap on / direct_io off&lt;/td&gt; &lt;td align="left"&gt;mmap on / direct_io off&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;[1]&lt;/th&gt; &lt;th align="left"&gt;[2]&lt;/th&gt; &lt;th align="left"&gt;[3]&lt;/th&gt; &lt;th align="left"&gt;[4]&lt;/th&gt; &lt;th align="left"&gt;[5]&lt;/th&gt; &lt;th align="left"&gt;[6]&lt;/th&gt; &lt;th align="left"&gt;Final PPL&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;15.2952&lt;/td&gt; &lt;td align="left"&gt;15.1950&lt;/td&gt; &lt;td align="left"&gt;15.7101&lt;/td&gt; &lt;td align="left"&gt;14.8037&lt;/td&gt; &lt;td align="left"&gt;14.5891&lt;/td&gt; &lt;td align="left"&gt;16.1745&lt;/td&gt; &lt;td align="left"&gt;16.1745 ¬± 0.1870&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL (previous)&lt;/td&gt; &lt;td align="left"&gt;14.7572&lt;/td&gt; &lt;td align="left"&gt;14.4954&lt;/td&gt; &lt;td align="left"&gt;15.0386&lt;/td&gt; &lt;td align="left"&gt;14.1713&lt;/td&gt; &lt;td align="left"&gt;14.1425&lt;/td&gt; &lt;td align="left"&gt;15.8605&lt;/td&gt; &lt;td align="left"&gt;15.8605 ¬± 0.1823&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;10.1764&lt;/td&gt; &lt;td align="left"&gt;10.1296&lt;/td&gt; &lt;td align="left"&gt;10.4917&lt;/td&gt; &lt;td align="left"&gt;9.8666&lt;/td&gt; &lt;td align="left"&gt;9.8629&lt;/td&gt; &lt;td align="left"&gt;10.7235&lt;/td&gt; &lt;td align="left"&gt;10.7235 ¬± 0.1052&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;UD-Q4_K_XL (current)&lt;/td&gt; &lt;td align="left"&gt;14.4241&lt;/td&gt; &lt;td align="left"&gt;14.2673&lt;/td&gt; &lt;td align="left"&gt;14.8671&lt;/td&gt; &lt;td align="left"&gt;14.0460&lt;/td&gt; &lt;td align="left"&gt;14.0444&lt;/td&gt; &lt;td align="left"&gt;15.7309&lt;/td&gt; &lt;td align="left"&gt;15.7309 ¬± 0.1803&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Below is a table comparing MXFP4 and Q4_K_XL quantization methods on the Nemotron-3-nano model. This table was also created using ChatGPT.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Q4_K_XL (previous)&lt;/th&gt; &lt;th align="left"&gt;MXFP4 (current)&lt;/th&gt; &lt;th align="left"&gt;Change (MXFP4 ‚àí Q4_K_XL)&lt;/th&gt; &lt;th align="left"&gt;Meaning&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Final PPL&lt;/td&gt; &lt;td align="left"&gt;7.7090&lt;/td&gt; &lt;td align="left"&gt;7.5294&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-0.1796&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;MXFP4 is lower ‚Üí based on this corpus, ‚Äúless accuracy loss (or more accurate)‚Äù&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;PPL error (¬±)&lt;/td&gt; &lt;td align="left"&gt;0.05361&lt;/td&gt; &lt;td align="left"&gt;0.05198&lt;/td&gt; &lt;td align="left"&gt;-0.00163&lt;/td&gt; &lt;td align="left"&gt;Uncertainty is nearly identical&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Prompt eval speed&lt;/td&gt; &lt;td align="left"&gt;763.26 tok/s&lt;/td&gt; &lt;td align="left"&gt;797.79 tok/s&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+34.53 tok/s (+4.5%)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4 is slightly faster&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Time per pass&lt;/td&gt; &lt;td align="left"&gt;24.74 s/pass&lt;/td&gt; &lt;td align="left"&gt;23.45 s/pass&lt;/td&gt; &lt;td align="left"&gt;-1.29 s/pass&lt;/td&gt; &lt;td align="left"&gt;MXFP4 is slightly shorter&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU model memory&lt;/td&gt; &lt;td align="left"&gt;21537 MiB&lt;/td&gt; &lt;td align="left"&gt;16782 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;-4755 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;MXFP4 uses &lt;strong&gt;significantly less model memory&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU free VRAM&lt;/td&gt; &lt;td align="left"&gt;2286 MiB&lt;/td&gt; &lt;td align="left"&gt;7040 MiB&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;+4754 MiB&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Available VRAM increases greatly&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU context memory&lt;/td&gt; &lt;td align="left"&gt;143 MiB&lt;/td&gt; &lt;td align="left"&gt;143 MiB&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;Same due to identical &lt;code&gt;n_ctx&lt;/code&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPU compute buffer&lt;/td&gt; &lt;td align="left"&gt;271 MiB&lt;/td&gt; &lt;td align="left"&gt;271 MiB&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;Same&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Host usage (total)&lt;/td&gt; &lt;td align="left"&gt;268 MiB&lt;/td&gt; &lt;td align="left"&gt;394 MiB&lt;/td&gt; &lt;td align="left"&gt;+126 MiB&lt;/td&gt; &lt;td align="left"&gt;Difference is small and of limited significance&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I rewrote this post to add the Nemotron-3-nano benchmark, and in the previous post, one user commented that perplexity and tool calling or coding are completely different domains. They mentioned that using the HumanEval benchmark would provide values more directly related to tool calling and coding performance. If I get the chance, I plan to test again using the HumanEval benchmark in the future.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qrwnd4/comment/o2rape9/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1qrwnd4/comment/o2rape9/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To be honest, after seeing these benchmark results, I hoped that perplexity would be directly related to coding and tool calling performance, so it is a bit disappointing.&lt;br /&gt; If anyone has other opinions, I would appreciate it if you could share them.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/East-Engineering-653"&gt; /u/East-Engineering-653 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrzyaz/i_found_that_mxfp4_has_lower_perplexity_than_q4_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T11:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qrsy4q</id>
    <title>How close are open-weight models to "SOTA"? My honest take as of today, benchmarks be damned.</title>
    <updated>2026-01-31T04:49:42+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"&gt; &lt;img alt="How close are open-weight models to &amp;quot;SOTA&amp;quot;? My honest take as of today, benchmarks be damned." src="https://preview.redd.it/k38sg20q7mgg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f1d06005e56d7a9be9a1c820f7096aa2805c52dc" title="How close are open-weight models to &amp;quot;SOTA&amp;quot;? My honest take as of today, benchmarks be damned." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k38sg20q7mgg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qrsy4q/how_close_are_openweight_models_to_sota_my_honest/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-31T04:49:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
