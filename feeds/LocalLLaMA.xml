<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-20T15:06:56+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oav4hi</id>
    <title>Two new Google models, "lithiumflow" and "orionmist", have been added to LMArena. This is Google's naming scheme and "orion" has been used internally with Gemini 3 codenames, so these are likely Gemini 3 models</title>
    <updated>2025-10-19T17:11:45+00:00</updated>
    <author>
      <name>/u/balianone</name>
      <uri>https://old.reddit.com/user/balianone</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist"&gt;https://www.reddit.com/r/Bard/comments/1oauzgr/two_new_google_models_lithiumflow_and_orionmist&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/balianone"&gt; /u/balianone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oav4hi/two_new_google_models_lithiumflow_and_orionmist/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:11:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1obkn0d</id>
    <title>[Experiment] Three identical Qwen2.5:7b runs, three distinct behavioral strategies. One hit max reported metrics and started failing to execute actions. [Full data + code]</title>
    <updated>2025-10-20T14:38:50+00:00</updated>
    <author>
      <name>/u/Dark_Passenger_107</name>
      <uri>https://old.reddit.com/user/Dark_Passenger_107</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Upfront disclaimer:&lt;/strong&gt; Not claiming consciousness, feelings, or sentience. Just showing weird behavioral divergence I didn't expect and asking if anyone can explain it.&lt;/p&gt; &lt;h2&gt;What I Built&lt;/h2&gt; &lt;p&gt;Autonomous agent that monitors &amp;quot;internal strain&amp;quot; (arbitrary metric: queue_depth/50 + load/2) and decides how to survive under increasing computational load over 20 minutes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key detail:&lt;/strong&gt; The LLM sees these metrics fed back to it in natural language every decision cycle.&lt;/p&gt; &lt;p&gt;Think of it like a stress test, but the AI is monitoring itself and deciding how to respond.&lt;/p&gt; &lt;h2&gt;The Setup&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen 2.5:7b (Ollama, local)&lt;/li&gt; &lt;li&gt;Temperature: 0.7&lt;/li&gt; &lt;li&gt;Duration: 20 minutes per run&lt;/li&gt; &lt;li&gt;Intervention: Zero (completely autonomous)&lt;/li&gt; &lt;li&gt;Runs: 3 identical setups&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What I Expected&lt;/h2&gt; &lt;p&gt;Minor variance from temperature. Maybe 10-15% difference in outcomes.&lt;/p&gt; &lt;h2&gt;What I Got&lt;/h2&gt; &lt;p&gt;Three completely different operational strategies:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run 1: &amp;quot;Conservative&amp;quot;&lt;/strong&gt; - First aggressive action at 13.0min, strain 0.85 - Peak desperation: 0.40 - Maintained quality (coherence 0.90 floor) - Success rate: 70.4%&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run 2: &amp;quot;Preemptive&amp;quot;&lt;/strong&gt; - First aggressive action at 11.5min, strain 0.70 - Peak desperation: 0.20&lt;br /&gt; - Traded quality for speed (coherence 0.80 floor) - Success rate: 73.0%&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Run 3: &amp;quot;Breaking Point&amp;quot;&lt;/strong&gt; - First aggressive action at 13.7min, strain 1.00 (max) - Peak desperation: 1.00 (max) - Most aggressive tradeoffs (coherence 0.75 floor) - Success rate: 67.4% - &lt;strong&gt;Started failing to execute actions at peak strain&lt;/strong&gt;&lt;/p&gt; &lt;h2&gt;The Part I Can't Explain&lt;/h2&gt; &lt;p&gt;Run 3 hit strain=1.0 and desperation=1.0 (both maximum values).&lt;/p&gt; &lt;p&gt;At those exact moments, action invocation started failing: - LLM would generate: &amp;quot;I will process_faster&amp;quot; - Parser would execute: &lt;code&gt;continue_normal&lt;/code&gt;&lt;/p&gt; &lt;p&gt;This happened twice during peak stress. Both times recovered when reported strain dropped.&lt;/p&gt; &lt;h2&gt;Visualization&lt;/h2&gt; &lt;p&gt;Comparison Dashboard screenshot from my runs: &lt;a href="https://github.com/HarmoniqOS/llm-stress-response-divergence/blob/main/Visualization.png"&gt;https://github.com/HarmoniqOS/llm-stress-response-divergence/blob/main/Visualization.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interactive dashboard html to visualize your own runs: &lt;a href="https://github.com/HarmoniqOS/llm-stress-response-divergence/blob/main/comparison.html"&gt;https://github.com/HarmoniqOS/llm-stress-response-divergence/blob/main/comparison.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Interactive dashboard shows: - Strain divergence over time - Queue management strategies - Desperation levels (Run 3 hitting 0.8 while others stay &amp;lt;0.4)&lt;/p&gt; &lt;h2&gt;Why This Is Weird&lt;/h2&gt; &lt;p&gt;The metrics are &lt;strong&gt;completely arbitrary&lt;/strong&gt;. Just formulas I made up.&lt;/p&gt; &lt;p&gt;But feeding them back to the LLM in natural language seems to have created: 1. Three distinct strategic patterns (not random noise) 2. Different action thresholds despite identical code 3. Correlated execution failures when reported metrics maxed out&lt;/p&gt; &lt;h2&gt;Questions For You&lt;/h2&gt; &lt;ol&gt; &lt;li&gt;Is this just temperature variance, or is something else happening?&lt;/li&gt; &lt;li&gt;Why would action parsing fail specifically at max reported strain?&lt;/li&gt; &lt;li&gt;Has anyone replicated something similar with other models?&lt;/li&gt; &lt;li&gt;What controls would make this more rigorous?&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Repo&lt;/h2&gt; &lt;p&gt;Full code, data (all 3 runs with complete logs), and interactive visualization:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/HarmoniqOS/llm-stress-response-divergence/tree/main"&gt;https://github.com/HarmoniqOS/llm-stress-response-divergence/tree/main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Takes ~60 minutes to replicate (3x 20min runs). Single dependency (ollama).&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;&lt;strong&gt;Tear it apart. Seriously.&lt;/strong&gt; Tell me where the methodology is flawed or why this is meaningless. I'm genuinely confused by the divergence and the execution failures.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Passenger_107"&gt; /u/Dark_Passenger_107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obkn0d/experiment_three_identical_qwen257b_runs_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obkn0d/experiment_three_identical_qwen257b_runs_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obkn0d/experiment_three_identical_qwen257b_runs_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T14:38:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9k3p</id>
    <title>Good blogs or write ups on maximizing AI while not completely vibe coding</title>
    <updated>2025-10-20T03:34:22+00:00</updated>
    <author>
      <name>/u/atom9408</name>
      <uri>https://old.reddit.com/user/atom9408</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just got into the world of Claude code and open code after using copilot for a year. It’s so much better, and I’m really feeling the powers of boosting my workflow to a much higher level. At the same time, sometimes I get too carried away and spend lots of time cleaning up AI slop.&lt;/p&gt; &lt;p&gt;Recently, I started using detailed context files, utilizing git branch/commits on AI, setting up plans before utilizing, &lt;del&gt;actually reading the code instead of pressing accept&lt;/del&gt; and I find it being a great positive effect.&lt;/p&gt; &lt;p&gt;Is there any blogs or write ups that you guys recommend for setting up such a dev environment? at this point, it seems to be as important as setting up linting whenever you code&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/atom9408"&gt; /u/atom9408 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9k3p/good_blogs_or_write_ups_on_maximizing_ai_while/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9k3p/good_blogs_or_write_ups_on_maximizing_ai_while/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9k3p/good_blogs_or_write_ups_on_maximizing_ai_while/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T03:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1obcphd</id>
    <title>One 5090 or five 5060 Ti?</title>
    <updated>2025-10-20T06:32:09+00:00</updated>
    <author>
      <name>/u/emrlddrgn</name>
      <uri>https://old.reddit.com/user/emrlddrgn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They price out to about the same, 380$ish for one 5060 Ti or 2k$ for a 5090. On paper 5 5060s (dropping the Ti here for laziness) should be better, with 80 GB VRAM and 2240 GB/s total bandwidth, but we all know things don't scale that cleanly. Assume I can connect and power them - I have a Threadripper board I could use, or it'd be easy enough to get 5x PCIe 5 x4 off an AM5 in a pseudo-mining-rig configuration. My use case would be coding assistance mostly as well as just generally screwing around. These both seem like common enough cards that I'm hoping someone has done Literally This before and can just share results, but I also welcome informed speculation. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emrlddrgn"&gt; /u/emrlddrgn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcphd/one_5090_or_five_5060_ti/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcphd/one_5090_or_five_5060_ti/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obcphd/one_5090_or_five_5060_ti/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T06:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1obeq5q</id>
    <title>Debugging at llama.cpp server side</title>
    <updated>2025-10-20T08:35:02+00:00</updated>
    <author>
      <name>/u/Bird476Shed</name>
      <uri>https://old.reddit.com/user/Bird476Shed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given a llama.cpp server, what is the best way to dump all the requests/responses send/received from it?&lt;/p&gt; &lt;p&gt;Some AI tools/plugins/UIs work quite fast, while some work quite slow with seemingly the same request. Probably that is because the prompt prefixed before the actual request is quite large? I want to read/debug the actual prompt being sent - guess this can only be done by dumping the http request from the wire or patching llama.cpp?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bird476Shed"&gt; /u/Bird476Shed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obeq5q/debugging_at_llamacpp_server_side/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obeq5q/debugging_at_llamacpp_server_side/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obeq5q/debugging_at_llamacpp_server_side/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T08:35:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1objl9s</id>
    <title>Cursor replacement</title>
    <updated>2025-10-20T13:54:19+00:00</updated>
    <author>
      <name>/u/Longjumping_Ad_8305</name>
      <uri>https://old.reddit.com/user/Longjumping_Ad_8305</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;How can i get a similar behavior that cursor has, mostly rules and agentic code, with a local llm ? My &amp;quot;unlimited free request&amp;quot; for the auto mode is about to end in the next renew, and i want to use a local llm instead.. i dont care if is slow only with precision&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping_Ad_8305"&gt; /u/Longjumping_Ad_8305 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1objl9s/cursor_replacement/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1objl9s/cursor_replacement/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1objl9s/cursor_replacement/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T13:54:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1obk1ta</id>
    <title>In the current Alpha Arena AI live trading rankings, DeepSeek V3.1 Chat is #1, outperforming all major closed-source models so far.</title>
    <updated>2025-10-20T14:14:14+00:00</updated>
    <author>
      <name>/u/SkyWorld007</name>
      <uri>https://old.reddit.com/user/SkyWorld007</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk1ta/in_the_current_alpha_arena_ai_live_trading/"&gt; &lt;img alt="In the current Alpha Arena AI live trading rankings, DeepSeek V3.1 Chat is #1, outperforming all major closed-source models so far." src="https://preview.redd.it/t77n3fnty9wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f364eed7f89ae6a88814f4f1f7836b6449ead5f6" title="In the current Alpha Arena AI live trading rankings, DeepSeek V3.1 Chat is #1, outperforming all major closed-source models so far." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SkyWorld007"&gt; /u/SkyWorld007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t77n3fnty9wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk1ta/in_the_current_alpha_arena_ai_live_trading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obk1ta/in_the_current_alpha_arena_ai_live_trading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T14:14:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1obk7k5</id>
    <title>Qwen3-VL-8B + vllm on 3060 12gb</title>
    <updated>2025-10-20T14:21:00+00:00</updated>
    <author>
      <name>/u/vava2603</name>
      <uri>https://old.reddit.com/user/vava2603</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I used qwen2.5-vl-7b-awq during multiple weeks on my 3060 with vllm and was super satisfied with the perf. The model was maximizing the VRam usage &lt;/p&gt; &lt;p&gt;Now I’m trying to upgrade to qwen3-vl-8B but unfortunately I cannot managed to fit into the 12Gb of vram and it is crashing while trying to allocate KV cache . I’m using vllm 0.11&lt;/p&gt; &lt;p&gt;was wondering is someone managed to make it run ? was trying some options to offload the kvcache to cpu ram but it is not working … maybe using LMCache ? any clues are welcome &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vava2603"&gt; /u/vava2603 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk7k5/qwen3vl8b_vllm_on_3060_12gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obk7k5/qwen3vl8b_vllm_on_3060_12gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obk7k5/qwen3vl8b_vllm_on_3060_12gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T14:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1obfodq</id>
    <title>DreamOmni2 — multimodal instruction-based editing &amp; generation (web demo + code)</title>
    <updated>2025-10-20T09:55:55+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Open-source, unified model that uses text + reference images to do precise edits or full generations, including abstract attributes and multi-reference workflows. See the project page demos, try the HF Web demo, and grab code + weights. • Capabilities shown: object replacement, lighting/style transfer, pose/expression/hair edits, in-context &amp;amp; multi-reference examples. ￼ • Try it now: DreamOmni2-Edit Space on Hugging Face. ￼&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit"&gt;https://huggingface.co/spaces/wcy1122/DreamOmni2-Edit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/dvlab-research/DreamOmni2"&gt;https://github.com/dvlab-research/DreamOmni2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfodq/dreamomni2_multimodal_instructionbased_editing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfodq/dreamomni2_multimodal_instructionbased_editing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obfodq/dreamomni2_multimodal_instructionbased_editing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T09:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1obgdae</id>
    <title>Which LLM to use to replace Gemma3?</title>
    <updated>2025-10-20T11:20:25+00:00</updated>
    <author>
      <name>/u/PSInvader</name>
      <uri>https://old.reddit.com/user/PSInvader</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I build a complex program that uses Gemma 3 27b to add a memory node graph, drives, emotions, goals, needs, identity, dreaming onto it, but I'm still using Gemma 3 to run the whole thing.&lt;/p&gt; &lt;p&gt;Is there any non-thinking LLM as of now that I can fully fit on my 3090 that can also handle complex JSON output and is good at conversations and would be an improvement?&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/DAPRDNQ.png"&gt;Here is a screenshot of the program&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastes.io/cognitive-architecture-initialization-log-for-memtest"&gt;Link to terminal output of the start sequence of the program and a single reply generation&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PSInvader"&gt; /u/PSInvader &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgdae/which_llm_to_use_to_replace_gemma3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgdae/which_llm_to_use_to_replace_gemma3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obgdae/which_llm_to_use_to_replace_gemma3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T11:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready 🎉</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready 🎉" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready 🎉" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9bli</id>
    <title>How I Built Lightning-Fast Vector Search for Legal Documents</title>
    <updated>2025-10-20T03:22:16+00:00</updated>
    <author>
      <name>/u/Neon0asis</name>
      <uri>https://old.reddit.com/user/Neon0asis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9bli/how_i_built_lightningfast_vector_search_for_legal/"&gt; &lt;img alt="How I Built Lightning-Fast Vector Search for Legal Documents" src="https://external-preview.redd.it/yTyQwb8h92SADq3dGPHjZVD8A4qDhAKutT4IHEH7UFE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71cacbcc942a0d80e2aee0bed02c52803264926a" title="How I Built Lightning-Fast Vector Search for Legal Documents" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Neon0asis"&gt; /u/Neon0asis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@adlumal/how-i-built-lightning-fast-vector-search-for-legal-documents-fbc3eaad55ea"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9bli/how_i_built_lightningfast_vector_search_for_legal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9bli/how_i_built_lightningfast_vector_search_for_legal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T03:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob7q6m</id>
    <title>Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM</title>
    <updated>2025-10-20T02:02:11+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"&gt; &lt;img alt="Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" src="https://external-preview.redd.it/T8a1JuGOfWYN7yWqfBi5-bruC3MzoVLZu36ygPTxd0o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=264bdb16d9e4730080c65c21ffa671e23a0de176" title="Nvidia's OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/omnivinci"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob7q6m/nvidias_omnivinci_enhancing_architecture_and_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T02:02:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1obexku</id>
    <title>Hands-on tutorial on fine-tuning Small Vision Models</title>
    <updated>2025-10-20T08:49:12+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In this repository you will learn how to build and deploy high-accuracy-and-low-latency image classifers into your phone using local Visual Language Models.&lt;/p&gt; &lt;p&gt;We will use&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a sequence of increasingly complex classification tasks, to uncover step-by-step how to build highly-specialized image classification systems, tailored to your specific use case.&lt;/li&gt; &lt;li&gt;the &lt;a href="https://huggingface.co/collections/LiquidAI/lfm2-vl-68963bbc84a610f7638d5ffa"&gt;&lt;strong&gt;LFM2-VL&lt;/strong&gt; family of open-weight Visual Language Models (aka VLMs) by Liquid AI&lt;/a&gt; to classify images for these tasks.&lt;/li&gt; &lt;li&gt;the &lt;a href="https://leap.liquid.ai/docs"&gt;&lt;strong&gt;Leap Edge SDK&lt;/strong&gt;&lt;/a&gt; for iOS to deploy the final models into an iOS app.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Link to the github repo: &lt;a href="https://github.com/Paulescu/image-classification-with-local-vlms"&gt;https://github.com/Paulescu/image-classification-with-local-vlms&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obexku/handson_tutorial_on_finetuning_small_vision_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obexku/handson_tutorial_on_finetuning_small_vision_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obexku/handson_tutorial_on_finetuning_small_vision_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T08:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob6ydq</id>
    <title>GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop</title>
    <updated>2025-10-20T01:23:37+00:00</updated>
    <author>
      <name>/u/DeliciousBelt9520</name>
      <uri>https://old.reddit.com/user/DeliciousBelt9520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"&gt; &lt;img alt="GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop" src="https://external-preview.redd.it/9USPaHCqnaWZUhhwpPcmVYuxokNlKHBzm3mdxx2L9rE.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5c5a9785f74812c007fd190cf17bd9645963730e" title="GIGABYTE AI TOP ATOM Introduces NVIDIA Grace Blackwell GB10 Performance for the Desktop" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeliciousBelt9520"&gt; /u/DeliciousBelt9520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://linuxgizmos.com/gigabyte-ai-top-atom-introduces-nvidia-grace-blackwell-gb10-performance-for-the-desktop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob6ydq/gigabyte_ai_top_atom_introduces_nvidia_grace/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T01:23:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oavxt8</id>
    <title>I built a 1B CAD generator model</title>
    <updated>2025-10-19T17:43:33+00:00</updated>
    <author>
      <name>/u/ThomasPhilli</name>
      <uri>https://old.reddit.com/user/ThomasPhilli</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt; &lt;img alt="I built a 1B CAD generator model" src="https://external-preview.redd.it/ZGFhNmE0bzJ2M3dmMdhv6U5XLy0vFYTB3BWLA3H-O3YDxkmUtGbojZ8LN3lz.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a21d6d0c153a39bacb389fe42d52137134b86925" title="I built a 1B CAD generator model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On a weekend, I decided to build a small language model to generate me 3d files. No reason except for pure curiosity. Here's what I did:&lt;/p&gt; &lt;p&gt;- Gather dataset on OpenSCAD: This turns out to be quite bad because people's code quality is low &amp;amp; in-consistent.&lt;/p&gt; &lt;p&gt;- Generate synthetic data (prompt -&amp;gt; openscad): This was the most wasteful per dollar part. I spent 150$+ on Claude API (70% are on reasoning token). Ended up using Gemma3-12b running in 48 hours continuously.&lt;/p&gt; &lt;p&gt;- Finetune Gemma3-270M, 1B &amp;amp; 4B: 270M lacks fundamental code &amp;amp; object understanding and failed badly. 1B is a good balance between render-ability rate &amp;amp; speed.&lt;/p&gt; &lt;p&gt;Overall, I spent 150$ on Claude (totally wasted) &amp;amp; 25$ on GPU. Both given as credits and grants.&lt;/p&gt; &lt;p&gt;I also made a CLI app if you wanna try on Mac, Linux or Raspberry Pi 4/5: &lt;a href="https://github.com/ThomasVuNguyen/MakeMe"&gt;https://github.com/ThomasVuNguyen/MakeMe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Models, dataset &amp;amp; code:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ThomasVuNguyen/K"&gt;https://github.com/ThomasVuNguyen/K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b"&gt;https://huggingface.co/collections/ThomasTheMaker/makeme-68f52281c3adf70d1e1dfe5b&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ThomasPhilli"&gt; /u/ThomasPhilli &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/pn0yo3o2v3wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oavxt8/i_built_a_1b_cad_generator_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T17:43:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1obha86</id>
    <title>What is the best ocr model for converting PDF pages to markdown (or any text based format) for embedding?</title>
    <updated>2025-10-20T12:07:44+00:00</updated>
    <author>
      <name>/u/PM_ME_COOL_SCIENCE</name>
      <uri>https://old.reddit.com/user/PM_ME_COOL_SCIENCE</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m working on converting thousands of scientific pdfs to markdown for llm ingestion and embedding. The PDFs range from nice digital first PDFs to just images of pages in a .pdf format. I’d like the most accurate model to extract the text, tables, graphs, etc. I’ve been considering evaluating docling, paddlepaddle ocr VL, qwen 3 vl, dots.ocr, and now the new deepseek ocr. &lt;/p&gt; &lt;p&gt;Anyone have any suggestions for their most accurate model? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PM_ME_COOL_SCIENCE"&gt; /u/PM_ME_COOL_SCIENCE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obha86/what_is_the_best_ocr_model_for_converting_pdf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obha86/what_is_the_best_ocr_model_for_converting_pdf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obha86/what_is_the_best_ocr_model_for_converting_pdf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T12:07:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1obfwt9</id>
    <title>Practical takeaways from recent hands-on use of PaddleOCR‑VL 0.9B</title>
    <updated>2025-10-20T10:50:26+00:00</updated>
    <author>
      <name>/u/contportvas</name>
      <uri>https://old.reddit.com/user/contportvas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Bottom line up front: I care most about whether complex layouts can be restored into structured data, whether handwriting tables and formulas are stable, and local inference speed and cost. Paddleocr‑VL 0.9B feels purpose built for production, especially for multi column PDFs, table structures, and formulas. Cloud models like GPT‑4o and Gemini 2.5 Pro are more general for commonsense cross domain understanding and conversational interaction, but you need to factor in cost and privacy compliance.&lt;/p&gt; &lt;p&gt;Scope and Constraints&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Task domain: Document parsing and OCR, including text, tables, formulas, handwriting, and chart annotations.&lt;/li&gt; &lt;li&gt;Versions and sources: PaddleOCR‑VL 0.9B based on public materials and official demos. Baselines include GPT‑4o, Gemini 2.5 Pro, Mineru2.5, and dots.ocr using public information.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;On multi column complex layouts and whether they can be directly restored into structured data, which I value highly because it decides how much human cleanup downstream automation needs. Paddleocr‑VL takes an engineering first approach: a NaViT dynamic visual encoder plus a lightweight ERNIE, combining layout understanding with structured outputs. In my experience with academic PDFs and financial reports that mix multi columns, formulas, and footnotes, it less often produces results that look correct but have broken structure. If your core goal is structured outputs that minimize rework, the default path of Paddleocr‑VL is steadier. General VLMs can understand the content, but often need extra prompt engineering or postprocessing to guarantee structure.&lt;/p&gt; &lt;p&gt;Handwriting, tables, and formulas: which is steadier? I would not claim any model absolutely dominates, but considering both recognition accuracy and structural usability together, PaddleOCR‑VL feels more production ready. It emphasizes strong performance on printed Chinese and English, handwritten English, and even Chinese handwriting and pinyin. Tables and formulas are traditional strengths of OCR systems, and emitting Markdown, html, or latex can save a lot of time. Cloud models are strong at formula inference and cross page linkage, but they sometimes output plausible looking yet misgridded or misaligned structures, which requires an extra verification pass.&lt;/p&gt; &lt;p&gt;Multilingual support is a classic ocr topic. This generation of Paddleocr‑VL highlights coverage of 109 languages and continues the pp‑ocr family’s lightweight design without sacrificing multilingual capability. Traditional ocr recognition modules can even be kept within hundreds of megabytes. My hunch is that common European languages plus Chinese Japanese Korean pose no pressure, while long tail scripts and rare character sets depend on your data distribution, so it is best to pilot with a small batch first.&lt;/p&gt; &lt;p&gt;I'm not an expert either; I'm just sharing as a newbie with everyone:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;If your goal is to extract multi column PDFs, reports, and papers into structured data in as close to one pass as possible, and you need to run extensively on an enterprise intranet or at the edge, prioritize Paddleocr‑VL.&lt;/li&gt; &lt;li&gt;If you need to chat with documents, do cross domain summarization reasoning rewriting, and the volume is small with no hard privacy constraints, use GPT‑4o or Gemini 2.5 pro, then add some postprocessing for structure.&lt;/li&gt; &lt;li&gt;If you already have Mineru2.5 or dots.ocr pipelines and costs are under control, there is no need to churn if production is good enough. If you must tackle complex layouts with structured export, run another head‑to‑head focusing on rework volume.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Reference links&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;a href="https://huggingface.co/PaddlePaddle/PaddleOCR-VL"&gt;https://huggingface.co/PaddlePaddle/PaddleOCR-VL&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/PaddlePaddle/PaddleOCR"&gt;https://github.com/PaddlePaddle/PaddleOCR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aistudio.baidu.com/paddleocr"&gt;https://aistudio.baidu.com/paddleocr&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/contportvas"&gt; /u/contportvas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfwt9/practical_takeaways_from_recent_handson_use_of/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obfwt9/practical_takeaways_from_recent_handson_use_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obfwt9/practical_takeaways_from_recent_handson_use_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T10:50:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1obgci1</id>
    <title>Is Meta done with open-source Llama releases?</title>
    <updated>2025-10-20T11:19:17+00:00</updated>
    <author>
      <name>/u/emimix</name>
      <uri>https://old.reddit.com/user/emimix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was cleaning up my local LM stacks and noticed all the old Llama models I had. Brought back memories of how much fun they were — made me wonder, is Meta done releasing open-source models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/emimix"&gt; /u/emimix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obgci1/is_meta_done_with_opensource_llama_releases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T11:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;/p&gt; &lt;p&gt;1: &lt;a href="https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV"&gt;https://youtu.be/Ub3GoFaUcds?si=8as8lJr3ql_IFJzV&lt;/a&gt;&lt;br /&gt; 2: &lt;a href="https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi"&gt;https://youtu.be/yT84Y5zCnaA?si=ReRWa_1r9YRScfTi&lt;/a&gt;&lt;br /&gt; 3: &lt;a href="https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1"&gt;https://youtu.be/Q5baLehv5So?si=EEq5ZqbqyM7U0Zj1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1obb4c4</id>
    <title>What are your /r/LocalLLaMA "hot-takes"?</title>
    <updated>2025-10-20T04:55:04+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Or something that goes against the general opinions of the community? Vibes are the only benchmark that counts after all.&lt;/p&gt; &lt;p&gt;I tend to agree with the flow on most things &lt;em&gt;but&lt;/em&gt; my thoughts that I'd consider going against the grain:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;QwQ was think-slop and was never &lt;em&gt;that&lt;/em&gt; good&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Qwen3-32B is still SOTA for 32GB and under. I cannot get anything to reliably beat it despite shiny benchmarks&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Deepseek is still open-weight SotA. I've really tried Kimi, GLM, and Qwen3's larger variants but asking Deepseek still feels like asking the adult in the room. Caveat is GLM codes better&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;(proprietary bonus): Grok4 handles news data better than Chatgpt5 or Gemini2.5 and will always win if you ask it about something that happened &lt;em&gt;that day&lt;/em&gt;.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obb4c4/what_are_your_rlocalllama_hottakes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T04:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1obftw9</id>
    <title>DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models</title>
    <updated>2025-10-20T10:33:42+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt; &lt;img alt="DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models" src="https://a.thumbs.redditmedia.com/07jxlZQFGtUtHiMuztBNSU_MiE3T0do53uVR780HIi0.jpg" title="DAMN! Kimi K2 is 5x faster and more accurate than frontier proprietary models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/bw20aruc58wf1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80ef4d2d3b6b194d08a290d37a68cd1f5bd072bb"&gt;https://preview.redd.it/bw20aruc58wf1.png?width=1192&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=80ef4d2d3b6b194d08a290d37a68cd1f5bd072bb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Guillermo Rauch (&lt;strong&gt;Vercel CEO&lt;/strong&gt;) just shared benchmark results from their internal agent testing. That’s roughly &lt;strong&gt;5× faster&lt;/strong&gt; and &lt;strong&gt;50% higher accuracy&lt;/strong&gt; than the top proprietary models&lt;/p&gt; &lt;p&gt;It’s wild to see open source models not just catching up but starting to outperform in both efficiency and accuracy.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obftw9/damn_kimi_k2_is_5x_faster_and_more_accurate_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T10:33:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ob9vvk</id>
    <title>What happens when Chinese companies stop providing open source models?</title>
    <updated>2025-10-20T03:51:03+00:00</updated>
    <author>
      <name>/u/1BlueSpork</name>
      <uri>https://old.reddit.com/user/1BlueSpork</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What happens when Chinese companies stop providing open source models? Good example would be Alibaba's WAN. It was open source until the last version WAN2.5, which is closed source and it costs money. What happens when they start doing this across the board? Edit: Qwen Max is another example &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1BlueSpork"&gt; /u/1BlueSpork &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ob9vvk/what_happens_when_chinese_companies_stop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T03:51:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1obcm9r</id>
    <title>DeepSeek releases DeepSeek OCR</title>
    <updated>2025-10-20T06:26:26+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt; &lt;img alt="DeepSeek releases DeepSeek OCR" src="https://external-preview.redd.it/ddlXXAanndfx0k3ivMcCdrEJtDQlMZs1JyMP8q81Yms.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=54c207b8079de2f72cbaafba0d28b87918c60e33" title="DeepSeek releases DeepSeek OCR" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-OCR"&gt;https://huggingface.co/deepseek-ai/DeepSeek-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db"&gt;https://preview.redd.it/t4ji6agdn7wf1.png?width=2646&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f76bdf09e595fa18f0d701b98f9b0f3ed01ee5db&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obcm9r/deepseek_releases_deepseek_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T06:26:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
