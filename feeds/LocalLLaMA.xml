<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-06T10:18:07+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qxbz2u</id>
    <title>SenseTime just open-sourced SenseNova-SI 1.3, the latest model that scales on Spatial Intelligence.</title>
    <updated>2026-02-06T08:15:51+00:00</updated>
    <author>
      <name>/u/Soggy_Mission3372</name>
      <uri>https://old.reddit.com/user/Soggy_Mission3372</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbz2u/sensetime_just_opensourced_sensenovasi_13_the/"&gt; &lt;img alt="SenseTime just open-sourced SenseNova-SI 1.3, the latest model that scales on Spatial Intelligence." src="https://a.thumbs.redditmedia.com/LYL1zrERQG1pZlB7VdYiJLvRYldZ6OwSfRgcEJez1h4.jpg" title="SenseTime just open-sourced SenseNova-SI 1.3, the latest model that scales on Spatial Intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On the &lt;a href="https://x.com/hashtag/EASI?src=hashtag_click"&gt;EASI&lt;/a&gt; leaderboard, it ranks No.1 overall under EASI-8, outperforming &lt;a href="https://x.com/hashtag/Gemini3?src=hashtag_click"&gt;Gemini3&lt;/a&gt; in average performance across eight spatial intelligence benchmarks. &lt;/p&gt; &lt;p&gt;From safer &lt;a href="https://x.com/hashtag/AutonomousDriving?src=hashtag_click"&gt;AutonomousDriving&lt;/a&gt; in complex environments to smarter home &lt;a href="https://x.com/hashtag/robots?src=hashtag_click"&gt;robots&lt;/a&gt; , SenseNova-SI 1.3 accelerates and broadens deployment opportunities across enterprise and consumer applications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k8k5bour1uhg1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c99d843fed6b6791c2c2aada730257a273299b7c"&gt;https://preview.redd.it/k8k5bour1uhg1.jpg?width=900&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=c99d843fed6b6791c2c2aada730257a273299b7c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Open-Source Resources: &lt;a href="https://huggingface.co/collections/sensenova/sensenova-si"&gt;SenseNova-SI - a sensenova Collection&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SenseNova-SI Code: &lt;a href="https://github.com/OpenSenseNova/SenseNova-SI"&gt;OpenSenseNova/SenseNova-SI: Scaling Spatial Intelligence with Multimodal Foundation Models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Soggy_Mission3372"&gt; /u/Soggy_Mission3372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbz2u/sensetime_just_opensourced_sensenovasi_13_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbz2u/sensetime_just_opensourced_sensenovasi_13_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbz2u/sensetime_just_opensourced_sensenovasi_13_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:15:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxcdx0</id>
    <title>Do you find AI memory features actually helpful?</title>
    <updated>2026-02-06T08:41:48+00:00</updated>
    <author>
      <name>/u/Deep_Traffic_7873</name>
      <uri>https://old.reddit.com/user/Deep_Traffic_7873</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tried using them but find them confusing and opaque. Instead, I'm experimenting with a simpler approach using .md files:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Keep a file with important info and rules&lt;/li&gt; &lt;li&gt;Explicitly reference it at conversation start&lt;/li&gt; &lt;li&gt;Update it manually when needed&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This feels more reliable because:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I know exactly what's in context&lt;/li&gt; &lt;li&gt;No mystery &amp;quot;remembering&amp;quot; of things I forgot I mentioned&lt;/li&gt; &lt;li&gt;Easier to debug when the AI behaves weirdly&lt;/li&gt; &lt;li&gt;No token bloat from accumulated junk&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The tradeoff is more manual work, but I'm wondering if that's actually better than hoping the memory system captured the right stuff.&lt;/p&gt; &lt;p&gt;What's your experience? Do you use memory features religiously, avoid them, or handle context differently?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep_Traffic_7873"&gt; /u/Deep_Traffic_7873 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcdx0/do_you_find_ai_memory_features_actually_helpful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcdx0/do_you_find_ai_memory_features_actually_helpful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcdx0/do_you_find_ai_memory_features_actually_helpful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:41:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwngbv</id>
    <title>OpenWebui + Ace Step 1.5</title>
    <updated>2026-02-05T14:57:36+00:00</updated>
    <author>
      <name>/u/iChrist</name>
      <uri>https://old.reddit.com/user/iChrist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwngbv/openwebui_ace_step_15/"&gt; &lt;img alt="OpenWebui + Ace Step 1.5" src="https://b.thumbs.redditmedia.com/epKePg4NU33-mx1EvCIYhJsukfYPYtcAcJVLZLyiqyA.jpg" title="OpenWebui + Ace Step 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With the new Ace-Step 1.5 music generation model and the awesome developer of the tools:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Haervwe/open-webui-tools"&gt;https://github.com/Haervwe/open-webui-tools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;With a beefy GPU (24GB) you can use a decent LLM like GPT-OSS:20b or Ministral alongside the full ace step model and generate music on the go!&lt;/p&gt; &lt;p&gt;I hope you guys found it awesome and star his github page, he has so many good tools for openwebui!&lt;/p&gt; &lt;p&gt;We are at a point where you can hook up Flux Klein for image generation and image editing, use ace step to create music, all with one interface, model with tool support are a game changer.&lt;/p&gt; &lt;p&gt;With all the other benefits like web search, computer use through playwright mcp, youtube summarizing or basically anything you need.&lt;/p&gt; &lt;p&gt;What competitive edge does ChatGPT and the likes still poses?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iChrist"&gt; /u/iChrist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwngbv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwngbv/openwebui_ace_step_15/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwngbv/openwebui_ace_step_15/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T14:57:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwy1ca</id>
    <title>sim.ai is no longer fully open-source</title>
    <updated>2026-02-05T21:20:53+00:00</updated>
    <author>
      <name>/u/freehuntx</name>
      <uri>https://old.reddit.com/user/freehuntx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a heads up for anyone currently using or tracking sim.ai. &lt;/p&gt; &lt;p&gt;It looks like they‚Äôve pivoted away from being fully open source. &lt;/p&gt; &lt;p&gt;I spotted a recent commit that significantly changes the licensing and code availability. If you're building on top of this or planning to, you should definitely check the diffs and the new terms before committing more time to it. &lt;/p&gt; &lt;p&gt;Here‚Äôs the commit in question:&lt;br /&gt; &lt;a href="https://github.com/simstudioai/sim/commit/46822e91f327c591a6f537275a0fd83fb83ff504#diff-1091f99ae5606ec884abb378eb612ea29534be2044a8dfce6d52bbb918f4f6ac"&gt;https://github.com/simstudioai/sim/commit/46822e91f327c591a6f537275a0fd83fb83ff504#diff-1091f99ae5606ec884abb378eb612ea29534be2044a8dfce6d52bbb918f4f6ac&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freehuntx"&gt; /u/freehuntx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwy1ca/simai_is_no_longer_fully_opensource/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwy1ca/simai_is_no_longer_fully_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwy1ca/simai_is_no_longer_fully_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T21:20:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwt8av</id>
    <title>Vibe-coding client now in Llama.cpp! (maybe)</title>
    <updated>2026-02-05T18:27:21+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwt8av/vibecoding_client_now_in_llamacpp_maybe/"&gt; &lt;img alt="Vibe-coding client now in Llama.cpp! (maybe)" src="https://external-preview.redd.it/SfXQgLioflEqGI2A5Yr41nnHSaue_y-63o8dkAbmgPc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de5472454203e6adaa20045c05732a71981fde83" title="Vibe-coding client now in Llama.cpp! (maybe)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've created a small proof-of-concept MCP client on top llama.cpp's `llama-cli`.&lt;/p&gt; &lt;p&gt;Now you can add MCP servers (I've added a config with Serena, a great MCP coding server that can instantly turn your CLI into a full-fledged terminal coder) and use them directly in `llama-cli`.&lt;/p&gt; &lt;p&gt;Features an `--mcp-yolo` mode for all you hardcore `rm -rf --no-preserve-root /` fans!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19373"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwt8av/vibecoding_client_now_in_llamacpp_maybe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwt8av/vibecoding_client_now_in_llamacpp_maybe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T18:27:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwboqn</id>
    <title>Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy</title>
    <updated>2026-02-05T04:37:05+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"&gt; &lt;img alt="Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy" src="https://external-preview.redd.it/Xfy8b5oz8xAgNpbj0L9Mmjzxactj5HdaKRFOmBPu0YE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c9dad5b13e20f57d64f5fc0bbc7415c9f4186b1d" title="Google Research announces Sequential Attention: Making AI models leaner and faster without sacrificing accuracy" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.google/blog/sequential-attention-making-ai-models-leaner-and-faster-without-sacrificing-accuracy/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwboqn/google_research_announces_sequential_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T04:37:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxaam4</id>
    <title>Just scored 2 MI50 32GB what should I run?</title>
    <updated>2026-02-06T06:35:26+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like the title says. I just got two MI50 32GB cards. So 64gb VRAM. I‚Äôve been playing around with the ministral models on my 7900 XT and 6800 16 gb. Currently I can‚Äôt run both mi50‚Äôs in my rig so I‚Äôm using the 7900 and one MI50. So 52GB of VRAM atm. So what should I run now?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxaam4/just_scored_2_mi50_32gb_what_should_i_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxaam4/just_scored_2_mi50_32gb_what_should_i_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxaam4/just_scored_2_mi50_32gb_what_should_i_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T06:35:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwobcc</id>
    <title>Strix Halo benchmarks: 13 models, 15 llama.cpp builds</title>
    <updated>2026-02-05T15:30:19+00:00</updated>
    <author>
      <name>/u/Beneficial-Shame-483</name>
      <uri>https://old.reddit.com/user/Beneficial-Shame-483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"&gt; &lt;img alt="Strix Halo benchmarks: 13 models, 15 llama.cpp builds" src="https://preview.redd.it/feayylk82phg1.png?width=140&amp;amp;height=48&amp;amp;auto=webp&amp;amp;s=0f252b48bce559e0c572a43100cbd3ea8a9ccb86" title="Strix Halo benchmarks: 13 models, 15 llama.cpp builds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/feayylk82phg1.png?width=3469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd82806fb3743ba1b57c2ade12ef4d71e25679bf"&gt;https://preview.redd.it/feayylk82phg1.png?width=3469&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fd82806fb3743ba1b57c2ade12ef4d71e25679bf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Ran a software ablation study on the Strix Halo's iGPU testing anything I could fine (ROCm, Vulkan, gfx version, hipblaslt on/off, rocWMMA, various Vulkan/RADV options) across different build configurations. Rather than fighting dependency hell to find &amp;quot;the&amp;quot; working setup, I dockerized 15 different llama.cpp builds and let them all run. Some failed but that's ok, that's data too.&lt;/p&gt; &lt;p&gt;&lt;a href="https://whylucian.github.io/softab/results-tables/results.html"&gt;https://whylucian.github.io/softab/results-tables/results.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Beneficial-Shame-483"&gt; /u/Beneficial-Shame-483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwobcc/strix_halo_benchmarks_13_models_15_llamacpp_builds/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:30:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwz0x6</id>
    <title>Any feedback on step-3.5-flash ?</title>
    <updated>2026-02-05T21:58:09+00:00</updated>
    <author>
      <name>/u/Jealous-Astronaut457</name>
      <uri>https://old.reddit.com/user/Jealous-Astronaut457</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It was overshadowed by qwen3-next-coder and was not supported by llamacpp at launch, but it looks like a very promising model for local inference. My first impression of stepfun's chat is that the model is a thinker, but what are your impressions few days after the release ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Jealous-Astronaut457"&gt; /u/Jealous-Astronaut457 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwz0x6/any_feedback_on_step35flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwz0x6/any_feedback_on_step35flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwz0x6/any_feedback_on_step35flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T21:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx5xyc</id>
    <title>fine-tuned a multilingual TTS model for colloquial Egyptian Arabic (open-source + samples)</title>
    <updated>2026-02-06T02:55:19+00:00</updated>
    <author>
      <name>/u/Economy_Emphasis9898</name>
      <uri>https://old.reddit.com/user/Economy_Emphasis9898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I wanted to share a small project I‚Äôve been working on.&lt;/p&gt; &lt;p&gt;Most open Arabic TTS systems focus on MSA, which sounds very different from spoken Egyptian Arabic. I fine-tuned the multilingual Chatterbox TTS model specifically for &lt;strong&gt;colloquial Egyptian Arabic&lt;/strong&gt;, aiming for native pronunciation and rhythm rather than formal MSA.&lt;/p&gt; &lt;p&gt;I‚Äôve made everything public:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub repo (training + preprocessing)&lt;/li&gt; &lt;li&gt;Hugging Face model&lt;/li&gt; &lt;li&gt;A few Egyptian Arabic audio samples&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning?utm_source=chatgpt.com"&gt;https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning&lt;/a&gt;&lt;br /&gt; Samples: &lt;a href="https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning/tree/main/samples?utm_source=chatgpt.com"&gt;https://github.com/AliAbdallah21/Chatterbox-Multilingual-TTS-Fine-Tuning/tree/main/samples&lt;/a&gt;&lt;br /&gt; HF model: &lt;a href="https://huggingface.co/AliAbdallah/egyptian-arabic-tts-chatterbox"&gt;https://huggingface.co/AliAbdallah/egyptian-arabic-tts-chatterbox&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would really appreciate feedback from people who‚Äôve worked with TTS or multilingual models especially on audio quality and what could be improved next.&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Economy_Emphasis9898"&gt; /u/Economy_Emphasis9898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx5xyc/finetuned_a_multilingual_tts_model_for_colloquial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx5xyc/finetuned_a_multilingual_tts_model_for_colloquial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx5xyc/finetuned_a_multilingual_tts_model_for_colloquial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T02:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwue2w</id>
    <title>SoproTTS v1.5: A 135M zero-shot voice cloning TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU</title>
    <updated>2026-02-05T19:08:29+00:00</updated>
    <author>
      <name>/u/SammyDaBeast</name>
      <uri>https://old.reddit.com/user/SammyDaBeast</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"&gt; &lt;img alt="SoproTTS v1.5: A 135M zero-shot voice cloning TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU" src="https://external-preview.redd.it/CWCqETRtx5uPv_SVUC4tOtF3EsWY3Pg-rooYdIufOP0.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=3e1da0ab72d9427c772c087709903112c521ae66" title="SoproTTS v1.5: A 135M zero-shot voice cloning TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, thank you for the support on my first release. &lt;/p&gt; &lt;p&gt;Today, I'm releasing a new version of my side project: SoproTTS &lt;/p&gt; &lt;p&gt;A 135M parameter TTS model trained for ~$100 on 1 GPU, running ~20√ó real-time on a base MacBook M3 CPU. &lt;/p&gt; &lt;p&gt;v1.5 highlights (on CPU): &lt;/p&gt; &lt;p&gt;‚Ä¢ 250 ms TTFA streaming latency&lt;br /&gt; ‚Ä¢ 0.05 RTF (~20√ó real-time)&lt;br /&gt; ‚Ä¢ Zero-shot voice cloning&lt;br /&gt; ‚Ä¢ Smaller, faster, more stable &lt;/p&gt; &lt;p&gt;Still not perfect (OOD voices can be tricky, and there are still some artifacts), but a decent upgrade. Training code TBA.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/samuel-vitorino/sopro"&gt;https://github.com/samuel-vitorino/sopro&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qwue2w/video/y114to0a2qhg1/player"&gt;https://reddit.com/link/1qwue2w/video/y114to0a2qhg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SammyDaBeast"&gt; /u/SammyDaBeast &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwue2w/soprotts_v15_a_135m_zeroshot_voice_cloning_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T19:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwrpom</id>
    <title>really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025</title>
    <updated>2026-02-05T17:33:54+00:00</updated>
    <author>
      <name>/u/datascienceharp</name>
      <uri>https://old.reddit.com/user/datascienceharp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"&gt; &lt;img alt="really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025" src="https://b.thumbs.redditmedia.com/lemBjuywLXHSh55oKPeQldeNetPiKExhrecGB4VPRXY.jpg" title="really impressed with these new ocr models (lightonocr-2 and glm-ocr). much better than what i saw come out in nov-dec 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;gif 1: LightOnOCR-2-1B&lt;/p&gt; &lt;p&gt;docs page: &lt;a href="https://docs.voxel51.com/plugins/plugins_ecosystem/lightonocr_2.html"&gt;https://docs.voxel51.com/plugins/plugins_ecosystem/lightonocr_2.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quickstart nb: &lt;a href="https://github.com/harpreetsahota204/LightOnOCR-2/blob/main/lightonocr2_fiftyone_example.ipynb"&gt;https://github.com/harpreetsahota204/LightOnOCR-2/blob/main/lightonocr2_fiftyone_example.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;gif 2: GLM-OCR&lt;/p&gt; &lt;p&gt;docs page: &lt;a href="https://docs.voxel51.com/plugins/plugins_ecosystem/glm_ocr.html"&gt;https://docs.voxel51.com/plugins/plugins_ecosystem/glm_ocr.html&lt;/a&gt;&lt;/p&gt; &lt;p&gt;quickstart nb: &lt;a href="https://github.com/harpreetsahota204/glm_ocr/blob/main/glm_ocr_fiftyone_example.ipynb"&gt;https://github.com/harpreetsahota204/glm_ocr/blob/main/glm_ocr_fiftyone_example.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;imo, glm-ocr takes the cake. much faster, and you can get pretty reliable structured output &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/datascienceharp"&gt; /u/datascienceharp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwrpom"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwrpom/really_impressed_with_these_new_ocr_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T17:33:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx4alp</id>
    <title>Qwen3-Coder-Next; Unsloth Quants having issues calling tools?</title>
    <updated>2026-02-06T01:40:16+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is regarding Q4 and Q5 quants that I've tried.&lt;/p&gt; &lt;p&gt;Qwen3-Coder-Next seems to write good code, but man does it keep erroring out on tool calls!&lt;/p&gt; &lt;p&gt;Rebuilt llama CPP from latest a few days ago. The errors don't seem to bubble up to the tool I'm using (Claude Code, Qwen-Code) but rather in the llama-cpp logs, and it seems to be a bunch of regex that's different each time.&lt;/p&gt; &lt;p&gt;Are there known issues?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx4alp/qwen3codernext_unsloth_quants_having_issues/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T01:40:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxbl7j</id>
    <title>Kimi K2.5 on 4x RTX 6000 Pro Blackwell runpod Benchmarks</title>
    <updated>2026-02-06T07:52:29+00:00</updated>
    <author>
      <name>/u/skysthelimit187</name>
      <uri>https://old.reddit.com/user/skysthelimit187</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to test the performance of Kimi K2.5 (mainly TTFT and Tok/s) on a Setup with 4x RTX 6000 Pro Blackwell. So I rented a system on runpod (for ~7$ per hour).&lt;/p&gt; &lt;p&gt;Problem is I am a absolute beginner in Terms of Local LLMs. I figured that SGLang with KT-Kernel seem to be a good way for performance, if the entire model does not fit into VRAM.&lt;/p&gt; &lt;p&gt;My whole command line looks like this:&lt;/p&gt; &lt;p&gt;&lt;code&gt; python3 -m sglang.launch_server \ --host 0.0.0.0 \ --port 8090 \ --model /workspace/models/Kimi-K2.5 \ --tp-size 4 \ --kt-weight-path /workspace/models/Kimi-K2.5 \ --kt-cpuinfer 128 \ --kt-threadpool-count 2 \ --kt-num-gpu-experts 180 \ --kt-method RAWINT4 \ --kt-gpu-prefill-token-threshold 2048 \ --mem-fraction-static 0.85 \ --trust-remote-code \ --served-model-name Kimi-K2.5 \ --reasoning-parser kimi_k2 \ --tool-call-parser kimi_k2 \ --enable-mixed-chunk \ --attention-backend flashinfer \ --context-length 131072 \ --max-total-tokens 150000 \ --enable-p2p-check &lt;/code&gt;&lt;/p&gt; &lt;p&gt;Here are benchmark results with diffferent parameters:&lt;/p&gt; &lt;p&gt;``` python3 -m sglang.bench_serving --host 127.0.0.1 --port 8090 --dataset-name sharegpt --num-prompts 100&lt;/p&gt; &lt;p&gt;Kimi-K2.5 4x RTX 6000 PRO --mem-fraction-static 0.90 --kt-num-gpu-experts 20 --kt-gpu-prefill-token-threshold 1000 ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 100&lt;br /&gt; Benchmark duration (s): 797.57&lt;br /&gt; Total input tokens: 33147&lt;br /&gt; Total input text tokens: 33147&lt;br /&gt; Total generated tokens: 21350&lt;br /&gt; Total generated tokens (retokenized): 21343&lt;br /&gt; Request throughput (req/s): 0.13&lt;br /&gt; Input token throughput (tok/s): 41.56&lt;br /&gt; Output token throughput (tok/s): 26.77&lt;br /&gt; Peak output token throughput (tok/s): 99.00&lt;br /&gt; Peak concurrent requests: 100&lt;br /&gt; Total token throughput (tok/s): 68.33&lt;br /&gt; Concurrency: 40.28&lt;br /&gt; ----------------End-to-End Latency---------------- Mean E2E Latency (ms): 321229.26 Median E2E Latency (ms): 302115.02 P90 E2E Latency (ms): 649477.80 P99 E2E Latency (ms): 734740.50 ---------------Time to First Token---------------- Mean TTFT (ms): 43683.46&lt;br /&gt; Median TTFT (ms): 39622.10&lt;br /&gt; P99 TTFT (ms): 63386.48&lt;br /&gt; -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 2308.10&lt;br /&gt; Median TPOT (ms): 1744.01&lt;br /&gt; P99 TPOT (ms): 7974.68&lt;br /&gt; ---------------Inter-Token Latency---------------- Mean ITL (ms): 1306.10&lt;br /&gt; Median ITL (ms): 1376.37&lt;br /&gt; P95 ITL (ms): 1999.40&lt;br /&gt; P99 ITL (ms): 5206.45 &lt;/p&gt; &lt;h1&gt;Max ITL (ms): 12761.78 &lt;/h1&gt; &lt;p&gt;Kimi-K2.5 4x RTX 6000 PRO --mem-fraction-static 0.80 --kt-num-gpu-experts 64 --kt-gpu-prefill-token-threshold 2048 ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 100&lt;br /&gt; Benchmark duration (s): 720.88&lt;br /&gt; Total input tokens: 33147&lt;br /&gt; Total input text tokens: 33147&lt;br /&gt; Total generated tokens: 21350&lt;br /&gt; Total generated tokens (retokenized): 21345&lt;br /&gt; Request throughput (req/s): 0.14&lt;br /&gt; Input token throughput (tok/s): 45.98&lt;br /&gt; Output token throughput (tok/s): 29.62&lt;br /&gt; Peak output token throughput (tok/s): 99.00&lt;br /&gt; Peak concurrent requests: 100&lt;br /&gt; Total token throughput (tok/s): 75.60&lt;br /&gt; Concurrency: 42.07&lt;br /&gt; ----------------End-to-End Latency---------------- Mean E2E Latency (ms): 303249.40 Median E2E Latency (ms): 285529.22 P90 E2E Latency (ms): 593663.77 P99 E2E Latency (ms): 666586.61 ---------------Time to First Token---------------- Mean TTFT (ms): 49258.67&lt;br /&gt; Median TTFT (ms): 44937.76&lt;br /&gt; P99 TTFT (ms): 68691.17&lt;br /&gt; -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 2227.62&lt;br /&gt; Median TPOT (ms): 1599.91&lt;br /&gt; P99 TPOT (ms): 7969.61&lt;br /&gt; ---------------Inter-Token Latency---------------- Mean ITL (ms): 1195.25&lt;br /&gt; Median ITL (ms): 1293.28&lt;br /&gt; P95 ITL (ms): 2125.91&lt;br /&gt; P99 ITL (ms): 5073.84 &lt;/p&gt; &lt;h1&gt;Max ITL (ms): 13245.65 &lt;/h1&gt; &lt;p&gt;Kimi-K2.5 4x RTX 6000 PRO --mem-fraction-static 0.85 --kt-num-gpu-experts 180 --kt-gpu-prefill-token-threshold 2048 ============ Serving Benchmark Result ============ Backend: sglang&lt;br /&gt; Traffic request rate: inf&lt;br /&gt; Max request concurrency: not set&lt;br /&gt; Successful requests: 100&lt;br /&gt; Benchmark duration (s): 569.87&lt;br /&gt; Total input tokens: 33147&lt;br /&gt; Total input text tokens: 33147&lt;br /&gt; Total generated tokens: 21350&lt;br /&gt; Total generated tokens (retokenized): 21346&lt;br /&gt; Request throughput (req/s): 0.18&lt;br /&gt; Input token throughput (tok/s): 58.17&lt;br /&gt; Output token throughput (tok/s): 37.46&lt;br /&gt; Peak output token throughput (tok/s): 123.00&lt;br /&gt; Peak concurrent requests: 100&lt;br /&gt; Total token throughput (tok/s): 95.63&lt;br /&gt; Concurrency: 44.35&lt;br /&gt; ----------------End-to-End Latency---------------- Mean E2E Latency (ms): 252740.99 Median E2E Latency (ms): 240023.88 P90 E2E Latency (ms): 448283.65 P99 E2E Latency (ms): 505817.34 ---------------Time to First Token---------------- Mean TTFT (ms): 75851.65&lt;br /&gt; Median TTFT (ms): 70053.38&lt;br /&gt; P99 TTFT (ms): 99228.64&lt;br /&gt; -----Time per Output Token (excl. 1st token)------ Mean TPOT (ms): 1908.22&lt;br /&gt; Median TPOT (ms): 1081.44&lt;br /&gt; P99 TPOT (ms): 9853.65&lt;br /&gt; ---------------Inter-Token Latency---------------- Mean ITL (ms): 832.42&lt;br /&gt; Median ITL (ms): 774.26&lt;br /&gt; P95 ITL (ms): 1237.89&lt;br /&gt; P99 ITL (ms): 2973.36 &lt;/p&gt; &lt;h1&gt;Max ITL (ms): 22928.28 &lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;Do you have any suggestions on how to tweak this better?&lt;/p&gt; &lt;p&gt;If you are asking yourself why I am testing this o 4x RTX 6000 Pro Bw? I want to buy a Dell Precision7960 Tower Workstation with that Setup to run large Models like Kimi K2.5. It cost around 90k ‚Ç¨.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/skysthelimit187"&gt; /u/skysthelimit187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbl7j/kimi_k25_on_4x_rtx_6000_pro_blackwell_runpod/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbl7j/kimi_k25_on_4x_rtx_6000_pro_blackwell_runpod/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxbl7j/kimi_k25_on_4x_rtx_6000_pro_blackwell_runpod/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T07:52:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxc8qj</id>
    <title>"Minimum Buy-in" Build</title>
    <updated>2026-02-06T08:32:30+00:00</updated>
    <author>
      <name>/u/jmuff98</name>
      <uri>https://old.reddit.com/user/jmuff98</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxc8qj/minimum_buyin_build/"&gt; &lt;img alt="&amp;quot;Minimum Buy-in&amp;quot; Build" src="https://preview.redd.it/exb6j45a5uhg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0ebdf0ecbf44394a813508836f918d0e6781d0b" title="&amp;quot;Minimum Buy-in&amp;quot; Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished putting this together. &lt;/p&gt; &lt;p&gt;Supermicro x10drh One Radeon pro v340 on each 6 pcie 3.0 x8 slots. The only x16 slot is bifurcated to x8x4x4 for dual Nvme drives and another GPU down the line. But testing first for peak power. I have 15A 120v socket only.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jmuff98"&gt; /u/jmuff98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/exb6j45a5uhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxc8qj/minimum_buyin_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxc8qj/minimum_buyin_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:32:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwo9j0</id>
    <title>We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF</title>
    <updated>2026-02-05T15:28:27+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"&gt; &lt;img alt="We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF" src="https://external-preview.redd.it/bmIycDZuMHYxcGhnMTkRUzZawZzMWm4JXBBoVayTVh3fNrkxvwbY4-FVurAN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=416b047c4576396c89a7eee16410255cdb27cd61" title="We built an 8B world model that beats 402B Llama 4 by generating web code instead of pixels ‚Äî open weights on HF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Here's something new for you: Mobile World Models.&lt;br /&gt; We just released gWorld ‚Äî open-weight visual world models for mobile GUIs (8B and 32B).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo Video Explanation:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here's gWorld 32B imagining a multi-step Booking dot com session ‚Äî zero access to the real app:&lt;br /&gt; 1. Sees flight search form (Detroit ‚Üí Chicago)&lt;br /&gt; 2. Click &amp;quot;Search&amp;quot; ‚Üí writes code ‚Üí renders full results page with airlines, prices, times&lt;br /&gt; 3. Click destination field ‚Üí predicts the search UI with history &lt;/p&gt; &lt;p&gt;Every screen = executable HTML/CSS/JS rendered to pixels.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The core idea:&lt;/strong&gt; Instead of predicting the next screen as pixels (diffusion, autoregressive image gen), gWorld predicts it as executable web code. You render the code, you get the image. This sounds simple but it works remarkably well because VLMs already have strong priors on structured web code from pre-training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why code instead of pixels?&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Text-based world models lose visual fidelity (can't represent layouts, colors, images)&lt;/li&gt; &lt;li&gt;Pixel-generation models hallucinate text and structural elements&lt;/li&gt; &lt;li&gt;Code generation gives you the best of both: precise text rendering from linguistic priors + high-fidelity visuals from structured code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results on MWMBench (6 benchmarks, 4 ID + 2 OOD):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Avg Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;29.2%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Scout&lt;/td&gt; &lt;td align="left"&gt;109B (A17B)&lt;/td&gt; &lt;td align="left"&gt;50.0%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 4 Maverick&lt;/td&gt; &lt;td align="left"&gt;402B (A17B)&lt;/td&gt; &lt;td align="left"&gt;55.7%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3 VL&lt;/td&gt; &lt;td align="left"&gt;235B (A22B)&lt;/td&gt; &lt;td align="left"&gt;51.5%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.6V&lt;/td&gt; &lt;td align="left"&gt;106B&lt;/td&gt; &lt;td align="left"&gt;67.4%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gWorld&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;74.9%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;gWorld&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;32B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.6%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The 8B model beats everything up to 50√ó its size. Render failure rate is &amp;lt;1% (vs 40% for base Qwen3 VL 8B before our training).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other things worth noting:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Data scaling follows a power law with R¬≤ ‚â• 0.94 ‚Äî gains are predictable and nowhere near saturating&lt;/li&gt; &lt;li&gt;We include a Korean apps benchmark (KApps) as OOD eval ‚Äî the models generalize well cross-lingually&lt;/li&gt; &lt;li&gt;The data pipeline is automated: repurpose existing trajectory data ‚Üí cross-modal relabeling to code ‚Üí synthetic reasoning traces&lt;/li&gt; &lt;li&gt;We also show that better world models ‚Üí better downstream GUI agent performance&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why this matters beyond benchmarks:&lt;/strong&gt; The bottleneck for training GUI agents with online RL is device-policy coupling ‚Äî every rollout needs a real Android emulator. World models could decouple this entirely, enabling massively parallel rollouts on pure compute. gWorld is a step in that direction.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ü§ó gWorld 8B: &lt;a href="https://huggingface.co/trillionlabs/gWorld-8B"&gt;https://huggingface.co/trillionlabs/gWorld-8B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ü§ó gWorld 32B: &lt;a href="https://huggingface.co/trillionlabs/gWorld-32B"&gt;https://huggingface.co/trillionlabs/gWorld-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üíª Code: &lt;a href="https://github.com/trillion-labs/gWorld"&gt;https://github.com/trillion-labs/gWorld&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üìÑ Paper: &lt;a href="https://huggingface.co/papers/2602.01576"&gt;https://huggingface.co/papers/2602.01576&lt;/a&gt;&lt;/li&gt; &lt;li&gt;üåê Project page (and demos): &lt;a href="https://trillionlabs-gworld.github.io/"&gt;https://trillionlabs-gworld.github.io&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Benchmarks (incl. K-Apps) coming soon.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Happy to answer questions.&lt;br /&gt; Built by Trillion Labs √ó KAIST AI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/37uavl0v1phg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwo9j0/we_built_an_8b_world_model_that_beats_402b_llama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T15:28:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx2teh</id>
    <title>~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)</title>
    <updated>2026-02-06T00:34:05+00:00</updated>
    <author>
      <name>/u/Spiritual_Tie_5574</name>
      <uri>https://old.reddit.com/user/Spiritual_Tie_5574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"&gt; &lt;img alt="~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)" src="https://b.thumbs.redditmedia.com/HZiUyd9n-ZrICOCtj5qj7gJrQgYD4FauYK8iUuHW7_s.jpg" title="~26 tok/sec with Unsloth Qwen3-Coder-Next-Q4_K_S on RTX 5090 (Windows/llama.cpp)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/9gfytpz5srhg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11f99eb16917695fa52dbf8ebec6acaf0105e1e9"&gt;https://preview.redd.it/9gfytpz5srhg1.png?width=692&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=11f99eb16917695fa52dbf8ebec6acaf0105e1e9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hey all,&lt;/p&gt; &lt;p&gt;Just a quick one in case it saves someone else a headache. I was getting really poor throughput (~10 tok/sec) with Qwen3-Coder-Next-Q4_K_S.gguf on llama.cpp, like ‚Äúthis can‚Äôt be right‚Äù levels, and eventually found a set of args that fixed it for me.&lt;/p&gt; &lt;p&gt;My rig:&lt;/p&gt; &lt;p&gt;- RTX 5090&lt;/p&gt; &lt;p&gt;- 9950X3D&lt;/p&gt; &lt;p&gt;- 96GB RAM&lt;/p&gt; &lt;p&gt;Driver 591.86 / CUDA 13.1&lt;/p&gt; &lt;p&gt;llama.cpp b7951&lt;/p&gt; &lt;p&gt;Model: Unsloth GGUF Qwen3-Coder-Next-Q4_K_S.gguf&lt;/p&gt; &lt;p&gt;What worked:&lt;/p&gt; &lt;p&gt;&lt;code&gt;-c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -np 1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Full command:&lt;/p&gt; &lt;p&gt;&lt;code&gt;.\llama-bin\llama-server.exe -m &amp;quot;C:\path\to\Qwen3-Coder-Next-Q4_K_S.gguf&amp;quot; -c 32768 -ngl 999 --flash-attn auto -ctk q8_0 -ctv q8_0 -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; -np 1 --host&lt;/code&gt; &lt;a href="http://127.0.0.1"&gt;&lt;code&gt;127.0.0.1&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;From what I can tell, the big win here is:&lt;/p&gt; &lt;p&gt;- Offloading the MoE expert tensors (the .ffn_.*_exps ones) to CPU, which seems to reduce VRAM pressure / weird paging/traffic on this *huge* model&lt;/p&gt; &lt;p&gt;- Quantising KV cache (ctk/ctv q8_0) helps a lot at 32k context&lt;/p&gt; &lt;p&gt;Small warning: the &lt;code&gt;-ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/code&gt; bit seems great for this massive Qwen3-Next GGUF, but I‚Äôve seen it hurt smaller MoE models (extra CPU work / transfers), so definitely benchmark on your own setup.&lt;/p&gt; &lt;p&gt;Hope that helps someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spiritual_Tie_5574"&gt; /u/Spiritual_Tie_5574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx2teh/26_toksec_with_unsloth_qwen3codernextq4_k_s_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T00:34:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx0gxy</id>
    <title>Any hope for Gemma 4 release?</title>
    <updated>2026-02-05T22:54:46+00:00</updated>
    <author>
      <name>/u/gamblingapocalypse</name>
      <uri>https://old.reddit.com/user/gamblingapocalypse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given that there been a lot of great releases, do you think Gemma 4 would be similar to or even better than what we've seen? Or did Google give up on the project?&lt;/p&gt; &lt;p&gt;What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gamblingapocalypse"&gt; /u/gamblingapocalypse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0gxy/any_hope_for_gemma_4_release/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0gxy/any_hope_for_gemma_4_release/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0gxy/any_hope_for_gemma_4_release/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T22:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx0kzb</id>
    <title>PR to implemt tensor parallelism in Llama.cpp</title>
    <updated>2026-02-05T22:59:13+00:00</updated>
    <author>
      <name>/u/keyboardhack</name>
      <uri>https://old.reddit.com/user/keyboardhack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0kzb/pr_to_implemt_tensor_parallelism_in_llamacpp/"&gt; &lt;img alt="PR to implemt tensor parallelism in Llama.cpp" src="https://external-preview.redd.it/QSt5C9i-4IS4QnEvc5D4DF24jORBMQJOEdeWPERjEmk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b5c0fef7864004ff1e03585a93bc0bfb5770856e" title="PR to implemt tensor parallelism in Llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/keyboardhack"&gt; /u/keyboardhack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19378"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0kzb/pr_to_implemt_tensor_parallelism_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx0kzb/pr_to_implemt_tensor_parallelism_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T22:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx95kh</id>
    <title>Deep what do you think?</title>
    <updated>2026-02-06T05:33:40+00:00</updated>
    <author>
      <name>/u/fais-1669</name>
      <uri>https://old.reddit.com/user/fais-1669</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx95kh/deep_what_do_you_think/"&gt; &lt;img alt="Deep what do you think?" src="https://preview.redd.it/xn34gdcd9thg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=26b7c44cef48d1ee6d337de27cabf4346beab450" title="Deep what do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fais-1669"&gt; /u/fais-1669 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xn34gdcd9thg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx95kh/deep_what_do_you_think/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx95kh/deep_what_do_you_think/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T05:33:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qxcm5g</id>
    <title>No NVIDIA? No Problem. My 2018 "Potato" 8th Gen i3 hits 10 TPS on 16B MoE.</title>
    <updated>2026-02-06T08:56:17+00:00</updated>
    <author>
      <name>/u/RelativeOperation483</name>
      <uri>https://old.reddit.com/user/RelativeOperation483</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"&gt; &lt;img alt="No NVIDIA? No Problem. My 2018 &amp;quot;Potato&amp;quot; 8th Gen i3 hits 10 TPS on 16B MoE." src="https://b.thumbs.redditmedia.com/FlN9zjU8g_h6h9JM_gRcOC3oluZt1E_e4NDcZO0YLwQ.jpg" title="No NVIDIA? No Problem. My 2018 &amp;quot;Potato&amp;quot; 8th Gen i3 hits 10 TPS on 16B MoE." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm writing this from Burma. Out here, we can‚Äôt all afford the latest NVIDIA 4090s or high-end MacBooks. If you have a tight budget, corporate AI like ChatGPT will try to gatekeep you. If you ask it if you can run a 16B model on an old dual-core i3, it‚Äôll tell you it‚Äôs &amp;quot;impossible.&amp;quot;&lt;/p&gt; &lt;p&gt;I spent a month figuring out how to prove them wrong.&lt;/p&gt; &lt;p&gt;After 30 days of squeezing every drop of performance out of my hardware, I found the peak. I‚Äôm running DeepSeek-Coder-V2-Lite (16B MoE) on an HP ProBook 650 G5 (i3-8145U, 16GB Dual-Channel RAM) at near-human reading speeds.&lt;/p&gt; &lt;p&gt;#### The Battle: CPU vs iGPU&lt;/p&gt; &lt;p&gt;I ran a 20-question head-to-head test with no token limits and real-time streaming.&lt;/p&gt; &lt;p&gt;| Device | Average Speed | Peak Speed | My Rating |&lt;/p&gt; &lt;p&gt;| --- | --- | --- | --- |&lt;/p&gt; &lt;p&gt;| CPU | 8.59 t/s | 9.26 t/s | 8.5/10 - Snappy and solid logic. |&lt;/p&gt; &lt;p&gt;| iGPU (UHD 620) | 8.99 t/s | 9.73 t/s | 9.0/10 - A beast once it warms up. |&lt;/p&gt; &lt;p&gt;The Result: The iGPU (OpenVINO) is the winner, proving that even integrated Intel graphics can handle heavy lifting if you set it up right.&lt;/p&gt; &lt;p&gt;## How I Squeezed the Performance:&lt;/p&gt; &lt;p&gt;* MoE is the &amp;quot;Cheat Code&amp;quot;: 16B parameters sounds huge, but it only calculates 2.4B per token. It‚Äôs faster and smarter than 3B-4B dense models.&lt;/p&gt; &lt;p&gt;* Dual-Channel is Mandatory: I‚Äôm running 16GB (2x8GB). If you have single-channel, don't even bother; your bandwidth will choke.&lt;/p&gt; &lt;p&gt;* Linux is King: I did this on Ubuntu. Windows background processes are a luxury my &amp;quot;potato&amp;quot; can't afford.&lt;/p&gt; &lt;p&gt;* OpenVINO Integration: Don't use OpenVINO alone‚Äîit's dependency hell. Use it as a backend for llama-cpp-python.&lt;/p&gt; &lt;p&gt;## The Reality Check&lt;/p&gt; &lt;ol&gt; &lt;li&gt;First-Run Lag: The iGPU takes time to compile. It might look stuck. Give it a minute‚Äîthe &amp;quot;GPU&amp;quot; is just having his coffee.&lt;/li&gt; &lt;li&gt;Language Drift: On iGPU, it sometimes slips into Chinese tokens, but the logic never breaks.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I‚Äôm sharing this because you shouldn't let a lack of money stop you from learning AI. If I can do this on an i3 in Burma, you can do it too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RelativeOperation483"&gt; /u/RelativeOperation483 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qxcm5g"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qxcm5g/no_nvidia_no_problem_my_2018_potato_8th_gen_i3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T08:56:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx9u62</id>
    <title>Report claims Nvidia will not be releasing any new RTX gaming GPUs in 2026, RTX 60 series likely debuting in 2028</title>
    <updated>2026-02-06T06:10:08+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx9u62/report_claims_nvidia_will_not_be_releasing_any/"&gt; &lt;img alt="Report claims Nvidia will not be releasing any new RTX gaming GPUs in 2026, RTX 60 series likely debuting in 2028" src="https://external-preview.redd.it/Vhe0E1hknQdCzPfipp4a4FDDTKpsoaiucv4xmdoIbE4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=05bef7b7b638f0b1de9b82e717df9072f9485b20" title="Report claims Nvidia will not be releasing any new RTX gaming GPUs in 2026, RTX 60 series likely debuting in 2028" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/report-claims-nvidia-will-not-be-releasing-any-new-rtx-gaming-gpus-in-2026-rtx-60-series-likely-debuting-in-2028"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx9u62/report_claims_nvidia_will_not_be_releasing_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx9u62/report_claims_nvidia_will_not_be_releasing_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T06:10:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qx77xm</id>
    <title>I am absolutely loving qwen3-235b</title>
    <updated>2026-02-06T03:55:56+00:00</updated>
    <author>
      <name>/u/TwistedDiesel53</name>
      <uri>https://old.reddit.com/user/TwistedDiesel53</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I installed qwen3-235b on my desktop system, and I had to join here to brag about it. It's such a careful model, the accuracy of it's output is unbelievable and I've found myself using it absolutely constantly to the point my chatgpt pro subscription is getting left behind. The ability to get carefully curated information of this quality from your own desktop PC is astounding to me and for my use puts all the commercial subscriptions to shame. Sorry for the rant lol!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TwistedDiesel53"&gt; /u/TwistedDiesel53 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qx77xm/i_am_absolutely_loving_qwen3235b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-06T03:55:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qwxtf8</id>
    <title>BalatroBench - Benchmark LLMs' strategic performance in Balatro</title>
    <updated>2026-02-05T21:12:37+00:00</updated>
    <author>
      <name>/u/S1M0N38</name>
      <uri>https://old.reddit.com/user/S1M0N38</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/"&gt; &lt;img alt="BalatroBench - Benchmark LLMs' strategic performance in Balatro" src="https://preview.redd.it/we7y7tzvrqhg1.png?width=140&amp;amp;height=111&amp;amp;auto=webp&amp;amp;s=388833c8320341635311505c2b2e13565a687f71" title="BalatroBench - Benchmark LLMs' strategic performance in Balatro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;If you own a copy of Balatro, you can make your local LLM play it.&lt;/p&gt; &lt;p&gt;I built tools to let LLMs play Balatro autonomously. The LLM gets the game state as text, decides what to do (play, discard, buy from shop...), and the action executes in the actual game. No hard-coded heuristics ‚Äî all decisions come from the LLM.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/coder/balatrobot"&gt;BalatroBot&lt;/a&gt; is a mod that exposes an HTTP API for game state and controls. &lt;a href="https://github.com/coder/balatrollm"&gt;BalatroLLM&lt;/a&gt; is the bot framework ‚Äî it works with any OpenAI-compatible endpoint (Ollama, vLLM, etc.).&lt;/p&gt; &lt;p&gt;You can write your own &lt;strong&gt;strategy&lt;/strong&gt; (Jinja2 templates that define how game state is prompted and what the LLM's decision philosophy should be). Different strategies lead to very different results with the same model.&lt;/p&gt; &lt;p&gt;Benchmark results across various models (including open-weight ones) are on &lt;a href="https://balatrobench.com/"&gt;BalatroBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Resources: - &lt;a href="https://github.com/coder/balatrobot"&gt;BalatroBot&lt;/a&gt;: Balatro mod with HTTP API - &lt;a href="https://github.com/coder/balatrollm"&gt;BalatroLLM&lt;/a&gt;: Bot framework ‚Äî create strategies, plug in your model - &lt;a href="https://balatrobench.com/"&gt;BalatroBench&lt;/a&gt;: Leaderboard and results (&lt;a href="https://github.com/coder/balatrobench"&gt;source&lt;/a&gt;) - &lt;a href="https://discord.gg/SBaRyVDmFg"&gt;Discord&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PS:&lt;/strong&gt; You can watch an LLM struggling to play Balatro live on &lt;a href="https://www.twitch.tv/S1M0N38"&gt;Twitch&lt;/a&gt; - rn Opus 4.6 is playing&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/S1M0N38"&gt; /u/S1M0N38 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qwxtf8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qwxtf8/balatrobench_benchmark_llms_strategic_performance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-05T21:12:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
