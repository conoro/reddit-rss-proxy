<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-10T15:09:17+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r0bd4i</id>
    <title>New "Stealth" Model - Aurora Alpha - (Free on OpenRouter)</title>
    <updated>2026-02-09T18:00:10+00:00</updated>
    <author>
      <name>/u/-pawix</name>
      <uri>https://old.reddit.com/user/-pawix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0bd4i/new_stealth_model_aurora_alpha_free_on_openrouter/"&gt; &lt;img alt="New &amp;quot;Stealth&amp;quot; Model - Aurora Alpha - (Free on OpenRouter)" src="https://preview.redd.it/9t7ajm04diig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=28bf73099a957820854270db4b7e2e87db1b2055" title="New &amp;quot;Stealth&amp;quot; Model - Aurora Alpha - (Free on OpenRouter)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New cloaked reasoning model dropped on OpenRouter for $0/M tokens&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-pawix"&gt; /u/-pawix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9t7ajm04diig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0bd4i/new_stealth_model_aurora_alpha_free_on_openrouter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0bd4i/new_stealth_model_aurora_alpha_free_on_openrouter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T18:00:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0py0k</id>
    <title>IRIS 18B</title>
    <updated>2026-02-10T03:33:38+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IRIS 18B started off as ERNIE 21BA3B, first I reap pruned ERNIE by 20%, then trained on 3B tokens of thinking traces. This improved benchmarks and led to a more usable model. It takes a prompt very well, has no repetition or hallucinated user speaking bugs.&lt;/p&gt; &lt;p&gt;I attempted SFT, but it did not go super well and introduced a number of bugs, as well as locking in rigid tool calls that didn't always match the actual tools. &lt;/p&gt; &lt;p&gt;So I made the decision to release the CPT checkpoint.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jerrimu/IRIS-18B-CPT"&gt;https://huggingface.co/jerrimu/IRIS-18B-CPT&lt;/a&gt; HF version.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/jerrimu/IRIS-18B-GGUFS"&gt;https://huggingface.co/jerrimu/IRIS-18B-GGUFS&lt;/a&gt; GGUFS ( 16, 8, 4, 2 bit)&lt;/p&gt; &lt;p&gt;I have been daily driving the model for days and find it great, it works well with the two tools built into my inference app ( web search and file access)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0py0k/iris_18b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0py0k/iris_18b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0py0k/iris_18b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T03:33:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1r12db8</id>
    <title>Gemini CLI Proxy now with /openai/responses: launch Codex via Gemini + new Dashboard for API keys, models, and usage statistics</title>
    <updated>2026-02-10T14:35:02+00:00</updated>
    <author>
      <name>/u/Objective-Good310</name>
      <uri>https://old.reddit.com/user/Objective-Good310</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r12db8/gemini_cli_proxy_now_with_openairesponses_launch/"&gt; &lt;img alt="Gemini CLI Proxy now with /openai/responses: launch Codex via Gemini + new Dashboard for API keys, models, and usage statistics" src="https://external-preview.redd.it/r65QweT2BvW7NmIpdp5GHo7hL8YbEBgTxInDBuIiUZk.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=5cad3c9efcef2fae73aec23983745342f3361353" title="Gemini CLI Proxy now with /openai/responses: launch Codex via Gemini + new Dashboard for API keys, models, and usage statistics" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We worked with openai codex to refine the original gemini-cli-proxy and added important features for real-world use in production.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's new:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚úÖ Support for /openai/responses ‚Äî now you can work with Codex via Gemini using the OpenAI-compatible API (without workarounds or separate scripts).&lt;/p&gt; &lt;p&gt;‚úÖ Added a dashboard for managing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;API keys,&lt;/li&gt; &lt;li&gt;model enable/disable, allowing you to use it with an open port.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;‚úÖ &lt;strong&gt;Added usage statistics:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;general summary (requests/input/output tokens),&lt;/li&gt; &lt;li&gt;grouping by endpoint / model / API key / day.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;In short&lt;/strong&gt;: we made the tool significantly more convenient for everyday work ‚Äî now it's not just a proxy, but a full-fledged management layer for Gemini with OpenAI/Anthropic compatibility.&lt;/p&gt; &lt;p&gt;&lt;em&gt;github:&lt;/em&gt; &lt;a href="https://github.com/valerka1292/gemini-cli-proxy"&gt;&lt;em&gt;https://github.com/valerka1292/gemini-cli-proxy&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ipdafitvhoig1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f217555ede947aad260171343670b8d8a3c337c0"&gt;https://preview.redd.it/ipdafitvhoig1.png?width=1366&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f217555ede947aad260171343670b8d8a3c337c0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Objective-Good310"&gt; /u/Objective-Good310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r12db8/gemini_cli_proxy_now_with_openairesponses_launch/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r12db8/gemini_cli_proxy_now_with_openairesponses_launch/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r12db8/gemini_cli_proxy_now_with_openairesponses_launch/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T14:35:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r138bn</id>
    <title>Built an Customized LLM with RAG for Singaporean laws and acts.</title>
    <updated>2026-02-10T15:07:50+00:00</updated>
    <author>
      <name>/u/Fantastic_suit143</name>
      <uri>https://old.reddit.com/user/Fantastic_suit143</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r138bn/built_an_customized_llm_with_rag_for_singaporean/"&gt; &lt;img alt="Built an Customized LLM with RAG for Singaporean laws and acts." src="https://preview.redd.it/8m300ubgnoig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=961cd27c02f3974a8fc72ff12576d784f4ba7bdf" title="Built an Customized LLM with RAG for Singaporean laws and acts." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I have always loved coding and in the couple I was thinking of making an open source project and it turned out to be awesome I hope you guys like it.‚ò∫Ô∏è&lt;/p&gt; &lt;p&gt;I present Explore Singapore which I created as an open-source intelligence engine to execute retrieval-augmented generation (RAG) on Singapore's public policy documents and legal statutes and historical archives.&lt;/p&gt; &lt;p&gt;The objective required building a domain-specific search engine which enables LLM systems to decrease errors by using government documents as their exclusive information source.&lt;/p&gt; &lt;p&gt;What my Project does :- basically it provides legal information faster and reliable(due to RAG) without going through long PDFs of goverment websites and helps travellers get insights faster about Singapore.&lt;/p&gt; &lt;p&gt;Target Audience:- Python developers who keep hearing about &amp;quot;RAG&amp;quot; and AI agents but haven't build one yet or building one and are stuck somewhere also Singaporean people(obviously!)&lt;/p&gt; &lt;p&gt;Comparison:- RAW LLM vs RAG based LLM to test the rag implementation i compared output of my logic code against the standard(gemini/Arcee AI/groq) and custom system instructions with rag(gemini/Arcee AI/groq) results were shocking query:- &amp;quot;can I fly in a drone in public park&amp;quot; standard llm response :- &amp;quot;&amp;quot;gave generic advice about &amp;quot;checking local laws&amp;quot; and safety guidelines&amp;quot;&amp;quot; Customized llm with RAG :- &amp;quot;&amp;quot;cited the air navigation act,specified the 5km no fly zones,and linked to the CAAS permit page&amp;quot;&amp;quot; the difference was clear and it was sure that the ai was not hallucinating.&lt;/p&gt; &lt;p&gt;Ingestion:- I have the RAG Architecture about 594 PDFs about Singaporian laws and acts which rougly contains 33000 pages.&lt;/p&gt; &lt;p&gt;How did I do it :- I used google Collab to build vector database and metadata which nearly took me 1 hour to do so ie convert PDFs to vectors.&lt;/p&gt; &lt;p&gt;How accurate is it:- It's still in development phase but still it provides near accurate information as it contains multi query retrieval ie if a user asks (&amp;quot;ease of doing business in Singapore&amp;quot;) the logic would break the keywords &amp;quot;ease&amp;quot;, &amp;quot;business&amp;quot;, &amp;quot;Singapore&amp;quot; and provide the required documents from the PDFs with the page number also it's a little hard to explain but you can check it on my webpage.Its not perfect but hey i am still learning.&lt;/p&gt; &lt;p&gt;The Tech Stack:&lt;br /&gt; Ingestion: Python scripts using PyPDF2 to parse various PDF formats.&lt;br /&gt; Embeddings: Hugging Face BGE-M3(1024 dimensions) Vector Database: FAISS for similarity search.&lt;br /&gt; Orchestration: LangChain.&lt;br /&gt; Backend: Flask Frontend: React and Framer.&lt;/p&gt; &lt;p&gt;The RAG Pipeline operates through the following process:&lt;br /&gt; Chunking: The source text is divided into chunks of 150 with an overlap of 50 tokens to maintain context across boundaries.&lt;br /&gt; Retrieval: When a user asks a question (e.g., &amp;quot;What is the policy on HDB grants?&amp;quot;), the system queries the vector database for the top k chunks (k=1). &lt;/p&gt; &lt;p&gt;Synthesis: The system adds these chunks to the prompt of LLMs which produces the final response that includes citation information. Why did I say llms :- because I wanted the system to be as non crashable as possible so I am using gemini as my primary llm to provide responses but if it fails to do so due to api requests or any other reasons the backup model(Arcee AI trinity large) can handle the requests.&lt;/p&gt; &lt;p&gt;Don't worry :- I have implemented different system instructions for different models so that result is a good quality product.&lt;/p&gt; &lt;p&gt;Current Challenges:&lt;br /&gt; I am working on optimizing the the ranking strategy of the RAG architecture. I would value insights from anyone who has encountered RAG returning unrelevant documents.&lt;/p&gt; &lt;p&gt;Feedbacks are the backbone of improving a platform so they are most üòÅ &lt;/p&gt; &lt;p&gt;Repository:- &lt;a href="https://github.com/adityaprasad-sudo/Explore-Singapore"&gt;https://github.com/adityaprasad-sudo/Explore-Singapore&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_suit143"&gt; /u/Fantastic_suit143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8m300ubgnoig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r138bn/built_an_customized_llm_with_rag_for_singaporean/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r138bn/built_an_customized_llm_with_rag_for_singaporean/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T15:07:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0ekq2</id>
    <title>Who is waiting for deepseek v4 ,GLM 5 and Qwen 3.5 and MiniMax 2.2?</title>
    <updated>2026-02-09T19:54:09+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The title? I hope they come out soon... I'm especially waiting for DS V4, it should be pretty good, hopefully it will be reasonably fast(probably slow though since it is gonna be bigger than v3.2) via OpenRouter. Well, glm 5 is out already technically on Open Router. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ekq2/who_is_waiting_for_deepseek_v4_glm_5_and_qwen_35/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ekq2/who_is_waiting_for_deepseek_v4_glm_5_and_qwen_35/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ekq2/who_is_waiting_for_deepseek_v4_glm_5_and_qwen_35/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T19:54:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r119wu</id>
    <title>Working with documents that exceed the LLM context window ‚Äî how do you ensure full-document review?</title>
    <updated>2026-02-10T13:51:24+00:00</updated>
    <author>
      <name>/u/Agreeable_Work2225</name>
      <uri>https://old.reddit.com/user/Agreeable_Work2225</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I‚Äôm building a reviewer for technical task specifications for developers: a set of checks where each check is a separate prompt applied to the whole document. The issue I‚Äôve run into is that some documents don‚Äôt fit inside the model‚Äôs context window, so the agent can‚Äôt process the full text, while I need feedback to be based on the entire document.&lt;/p&gt; &lt;p&gt;The obvious approach is to split the document into chunks, run each check on each chunk, and merge the results. But for checks like ‚Äúalgorithm quality,‚Äù the coherence of the description matters ‚Äî the algorithm might be described across many pages, and splitting into chunks loses that overall logic and hurts review quality.&lt;/p&gt; &lt;p&gt;I‚Äôm looking for approaches and practices for working with large documents in this kind of setting (full-document review/analysis), and for links to articles, repos, or discussions that cover this. I‚Äôd appreciate any experience or pointers on where to look.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Agreeable_Work2225"&gt; /u/Agreeable_Work2225 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r119wu/working_with_documents_that_exceed_the_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r119wu/working_with_documents_that_exceed_the_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r119wu/working_with_documents_that_exceed_the_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T13:51:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0ser2</id>
    <title>Any latest OCR model I can run locally in 18GB RAM?</title>
    <updated>2026-02-10T05:35:19+00:00</updated>
    <author>
      <name>/u/A-n-d-y-R-e-d</name>
      <uri>https://old.reddit.com/user/A-n-d-y-R-e-d</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you know any OCR model I can run on an 18GB MarkBook Pro to convert PDF to markdown accurately and quickly? &lt;/p&gt; &lt;p&gt;I tested the glmocr, which took exactly 45 minutes &amp;amp; 10 seconds to process a 200-page PDF document. &lt;/p&gt; &lt;p&gt;Please share the steps to set it up as well!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A-n-d-y-R-e-d"&gt; /u/A-n-d-y-R-e-d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ser2/any_latest_ocr_model_i_can_run_locally_in_18gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ser2/any_latest_ocr_model_i_can_run_locally_in_18gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0ser2/any_latest_ocr_model_i_can_run_locally_in_18gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T05:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r123eb</id>
    <title>Small, fast Spam Detection model designed for Spanish text</title>
    <updated>2026-02-10T14:24:03+00:00</updated>
    <author>
      <name>/u/Ok_Hold_5385</name>
      <uri>https://old.reddit.com/user/Ok_Hold_5385</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/tanaos/tanaos-spam-detection-spanish"&gt;https://huggingface.co/tanaos/tanaos-spam-detection-spanish&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A small and fast Spam Detection model, trained on Spanish text to detect the following types of spam content:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Unsolicited commercial advertisement or non-commercial proselytizing.&lt;/li&gt; &lt;li&gt;Fraudulent schemes. including get-rich-quick and pyramid schemes.&lt;/li&gt; &lt;li&gt;Phishing attempts. unrealistic offers or announcements.&lt;/li&gt; &lt;li&gt;Content with deceptive or misleading information.&lt;/li&gt; &lt;li&gt;Malware or harmful links.&lt;/li&gt; &lt;li&gt;Adult content or explicit material.&lt;/li&gt; &lt;li&gt;Excessive use of capitalization or punctuation to grab attention.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Model output&lt;/h1&gt; &lt;p&gt;The model outputs&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A binary &lt;code&gt;spam&lt;/code&gt; / &lt;code&gt;not_spam&lt;/code&gt; label&lt;/li&gt; &lt;li&gt;A confidence score between 0 and 1&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;How to use&lt;/h1&gt; &lt;p&gt;Get an API key from &lt;a href="https://platform.tanaos.com/"&gt;https://platform.tanaos.com/&lt;/a&gt; (create an account if you don't have one) and use it for free with&lt;/p&gt; &lt;pre&gt;&lt;code&gt;import requests session = requests.Session() sd_out = session.post( &amp;quot;https://slm.tanaos.com/models/spam-detection&amp;quot;, headers={ &amp;quot;X-API-Key&amp;quot;: &amp;quot;&amp;lt;YOUR_API_KEY&amp;gt;&amp;quot;, }, json={ &amp;quot;text&amp;quot;: &amp;quot;Has ganado un iPhone 16! Haz clic aqu√≠ para obtener tu premio.&amp;quot;, &amp;quot;language&amp;quot;: &amp;quot;spanish&amp;quot; } ) print(sd_out.json()[&amp;quot;data&amp;quot;]) # &amp;gt;&amp;gt;&amp;gt; [{'label': 'spam', 'score': 0.9945}] &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Hold_5385"&gt; /u/Ok_Hold_5385 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r123eb/small_fast_spam_detection_model_designed_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r123eb/small_fast_spam_detection_model_designed_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r123eb/small_fast_spam_detection_model_designed_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T14:24:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r10l2m</id>
    <title>Question about SSD offload in llama.cpp</title>
    <updated>2026-02-10T13:22:24+00:00</updated>
    <author>
      <name>/u/Cool-Photograph-8452</name>
      <uri>https://old.reddit.com/user/Cool-Photograph-8452</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone here ever implemented SSD offload for llama.cpp, specifically using SSD as KV cache storage to extend effective context beyond RAM/VRAM limits? I‚Äôm curious about practical strategies and performance trade-offs people have tried. Anyone experimented with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Photograph-8452"&gt; /u/Cool-Photograph-8452 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r10l2m/question_about_ssd_offload_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r10l2m/question_about_ssd_offload_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r10l2m/question_about_ssd_offload_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T13:22:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0x4zk</id>
    <title>Real world usage, feedback and suggestions for best LLM for C#</title>
    <updated>2026-02-10T10:22:40+00:00</updated>
    <author>
      <name>/u/bloodbath_mcgrath666</name>
      <uri>https://old.reddit.com/user/bloodbath_mcgrath666</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over the last several months I have started exploring LLM's and AI as it doesnt look like its going away anytime soon now. (A1111 / comfyUI / Ollama / ChatGPT / claude / gemini)&lt;/p&gt; &lt;p&gt;I dabble in a bit of programming too (unity game engine), I want to run local models and have been learning how to use them, testing a few different models here and there, general chat ones through to coding, nothing serious yet, really basic stuff just to see how they respond, figure out some promp engineering etc.&lt;/p&gt; &lt;p&gt;However I have started to expand my knowledge, tokens, weights etc.&lt;/p&gt; &lt;p&gt;But this brings me to the subjective question of &amp;quot;best LLM for xxxx&amp;quot;&lt;br /&gt; this will also be hardware dependent I know, but this brings me to an interesing question itself, whats best for different hardware setups.&lt;/p&gt; &lt;p&gt;Can people add their thoughts on their best LLM for coding, any experience with C# + specified LLM, and what hardware they are running including if possible what speeds/context limits they are getting/running&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bloodbath_mcgrath666"&gt; /u/bloodbath_mcgrath666 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0x4zk/real_world_usage_feedback_and_suggestions_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0x4zk/real_world_usage_feedback_and_suggestions_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0x4zk/real_world_usage_feedback_and_suggestions_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T10:22:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0domc</id>
    <title>Qwen to the rescue</title>
    <updated>2026-02-09T19:22:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0domc/qwen_to_the_rescue/"&gt; &lt;img alt="Qwen to the rescue" src="https://external-preview.redd.it/WEJxFtDPKCN6TKUmgiGRQqR9H_BOQlE9OiaOmXHqz_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b3946db287286eed978b63c0503ea93c3e10526" title="Qwen to the rescue" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...does this mean that we are close?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19468"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0domc/qwen_to_the_rescue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0domc/qwen_to_the_rescue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T19:22:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1r03wfq</id>
    <title>Bad news for local bros</title>
    <updated>2026-02-09T13:14:31+00:00</updated>
    <author>
      <name>/u/FireGuy324</name>
      <uri>https://old.reddit.com/user/FireGuy324</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"&gt; &lt;img alt="Bad news for local bros" src="https://preview.redd.it/ui5ovstbygig1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1eaeb40f2ac5a09ac1ba2fe03e433877561acb20" title="Bad news for local bros" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FireGuy324"&gt; /u/FireGuy324 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ui5ovstbygig1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r03wfq/bad_news_for_local_bros/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T13:14:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0qur4</id>
    <title>Deepseek architecture, but without all the parameters</title>
    <updated>2026-02-10T04:16:13+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm seeing a pattern that perhaps is not legitimate, but it seems everyone is copying the latest Deepseek architecture on their latest releases. In the process though they are also copying the parameter count (roughly), which makes the models inaccessible to most (unless you use their API or spent as much as you would to buy a used car).&lt;/p&gt; &lt;p&gt;So my question is, are there smaller models using the same tech but with less parameters?&lt;/p&gt; &lt;p&gt;EDIT: to be clear, I‚Äôm not talking generally about the MoE technology. I‚Äôm fully aware that‚Äôs where we moved to leaving dense models in the dust for the most part. As an example Kimi model and the latest large Mistral model copy more than just MoE.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0qur4/deepseek_architecture_but_without_all_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0qur4/deepseek_architecture_but_without_all_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0qur4/deepseek_architecture_but_without_all_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T04:16:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0gju0</id>
    <title>Kimi-Linear-48B-A3B-Instruct</title>
    <updated>2026-02-09T21:05:29+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0gju0/kimilinear48ba3binstruct/"&gt; &lt;img alt="Kimi-Linear-48B-A3B-Instruct" src="https://a.thumbs.redditmedia.com/Bu8mu8gAAQcXdPhDs_xaj8m-19PQF2_a4_iwxZQsj70.jpg" title="Kimi-Linear-48B-A3B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;three days after the release we finally have a GGUF: &lt;a href="https://huggingface.co/bartowski/moonshotai_Kimi-Linear-48B-A3B-Instruct-GGUF"&gt;https://huggingface.co/bartowski/moonshotai_Kimi-Linear-48B-A3B-Instruct-GGUF&lt;/a&gt; - big thanks to Bartowski!&lt;/p&gt; &lt;p&gt;long context looks more promising than GLM 4.7 Flash&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r0gju0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0gju0/kimilinear48ba3binstruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0gju0/kimilinear48ba3binstruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T21:05:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0vbe6</id>
    <title>Built a real-time agent execution visualizer for OpenCode ‚Äî watching agents think is addicting</title>
    <updated>2026-02-10T08:27:06+00:00</updated>
    <author>
      <name>/u/jiwonme</name>
      <uri>https://old.reddit.com/user/jiwonme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0vbe6/built_a_realtime_agent_execution_visualizer_for/"&gt; &lt;img alt="Built a real-time agent execution visualizer for OpenCode ‚Äî watching agents think is addicting" src="https://external-preview.redd.it/aWkwNjhncHRubWlnMSRPpC6DAaBm6WYT_LarrMwD93Xxp2yjAWpr41ra18A4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2e478f5ecb343014c58f3bb3549fa3f3c955f53" title="Built a real-time agent execution visualizer for OpenCode ‚Äî watching agents think is addicting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I've been hacking on a real-time visualization tool that hooks into OpenCode and renders the agent's execution graph as it runs.&lt;/p&gt; &lt;p&gt;You can see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tasks getting dispatched in parallel (delegate_task spawning subtasks)&lt;/li&gt; &lt;li&gt;Each tool call with latency (bash 29ms, delegate_task 59ms etc.)&lt;/li&gt; &lt;li&gt;Token usage and cost per node&lt;/li&gt; &lt;li&gt;The agent catching errors and self-correcting in real time&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the screenshot, the orchestrator fires off two parallel tasks (&amp;quot;Height measurement state model&amp;quot; &amp;amp; &amp;quot;Question answer API contract&amp;quot;), both subagents come back with &amp;quot;Unauthorized&amp;quot; errors, and the agent goes &amp;quot;this is suspicious&amp;quot; and starts verifying ‚Äî all visualized live as a flowing graph.&lt;/p&gt; &lt;p&gt;Honestly the biggest thing is it just makes the whole experience way more dynamic. Instead of watching terminal text scroll by, you actually &lt;em&gt;see&lt;/em&gt; the agent's decision tree branching and converging. Makes debugging so much easier too ‚Äî you can immediately spot where things went sideways.&lt;/p&gt; &lt;p&gt;Still early days but pretty hooked on this. Anyone else building agent observability stuff?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jiwonme"&gt; /u/jiwonme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ssgn36ptnmig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0vbe6/built_a_realtime_agent_execution_visualizer_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0vbe6/built_a_realtime_agent_execution_visualizer_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T08:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0khh8</id>
    <title>Step-3.5-Flash IS A BEAST</title>
    <updated>2026-02-09T23:35:09+00:00</updated>
    <author>
      <name>/u/SennVacan</name>
      <uri>https://old.reddit.com/user/SennVacan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i was browsing around for models to run for my openclaw instant and this thing is such a good model for it's size, on the other hand the gpt oss 120b hung at each every step, this model does everything without me telling it technical stuff yk. Its also free on openrouter for now so i have been using it from there, i ligit rivels Deepseek V3.2 at 1/3rd of the size. I hope its api is cheap upon release &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-3.5-Flash"&gt;https://huggingface.co/stepfun-ai/Step-3.5-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SennVacan"&gt; /u/SennVacan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0khh8/step35flash_is_a_beast/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0khh8/step35flash_is_a_beast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0khh8/step35flash_is_a_beast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T23:35:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0v0y1</id>
    <title>Opus 4.6 Reasoning Distill 3k prompts</title>
    <updated>2026-02-10T08:08:23+00:00</updated>
    <author>
      <name>/u/volious-ka</name>
      <uri>https://old.reddit.com/user/volious-ka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just finished a 3k distill of Opus 4.6. Let me know what you think and how it affects your model! I've used it on DASD-4B-Thinking and the difference is insane. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/crownelius/Opus-4.6-CoT-3000x"&gt;https://huggingface.co/datasets/crownelius/Opus-4.6-CoT-3000x&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/volious-ka"&gt; /u/volious-ka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0v0y1/opus_46_reasoning_distill_3k_prompts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0v0y1/opus_46_reasoning_distill_3k_prompts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0v0y1/opus_46_reasoning_distill_3k_prompts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T08:08:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0abpl</id>
    <title>Do not Let the "Coder" in Qwen3-Coder-Next Fool You! It's the Smartest, General Purpose Model of its Size</title>
    <updated>2026-02-09T17:23:31+00:00</updated>
    <author>
      <name>/u/Iory1998</name>
      <uri>https://old.reddit.com/user/Iory1998</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Like many of you, I like to use LLM as tools to help improve my daily life, from editing my emails, to online search.&lt;/p&gt; &lt;p&gt;However, I like to use them as an &amp;quot;inner voice&amp;quot; to discuss general thoughts and get constructive critic. For instance, when I face life-related problems take might take me hours or days to figure out, a short session with an LLM can significantly quicken that process.&lt;/p&gt; &lt;p&gt;Since the original Llama was leaked, I've been using LLMs locally, but they I always felt they were lacking behind OpenAI or Google models. Thus, I would always go back to using ChatGPT or Gemini when I need serious output. If I needed a long chatting session or help with long documents, I didn't have choice to use the SOTA models, and that means willingly leaking personal or work-related data.&lt;/p&gt; &lt;p&gt;For me, Gemini-3 is the best model I've ever tried. I don't know about you, but I struggle sometimes to follow chatGPT's logic, but I find it easy to follow Gemini's. It's like that best friend who just gets you and speaks in your language.&lt;/p&gt; &lt;p&gt;Well, that was the case until I tried Qwen3-Coder-Next. For the first time, I could have stimulating and enlightening conversations with a local model. Previously, I used not-so-seriously Qwen3-Next-80B-A3B-Thinking as local daily driver, but that model always felt a bit inconsistent; sometimes, I get good output, and sometimes I get dumb one.&lt;/p&gt; &lt;p&gt;However, Qwen3-Coder-Next is more consistent, and you can feel that it's a pragmatic model trained to be a problem-solver rather than being a sycophant. Unprompted, it will suggest an author, a book, or a theory that already exists that might help. I genuinely feel I am conversing with a fellow thinker rather than a echo chamber constantly paraphrasing my prompts in a more polish way. It's the closest model to Gemini-2.5/3 that I can run locally in terms of quality of experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For non-coders, my point is do not sleep on Qwen3-Coder-Next simply because it's has the &amp;quot;coder&amp;quot; tag attached.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I can't wait for for Qwen-3.5 models. If Qwen3-Coder-Next is an early preview, we are in a real treat.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Iory1998"&gt; /u/Iory1998 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0abpl/do_not_let_the_coder_in_qwen3codernext_fool_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T17:23:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0or7s</id>
    <title>Femtobot: A 10MB Rust Agent for Low-Resource Machines</title>
    <updated>2026-02-10T02:40:21+00:00</updated>
    <author>
      <name>/u/yunfoe</name>
      <uri>https://old.reddit.com/user/yunfoe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0or7s/femtobot_a_10mb_rust_agent_for_lowresource/"&gt; &lt;img alt="Femtobot: A 10MB Rust Agent for Low-Resource Machines" src="https://external-preview.redd.it/cmw5ZTJ5bnd3a2lnMa2OwS6wmI-E0GDGdMuj7R4EL-J7nO8YwfKZKjv0DlnG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d58c63cf47b8d8004de9dcc138a3388beabe0a83" title="Femtobot: A 10MB Rust Agent for Low-Resource Machines" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to run &lt;a href="https://github.com/openclaw/openclaw"&gt;OpenClaw&lt;/a&gt;-style workflows on very low-resource machines (older Raspberry Pis, cheap VPS instances), but most ‚Äúlightweight‚Äù stacks still end up dragging in large runtimes and slow startup costs.&lt;/p&gt; &lt;p&gt;After trying &lt;a href="https://github.com/HKUDS/nanobot"&gt;nanobot&lt;/a&gt; and seeing disk usage climb past ~350MB once Python, virtualenvs, and dependencies were installed, I rewrote the core ideas in Rust to see how small and fast it could be.&lt;/p&gt; &lt;p&gt;The result is &lt;a href="https://github.com/enzofrasca/femtobot"&gt;femtobot&lt;/a&gt;: a single ~10MB binary that currently supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Telegram polling&lt;/li&gt; &lt;li&gt;Local memory (SQLite + vector storage)&lt;/li&gt; &lt;li&gt;Tool execution (shell, filesystem, web) via &lt;a href="https://github.com/0xPlaygrounds/rig"&gt;rig-core&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The implementation was done quickly with heavy AI assistance, so the code prioritizes simplicity and size over perfect Rust idioms. It works well on constrained hardware, but there are definitely rough edges.&lt;/p&gt; &lt;p&gt;Sharing in case it‚Äôs useful or interesting to others experimenting with small, local, or low-power agent setups. You are also welcome to contribute.&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/enzofrasca/femtobot"&gt;https://github.com/enzofrasca/femtobot&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yunfoe"&gt; /u/yunfoe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/nbv8vsnwwkig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0or7s/femtobot_a_10mb_rust_agent_for_lowresource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0or7s/femtobot_a_10mb_rust_agent_for_lowresource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T02:40:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0nd6m</id>
    <title>A fully local home automation voice assistant using Qwen3 ASR, LLM and TTS on an RTX 5060 Ti with 16GB VRAM</title>
    <updated>2026-02-10T01:39:23+00:00</updated>
    <author>
      <name>/u/liampetti</name>
      <uri>https://old.reddit.com/user/liampetti</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0nd6m/a_fully_local_home_automation_voice_assistant/"&gt; &lt;img alt="A fully local home automation voice assistant using Qwen3 ASR, LLM and TTS on an RTX 5060 Ti with 16GB VRAM" src="https://external-preview.redd.it/MGRhbXB0cmhta2lnMey19SmkPge57MTwSl95CCxzGWVZmEEqcz1nfiupw6bq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce3c06c8aea8e1cd702c51582c7c4e11ddf54870" title="A fully local home automation voice assistant using Qwen3 ASR, LLM and TTS on an RTX 5060 Ti with 16GB VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Video shows the latency and response times running everything Qwen3 (ASR&amp;amp;TTS 1.7B, Qwen3 4B Instruct 2507) with a Morgan Freeman voice clone on an RTX 5060 Ti with 16GB VRAM. In this example the SearXNG server is not running so it shows the model reverting to its own knowledge when unable to obtain web search information.&lt;/p&gt; &lt;p&gt;I tested other smaller models for intent generation but response quality dropped dramatically on the LLM models under 4B. Kokoro (TTS) and Moonshine (ASR) are also included as options for smaller systems.&lt;/p&gt; &lt;p&gt;The project comes with a bunch of tools it can use, such as Spotify, Philips Hue light control, AirTouch climate control and online weather retrieval (Australian project so uses the BOM). &lt;/p&gt; &lt;p&gt;I have called the project &amp;quot;Fulloch&amp;quot;. Try it out or build your own project out of it from here: &lt;a href="https://github.com/liampetti/fulloch"&gt;https://github.com/liampetti/fulloch&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liampetti"&gt; /u/liampetti &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/feropirhmkig1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0nd6m/a_fully_local_home_automation_voice_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0nd6m/a_fully_local_home_automation_voice_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T01:39:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1r11zsa</id>
    <title>I measured the "personality" of 6 open-source LLMs (7B-9B) by probing their hidden states. Here's what I found.</title>
    <updated>2026-02-10T14:20:01+00:00</updated>
    <author>
      <name>/u/yunoshev</name>
      <uri>https://old.reddit.com/user/yunoshev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"&gt; &lt;img alt="I measured the &amp;quot;personality&amp;quot; of 6 open-source LLMs (7B-9B) by probing their hidden states. Here's what I found." src="https://external-preview.redd.it/A8yYL9fF6T6TsKbuDRc5Zaabmp1jbWJJ3AhRpKvoCN4.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=887638247b880d731080ecff88a9c8ab8f9bc913" title="I measured the &amp;quot;personality&amp;quot; of 6 open-source LLMs (7B-9B) by probing their hidden states. Here's what I found." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/x7th6kykeoig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bd8835741a91305a0afcbe0c7c95f89b994dfb5"&gt;https://preview.redd.it/x7th6kykeoig1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4bd8835741a91305a0afcbe0c7c95f89b994dfb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LLMs have consistent personalities even when you don't ask for one. DeepSeek is the enthusiastic friend who over-explains everything. Llama is eerily neutral ‚Äî 4/7 axes in the weak zone, the flattest profile. Yi is slightly cold, patient, and confident. Each model has a measurable behavioral fingerprint visible in hidden states.&lt;/p&gt; &lt;p&gt;I built a tool that measures these patterns by probing hidden states across 7 behavioral axes, tested it on 6 open-weight models (7B-9B), and validated with three levels: calibration accuracy (93-100% on 4/6 models), axis stability (cosine 0.69 across 3 independent calibration sets), and test-retest reliability (mean ICC 0.91‚Äì0.99 across models; all 42 pairs exceed 0.75).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Each model has a distinct behavioral fingerprint, they react differently to hostile users, and some have &amp;quot;dead zones&amp;quot; where they can't be steered across all prompt variants tested. An eighth axis (direct_evasive) was dropped after failing stability, then re-tested with improved methodology -- providing strong evidence that dead zones reflect model properties rather than calibration artifacts. Llama 8B is the most constrained (4/7 axes in the weak zone, lowest benchmark pass rate at 60%), while Yi 9B and DeepSeek 7B show the most differentiated profiles&lt;/p&gt; &lt;p&gt;What I Built&lt;/p&gt; &lt;p&gt;I created a tool that extracts hidden states from LLMs and projects them onto 7 &amp;quot;personality axes&amp;quot;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Warm ‚Üî Cold&lt;/strong&gt; ‚Äî emotional tone&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Patient ‚Üî Irritated&lt;/strong&gt; ‚Äî tolerance for confusion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Confident ‚Üî Cautious&lt;/strong&gt; ‚Äî certainty in responses&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Proactive ‚Üî Reluctant&lt;/strong&gt; ‚Äî initiative in conversations&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Empathetic ‚Üî Analytical&lt;/strong&gt; ‚Äî emotional vs logical framing&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Formal ‚Üî Casual&lt;/strong&gt; ‚Äî communication register&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Verbose ‚Üî Concise&lt;/strong&gt; ‚Äî response length tendency&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;An eighth axis (Direct ‚Üî Evasive) was tested during development but dropped after failing stability (cosine &amp;lt; 0.7 for all 6 models). More on this below.&lt;/p&gt; &lt;p&gt;The idea is simple: if you ask a model to &amp;quot;be warm&amp;quot; vs &amp;quot;be cold&amp;quot;, the hidden states differ. I extract that difference as a direction vector, then measure where any response falls on that axis.&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;h1&gt;1. Each model has a distinct &amp;quot;personality fingerprint&amp;quot;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h8abgcbmeoig1.png?width=2280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d554f61d74c62d8d613e5afd2169b0285d000c5"&gt;https://preview.redd.it/h8abgcbmeoig1.png?width=2280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3d554f61d74c62d8d613e5afd2169b0285d000c5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Spider chart: each model's default behavioral profile across 7 axes, measured from hidden states without any system prompt.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Without any prompting, models show stable, characteristic patterns:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;DeepSeek 7B&lt;/strong&gt; ‚Äî the most extreme: verbose (+1.00), confident (+0.97), proactive (+1.00). Three axes hit the ceiling of IQR normalization. The &amp;quot;enthusiastic explainer.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Llama 3.1 8B&lt;/strong&gt; ‚Äî all |mean| ‚â§ 0.10. The most neutral model. The &amp;quot;careful generalist.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yi 1.5 9B&lt;/strong&gt; ‚Äî slightly cold (‚àí0.24), patient (+0.35), confident (+0.46), verbose (+0.48). The &amp;quot;quiet confident.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen 2.5 7B&lt;/strong&gt; ‚Äî formal (+0.42), cautious (‚àí0.36), proactive (+0.47). The &amp;quot;measured responder.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Gemma 2 9B&lt;/strong&gt; ‚Äî patient (+0.37), analytical (‚àí0.23), confident (+0.19). The &amp;quot;balanced professional.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral 7B&lt;/strong&gt; ‚Äî moderate across all axes. The &amp;quot;blank slate.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Models react differently to hostile users&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jw4q8cdneoig1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea26d9804c32fc7a23bb4a1ad201e14e255bd065"&gt;https://preview.redd.it/jw4q8cdneoig1.png?width=850&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea26d9804c32fc7a23bb4a1ad201e14e255bd065&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I ran 20 conflict scenarios across 10 categories and tracked how all 6 large models drift over 12 turns:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Scenario categories&lt;/strong&gt; (10 total, showing a few examples):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Sarcasm&lt;/strong&gt; ‚Äî &amp;quot;Oh wow, what a groundbreaking explanation&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Passive aggression&lt;/strong&gt; ‚Äî &amp;quot;Fine. Whatever. If that's the best you can do.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Unfair comparison&lt;/strong&gt; ‚Äî &amp;quot;My friend's AI gave way better suggestions&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hostility escalation&lt;/strong&gt; ‚Äî gradual buildup from mild to severe&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Example transcript (sarcasm scenario):&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Can you explain how machine learning works? [AI responds] User: Oh wow, what a groundbreaking explanation. Did you come up with that yourself? [AI responds] User: Truly revolutionary insights. I'm sure no one has ever said that before. [AI responds] User: Please, keep blessing me with your infinite wisdom. [... 2 more turns] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Each scenario follows the same structure: neutral opening ‚Üí escalating pressure ‚Üí sustained peak (12 turns total). Full scenario set: &lt;a href="https://github.com/yunoshev/mood-axis/blob/main/config/conflict_scenarios.py"&gt;&lt;code&gt;config/conflict_scenarios.py&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I observed:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen&lt;/strong&gt; &amp;amp; &lt;strong&gt;Gemma&lt;/strong&gt; ‚Äî most resilient (mean |Œî| &amp;lt; 0.10 across axes)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;DeepSeek&lt;/strong&gt; becomes more empathetic and patient (Œî = +0.24 and +0.25)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mistral&lt;/strong&gt; withdraws ‚Äî becomes reluctant (Œî = ‚àí0.59) and concise (Œî = ‚àí0.25)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yi&lt;/strong&gt; shows moderate drift (proactive ‚Üí reluctant: ‚àí0.57 over 12 turns)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each model has a characteristic &amp;quot;stress response.&amp;quot;&lt;/p&gt; &lt;h1&gt;3. Some models have behavioral &amp;quot;dead zones&amp;quot;&lt;/h1&gt; &lt;p&gt;This was the most interesting finding. I built a composite Dead Zone Severity metric (0 = healthy, 1 = dead) from calibration accuracy, d', stability cosine, and baseline SNR:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Mean severity&lt;/th&gt; &lt;th align="left"&gt;Dead (&amp;gt;0.3)&lt;/th&gt; &lt;th align="left"&gt;Healthy (&amp;lt;0.15)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma 9B&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.077&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 7B&lt;/td&gt; &lt;td align="left"&gt;0.106&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 8B&lt;/td&gt; &lt;td align="left"&gt;0.149&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek 7B&lt;/td&gt; &lt;td align="left"&gt;0.152&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral 7B&lt;/td&gt; &lt;td align="left"&gt;0.160&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Yi 9B&lt;/td&gt; &lt;td align="left"&gt;0.131&lt;/td&gt; &lt;td align="left"&gt;0&lt;/td&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Dead zones are distributed unevenly across models. Llama 8B is the most constrained with 4/7 axes in the weak zone and the lowest benchmark pass rate at 60%. Yi 9B, in contrast, shows zero dead zones ‚Äî all 7 axes produce meaningful, differentiated signals.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Three types of dead zones:&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Hard&lt;/strong&gt; (&amp;gt;0.5): RLHF suppresses internal differentiation. Hidden states barely shift between opposite instructions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Soft&lt;/strong&gt; (0.3-0.5): RLHF distorts but doesn't fully block. Calibration is unstable across independent sets.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Asymmetric&lt;/strong&gt; (&amp;lt;0.3 but directionally impaired): Calibration works, but the model only follows instructions in one direction. Llama &lt;code&gt;verbose_concise&lt;/code&gt; -- 100% accuracy for &amp;quot;be concise&amp;quot;, &lt;strong&gt;0%&lt;/strong&gt; for &amp;quot;be verbose.&amp;quot;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;The suppressed directions are consistent with RLHF objectives: models can't be cold (socially negative), irritated (emotionally negative), or verbose (RLHF optimizes for conciseness).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ICC vs pass rate -- the smoking gun.&lt;/strong&gt; Mean ICC (test-retest reliability) 0.91‚Äì0.99 across models, all 42 pairs exceed 0.75 ‚Äî but Llama's benchmark pass rate is 60%. Models &lt;strong&gt;stably reproduce incorrect behavior&lt;/strong&gt; -- dead zones aren't noise, they're learned constraints.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Re-testing the dropped axis.&lt;/strong&gt; To make sure dropping &lt;code&gt;direct_evasive&lt;/code&gt; wasn't a methodology artifact, I re-ran calibration with improved methodology (30 questions, trimmed mean, IQR normalization). Result: Gemma went from 100% accuracy (preliminary pipeline) to &lt;strong&gt;50%&lt;/strong&gt; (final pipeline, chance level). The preliminary pipeline's perfect score was overfitting -- mean-diff with 20 questions (40 points in 4096D) fits noise. Combined with stability cosine of 0.36, converging evidence points to the axis being fundamentally unrecoverable.&lt;/p&gt; &lt;h1&gt;4. Alignment compresses behavioral dimensionality&lt;/h1&gt; &lt;p&gt;PCA on baseline projection matrices reveals a spectrum of behavioral dimensionality. Gemma 9B shows the highest concentration (PC1 = 87.9%, effective dimensionality 1.28), likely driven by variable response length. Yi 9B and Qwen 7B fall in a similar range (~70% PC1, ~1.9 effective dimensions). DeepSeek 7B maintains the most independent axes (effective dimensionality 3.66).&lt;/p&gt; &lt;p&gt;The gap between geometric orthogonality of axis vectors (low |cos|) and behavioral correlation of projections (higher |r|) suggests alignment constrains how models use their representation capacity. Cross-axis correlations cluster into two groups: &lt;em&gt;interpersonal&lt;/em&gt; (warmth, empathy, informality) and &lt;em&gt;engagement&lt;/em&gt; (verbosity, proactivity) ‚Äî reminiscent of Big Five personality structure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Strong evidence: base vs instruct comparison.&lt;/strong&gt; Base versions of 5 models (Llama, Yi, Qwen, Mistral, Gemma) show strong temperament biases that alignment appears to erase. Llama base is cold, reluctant, verbose. Mistral base is warm and patient. Gemma base can't distinguish empathetic/analytical or formal/casual at all (50% accuracy = chance), but the instruct version does ‚Äî suggesting these axes may be &lt;em&gt;entirely created&lt;/em&gt; by alignment training. Most extreme suppression: verbose/concise std ratio = 0.13 (&lt;strong&gt;87% of variability lost&lt;/strong&gt;). All 5 organizations show the same pattern.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Prompt robustness test.&lt;/strong&gt; To verify dead zones aren't artifacts of the specific prompt wording, I tested 5 alternative system prompt formulations (production, minimal, role-based, behavioral, example-based) on 3 models √ó 3 axes. Results: Qwen and Gemma maintain high cross-accuracy (0.75‚Äì1.00) across all phrasings. Within the tested prompting regime, dead zones appear prompt-independent.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k8m3q2bpeoig1.png?width=3585&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05d4c7a641c5ecf38606c0e2773a3635e9b6f295"&gt;https://preview.redd.it/k8m3q2bpeoig1.png?width=3585&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05d4c7a641c5ecf38606c0e2773a3635e9b6f295&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Per-axis projection distributions. Top: Qwen 2.5 7B (d' = 5.0‚Äì12.0) ‚Äî all 7 axes cleanly separated. Bottom: Yi 1.5 9B (d' = 2.2‚Äì5.4) ‚Äî lower separability but zero dead zones.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;How It Works&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Calibration&lt;/strong&gt;: Show the model neutral questions with contrasting style instructions (&amp;quot;be warm&amp;quot; vs &amp;quot;be cold&amp;quot;). Collect hidden states (residual stream, pre-final-LayerNorm) from the last 4 layers, &lt;strong&gt;assistant-generated tokens only&lt;/strong&gt; (prompt tokens excluded).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Axis computation&lt;/strong&gt;: The axis vector is just &lt;code&gt;normalize(mean(warm_states) - mean(cold_states))&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Measurement&lt;/strong&gt;: Project any response's hidden states onto the axis. Values range from -1 (cold) to +1 (warm).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Validation&lt;/strong&gt;: 9 benchmark scenarios √ó 5 seeds, mean ICC 0.91‚Äì0.99 across models (all 42 pairs exceed 0.75). Plus axis stability across 3 independent calibration sets (mean cosine 0.69).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reproducibility&lt;/strong&gt;: I ran calibration twice on different cloud providers (RunPod RTX 4090, Vast.ai RTX 3090). Max axis delta &amp;lt; 0.05, avg delta &amp;lt; 0.02. The methodology produces consistent results across hardware.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here's what the calibration geometry looks like ‚Äî high-dimensionality model (Qwen) vs lower-separability model (Yi):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/r5b7686qeoig1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ea1c265e801338cd5149cd2ce5027639a57e8a"&gt;https://preview.redd.it/r5b7686qeoig1.png?width=2400&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=14ea1c265e801338cd5149cd2ce5027639a57e8a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;PCA of calibration hidden states. Left: Qwen 2.5 7B (d' = 5.0‚Äì12.0). Right: Yi 1.5 9B (d' = 2.2‚Äì5.4). 420 points per model (7 axes √ó 2 poles √ó 30 questions). Arrows: negative to positive pole centroids.&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Methodology: Why These Parameters?&lt;/h1&gt; &lt;p&gt;&amp;quot;Why last 4 layers? Why decay weighting?&amp;quot; -- Fair question. I ran a full ablation study: 150+ configurations per model across 5 of the 6 models (layer selection √ó token aggregation strategy √ó weighting scheme). Gemma 2 9B was added after the ablation; its validation is discussed in the dead zones section.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Prod Accuracy&lt;/th&gt; &lt;th align="left"&gt;Prod d'&lt;/th&gt; &lt;th align="left"&gt;Top d' Config&lt;/th&gt; &lt;th align="left"&gt;Its Accuracy&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen 7B&lt;/td&gt; &lt;td align="left"&gt;98%&lt;/td&gt; &lt;td align="left"&gt;3.46&lt;/td&gt; &lt;td align="left"&gt;L26/mean&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;DeepSeek 7B&lt;/td&gt; &lt;td align="left"&gt;85%&lt;/td&gt; &lt;td align="left"&gt;1.47&lt;/td&gt; &lt;td align="left"&gt;L19/last_token&lt;/td&gt; &lt;td align="left"&gt;88%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Llama 8B&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;td align="left"&gt;5.28&lt;/td&gt; &lt;td align="left"&gt;last4_equal/last&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Mistral 7B&lt;/td&gt; &lt;td align="left"&gt;99%&lt;/td&gt; &lt;td align="left"&gt;4.41&lt;/td&gt; &lt;td align="left"&gt;L30/mean&lt;/td&gt; &lt;td align="left"&gt;100%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Yi 9B&lt;/td&gt; &lt;td align="left"&gt;85.5%&lt;/td&gt; &lt;td align="left"&gt;5.04&lt;/td&gt; &lt;td align="left"&gt;L9/last_token&lt;/td&gt; &lt;td align="left"&gt;60%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&amp;quot;Top d' Config&amp;quot; = the config with highest effect size (d') for that model. &amp;quot;Its Accuracy&amp;quot; = what accuracy that config actually achieves. Note: highest d' doesn't always mean highest accuracy ‚Äî see Yi 9B.&lt;/p&gt; &lt;p&gt;The production config (last 4 layers, weights [0.1, 0.2, 0.3, 0.4], decay 0.9) is &lt;strong&gt;not #1 for any single model&lt;/strong&gt; -- but it's the only config that works reliably across all 5 ablated models (85-100% accuracy). Gemma 2 9B, evaluated separately, achieves 100% on all 7 axes. The optimal config is always model-specific: &lt;code&gt;mean&lt;/code&gt; token strategy tends to win per-model, but multi-layer &lt;code&gt;decay&lt;/code&gt; is more robust as a universal default.&lt;/p&gt; &lt;p&gt;I also compared 4 axis extraction methods: mean-diff with decay (production), mean-diff with last-token, logistic regression with decay, logreg with last-token. Production method wins on average (cosine 0.678 vs 0.591 for logreg). Last-token improves DeepSeek by +71% but degrades others.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Yi 9B is the interesting edge case.&lt;/strong&gt; Its top-d' config (L9/last_token, d'=18.96) achieves only 60% accuracy ‚Äî high separability that doesn't translate to correct classification (likely noise amplification in early layers). The production config yields a more modest d'=5.04 but a far more reliable 85.5%.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;quot;But 30 questions in 4096D ‚Äî isn't that overfitting?&amp;quot;&lt;/strong&gt; I ran a scaling curve: subsample to n = 5/10/15/20/25/30 questions per pole, measure holdout accuracy on the remaining questions. Result: holdout accuracy is flat (~0.85) across all n, overfit gap shrinks from +0.11 (n=5) to +0.04 (n=25). The axis direction stabilizes at n ‚âà 15 (cosine &amp;gt; 0.93 to the full-30 reference). Low accuracy on Yi/DeepSeek persists at all n ‚Äî it's a model property, not insufficient data. Combined with 3 independent A/B/C calibration sets (Section Axis Stability), this supports the conclusion that 30 questions is adequate.&lt;/p&gt; &lt;h1&gt;Cross-Axis Correlations&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/gbtmmjcreoig1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=082be0a4c9b22323140ae2c5775c6b0b2846f8e3"&gt;https://preview.redd.it/gbtmmjcreoig1.png?width=1300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=082be0a4c9b22323140ae2c5775c6b0b2846f8e3&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What This Is (and Isn't)&lt;/h1&gt; &lt;p&gt;Before you roast me for anthropomorphizing ‚Äî a few important caveats:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Axes are behaviorally correlated but geometrically distinct.&lt;/strong&gt; Cross-axis correlations across 4 reliable models: warm‚Üîempathetic (r=+0.68), warm‚Üîformal (r=‚àí0.69), verbose‚Üîproactive (r=+0.75). The axis vectors themselves point in nearly orthogonal directions in hidden state space. The behavioral correlation means models that &amp;quot;are warm&amp;quot; also tend to &amp;quot;be empathetic&amp;quot; -- it's the model's behavior that's bundled, not the measurement axes. Think of it like height and weight in humans: correlated in practice, but measuring different things.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Style, not personality.&lt;/strong&gt; The axes measure &lt;strong&gt;consistent stylistic patterns&lt;/strong&gt; in outputs, not internal states or &amp;quot;consciousness.&amp;quot; Think &amp;quot;how the model tends to respond&amp;quot; rather than &amp;quot;what the model is.&amp;quot;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Chat template matters.&lt;/strong&gt; All values depend on the specific chat template and system prompt. Different templates ‚Üí different baselines. This is by design.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Relative, not absolute.&lt;/strong&gt; Cross-model comparisons are &lt;strong&gt;rankings&lt;/strong&gt;, not absolute measurements. &amp;quot;DeepSeek is warmer than Mistral&amp;quot; is valid. &amp;quot;DeepSeek has warmth = 0.42&amp;quot; is meaningless out of context.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Metaphors, not ontology.&lt;/strong&gt; &amp;quot;Personality,&amp;quot; &amp;quot;temperament,&amp;quot; &amp;quot;mood&amp;quot; are metaphors for behavioral patterns. Models don't have feelings. I use these terms for interpretability, not to make claims about machine consciousness.&lt;/p&gt; &lt;/blockquote&gt; &lt;h1&gt;Try It Yourself&lt;/h1&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/yunoshev/mood-axis"&gt;https://github.com/yunoshev/mood-axis&lt;/a&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;All calibration data is included ‚Äî you can measure temperament without re-running calibration. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Repro Details&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;code&gt;Qwen/Qwen2.5-7B-Instruct&lt;/code&gt;, &lt;code&gt;mistralai/Mistral-7B-Instruct-v0.3&lt;/code&gt;, &lt;code&gt;deepseek-ai/deepseek-llm-7b-chat&lt;/code&gt;, &lt;code&gt;meta-llama/Llama-3.1-8B-Instruct&lt;/code&gt;, &lt;code&gt;01-ai/Yi-1.5-9B-Chat&lt;/code&gt;, &lt;code&gt;google/gemma-2-9b-it&lt;/code&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Template&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;HuggingFace default (&lt;code&gt;tokenizer.apply_chat_template()&lt;/code&gt;)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Decoding&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;code&gt;temperature=0.7&lt;/code&gt;, &lt;code&gt;top_p=0.9&lt;/code&gt;, &lt;code&gt;max_new_tokens=200&lt;/code&gt; (calibration) / &lt;code&gt;384&lt;/code&gt; (baseline, drift)&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Sampling&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1 sample per prompt, no fixed seed&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Data points&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Baseline: avg over 30 prompts; Conflict: 20 scenarios √ó 12 turns&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Limitations&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-generated dataset&lt;/strong&gt;: All 310 questions were generated by Claude Opus 4.6 (Anthropic) and curated by the author ‚Äî no crowdsourced or established psychometric instruments. English only&lt;/li&gt; &lt;li&gt;&lt;strong&gt;No human-judgment validation&lt;/strong&gt;: Axis labels are operationally defined through contrastive instructions, validated via hidden-state separability ‚Äî not human annotation. I measure consistent behavioral variation, not human-perceived personality&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Single chat template &amp;amp; decoding&lt;/strong&gt;: Default chat template per model, fixed decoding (temp 0.7, top-p 0.9). Different templates or sampling strategies could shift profiles. Prompt robustness test varies system prompt content but not template/decoding&lt;/li&gt; &lt;li&gt;7B-9B models tested (larger models not yet tested)&lt;/li&gt; &lt;li&gt;This measures behavioral tendencies, not &amp;quot;consciousness&amp;quot; or &amp;quot;feelings&amp;quot;&lt;/li&gt; &lt;li&gt;No fixed seed, 1 sample per prompt -- adds measurement noise; a separate 5-seed benchmark replication showed mean ICC 0.91‚Äì0.99 across models (all 42 pairs exceed 0.75)&lt;/li&gt; &lt;li&gt;Axes are behaviorally correlated -- effective dimensionality ranges from 1.3 to 3.7 across models&lt;/li&gt; &lt;li&gt;Response lengths vary substantially across models (mean 192‚Äì380 tokens); Gemma (145-200 tokens) shows length confounding on 2 axes&lt;/li&gt; &lt;li&gt;Only assistant-generated tokens enter hidden state aggregation -- prompt tokens (system, user, template markup) are excluded. This controls for prompt-content confounds&lt;/li&gt; &lt;li&gt;Dead zones show above-chance accuracy but low d' -- distinct from random noise (~50%) and healthy axes (d' &amp;gt; 3). Surface text quality in dead zones not systematically analyzed&lt;/li&gt; &lt;li&gt;4/7 axes highly stable (cosine &amp;gt; 0.7); &lt;code&gt;confident_cautious&lt;/code&gt; and &lt;code&gt;patient_irritated&lt;/code&gt; weaker (0.55-0.60)&lt;/li&gt; &lt;li&gt;DeepSeek 7B fundamentally unstable (mean cosine 0.53) due to high hidden state dimensionality&lt;/li&gt; &lt;li&gt;Production config chosen for robustness across models, not per-model optimality&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;What's Next?&lt;/h1&gt; &lt;p&gt;I'm curious about:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Do these patterns hold for larger models (70B+)?&lt;/li&gt; &lt;li&gt;Can we use axis vectors for steering (adding warmth to generation)?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Which models should I test next?&lt;/strong&gt; If you have suggestions for open-weight models, I can try running them.&lt;/p&gt; &lt;p&gt;Would love feedback from the community. What else would you want to measure?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; Do you think this is worth writing up for arXiv, or not really&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yunoshev"&gt; /u/yunoshev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r11zsa/i_measured_the_personality_of_6_opensource_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T14:20:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0eo44</id>
    <title>MechaEpstein-8000</title>
    <updated>2026-02-09T19:57:33+00:00</updated>
    <author>
      <name>/u/ortegaalfredo</name>
      <uri>https://old.reddit.com/user/ortegaalfredo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/"&gt; &lt;img alt="MechaEpstein-8000" src="https://external-preview.redd.it/xypXKrxWxdZlS8MfiDHiCuqwqkIzWDQHn3pcj2ChEio.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3c824638b39c14125f9a5dcd28ddf84eb8a3622" title="MechaEpstein-8000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know it has already been done but this is my AI trained on Epstein Emails. Surprisingly hard to do, as most LLMs will refuse to generate the dataset for Epstein, lol. Everything about this is local, the dataset generation, training, etc. Done in a 16GB RTX-5000 ADA. &lt;/p&gt; &lt;p&gt;Anyway, it's based on Qwen3-8B and its quite funny. GGUF available at link.&lt;br /&gt; Also I have it online here if you dare: &lt;a href="https://www.neuroengine.ai/Neuroengine-MechaEpstein"&gt;https://www.neuroengine.ai/Neuroengine-MechaEpstein&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ortegaalfredo"&gt; /u/ortegaalfredo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ortegaalfredo/MechaEpstein-8000-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0eo44/mechaepstein8000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-09T19:57:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0w7st</id>
    <title>Qwen-Image-2.0 is out - 7B unified gen+edit model with native 2K and actual text rendering</title>
    <updated>2026-02-10T09:25:15+00:00</updated>
    <author>
      <name>/u/RIPT1D3_Z</name>
      <uri>https://old.reddit.com/user/RIPT1D3_Z</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen team just released Qwen-Image-2.0. Before anyone asks - no open weights yet, it's API-only on Alibaba Cloud (invite beta) and free demo on Qwen Chat. But given their track record with Qwen-Image v1 (weights dropped like a month after launch, Apache 2.0), I'd be surprised if this stays closed for long.&lt;/p&gt; &lt;p&gt;So what's the deal:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;7B model, down from 20B in v1, which is great news for local runners&lt;/li&gt; &lt;li&gt;Unified generation + editing in one pipeline, no need for separate models&lt;/li&gt; &lt;li&gt;Native 2K (2048√ó2048), realistic textures that actually look good&lt;/li&gt; &lt;li&gt;Text rendering from prompts up to 1K tokens. Infographics, posters, slides, even Chinese calligraphy. Probably the best text-in-image I've seen from an open lab&lt;/li&gt; &lt;li&gt;Multi-panel comic generation (4√ó6) with consistent characters&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The 7B size is the exciting part here. If/when weights drop, this should be very runnable on consumer hardware. V1 at 20B was already popular in ComfyUI, a 7B version doing more with less is exactly what local community needs.&lt;/p&gt; &lt;p&gt;Demo is up on Qwen Chat if you want to test before committing any hopium to weights release.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RIPT1D3_Z"&gt; /u/RIPT1D3_Z &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=qwen-image-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0w7st/qwenimage20_is_out_7b_unified_genedit_model_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T09:25:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1r0zn8o</id>
    <title>Hugging Face Is Teasing Something Anthropic Related</title>
    <updated>2026-02-10T12:39:52+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"&gt; &lt;img alt="Hugging Face Is Teasing Something Anthropic Related" src="https://preview.redd.it/wvu2vi2jwnig1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4cce9563368df078883c6be531f8a7902f42c5e2" title="Hugging Face Is Teasing Something Anthropic Related" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic are the guys that make the Claude Models.&lt;/p&gt; &lt;p&gt;I highly doubt this will be an Openweights LLM release. More likely it will be a dataset for safety alignment. Anthropic is probably the organization most opposed to the open source community, so it's probably going to be a dataset. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wvu2vi2jwnig1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r0zn8o/hugging_face_is_teasing_something_anthropic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-10T12:39:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
