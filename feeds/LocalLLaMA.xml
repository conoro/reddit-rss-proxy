<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-20T14:46:54+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1r9v4zq</id>
    <title>Worst llama.cpp bugs</title>
    <updated>2026-02-20T13:18:54+00:00</updated>
    <author>
      <name>/u/Equivalent-Belt5489</name>
      <uri>https://old.reddit.com/user/Equivalent-Belt5489</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- Stop signals are not sent or not carried out by the server, meaning if some extension receives the stop signal in the interface, normally it doesnt stop the execution of the model, the model just continues&lt;br /&gt; - Changing the thread is not respected, it might lead to unexpected behavior like mixing up of contexts... When I start the execution on one thread in Cline in VS Code then it reads the context of this issue in the context, when I then change the thread in Roo / Cline it might just add the context of the new thread on top of the old... it continues calculation at lets say 17k where it stopped in the old thread then it fill context from the new thread, but starts at 17k until 40k which is the context of the new thread...&lt;br /&gt; - The prompt cache is not completely deleted when chaing thread, while the speed decreases with more context, when we change the thread, the speed says the same limit, it doesnt gets fast again... so this means the prompt cache is not deleted when changing the thread... this creates a huge mess, we need to stop the server with every thread change to make sure it doesnt mess things up :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Belt5489"&gt; /u/Equivalent-Belt5489 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9v4zq/worst_llamacpp_bugs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9v4zq/worst_llamacpp_bugs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9v4zq/worst_llamacpp_bugs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:18:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vsye</id>
    <title>Nice interactive explanation of Speculative Decoding</title>
    <updated>2026-02-20T13:47:19+00:00</updated>
    <author>
      <name>/u/individual_kex</name>
      <uri>https://old.reddit.com/user/individual_kex</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"&gt; &lt;img alt="Nice interactive explanation of Speculative Decoding" src="https://external-preview.redd.it/EhW4bQWT9WIeRw5amz2pS-lzd3lb6K6qLMCB-e4QXzU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb9aba232415d2dd8db7afab8bd1d38bfcb06a5d" title="Nice interactive explanation of Speculative Decoding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/individual_kex"&gt; /u/individual_kex &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.adaptive-ml.com/post/speculative-decoding-visualized"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vsye/nice_interactive_explanation_of_speculative/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:47:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vv4w</id>
    <title>Pure WebGPU BitNet inference ‚Äî run LLMs in your browser on any GPU, no CUDA</title>
    <updated>2026-02-20T13:49:54+00:00</updated>
    <author>
      <name>/u/Few_Willingness_7382</name>
      <uri>https://old.reddit.com/user/Few_Willingness_7382</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wrote all NN kernels in WGSL from scratch. Runs BitNet models on any GPU through WebGPU ‚Äî no NVIDIA dependency. Works in Chrome and natively via wgpu-native. Looking for feedback!&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/m96-chan/0xBitNet"&gt;https://huggingface.co/spaces/m96-chan/0xBitNet&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Willingness_7382"&gt; /u/Few_Willingness_7382 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vv4w/pure_webgpu_bitnet_inference_run_llms_in_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vv4w/pure_webgpu_bitnet_inference_run_llms_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vv4w/pure_webgpu_bitnet_inference_run_llms_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99wrj</id>
    <title>Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]</title>
    <updated>2026-02-19T20:10:06+00:00</updated>
    <author>
      <name>/u/Disastrous_Theme5906</name>
      <uri>https://old.reddit.com/user/Disastrous_Theme5906</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"&gt; &lt;img alt="Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]" src="https://preview.redd.it/492jsbpjkhkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=265f565b1e36802fcf3f5931428ad7a9cb4cc05a" title="Can GLM-5 Survive 30 Days on FoodTruck Bench? [Full Review]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM 5 was the most requested model since launch. Ran it through the full benchmark ‚Äî wrote a deep dive with a side-by-side vs Sonnet 4.5 and DeepSeek V3.2.&lt;/p&gt; &lt;p&gt;Results: GLM 5 survived 28 of 30 days ‚Äî the closest any bankrupt model has come to finishing. Placed #5 on the leaderboard, between Sonnet 4.5 (survived) and DeepSeek V3.2 (bankrupt Day 22). More revenue than Sonnet ($11,965 vs $10,753), less food waste than both ‚Äî but still went bankrupt from staff costs eating 67% of revenue.&lt;/p&gt; &lt;p&gt;The interesting part is how it failed. The model diagnosed every problem correctly, stored 123 memory entries, and used 82% of available tools. Then ignored its own analysis.&lt;/p&gt; &lt;p&gt;Full case study with day-by-day timeline and verbatim model quotes: &lt;a href="https://foodtruckbench.com/blog/glm-5"&gt;https://foodtruckbench.com/blog/glm-5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Leaderboard updated: &lt;a href="https://foodtruckbench.com"&gt;https://foodtruckbench.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous_Theme5906"&gt; /u/Disastrous_Theme5906 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/492jsbpjkhkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99wrj/can_glm5_survive_30_days_on_foodtruck_bench_full/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:10:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9swgk</id>
    <title>Curious, Would We Get A GLM 5 Flash?</title>
    <updated>2026-02-20T11:28:42+00:00</updated>
    <author>
      <name>/u/Significant_Fig_7581</name>
      <uri>https://old.reddit.com/user/Significant_Fig_7581</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any announcements? Is it under 80B?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Significant_Fig_7581"&gt; /u/Significant_Fig_7581 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9swgk/curious_would_we_get_a_glm_5_flash/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9swgk/curious_would_we_get_a_glm_5_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9swgk/curious_would_we_get_a_glm_5_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T11:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9rboa</id>
    <title>Buying cheap 'no display' gpus from ebay?</title>
    <updated>2026-02-20T09:57:35+00:00</updated>
    <author>
      <name>/u/getpodapp</name>
      <uri>https://old.reddit.com/user/getpodapp</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm finding these RTX 4080/90's for like 200-300GBP on ebay marked as 'no display', clearly theres a risk that they're completely fucked. &lt;/p&gt; &lt;p&gt;If its literally just 'no display' but compute works it seems a stupid easy way of getting a bunch of vRAM on modern GPUs...?&lt;/p&gt; &lt;p&gt;Does anyone experience with this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/getpodapp"&gt; /u/getpodapp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9rboa/buying_cheap_no_display_gpus_from_ebay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T09:57:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9pxxc</id>
    <title>Context Size Frustration</title>
    <updated>2026-02-20T08:31:43+00:00</updated>
    <author>
      <name>/u/Aggressive-Spinach98</name>
      <uri>https://old.reddit.com/user/Aggressive-Spinach98</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi Guys&lt;/p&gt; &lt;p&gt;So this post might be a little bit longer as I got really frustrated with local AI and Context Size in particular. If you check my other posts you might notice that this topic has come up for me from time to time already and I`m once again seeking help. &lt;/p&gt; &lt;p&gt;Tl:dr What method do you use if you want to calculate how much context size you can have with your given hardware for Model X in a safe way?&lt;/p&gt; &lt;p&gt;So my use case is that I want to run an LLM Model locally and I want to get a feel for how much context size I can use on my hardware. &lt;/p&gt; &lt;p&gt;My setup is LM Studio, a RTX 6000 Pro Blackwell as well as 128GB DDR5 Ram. &lt;/p&gt; &lt;p&gt;I already know what tokens are, what context size in general is and where I can find in the model description or config file how much context size it should be able to run in theory. &lt;/p&gt; &lt;p&gt;Now if you search for information about context size you get either a lot of surface level knowledge or really in depth essays that are at the moment to complicated for me, if I`m a 100% honest. So what I did was trying to figure out, atleast roughly, how much context size I could plan with. So I took my Vram, subtracted the &amp;quot;size&amp;quot; of the modell in the chosen quantification level and then trying to calculate how much tokens I can squeeze into the remaining free space while leaving some buffer of an additional 10% for safety. The results of that was a formula like this: &lt;/p&gt; &lt;p&gt;&lt;em&gt;KV per token = 2 √ó num_layers √ó num_kv_heads √ó head_dim √ó bytes&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Were the necessary data comes from the config file of the model in question on huggingface. &lt;/p&gt; &lt;p&gt;The numbers behind the &amp;quot;=&amp;quot; are an example based on the Nevoria Modell:&lt;/p&gt; &lt;p&gt;&lt;em&gt;Number of layers (num_hidden_layers) = 80&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Number of KV heads (num_key_value_heads) = 8&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Head dimension (head_dim) = 128&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Data type for KV cache = Usually BF16 so 2 Bytes per Value&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Two tensors per token ‚Üí Key + Value (should be fixed, except for special structures)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;So to put these numbers into the formula it would look like this: &lt;/p&gt; &lt;p&gt;&lt;em&gt;KV per Token = 2 \&lt;/em&gt; 80 * 8 * 128 * 2*&lt;/p&gt; &lt;p&gt;&lt;em&gt;= 327.680 Bytes per Token&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;~320 KB per Token or 327.68 KB per Token&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Then I continued with: &lt;/p&gt; &lt;p&gt;&lt;em&gt;Available VRAM = Total GPU VRAM - Model Size - Safety Buffer&lt;/em&gt;&lt;/p&gt; &lt;p&gt;so in numbers: &lt;/p&gt; &lt;p&gt;&lt;em&gt;96 GB - 75 GB - 4 GB&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;= 17 GB&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Since I had the free space and the cost per token the last formula was: &lt;/p&gt; &lt;p&gt;&lt;em&gt;MAX Tokens = 17 GB in Bytes / 327.680 Bytes (Not KB)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Conversion = 17 GB \&lt;/em&gt; 1024 (MB) * 1024 (KB) * 1024 (Byte)*&lt;/p&gt; &lt;p&gt;&lt;em&gt;= ~55.706 Token&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Then usually I subtract an additional amount of tokens just to be more safe, so in this example I would go with 50k tokens context size. &lt;/p&gt; &lt;p&gt;This method worked for me and was most of the time save until two days ago when I hit a context problem that would literally crash my PC. While processing and generating an answer my PC would simply turn of, with the white Power LED still glowing. I had to completly restart everything. After some tests, and log files checking it seems that I have no hardware or heat problem but the context was simply to big so I ran out of memory or it caused another problem. &lt;/p&gt; &lt;p&gt;So while investigating I found an article that says, the more context you give the bigger the amount of (v)RAM you need as the requirements grow rapedly and are not linear, which I guess makes my formula redundant? The table goes like this: &lt;/p&gt; &lt;p&gt;4k context: Approximately 2-4 GB of (V)Ram&lt;/p&gt; &lt;p&gt;8k context: Approximately 4-8 GB of (V)Ram&lt;/p&gt; &lt;p&gt;32k context: Approximately 16-24 GB of (V)Ram&lt;/p&gt; &lt;p&gt;128k context: Approximately 64-96 GB of (V)Ram&lt;/p&gt; &lt;p&gt;The article I read also mentioned a lot of tricks or features that reduce these requirements like: Flash Attention, Sparse Attention, Sliding window Attention, Positional Embeddings and KV Cache Optimization. But not stating how much these methods would actually reduce the needed amount of RAM, if it is even possible to calculate that. &lt;/p&gt; &lt;p&gt;So, I once again feel like I`m standing in a forest unable to see the trees. Since I managed to kill my hardware atleast once, most likely because of context size, I`m really interested to get a better feeling for how many context size is safe to set, without just defaulting to 4k or something equally small. &lt;/p&gt; &lt;p&gt;Any help is greatly appreciated&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Spinach98"&gt; /u/Aggressive-Spinach98 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9pxxc/context_size_frustration/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9pxxc/context_size_frustration/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9pxxc/context_size_frustration/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:31:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9w84r</id>
    <title>Persistent Memory Solutions</title>
    <updated>2026-02-20T14:04:58+00:00</updated>
    <author>
      <name>/u/Itchy_Supermarket_43</name>
      <uri>https://old.reddit.com/user/Itchy_Supermarket_43</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;I am building a local first AI agent in my linux system (ubuntu). I am in the phase of implementing a persistent long term memory. I am currently thinking of starting off with creating a local JSON format. What do you suggest?&lt;/p&gt; &lt;p&gt;Thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Itchy_Supermarket_43"&gt; /u/Itchy_Supermarket_43 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9w84r/persistent_memory_solutions/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9w84r/persistent_memory_solutions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9w84r/persistent_memory_solutions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:04:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mcjw</id>
    <title>GPT-OSS-120b on 2X RTX5090</title>
    <updated>2026-02-20T05:02:12+00:00</updated>
    <author>
      <name>/u/Interesting-Ad4922</name>
      <uri>https://old.reddit.com/user/Interesting-Ad4922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"&gt; &lt;img alt="GPT-OSS-120b on 2X RTX5090" src="https://preview.redd.it/atfvw7c10lkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1c2e6695819ff38848b821da3efa31d5b86b8bb9" title="GPT-OSS-120b on 2X RTX5090" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just got GPT-OSS-120b deployed on dual RTX5090 rig. 128k context (Significant CPU offloading ~10t/s) I know it's nothing amazing I'm just a little proud of myself and needed to tell someone! Thanks for lookin!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Ad4922"&gt; /u/Interesting-Ad4922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/atfvw7c10lkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mcjw/gptoss120b_on_2x_rtx5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:02:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wbl3</id>
    <title>ggml / llama.cpp joining Hugging Face ‚Äî implications for local inference?</title>
    <updated>2026-02-20T14:08:56+00:00</updated>
    <author>
      <name>/u/pmv143</name>
      <uri>https://old.reddit.com/user/pmv143</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ggml / llama.cpp joining HF feels like a significant moment for local inference.&lt;/p&gt; &lt;p&gt;On one hand, this could massively accelerate tooling, integration, and long-term support for local AI. On the other, it concentrates even more of the open model stack under one umbrella.&lt;/p&gt; &lt;p&gt;Is this a net win for the community?&lt;/p&gt; &lt;p&gt;What does this mean for alternative runtimes and independent inference stacks?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmv143"&gt; /u/pmv143 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wbl3/ggml_llamacpp_joining_hugging_face_implications/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9mkgj</id>
    <title>PaddleOCR-VL now in llama.cpp</title>
    <updated>2026-02-20T05:13:34+00:00</updated>
    <author>
      <name>/u/PerfectLaw5776</name>
      <uri>https://old.reddit.com/user/PerfectLaw5776</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/releases/tag/b8110"&gt;https://github.com/ggml-org/llama.cpp/releases/tag/b8110&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far this is the best performing open-source multilingual OCR model I've seen, would appreciate if other people can share their findings. It's 0.9b so it shouldn't brick our machines. &lt;a href="https://huggingface.co/octopusmegalopod/some-paddleocr1.5-vl-ggufs"&gt;Some GGUFs&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PerfectLaw5776"&gt; /u/PerfectLaw5776 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9mkgj/paddleocrvl_now_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T05:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9gve8</id>
    <title>I feel left behind. What is special about OpenClaw?</title>
    <updated>2026-02-20T00:44:48+00:00</updated>
    <author>
      <name>/u/Recent_Jellyfish2190</name>
      <uri>https://old.reddit.com/user/Recent_Jellyfish2190</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;While there are tools like Manus ai, It seems like everyone is excited about OpenClaw lately, and I genuinely don‚Äôt fully understand the differentiation. What exactly is the shift here? Is it UX, architecture, control layer, distribution? Not criticizing, just trying to understand what I‚Äôm missing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent_Jellyfish2190"&gt; /u/Recent_Jellyfish2190 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9gve8/i_feel_left_behind_what_is_special_about_openclaw/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:44:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9h3g8</id>
    <title>Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)...</title>
    <updated>2026-02-20T00:54:46+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt; &lt;img alt="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." src="https://preview.redd.it/5ouemzagqjkg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=ae25e9c86a516f23c1f47828293a3fbe972468b8" title="Qwen3 Coder Next 8FP in the process of converting the entire Flutter documentation for 12 hours now with just 3 sentence prompt with 64K max tokens at around 102GB memory (out of 128GB)..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A remarkable LLM -- we really have a winner.&lt;/p&gt; &lt;p&gt;(Most of the models below were NVFP4)&lt;/p&gt; &lt;p&gt;GPT OSS 120B can't do this (though it's a bit outdated now)&lt;br /&gt; GLM 4.7 Flash can't do this&lt;br /&gt; SERA 32B tokens too slow&lt;br /&gt; Devstral 2 Small can't do this&lt;br /&gt; SEED OSS freezes while thinking&lt;br /&gt; Nemotron 3 Nano can't do this &lt;/p&gt; &lt;p&gt;(Unsure if it's Cline (when streaming &amp;lt;think&amp;gt;) or the LLM, but GPT OSS, GLM, Devstral, and Nemotron go on an insanity loop, for thinking, coding, or both)&lt;/p&gt; &lt;p&gt;Markdown isn't exactly coding, but for multi-iteration (because it runs out of context tokens) conversions, it's flawless.&lt;/p&gt; &lt;p&gt;Now I just wish VS Codium + Cline handles all these think boxes (on the right side of the UI) better. It's impossible to scroll even with 32GB RAM.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1r9h3g8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9h3g8/qwen3_coder_next_8fp_in_the_process_of_converting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T00:54:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uu5h</id>
    <title>Qwen3 Coder Next on 8GB VRAM</title>
    <updated>2026-02-20T13:05:21+00:00</updated>
    <author>
      <name>/u/Juan_Valadez</name>
      <uri>https://old.reddit.com/user/Juan_Valadez</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;I have a PC with 64 GB of RAM and an RTX 3060 12 GB, and I'm running Qwen3 Coder Next in MXFP4 with 131,072 context tokens.&lt;/p&gt; &lt;p&gt;I get a sustained speed of around 23 t/s throughout the entire conversation.&lt;/p&gt; &lt;p&gt;I mainly use it for front-end and back-end web development, and it works perfectly.&lt;/p&gt; &lt;p&gt;I've stopped paying for my Claude Max plan ($100 USD per month) to use only Claude Code with the following configuration:&lt;/p&gt; &lt;p&gt;&lt;code&gt;set GGML_CUDA_GRAPH_OPT=1&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-server -m ../GGUF/qwen3-coder-next-mxfp4.gguf -ngl 999 -sm none -mg 0 -t 12 -fa on -cmoe -c 131072 -b 512 -ub 512 -np 1 --jinja --temp 1.0 --top-p 0.95 --top-k 40 --min-p 0.01 --repeat-penalty 1.0 --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--port 8080&lt;/code&gt;&lt;/p&gt; &lt;p&gt;I promise you it works fast enough and with incredible quality to work with complete SaaS applications (I know how to program, obviously, but I'm delegating practically everything to AI).&lt;/p&gt; &lt;p&gt;If you have at least 64 GB of RAM and 8 GB of VRAM, I recommend giving it a try; you won't regret it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Juan_Valadez"&gt; /u/Juan_Valadez &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uu5h/qwen3_coder_next_on_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9fkks</id>
    <title>We will have Gemini 3.1 before Gemma 4...</title>
    <updated>2026-02-19T23:49:53+00:00</updated>
    <author>
      <name>/u/xandep</name>
      <uri>https://old.reddit.com/user/xandep</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt; &lt;img alt="We will have Gemini 3.1 before Gemma 4..." src="https://preview.redd.it/hd5oal2ngjkg1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0c08bae0f338dab67384ce398502fe29f5b06645" title="We will have Gemini 3.1 before Gemma 4..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Appeared on Antigravity...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xandep"&gt; /u/xandep &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/hd5oal2ngjkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9fkks/we_will_have_gemini_31_before_gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T23:49:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9wvg4</id>
    <title>GGML and llama.cpp join HF to ensure the long-term progress of Local AI</title>
    <updated>2026-02-20T14:31:22+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt; &lt;img alt="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" src="https://external-preview.redd.it/tLGg2WMvFn2R5w7Nf2m6oJPphAYJILLSWaWPLPoW8i4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e6bb0cd5000a00c0e28c8ae17203068e5acfb352" title="GGML and llama.cpp join HF to ensure the long-term progress of Local AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;article by Georgi Gerganov, Xuan-Son Nguyen, Aleksander Grygier, Lysandre, Victor Mustar, Julien Chaumond&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/ggml-joins-hf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9wvg4/ggml_and_llamacpp_join_hf_to_ensure_the_longterm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T14:31:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9ours</id>
    <title>Qwen3.5 Plus, GLM 5, Gemini 3.1 Pro, Sonnet 4.6, three new open source agents, and a lot more added to SanityBoard</title>
    <updated>2026-02-20T07:24:18+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link: &lt;a href="https://sanityboard.lr7.dev/"&gt;https://sanityboard.lr7.dev/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Yeah I've been running evals and working on this for over 3 days straight all day to get this all finished. Too tired to do a proper writeup, so I will give some bullet points and a disclaimer.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;27 New eval results added in total&lt;/li&gt; &lt;li&gt;Got our first 4 community submissions, which brings us GPT 5.3 Codex Spark results, and a few Droid + Skills results to show us how big of a difference a suitable skills file can make.&lt;/li&gt; &lt;li&gt;3 New OSS coding agents; kilocode cli, cline cli, and pi*&lt;/li&gt; &lt;li&gt;Some site UI improvements, like date slider filter, being able to expand the filter options window, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Interesting pattern I realized. GPT-codex models do really well cause they like to iterate, a lot. These kinds of evals favor models with this kind of tendency. Claude models don't iterate as much, so they sometimes get edged out in these kinds of evals. In an actual interactive coding scenario, I do believe the claude models are still better. Now if you want to just assign a long running task and forget it, that's where the gpt-codex models shine. They just keep going and going until done, they're good at that.&lt;/p&gt; &lt;p&gt;A somewhat important note, the infra used makes a HUGE difference in scores. I noticed this very early on, back when I used to run a ton of terminal bench evals, and especially when I decided to run it against as many different providers as I could to see which one was the best for Kimi K2 thinking. Even the speed affected scores a lot. My bench is no different in this regard, although I tried my best to work around this by having generous retry limits, and manually vetting every run for infra issues (which probably takes up the majority of my time), and rerunning any evals that looked like they may have suffered infra issues. This however isn't perfect, I am human. The reason I mention this is cause &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; infra is dying. It made it almost impossible to bench against the official api. It was actually more expensive to use than paying standard api rates to claude for opus lol. They ghosted after I asked if I could have credits back for the wasted tokens I never got.. but that's neither here nor there. And also you might see some of the same models but from different providers score differently for infra reasons. Even the date of eval might matter for this, since sometimes providers change, either improving and fixing things, or otherwise. Also worth noting since some runs are older than others, some things might not score as well, being on an older agent version. Hopefully the filter by date slider I added can help with this.&lt;/p&gt; &lt;p&gt;*Pi was a large part of why this took me so much time and reruns. The retry logic had to be changed cause it's the only agent that does not have streaming stdout for some reason, and buffers it all until it's done. It also has 0 iteration whatsoever, it just does everything on one shot and never iterates on it again, leading to very poor scores. No other agents behave like this. These changes introduced bugs, which meant a lot of time spent fixing things and having to rerun things for fair evals. Pi I think is really cool, but since it's headless mode or whatever you want to call it is only a half complete implementation at best, it's almost impossible to get a fair evaluation of it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9ours/qwen35_plus_glm_5_gemini_31_pro_sonnet_46_three/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T07:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9e27i</id>
    <title>Free ASIC Llama 3.1 8B inference at 16,000 tok/s - no, not a joke</title>
    <updated>2026-02-19T22:48:03+00:00</updated>
    <author>
      <name>/u/Easy_Calligrapher790</name>
      <uri>https://old.reddit.com/user/Easy_Calligrapher790</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A fast inference hardware startup, Taalas, has released a free chatbot interface and API endpoint running on their chip. They chose a small model intentionally as proof of concept. Well, it worked out really well, it runs at 16k tps! I know this model is quite limited but there likely exists a group of users who find it sufficient and would benefit from hyper-speed on offer.&lt;/p&gt; &lt;p&gt;Anyways, they are of course moving on to bigger and better models, but are giving free access to their proof-of-concept to people who want it.&lt;/p&gt; &lt;p&gt;More info: &lt;a href="https://taalas.com/the-path-to-ubiquitous-ai/"&gt;https://taalas.com/the-path-to-ubiquitous-ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chatbot demo: &lt;a href="https://chatjimmy.ai/"&gt;https://chatjimmy.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Inference API service: &lt;a href="https://taalas.com/api-request-form"&gt;https://taalas.com/api-request-form&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's worth trying out the chatbot even just for a bit, the speed is really something to experience. Cheers!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Easy_Calligrapher790"&gt; /u/Easy_Calligrapher790 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9e27i/free_asic_llama_31_8b_inference_at_16000_toks_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T22:48:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1r99yda</id>
    <title>Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû</title>
    <updated>2026-02-19T20:11:42+00:00</updated>
    <author>
      <name>/u/CesarOverlorde</name>
      <uri>https://old.reddit.com/user/CesarOverlorde</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt; &lt;img alt="Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû" src="https://preview.redd.it/ogkdei4udikg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8834b06cb1ae3aaa95c27230b622dd640e7d9634" title="Pack it up guys, open weight AI models running offline locally on PCs aren't real. üòû" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CesarOverlorde"&gt; /u/CesarOverlorde &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ogkdei4udikg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r99yda/pack_it_up_guys_open_weight_ai_models_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T20:11:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9tdvr</id>
    <title>Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain</title>
    <updated>2026-02-20T11:54:25+00:00</updated>
    <author>
      <name>/u/aiprod</name>
      <uri>https://old.reddit.com/user/aiprod</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt; &lt;img alt="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" src="https://preview.redd.it/c1z228f22nkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=57a4ecba13b26df8634c1b123271ef9c3a609c4f" title="Kimi K2.5 better than Opus 4.6 on hallucination benchmark in pharmaceutical domain" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know the benchmark is mostly commercial models but Kimi K2.5 was part of it and I was actually surprised how well it did against its commercial counterparts.&lt;/p&gt; &lt;p&gt;The benchmark test 7 recent models for hallucinations on a realistic use case and data from the pharmaceutical domain.&lt;/p&gt; &lt;p&gt;Surprisingly, Opus 4.6 has the highest hallucination rate.&lt;/p&gt; &lt;p&gt;I labeled a good chunk of the data and from my impressions, it just invented clinical protocols or tests that weren‚Äôt in the source data (probably trying to be helpful).&lt;/p&gt; &lt;p&gt;Kimi K2.5 did much better (albeit still not great).&lt;/p&gt; &lt;p&gt;You can read the full benchmark here: &lt;a href="https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma"&gt;https://www.blueguardrails.com/en/blog/placebo-bench-an-llm-hallucination-benchmark-for-pharma&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Dataset is also available on hugging face.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aiprod"&gt; /u/aiprod &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c1z228f22nkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9tdvr/kimi_k25_better_than_opus_46_on_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T11:54:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9vywq</id>
    <title>GGML.AI has got acquired by Huggingface</title>
    <updated>2026-02-20T13:54:26+00:00</updated>
    <author>
      <name>/u/Time_Reaper</name>
      <uri>https://old.reddit.com/user/Time_Reaper</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt; &lt;img alt="GGML.AI has got acquired by Huggingface" src="https://external-preview.redd.it/l687iazpdDZhrDlIbQBxf8OTcfiJg6WGdsBpv03NqVo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9e45ab199a5cdbdf8c5eb1968743c094b946e98" title="GGML.AI has got acquired by Huggingface" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Time_Reaper"&gt; /u/Time_Reaper &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/19759"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9vywq/ggmlai_has_got_acquired_by_huggingface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:54:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9uuc6</id>
    <title>Deepseek and Gemma ??</title>
    <updated>2026-02-20T13:05:36+00:00</updated>
    <author>
      <name>/u/ZeusZCC</name>
      <uri>https://old.reddit.com/user/ZeusZCC</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt; &lt;img alt="Deepseek and Gemma ??" src="https://preview.redd.it/84ph0pirenkg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d2b363b1900aae44bcfc12c0eeb9d8e2caa7d08" title="Deepseek and Gemma ??" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ZeusZCC"&gt; /u/ZeusZCC &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/84ph0pirenkg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9uuc6/deepseek_and_gemma/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T13:05:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1r9qa7l</id>
    <title>Kimi has context window expansion ambitions</title>
    <updated>2026-02-20T08:54:10+00:00</updated>
    <author>
      <name>/u/omarous</name>
      <uri>https://old.reddit.com/user/omarous</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt; &lt;img alt="Kimi has context window expansion ambitions" src="https://preview.redd.it/3cvl2bdh5mkg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e22f6604997ccccf6f6215ae239ab8f8b1dd09c3" title="Kimi has context window expansion ambitions" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/omarous"&gt; /u/omarous &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/3cvl2bdh5mkg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r9qa7l/kimi_has_context_window_expansion_ambitions/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-20T08:54:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1r60qu9</id>
    <title>AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)</title>
    <updated>2026-02-16T05:11:16+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt; &lt;img alt="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" src="https://preview.redd.it/u11uh8jfisjg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc30b2b6ae673f2e940109e2001bb498bd818ad" title="AMA Announcement: StepFun AI, The Opensource Lab Behind Step-3.5-Flash Model (Thursday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Thursday's guests: &lt;strong&gt;The StepFun Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Thursday, Feb. 19th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/u11uh8jfisjg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r60qu9/ama_announcement_stepfun_ai_the_opensource_lab/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-16T05:11:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
