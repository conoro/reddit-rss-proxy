<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-22T08:09:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nn2xu1</id>
    <title>MTEB still best for choosing an embedding model?</title>
    <updated>2025-09-21T20:36:58+00:00</updated>
    <author>
      <name>/u/divide0verfl0w</name>
      <uri>https://old.reddit.com/user/divide0verfl0w</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;Long time reader, first time poster. Love this community. Learned so much, and I hope I can pay forward one day.&lt;/p&gt; &lt;p&gt;But before that :) Is MTEB still the best place for choosing an embedding model for RAG?&lt;/p&gt; &lt;p&gt;And I see an endless list of tasks (not task type e.g. retrieval, reranking, etc.) that I realized I know nothing about. Can anyone point me to an article for understanding what these tasks are?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/divide0verfl0w"&gt; /u/divide0verfl0w &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn2xu1/mteb_still_best_for_choosing_an_embedding_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn2xu1/mteb_still_best_for_choosing_an_embedding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn2xu1/mteb_still_best_for_choosing_an_embedding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T20:36:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmr43i</id>
    <title>Is Qwen3 4B enough?</title>
    <updated>2025-09-21T12:50:03+00:00</updated>
    <author>
      <name>/u/Dreamingmathscience</name>
      <uri>https://old.reddit.com/user/Dreamingmathscience</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run my coding agent locally so I am looking for a appropriate model. &lt;/p&gt; &lt;p&gt;I don't really need tool calling abilities. Instead I want better quality of the generated code. &lt;/p&gt; &lt;p&gt;I am finding 4B to 10B models and if they don't have dramatic code quality diff I prefer the small one. &lt;/p&gt; &lt;p&gt;Is Qwen3 enough for me? Is there any alternative? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dreamingmathscience"&gt; /u/Dreamingmathscience &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmr43i/is_qwen3_4b_enough/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T12:50:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn2nqz</id>
    <title>Getting counter-intuitive results with local KV Cache Quantization Benchmark - am I doing something wrong?</title>
    <updated>2025-09-21T20:26:07+00:00</updated>
    <author>
      <name>/u/Pentium95</name>
      <uri>https://old.reddit.com/user/Pentium95</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been running some benchmarks on KV cache quantization for long-context tasks, and I'm getting results that don't make much sense to me. I'm hoping this community could take a look at my methodology and point out if I'm making any obvious mistakes.&lt;/p&gt; &lt;p&gt;You can find all the details, scripts, and results in my GitHub repo: &lt;a href="https://pento95.github.io/LongContext-KVCacheQuantTypesBench"&gt;https://pento95.github.io/LongContext-KVCacheQuantTypesBench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Goal:&lt;/strong&gt; I wanted to test the impact of all 16 &lt;code&gt;llama.cpp&lt;/code&gt; KV cache quantization combinations on the Qwen3-30B model using a subset of the LongBench-v2 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;code&gt;Qwen3-30B-A3B-Instruct-2507&lt;/code&gt; (Unsloth Q4_K_XL GGUF)&lt;/li&gt; &lt;li&gt;Linux fedora, RTX 3090 Ti (24GB, full GPU offload)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Method:&lt;/strong&gt; I used the &lt;code&gt;llama.cpp&lt;/code&gt; server, running it for each of the 16 &lt;code&gt;cache-type-k&lt;/code&gt; and &lt;code&gt;cache-type-v&lt;/code&gt; combinations. The test uses 131 samples from LongBench-v2 (16k to 51k tokens) and evaluates the model's accuracy on multiple-choice questions. I used a temperature of 0.0 for deterministic output.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Weird Results:&lt;/strong&gt; I was expecting to see a clear trend where higher quantization (like q4_0) would lead to a drop in accuracy compared to the &lt;code&gt;f16&lt;/code&gt; baseline. Instead, I'm seeing the opposite. My best performing combination is &lt;code&gt;k-f16_v-q5_0&lt;/code&gt; with &lt;strong&gt;16.79%&lt;/strong&gt; accuracy, while the &lt;code&gt;f16&lt;/code&gt;-&lt;code&gt;f16&lt;/code&gt; baseline only gets &lt;strong&gt;13.74%&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;It seems counter-intuitive that quantizing the KV cache would &lt;em&gt;improve&lt;/em&gt; performance. I've run the synchronous combinations three times now and the pattern holds.&lt;/p&gt; &lt;p&gt;I'm starting to think my testing methodology is flawed. I've detailed the whole process in the &lt;a href="http://README.md"&gt;&lt;code&gt;README.md&lt;/code&gt;&lt;/a&gt; on the repo. Could you please take a look? I'm probably making a rookie mistake somewhere in the process, either in how I'm running the server, how I'm filtering the dataset, or how I'm extracting the answers.&lt;/p&gt; &lt;p&gt;Any feedback, criticism, or suggestions would be incredibly helpful. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Pentium95"&gt; /u/Pentium95 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn2nqz/getting_counterintuitive_results_with_local_kv/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn2nqz/getting_counterintuitive_results_with_local_kv/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn2nqz/getting_counterintuitive_results_with_local_kv/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T20:26:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn1elw</id>
    <title>Any recommended tools for best PDF extraction to prep data for an LLM?</title>
    <updated>2025-09-21T19:36:55+00:00</updated>
    <author>
      <name>/u/richardanaya</name>
      <uri>https://old.reddit.com/user/richardanaya</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm curious if anyone has any thoughts on tools that do an amazing job at pdf extraction? Thinking in particular about PDFs that have exotic elements like tables, random quote blocks, sidebars, etc.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardanaya"&gt; /u/richardanaya &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1elw/any_recommended_tools_for_best_pdf_extraction_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1elw/any_recommended_tools_for_best_pdf_extraction_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn1elw/any_recommended_tools_for_best_pdf_extraction_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nms85w</id>
    <title>Anyone got an iPhone 17 Pro to test prompt processing? I have an iPhone 16 Pro for comparison.</title>
    <updated>2025-09-21T13:39:12+00:00</updated>
    <author>
      <name>/u/auradragon1</name>
      <uri>https://old.reddit.com/user/auradragon1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nms85w/anyone_got_an_iphone_17_pro_to_test_prompt/"&gt; &lt;img alt="Anyone got an iPhone 17 Pro to test prompt processing? I have an iPhone 16 Pro for comparison." src="https://b.thumbs.redditmedia.com/1_bLYXMQp6ZTc5kwIU_vMMLR36v7_IjEMzxYOYbNOnA.jpg" title="Anyone got an iPhone 17 Pro to test prompt processing? I have an iPhone 16 Pro for comparison." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;p&gt;Download Pocket Pal from iOS app store&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Download and load model Gemma-2-2b-it (Q6_K)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Go to settings and enable Metal. Slide all the way to right. &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Go to Benchmark mode (hamburger menu in top left)&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Post results here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/auradragon1"&gt; /u/auradragon1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nms85w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nms85w/anyone_got_an_iphone_17_pro_to_test_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nms85w/anyone_got_an_iphone_17_pro_to_test_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T13:39:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnfzgx</id>
    <title>Is there any performance / stability difference between Windows and Linux (due to NVIDIA drivers?)</title>
    <updated>2025-09-22T07:25:36+00:00</updated>
    <author>
      <name>/u/zeddyzed</name>
      <uri>https://old.reddit.com/user/zeddyzed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, newbie to AI stuff here, wanting to get started.&lt;/p&gt; &lt;p&gt;It's commonly known by the gaming community that the Linux drivers for NVIDIA aren't as good as we would want. I just wanted to ask whether this has any impact on Local AI stuff? (Which I understand also runs on the GPU.)&lt;/p&gt; &lt;p&gt;I'm dual booting Windows and Linux, so I wanted to know which OS I should install my AI stuff on.&lt;/p&gt; &lt;p&gt;Any advice would be much appreciated, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zeddyzed"&gt; /u/zeddyzed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfzgx/is_there_any_performance_stability_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfzgx/is_there_any_performance_stability_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfzgx/is_there_any_performance_stability_difference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T07:25:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn8csq</id>
    <title>Sophia NLU Engine Upgrade - New and Improved POS Tagger</title>
    <updated>2025-09-22T00:33:14+00:00</updated>
    <author>
      <name>/u/mdizak</name>
      <uri>https://old.reddit.com/user/mdizak</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released large upgrade to Sophia NLU Engine, which includes a new and improved POS tagger along with a revamped automated spelling corrections system. POS tagger now gets 99.03% accuracy across 34 million validation tokens, still blazingly fast at ~20,000 words/sec, plus the size of the vocab data store dropped from 238MB to 142MB for a savings of 96MB which was a nice bonus.&lt;/p&gt; &lt;p&gt;Full details, online demo and source code at: &lt;a href="https://cicero.sh/sophia/"&gt;https://cicero.sh/sophia/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Release announcement at: &lt;a href="https://cicero.sh/r/sophia-upgrade-pos-tagger"&gt;https://cicero.sh/r/sophia-upgrade-pos-tagger&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/cicero/cicero-ai/"&gt;https://github.com/cicero/cicero-ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy! More coming, namely contextual awareness shortly.&lt;/p&gt; &lt;p&gt;Sophia = self hosted, privacy focused NLU (natural language understanding) engine. No external dependencies or API calls to big tech, self contained, blazingly fast, and accurate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mdizak"&gt; /u/mdizak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn8csq/sophia_nlu_engine_upgrade_new_and_improved_pos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn8csq/sophia_nlu_engine_upgrade_new_and_improved_pos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn8csq/sophia_nlu_engine_upgrade_new_and_improved_pos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T00:33:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn7gdv</id>
    <title>Perplexica for Siri</title>
    <updated>2025-09-21T23:50:34+00:00</updated>
    <author>
      <name>/u/No_Information9314</name>
      <uri>https://old.reddit.com/user/No_Information9314</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For users of &lt;a href="https://github.com/ItzCrazyKns/Perplexica"&gt;Perplexica&lt;/a&gt;, the open source AI search tool: &lt;/p&gt; &lt;p&gt;I created this iOS shortcut that leverages the Perplexica api so I could send search queries to my Perplexica instance while in my car. Wanted to share because it's been super useful to have a completely private AI voice search using carplay. Also works with Siri on an iPhone. Enjoy!&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.icloud.com/shortcuts/64b69e50a0144c6799b47947c13505e3"&gt;https://www.icloud.com/shortcuts/64b69e50a0144c6799b47947c13505e3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Information9314"&gt; /u/No_Information9314 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn7gdv/perplexica_for_siri/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn7gdv/perplexica_for_siri/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn7gdv/perplexica_for_siri/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T23:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmg185</id>
    <title>Qwen3Omni</title>
    <updated>2025-09-21T02:08:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt; &lt;img alt="Qwen3Omni" src="https://preview.redd.it/wcxu5ypyefqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5b0e169e57d635253c780f31d6542861df594c98" title="Qwen3Omni" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wcxu5ypyefqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmg185/qwen3omni/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T02:08:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmkswn</id>
    <title>Just dropped: Qwen3-4B Function calling on just 6GB VRAM</title>
    <updated>2025-09-21T06:37:33+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to bring this to you if you are looking for a superior model for toolcalling to use with ollama for local Codex style personal coding assistant on terminal:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex"&gt;https://huggingface.co/Manojb/Qwen3-4B-toolcalling-gguf-codex&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Fine-tuned on 60K function calling examples&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;4B parameters&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;GGUF format&lt;/strong&gt; (optimized for CPU/GPU inference)&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;3.99GB download&lt;/strong&gt; (fits on any modern system)&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Production-ready&lt;/strong&gt; with 0.518 training loss&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;this works with&lt;br /&gt; &lt;a href="https://github.com/ymichael/open-codex/"&gt;https://github.com/ymichael/open-codex/&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/8ankur8/anything-codex"&gt;https://github.com/8ankur8/anything-codex&lt;/a&gt;&lt;br /&gt; &lt;a href="https://github.com/dnakov/anon-codex"&gt;https://github.com/dnakov/anon-codex&lt;/a&gt;&lt;br /&gt; preferable: &lt;a href="https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code"&gt;https://github.com/search?q=repo%3Adnakov%2Fanon-codex%20ollama&amp;amp;type=code&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;Looks like ollama is fragile and can have compatibility issues with system/tokenizer. I have pushed the way I did evals with the model &amp;amp; used with codex: with llamacpp.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Manojb/Qwen3-4b-toolcall-gguf-llamacpp-codex"&gt;https://huggingface.co/Manojb/Qwen3-4b-toolcall-gguf-llamacpp-codex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;it has ample examples. ‚úåÔ∏è&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Update:&lt;/p&gt; &lt;p&gt;If it doesn't work as expected, try running this first but it requires 9-12GB RAM for 4k+ context. If it does work, then please share as there might be something wrong with tokenization. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Manojb/Qwen-7B-toolcalling-ReSearch-gguf-Q8_0-codex"&gt;https://huggingface.co/Manojb/Qwen-7B-toolcalling-ReSearch-gguf-Q8_0-codex&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmkswn/just_dropped_qwen34b_function_calling_on_just_6gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T06:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn0o62</id>
    <title>What GUI/interface do most people here use to run their models?</title>
    <updated>2025-09-21T19:09:10+00:00</updated>
    <author>
      <name>/u/tech4marco</name>
      <uri>https://old.reddit.com/user/tech4marco</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I used to be a big fan of &lt;a href="https://github.com/nomic-ai/gpt4all"&gt;https://github.com/nomic-ai/gpt4all&lt;/a&gt; but all development has stopped, which is a shame as this was quite lightweight and worked pretty well. &lt;/p&gt; &lt;p&gt;What do people here use to run models in GGUF format? &lt;/p&gt; &lt;p&gt;NOTE: I am not really up to date with everything in LLMA's and dont know what the latest bleeding edge model extension is or what must have applications run these things.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tech4marco"&gt; /u/tech4marco &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0o62/what_guiinterface_do_most_people_here_use_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0o62/what_guiinterface_do_most_people_here_use_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0o62/what_guiinterface_do_most_people_here_use_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn7k4h</id>
    <title>Need some advice on building a dedicated LLM server</title>
    <updated>2025-09-21T23:55:35+00:00</updated>
    <author>
      <name>/u/SomeKindOfSorbet</name>
      <uri>https://old.reddit.com/user/SomeKindOfSorbet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My mom wants me to build her a server for her business so she can query some LLMs locally for things that involve confidential/copyrighted data. I'm currently imagining something that can hit 20-30B models like Gemma 3 27B with a decently large context window. I've got a solid idea of what to build, but I'd like some of y'all's opinions and recommendations.&lt;/p&gt; &lt;h1&gt;GPU&lt;/h1&gt; &lt;p&gt;I'm currently looking at the RTX 5090. It's relatively expensive, but my mom insists that she wants &lt;em&gt;the best&lt;/em&gt; out there (within reason obviously, so an RTX PRO 6000 is out of the question lol). However some things about the 5090 concern me, particularly the 12HPWR connector. I'm not really up-to-date on the whole ordeal, but I don't think I'd be comfortable letting a machine running 24/7 in our basement unchecked with this connector.&lt;/p&gt; &lt;p&gt;Maybe it would be worth looking into a 7900XTX? It has 8 GB less VRAM and significantly lower inference speeds, but it's also less than 1/3rd the price, not to mention it won't require as beefy a PSU and as big a case. To me the 7900XTX sounds like the saner option, but I'd like some external input.&lt;/p&gt; &lt;h1&gt;Other components&lt;/h1&gt; &lt;p&gt;Beyond the GPU, I'm not really sure what components I should be looking to get for a dedicated inference host. Case and PSU aside, would it be fine to go with a cheap AM4 system? Or would DDR5 and a PCIe 5.0 x 16 slot make it worth going for an AM5 system?&lt;/p&gt; &lt;p&gt;For storage, I'm thinking it would be nice to have something with relatively high read bandwidth to reduce that waiting time when a model is being loaded into memory. I'm thinking of getting 2 decently fast SSDs and pairing them in a RAID0 configuration. Would that be a good option or should I just get a single, really expensive PCIe 5.0 SSD with really fast read speeds? If I'm going with the RAID0 config, would motherboard RAID0 do the job or should I look at dedicated RAID hardware (or software)?&lt;/p&gt; &lt;h1&gt;Software&lt;/h1&gt; &lt;p&gt;For now, I'm thinking of setting up Open WebUI with either llama.cpp or Ollama. My mom seems to like Open WebUI and it's a solid chatbot wrapper overall, but are there other options that are worth considering? I've only dabbled with local LLMs and don't really know about the alternatives.&lt;/p&gt; &lt;p&gt;I'm also not sure what flavour of Linux I should be using for a headless server, so I'll take any recommendations. Preferably something stable that can play well with Nvidia drivers (if I end up getting a 5090).&lt;/p&gt; &lt;p&gt;Any input is greatly appreciated!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SomeKindOfSorbet"&gt; /u/SomeKindOfSorbet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn7k4h/need_some_advice_on_building_a_dedicated_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn7k4h/need_some_advice_on_building_a_dedicated_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn7k4h/need_some_advice_on_building_a_dedicated_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T23:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nne3ra</id>
    <title>How do I disable thinking in Deepseek V3.1?</title>
    <updated>2025-09-22T05:29:09+00:00</updated>
    <author>
      <name>/u/MengerianMango</name>
      <uri>https://old.reddit.com/user/MengerianMango</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;``` llama-cli -hf unsloth/DeepSeek-V3.1-GGUF:Q5_K_XL \ --jinja --mlock \ --prio 3 -ngl 99 --cpu-moe \&lt;br /&gt; --temp 0.6 --top_p 0.95 --min_p 0.01 --ctx-size $((128*1024)) \ -t 128 -b 10240 \ -p &amp;quot;Tell me about PCA.&amp;quot; --verbose-prompt&lt;/p&gt; &lt;h1&gt;... log output&lt;/h1&gt; &lt;p&gt;main: prompt: '/no&lt;em&gt;think Tell me about PCA.' main: number of tokens in prompt = 12 0 -&amp;gt; '&amp;lt;ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú&amp;gt;' 128803 -&amp;gt; '&amp;lt;ÔΩúUserÔΩú&amp;gt;' 91306 -&amp;gt; '/no' 65 -&amp;gt; '&lt;/em&gt;' 37947 -&amp;gt; 'think' 32536 -&amp;gt; ' Tell' 678 -&amp;gt; ' me' 943 -&amp;gt; ' about' 78896 -&amp;gt; ' PCA' 16 -&amp;gt; '.' 128804 -&amp;gt; '&amp;lt;ÔΩúAssistantÔΩú&amp;gt;' 128798 -&amp;gt; '&amp;lt;think&amp;gt;'&lt;/p&gt; &lt;h1&gt;more log output&lt;/h1&gt; &lt;p&gt;Tell me about PCA.&amp;lt;think&amp;gt;Hmm, the user asked about PCA. They probably want a straightforward, jargon-free explanation without overcomplicating it. Since PCA is a technical topic, I should balance simplicity with accuracy. &lt;/p&gt; &lt;p&gt;I'll start with a high-level intuition‚Äîcomparing it to photo compression‚Äîto make it relatable. Then, I'll break down the core ideas: variance, eigenvectors, and dimensionality reduction, but keep it concise. No need for deep math unless the user asks. &lt;/p&gt; &lt;p&gt;The response should end with a clear summary of pros and cons, since practical use cases matter. Avoid tangents‚Äîstick to what PCA is, why it's useful, and when to use it.&amp;lt;/think&amp;gt;Of course. Here is a straightforward explanation of Principal Component Analysis (PCA).&lt;/p&gt; &lt;h3&gt;The Core Idea in Simple Terms&lt;/h3&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;I've tried /no_think, \no_think, --reasoning-budget 0, etc. None of that seems to work.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MengerianMango"&gt; /u/MengerianMango &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nne3ra/how_do_i_disable_thinking_in_deepseek_v31/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nne3ra/how_do_i_disable_thinking_in_deepseek_v31/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nne3ra/how_do_i_disable_thinking_in_deepseek_v31/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T05:29:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn45cx</id>
    <title>Kokoro-82M-FP16-OpenVINO</title>
    <updated>2025-09-21T21:24:50+00:00</updated>
    <author>
      <name>/u/Echo9Zulu-</name>
      <uri>https://old.reddit.com/user/Echo9Zulu-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Echo9Zulu/Kokoro-82M-FP16-OpenVINO"&gt;https://huggingface.co/Echo9Zulu/Kokoro-82M-FP16-OpenVINO&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I converted this model in prep for &lt;a href="https://github.com/SearchSavior/OpenArc"&gt;OpenArc&lt;/a&gt; 2.0.0. We have support for CPU only inference with Kokoro-82M-FP16-OpenVINO, accessible through /v1/audio/speech openai endpoint. &lt;/p&gt; &lt;p&gt;/v1/audio/transcription was also implemented this weekend, targeting whisper.&lt;/p&gt; &lt;p&gt;Conversion code which created this model was taken from an example Intel provides, linked in the model card. My plan is to apply what I learned working with Kokoro to Kitten-TTS models, then implement in OpenArc as part of a future release. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Echo9Zulu-"&gt; /u/Echo9Zulu- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn45cx/kokoro82mfp16openvino/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn45cx/kokoro82mfp16openvino/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn45cx/kokoro82mfp16openvino/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T21:24:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmii5y</id>
    <title>Magistral 1.2 is incredible. Wife prefers it over Gemini 2.5 Pro.</title>
    <updated>2025-09-21T04:21:00+00:00</updated>
    <author>
      <name>/u/My_Unbiased_Opinion</name>
      <uri>https://old.reddit.com/user/My_Unbiased_Opinion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL:DR - AMAZING general use model. Y'all gotta try it. &lt;/p&gt; &lt;p&gt;Just wanna let y'all know that Magistral is worth trying. Currently running the UD Q3KXL quant from Unsloth on Ollama with Openwebui. &lt;/p&gt; &lt;p&gt;The model is incredible. It doesn't overthink and waste tokens unnecessarily in the reasoning chain. &lt;/p&gt; &lt;p&gt;The responses are focused, concise and to the point. No fluff, just tells you what you need to know. &lt;/p&gt; &lt;p&gt;The censorship is VERY minimal. My wife has been asking it medical-adjacent questions and it always gives you a solid answer. I am an ICU nurse by trade and am studying for advanced practice and can vouch for the advice magistral is giving is legit. &lt;/p&gt; &lt;p&gt;Before this, wife has been using Gemini 2.5 pro and hates the censorship and the way it talks to you like a child (let's break this down, etc). &lt;/p&gt; &lt;p&gt;The general knowledge in Magistral is already really good. Seems to know obscure stuff quite well. &lt;/p&gt; &lt;p&gt;Now, once you hook it up to a web search tool call is where this model I feel like can hit as hard as proprietary LLMs. The model really does wake up even more when hooked up to the web. &lt;/p&gt; &lt;p&gt;Model even supports image input. I have not tried that specifically but I loved image processing from Mistral 3.2 2506 so I expect no issues there.&lt;/p&gt; &lt;p&gt;Currently using with Openwebui with the recommended parameters. If you do use it with OWUI, be sure to set up the reasoning tokens in the model settings so thinking is kept separate from the model response. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/My_Unbiased_Opinion"&gt; /u/My_Unbiased_Opinion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmii5y/magistral_12_is_incredible_wife_prefers_it_over/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T04:21:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnfwmo</id>
    <title>Moving from Cursor to Qwen-code</title>
    <updated>2025-09-22T07:20:39+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Never been faster &amp;amp; happier, I basically live on terminal. Definitely recommend.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnfwmo/moving_from_cursor_to_qwencode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T07:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn18k2</id>
    <title>Predicting the next "attention is all you need"</title>
    <updated>2025-09-21T19:30:36+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;NeurIPS 2025 &lt;a href="https://neurips.cc/Downloads/2025"&gt;accepted papers&lt;/a&gt; are out! If you didn't know, &amp;quot;Attention is all you Need&amp;quot; was published in &lt;a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"&gt;NeurIPS 2017&lt;/a&gt; and spawned the modern wave of Transformer-based large language models; but few would have predicted this back in 2017. Which NeurIPS 2025 paper do you think is the bext &amp;quot;Attention is all you Need&amp;quot;? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://neurips.cc/Downloads/2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn18k2/predicting_the_next_attention_is_all_you_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn18k2/predicting_the_next_attention_is_all_you_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:30:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnefs0</id>
    <title>GLM-4.5V model for local computer use</title>
    <updated>2025-09-22T05:49:19+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"&gt; &lt;img alt="GLM-4.5V model for local computer use" src="https://external-preview.redd.it/MTdjemJhbjlubnFmMbWADQNBSkImjVESNjfi_q43l9ostHKNGAFX_QJdfnS0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e71ff3c4f0395f4af1a821212361417ff7c59ec" title="GLM-4.5V model for local computer use" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;On OSWorld-V, it scores 35.8% - beating UI-TARS-1.5, matching Claude-3.7-Sonnet-20250219, and setting SOTA for fully open-source computer-use models.&lt;/p&gt; &lt;p&gt;Run it with Cua either: Locally via Hugging Face Remotely via OpenRouter&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua"&gt;https://github.com/trycua&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Docs + examples: &lt;a href="https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v"&gt;https://docs.trycua.com/docs/agent-sdk/supported-agents/computer-use-agents#glm-45v&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s7ecb9y9nnqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnefs0/glm45v_model_for_local_computer_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T05:49:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn01bj</id>
    <title>Qwen3-Coder-480B on the M3 Ultra 512GB Mac Studio is perfect for agentic coding</title>
    <updated>2025-09-21T18:45:29+00:00</updated>
    <author>
      <name>/u/ButThatsMyRamSlot</name>
      <uri>https://old.reddit.com/user/ButThatsMyRamSlot</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-480b runs in MLX with 8bit quantization and just barely fits the full 256k context window within 512GB.&lt;/p&gt; &lt;p&gt;With Roo code/cline, Q3C works exceptionally well when working within an existing codebase.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;RAG (with Qwen3-Embed) retrieves API documentation and code samples which eliminates hallucinations.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;The long context length can handle entire source code files for additional details.&lt;/li&gt; &lt;li&gt;Prompt adherence is great, and the subtasks in Roo work very well to gather information without saturating the main context.&lt;/li&gt; &lt;li&gt;VSCode hints are read by Roo and provide feedback about the output code.&lt;/li&gt; &lt;li&gt;Console output is read back to identify compile time and runtime errors.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Green grass is more difficult, Q3C doesn‚Äôt do the best job at architecting a solution given a generic prompt. It‚Äôs much better to explicitly provide a design or at minimum design constraints rather than just ‚Äúimplement X using Y‚Äù.&lt;/p&gt; &lt;p&gt;Prompt processing, especially at full 256k context, can be quite slow. For an agentic workflow, this doesn‚Äôt matter much, since I‚Äôm running it in the background. I find Q3C difficult to use as a coding &lt;em&gt;assistant&lt;/em&gt;, at least the 480b version.&lt;/p&gt; &lt;p&gt;I was on the fence about this machine 6 months ago when I ordered it, but I‚Äôm quite happy with what it can do now. An alternative option I considered was to buy an RTX Pro 6000 for my 256GB threadripper system, but the throughout benefits are far outweighed by the ability to run larger models at higher precision in my use case.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ButThatsMyRamSlot"&gt; /u/ButThatsMyRamSlot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn01bj/qwen3coder480b_on_the_m3_ultra_512gb_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T18:45:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn72ji</id>
    <title>Optimizing gpt-oss-120b local inference speed on consumer hardware</title>
    <updated>2025-09-21T23:32:32+00:00</updated>
    <author>
      <name>/u/carteakey</name>
      <uri>https://old.reddit.com/user/carteakey</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Got GPT‚ÄëOSS‚Äë120B running with llama.cpp on mid‚Äërange hardware ‚Äì i5‚Äë12600K + RTX 4070 (12 GB) + 64 GB DDR5 ‚Äì ‚âà191 tps prompt, ‚âà10 tps generation with a 24k context window.&lt;/li&gt; &lt;li&gt;Distilled &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; tips &amp;amp; community tweaks into an article (run script, benchmarks).&lt;/li&gt; &lt;li&gt;Feedback and further tuning ideas welcome!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;script + step‚Äëby‚Äëstep tuning guide ‚ûú&lt;/em&gt; &lt;a href="https://carteakey.dev/optimizing%20gpt-oss-120b-local%20inference/"&gt;https://carteakey.dev/optimizing%20gpt-oss-120b-local%20inference/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/carteakey"&gt; /u/carteakey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://carteakey.dev/optimizing%20gpt-oss-120b-local%20inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn72ji/optimizing_gptoss120b_local_inference_speed_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn72ji/optimizing_gptoss120b_local_inference_speed_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T23:32:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nmzio1</id>
    <title>LongCat-Flash-Thinking</title>
    <updated>2025-09-21T18:25:42+00:00</updated>
    <author>
      <name>/u/Xhehab_</name>
      <uri>https://old.reddit.com/user/Xhehab_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"&gt; &lt;img alt="LongCat-Flash-Thinking" src="https://preview.redd.it/l7o00pbb9kqf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fb8a419e2a1961bdcf7234e4eb8e33897a2904f" title="LongCat-Flash-Thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üöÄ LongCat-Flash-Thinking: Smarter reasoning, leaner costs!&lt;/p&gt; &lt;p&gt;üèÜ Performance: SOTA open-source models on Logic/Math/Coding/Agent tasks&lt;/p&gt; &lt;p&gt;üìä Efficiency: 64.5% fewer tokens to hit top-tier accuracy on AIME25 with native tool use, agent-friendly&lt;/p&gt; &lt;p&gt;‚öôÔ∏è Infrastructure: Async RL achieves a 3x speedup over Sync frameworks&lt;/p&gt; &lt;p&gt;üîóModel: &lt;a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking"&gt;https://huggingface.co/meituan-longcat/LongCat-Flash-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Try Now: &lt;a href="http://longcat.ai"&gt;longcat.ai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Xhehab_"&gt; /u/Xhehab_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/l7o00pbb9kqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nmzio1/longcatflashthinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T18:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nn0u8p</id>
    <title>Why is Hugging Face blocked in China when so many open‚Äëweight models are released by Chinese companies?</title>
    <updated>2025-09-21T19:15:37+00:00</updated>
    <author>
      <name>/u/zoxtech</name>
      <uri>https://old.reddit.com/user/zoxtech</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently learned that HF is inaccessible from mainland China. At the same time, a large share of the open‚Äëweight LLMs are published by Chinese firms.&lt;/p&gt; &lt;p&gt;Is this a legal prohibition on publishing Chinese models, or simply a network‚Äëlevel block that prevents users inside China from reaching the site?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zoxtech"&gt; /u/zoxtech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nn0u8p/why_is_hugging_face_blocked_in_china_when_so_many/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-21T19:15:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nncyvv</id>
    <title>baidu releases Qianfan-VL 70B/8B/3B</title>
    <updated>2025-09-22T04:23:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-8B"&gt;https://huggingface.co/baidu/Qianfan-VL-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-70B"&gt;https://huggingface.co/baidu/Qianfan-VL-70B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/baidu/Qianfan-VL-3B"&gt;https://huggingface.co/baidu/Qianfan-VL-3B&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Model Description&lt;/h1&gt; &lt;p&gt;Qianfan-VL is a series of general-purpose multimodal large language models enhanced for enterprise-level multimodal applications. The models offer deep optimization for high-frequency scenarios in industrial deployment while maintaining strong general capabilities.&lt;/p&gt; &lt;h1&gt;Model Variants&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Parameters&lt;/th&gt; &lt;th align="left"&gt;Context Length&lt;/th&gt; &lt;th align="left"&gt;CoT Support&lt;/th&gt; &lt;th align="left"&gt;Best For&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;3B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;‚ùå&lt;/td&gt; &lt;td align="left"&gt;Edge deployment, real-time OCR&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-8B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;8B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;Server-side general scenarios, fine-tuning&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qianfan-VL-70B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;70B&lt;/td&gt; &lt;td align="left"&gt;32k&lt;/td&gt; &lt;td align="left"&gt;‚úÖ&lt;/td&gt; &lt;td align="left"&gt;Complex reasoning, data synthesis&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Architecture&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Language Model&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;Qianfan-VL-3B: Based on Qwen2.5-3B&lt;/li&gt; &lt;li&gt;Qianfan-VL-8B/70B: Based on Llama 3.1 architecture&lt;/li&gt; &lt;li&gt;Enhanced with 3T multilingual corpus&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision Encoder&lt;/strong&gt;: InternViT-based, supports dynamic patching up to 4K resolution&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cross-modal Fusion&lt;/strong&gt;: MLP adapter for efficient vision-language bridging&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Key Capabilities&lt;/h1&gt; &lt;h1&gt;üîç OCR &amp;amp; Document Understanding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full-Scenario OCR&lt;/strong&gt;: Handwriting, formulas, natural scenes, cards/documents&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Document Intelligence&lt;/strong&gt;: Layout analysis, table parsing, chart understanding, document Q&amp;amp;A&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High Precision&lt;/strong&gt;: Industry-leading performance on OCR benchmarks&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üßÆ Chain-of-Thought Reasoning (8B &amp;amp; 70B)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Complex chart analysis and reasoning&lt;/li&gt; &lt;li&gt;Mathematical problem-solving with step-by-step derivation&lt;/li&gt; &lt;li&gt;Visual reasoning and logical inference&lt;/li&gt; &lt;li&gt;Statistical computation and trend prediction&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nncyvv/baidu_releases_qianfanvl_70b8b3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T04:23:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nncssq</id>
    <title>Qwen3-Omni Promotional Video</title>
    <updated>2025-09-22T04:14:46+00:00</updated>
    <author>
      <name>/u/Mysterious_Finish543</name>
      <uri>https://old.reddit.com/user/Mysterious_Finish543</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=RRlAen2kIUU"&gt;https://www.youtube.com/watch?v=RRlAen2kIUU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Qwen dropped a promotional video for Qwen3-Omni, looks like the weights are just around the corner!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious_Finish543"&gt; /u/Mysterious_Finish543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nncssq/qwen3omni_promotional_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T04:14:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nnb8sq</id>
    <title>I'll show you mine, if you show me yours: Local AI tech stack September 2025</title>
    <updated>2025-09-22T02:53:16+00:00</updated>
    <author>
      <name>/u/JLeonsarmiento</name>
      <uri>https://old.reddit.com/user/JLeonsarmiento</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"&gt; &lt;img alt="I'll show you mine, if you show me yours: Local AI tech stack September 2025" src="https://preview.redd.it/rq2ple7trmqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0ba6a2e65b89f81d77c39c353119c9e596157a9b" title="I'll show you mine, if you show me yours: Local AI tech stack September 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JLeonsarmiento"&gt; /u/JLeonsarmiento &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq2ple7trmqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nnb8sq/ill_show_you_mine_if_you_show_me_yours_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-22T02:53:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1njjz7j</id>
    <title>Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)</title>
    <updated>2025-09-17T17:44:17+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt; &lt;img alt="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" src="https://preview.redd.it/4xt9enbairpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1cc1ac16a9a2b96134934fcb2f81a9f3d4916b31" title="Our 4th AMA: The LMStudio Team! (Thursday, 11 AM-1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4xt9enbairpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1njjz7j/our_4th_ama_the_lmstudio_team_thursday_11_am1_pm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-17T17:44:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1nkft9l</id>
    <title>AMA with the LM Studio team</title>
    <updated>2025-09-18T18:12:24+00:00</updated>
    <author>
      <name>/u/yags-lms</name>
      <uri>https://old.reddit.com/user/yags-lms</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;! We're excited for this AMA. Thank you for having us here today. We got a full house from the LM Studio team: &lt;/p&gt; &lt;p&gt;- Yags &lt;a href="https://t.co/ERfA4NrR96"&gt;https://reddit.com/user/yags-lms/&lt;/a&gt; (founder)&lt;br /&gt; - Neil &lt;a href="https://t.co/KyiHVfv0QG"&gt;https://reddit.com/user/neilmehta24/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Will &lt;a href="https://t.co/IjAZJL2JMK"&gt;https://reddit.com/user/will-lms/&lt;/a&gt; (LLM engines and runtime)&lt;br /&gt; - Matt &lt;a href="https://t.co/6MNkItPYnI"&gt;https://reddit.com/user/matt-lms/&lt;/a&gt; (LLM engines, runtime, and APIs)&lt;br /&gt; - Ryan &lt;a href="https://t.co/0snuNUPizo"&gt;https://reddit.com/user/ryan-lms/&lt;/a&gt; (Core system and APIs)&lt;br /&gt; - Rugved &lt;a href="https://t.co/xGtYHsJZI3"&gt;https://reddit.com/user/rugved_lms/&lt;/a&gt; (CLI and SDKs)&lt;br /&gt; - Alex &lt;a href="https://t.co/wtT2IFf0z6"&gt;https://reddit.com/user/alex-lms/&lt;/a&gt; (App)&lt;br /&gt; - Julian &lt;a href="https://www.reddit.com/user/julian-lms/"&gt;https://www.reddit.com/user/julian-lms/&lt;/a&gt; (Ops) &lt;/p&gt; &lt;p&gt;Excited to chat about: the latest local models, UX for local models, steering local models effectively, LM Studio SDK and APIs, how we support multiple LLM engines (llama.cpp, MLX, and more), privacy philosophy, why local AI matters, our open source projects (mlx-engine, lms, lmstudio-js, lmstudio-python, venvstacks), why ggerganov and Awni are the GOATs, where is TheBloke, and more. &lt;/p&gt; &lt;p&gt;Would love to hear about people's setup, which models you use, use cases that really work, how you got into local AI, what needs to improve in LM Studio and the ecosystem as a whole, how you use LM Studio, and anything in between!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Everyone: it was awesome to see your questions here today and share replies! Thanks a lot for the welcoming AMA. We will continue to monitor this post for more questions over the next couple of days, but for now we're signing off to continue building üî®&lt;/p&gt; &lt;p&gt;We have several marquee features we've been working on for a loong time coming out later this month that we hope you'll love and find lots of value in. And don't worry, UI for n cpu moe is on the way too :)&lt;/p&gt; &lt;p&gt;Special shoutout and thanks to ggerganov, Awni Hannun, TheBloke, Hugging Face, and all the rest of the open source AI community!&lt;/p&gt; &lt;p&gt;Thank you and see you around! - Team LM Studio üëæ&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yags-lms"&gt; /u/yags-lms &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nkft9l/ama_with_the_lm_studio_team/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-18T18:12:24+00:00</published>
  </entry>
</feed>
