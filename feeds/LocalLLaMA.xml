<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-24T11:22:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1noikw2</id>
    <title>Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board</title>
    <updated>2025-09-23T14:15:42+00:00</updated>
    <author>
      <name>/u/Ok-Actuary-4527</name>
      <uri>https://old.reddit.com/user/Ok-Actuary-4527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"&gt; &lt;img alt="Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board" src="https://a.thumbs.redditmedia.com/pcm1xAY4zY-tgo1Qcr29HPiU2wVkxx0BAdZ2I0j9uf8.jpg" title="Dual Modded 4090 48GBs on a consumer ASUS ProArt Z790 board" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are some curiosities and questions here about the modded 4090 48GB cards. For my local AI test environment, I need a setup with a larger VRAM pool to run some tests, so I got my hands on a dual-card rig with these. I've run some initial benchmarks and wanted to share the data.&lt;/p&gt; &lt;p&gt;The results are as expected, and I think it's a good idea to have these modded 4090 48GB cards.&lt;/p&gt; &lt;h1&gt;Test 1: Single Card GGUF Speed (GPUStack llama-box/llama.cpp)&lt;/h1&gt; &lt;p&gt;Just a simple, raw generation speed test on a single card to see how they compare head-to-head.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-32B (GGUF, Q4_K_M)&lt;/li&gt; &lt;li&gt;Backend: llama-box (llama-box in GPUStack)&lt;/li&gt; &lt;li&gt;Test: Single short prompt request generation via GPUStack UI's compare feature.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modded 4090 48GB: 38.86 t/s&lt;/li&gt; &lt;li&gt;Standard 4090 24GB (ASUS TUF): 39.45 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observation: The standard 24GB card was slightly faster. Not by much, but consistently.&lt;/p&gt; &lt;h1&gt;Test 2: Single Card vLLM Speed&lt;/h1&gt; &lt;p&gt;The same test but with a smaller model on vLLM to see if the pattern held.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-8B (FP16)&lt;/li&gt; &lt;li&gt;Backend: vLLM v0.10.2 in GPUStack (custom backend)&lt;/li&gt; &lt;li&gt;Test: Single short request generation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Modded 4090 48GB: 55.87 t/s&lt;/li&gt; &lt;li&gt;Standard 4090 24GB: 57.27 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Observation: Same story. The 24GB card is again marginally faster in a simple, single-stream inference task. The extra VRAM doesn't translate to more speed for a single request, which is expected, and there might be a tiny performance penalty for the modded memory.&lt;/p&gt; &lt;h1&gt;Test 3: Multi-GPU Stress Test (2x 48GB vs 4x 24GB)&lt;/h1&gt; &lt;p&gt;This is where I compared my dual 48GB rig against a cloud machine with four standard 4090s. Both setups have 96GB of total VRAM running the same large model under a heavy concurrent load.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model: Qwen-32B (FP16)&lt;/li&gt; &lt;li&gt;Backend: vLLM v0.10.2 in GPUStack (custom backend)&lt;/li&gt; &lt;li&gt;Tool: evalscope (100 concurrent users, 400 total requests)&lt;/li&gt; &lt;li&gt;Setup A (Local): 2x Modded 4090 48GB (TP=2) on an ASUS ProArt Z790&lt;/li&gt; &lt;li&gt;Setup B (Cloud): 4x Standard 4090 24GB (TP=4) on a server-grade board&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Results (Cloud 4x24GB was significantly better):&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Metric&lt;/th&gt; &lt;th align="left"&gt;2x 4090 48GB (Our Rig)&lt;/th&gt; &lt;th align="left"&gt;4x 4090 24GB (Cloud)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Output Throughput (tok/s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;1054.1&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;1262.95&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. Latency (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;105.46&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;86.99&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. TTFT (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.4179&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.3947&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Avg. Time Per Output Token (s)&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;0.0844&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;0.0690&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Analysis: The 4-card setup on the server was clearly superior across all metrics—almost 20% higher throughput and significantly lower latency. My initial guess was the motherboard's PCIe topology (PCIE 5.0 x16 PHB on my Z790 vs. a better link on the server, which is also PCIE).&lt;/p&gt; &lt;p&gt;To confirm this, I ran nccl-test to measure the effective inter-GPU bandwidth. The results were clear:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Local 2x48GB Rig:&lt;/strong&gt; Avg bus bandwidth was &lt;strong&gt;~3.0 GB/s&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cloud 4x24GB Rig:&lt;/strong&gt; Avg bus bandwidth was &lt;strong&gt;~3.3 GB/s&lt;/strong&gt;. &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;That ~10% higher bus bandwidth on the server board seems to be the key difference, allowing it to overcome the extra communication overhead of a larger tensor parallel group (TP=4 vs TP=2) and deliver much better performance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Actuary-4527"&gt; /u/Ok-Actuary-4527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1noikw2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noikw2/dual_modded_4090_48gbs_on_a_consumer_asus_proart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T14:15:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1noxalu</id>
    <title>GPT-OSS is insane at leetcode</title>
    <updated>2025-09-23T23:49:44+00:00</updated>
    <author>
      <name>/u/JsThiago5</name>
      <uri>https://old.reddit.com/user/JsThiago5</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noxalu/gptoss_is_insane_at_leetcode/"&gt; &lt;img alt="GPT-OSS is insane at leetcode" src="https://b.thumbs.redditmedia.com/f4nPwd4GWT7wSRT5ReI-2RyDdwmDwoWIeexdVZedDhA.jpg" title="GPT-OSS is insane at leetcode" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've tested several open-source models on this problem—specifically ones that fit within 16GB of VRAM—and none could solve it. Even GPT-4o had some trouble with it previously. I was impressed that this model nailed it on the first attempt, achieving a 100% score for time and space complexity. And, for some reason, GPT-OSS is a lot faster than others models at prompt eval.&lt;/p&gt; &lt;p&gt;Problem:&lt;br /&gt; &lt;a href="https://leetcode.com/problems/maximum-employees-to-be-invited-to-a-meeting/submissions/1780701076/"&gt;https://leetcode.com/problems/maximum-employees-to-be-invited-to-a-meeting/submissions/1780701076/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/c9ixfvgd40rf1.png?width=1034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e3bb1bd3145ca9117ccb2ff0c8883c993fa595a"&gt;https://preview.redd.it/c9ixfvgd40rf1.png?width=1034&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3e3bb1bd3145ca9117ccb2ff0c8883c993fa595a&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JsThiago5"&gt; /u/JsThiago5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noxalu/gptoss_is_insane_at_leetcode/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noxalu/gptoss_is_insane_at_leetcode/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noxalu/gptoss_is_insane_at_leetcode/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T23:49:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nonsvg</id>
    <title>Xet powers 5M models and datasets on Hugging Face</title>
    <updated>2025-09-23T17:32:52+00:00</updated>
    <author>
      <name>/u/clem59480</name>
      <uri>https://old.reddit.com/user/clem59480</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nonsvg/xet_powers_5m_models_and_datasets_on_hugging_face/"&gt; &lt;img alt="Xet powers 5M models and datasets on Hugging Face" src="https://preview.redd.it/8nzs9ffk9yqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=900d49ab7de15060ca933d082069e2b24385301e" title="Xet powers 5M models and datasets on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem59480"&gt; /u/clem59480 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8nzs9ffk9yqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nonsvg/xet_powers_5m_models_and_datasets_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nonsvg/xet_powers_5m_models_and_datasets_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T17:32:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1np8m4r</id>
    <title>retraining the model with a new tokenizer and response format</title>
    <updated>2025-09-24T10:31:58+00:00</updated>
    <author>
      <name>/u/Objective-Good310</name>
      <uri>https://old.reddit.com/user/Objective-Good310</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had an idea to take the qwen model and train it on the gpt oss tokenizer with its chat format, as I prefer it, but gpt oss is too large for local inference on my laptop. Is it possible to retrain qwen on the gpt oss tokenizer and chat format?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Objective-Good310"&gt; /u/Objective-Good310 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8m4r/retraining_the_model_with_a_new_tokenizer_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8m4r/retraining_the_model_with_a_new_tokenizer_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np8m4r/retraining_the_model_with_a_new_tokenizer_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T10:31:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nowsyu</id>
    <title>Intel just released a LLM finetuning app for their ARC GPUs</title>
    <updated>2025-09-23T23:27:39+00:00</updated>
    <author>
      <name>/u/Aggressive-Breath852</name>
      <uri>https://old.reddit.com/user/Aggressive-Breath852</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I discovered that Intel has a LLM finetuning tool on their GitHub repository: &lt;a href="https://github.com/open-edge-platform/edge-ai-tuning-kit"&gt;https://github.com/open-edge-platform/edge-ai-tuning-kit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Breath852"&gt; /u/Aggressive-Breath852 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nowsyu/intel_just_released_a_llm_finetuning_app_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nowsyu/intel_just_released_a_llm_finetuning_app_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nowsyu/intel_just_released_a_llm_finetuning_app_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T23:27:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1noe09l</id>
    <title>2 new open source models from Qwen today</title>
    <updated>2025-09-23T10:44:18+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt; &lt;img alt="2 new open source models from Qwen today" src="https://preview.redd.it/goah9v2r8wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=07a67dbd5f99e7851c1f27295952913b340ead4d" title="2 new open source models from Qwen today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/goah9v2r8wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noe09l/2_new_open_source_models_from_qwen_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1np57u7</id>
    <title>Raspberry Pi 5 + IMX500 AI Camera Risk Monitoring</title>
    <updated>2025-09-24T06:45:40+00:00</updated>
    <author>
      <name>/u/Wraithraisrr</name>
      <uri>https://old.reddit.com/user/Wraithraisrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m planning a capstone project using a &lt;strong&gt;Raspberry Pi 5 (8GB)&lt;/strong&gt; with a &lt;strong&gt;Sony IMX500 AI camera&lt;/strong&gt; to monitor individuals for fall risks and hazards. The camera will run object detection directly on-sensor, while a separate PC will handle a Vision-Language Model (VLM) to interpret events and generate alerts. I want to confirm whether a Pi 5 (8GB) is sufficient to handle the IMX500 and stream only detection metadata to the server, and whether this setup would be better than using a normal Pi camera with an external accelerator like a Hailo-13T or Hailo-26T for this use case. in addition, im also considering which is most cost efficient. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wraithraisrr"&gt; /u/Wraithraisrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np57u7/raspberry_pi_5_imx500_ai_camera_risk_monitoring/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np57u7/raspberry_pi_5_imx500_ai_camera_risk_monitoring/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np57u7/raspberry_pi_5_imx500_ai_camera_risk_monitoring/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T06:45:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1np6qlc</id>
    <title>GitHub - shantur/jarvis-mcp: Bring your AI to life—talk to assistants instantly in your browser. Zero hasle, No API keys, No Whisper</title>
    <updated>2025-09-24T08:28:30+00:00</updated>
    <author>
      <name>/u/Recent-Success-1520</name>
      <uri>https://old.reddit.com/user/Recent-Success-1520</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np6qlc/github_shanturjarvismcp_bring_your_ai_to_lifetalk/"&gt; &lt;img alt="GitHub - shantur/jarvis-mcp: Bring your AI to life—talk to assistants instantly in your browser. Zero hasle, No API keys, No Whisper" src="https://external-preview.redd.it/2V7Z3Ld5qugJkRQYIrVOWknaoGQak8bMDIIrJLFZB6s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9331bf926c23d1fd88fc74e2ac52b7a2f1595d88" title="GitHub - shantur/jarvis-mcp: Bring your AI to life—talk to assistants instantly in your browser. Zero hasle, No API keys, No Whisper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recent-Success-1520"&gt; /u/Recent-Success-1520 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/shantur/jarvis-mcp"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np6qlc/github_shanturjarvismcp_bring_your_ai_to_lifetalk/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np6qlc/github_shanturjarvismcp_bring_your_ai_to_lifetalk/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T08:28:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nozt72</id>
    <title>I built a tribute to Terry Davis's TempleOS using a local LLM. It's a holy DnD campaign where "God" is a random number generator and the DM is a local llama</title>
    <updated>2025-09-24T01:49:22+00:00</updated>
    <author>
      <name>/u/Temporary_Exam_3620</name>
      <uri>https://old.reddit.com/user/Temporary_Exam_3620</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been haunted for years by the ghost of Terry Davis and his incomprehensible creation, TempleOS. Terry's core belief—that he could speak with God by generating random numbers and mapping them to the Bible—was a fascinating interction of faith and programming genius. &lt;/p&gt; &lt;p&gt;While building an OS is beyond me, I wanted to pay tribute to his core concept in a modern way. So, I created &lt;strong&gt;Portals&lt;/strong&gt;, a project that reimagines TempleOS's &amp;quot;divine random number generator&amp;quot; as a story-telling engine, powered entirely by a local LLM.&lt;/p&gt; &lt;p&gt;The whole thing runs locally with Streamlit and Ollama. It's a deeply personal, offline experience, just as Terry would have wanted.&lt;/p&gt; &lt;h1&gt;The Philosophy: A Modern Take on Terry's &amp;quot;Offering&amp;quot;&lt;/h1&gt; &lt;p&gt;Terry believed you had to make an &amp;quot;offering&amp;quot;—a significant, life-altering act—to get God's attention before generating a number. My project embraces this. The idea isn't just to click a button, but to engage with the app after you've done something meaningful in your own life.&lt;/p&gt; &lt;h1&gt;How It Works:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;The &amp;quot;Offering&amp;quot; (The Human Part):&lt;/strong&gt; This happens entirely outside the app. It's a personal commitment, a change in perspective, a difficult choice. This is you, preparing to &amp;quot;talk to God.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Consult the Oracle:&lt;/strong&gt; You run the app and click the button. A random number is generated, just like in TempleOS.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;A Verse is Revealed:&lt;/strong&gt; The number is mapped to a specific line in a numbered Bible text file, and a small paragraph around that line is pulled out. This is the &amp;quot;divine message.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Semantic Resonance (The LLM Part):&lt;/strong&gt; This is where the magic happens. The local LLM (I'm using Llama 3) reads the Bible verse and compares it to the last chapter of your ongoing D&amp;amp;D campaign story. It then decides if the verse has &amp;quot;High Resonance&amp;quot; or &amp;quot;Low Resonance&amp;quot; with the story's themes of angels, demons, and apocalypse.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The Story Unfolds:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;If it's &lt;strong&gt;&amp;quot;High Resonance,&amp;quot;&lt;/strong&gt; your offering was accepted. The LLM then uses the verse as inspiration to write the next chapter of your D&amp;amp;D campaign, introducing a new character, monster, location, or artifact inspired by the text.&lt;/li&gt; &lt;li&gt;If it's &lt;strong&gt;&amp;quot;Low Resonance,&amp;quot;&lt;/strong&gt; the offering was &amp;quot;boring,&amp;quot; as Terry would say. The heavens are silent, and the story doesn't progress. You're told to try again when you have something more significant to offer.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;It's essentially a solo D&amp;amp;D campaign where the Dungeon Master is a local LLM, and the plot twists are generated by the chaotic, divine randomness that Terry Davis revered. The LLM doesn't know your offering; it only interprets the synchronicity between the random verse and your story.&lt;/p&gt; &lt;p&gt;This feels like the closest I can get to the spirit of TempleOS without dedicating my life to kernel development. It's a system for generating meaning from chaos, all running privately on your own hardware.&lt;/p&gt; &lt;p&gt;I'd love for you guys to check it out, and I'm curious to hear your thoughts on this intersection of local AI, randomness, and the strange, brilliant legacy of Terry Davis.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub Repo&lt;/strong&gt; &lt;a href="https://github.com/iblameandrew/portals/tree/main"&gt;happy jumping&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1nozt72/video/sonesfylo0rf1/player"&gt;https://reddit.com/link/1nozt72/video/sonesfylo0rf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Temporary_Exam_3620"&gt; /u/Temporary_Exam_3620 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozt72/i_built_a_tribute_to_terry_daviss_templeos_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozt72/i_built_a_tribute_to_terry_daviss_templeos_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nozt72/i_built_a_tribute_to_terry_daviss_templeos_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T01:49:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1np1ytk</id>
    <title>What is the best 9B model or under ?</title>
    <updated>2025-09-24T03:35:45+00:00</updated>
    <author>
      <name>/u/Prior-Blood5979</name>
      <uri>https://old.reddit.com/user/Prior-Blood5979</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best model I can run on my system ? &lt;/p&gt; &lt;p&gt;I can run anything that's 9B or under it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You can include third party finetunes of it too&lt;/strong&gt;. On the side note, I believe we are not getting as many finetunes as before. Can it take that base models are better themselves ? or it's getting harder to finetuning. &lt;/p&gt; &lt;p&gt;It's just for personal use. Right now I'm using Gemma 4b, 3n and the old 9b model. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prior-Blood5979"&gt; /u/Prior-Blood5979 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np1ytk/what_is_the_best_9b_model_or_under/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np1ytk/what_is_the_best_9b_model_or_under/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np1ytk/what_is_the_best_9b_model_or_under/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T03:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nomrj7</id>
    <title>Leaderboards &amp; Benchmarks</title>
    <updated>2025-09-23T16:53:47+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"&gt; &lt;img alt="Leaderboards &amp;amp; Benchmarks" src="https://preview.redd.it/n79ymm450yqf1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=366296e43fd0844292650a0fe0b1176903e5bd77" title="Leaderboards &amp;amp; Benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Many Leaderboards are not up to date, recent models are missing. Don't know what happened to GPU Poor LLM Arena? I check Livebench, Dubesor, EQ-Bench, oobabooga often. Like these boards because these come with more Small &amp;amp; Medium size models(Typical boards usually stop with 30B at bottom &amp;amp; only few small models). For my laptop config(8GB VRAM &amp;amp; 32GB RAM), I need models 1-35B models. Dubesor's benchmark comes with Quant size too which is convenient &amp;amp; nice.&lt;/p&gt; &lt;p&gt;It's really heavy &amp;amp; consistent work to keep things up to date so big kudos to all leaderboards. What leaderboards do you check usually?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit:&lt;/strong&gt; Forgot to add oobabooga&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n79ymm450yqf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nomrj7/leaderboards_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:53:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nolz9e</id>
    <title>Qwen3Guard - a Qwen Collection</title>
    <updated>2025-09-23T16:24:35+00:00</updated>
    <author>
      <name>/u/Few_Painter_5588</name>
      <uri>https://old.reddit.com/user/Few_Painter_5588</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"&gt; &lt;img alt="Qwen3Guard - a Qwen Collection" src="https://external-preview.redd.it/SybQlpd57ri5DOffonwxQ3RJbORPPReSb_vD77lSWek.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa8f4c7470cc8c0c033a57dba7ee804d9d9e1d36" title="Qwen3Guard - a Qwen Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Painter_5588"&gt; /u/Few_Painter_5588 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3guard-68d2729abbfae4716f3343a1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nolz9e/qwen3guard_a_qwen_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T16:24:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1noru3p</id>
    <title>GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA</title>
    <updated>2025-09-23T20:05:10+00:00</updated>
    <author>
      <name>/u/On1ineAxeL</name>
      <uri>https://old.reddit.com/user/On1ineAxeL</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"&gt; &lt;img alt="GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA" src="https://external-preview.redd.it/U0uchtAJMRqNemoxp-a8VWVQpNPPATSQ8bLLGYLEcHQ.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2df1ad2c345943a90fdd8666f38e455de3a4819c" title="GPU Fenghua No.3, 112GB HBM, DX12, Vulcan 1.2, Claims to Support CUDA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Over 112 GB high-bandwidth memory for large-scale AI workloads&lt;/li&gt; &lt;li&gt;First Chinese GPU with hardware ray tracing support&lt;/li&gt; &lt;li&gt;vGPU design architecture with hardware virtualization&lt;/li&gt; &lt;li&gt;Supports DirectX 12, Vulkan 1.2, OpenGL 4.6, and up to six 8K displays&lt;/li&gt; &lt;li&gt;Domestic design based on OpenCore RISC-V CPU and full set of IP&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://videocardz.com/newz/innosilicon-unveils-fenghua-3-gpu-with-directx12-support-and-hardware-ray-tracing"&gt;https://videocardz.com/newz/innosilicon-unveils-fenghua-3-gpu-with-directx12-support-and-hardware-ray-tracing&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai"&gt;https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;&lt;a href="https://www.techpowerup.com/341268/innosilicons-fenghua-no-3-gpu-launches-with-112gb-hbm-memory-and-claims-to-support-cuda"&gt;Claims to Support CUDA&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kxpifn9e0zqf1.jpg?width=168&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebccecdf4a52af907db2b392b698eb7557d75e74"&gt;https://preview.redd.it/kxpifn9e0zqf1.jpg?width=168&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ebccecdf4a52af907db2b392b698eb7557d75e74&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/On1ineAxeL"&gt; /u/On1ineAxeL &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1noru3p/gpu_fenghua_no3_112gb_hbm_dx12_vulcan_12_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1np75y4</id>
    <title>[Rant] Magistral-Small-2509 &gt; Claude4</title>
    <updated>2025-09-24T08:58:16+00:00</updated>
    <author>
      <name>/u/OsakaSeafoodConcrn</name>
      <uri>https://old.reddit.com/user/OsakaSeafoodConcrn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So unsure if many of you use Claude4 for non-coding stuff...but it's been turned into a blithering idiot thanks to Anthropic giving us a dumb quant that cannot follow simple writing instructions (professional writing about such exciting topics as science/etc).&lt;/p&gt; &lt;p&gt;Claude4 is amazing for 3-4 business days after they come out with a new release. I believe this is due to them giving the public the full precision model for a few days to generate publicity and buzz...then forcing everyone onto a dumbed-down quant to save money on compute/etc. &lt;/p&gt; &lt;p&gt;That said...&lt;/p&gt; &lt;p&gt;I recall some guy on here saying his wife felt that Magistral-Small-2509 was better than Claude. Based on this random lady mentioned in a random anecdote, I downloaded Magistral-Small-2509-Q6_K.gguf from Bartowski and was able to fit it on my 3060 and 64GB DDR4 RAM.&lt;/p&gt; &lt;p&gt;Loaded up Oobabooga, set &amp;quot;cache type&amp;quot; to Q6 (assuming that's the right setting), and set &amp;quot;enable thinking&amp;quot; to &amp;quot;high.&amp;quot;&lt;/p&gt; &lt;p&gt;Magistral, even at a Q6 quant on my shitty 3060 and 64GB of RAM was better able to adhere to a prompt and follow a list of grammar rules WAY better than Claude4. &lt;/p&gt; &lt;p&gt;The tokens per second are surprisingly fast (I know that is subjective...but it types at the speed of a competent human typer).&lt;/p&gt; &lt;p&gt;While full precision Claude4 would blow anything local out of the water and dance the Irish jig on its rotting corpse....for some reason the major AI companies are giving us dumbed-down quants. Not talking shit about Magistral, nor all their hard work. &lt;/p&gt; &lt;p&gt;But one would expect a Q6 SMALL model to be a pile of shit compared to the billion-dollar AI models from Anthropic and their ilk. So, I'm absolutely blown away at how this little model that can is punching WELL above its weight class.&lt;/p&gt; &lt;p&gt;Thank you to Magistral. You have saved me hours of productivity lost by constantly forcing Claude4 to fix its fuckups and errors. For the most part, Magistral gives me what I need on the first or 2nd prompt.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OsakaSeafoodConcrn"&gt; /u/OsakaSeafoodConcrn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np75y4/rant_magistralsmall2509_claude4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np75y4/rant_magistralsmall2509_claude4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np75y4/rant_magistralsmall2509_claude4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T08:58:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nopcry</id>
    <title>Huawei Plans Three-Year Campaign to Overtake Nvidia in AI Chips</title>
    <updated>2025-09-23T18:31:10+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopcry/huawei_plans_threeyear_campaign_to_overtake/"&gt; &lt;img alt="Huawei Plans Three-Year Campaign to Overtake Nvidia in AI Chips" src="https://external-preview.redd.it/RG_Drphb5Z3LeMOBYSjdaBWtUZI-aLmpgWBBRijj4mk.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d14901df55c903a2634eb230416e5ea6c1bf81c" title="Huawei Plans Three-Year Campaign to Overtake Nvidia in AI Chips" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://finance.yahoo.com/news/huawei-plans-three-campaign-overtake-052622404.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nopcry/huawei_plans_threeyear_campaign_to_overtake/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nopcry/huawei_plans_threeyear_campaign_to_overtake/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T18:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nouiqj</id>
    <title>Qwen3-Omni thinking model running on local H100 (major leap over 2.5)</title>
    <updated>2025-09-23T21:49:43+00:00</updated>
    <author>
      <name>/u/Weary-Wing-6806</name>
      <uri>https://old.reddit.com/user/Weary-Wing-6806</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"&gt; &lt;img alt="Qwen3-Omni thinking model running on local H100 (major leap over 2.5)" src="https://external-preview.redd.it/ZG5qNW92cXRoenFmMQidY-VedNK5oWhNvWMcKBJGzqCaGjB2dyVwW_xfHksA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49e20215e9b1e3d03e6f379d9005a12d480bacc6" title="Qwen3-Omni thinking model running on local H100 (major leap over 2.5)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just gave the new Qwen3-Omni (thinking model) a run on my local H100.&lt;/p&gt; &lt;p&gt;Running FP8 dynamic quant with a 32k context size, enough room for 11x concurrency without issue. Latency is higher (which is expected) since thinking is enabled and it's streaming reasoning tokens.&lt;/p&gt; &lt;p&gt;But the output is sharp, and it's clearly smarter than Qwen 2.5 with better reasoning, memory, and real-world awareness.&lt;/p&gt; &lt;p&gt;It consistently understands what I’m saying, and even picked up when I was “singing” (just made some boop boop sounds lol).&lt;/p&gt; &lt;p&gt;Tool calling works too, which is huge. More on that + load testing soon!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weary-Wing-6806"&gt; /u/Weary-Wing-6806 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hsp0mvqthzqf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nouiqj/qwen3omni_thinking_model_running_on_local_h100/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T21:49:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nodc6q</id>
    <title>How are they shipping so fast 💀</title>
    <updated>2025-09-23T10:04:43+00:00</updated>
    <author>
      <name>/u/Independent-Wind4462</name>
      <uri>https://old.reddit.com/user/Independent-Wind4462</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt; &lt;img alt="How are they shipping so fast 💀" src="https://preview.redd.it/8higdv9r1wqf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c6b83f7127ef234c48c0416953381b9c3c6004a7" title="How are they shipping so fast 💀" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Well good for us &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Wind4462"&gt; /u/Independent-Wind4462 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8higdv9r1wqf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nodc6q/how_are_they_shipping_so_fast/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T10:04:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1np8uv6</id>
    <title>InclusionAI published GGUFs for the Ring-mini and Ling-mini models (MoE 16B A1.4B)</title>
    <updated>2025-09-24T10:46:45+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ring-mini-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF"&gt;https://huggingface.co/inclusionAI/Ling-mini-2.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;!!! warning !!! PRs are still not merged (read the discussions) you must use their version of llama.cpp&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16063"&gt;https://github.com/ggml-org/llama.cpp/pull/16063&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16028"&gt;https://github.com/ggml-org/llama.cpp/pull/16028&lt;/a&gt;&lt;/p&gt; &lt;p&gt;models:&lt;/p&gt; &lt;p&gt;Today, we are excited to announce the open-sourcing of &lt;strong&gt;Ling 2.0&lt;/strong&gt; — a family of MoE-based large language models that combine &lt;strong&gt;SOTA performance&lt;/strong&gt; with &lt;strong&gt;high efficiency&lt;/strong&gt;. The first released version, Ling-mini-2.0, is compact yet powerful. It has &lt;strong&gt;16B total parameters&lt;/strong&gt;, but only &lt;strong&gt;1.4B&lt;/strong&gt; are activated per input token (non-embedding 789M). Trained on more than &lt;strong&gt;20T tokens&lt;/strong&gt; of high-quality data and enhanced through multi-stage supervised fine-tuning and reinforcement learning, Ling-mini-2.0 achieves remarkable improvements in complex reasoning and instruction following. With just 1.4B activated parameters, it still reaches the top-tier level of sub-10B dense LLMs and even matches or surpasses much larger MoE models.&lt;/p&gt; &lt;p&gt;Ring is a reasoning and Ling is an instruct model (thanks &lt;a href="/u/Obvious-Ad-2454"&gt;u/Obvious-Ad-2454&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I hope they will also publish GGUFs for the 103B models soon.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np8uv6/inclusionai_published_ggufs_for_the_ringmini_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T10:46:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1not4up</id>
    <title>Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct</title>
    <updated>2025-09-23T20:55:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt; &lt;img alt="Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct" src="https://external-preview.redd.it/buohQYfptNXWK_RjSglwO9z3swviJ-ly59KfrJBuCDs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e7b84536da0880f9dbcad1c25d46560ac5263a67" title="Qwen3-VL-235B-A22B-Thinking and Qwen3-VL-235B-A22B-Instruct" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct"&gt;https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Meet Qwen3-VL — the most powerful vision-language model in the Qwen series to date.&lt;/p&gt; &lt;p&gt;This generation delivers comprehensive upgrades across the board: superior text understanding &amp;amp; generation, deeper visual perception &amp;amp; reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.&lt;/p&gt; &lt;p&gt;Available in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoning‑enhanced Thinking editions for flexible, on‑demand deployment.&lt;/p&gt; &lt;h1&gt;Key Enhancements:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Visual Agent&lt;/strong&gt;: Operates PC/mobile GUIs—recognizes elements, understands functions, invokes tools, completes tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Coding Boost&lt;/strong&gt;: Generates &lt;a href="http://Draw.io/HTML/CSS/JS"&gt;Draw.io/HTML/CSS/JS&lt;/a&gt; from images/videos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Advanced Spatial Perception&lt;/strong&gt;: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long Context &amp;amp; Video Understanding&lt;/strong&gt;: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Enhanced Multimodal Reasoning&lt;/strong&gt;: Excels in STEM/Math—causal analysis and logical, evidence-based answers.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Upgraded Visual Recognition&lt;/strong&gt;: Broader, higher-quality pretraining is able to “recognize everything”—celebrities, anime, products, landmarks, flora/fauna, etc.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Expanded OCR&lt;/strong&gt;: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Text Understanding on par with pure LLMs&lt;/strong&gt;: Seamless text–vision fusion for lossless, unified comprehension.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yx8zvj9z9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3dd9ee51e1ed2ee5b561d613720f88406ce22a14"&gt;https://preview.redd.it/yx8zvj9z9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3dd9ee51e1ed2ee5b561d613720f88406ce22a14&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/k2lvsypz9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dff5af92d79eb12f9b7ac684373c589fa924d18"&gt;https://preview.redd.it/k2lvsypz9zqf1.jpg?width=4583&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=2dff5af92d79eb12f9b7ac684373c589fa924d18&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1not4up/qwen3vl235ba22bthinking_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:55:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1nosdxy</id>
    <title>Qwen3-VL: Sharper Vision, Deeper Thought, Broader Action</title>
    <updated>2025-09-23T20:26:06+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://qwen.ai/blog?id=99f0335c4ad9ff6153e517418d48535ab6d8afef&amp;amp;from=research.latest-advancements-list"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nosdxy/qwen3vl_sharper_vision_deeper_thought_broader/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nosdxy/qwen3vl_sharper_vision_deeper_thought_broader/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T20:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nozz23</id>
    <title>The Ryzen AI MAX+ 395 is a true unicorn (In a good way)</title>
    <updated>2025-09-24T01:57:05+00:00</updated>
    <author>
      <name>/u/simracerman</name>
      <uri>https://old.reddit.com/user/simracerman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I put an order for the &lt;a href="https://frame.work/products/framework-desktop-mainboard-amd-ryzen-ai-max-300-series?v=FRAFMK0006"&gt;128GB version of the Framework Desktop Board&lt;/a&gt; for AI inference mainly, and while I've been waiting patiently for it to ship, I had doubts recently about the cost to benefit/future upgrade-ability since the RAM, CPU/iGPU are soldered into the motherboard.&lt;/p&gt; &lt;p&gt;So I decided to do a quick exercise of PC part picking to match the specs Framework is offering in their 128GB Board. I started looking at Motherboards offering 4 Channels, and thought I'd find something cheap.. wrong! &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Cheapest consumer level MB offering DDR5 at a high speed (8000 MT/s) with more than 2 channels is $600+. &lt;/li&gt; &lt;li&gt;CPU equivalent to the 395 MAX+ in benchmarks is the &lt;a href="https://www.amazon.com/AMD-Ryzen-9950X3D-16-Core-Processor/dp/B0DVZSG8D5"&gt;9955HX3d&lt;/a&gt;, which runs about ~$660 from Amazon. A quiet heat sink with dual fans from &lt;a href="https://www.amazon.com/Noctua-NH-D15-heatpipe-NF-A15-140mm/dp/B00L7UZMAK?s=electronics"&gt;Noctua&lt;/a&gt; is $130&lt;/li&gt; &lt;li&gt;RAM from &lt;a href="https://www.amazon.com/G-SKILL-Trident-CL38-48-48-128-Desktop-Computer/dp/B0F4M6C65N"&gt;G.Skill 4x24&lt;/a&gt; (128GB total) at 8000 MT/s runs you closer to $450. &lt;/li&gt; &lt;li&gt;The 8060s iGPU is similar in performance to the RTX 4060 or &lt;a href="https://www.amazon.com/MSI-Gaming-GeForce-GDRR6-Boost/dp/B0D3KGNMXP"&gt;4060 Ti 16gb&lt;/a&gt;, runs about $400.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Total for this build is ~&lt;strong&gt;$2240. I&lt;/strong&gt;t's obviously a good $500+ more than Framework's board. Cost aside, the speed is compromised as the GPU in this setup will access most of the system RAM at some a loss since it lives outside the GPU chip, and has to traverse the PCIE 5 to access the Memory directly. Total power draw out the wall at full system load at least double the 395's setup. More power = More fan noise = &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nogrv2/computer_literally_warms_my_room_by_5_degrees/"&gt;More heat&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;To compare, the M4 Pro/Max offer higher memory bandwidth, but suck at running diffusion models, also runs at 2X the cost at the same RAM/GPU specs. The 395 runs Linux/Windows, more flexibility and versatility (Games on Windows, Inference on Linux). Nvidia is so far out in the cost alone it makes no sense to compare it. The closest equivalent (but at much higher inference speed) is 4x 3090 which costs more, consumes multiple times the power, and generates a ton more heat.&lt;/p&gt; &lt;p&gt;AMD has a true unicorn here. For tinkers and hobbyists looking to develop, test, and gain more knowledge in this field, the MAX+ 395 is pretty much the only viable option at this $$ amount, with this low power draw. I decided to continue on with my order, but wondering if anyone else went down this rabbit hole seeking similar answers..!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/simracerman"&gt; /u/simracerman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nozz23/the_ryzen_ai_max_395_is_a_true_unicorn_in_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T01:57:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1np2v1i</id>
    <title>Large Language Model Performance Doubles Every 7 Months</title>
    <updated>2025-09-24T04:24:24+00:00</updated>
    <author>
      <name>/u/Aralknight</name>
      <uri>https://old.reddit.com/user/Aralknight</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"&gt; &lt;img alt="Large Language Model Performance Doubles Every 7 Months" src="https://external-preview.redd.it/FIe2X4pB5JIPoblqtKC-Psg0C0IDm1Mq5ljjHekoesw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=74fd271c0f36614a182e5a476492961d5ccd453d" title="Large Language Model Performance Doubles Every 7 Months" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aralknight"&gt; /u/Aralknight &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://spectrum.ieee.org/large-language-model-performance"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np2v1i/large_language_model_performance_doubles_every_7/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T04:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1nor65d</id>
    <title>Qwen 3 max released</title>
    <updated>2025-09-23T19:40:02+00:00</updated>
    <author>
      <name>/u/clem844</name>
      <uri>https://old.reddit.com/user/clem844</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list"&gt;https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&amp;amp;from=research.latest-advancements-list&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Following the release of the Qwen3-2507 series, we are thrilled to introduce Qwen3-Max — our largest and most capable model to date. The preview version of Qwen3-Max-Instruct currently ranks third on the Text Arena leaderboard, surpassing GPT-5-Chat. The official release further enhances performance in coding and agent capabilities, achieving state-of-the-art results across a comprehensive suite of benchmarks — including knowledge, reasoning, coding, instruction following, human preference alignment, agent tasks, and multilingual understanding. We invite you to try Qwen3-Max-Instruct via its API on Alibaba Cloud or explore it directly on Qwen Chat. Meanwhile, Qwen3-Max-Thinking — still under active training — is already demonstrating remarkable potential. When augmented with tool usage and scaled test-time compute, the Thinking variant has achieved 100% on challenging reasoning benchmarks such as AIME 25 and HMMT. We look forward to releasing it publicly in the near future.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/clem844"&gt; /u/clem844 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nor65d/qwen_3_max_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-23T19:40:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1np5ey8</id>
    <title>MiniModel-200M-Base</title>
    <updated>2025-09-24T06:58:12+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"&gt; &lt;img alt="MiniModel-200M-Base" src="https://preview.redd.it/clbzeq0i82rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=056ef5c77a2001c2a6d5509cbdcb9173566b1c52" title="MiniModel-200M-Base" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most “efficient” small models still need days of training or massive clusters. &lt;strong&gt;MiniModel-200M-Base&lt;/strong&gt; was trained &lt;strong&gt;from scratch on just 10B tokens&lt;/strong&gt; in &lt;strong&gt;110k steps (≈1 day)&lt;/strong&gt; on a &lt;strong&gt;single RTX 5090&lt;/strong&gt;, using &lt;strong&gt;no gradient accumulation&lt;/strong&gt; yet still achieving a &lt;strong&gt;batch size of 64 x 2048 tokens&lt;/strong&gt; and with peak memory &lt;strong&gt;&amp;lt;30 GB VRAM&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Key efficiency techniques:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Adaptive Muon optimizer&lt;/strong&gt;: 2.1× more data-efficient than AdamW&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Float8 pretraining&lt;/strong&gt;: ~30% less VRAM, ~20% higher throughput (attention kept in bf16)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;ReLU² activation&lt;/strong&gt; (from Google’s &lt;em&gt;Primer&lt;/em&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bin-packing&lt;/strong&gt;: reduced padding from &amp;gt;70% → &amp;lt;5%&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Full attention + QK-norm without scalars&lt;/strong&gt; for stability&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Despite its size, it shows surprising competence:&lt;/p&gt; &lt;p&gt;✅ &lt;strong&gt;Fibonacci (temp=0.0001)&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def fibonacci(n: int): if n &amp;lt; 2: return n return fibonacci(n - 1) + fibonacci(n - 2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;✅ &lt;strong&gt;Digits of π (temp=0.0001)&lt;/strong&gt;&lt;br /&gt; Recites &lt;strong&gt;3.14159265358979323846…&lt;/strong&gt; correctly — the first 20+ digits.&lt;/p&gt; &lt;p&gt;It’s &lt;strong&gt;Apache 2.0 licensed&lt;/strong&gt;, with public config, tokenizer, and safetensors weights. No instruct-tuning yet, as this is pure pretraining on educational data (Ultra-FineWeb, Python tutorials, math).&lt;/p&gt; &lt;p&gt;Not perfect (it thinks Earth’s radius is 375,000 miles), but for a 200M model trained in a day it’s a solid base for experimentation, distillation, or local prototyping.&lt;/p&gt; &lt;p&gt;🔗 &lt;a href="https://huggingface.co/xTimeCrystal/MiniModel-200M-Base"&gt;Hugging Face: MiniModel-200M-Base&lt;/a&gt;&lt;br /&gt; 🧠 200M | 🌐 en/zh/Python | 📜 Apache 2.0&lt;/p&gt; &lt;p&gt;Any feedback is welcome, especially on replicating the training setup or improving data efficiency!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/clbzeq0i82rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np5ey8/minimodel200mbase/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T06:58:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1np5te1</id>
    <title>Oh my God, what a monster is this?</title>
    <updated>2025-09-24T07:24:01+00:00</updated>
    <author>
      <name>/u/NearbyBig3383</name>
      <uri>https://old.reddit.com/user/NearbyBig3383</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"&gt; &lt;img alt="Oh my God, what a monster is this?" src="https://preview.redd.it/1pxmwf50e2rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9d1eb3d320d1305fe702c08f9c69cd841db5fd1" title="Oh my God, what a monster is this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NearbyBig3383"&gt; /u/NearbyBig3383 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1pxmwf50e2rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1np5te1/oh_my_god_what_a_monster_is_this/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T07:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
