<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-16T12:28:23+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nidrwy</id>
    <title>Docling Interferes with Embedding &amp; Reranking</title>
    <updated>2025-09-16T10:30:55+00:00</updated>
    <author>
      <name>/u/Cyp9715</name>
      <uri>https://old.reddit.com/user/Cyp9715</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidrwy/docling_interferes_with_embedding_reranking/"&gt; &lt;img alt="Docling Interferes with Embedding &amp;amp; Reranking" src="https://a.thumbs.redditmedia.com/WHi6pRTcxFhpc1e0itpBMZwx3JJcVtNT1zkzRxIEzZ0.jpg" title="Docling Interferes with Embedding &amp;amp; Reranking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been testing a variety of content extractors, embedding models, and reranking models lately. In my experience, Docling offers the best quality among all free‑to‑use content extractors, but many embedding and reranking models fail to correctly interpret tabular layouts. As a result, they often place irrelevant or mismatched data in the output.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1oufleoq9ipf1.png?width=858&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=388cdcc28bac21add964aaeb1e9139750e89eaeb"&gt;Qwen3 Embedding &amp;amp; Qwen3 Reranker : Document is a normal document that contains many tables.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This issue is quite severe-in certain documents, unless you feed the entire document context directly to the model, using Docling becomes impractical.(In other words, I used Docling to have the tables recognized correctly, but because of compatibility with the Embedding and Reranker models, I can’t make proper use of it; to use it properly you have to either turn off table recognition, or use the “full‑context” mode.)&lt;/p&gt; &lt;p&gt;If anyone has encountered the same problem or managed to work around it, I’d love to hear your thoughts and solutions.&lt;/p&gt; &lt;p&gt;Models I’ve tried:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;BAAI(m3, v2-gamma, v2-m3, etc...)&lt;/li&gt; &lt;li&gt;Qwen3(reranker, embedding)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And, as expected, replacing it with Tika or a similar tool eliminates all problems. The fundamental solution would be to retrain the model to match Docling’s output format, or to wait for the main LLM to evolve enough to handle very long contexts, but I’m curious whether there’s a smarter way.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cyp9715"&gt; /u/Cyp9715 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidrwy/docling_interferes_with_embedding_reranking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidrwy/docling_interferes_with_embedding_reranking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nidrwy/docling_interferes_with_embedding_reranking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T10:30:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhdi2u</id>
    <title>Update: we got our revenge and now beat Deepmind, Microsoft, Zhipu AI and Alibaba</title>
    <updated>2025-09-15T05:32:12+00:00</updated>
    <author>
      <name>/u/Connect-Employ-4708</name>
      <uri>https://old.reddit.com/user/Connect-Employ-4708</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Three weeks ago we open-sourced our agent that uses mobile apps like a human. At that moment, we were #2 on AndroidWorld (behind Zhipu AI).&lt;/p&gt; &lt;p&gt;Since, we worked hard and improved the performance of our agent: &lt;strong&gt;we’re now officially #1&lt;/strong&gt; on the &lt;a href="https://docs.google.com/spreadsheets/d/1cchzP9dlTZ3WXQTfYNhh3avxoLipqHN75v1Tb86uhHo/edit?pli=1&amp;amp;gid=0#gid=0"&gt;AndroidWorld leaderboard&lt;/a&gt;, surpassing Deepmind, Microsoft Research, Zhipu AI and Alibaba.&lt;/p&gt; &lt;p&gt;It handles mobile tasks: booking rides, ordering food, navigating apps, just like a human would. Still working on improvements and building an RL gym for fine-tuning :)&lt;/p&gt; &lt;p&gt;The agent is completely open-source: &lt;a href="http://github.com/minitap-ai/mobile-use"&gt;github.com/minitap-ai/mobile-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What mobile tasks would you want an AI agent to handle for you? Always looking for feedback and contributors!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Connect-Employ-4708"&gt; /u/Connect-Employ-4708 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhdi2u/update_we_got_our_revenge_and_now_beat_deepmind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:32:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1nieqgy</id>
    <title>Vector DBs and LM Studio, how does it work in practicality?</title>
    <updated>2025-09-16T11:24:07+00:00</updated>
    <author>
      <name>/u/TunnelToTheMoon</name>
      <uri>https://old.reddit.com/user/TunnelToTheMoon</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I'm going to take a backup of the vectors made in LM Studio from a RAG, and I expect that to go just well with ChromaDB. But when I want to hook up those vectors with a new chat then I'm not sure how to proceed in LMS. I can't find any &amp;quot;load vector DB&amp;quot; anywhere, but I might not have looked well enough. I'm obviously not very experienced with using vectors from one chat to another, so this might seem trivial to some, but I'm still outside a tall gate on this right now. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TunnelToTheMoon"&gt; /u/TunnelToTheMoon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nieqgy/vector_dbs_and_lm_studio_how_does_it_work_in/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nieqgy/vector_dbs_and_lm_studio_how_does_it_work_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nieqgy/vector_dbs_and_lm_studio_how_does_it_work_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:24:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhv42c</id>
    <title>NCSOFT/VARCO-VISION-2.0-14B · Hugging Face</title>
    <updated>2025-09-15T19:12:50+00:00</updated>
    <author>
      <name>/u/ninjasaid13</name>
      <uri>https://old.reddit.com/user/ninjasaid13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhv42c/ncsoftvarcovision2014b_hugging_face/"&gt; &lt;img alt="NCSOFT/VARCO-VISION-2.0-14B · Hugging Face" src="https://external-preview.redd.it/Zqwx3E1Z_EElc-Wqav8y07fmTzCY9Mbf2KZGmL-cSJg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8273519f7df58dceaae1c21bddf409931659286d" title="NCSOFT/VARCO-VISION-2.0-14B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Abstract&lt;/p&gt; &lt;p&gt;&lt;strong&gt;VARCO-VISION-2.0&lt;/strong&gt; is a multimodal AI model capable of understanding both images and text to answer user queries. It supports multi-image inputs, enabling effective processing of complex content such as documents, tables, and charts. The model demonstrates strong comprehension in both Korean and English, with significantly improved text generation capabilities and a deeper understanding of Korean cultural context. Compared to its predecessor, performance has been notably enhanced across various benchmarks, and its usability in real-world scenarios—such as everyday Q&amp;amp;A and information summarization—has also improved.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ninjasaid13"&gt; /u/ninjasaid13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NCSOFT/VARCO-VISION-2.0-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhv42c/ncsoftvarcovision2014b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhv42c/ncsoftvarcovision2014b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T19:12:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1nig0zp</id>
    <title>Hardware and model recommendations for on-prem LLM deployment</title>
    <updated>2025-09-16T12:26:23+00:00</updated>
    <author>
      <name>/u/neenawa</name>
      <uri>https://old.reddit.com/user/neenawa</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've delivered a couple of projects using frontier models, but my latest client wants something on-prem for his team of ~10. The application will have a RAG pipeline. Starting with ~100 PDFs. Later I will need to add and some agentic reasoning. &lt;/p&gt; &lt;p&gt;Questions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Which open-source LLM is a good place to start for RAG? I will experiment a bit, but nice to have some working experience.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Viable hardware: do I need Nvidia? AMD? I've only ever used cloud-based systems, so this is a bit new to me, and the part I feel less sure about.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help would be appreciated, thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/neenawa"&gt; /u/neenawa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nig0zp/hardware_and_model_recommendations_for_onprem_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nig0zp/hardware_and_model_recommendations_for_onprem_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nig0zp/hardware_and_model_recommendations_for_onprem_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T12:26:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhn5sy</id>
    <title>Testers w/ 4th-6th Generation Xeon CPUs wanted to test changes to llama.cpp</title>
    <updated>2025-09-15T14:19:58+00:00</updated>
    <author>
      <name>/u/DataGOGO</name>
      <uri>https://old.reddit.com/user/DataGOGO</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all,.&lt;/p&gt; &lt;p&gt;I have been working on improving AMX acceleration in llama.cpp. Currently, even if you have a a supported CPU and have built llama.cpp with all the required build flags, AMX acceleration is disabled if you have a GPU present.&lt;/p&gt; &lt;p&gt;I modified the way that llama.cpp exposes the &amp;quot;extra&amp;quot; CPU buffers so that AMX will remain functional in CPU/GPU hybrids, resulting in a 20-40% increase in performance for CPU offloaded layers / CPU offloaded experts.&lt;/p&gt; &lt;p&gt;Since I have limited hardware to test with I made a temporary fork and I am looking for testers make sure everything is good before I open a PR to roll the changes into mainline llama.cpp.&lt;/p&gt; &lt;p&gt;4th-6th Generation Xeons accelerations supported in hybrid: AVX-512VNNI, AMXInt8, AMXBF16&lt;/p&gt; &lt;p&gt;&lt;em&gt;Note: I have made the changes to AMX.cpp to implement AMXInt4, but since I don't have a 6th generation Xeon, I can't test it, so I left it out for now.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;To enable the new behavior you just place &amp;quot;--amx&amp;quot; in your launch command string, to revert to base behavior, just remove the &amp;quot;--amx&amp;quot; flag.&lt;/p&gt; &lt;p&gt;If you test please leave a comment in the discussions in the Github with your CPU/RAM/GPU hardware information and your results with and without the &amp;quot;--amx&amp;quot; flag using the example llama-bench and llama-cli commands (takes less that 1 min each) it would be very helpful. Feel free to include any other tests that you do, the more the better.&lt;/p&gt; &lt;p&gt;Huge thank you in advance!&lt;/p&gt; &lt;p&gt;Here is the github: Instructions and example commands are in the readme.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Gadflyii/llama.cpp"&gt;https://github.com/Gadflyii/llama.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DataGOGO"&gt; /u/DataGOGO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhn5sy/testers_w_4th6th_generation_xeon_cpus_wanted_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhn5sy/testers_w_4th6th_generation_xeon_cpus_wanted_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhn5sy/testers_w_4th6th_generation_xeon_cpus_wanted_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T14:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhpy35</id>
    <title>A lightweight and tunable python chat interface to interact with LLM, featuring persistent memory</title>
    <updated>2025-09-15T16:03:39+00:00</updated>
    <author>
      <name>/u/Vicouille6</name>
      <uri>https://old.reddit.com/user/Vicouille6</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhpy35/a_lightweight_and_tunable_python_chat_interface/"&gt; &lt;img alt="A lightweight and tunable python chat interface to interact with LLM, featuring persistent memory" src="https://preview.redd.it/olzso2n2qcpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b502586d78f6bcec10593dca6d7a69c2f9f80094" title="A lightweight and tunable python chat interface to interact with LLM, featuring persistent memory" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I developed a lightweight Python tool that allows local LLM to maintain persistent memory, and I’m sharing it here.&lt;/p&gt; &lt;p&gt;Local models are great for privacy and offline use, but they typically lose all context between sessions unlike online services, as you all know.&lt;/p&gt; &lt;p&gt;Previously, I built a project that captured conversations from LM Studio and stored them in a database to enrich prompts sent to models. This new version is a direct chat interface (leveraging easy-llama by u/master-meal-77, many thanks to him) that makes the memory process completely seamless and invisible to the user.&lt;/p&gt; &lt;h1&gt;Key features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Fully local, no external API dependencies&lt;/li&gt; &lt;li&gt;Short-term and long-term memory for fluid conversations and contextually relevant responses -&lt;/li&gt; &lt;li&gt;Fully customizable depth of memory and model parameters&lt;/li&gt; &lt;li&gt;Workspaces to separate different projects&lt;/li&gt; &lt;li&gt;Built-in visualizations to track memory data and semantic indicators&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Upcoming developments:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Document support (PDF, Word, Excel, images) for targeted queries&lt;/li&gt; &lt;li&gt;Integrated web search to supplement local memory with the most recent information&lt;/li&gt; &lt;li&gt;Selective import/export of personal memory through workspaces for sharing within a team&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think this project could be of interest to some users of this sub.&lt;/p&gt; &lt;p&gt;The code is here : &lt;a href="https://github.com/victorcarre6/LocalMind"&gt;GitHub repository&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to use it as you want and to share your feedback! :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vicouille6"&gt; /u/Vicouille6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/olzso2n2qcpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhpy35/a_lightweight_and_tunable_python_chat_interface/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhpy35/a_lightweight_and_tunable_python_chat_interface/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T16:03:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhx3jp</id>
    <title>What’s the most cost-effective and best AI model for coding in your experience?</title>
    <updated>2025-09-15T20:25:39+00:00</updated>
    <author>
      <name>/u/Mammoth-Leopard6549</name>
      <uri>https://old.reddit.com/user/Mammoth-Leopard6549</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;br /&gt; I’m curious to hear from developers here: which AI model do you personally find the most cost-effective and reliable for coding tasks?&lt;/p&gt; &lt;p&gt;I know it can depend a lot on use cases (debugging, writing new code, learning, pair programming, etc.), but I’d love to get a sense of what actually works well for you in real projects.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which model do you use the most?&lt;/li&gt; &lt;li&gt;Do you combine multiple models depending on the task?&lt;/li&gt; &lt;li&gt;If you pay for one, do you feel the price is justified compared to free or open-source options?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I think it’d be really helpful to compare experiences across the community, so please share your thoughts!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mammoth-Leopard6549"&gt; /u/Mammoth-Leopard6549 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhx3jp/whats_the_most_costeffective_and_best_ai_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhx3jp/whats_the_most_costeffective_and_best_ai_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhx3jp/whats_the_most_costeffective_and_best_ai_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T20:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni2vqw</id>
    <title>Single Install for GGUF Across CPU/GPU/NPU - Goodbye Multiple Builds</title>
    <updated>2025-09-16T00:24:25+00:00</updated>
    <author>
      <name>/u/Different-Effect-724</name>
      <uri>https://old.reddit.com/user/Different-Effect-724</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2vqw/single_install_for_gguf_across_cpugpunpu_goodbye/"&gt; &lt;img alt="Single Install for GGUF Across CPU/GPU/NPU - Goodbye Multiple Builds" src="https://external-preview.redd.it/CVSxdiqWqp-ZrX8yVw3cfjqBie1BbHNW--0BWHDVzxg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=856312cd81fead7db026713fa8f52842ec93752d" title="Single Install for GGUF Across CPU/GPU/NPU - Goodbye Multiple Builds" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;br /&gt; AI developers need flexibility and simplicity when running and developing with local models, yet popular on-device runtimes such as llama.cpp and Ollama still often fall short:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Separate installers for CPU, GPU, and NPU&lt;/li&gt; &lt;li&gt;Conflicting APIs and function signatures&lt;/li&gt; &lt;li&gt;NPU-optimized formats are limited&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For anyone building on-device LLM apps, these hurdles slow development and fragment the stack.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;To solve this:&lt;/strong&gt;&lt;br /&gt; I upgraded Nexa SDK so that it supports:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;One core API for LLM/VLM/embedding/ASR&lt;/li&gt; &lt;li&gt;Backend plugins for CPU, GPU, and NPU that load only when needed&lt;/li&gt; &lt;li&gt;Automatic registry to pick the best accelerator at runtime&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1ni2vqw/video/uucn4t7p6fpf1/player"&gt;https://reddit.com/link/1ni2vqw/video/uucn4t7p6fpf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;On an HP OmniBook with Snapdragon Elite X, I ran the same LLaMA-3.2-3B GGUF model and achieved:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;On CPU: 17 tok/s&lt;/li&gt; &lt;li&gt;On GPU: 10 tok/s&lt;/li&gt; &lt;li&gt;On NPU (Turbo engine): 29 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I didn’t need to switch backends or make any extra code changes; everything worked with the same SDK.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;You Can Achieve&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ship a single build that scales from laptops to edge devices&lt;/li&gt; &lt;li&gt;Mix GGUF and vendor-optimized formats without rewriting code&lt;/li&gt; &lt;li&gt;Cut cold-start times to milliseconds while keeping the package size small&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Download one installer, choose your model, and deploy across CPU, GPU, and NPU—without changing a single line of code, so AI developers can focus on the actual products instead of wrestling with hardware differences. &lt;/p&gt; &lt;p&gt;Try it today and leave a star if you find it helpful: &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;GitHub repo&lt;/a&gt;&lt;br /&gt; Please let me know any feedback or thoughts. I look forward to keeping updating this project based on requests.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Different-Effect-724"&gt; /u/Different-Effect-724 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2vqw/single_install_for_gguf_across_cpugpunpu_goodbye/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2vqw/single_install_for_gguf_across_cpugpunpu_goodbye/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2vqw/single_install_for_gguf_across_cpugpunpu_goodbye/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T00:24:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhd5ks</id>
    <title>Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k</title>
    <updated>2025-09-15T05:11:27+00:00</updated>
    <author>
      <name>/u/MLDataScientist</name>
      <uri>https://old.reddit.com/user/MLDataScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt; &lt;img alt="Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k" src="https://b.thumbs.redditmedia.com/fnXtmv3xMQuxl9xhdyUEPfnB7lkOh7QN0YQdqGLHvKc.jpg" title="Completed 8xAMD MI50 - 256GB VRAM + 256GB RAM rig for $3k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;A few months ago I posted about how I was able to purchase 4xMI50 for $600 and run them using my consumer PC. Each GPU could run at PCIE3.0 x4 speed and my consumer PC did not have enough PCIE lanes to support more than 6x GPUs. My final goal was to run all 8 GPUs at proper PCIE4.0 x16 speed. &lt;/p&gt; &lt;p&gt;I was finally able to complete my setup. Cost breakdown:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;ASRock ROMED8-2T Motherboard with 8x32GB DDR4 3200Mhz and AMD Epyc 7532 CPU (32 cores), dynatron 2U heatsink - $1000&lt;/li&gt; &lt;li&gt;6xMI50 and 2xMI60 - $1500&lt;/li&gt; &lt;li&gt;10x blower fans (all for $60), 1300W PSU ($120) + 850W PSU (already had this), 6x 300mm riser cables (all for $150), 3xPCIE 16x to 8x8x bifurcation cards (all for $70), 8x PCIE power cables and fan power controller (for $100)&lt;/li&gt; &lt;li&gt;GTX 1650 4GB for video output (already had this)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In total, I spent around ~$3k for this rig. All used parts.&lt;/p&gt; &lt;p&gt;ASRock ROMED8-2T was an ideal motherboard for me due to its seven x16 full physical PCIE4.0 slots.&lt;/p&gt; &lt;p&gt;Attached photos below.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/b052o7hi99pf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=20fb34bd86438c2a2111fb0eb52a70b26b3b9685"&gt;8xMI50/60 32GB with GTX 1650 top view&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cnnr3ixn99pf1.jpg?width=4080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=273be5463afc2508a46f17ea5e63b6e6de51b5fb"&gt;8xMI50/60 32GB in open frame rack with motherboard and PSU. My consumer PC is on the right side (not used here)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have not done many LLM tests yet. PCIE4.0 connection was not stable since I am using longer PCIE risers. So, I kept the speed for each PCIE slot at 3.0 x16. Some initial performance metrics are below. Installed Ubuntu 24.04.3 with ROCm 6.4.3 (needed to copy paste gfx906 tensiles to fix deprecated support).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CPU alone: gpt-oss 120B (65GB Q8) runs at ~25t/s with ~120t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;2xMI50: gpt-oss 120B (65GB Q8) runs at ~58t/s with 750t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;8xMI50: qwen3 235B Q4_1 runs at ~21t/s with 350t/s prompt processing (llama.cpp)&lt;/li&gt; &lt;li&gt;2xMI60 vllm gfx906: llama3.3 70B AWQ: 25t/s with ~240 t/s prompt processing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Idle power consumption is around ~400W (20w for each GPU, 15w for each blower fan, ~100W for motherboard, RAM, fan and CPU). llama.cpp inference averages around 750W (using wall meter). For a few seconds during inference, the power spikes up to 1100W&lt;/p&gt; &lt;p&gt;I will do some more performance tests. Overall, I am happy with what I was able to build and run. &lt;/p&gt; &lt;p&gt;Fun fact: the entire rig costs around the same price as a single RTX 5090 (variants like ASUS TUF).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MLDataScientist"&gt; /u/MLDataScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhd5ks/completed_8xamd_mi50_256gb_vram_256gb_ram_rig_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T05:11:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nic4ws</id>
    <title>Can I run Parakeet v3 Multilingual locally with my AMD RX 5700 XT?</title>
    <updated>2025-09-16T08:49:26+00:00</updated>
    <author>
      <name>/u/solcid1</name>
      <uri>https://old.reddit.com/user/solcid1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m a law student in Spain and I’ve been using Whisper v3 Turbo for my note-taking. It works, but for something like a 1.5-hour class, the transcription ends up taking me almost 2 hours when I run it locally. &lt;/p&gt; &lt;p&gt;I also have an AMD RX 5700 XT, but I’m not sure if I can use it to run Parakeet v3 0.6 locally to make things faster. Is that possible? And if yes, how would I set it up? Would I need to use my own GPU?&lt;/p&gt; &lt;p&gt;If anyone could share a tutorial or point me in the right direction, I’d really appreciate it.&lt;/p&gt; &lt;p&gt;Thanks a lot!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/solcid1"&gt; /u/solcid1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nic4ws/can_i_run_parakeet_v3_multilingual_locally_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nic4ws/can_i_run_parakeet_v3_multilingual_locally_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nic4ws/can_i_run_parakeet_v3_multilingual_locally_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T08:49:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1niezv4</id>
    <title>I built a tool to search content in my local files using semantic search</title>
    <updated>2025-09-16T11:37:33+00:00</updated>
    <author>
      <name>/u/Interesting-Area6418</name>
      <uri>https://old.reddit.com/user/Interesting-Area6418</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone&lt;/p&gt; &lt;p&gt;A while back I shared an open source tool called DeepDoc that I built to explore local files using a research type workflow. The support and feedback I got here really meant a lot and kept me building more so thank you&lt;/p&gt; &lt;p&gt;The idea is simple. Instead of manually going through pdfs, docs, or notes I wanted a smarter way to search the content of my own files&lt;br /&gt; You just point it to a folder with pdf docx txt or image files. It extracts the text splits it into chunks does semantic search based on your query and builds a structured markdown report step by step&lt;/p&gt; &lt;p&gt;Here is the repo if you want to take a look&lt;br /&gt; &lt;a href="https://github.com/Datalore-ai/deepdoc"&gt;https://github.com/Datalore-ai/deepdoc&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It recently reached 95 stars which honestly means a lot to me. Knowing that people actually use it and find it useful really made my day&lt;/p&gt; &lt;p&gt;Many people suggested adding OneDrive Google Drive integrations and support for more file formats which I am planning to add soon. and keep making it better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Interesting-Area6418"&gt; /u/Interesting-Area6418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niezv4/i_built_a_tool_to_search_content_in_my_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1niezv4/i_built_a_tool_to_search_content_in_my_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1niezv4/i_built_a_tool_to_search_content_in_my_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:37:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni1uw3</id>
    <title>Anyone else have small models just "forget" MCP tools exist?</title>
    <updated>2025-09-15T23:37:53+00:00</updated>
    <author>
      <name>/u/TheLostWanderer47</name>
      <uri>https://old.reddit.com/user/TheLostWanderer47</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to stitch together a lightweight &amp;quot;local research assistant&amp;quot; setup with MCP, but running into weird behavior:&lt;/p&gt; &lt;p&gt;Stack:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/brightdata/brightdata-mcp"&gt;Bright Data MCP&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/CherryHQ/cherry-studio"&gt;Cherry Studio&lt;/a&gt; built-in knowledge graph MCP&lt;/li&gt; &lt;li&gt;Ollama connected w/ Qwen3-4B-Instruct-2507 as the model&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Most of the time, Qwen doesn’t even seem to know that the MCP tools are there. Paraphrasing the problem here:&lt;/p&gt; &lt;p&gt;Me: &amp;quot;Fetch this URL, then summarize it in 3 bullets, and finally, store it in the knowledge graph with observations.&amp;quot;&lt;br /&gt; Qwen: &amp;quot;Sorry, I don't have any tools that can browse the internet to fetch the contents of that page for you.&amp;quot;&lt;/p&gt; &lt;p&gt;…but maybe 1 out of 3 tries, it does call the Bright Data MCP and returns clean markdown???&lt;/p&gt; &lt;p&gt;Same with Cherry’s knowledge graph. sometimes it builds links between entities, sometimes the model acts like the tool was never registered.&lt;/p&gt; &lt;p&gt;I've tried explicitly reminding the model, &amp;quot;you have these tools available,&amp;quot; but it doesn't stick.&lt;/p&gt; &lt;p&gt;Have I messed up the config somewhere? Has anyone else run into this &amp;quot;tool amnesia&amp;quot; issue with Cherry studio or MCP servers?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLostWanderer47"&gt; /u/TheLostWanderer47 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni1uw3/anyone_else_have_small_models_just_forget_mcp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni1uw3/anyone_else_have_small_models_just_forget_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni1uw3/anyone_else_have_small_models_just_forget_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T23:37:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni8noo</id>
    <title>LLMs for detailed book summaries?</title>
    <updated>2025-09-16T05:09:50+00:00</updated>
    <author>
      <name>/u/JealousAmoeba</name>
      <uri>https://old.reddit.com/user/JealousAmoeba</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am picturing a tool that I can throw any arbitrary ePub novel at and get back a SparkNotes-style summary:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.sparknotes.com/lit/pride/"&gt;https://www.sparknotes.com/lit/pride/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(This page has a plot overview but there are other pages that do deeper dives into the material.)&lt;/p&gt; &lt;p&gt;It seems like something an LLM could do in principle if you could avoid hallucinations and maintain coherency. I don’t really think dumping the entire book into context would work, especially since some books are too long to reasonably fit.&lt;/p&gt; &lt;p&gt;Has anyone had success on this?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JealousAmoeba"&gt; /u/JealousAmoeba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni8noo/llms_for_detailed_book_summaries/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni8noo/llms_for_detailed_book_summaries/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni8noo/llms_for_detailed_book_summaries/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T05:09:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni87hl</id>
    <title>Feedback on trimmed-down AI workstation build (based on a16z specs)</title>
    <updated>2025-09-16T04:44:46+00:00</updated>
    <author>
      <name>/u/cuuuuuooooongg</name>
      <uri>https://old.reddit.com/user/cuuuuuooooongg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m putting together a local AI workstation build inspired by the &lt;a href="https://a16z.com/building-a16zs-personal-ai-workstation-with-four-nvidia-rtx-6000-pro-blackwell-max-q-gpus/"&gt;a16z setup&lt;/a&gt;. The idea is to stop bleeding money on GCP/AWS for GPU hours and finally have a home rig for quick ideation and prototyping. I’ll mainly be using it to train and finetune custom architectures.&lt;/p&gt; &lt;p&gt;I’ve slimmed down the original spec to make it (slightly) more reasonable while keeping room to expand in the future. I’d love feedback from this community before pulling the trigger.&lt;/p&gt; &lt;p&gt;Here are the main changes vs the reference build:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;4× GPU → 1× GPU (will expand later if needed)&lt;/li&gt; &lt;li&gt;256GB RAM → 128GB RAM&lt;/li&gt; &lt;li&gt;8TB storage → 2TB storage&lt;/li&gt; &lt;li&gt;Sticking with the same PSU for headroom if I add GPUs later&lt;/li&gt; &lt;li&gt;Unsure if the motherboard swap is the right move (original was GIGABYTE MH53-G40, I picked the ASUS Pro WS WRX90E-SAGE SE — any thoughts here?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Current parts list:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Category&lt;/th&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;NVIDIA RTX PRO 6000 Blackwell Max-Q&lt;/td&gt; &lt;td align="left"&gt;$8,449.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;AMD Ryzen Threadripper PRO 7975WX 32-core 5.3GHz Computer Processor&lt;/td&gt; &lt;td align="left"&gt;$3,400.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Pro WS WRX90E-SAGE SE&lt;/td&gt; &lt;td align="left"&gt;$1,299.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;OWC DDR5 4×32GB&lt;/td&gt; &lt;td align="left"&gt;$700.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;WD_BLACK 2TB SN8100 NVMe SSD Internal Solid State Drive - Gen 5 PCIe 5.0x4, M.2 2280&lt;/td&gt; &lt;td align="left"&gt;$230.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;Thermaltake Toughpower GF3&lt;/td&gt; &lt;td align="left"&gt;$300.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;CPU Cooler&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;ARCTIC Liquid Freezer III Pro 420 A-RGB – AIO CPU Cooler, 3 × 140 mm Water Cooling, 38 mm Radiator, PWM Pump, VRM Fan, for AMD/Intel sockets&lt;/td&gt; &lt;td align="left"&gt;$115.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Total&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;$14,493.00&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Any advice on the component choices or obvious oversights would be super appreciated. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cuuuuuooooongg"&gt; /u/cuuuuuooooongg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni87hl/feedback_on_trimmeddown_ai_workstation_build/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni87hl/feedback_on_trimmeddown_ai_workstation_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni87hl/feedback_on_trimmeddown_ai_workstation_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T04:44:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni815f</id>
    <title>Voice Assistant Running on a Raspyberry Pi</title>
    <updated>2025-09-16T04:34:43+00:00</updated>
    <author>
      <name>/u/localslm</name>
      <uri>https://old.reddit.com/user/localslm</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni815f/voice_assistant_running_on_a_raspyberry_pi/"&gt; &lt;img alt="Voice Assistant Running on a Raspyberry Pi" src="https://external-preview.redd.it/Y3RvYW1ibmRnZ3BmMWGGnl44cMWlAhwBPOVxHgzmQ7jmQEBvcqJECv2kUwPF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=052f93c040b3819082eb24799548218152addf9c" title="Voice Assistant Running on a Raspyberry Pi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, I just published a write-up on a project I’ve been working on: pi-assistant — a local, open-source voice assistant that runs fully offline on a Raspberry Pi 5.&lt;/p&gt; &lt;p&gt;Blog post: &lt;a href="https://alexfi.dev/blog/raspberry-pi-assistant"&gt;https://alexfi.dev/blog/raspberry-pi-assistant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/alexander-fischer/pi-assistant"&gt;https://github.com/alexander-fischer/pi-assistant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What it is&lt;/p&gt; &lt;p&gt;pi-assistant is a modular, tool-calling voice assistant that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Listens for a wake word (e.g., “Hey Jarvis”)&lt;/li&gt; &lt;li&gt;Transcribes your speech&lt;/li&gt; &lt;li&gt;Uses small LLMs to interpret commands and call tools (weather, Wikipedia, smart home)&lt;/li&gt; &lt;li&gt;Speaks the answer back to you —all without sending data to the cloud.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tech stack&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Wake word detection: openWakeWord&lt;/li&gt; &lt;li&gt;ASR: nemo-parakeet-tdt-0.6b-v2 / nvidia/canary-180m-flash&lt;/li&gt; &lt;li&gt;Function calling: Arch-Function 1.5B&lt;/li&gt; &lt;li&gt;Answer generation: Gemma3 1B&lt;/li&gt; &lt;li&gt;TTS: Piper&lt;/li&gt; &lt;li&gt;Hardware: Raspberry Pi 5 (16 GB), Jabra Speak 410&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can easily change the language models for a bigger hardware setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/localslm"&gt; /u/localslm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lh67dy5eggpf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni815f/voice_assistant_running_on_a_raspyberry_pi/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni815f/voice_assistant_running_on_a_raspyberry_pi/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T04:34:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nic6tz</id>
    <title>Why my server uses only 5-6% ram with f16 llama 8b model.</title>
    <updated>2025-09-16T08:52:52+00:00</updated>
    <author>
      <name>/u/Independent-Olive-66</name>
      <uri>https://old.reddit.com/user/Independent-Olive-66</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My server uses well cpu with my settings but ram usage is low. Under 10%. I have 48GB ddr3 RAM. Software is GPT4ALL and f16 model.&lt;/p&gt; &lt;p&gt;Context length: 8192&lt;/p&gt; &lt;p&gt;prompt batch size: 485&lt;/p&gt; &lt;p&gt;Max length: 12 250&lt;/p&gt; &lt;p&gt;Temperature: 0,9&lt;/p&gt; &lt;p&gt;20 value cpu threads.&lt;/p&gt; &lt;p&gt;What do you could chance values etc. I have tested different values.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Olive-66"&gt; /u/Independent-Olive-66 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nic6tz/why_my_server_uses_only_56_ram_with_f16_llama_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nic6tz/why_my_server_uses_only_56_ram_with_f16_llama_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nic6tz/why_my_server_uses_only_56_ram_with_f16_llama_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T08:52:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhz4dn</id>
    <title>Qwen-next - no gguf yet</title>
    <updated>2025-09-15T21:44:23+00:00</updated>
    <author>
      <name>/u/mgr2019x</name>
      <uri>https://old.reddit.com/user/mgr2019x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;does anyone know why llama.cpp has not implemented the new architecture yet?&lt;/p&gt; &lt;p&gt;I am not complaining, i am just wondering what the reason(s) might be. The feature request on github seems quite stuck to me.&lt;/p&gt; &lt;p&gt;Sadly there is no skill on my side, so i am not able to help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mgr2019x"&gt; /u/mgr2019x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhz4dn/qwennext_no_gguf_yet/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T21:44:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1nhtv5f</id>
    <title>Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info.</title>
    <updated>2025-09-15T18:27:22+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"&gt; &lt;img alt="Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info." src="https://preview.redd.it/5difgej3fdpf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f699faed3067a46c354771c3653e38f77a492e56" title="Some GPU (5090,4090,3090,A600) idle power consumption, headless on Linux (Fedora 42), and some undervolt/overclock info." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just an small post about some power consumption of those some GPUs if some people are interested.&lt;/p&gt; &lt;p&gt;As extra info, all the cards are both undervolted + power limited, but it shouldn't affect idle power consumption.&lt;/p&gt; &lt;p&gt;Undervolt was done with LACT, and they are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3090s: 1875Mhz max core clock, +150Mhz core clock offset, +1700Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;A6000: 1740Mhz max core clock, +150Mhz core clock offset, +2000 Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;4090 (1): 2850Mhz max core clock, +150Mhz core clock offset, +2700Mhz VRAM.&lt;/li&gt; &lt;li&gt;4090 (2): 2805Mhz max core clock, +180Mhz core clock offset, +1700Mhz VRAM offset.&lt;/li&gt; &lt;li&gt;5090s: 3010Mhz max core clock, +1000Mhz core clock offset, +4400Mhz VRAM offset.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If someone wants to know how to use LACT just let me know, but I basically use SDDM (sudo systemctl start sddm), LACT for the GUI, set the values and then run&lt;/p&gt; &lt;p&gt;sudo a (it does nothing, but helps for the next command)&lt;br /&gt; (echo suspend | sudo tee /proc/driver/nvidia/suspend ;echo resume | sudo tee /proc/driver/nvidia/suspend)&amp;amp;&lt;/p&gt; &lt;p&gt;Then run sudo systemctl stop sddm.&lt;/p&gt; &lt;p&gt;This mostly puts the 3090s, A6000 and 4090 (2) at 0.9V. 4090 (1) is at 0.915V, and 5090s are at 0.895V.&lt;/p&gt; &lt;p&gt;Also this offset in VRAM is MT/s basically, so on Windows comparatively, it is half of that (+1700Mhz = +850Mhz on MSI Afterburner, +1800 = +900, +2700 = 1350, +4400 = +2200)&lt;/p&gt; &lt;p&gt;EDIT: Just as an info, maybe (not) surprisingly, the GPUs that idle at the lower power are the most efficient.&lt;/p&gt; &lt;p&gt;I.e. 5090 2 is more efficient than 5090 0, or 4090 6 is more efficient than 4090 1.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5difgej3fdpf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nhtv5f/some_gpu_509040903090a600_idle_power_consumption/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T18:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nidixx</id>
    <title>Think twice before spending on GPU?</title>
    <updated>2025-09-16T10:16:07+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen team is shifting paradigm. Qwen Next is probably first big step of many that Qwen (and other chinese labs) are taking towards sparse models, because they do not have the required GPUs to train on.&lt;/p&gt; &lt;p&gt;10% of the training cost, 10x inference throughout, 512 experts, ultra long context (though not good enough yet).&lt;/p&gt; &lt;p&gt;They have a huge incentive to train this model further (on 36T tokens instead of 15T). They will probably release the final checkpoint in coming months or even weeks. Think of the electricity savings running (and on idle) a pretty capable model. We might be able to run a qwen 235B equivalent locally on a hardware under $1500. 128GB of RAM could be enough for the models this year and it's easily upgradable to 256GB for the next.&lt;/p&gt; &lt;p&gt;Wdyt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nidixx/think_twice_before_spending_on_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T10:16:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni5tq3</id>
    <title>AMD Max+ 395 with a 7900xtx as a little helper.</title>
    <updated>2025-09-16T02:41:02+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I finally got around to hooking up my 7900xtx to my GMK X2. A while back some people were interested in numbers for this so here are some numbers for OSS 120B. The big win is that adding the 7900xtx didn't make it slower and in fact made everything a little faster. My experience going multi-gpu is that there is a speed penalty. In this case adding the 7900xtx is effectively like just having another 24GB added to the 128GB.&lt;/p&gt; &lt;p&gt;I'll start with a baseline run in Vulkan on just the Max+ 395.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_vulkan: 0 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 | 473.93 ± 3.64 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 | 51.49 ± 0.03 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | pp512 @ d20000 | 261.49 ± 0.58 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 0 | tg128 @ d20000 | 41.03 ± 0.01 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here's a run in Vulkan split between the Max+ and the 7900xtx.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_vulkan: Found 2 Vulkan devices: ggml_vulkan: 0 = Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat ggml_vulkan: 1 = AMD Radeon Graphics (RADV GFX1151) (radv) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat | model | size | params | backend | ngl | fa | ts | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | ------------ | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | pp512 | 615.07 ± 3.11 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | tg128 | 53.08 ± 0.31 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | pp512 @ d20000 | 343.58 ± 5.11 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | Vulkan,RPC | 9999 | 1 | 36.00/64.00 | 0 | tg128 @ d20000 | 40.53 ± 0.13 | &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And lastly, here's a split ROCm run for comparison. Vulkan is still king. Particularly as the context grows.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;ggml_cuda_init: found 2 ROCm devices: Device 0: Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: no, Wave Size: 32 Device 1: AMD Radeon Graphics, gfx1151 (0x1151), VMM: no, Wave Size: 32 | model | size | params | backend | ngl | main_gpu | fa | ts | mmap | test | t/s | | ------------------------------ | ---------: | ---------: | ---------- | --: | ---------: | -: | ------------ | ---: | --------------: | -------------------: | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | pp512 | 566.14 ± 4.61 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | tg128 | 46.88 ± 0.15 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | pp512 @ d20000 | 397.01 ± 0.99 | | gpt-oss 120B MXFP4 MoE | 59.02 GiB | 116.83 B | ROCm,RPC | 9999 | 1 | 1 | 36.00/64.00 | 0 | tg128 @ d20000 | 18.09 ± 0.06 | &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5tq3/amd_max_395_with_a_7900xtx_as_a_little_helper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5tq3/amd_max_395_with_a_7900xtx_as_a_little_helper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5tq3/amd_max_395_with_a_7900xtx_as_a_little_helper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T02:41:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1nif778</id>
    <title>Unofficial VibeVoice finetuning code released!</title>
    <updated>2025-09-16T11:47:48+00:00</updated>
    <author>
      <name>/u/Downtown-Accident-87</name>
      <uri>https://old.reddit.com/user/Downtown-Accident-87</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just came across this on discord: &lt;a href="https://github.com/voicepowered-ai/VibeVoice-finetuning"&gt;https://github.com/voicepowered-ai/VibeVoice-finetuning&lt;/a&gt;&lt;br /&gt; I will try training a lora soon, I hope it works :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Downtown-Accident-87"&gt; /u/Downtown-Accident-87 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nif778/unofficial_vibevoice_finetuning_code_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:47:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni5bao</id>
    <title>Fully local data analysis assistant (plus new Model)</title>
    <updated>2025-09-16T02:16:04+00:00</updated>
    <author>
      <name>/u/mshintaro777</name>
      <uri>https://old.reddit.com/user/mshintaro777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5bao/fully_local_data_analysis_assistant_plus_new_model/"&gt; &lt;img alt="Fully local data analysis assistant (plus new Model)" src="https://preview.redd.it/ifula3tiqfpf1.gif?width=640&amp;amp;crop=smart&amp;amp;s=e334cd9bdf8f0d006bb34767d9f02bebb39206c1" title="Fully local data analysis assistant (plus new Model)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi community! Today I’m releasing an open-source, fully local data analysis assistant along with a lightweight LLM trained for it, called &lt;a href="https://quelmap.com"&gt;&lt;strong&gt;quelmap&lt;/strong&gt;&lt;/a&gt; and &lt;strong&gt;Lightning-4b&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;LLMs are amazing, but handing over all your data to a major LLM provider isn’t how it should be. Nowadays, data analysis has relied on huge context windows and very large models. Instead, we tried to see if we could cover most common analysis tasks with an efficient XML-based output format and GRPO training.&lt;/p&gt; &lt;p&gt;It even works smoothly on my &lt;strong&gt;M4 MacBook Air (16GB)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Basic Features&lt;/strong&gt;&lt;br /&gt; 📊 Data visualization&lt;br /&gt; 🚀 Table joins&lt;br /&gt; 📈 Run statistical tests&lt;br /&gt; 📂 Unlimited rows, analyze 30+ tables at once&lt;br /&gt; 🐍 Built-in Python sandbox&lt;br /&gt; 🦙 Ollama or LM Studio API integration&lt;/p&gt; &lt;p&gt;Lightning-4b is trained specifically for quelmap, and it’s been accurate and stable in generating structured outputs and Python code—more consistent than gpt-oss-120b or even Qwen3-235B in simple analysis tasks on quelmap. You can check the training details and performance here:&lt;br /&gt; 👉 &lt;a href="https://www.quelmap.com/lightning-4b/"&gt;https://www.quelmap.com/lightning-4b/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It’s not meant for writing complex research reports or high-level business advice like Gemini-DeepResearch. But I hope it can be a helpful tool for privacy-conscious analysts and beginners who just want to explore or analyze their data safely.&lt;/p&gt; &lt;p&gt;All details, installation instructions, and source code are here:&lt;br /&gt; 🔗 Github: &lt;a href="https://github.com/quelmap-inc/quelmap"&gt;https://github.com/quelmap-inc/quelmap&lt;/a&gt;&lt;br /&gt; 🔗 HuggingFace: &lt;a href="https://huggingface.co/quelmap/Lightning-4b"&gt;https://huggingface.co/quelmap/Lightning-4b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If people find this useful, I’d love to keep working on this project (agent mode, new models and more). Let me know what you think—I’d love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mshintaro777"&gt; /u/mshintaro777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ifula3tiqfpf1.gif"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5bao/fully_local_data_analysis_assistant_plus_new_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni5bao/fully_local_data_analysis_assistant_plus_new_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T02:16:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1ni2chb</id>
    <title>Qwen3-Next 80b MLX (Mac) runs on latest LM Studio</title>
    <updated>2025-09-15T23:59:55+00:00</updated>
    <author>
      <name>/u/jarec707</name>
      <uri>https://old.reddit.com/user/jarec707</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was excited to see this work. About 35 tps on my M1 Mac Studio 64 gb. Takes about 42 gb. Edit: &lt;a href="https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit"&gt;https://huggingface.co/mlx-community/Qwen3-Next-80B-A3B-Instruct-4bit&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jarec707"&gt; /u/jarec707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ni2chb/qwen3next_80b_mlx_mac_runs_on_latest_lm_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-15T23:59:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nifajh</id>
    <title>I bought a modded 4090 48GB in Shenzhen. This is my story.</title>
    <updated>2025-09-16T11:52:20+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt; &lt;img alt="I bought a modded 4090 48GB in Shenzhen. This is my story." src="https://external-preview.redd.it/1vD_R63iqu4vnM_qQf7pZNwXb9dy_UDc_Gl2j3LnTpU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e5102c5612db16c04c26877a1e72e86700648e25" title="I bought a modded 4090 48GB in Shenzhen. This is my story." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe"&gt;https://preview.redd.it/ume4fe3jmipf1.jpg?width=4032&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=9aa908d45211be937b291377b1c495c9917834fe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A few years ago, before ChatGPT became popular, I managed to score a Tesla P40 on eBay for around $150 shipped. With a few tweaks, I installed it in a Supermicro chassis. At the time, I was mostly working on video compression and simulation. It worked, but the card consistently climbed to 85°C.&lt;/p&gt; &lt;p&gt;When DeepSeek was released, I was impressed and installed Ollama in a container. With 24GB of VRAM, it worked—but slowly. After trying Stable Diffusion, it became clear that an upgrade was necessary.&lt;/p&gt; &lt;p&gt;The main issue was finding a modern GPU that could actually &lt;strong&gt;fit&lt;/strong&gt; in the server chassis. Standard 4090/5090 cards are designed for desktops: they're too large, and the power plug is inconveniently placed on top. After watching the LTT video featuring a modded 4090 with 48GB (and a follow-up from Gamers Nexus), I started searching the only place I knew might have one: Alibaba.com.&lt;/p&gt; &lt;p&gt;I contacted a seller and got a quote: &lt;strong&gt;CNY 22,900&lt;/strong&gt;. Pricey, but cheaper than expected. However, Alibaba enforces VAT collection, and I’ve had bad experiences with DHL—there was a non-zero chance I’d be charged twice for taxes. I was already over €700 in taxes and fees.&lt;/p&gt; &lt;p&gt;Just for fun, I checked &lt;a href="http://Trip.com"&gt;Trip.com&lt;/a&gt; and realized that for the same amount of money, I could fly to Hong Kong and back, with a few days to explore. After confirming with the seller that they’d meet me at their business location, I booked a flight and an Airbnb in Hong Kong.&lt;/p&gt; &lt;p&gt;For context, I don’t speak Chinese at all. Finding the place using a Chinese address was tricky. Google Maps is useless in China, Apple Maps gave some clues, and Baidu Maps was beyond my skill level. With a little help from DeepSeek, I decoded the address and located the place in an industrial estate outside the city center. Thanks to Shenzhen’s extensive metro network, I didn’t need a taxi.&lt;/p&gt; &lt;p&gt;After arriving, the manager congratulated me for being the first foreigner to find them unassisted. I was given the card from a large batch—they’re clearly producing these in volume at a factory elsewhere in town (I was proudly shown videos of the assembly line). I asked them to retest the card so I could verify its authenticity.&lt;/p&gt; &lt;p&gt;During the office tour, it was clear that their next frontier is repurposing old mining cards. I saw a large collection of NVIDIA Ampere mining GPUs. I was also told that modded 5090s with over 96GB of VRAM are in development.&lt;/p&gt; &lt;p&gt;After the test was completed, I paid in cash (a &lt;em&gt;lot&lt;/em&gt; of banknotes!) and returned to Hong Kong with my new purchase.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nifajh/i_bought_a_modded_4090_48gb_in_shenzhen_this_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-16T11:52:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
