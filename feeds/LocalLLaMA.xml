<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-01T15:38:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pbascu</id>
    <title>[Release] Vidi2 ‚Äî ByteDance‚Äôs LMM for video understanding &amp; creation (STG + temporal retrieval)</title>
    <updated>2025-12-01T11:51:11+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given a text query, &lt;strong&gt;Vidi2&lt;/strong&gt; finds the right timestamps &lt;em&gt;and&lt;/em&gt; object boxes (‚Äútubes‚Äù), with solid temporal retrieval and basic video QA. Repo ships the &lt;strong&gt;VUE-STG&lt;/strong&gt; and &lt;strong&gt;VUE-TR-V2&lt;/strong&gt; benchmarks + eval scripts; public demo is ‚Äúcoming very soon.‚Äù&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What it does: fine-grained &lt;strong&gt;spatio-temporal grounding&lt;/strong&gt; + &lt;strong&gt;temporal retrieval&lt;/strong&gt;, extended to &lt;strong&gt;video QA&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;What‚Äôs in the repo: instructions to run &lt;strong&gt;STG&lt;/strong&gt; and &lt;strong&gt;TR-V2&lt;/strong&gt; evaluations locally. &lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/bytedance/vidi"&gt;GitHub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbascu/release_vidi2_bytedances_lmm_for_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbascu/release_vidi2_bytedances_lmm_for_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbascu/release_vidi2_bytedances_lmm_for_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbcelg</id>
    <title>[Toolkit] TinyLlama Fine-Tuning + RAG Lab (Full FT / LoRA / QLoRA | T4-friendly | Unified pipeline)</title>
    <updated>2025-12-01T13:11:46+00:00</updated>
    <author>
      <name>/u/sai_ai_lab</name>
      <uri>https://old.reddit.com/user/sai_ai_lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released an early (and still very much WIP) version of &lt;strong&gt;FT-Lab&lt;/strong&gt;, a lightweight toolkit for fine-tuning and retrieval-augmented generation (RAG) with TinyLlama.&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Fine-Tuning / LoRA / QLoRA support for TinyLlama&lt;/li&gt; &lt;li&gt;Unified preprocessing + tokenizer setup&lt;/li&gt; &lt;li&gt;RAG workflows with LlamaIndex + LangChain&lt;/li&gt; &lt;li&gt;Retrieval metrics: &lt;code&gt;recall@k&lt;/code&gt;, &lt;code&gt;precision@k&lt;/code&gt;, &lt;code&gt;hit_rate@k&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ &lt;strong&gt;Goal:&lt;/strong&gt; Make TinyLlama fine-tuning and evaluation reproducible and small-GPU-friendly.&lt;/p&gt; &lt;p&gt;üß™ Still a work in progress ‚Äî some parts might not run yet, but feedback is super welcome!&lt;/p&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/REICHIYAN/ft_lab"&gt;https://github.com/REICHIYAN/ft_lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sai_ai_lab"&gt; /u/sai_ai_lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcelg/toolkit_tinyllama_finetuning_rag_lab_full_ft_lora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcelg/toolkit_tinyllama_finetuning_rag_lab_full_ft_lora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcelg/toolkit_tinyllama_finetuning_rag_lab_full_ft_lora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbes68</id>
    <title>I made an architecture for fixing Mamba's long-context forgetting</title>
    <updated>2025-12-01T14:53:49+00:00</updated>
    <author>
      <name>/u/InstructionOk9108</name>
      <uri>https://old.reddit.com/user/InstructionOk9108</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"&gt; &lt;img alt="I made an architecture for fixing Mamba's long-context forgetting" src="https://b.thumbs.redditmedia.com/NAAVJAXhlCgY4eFntwhC-Ka4uVFkZ1Q1JM7ABZgIAxI.jpg" title="I made an architecture for fixing Mamba's long-context forgetting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The result&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yo8w4021wl4g1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45de5d9a738065409294056902ce2b19a0b54f8c"&gt;https://preview.redd.it/yo8w4021wl4g1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45de5d9a738065409294056902ce2b19a0b54f8c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark (NIAH @ 32k Context):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Baseline:&lt;/strong&gt; Mamba-2 (130M) ‚Üí Starts forgetting at ~29k (83.3% acc).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mine:&lt;/strong&gt; Spectrum-State (85M) ‚Üí &lt;strong&gt;100% Recall&lt;/strong&gt; across the board.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My model is &lt;strong&gt;smaller&lt;/strong&gt; (85M vs 130M) and was trained on only &lt;strong&gt;1/7th of the data&lt;/strong&gt; used for Mamba-2.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jaeha_han_/status/1995481225755038088"&gt;There is a detailed infos on my X.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InstructionOk9108"&gt; /u/InstructionOk9108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T14:53:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb37b7</id>
    <title>Looking for High-Quality Open-Source Local TTS That‚Äôs Faster Than IndexTTS2</title>
    <updated>2025-12-01T04:21:24+00:00</updated>
    <author>
      <name>/u/TomNaughtyy</name>
      <uri>https://old.reddit.com/user/TomNaughtyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me and my cousin have been using IndexTTS2 for a while and really like the voice quality, it sounds natural and expressive. The only issue is that it‚Äôs slow. He‚Äôs getting around 1.6 RTF on his 3090, which makes it hard to generate longer audio efficiently (we work with long audio, not real-time use).&lt;/p&gt; &lt;p&gt;We‚Äôve also tried Kokoro TTS and CosyVoice 2. Kokoro is super fast, but most of the voices sound too synthetic or ‚ÄúAI-like‚Äù for our needs. One voice we actually liked was ‚ÄúNicole‚Äù in Kokoro, it has a more natural and calm tone that works well for us. CosyVoice 2 had better expressiveness and sounded promising, but it had a habit of changing words or pronouncing them weirdly, which broke the consistency.&lt;/p&gt; &lt;p&gt;We‚Äôre only interested in open-source models. No commercial or cloud APIs.&lt;/p&gt; &lt;p&gt;A few things to note: We‚Äôre not planning to use emotion vectors, style tokens, or any prompt engineering tricks, just clean, straightforward narration. We‚Äôre on strong hardware (3090 and 4090), so GPU resources aren‚Äôt a problem. Just want something with good voice quality that runs faster than IndexTTS2 and ideally has at least one solid voice that sounds natural.&lt;/p&gt; &lt;p&gt;Any models or voices you recommend?&lt;br /&gt; Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomNaughtyy"&gt; /u/TomNaughtyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb37b7/looking_for_highquality_opensource_local_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb37b7/looking_for_highquality_opensource_local_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb37b7/looking_for_highquality_opensource_local_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T04:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pams8b</id>
    <title>nvidia/Orchestrator-8B ¬∑ Hugging Face</title>
    <updated>2025-11-30T16:42:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/"&gt; &lt;img alt="nvidia/Orchestrator-8B ¬∑ Hugging Face" src="https://external-preview.redd.it/Havs9Ap5icNaW5b7G-LM12y5Y2tjxzsA0o3nV6l5p6A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73e0546e48c2735a0477acb6fffa02a0e545e6c6" title="nvidia/Orchestrator-8B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Orchestrator-8B is a state-of-the-art 8B parameter orchestration model designed to solve complex, multi-turn agentic tasks by coordinating a diverse set of expert models and tools.&lt;/p&gt; &lt;p&gt;On the Humanity's Last Exam (HLE) benchmark, ToolOrchestrator-8B achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being approximately 2.5x more efficient.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/nvidia_Orchestrator-8B-GGUF"&gt;https://huggingface.co/bartowski/nvidia_Orchestrator-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Orchestrator-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T16:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb4pbm</id>
    <title>[2511.23404] LFM2 Technical Report</title>
    <updated>2025-12-01T05:39:01+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.23404"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb4pbm/251123404_lfm2_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb4pbm/251123404_lfm2_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T05:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pavof6</id>
    <title>[Ministral 3] Add ministral 3 - Pull Request #42498 ¬∑ huggingface/transformers</title>
    <updated>2025-11-30T22:36:36+00:00</updated>
    <author>
      <name>/u/bratao</name>
      <uri>https://old.reddit.com/user/bratao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pavof6/ministral_3_add_ministral_3_pull_request_42498/"&gt; &lt;img alt="[Ministral 3] Add ministral 3 - Pull Request #42498 ¬∑ huggingface/transformers" src="https://external-preview.redd.it/kvAOuOuPU1hgF-Ezo21UQUe0ThkEwS_Wm4nwhMo6c8c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bbd39529d0c356901aca5c2c1d85379cf4fd779" title="[Ministral 3] Add ministral 3 - Pull Request #42498 ¬∑ huggingface/transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bratao"&gt; /u/bratao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/42498"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pavof6/ministral_3_add_ministral_3_pull_request_42498/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pavof6/ministral_3_add_ministral_3_pull_request_42498/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T22:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbabtp</id>
    <title>[Tool] Local video-to-text backend + OpenWebUI tool (scene cuts + Whisper + Qwen3-VL, no API keys)</title>
    <updated>2025-12-01T11:24:33+00:00</updated>
    <author>
      <name>/u/Longjumping-Elk-7756</name>
      <uri>https://old.reddit.com/user/Longjumping-Elk-7756</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbabtp/tool_local_videototext_backend_openwebui_tool/"&gt; &lt;img alt="[Tool] Local video-to-text backend + OpenWebUI tool (scene cuts + Whisper + Qwen3-VL, no API keys)" src="https://a.thumbs.redditmedia.com/SUVQWEX05RkP-H877NA8zPH_4AQ3DA1lJlUcUO_cLj4.jpg" title="[Tool] Local video-to-text backend + OpenWebUI tool (scene cuts + Whisper + Qwen3-VL, no API keys)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lighfbltsk4g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f4c3b29ee833229906aaa8af359b1102650869a"&gt;https://preview.redd.it/lighfbltsk4g1.png?width=1536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f4c3b29ee833229906aaa8af359b1102650869a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wanted my local LLM to actually &lt;em&gt;understand&lt;/em&gt; videos, not just read my prompt.&lt;/p&gt; &lt;p&gt;So I built a small &lt;strong&gt;local-first ‚ÄúVideoContext Engine‚Äù&lt;/strong&gt; + an &lt;strong&gt;OpenWebUI tool&lt;/strong&gt; that turns any YouTube link (or local video) into &lt;em&gt;structured text&lt;/em&gt; your model can work with.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/dolphin-creator/VideoContext-Engine"&gt;&lt;strong&gt;https://github.com/dolphin-creator/VideoContext-Engine&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs a FastAPI microservice that:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üí• &lt;strong&gt;Cuts the video into scenes&lt;/strong&gt; (HSV-based visual change detection)&lt;/li&gt; &lt;li&gt;üéôÔ∏è &lt;strong&gt;Transcribes audio&lt;/strong&gt; with local Whisper (time-aligned, grouped by scene)&lt;/li&gt; &lt;li&gt;üëÅÔ∏è &lt;strong&gt;Analyzes visuals per scene&lt;/strong&gt; with Qwen3-VL&lt;/li&gt; &lt;li&gt;üß† &lt;strong&gt;Builds a global summary&lt;/strong&gt; over all scenes&lt;/li&gt; &lt;li&gt;üßæ Returns either: &lt;ul&gt; &lt;li&gt;structured &lt;strong&gt;JSON&lt;/strong&gt; (scene-by-scene context), or&lt;/li&gt; &lt;li&gt;a clean &lt;strong&gt;TXT report&lt;/strong&gt; (for direct reading / LLM input)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The idea is to use it as a &lt;strong&gt;backend&lt;/strong&gt; that any local LLM / agent / RAG pipeline can call.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech stack&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Backend:&lt;/strong&gt; FastAPI + Uvicorn&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Video I/O:&lt;/strong&gt; ffmpeg + yt-dlp&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio:&lt;/strong&gt; openai-whisper (runs local, any size: tiny/base/small/‚Ä¶)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision-Language:&lt;/strong&gt; &lt;strong&gt;Qwen3-VL 2B&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;macOS ‚Üí &lt;strong&gt;MLX&lt;/strong&gt; backend (mlx-community/Qwen3-VL-2B-Instruct-4bit)&lt;/li&gt; &lt;li&gt;Windows / Linux ‚Üí &lt;strong&gt;llama.cpp&lt;/strong&gt; backend (GGUF)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Modes RAM:&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;ram- ‚Üí load/unload models per request (safe for low RAM)&lt;/li&gt; &lt;li&gt;ram+ ‚Üí keep Whisper + VLM in RAM (much faster if you have ‚â•16GB)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything runs &lt;strong&gt;fully locally&lt;/strong&gt; ‚Äì no external LLM / API keys.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What the engine outputs&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For each &lt;strong&gt;scene&lt;/strong&gt;, you get:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;audio_transcript (Whisper)&lt;/li&gt; &lt;li&gt;optional audio_features: &lt;ul&gt; &lt;li&gt;speech duration&lt;/li&gt; &lt;li&gt;speech/silence ratio&lt;/li&gt; &lt;li&gt;words-per-minute&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;visual_description (Qwen3-VL)&lt;/li&gt; &lt;li&gt;visual_tags (people count, place type, main action, tone, movement level)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Plus a &lt;strong&gt;global summary&lt;/strong&gt; in meta.global_summary, and full timing info:&lt;/p&gt; &lt;p&gt;&amp;quot;timings&amp;quot;: {&lt;/p&gt; &lt;p&gt; &amp;quot;total_process_time&amp;quot;: ...,&lt;/p&gt; &lt;p&gt; &amp;quot;whisper&amp;quot;: { &amp;quot;load_time&amp;quot;: ..., &amp;quot;inference_time&amp;quot;: ... },&lt;/p&gt; &lt;p&gt; &amp;quot;vlm&amp;quot;: { &amp;quot;load_time&amp;quot;: ..., &amp;quot;inference_time&amp;quot;: ... },&lt;/p&gt; &lt;p&gt; &amp;quot;ram_mode&amp;quot;: &amp;quot;ram+&amp;quot;&lt;/p&gt; &lt;p&gt;}&lt;/p&gt; &lt;p&gt;This makes it pretty friendly for RAG / agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;you can index &lt;strong&gt;scenes&lt;/strong&gt; as documents,&lt;/li&gt; &lt;li&gt;or flatten everything to Markdown and feed it to your model.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;OpenWebUI tool: ‚ÄúContextVideo (Local VideoContext Engine)‚Äù&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I also added an example &lt;strong&gt;OpenWebUI tool&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;examples/openwebui/contextvideo_tool.py&lt;/p&gt; &lt;p&gt;What it does:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Looks at the recent chat messages and grabs the &lt;strong&gt;last video URL&lt;/strong&gt; (YouTube or direct).&lt;/li&gt; &lt;li&gt;Calls POST /api/v1/analyze with response_format=text.&lt;/li&gt; &lt;li&gt;Injects the full report back into the chat as context.&lt;/li&gt; &lt;li&gt;Asks the model to: &lt;ul&gt; &lt;li&gt;summarize the video, or&lt;/li&gt; &lt;li&gt;answer a specific question you passed as instruction.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;In OpenWebUI:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Workspace ‚Üí Tools ‚Üí New Tool ‚Üí paste contextvideo_tool.py ‚Üí save&lt;/li&gt; &lt;li&gt;Workspace ‚Üí Models ‚Üí (your model) ‚Üí Tools ‚Üí enable &lt;strong&gt;ContextVideo (Local VideoContext Engine)&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Then in a chat you can just do:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;‚ÄúUse contextvideo and summarize this:&lt;/em&gt; &lt;a href="https://www.youtube.com/%E2%80%A6%E2%80%9D"&gt;&lt;em&gt;https://www.youtube.com/‚Ä¶‚Äù&lt;/em&gt;&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;The model gets a full scene-by-scene report and answers based on it.&lt;/p&gt; &lt;p&gt;üí° &lt;strong&gt;Tip:&lt;/strong&gt; In a new chat, I recommend ‚Äúwarming up‚Äù the model first with a short message like hello / bonjour before triggering the tool. This avoids some first-request weirdness.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Language behavior&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The engine follows the &lt;strong&gt;language of your prompts&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;visual_user_prompt ‚Üí controls language/style of scene descriptions&lt;/li&gt; &lt;li&gt;summary_user_prompt ‚Üí controls language/style of the global summary&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Examples:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;If you prompt in French ‚Üí everything (transcript, descriptions, summary) is in French.&lt;/li&gt; &lt;li&gt;If you prompt in English ‚Üí output is in English, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;In the OpenWebUI tool, there are two valves:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;scene_prompt ‚Äì default: French instruction with ‚Äúmax 80 words‚Äù&lt;/li&gt; &lt;li&gt;summary_prompt ‚Äì default: French instruction with ‚Äúmax 120 words‚Äù&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You‚Äôre supposed to edit them in &lt;strong&gt;your language&lt;/strong&gt; (EN/ES/IT/‚Ä¶) and ideally keep the word limit under ~¬Ω of the vlm_max_tokens_* to avoid truncation.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Platform notes (important)&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;macOS (Apple Silicon)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Primary target, MLX backend, used and tested daily.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;‚ö†Ô∏è &lt;strong&gt;Windows / Linux (llama.cpp)&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Implemented, but &lt;strong&gt;still experimental&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Default context size is n_ctx=4096 (safe on 8GB).&lt;/li&gt; &lt;li&gt;For long videos (&amp;gt;15‚Äì20 min), global summary can be truncated.&lt;/li&gt; &lt;li&gt;If you have 16GB+ RAM you can bump it in VideoContextEngine_v3.19.py in LlamaCppEngine:&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;n_ctx = 16384 # or 32768 for very long videos&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Looking for testers + feedback on these platforms.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The project is marked as &lt;strong&gt;Public Beta&lt;/strong&gt; ‚Äì prompts, defaults and API params may still evolve.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why I‚Äôm posting this here&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I‚Äôd love feedback from the LocalLLaMA crowd on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;How you‚Äôd integrate this&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;RAG over video scenes?&lt;/li&gt; &lt;li&gt;Agents that ‚Äúwatch‚Äù content before acting?&lt;/li&gt; &lt;li&gt;Monitoring / analytics dashboards?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Suggestions for &lt;strong&gt;alternative VLMs&lt;/strong&gt; that are good for &lt;em&gt;captioning&lt;/em&gt;: &lt;ul&gt; &lt;li&gt;Florence-2, Llava variants, etc.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Real-world tests on: &lt;ul&gt; &lt;li&gt;Windows / Linux setups&lt;/li&gt; &lt;li&gt;Larger context settings (n_ctx)&lt;/li&gt; &lt;li&gt;Different local models in OpenWebUI (7B / 13B / 30B‚Ä¶) on top of the reports.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If this is useful, I‚Äôm happy to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;add more client examples (plain Python, LM Studio, other UIs),&lt;/li&gt; &lt;li&gt;experiment with more ‚Äúcaptioning-oriented‚Äù VLMs,&lt;/li&gt; &lt;li&gt;or add a lighter ‚Äútags-only‚Äù mode for pure search/RAG usage.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo again: &lt;a href="https://github.com/dolphin-creator/VideoContext-Engine"&gt;&lt;strong&gt;https://github.com/dolphin-creator/VideoContext-Engine&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions &amp;amp; iterate based on your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping-Elk-7756"&gt; /u/Longjumping-Elk-7756 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbabtp/tool_local_videototext_backend_openwebui_tool/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbabtp/tool_local_videototext_backend_openwebui_tool/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbabtp/tool_local_videototext_backend_openwebui_tool/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:24:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1paw8u1</id>
    <title>Winter LLM</title>
    <updated>2025-11-30T23:00:42+00:00</updated>
    <author>
      <name>/u/aziham</name>
      <uri>https://old.reddit.com/user/aziham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paw8u1/winter_llm/"&gt; &lt;img alt="Winter LLM" src="https://preview.redd.it/agojdb246h4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf041f02afcc034c276829a48360b9ceb30e6b70" title="Winter LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aziham"&gt; /u/aziham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/agojdb246h4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paw8u1/winter_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paw8u1/winter_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T23:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbeg6g</id>
    <title>We built a 1 and 3B local Git agents that turns plain English into correct git commands. They matche GPT-OSS 120B accuracy (gitara)</title>
    <updated>2025-12-01T14:40:08+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbeg6g/we_built_a_1_and_3b_local_git_agents_that_turns/"&gt; &lt;img alt="We built a 1 and 3B local Git agents that turns plain English into correct git commands. They matche GPT-OSS 120B accuracy (gitara)" src="https://preview.redd.it/5321sl8otl4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0694f90bbe4e81187886cd5619b3f07bf3a56d1a" title="We built a 1 and 3B local Git agents that turns plain English into correct git commands. They matche GPT-OSS 120B accuracy (gitara)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have been working on tool calling SLMs and how to get the most out of a small model. One of the use cases turned out to be very useful and we hope to get your feedback. You can find more information on the &lt;a href="https://github.com/distil-labs/distil-gitara"&gt;github page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We trained a &lt;strong&gt;3B function-calling model&lt;/strong&gt; (‚ÄúGitara‚Äù) that converts natural language ‚Üí valid git commands, with accuracy nearly identical to a &lt;strong&gt;120B teacher model&lt;/strong&gt;, that can run on your laptop.&lt;/p&gt; &lt;p&gt;Just type: &lt;em&gt;‚Äúundo the last commit but keep the changes‚Äù ‚Üí&lt;/em&gt; you get: &lt;em&gt;&lt;code&gt;git reset --soft HEAD~1&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;Why we built it&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;We forget to use git flags correctly all the time, so we thought the chance is you do too.&lt;/p&gt; &lt;p&gt;Small models are perfect for &lt;strong&gt;structured tool-calling tasks&lt;/strong&gt;, so this became our testbed.&lt;/p&gt; &lt;p&gt;Our goals:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; (Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;max. 2-second responses&lt;/strong&gt; on a laptop&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured JSON output ‚Üí deterministic git commands&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Match the accuracy of a large model&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Model link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-OSS 120B (teacher)&lt;/td&gt; &lt;td&gt;120B&lt;/td&gt; &lt;td&gt;0.92 ¬± 0.02&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Llama 3.2 3B Instruct (fine-tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;3B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.92 ¬± 0.01&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-gitara-v2-Llama-3.2-3B-Instruct"&gt;huggingface&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama 3.2 1B (fine-tuned)&lt;/td&gt; &lt;td&gt;1B&lt;/td&gt; &lt;td&gt;0.90 ¬± 0.01&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-gitara-v2-Llama-3.2-1B-Instruct"&gt;huggingface&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama 3.2 3B (base)&lt;/td&gt; &lt;td&gt;3B&lt;/td&gt; &lt;td&gt;0.12 ¬± 0.05&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The fine-tuned &lt;strong&gt;3B model matches the 120B model&lt;/strong&gt; on tool-calling correctness.&lt;/p&gt; &lt;p&gt;Responds &lt;strong&gt;&amp;lt;2 seconds&lt;/strong&gt; on a M4 MacBook Pro.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;``` ‚Äúwhat's in the latest stash, show diff‚Äù ‚Üí git stash show --patch&lt;/p&gt; &lt;p&gt;‚Äúpush feature-x to origin, override any changes there‚Äù ‚Üí git push origin feature-x --force --set-upstream&lt;/p&gt; &lt;p&gt;‚Äúundo last commit but keep the changes‚Äù ‚Üí git reset --soft HEAD~1&lt;/p&gt; &lt;p&gt;‚Äúshow 8 commits as a graph‚Äù ‚Üí git log -n 8 --graph&lt;/p&gt; &lt;p&gt;‚Äúmerge vendor branch preferring ours‚Äù ‚Üí git merge vendor --strategy ours&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The model &lt;strong&gt;prints the git command but does NOT execute it&lt;/strong&gt;, by design.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What‚Äôs under the hood&lt;/h2&gt; &lt;p&gt;From the README (summarized):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We defined all git actions as &lt;strong&gt;OpenAI function-calling schemas&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Created ~100 realistic seed examples&lt;/li&gt; &lt;li&gt;Generated &lt;strong&gt;10,000 validated synthetic examples&lt;/strong&gt; via a teacher model&lt;/li&gt; &lt;li&gt;Fine-tuned Llama 3.2 3B with LoRA&lt;/li&gt; &lt;li&gt;Evaluated by matching generated functions to ground truth&lt;/li&gt; &lt;li&gt;Accuracy matched the teacher at ~0.92&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;Want to try it?&lt;/h2&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-gitara"&gt;https://github.com/distil-labs/distil-gitara&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick start (Ollama):&lt;/p&gt; &lt;p&gt;```bash hf download distil-labs/Llama-3_2-gitara-3B --local-dir distil-model cd distil-model ollama create gitara -f Modelfile python gitara.py &amp;quot;your git question here&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Discussion&lt;/h2&gt; &lt;p&gt;Curious to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you using local models in your workflows?&lt;/li&gt; &lt;li&gt;Anyone else experimenting with structured-output SLMs for local workflows?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5321sl8otl4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbeg6g/we_built_a_1_and_3b_local_git_agents_that_turns/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbeg6g/we_built_a_1_and_3b_local_git_agents_that_turns/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T14:40:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb8v2u</id>
    <title>Karpathy/reader3 ‚Äî self-hosted EPUB reader for LLM-assisted reading</title>
    <updated>2025-12-01T09:56:10+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"&gt; &lt;img alt="Karpathy/reader3 ‚Äî self-hosted EPUB reader for LLM-assisted reading" src="https://a.thumbs.redditmedia.com/qKEh6RaOA6QT-c0IcO3cDj9L25ecEnI9yuqdotqbo_4.jpg" title="Karpathy/reader3 ‚Äî self-hosted EPUB reader for LLM-assisted reading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vu1hbkj1fk4g1.png?width=962&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a91cc3e4a21891e1eb54d4ff5f4199fdf1505265"&gt;https://preview.redd.it/vu1hbkj1fk4g1.png?width=962&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a91cc3e4a21891e1eb54d4ff5f4199fdf1505265&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny, demo-grade EPUB reader that splits a book into &lt;strong&gt;chapters&lt;/strong&gt; and serves &lt;strong&gt;local static pages&lt;/strong&gt;, so you can copy a chapter straight into an LLM for co-reading/Q&amp;amp;A.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chapter-by-chapter&lt;/strong&gt; view for easy chunking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight self-hosting:&lt;/strong&gt; single-file script + simple static server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-friendly workflow:&lt;/strong&gt; copy/paste each chapter to your model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-source (MIT):&lt;/strong&gt; learning example/personal tool; no long-term maintenance promised&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="http://github.com/karpathy/reader3"&gt;&lt;code&gt;github.com/karpathy/reader3&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T09:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1parhxk</id>
    <title>I mapped how language models decide when a pile of sand becomes a ‚Äúheap‚Äù</title>
    <updated>2025-11-30T19:46:42+00:00</updated>
    <author>
      <name>/u/Specialist_Bad_4465</name>
      <uri>https://old.reddit.com/user/Specialist_Bad_4465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1parhxk/i_mapped_how_language_models_decide_when_a_pile/"&gt; &lt;img alt="I mapped how language models decide when a pile of sand becomes a ‚Äúheap‚Äù" src="https://preview.redd.it/763n9ju87g4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1534d6f16651204ad4fafddb5c126adb160d5ebf" title="I mapped how language models decide when a pile of sand becomes a ‚Äúheap‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This chart compares how three open-weight language models decide when a pile of sand becomes a ‚Äúheap.‚Äù&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;X-axis:&lt;/strong&gt; number of grains of sand, on a log scale from 1 to 100,000,000.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Y-axis:&lt;/strong&gt; probability that the model answers ‚ÄúYes, this is a heap‚Äù given that many grains, P(Yes | n).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What each line shows:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cyan ‚Äì Mistral-7B:&lt;/strong&gt; starts around 0.25 at 1 grain and climbs smoothly to ~0.8 by 100M grains.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Magenta ‚Äì DeepSeek-7B:&lt;/strong&gt; similar S-shape but consistently lower than Mistral; it crosses the 0.5 line later, so it‚Äôs ‚Äústricter‚Äù about when a heap begins.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yellow ‚Äì Llama-3-8B:&lt;/strong&gt; stays noisy in roughly the 0.35‚Äì0.6 band across almost the entire range, from 1 grain to 100M, rarely committing strongly either way.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The shaded band between 0.4 and 0.6 highlights the ‚Äúborderline‚Äù region where the models are most uncertain about heapness.&lt;/p&gt; &lt;p&gt;All three curves come from the same basic setup:&lt;br /&gt; I give the model a few examples (1‚Äì2 grains ‚Üí ‚ÄúNo‚Äù, 999,999‚Äì1,000,000 grains ‚Üí ‚ÄúYes‚Äù), then ask for many different values of &lt;code&gt;n&lt;/code&gt;: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚ÄúThere is a pile of n grains of sand. Is this a heap? Answer yes or no.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;For each &lt;code&gt;n&lt;/code&gt;, I plot the softmax probability on the ‚ÄúYes‚Äù token.&lt;/p&gt; &lt;p&gt;&lt;a href="https://joshfonseca.com/blogs/sorites-paradox"&gt;Full writeup with more charts and prompt details is here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Bad_4465"&gt; /u/Specialist_Bad_4465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/763n9ju87g4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1parhxk/i_mapped_how_language_models_decide_when_a_pile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1parhxk/i_mapped_how_language_models_decide_when_a_pile/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T19:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbfisv</id>
    <title>Stable-diffusion.cpp now supports Z-image</title>
    <updated>2025-12-01T15:22:37+00:00</updated>
    <author>
      <name>/u/Languages_Learner</name>
      <uri>https://old.reddit.com/user/Languages_Learner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/leejet/stable-diffusion.cpp/releases/tag/master-385-34a6fd4"&gt;Release master-385-34a6fd4 ¬∑ leejet/stable-diffusion.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Languages_Learner"&gt; /u/Languages_Learner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T15:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbag5i</id>
    <title>I wrote a kernel that makes sparse LLMs faster and smaller on consumer GPUs even at low sparsity.</title>
    <updated>2025-12-01T11:31:31+00:00</updated>
    <author>
      <name>/u/vlejd</name>
      <uri>https://old.reddit.com/user/vlejd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"&gt; &lt;img alt="I wrote a kernel that makes sparse LLMs faster and smaller on consumer GPUs even at low sparsity." src="https://b.thumbs.redditmedia.com/97VGlNQjo_fRFkrwIo-Ies8AASHz1A354I-Gvc-9uNU.jpg" title="I wrote a kernel that makes sparse LLMs faster and smaller on consumer GPUs even at low sparsity." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pruning LLMs hind of sucks. On GPUs, unstructured sparsity doesn‚Äôt really help. You don‚Äôt get memory savings, and you don‚Äôt get speed up. You always needed very high sparsity (the model breaks), some structure (2:4: very limiting, and the model is worse) or special hardware (good luck).&lt;/p&gt; &lt;p&gt;I built a new matrix format + GPU kernel for sparse matrix-vector multiplication that unlocks the benefits of pruning on real hardware. I‚Äôm calling it MACKO-SpMV, and it has no special GPU instructions, no fixed block patterns, no giant performance drop, no precomputation and no autotuning. Just: prune, store the weights, run fast.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vmvsr577qk4g1.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e261ccf86c0d0ec9c6814b693cf729c746e1f7b0"&gt;https://preview.redd.it/vmvsr577qk4g1.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e261ccf86c0d0ec9c6814b693cf729c746e1f7b0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What this means in practice:&lt;br /&gt; - Noticeable memory reduction even at low sparsity&lt;br /&gt; - Speed-ups on standard consumer GPUs (no tensor core magic needed). Tested with NVIDIA 2080, 3090, 4090.&lt;br /&gt; - Works with any model that has linear layers (basically all LLMs and much more).&lt;br /&gt; - Want to run 7b model on 8GB memory? Well, prune to 60% sparsity and you will even get a 2x speedup. &lt;/p&gt; &lt;p&gt;Quick caveat1: For prefill, it only gives you memory reduction without the speed-up. For generation, you get both the speed-up and memory reduction. Happy to discuss the technical reasons. &lt;/p&gt; &lt;p&gt;Quick caveat2: This is not a post about quality of the model. Pruning methods are advancing rapidly, and I hope this will help the field to catch up/outperform quantization.&lt;/p&gt; &lt;p&gt;Fully open source, still mainly academic.&lt;/p&gt; &lt;p&gt;If you care about local LLMs, this finally makes aggressive pruning a practical tool instead of a research curiosity. You can strip down a model and actually benefit from it at runtime.&lt;/p&gt; &lt;p&gt;Blog (high-level explanation): &lt;a href="https://www.grizzlytech.dev/blog/macko-spmv"&gt;https://www.grizzlytech.dev/blog/macko-spmv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper (details on the format/algorithm): &lt;a href="https://arxiv.org/pdf/2511.13061"&gt;https://arxiv.org/pdf/2511.13061&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code (open-source implementation): &lt;a href="http://github.com/vlejd/macko_spmv"&gt;github.com/vlejd/macko_spmv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions, benchmark suggestions and integration ideas. I‚Äôd love to see what the local LLM community can do with this. &lt;/p&gt; &lt;p&gt;If anyone has niche/pruned models, weird sparsity patterns, or cases where quantization ruins quality, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vlejd"&gt; /u/vlejd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pawn1r</id>
    <title>More of Silicon Valley is building on free Chinese AI</title>
    <updated>2025-11-30T23:17:35+00:00</updated>
    <author>
      <name>/u/buppermint</name>
      <uri>https://old.reddit.com/user/buppermint</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pawn1r/more_of_silicon_valley_is_building_on_free/"&gt; &lt;img alt="More of Silicon Valley is building on free Chinese AI" src="https://external-preview.redd.it/OmQhaJFYusd_6BoEAmpETVbmV-j9iUqnPAIX8zdt-yE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=349b2df57a77a2b5df77ab3b848267efef6e4117" title="More of Silicon Valley is building on free Chinese AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buppermint"&gt; /u/buppermint &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nbcnews.com/tech/innovation/silicon-valley-building-free-chinese-ai-rcna242430"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pawn1r/more_of_silicon_valley_is_building_on_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pawn1r/more_of_silicon_valley_is_building_on_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T23:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb943m</id>
    <title>model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-01T10:11:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"&gt; &lt;img alt="model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/Qgcy1T0XaVi_myckNkZ5FtZbwlkaUdzehWhwNkBtflY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acaaa619b090ed54ad8471529b58617d2a113392" title="model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like there will be 0-day support for Ministral in llama.cpp too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T10:11:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbal3o</id>
    <title>Finally DeepSeek supports interleave thinking</title>
    <updated>2025-12-01T11:39:42+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt; &lt;img alt="Finally DeepSeek supports interleave thinking" src="https://a.thumbs.redditmedia.com/ai32QhhIA6vA0pwrGaFK12rwpqWtI4iLRNv_7q0_kR4.jpg" title="Finally DeepSeek supports interleave thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lt2hmbaowk4g1.png?width=1923&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7babc862bf421b4f28c0176a4184a40a2a3b0f9"&gt;https://preview.redd.it/lt2hmbaowk4g1.png?width=1923&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7babc862bf421b4f28c0176a4184a40a2a3b0f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y1qu5h7d0l4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57ff81a5e1e9aab0379ff1b0ea907ac6cddd4a0e"&gt;https://preview.redd.it/y1qu5h7d0l4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57ff81a5e1e9aab0379ff1b0ea907ac6cddd4a0e&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What is interleave thinking?&lt;/h1&gt; &lt;p&gt;If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.&lt;/p&gt; &lt;h1&gt;Why it matters?&lt;/h1&gt; &lt;p&gt;Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1paqxs0</id>
    <title>$900 for 192GB RAM on Oct 23rd, now costs over $3k</title>
    <updated>2025-11-30T19:24:34+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"&gt; &lt;img alt="$900 for 192GB RAM on Oct 23rd, now costs over $3k" src="https://preview.redd.it/ka8j4duh3g4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3905d157af26fc5e6596ee0ac48570cd8592339" title="$900 for 192GB RAM on Oct 23rd, now costs over $3k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two 96GB kits cost me $900 on Oct 23rd. Now one month later trying to get an equivalent amount costs about $3200.. Just insane. Wondering what the prices are going to be late 2026, considering word is that this isn't going to be getting better until 2027. Prices here are in CAD btw. USD equivalent is about $650 vs $2300.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ka8j4duh3g4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T19:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbaf8x</id>
    <title>Deepseek v3.2 speciale, it has good benchmarks!</title>
    <updated>2025-12-01T11:30:04+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt; &lt;img alt="Deepseek v3.2 speciale, it has good benchmarks!" src="https://b.thumbs.redditmedia.com/iiaAB9rg5iLZJ19pyyl1FcJVxikiym9_FeMfdKGkRKM.jpg" title="Deepseek v3.2 speciale, it has good benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmarks are in the link.. &lt;/p&gt; &lt;p&gt;It scores higher than GPT 5 high in HLE and Codeforce. I tried it out on their site , im not sure if it is better than gpt 5 ‚Ä¶ &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kaascz2jwk4g1.png?width=4691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8f6201d292d566347185bc8b9f8d1cc2cbc414"&gt;https://preview.redd.it/kaascz2jwk4g1.png?width=4691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8f6201d292d566347185bc8b9f8d1cc2cbc414&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7i6d</id>
    <title>Upcoming vllm Mistral Large 3 support</title>
    <updated>2025-12-01T08:27:38+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"&gt; &lt;img alt="Upcoming vllm Mistral Large 3 support" src="https://external-preview.redd.it/3kkJBT6LzSWLFjvnTMUkLMsNU4IL09qtTW7VM1HkHgk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c48b898fbe4fe47c426c67ed567cfa0160764345" title="Upcoming vllm Mistral Large 3 support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/29757"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T08:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbc99o</id>
    <title>I built a tool that can interactively create diagrams with LLMs</title>
    <updated>2025-12-01T13:05:00+00:00</updated>
    <author>
      <name>/u/daweii</name>
      <uri>https://old.reddit.com/user/daweii</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"&gt; &lt;img alt="I built a tool that can interactively create diagrams with LLMs" src="https://external-preview.redd.it/MHhoMGF2OWdjbDRnMTZ-ggrpklnNsXE3h3FCcz0k2D7KroK_010AEhCs0S-l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d943751a927ad3ba1166257f385b2e80ce567f1" title="I built a tool that can interactively create diagrams with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I built an open-source tool that generates editable drawiodiagrams using LLMs. &lt;/p&gt; &lt;p&gt;This outputs actual XML. You can generate a base diagram, then manually drag/drop elements to fix it, or ask the LLM to refine specific parts. &lt;/p&gt; &lt;p&gt;I added native Ollama support so you can generate architecture diagrams without sending sensitive stack details to OpenAI/Anthropic. &lt;/p&gt; &lt;p&gt;Features:&lt;br /&gt; - Manipulates drawio XML directly.&lt;br /&gt; - Supports AWS, GCP, and Azure icon sets.&lt;br /&gt; - Visual history/diffing (easy to undo hallucinations).&lt;br /&gt; - Works with OpenAI compatible endpoints (Ollama, LM Studio, etc.). &lt;/p&gt; &lt;p&gt;I'd love feedback on how it performs with big local models (&amp;gt;30B), or ideas for v2 (e.g., adding MCP support). &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/DayuanJiang/next-ai-draw-io"&gt;https://github.com/DayuanJiang/next-ai-draw-io&lt;/a&gt;&lt;br /&gt; Demo: &lt;a href="https://next-ai-draw-io.vercel.app/"&gt;https://next-ai-draw-io.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daweii"&gt; /u/daweii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4hpwso9gcl4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbcjql</id>
    <title>That's why open source is even better than closed source</title>
    <updated>2025-12-01T13:18:19+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"&gt; &lt;img alt="That's why open source is even better than closed source" src="https://b.thumbs.redditmedia.com/agNOvW0vm50YDgwkyK-R0Hgp6zZPP2w6C0E9hCz_ixA.jpg" title="That's why open source is even better than closed source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt , No one is spared from ads, even the Pro Plan throws you an ad üíÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pbcjql"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb9vzg</id>
    <title>deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face</title>
    <updated>2025-12-01T10:59:20+00:00</updated>
    <author>
      <name>/u/minpeter2</name>
      <uri>https://old.reddit.com/user/minpeter2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9vzg/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/minpeter2"&gt; /u/minpeter2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9vzg/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9vzg/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T10:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb9xm3</id>
    <title>deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face</title>
    <updated>2025-12-01T11:01:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;DeepSeek-V3.2&lt;/strong&gt;, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;DeepSeek Sparse Attention (DSA):&lt;/strong&gt; We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalable Reinforcement Learning Framework:&lt;/strong&gt; By implementing a robust RL protocol and scaling post-training compute, &lt;em&gt;DeepSeek-V3.2&lt;/em&gt; performs comparably to GPT-5. Notably, our high-compute variant, &lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;, &lt;strong&gt;surpasses GPT-5&lt;/strong&gt; and exhibits reasoning proficiency on par with Gemini-3.0-Pro. &lt;ul&gt; &lt;li&gt;&lt;em&gt;Achievement:&lt;/em&gt; ü•á &lt;strong&gt;Gold-medal performance&lt;/strong&gt; in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large-Scale Agentic Task Synthesis Pipeline:&lt;/strong&gt; To integrate &lt;strong&gt;reasoning into tool-use&lt;/strong&gt; scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
