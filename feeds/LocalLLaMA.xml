<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-14T18:48:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qctt0s</id>
    <title>AI Model Tracker: I was finding it hard to track suitable local models online, so I vibe-coded a simple open source tool using GLM 4.7 and OpenCode. Hope it helps others.</title>
    <updated>2026-01-14T17:40:53+00:00</updated>
    <author>
      <name>/u/mintybadgerme</name>
      <uri>https://old.reddit.com/user/mintybadgerme</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qctt0s/ai_model_tracker_i_was_finding_it_hard_to_track/"&gt; &lt;img alt="AI Model Tracker: I was finding it hard to track suitable local models online, so I vibe-coded a simple open source tool using GLM 4.7 and OpenCode. Hope it helps others." src="https://external-preview.redd.it/s9A0bKqIvcM1hMy5X6iV0j00-DwGVDnbszvCF4k_LlE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dd5d729aebef1e628f141938d6126bfd0467ddea" title="AI Model Tracker: I was finding it hard to track suitable local models online, so I vibe-coded a simple open source tool using GLM 4.7 and OpenCode. Hope it helps others." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mintybadgerme"&gt; /u/mintybadgerme &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nigelp/ai-model-tracker"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qctt0s/ai_model_tracker_i_was_finding_it_hard_to_track/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qctt0s/ai_model_tracker_i_was_finding_it_hard_to_track/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:40:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcid3l</id>
    <title>Pocket TTS: a 100M-parameter text-to-speech</title>
    <updated>2026-01-14T08:51:14+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"&gt; &lt;img alt="Pocket TTS: a 100M-parameter text-to-speech" src="https://external-preview.redd.it/-wU8cKM1ybBFD4hDGC_AsWfo00bhoyCdexKDfL5kTEQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3304eb1bb7dc20cd79911028958854a3039569d9" title="Pocket TTS: a 100M-parameter text-to-speech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/kyutai/pocket-tts"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcid3l/pocket_tts_a_100mparameter_texttospeech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T08:51:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcuo1u</id>
    <title>Mis-matched GPU options</title>
    <updated>2026-01-14T18:11:29+00:00</updated>
    <author>
      <name>/u/MrCuddles20</name>
      <uri>https://old.reddit.com/user/MrCuddles20</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a new computer with a 5090, 5070ti, and 96gb ram. I've been using text Gen webui with Llama.cpp to run GGUFs less than 48gb to keep it on both cards with 16000 context. &lt;/p&gt; &lt;p&gt;I've had fairly good luck using models as a language tutor, having the llm quiz me and me checking with Google to make sure the models aren't making things up. My main goals are somewhat fast LLM responses with accurate quizzing. I'd like to use bigger models, but the second I use ram the response time drops heavily. &lt;/p&gt; &lt;p&gt;But I have a few questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Am I right with this setup and use of chatting, I'm kind of stuck using Llama.cpp and GGUFs for mis matched gpus? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is there anyway tricks to use ram efficiently? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is there something better than text Gen webui? &lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any thoughts on any other uses I could do with 32/48 gbs of vram? Originally I was hoping that would be enough for agentic llms‚Äã but haven't found good instructions on how to set it up. ‚Äã&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrCuddles20"&gt; /u/MrCuddles20 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuo1u/mismatched_gpu_options/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuo1u/mismatched_gpu_options/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuo1u/mismatched_gpu_options/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:11:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcot7e</id>
    <title>Building a low-cost, business-level local LLM for small businesses ‚Äî hardware &amp; security advice needed</title>
    <updated>2026-01-14T14:34:09+00:00</updated>
    <author>
      <name>/u/eeprogrammer</name>
      <uri>https://old.reddit.com/user/eeprogrammer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm a complete beginner (zero background) but very interested in building a &lt;strong&gt;low-cost, business-level local LLM&lt;/strong&gt; that can run &lt;strong&gt;fully on-premise&lt;/strong&gt; for small businesses (no cloud, no data leaving the site).&lt;/p&gt; &lt;p&gt;I‚Äôd really appreciate advice from people with experience in this area, especially on:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1) Hardware&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What kind of CPU/GPU setup makes sense for a small business budget?&lt;/li&gt; &lt;li&gt;Is a single consumer GPU enough, or is multi-GPU necessary?&lt;/li&gt; &lt;li&gt;How much RAM and storage should I realistically plan for?&lt;/li&gt; &lt;li&gt;Any recommendations for cost-effective hardware that‚Äôs stable for 24/7 use?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;2) Architecture / Practical Considerations&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What model sizes are realistic for local deployment today?&lt;/li&gt; &lt;li&gt;Things beginners usually underestimate (power, cooling, noise, maintenance, etc.)&lt;/li&gt; &lt;li&gt;Whether virtualization or containers are recommended for this kind of setup&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;3) Security&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Key security risks when running a local LLM for business use&lt;/li&gt; &lt;li&gt;Best practices for data isolation, access control, and auditability&lt;/li&gt; &lt;li&gt;Any must-have protections to make customers feel confident their data is safe&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My goal is not cutting-edge performance, but &lt;strong&gt;reliable, affordable, and secure&lt;/strong&gt; local AI that small businesses can actually trust and run themselves.&lt;/p&gt; &lt;p&gt;Any guidance, resources, or real-world lessons would be hugely appreciated. Thanks in advance!&lt;/p&gt; &lt;h1&gt;&lt;/h1&gt; &lt;p&gt;Update&lt;/p&gt; &lt;p&gt;The system does not focus on insider threat mitigation and is designed under the assumption of a small, trusted user group (approximately 10 users). However, it enforces clear, role-based access levels to control who can see and operate what.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eeprogrammer"&gt; /u/eeprogrammer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcot7e/building_a_lowcost_businesslevel_local_llm_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcot7e/building_a_lowcost_businesslevel_local_llm_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcot7e/building_a_lowcost_businesslevel_local_llm_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T14:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcv64u</id>
    <title>What‚Äôs the deal with these fake GPU listings on eBay?</title>
    <updated>2026-01-14T18:29:25+00:00</updated>
    <author>
      <name>/u/humandisaster99</name>
      <uri>https://old.reddit.com/user/humandisaster99</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"&gt; &lt;img alt="What‚Äôs the deal with these fake GPU listings on eBay?" src="https://b.thumbs.redditmedia.com/65MpuHjYbWHFf1_tcI0FQmitzKDLLh1kLzUX95wfnoE.jpg" title="What‚Äôs the deal with these fake GPU listings on eBay?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing these around for a while. For most AI GPU searches there will be a couple on the first page. It‚Äôs always a zero review account that was created same-day selling for a third of the normal price. They‚Äôre very clearly scams, but how? eBay buyer protection will always provide a refund if you ask for it basically, so what‚Äôs the scam? Do they just send you a fake GPU and hope you don‚Äôt notice?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/humandisaster99"&gt; /u/humandisaster99 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qcv64u"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv64u/whats_the_deal_with_these_fake_gpu_listings_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:29:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qct4we</id>
    <title>VectorDBZ update: Pinecone, pgvector, custom embeddings, search stats</title>
    <updated>2026-01-14T17:16:32+00:00</updated>
    <author>
      <name>/u/snirjka</name>
      <uri>https://old.reddit.com/user/snirjka</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hey everyone,&lt;/p&gt; &lt;p&gt;A while ago I shared &lt;strong&gt;VectorDBZ, a desktop GUI for vector databases&lt;/strong&gt;, and the feedback from this community was incredibly useful. Thanks again! üôè&lt;/p&gt; &lt;p&gt;Since then, I‚Äôve added:&lt;br /&gt; ‚Ä¢ &lt;strong&gt;Pinecone&lt;/strong&gt; and &lt;strong&gt;pgvector&lt;/strong&gt; support&lt;br /&gt; ‚Ä¢ Search statistics for queries&lt;br /&gt; ‚Ä¢ Custom embedding functions directly in the search tab&lt;/p&gt; &lt;p&gt;Your earlier feedback helped shape a clear roadmap, and the app feels much more capable now.&lt;/p&gt; &lt;p&gt;I‚Äôd love more ideas and feedback:&lt;br /&gt; ‚Ä¢ What other databases or features would make this essential for your workflows?&lt;br /&gt; ‚Ä¢ Any UI/UX improvements for search or embeddings you‚Äôd suggest?&lt;br /&gt; ‚Ä¢ Is sparse vector worth implementing, and how have you used it?&lt;br /&gt; ‚Ä¢ If you do hybrid search with BM25, check the current search flow and tell me how you‚Äôd implement it UI-wise, since I feel like I might be overthinking it.&lt;br /&gt; ‚Ä¢ Other analytics or visualizations that would be useful?&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; GitHub: &lt;a href="https://github.com/vectordbz/vectordbz?utm_source=chatgpt.com"&gt;https://github.com/vectordbz/vectordbz&lt;/a&gt;&lt;br /&gt; Downloads: &lt;a href="https://github.com/vectordbz/vectordbz/releases"&gt;https://github.com/vectordbz/vectordbz/releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you find this useful, a ‚≠ê on GitHub would mean a lot and helps me keep building.&lt;/p&gt; &lt;p&gt;Thanks again for all your input!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/snirjka"&gt; /u/snirjka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct4we/vectordbz_update_pinecone_pgvector_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct4we/vectordbz_update_pinecone_pgvector_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qct4we/vectordbz_update_pinecone_pgvector_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcm8ds</id>
    <title>"Agent Skills" - The spec unified us. The paths divided us.</title>
    <updated>2026-01-14T12:39:50+00:00</updated>
    <author>
      <name>/u/phoneixAdi</name>
      <uri>https://old.reddit.com/user/phoneixAdi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcm8ds/agent_skills_the_spec_unified_us_the_paths/"&gt; &lt;img alt="&amp;quot;Agent Skills&amp;quot; - The spec unified us. The paths divided us." src="https://preview.redd.it/fe2fdwzb8bdg1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce4f4e91626483e0ed89ada9969d0385c9539a83" title="&amp;quot;Agent Skills&amp;quot; - The spec unified us. The paths divided us." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Skills are standardized now. But.....&lt;/p&gt; &lt;p&gt;.github/skills/&lt;/p&gt; &lt;p&gt;.claude/skills/&lt;/p&gt; &lt;p&gt;.codex/skills/&lt;/p&gt; &lt;p&gt;.copilot/skills/&lt;/p&gt; &lt;p&gt;Write once, store‚Ä¶ wherever your agent feels like.&lt;/p&gt; &lt;p&gt;Wish we just also agreed on standardized discovery path for skills (like agents.md). &lt;/p&gt; &lt;p&gt;So Agents Skills are truly interoperable when I am jumping between agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phoneixAdi"&gt; /u/phoneixAdi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fe2fdwzb8bdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcm8ds/agent_skills_the_spec_unified_us_the_paths/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcm8ds/agent_skills_the_spec_unified_us_the_paths/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T12:39:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcff41</id>
    <title>EXAONE MoE support has been merged into llama.cpp</title>
    <updated>2026-01-14T05:55:19+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"&gt; &lt;img alt="EXAONE MoE support has been merged into llama.cpp" src="https://external-preview.redd.it/zj2pPBSKKE7hlpLBhVdaJfKDygb15HG1H-ApMccLwl8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02a15a93673baf3e7e305c8147197d65844556ed" title="EXAONE MoE support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;K-EXAONE-236B-A23B&lt;/h1&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#introduction"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;K-EXAONE&lt;/strong&gt;, a large-scale multilingual language model developed by LG AI Research. Built using a Mixture-of-Experts architecture, K-EXAONE features &lt;strong&gt;236 billion total&lt;/strong&gt; parameters, with &lt;strong&gt;23 billion active&lt;/strong&gt; during inference. Performance evaluations across various benchmarks demonstrate that K-EXAONE excels in reasoning, agentic capabilities, general knowledge, multilingual understanding, and long-context processing.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B#key-features"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture &amp;amp; Efficiency:&lt;/strong&gt; Features a 236B fine-grained MoE design (23B active) optimized with &lt;strong&gt;Multi-Token Prediction (MTP)&lt;/strong&gt;, enabling self-speculative decoding that boosts inference throughput by approximately 1.5x.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Long-Context Capabilities:&lt;/strong&gt; Natively supports a &lt;strong&gt;256K context window&lt;/strong&gt;, utilizing a &lt;strong&gt;3:1 hybrid attention&lt;/strong&gt; scheme with a &lt;strong&gt;128-token sliding window&lt;/strong&gt; to significantly minimize memory usage during long-document processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual Support:&lt;/strong&gt; Covers 6 languages: Korean, English, Spanish, German, Japanese, and Vietnamese. Features a redesigned &lt;strong&gt;150k vocabulary&lt;/strong&gt; with &lt;strong&gt;SuperBPE&lt;/strong&gt;, improving token efficiency by ~30%.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic Capabilities:&lt;/strong&gt; Demonstrates superior tool-use and search capabilities via &lt;strong&gt;multi-agent strategies.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Safety &amp;amp; Ethics:&lt;/strong&gt; Aligned with &lt;strong&gt;universal human values&lt;/strong&gt;, the model uniquely incorporates &lt;strong&gt;Korean cultural and historical contexts&lt;/strong&gt; to address regional sensitivities often overlooked by other models. It demonstrates high reliability across diverse risk categories.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18543"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcff41/exaone_moe_support_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T05:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc9sw2</id>
    <title>Introducing GLM-Image</title>
    <updated>2026-01-14T01:25:35+00:00</updated>
    <author>
      <name>/u/ResearchCrafty1804</name>
      <uri>https://old.reddit.com/user/ResearchCrafty1804</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"&gt; &lt;img alt="Introducing GLM-Image" src="https://preview.redd.it/70ypvyc5w7dg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df4d302e9bb74550a3c16fd5342ab649a5bc3a53" title="Introducing GLM-Image" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing GLM-Image: A new milestone in open-source image generation.&lt;/p&gt; &lt;p&gt;GLM-Image uses a hybrid auto-regressive plus diffusion architecture, combining strong global semantic understanding with high fidelity visual detail. It matches mainstream diffusion models in overall quality while excelling at text rendering and knowledge intensive generation.&lt;/p&gt; &lt;p&gt;Tech Blog: &lt;a href="http://z.ai/blog/glm-image"&gt;http://z.ai/blog/glm-image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Experience it right now: &lt;a href="http://huggingface.co/zai-org/GLM-Image"&gt;http://huggingface.co/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="http://github.com/zai-org/GLM-Image"&gt;http://github.com/zai-org/GLM-Image&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResearchCrafty1804"&gt; /u/ResearchCrafty1804 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/70ypvyc5w7dg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9sw2/introducing_glmimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:25:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qbw325</id>
    <title>My wishes for 2026</title>
    <updated>2026-01-13T16:35:06+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt; &lt;img alt="My wishes for 2026" src="https://preview.redd.it/8knck5zv85dg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a8be13989bebb31b688873f7197d169cb43651e" title="My wishes for 2026" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Which do you think will happen first? And which won‚Äôt happen in 2026?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8knck5zv85dg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qbw325/my_wishes_for_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T16:35:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcjxex</id>
    <title>Renting "inconvenient" H200 (141 GB), A100 GPUs worth it?</title>
    <updated>2026-01-14T10:30:44+00:00</updated>
    <author>
      <name>/u/Select_Jellyfish9325</name>
      <uri>https://old.reddit.com/user/Select_Jellyfish9325</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm a junior research intern at an AI lab. We currently hold a lease on a cluster containing H200s, H100s, and A100s (plus some consumer cards, such as 4090s/5090s, which we have racked ourselves).&lt;/p&gt; &lt;p&gt;While we hit the cluster hard during major training runs, we have periods‚Äîsometimes weeks long‚Äîwhere the high-end capacity sits at 30-40% utilisation.&lt;/p&gt; &lt;p&gt;I‚Äôve been trying to convince the team to open up the idle capacity to the community to recoup some leasing costs. Based on our overhead, we could offer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;H200 (141GB): ~$9 - $10 / hr&lt;/li&gt; &lt;li&gt;A100 (80GB): ~$1.80 / hr&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Catch (and why I‚Äôm asking)&lt;strong&gt;:&lt;/strong&gt;&lt;br /&gt; We are not a cloud provider. We don't have a UI like RunPod or Lambda.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It would be SSH access via a jump host.&lt;/li&gt; &lt;li&gt;You get a Docker container (we can pre-load Unsloth/Axolotl).&lt;/li&gt; &lt;li&gt;No &amp;quot;One-Click Deploy.&amp;quot; Setup is manual.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My Question:&lt;br /&gt; Is that level of &amp;quot;bad UX&amp;quot; a dealbreaker?&lt;/p&gt; &lt;p&gt;I could spend a weekend building a simple web dashboard for reservations, but that might push the price slightly higher (to cover dev time/Stripe fees).&lt;/p&gt; &lt;p&gt;Do you guys prefer the raw, cheapest price with SSH, or is the dashboard worth the extra premium? Just trying to gauge if this is worth setting up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Jellyfish9325"&gt; /u/Select_Jellyfish9325 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxex/renting_inconvenient_h200_141_gb_a100_gpus_worth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxex/renting_inconvenient_h200_141_gb_a100_gpus_worth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxex/renting_inconvenient_h200_141_gb_a100_gpus_worth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T10:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc5nml</id>
    <title>Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!</title>
    <updated>2026-01-13T22:32:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt; &lt;img alt="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" src="https://external-preview.redd.it/amZxajFtZXF6NmRnMQO5kEggYbW8-0IppaPjE5mW-pGiD_HSvWQwK_psM6yd.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5a2ad1d1aac49f10f9a525a08d7b23d8a37a99b3" title="Soprano TTS training code released: Create your own 2000x realtime on-device text-to-speech model with Soprano-Factory!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;I‚Äôve been listening to all your feedback on Soprano, and I‚Äôve been working nonstop over these past three weeks to incorporate everything, so I have a TON of updates for you all!&lt;/p&gt; &lt;p&gt;For those of you who haven‚Äôt heard of Soprano before, it is an on-device text-to-speech model I designed to have highly natural intonation and quality with a small model footprint. It can run up to &lt;strong&gt;20x realtime&lt;/strong&gt; on CPU, and up to &lt;strong&gt;2000x&lt;/strong&gt; on GPU. It also supports lossless streaming with &lt;strong&gt;15 ms latency&lt;/strong&gt;, an order of magnitude lower than any other TTS model. You can check out Soprano here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt; &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;&lt;strong&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-80M"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-80M&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today, I am releasing training code for you guys! This was by far the most requested feature to be added, and I am happy to announce that you can now train your own ultra-lightweight, ultra-realistic TTS models like the one in the video with your &lt;strong&gt;own data&lt;/strong&gt; on your &lt;strong&gt;own hardware&lt;/strong&gt; with &lt;strong&gt;Soprano-Factory&lt;/strong&gt;! Using Soprano-Factory, you can add new &lt;strong&gt;voices&lt;/strong&gt;, &lt;strong&gt;styles&lt;/strong&gt;, and &lt;strong&gt;languages&lt;/strong&gt; to Soprano. The entire repository is just 600 lines of code, making it easily customizable to suit your needs.&lt;/p&gt; &lt;p&gt;In addition to the training code, I am also releasing &lt;strong&gt;Soprano-Encoder&lt;/strong&gt;, which converts raw audio into audio tokens for training. You can find both here:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Factory:&lt;/strong&gt; &lt;a href="https://github.com/ekwek1/soprano-factory"&gt;&lt;strong&gt;https://github.com/ekwek1/soprano-factory&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Soprano-Encoder:&lt;/strong&gt; &lt;a href="https://huggingface.co/ekwek/Soprano-Encoder"&gt;&lt;strong&gt;https://huggingface.co/ekwek/Soprano-Encoder&lt;/strong&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I hope you enjoy it! See you tomorrow,&lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;p&gt;Disclaimer: I did not originally design Soprano with finetuning in mind. As a result, I cannot guarantee that you will see good results after training. Personally, I have my doubts that an 80M-parameter model trained on just 1000 hours of data can generalize to OOD datasets, but I have seen bigger miracles on this sub happen, so knock yourself out :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/wnuwfpdqz6dg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc5nml/soprano_tts_training_code_released_create_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-13T22:32:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcgviy</id>
    <title>Unique 3.2M-word bilingual (DE-EN) literary erotica corpus available for AI training‚Äîteasers on Hugging Face</title>
    <updated>2026-01-14T07:17:55+00:00</updated>
    <author>
      <name>/u/kardinalzahl</name>
      <uri>https://old.reddit.com/user/kardinalzahl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;As an independent author, I've created a large original bilingual erotic fiction corpus (German originals + expanded English adaptations) that's well-suited for training or fine-tuning creative/uncensored models. Highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~3.2 million words across 500+ chapters&lt;/li&gt; &lt;li&gt;Long-form, character-driven narrative with progressive consensual kink (e.g., urophilia, period sex), rural/urban Vietnam settings&lt;/li&gt; &lt;li&gt;Sophisticated prose with philosophical references (Kant, Hegel, existential themes)&lt;/li&gt; &lt;li&gt;Bilingual parallel structure (German first, English creatively reworked‚Äîsometimes longer, sometimes shorter)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Three gated teaser datasets (~475k bilingual words total) are now live on Hugging Face:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Profile with all three: &lt;a href="https://huggingface.co/douglasvanwyck"&gt;https://huggingface.co/douglasvanwyck&lt;/a&gt; &lt;ul&gt; &lt;li&gt;With Anna in Saigon (complete mini-series, ~87k words)&lt;/li&gt; &lt;li&gt;&amp;quot;Phung's Quest&amp;quot; (ongoing series, 7 chapters, ~87k words)&lt;/li&gt; &lt;li&gt;&amp;quot;Center of the Universe&amp;quot;‚ÄîFirst 35 chapters (main saga teaser, ~301k words)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kardinalzahl"&gt; /u/kardinalzahl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcgviy/unique_32mword_bilingual_deen_literary_erotica/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T07:17:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcjxb4</id>
    <title>Would you watch a channel that builds real AI systems from scratch (local LLMs, CPU/GPU, pipelines)?</title>
    <updated>2026-01-14T10:30:35+00:00</updated>
    <author>
      <name>/u/Few_Tax650</name>
      <uri>https://old.reddit.com/user/Few_Tax650</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm considering starting a YouTube channel focused on building production-grade AI systems. Before I invest serious time into this, I want to know if this is something people would actually watch.&lt;/p&gt; &lt;p&gt;I‚Äôm a developer working on AI pipelines and multi-model systems, and I feel there‚Äôs a gap between ‚ÄúAI hype videos‚Äù and real, hands-on system building.&lt;/p&gt; &lt;p&gt;What I‚Äôd cover: ‚Ä¢ Building bots from zero (no fluff, real architecture) ‚Ä¢ CPU vs GPU optimization for local models ‚Ä¢ Multi-model pipelines: routers, fallbacks, model judges ‚Ä¢ Config-driven backends (swap models without rewriting code) ‚Ä¢ Complete workflows: idea ‚Üí architecture ‚Üí working system&lt;/p&gt; &lt;p&gt;Everything would be open-source. You‚Äôd see the code, the mistakes, the refactors, and the final result.&lt;/p&gt; &lt;p&gt;My questions for you: 1. Would you actually watch technical deep-dives like this? 2. What would you personally want more of? (local LLMs, performance benchmarks, agent architecture, deployment, etc.)&lt;/p&gt; &lt;p&gt;I‚Äôm a builder first, not a content creator ‚Äî so I want to make sure this is genuinely useful to real developers before committing.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few_Tax650"&gt; /u/Few_Tax650 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxb4/would_you_watch_a_channel_that_builds_real_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxb4/would_you_watch_a_channel_that_builds_real_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcjxb4/would_you_watch_a_channel_that_builds_real_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T10:30:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcqt2i</id>
    <title>Open-Source Claude Cowork is hereÔºÅ</title>
    <updated>2026-01-14T15:51:39+00:00</updated>
    <author>
      <name>/u/srtng</name>
      <uri>https://old.reddit.com/user/srtng</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcqt2i/opensource_claude_cowork_is_here/"&gt; &lt;img alt="Open-Source Claude Cowork is hereÔºÅ" src="https://external-preview.redd.it/wjJphIAVd6avbLLiGmZ3eL1RtVMLloihRM4BejJdCxQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ac9fd8ea15c5506ba05a95c2a3160fe3e89e65e9" title="Open-Source Claude Cowork is hereÔºÅ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Claude recently launched its Cowork feature, but it's currently limited to Max subscribers and lacks API support.&lt;/p&gt; &lt;p&gt;One of our talented community developers just changed the game! Using the MiniMax M2.1 SOTA model, he has built an open-source alternative that brings collaborative coding to everyone.&lt;/p&gt; &lt;p&gt;Project Highlights:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Fully Compatible: Works with Claude Code configuration files.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Powered by SDK: Built using the Claude Agent SDK for seamless performance.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Interactive: Supports real-time process interaction and manual intervention.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Cross-Platform Ready: Currently optimized for Apple M-series chips, with full platform build support.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Check out the repo here: &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/DevAgentForge/Claude-Cowork"&gt;https://github.com/DevAgentForge/Claude-Cowork&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We love seeing the MiniMax M-series community push the boundaries of what's possible with our latest models. We can't wait to see what you build next!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/srtng"&gt; /u/srtng &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/DevAgentForge/Claude-Cowork"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcqt2i/opensource_claude_cowork_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcqt2i/opensource_claude_cowork_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T15:51:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcj1lr</id>
    <title>What happened to 1.58bit LLMs?</title>
    <updated>2026-01-14T09:34:54+00:00</updated>
    <author>
      <name>/u/Sloppyjoeman</name>
      <uri>https://old.reddit.com/user/Sloppyjoeman</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last year I remember them being super hyped and largely theoretical. Since then, I understand there‚Äôs a growing body of evidence that larger sparse models outperform smaller denser models, which 1.58bit quantisation seems poised to drastically improve&lt;/p&gt; &lt;p&gt;I haven‚Äôt seen people going ‚Äúoh, the 1.58bit quantisation was overhyped‚Äù - did I just miss it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sloppyjoeman"&gt; /u/Sloppyjoeman &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcj1lr/what_happened_to_158bit_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T09:34:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcsmww</id>
    <title>We tried to automate product labeling in one prompt. It failed. 27 steps later, we've processed 10,000+ products.</title>
    <updated>2026-01-14T16:58:22+00:00</updated>
    <author>
      <name>/u/No-Reindeer-9968</name>
      <uri>https://old.reddit.com/user/No-Reindeer-9968</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We built an AI agent to localize imported food products for a retail client. The task sounds simple: extract product info, translate it contextually (not Google Translate), calculate nutritional values for local formats, check compliance with local regulations.&lt;/p&gt; &lt;p&gt;First attempt: one detailed prompt. Let the AI figure out the workflow.&lt;/p&gt; &lt;p&gt;Result: chaos. The AI would hallucinate numbers even with clean images. It would skip steps randomly. At scale, we had no idea where things broke. Every error was a mystery to debug.&lt;/p&gt; &lt;p&gt;So we broke it down. Way down. 27 steps.&lt;/p&gt; &lt;p&gt;Each column in our system handles one thing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extract product name&lt;/li&gt; &lt;li&gt;Extract weight&lt;/li&gt; &lt;li&gt;Extract nutritional values per serving&lt;/li&gt; &lt;li&gt;Convert units to local format&lt;/li&gt; &lt;li&gt;Translate product name (contextual, not literal)&lt;/li&gt; &lt;li&gt;Translate description&lt;/li&gt; &lt;li&gt;Check certification requirements&lt;/li&gt; &lt;li&gt;... and so on&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What changed:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Traceability.&lt;/strong&gt; When something fails, we know exactly which step. No more guessing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Fixability.&lt;/strong&gt; Client corrects a number extraction error once, we build a formula that prevents it downstream. Errors get fixed permanently, not repeatedly.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Consistency at scale.&lt;/strong&gt; The AI isn't &amp;quot;deciding&amp;quot; what to do. It's executing a defined process. Same input, same process, predictable output.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Human oversight actually works.&lt;/strong&gt; The person reviewing outputs learns where the AI struggles. Step 14 always needs checking. Step 22 is solid. They get faster over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The counterintuitive part:&lt;/strong&gt; making the AI &amp;quot;dumber&amp;quot; per step made the overall system smarter. One prompt trying to do everything is one prompt that can fail in infinite ways. 27 simple steps means 27 places where you can inspect, correct, and improve.&lt;/p&gt; &lt;p&gt;We've processed over 10,000 products this way. The manual process used to take 20 minutes per product. Now it's 3 minutes, mostly human review.&lt;/p&gt; &lt;p&gt;The boring truth about reliable AI agents: it's not about prompt engineering magic. It's about architecture that assumes AI will fail and makes failure easy to find and fix.&lt;/p&gt; &lt;p&gt;Happy to answer questions about the approach.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Reindeer-9968"&gt; /u/No-Reindeer-9968 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcsmww/we_tried_to_automate_product_labeling_in_one/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T16:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcv304</id>
    <title>NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3</title>
    <updated>2026-01-14T18:26:19+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"&gt; &lt;img alt="NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3" src="https://external-preview.redd.it/eGh0aTBhazZ5Y2RnMTTPucJdRjO2R67S5i-oYJkuLIwhwL3TAJbW3Q2hg2iU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a81ca1f90638b7e5fb617d89b9d1a2abf729cae1" title="NeuTTS Nano: 120M Parameter On-Device TTS based on Llama3" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;The team at Neuphonic is back with a new open-source release: NeuTTS Nano.&lt;/p&gt; &lt;p&gt;After NeuTTS Air trended #1 on HuggingFace last October, we received a lot of requests for something even smaller that could fit into tighter VRAM/RAM constraints for robotics and embedded agents.&lt;/p&gt; &lt;p&gt;Key Specs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Model Size: 120M active parameters (3x smaller than NeuTTS Air).&lt;/li&gt; &lt;li&gt;Architecture: Simple LM + codec architecture built off Llama3.&lt;/li&gt; &lt;li&gt;Format: Provided in GGML for easy deployment on mobile, Jetson, and Raspberry Pi.&lt;/li&gt; &lt;li&gt;Capabilities: Instant voice cloning (3s sample) and ultra-realistic prosody.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Why use this?&lt;/p&gt; &lt;p&gt;If you are building for smart home devices, robotics, or mobile apps where every MB of RAM matters, Nano is designed for you. It delivers the same &amp;quot;voice magic&amp;quot; but in a much lighter package.&lt;/p&gt; &lt;p&gt;Links:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/neuphonic/neutts"&gt;https://github.com/neuphonic/neutts&lt;/a&gt; &lt;/li&gt; &lt;li&gt;HuggingFace: &lt;a href="https://huggingface.co/neuphonic/neutts-nano"&gt;https://huggingface.co/neuphonic/neutts-nano&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Spaces: &lt;a href="https://huggingface.co/spaces/neuphonic/neutts-nano"&gt;https://huggingface.co/spaces/neuphonic/neutts-nano&lt;/a&gt; &lt;/li&gt; &lt;li&gt;Website: &lt;a href="https://www.neuphonic.com/"&gt;https://www.neuphonic.com/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We‚Äôre curious to see the RTF (Real-Time Factor) benchmarks the community gets on different hardware. What‚Äôs the smallest device you‚Äôre planning to run this on?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/2nikcyj6ycdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcv304/neutts_nano_120m_parameter_ondevice_tts_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:26:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1qct6h2</id>
    <title>How does my local LLM rig look?</title>
    <updated>2026-01-14T17:18:09+00:00</updated>
    <author>
      <name>/u/texasdude11</name>
      <uri>https://old.reddit.com/user/texasdude11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"&gt; &lt;img alt="How does my local LLM rig look?" src="https://preview.redd.it/z1xw8usylcdg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afc831716841f3b411148307f63ee880af80b163" title="How does my local LLM rig look?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In garage/ freezing MN temps are nice!&lt;/p&gt; &lt;p&gt;Key Specs:&lt;/p&gt; &lt;p&gt;Motherboard: ASUS Pro WS W790E-SAGE SE (workstation platform, multi-GPU + tons of PCIe)&lt;/p&gt; &lt;p&gt;CPU: Intel Xeon W9-3495X 56 cores 112 threads, Intel AMX primarily for ktransformers build in mind (moved from an engineering sample to retail)&lt;/p&gt; &lt;p&gt;Memory: 512GB DDR5 ECC (8√ó64GB) 4800 but overclocked to 6000 on an octa-channel platform&lt;/p&gt; &lt;p&gt;GPUs: 2√ó NVIDIA RTX PRO 6000 Blackwell Workstation Edition (96GB VRAM each)&lt;/p&gt; &lt;p&gt;Storage: Samsung 9100 PRO 4TB Gen5 NVMe for models + WD_BLACK SN850X 2TB for OS&lt;/p&gt; &lt;p&gt;Network: 10Gb local + 1Gb internet&lt;/p&gt; &lt;p&gt;Can you spot all other tools except for the server?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/texasdude11"&gt; /u/texasdude11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/z1xw8usylcdg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qct6h2/how_does_my_local_llm_rig_look/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T17:18:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qckjsq</id>
    <title>ZLUDA on llama.cpp -NEWS</title>
    <updated>2026-01-14T11:08:04+00:00</updated>
    <author>
      <name>/u/mossy_troll_84</name>
      <uri>https://old.reddit.com/user/mossy_troll_84</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.phoronix.com/news/ZLUDA-Q4-2025-Report"&gt;https://www.phoronix.com/news/ZLUDA-Q4-2025-Report&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mossy_troll_84"&gt; /u/mossy_troll_84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qckjsq/zluda_on_llamacpp_news/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qckjsq/zluda_on_llamacpp_news/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qckjsq/zluda_on_llamacpp_news/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T11:08:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1qc9m6x</id>
    <title>GLM-Image is released!</title>
    <updated>2026-01-14T01:17:16+00:00</updated>
    <author>
      <name>/u/foldl-li</name>
      <uri>https://old.reddit.com/user/foldl-li</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt; &lt;img alt="GLM-Image is released!" src="https://external-preview.redd.it/Ei4JzvCHJGNODl-Xo97JEKHuZJZU81UlEy5iyXWioSw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=251fac1763ed77fdaf4e281f649fddd4555de498" title="GLM-Image is released!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GLM-Image is an image generation model adopts a hybrid autoregressive + diffusion decoder architecture. In general image generation quality, GLM‚ÄëImage aligns with mainstream latent diffusion approaches, but it shows significant advantages in text-rendering and knowledge‚Äëintensive generation scenarios. It performs especially well in tasks requiring precise semantic understanding and complex information expression, while maintaining strong capabilities in high‚Äëfidelity and fine‚Äëgrained detail generation. In addition to text‚Äëto‚Äëimage generation, GLM‚ÄëImage also supports a rich set of image‚Äëto‚Äëimage tasks including image editing, style transfer, identity‚Äëpreserving generation, and multi‚Äësubject consistency.&lt;/p&gt; &lt;p&gt;Model architecture: a hybrid autoregressive + diffusion decoder design.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/foldl-li"&gt; /u/foldl-li &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-Image"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qc9m6x/glmimage_is_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T01:17:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcl543</id>
    <title>Which are the top LLMs under 8B right now?</title>
    <updated>2026-01-14T11:42:15+00:00</updated>
    <author>
      <name>/u/Additional_Secret_75</name>
      <uri>https://old.reddit.com/user/Additional_Secret_75</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I m looking to pick a local LLM and not sure what to go with anymore. There are a lot of ‚Äúbest‚Äù &amp;lt;8B models and every post says something different, even for the same model. What are people using for normal chat, research, or some coding, not super censored and runs well without a ton of VRAM. It doesn t have to be just one LLM, just the best in their category.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Additional_Secret_75"&gt; /u/Additional_Secret_75 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcl543/which_are_the_top_llms_under_8b_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T11:42:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcusnt</id>
    <title>Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M</title>
    <updated>2026-01-14T18:16:00+00:00</updated>
    <author>
      <name>/u/eugenekwek</name>
      <uri>https://old.reddit.com/user/eugenekwek</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt; &lt;img alt="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" src="https://external-preview.redd.it/NXZ5NDNuYTlzY2RnMX4ZwK1s5ENYxRsvoiSEu3mA0RmAAs2-sAvwRMu-2CtN.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2a20f3c8df9af0a0fbced04bbc8dc6ec0450abe" title="Soprano 1.1-80M released: 95% fewer hallucinations and 63% preference rate over Soprano-80M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;Today, I am announcing Soprano 1.1! I‚Äôve designed it for massively improved stability and audio quality over the original model. &lt;/p&gt; &lt;p&gt;While many of you were happy with the quality of Soprano, it had a tendency to start, well, &lt;em&gt;Mongolian throat singing&lt;/em&gt;. Contrary to its name, Soprano is &lt;strong&gt;NOT&lt;/strong&gt; supposed to be for singing, so I have reduced the frequency of these hallucinations by &lt;strong&gt;95%&lt;/strong&gt;. Soprano 1.1-80M also has a &lt;strong&gt;50%&lt;/strong&gt; lower WER than Soprano-80M, with comparable clarity to much larger models like Chatterbox-Turbo and VibeVoice. In addition, it now supports sentences up to &lt;strong&gt;30 seconds&lt;/strong&gt; long, up from 15.&lt;/p&gt; &lt;p&gt;The outputs of Soprano could sometimes have a lot of artifacting and high-frequency noise. This was because the model was severely undertrained. I have trained Soprano further to reduce these audio artifacts.&lt;/p&gt; &lt;p&gt;According to a blind study I conducted on my family (against their will), they preferred Soprano 1.1's outputs &lt;strong&gt;63%&lt;/strong&gt; of the time, so these changes have produced a noticeably improved model.&lt;/p&gt; &lt;p&gt;You can check out the new Soprano here:&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/ekwek/Soprano-1.1-80M"&gt;https://huggingface.co/ekwek/Soprano-1.1-80M&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Try Soprano 1.1 Now: &lt;a href="https://huggingface.co/spaces/ekwek/Soprano-TTS"&gt;https://huggingface.co/spaces/ekwek/Soprano-TTS&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/ekwek1/soprano"&gt;https://github.com/ekwek1/soprano&lt;/a&gt; &lt;/p&gt; &lt;p&gt;- Eugene&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eugenekwek"&gt; /u/eugenekwek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/v0c2rda9scdg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcusnt/soprano_1180m_released_95_fewer_hallucinations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:16:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qcuerc</id>
    <title>NVIDIA's new 8B model is Orchestrator-8B, a specialized 8-billion-parameter AI designed not to answer everything itself, but to intelligently manage and route complex tasks to different tools (like web search, code execution, other LLMs) for greater efficiency</title>
    <updated>2026-01-14T18:02:19+00:00</updated>
    <author>
      <name>/u/Fear_ltself</name>
      <uri>https://old.reddit.com/user/Fear_ltself</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve seen some arguments we‚Äôve reached AGI, it‚Äôs just about putting the separate pieces together in the right context. I think having a relatively small model that knows how to connect with other tools and models is exactly the correct route towards very functional systems. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fear_ltself"&gt; /u/Fear_ltself &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qcuerc/nvidias_new_8b_model_is_orchestrator8b_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-14T18:02:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
