<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-29T18:44:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pxss0m</id>
    <title>Senator in Tennessee introduces bill to felonize making AI "act as a companion" or "mirror human interactions"</title>
    <updated>2025-12-28T14:35:58+00:00</updated>
    <author>
      <name>/u/CanineAssBandit</name>
      <uri>https://old.reddit.com/user/CanineAssBandit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Call (202) 224-3121 for the Capitol switchboard to contact your representative. Tell them you oppose anything similar.&lt;/p&gt; &lt;p&gt;The bill:&lt;br /&gt; &lt;a href="https://legiscan.com/TN/bill/SB1493/2025"&gt;https://legiscan.com/TN/bill/SB1493/2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quotes from the bill (emphasis mine):&lt;/p&gt; &lt;p&gt;It is an offense for a person to knowingly train artificial intelligence to:&lt;br /&gt; (3) Provide emotional support, &lt;strong&gt;including through open-ended conversations&lt;/strong&gt; with a user;&lt;br /&gt; (4) Develop an emotional relationship with, or otherwise &lt;strong&gt;act as a companion&lt;/strong&gt; to, an individual;&lt;br /&gt; (6) Otherwise act as a sentient human or &lt;strong&gt;mirror interactions that a human user might have with another human user&lt;/strong&gt;, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;&lt;br /&gt; (8) &lt;strong&gt;Simulate a human being&lt;/strong&gt;, including in appearance, voice, or other mannerisms.&lt;/p&gt; &lt;p&gt;&amp;quot;Train&amp;quot;:&lt;br /&gt; (A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of &lt;strong&gt;making decisions based on information or other inputs&lt;/strong&gt; provided to the A.I.&lt;br /&gt; (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanineAssBandit"&gt; /u/CanineAssBandit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyikha</id>
    <title>EditMGT ‚Äî fast, localized image editing with Masked Generative Transformers</title>
    <updated>2025-12-29T10:05:26+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt; &lt;img alt="EditMGT ‚Äî fast, localized image editing with Masked Generative Transformers" src="https://a.thumbs.redditmedia.com/ipfAbO3mXGnc9dJmlMhzMZSQdDCR5P6AtDUpbQk9sR0.jpg" title="EditMGT ‚Äî fast, localized image editing with Masked Generative Transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/o8ngfvhy94ag1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af9bacb07a0773d78259c8a08f832a922d503104"&gt;https://preview.redd.it/o8ngfvhy94ag1.png?width=2626&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=af9bacb07a0773d78259c8a08f832a922d503104&lt;/a&gt;&lt;/p&gt; &lt;p&gt;First &lt;strong&gt;MGT-based&lt;/strong&gt; editing framework that confines changes to target regions, mitigating diffusion ‚Äúedit leakage.‚Äù &lt;strong&gt;&amp;lt;1B params&lt;/strong&gt;, reported &lt;strong&gt;~6√ó faster&lt;/strong&gt; edits (paper notes ~&lt;strong&gt;2s&lt;/strong&gt; per edit). &lt;/p&gt; &lt;ul&gt; &lt;li&gt;How it works: &lt;strong&gt;multi-layer attention consolidation&lt;/strong&gt; + &lt;strong&gt;region-hold sampling&lt;/strong&gt; for precise localization/preservation. &lt;a href="https://arxiv.org/html/2512.11715v1?utm_source=chatgpt.com"&gt;arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Data: &lt;strong&gt;CrispEdit-2M&lt;/strong&gt; (~2M hi-res, 7 categories) released for training/eval. &lt;a href="https://huggingface.co/datasets/WeiChow/CrispEdit-2M?utm_source=chatgpt.com"&gt;Hugging Face+1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Links: &lt;strong&gt;GitHub repo&lt;/strong&gt; &lt;a href="https://github.com/weichow23/EditMGT"&gt;https://github.com/weichow23/EditMGT&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyikha/editmgt_fast_localized_image_editing_with_masked/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1pysmcf</id>
    <title>What's the best LLM for 96gb VRAM with vision</title>
    <updated>2025-12-29T17:39:34+00:00</updated>
    <author>
      <name>/u/LiteratureAcademic34</name>
      <uri>https://old.reddit.com/user/LiteratureAcademic34</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've mostly been into the stable diffusion space, but I've been enjoying playing around with LLMs more often. I have access to an RTX Pro 6000 Blackwell and a Macbook Pro M4 Pro 24gb. I'm currently downloading Minimax m2.1 at IQ3_XXS for my 6000 Pro, but I want other options with vision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiteratureAcademic34"&gt; /u/LiteratureAcademic34 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pysmcf/whats_the_best_llm_for_96gb_vram_with_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pysmcf/whats_the_best_llm_for_96gb_vram_with_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pysmcf/whats_the_best_llm_for_96gb_vram_with_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:39:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyp88k</id>
    <title>Best ASR Model Right Now for English?</title>
    <updated>2025-12-29T15:32:09+00:00</updated>
    <author>
      <name>/u/ArcticTechnician</name>
      <uri>https://old.reddit.com/user/ArcticTechnician</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey y'all, looking for a solid open source/open weight ASR model to use. I've done some digging and places like &lt;a href="https://huggingface.co/spaces/hf-audio/open_asr_leaderboard"&gt;Hugging Face ASR Leaderboard&lt;/a&gt; says some Nvidia models (Parakeet, Canary) lead, but I've also heard that their WER metric is very misleading/doesn't reflect real world use.&lt;/p&gt; &lt;p&gt;I think my mind immediately goes to Whisper-large-v3, but I was wondering if folks had any other accuracy-first, offline transcription model (especially newer ones I might not have checked out). Use case is for a video editor I'm building where a lot of my users have footage they've filmed on their phone of &amp;quot;man on the street&amp;quot; style interactions (so we're not going to have clean podcast style audio). Definitely need timestamping as well.&lt;/p&gt; &lt;p&gt;Thanks for any help in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcticTechnician"&gt; /u/ArcticTechnician &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyp88k/best_asr_model_right_now_for_english/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyp88k/best_asr_model_right_now_for_english/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyp88k/best_asr_model_right_now_for_english/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T15:32:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyd6fk</id>
    <title>Day 21: 21 Days of Building a Small Language Model: Complete Journey Recap</title>
    <updated>2025-12-29T04:59:01+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;No blog today. I created a video instead to recap the journey, just wanted to say a big thank you to everyone for the support. üôè&lt;/p&gt; &lt;p&gt;Video link: &lt;a href="https://youtu.be/-rzMxb1JhuU"&gt;https://youtu.be/-rzMxb1JhuU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I can't believe we've made it to the end together. First, I want to say a massive thank you to everyone who has been following along, reading the blogs, engaging with the content, asking questions, and sharing your own learnings. &lt;/p&gt; &lt;p&gt;This journey has been absolutely incredible, and it wouldn't have been the same without your support and engagement.&lt;/p&gt; &lt;p&gt;Before we wrap up, I want to wish everyone a very Happy New Year! As we close out this year and begin a new one, I'm excited about what's ahead in the world of language models and AI. Until then, happy building!&lt;/p&gt; &lt;p&gt;I‚Äôve added all the links in the first comment.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyd6fk/day_21_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T04:59:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylwsa</id>
    <title>aichat: Claude-Code/Codex-CLI tool for fast full-text session search, and continue work without compaction</title>
    <updated>2025-12-29T13:10:34+00:00</updated>
    <author>
      <name>/u/SatoshiNotMe</name>
      <uri>https://old.reddit.com/user/SatoshiNotMe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylwsa/aichat_claudecodecodexcli_tool_for_fast_fulltext/"&gt; &lt;img alt="aichat: Claude-Code/Codex-CLI tool for fast full-text session search, and continue work without compaction" src="https://external-preview.redd.it/JdyCZEvOLOwYTcokadC6_MHgblhxIq7nS4oghO-Asps.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8bdb0cc548f2da13a724a9c5343afa64b0c16efa" title="aichat: Claude-Code/Codex-CLI tool for fast full-text session search, and continue work without compaction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1pylwsa/video/0kqoau6955ag1/player"&gt;Using the &amp;gt;resume trigger from a Claude Code session&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pylwsa/video/zzyrxor275ag1/player"&gt;aichat search TUI for fast Rust/Tantivy full-text session of sessions&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the &lt;a href="https://github.com/pchalasani/claude-code-tools"&gt;claude-code-tools&lt;/a&gt; repo, I I've been sharing various tools I've built to improve productivity when working with Claude-Code or Codex-CLI. I wanted to share a recent addition: the &lt;a href="https://github.com/pchalasani/claude-code-tools?tab=readme-ov-file#-aichat--session-search-and-continuation-without-compaction"&gt;aichat command&lt;/a&gt; which I use regularly to continue work &lt;strong&gt;without having to compact&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;TL/DR: Some ways to use this tool, once you've &lt;a href="https://github.com/pchalasani/claude-code-tools?tab=readme-ov-file#-quick-start"&gt;installed&lt;/a&gt; it and the associated &lt;a href="https://github.com/pchalasani/claude-code-tools?tab=readme-ov-file#claude-code-plugins"&gt;aichat plugin&lt;/a&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;in a Claude-code session nearing full context usage, type &lt;code&gt;&amp;gt;resume&lt;/code&gt; - this activates a &lt;code&gt;UserPromptSubmit&lt;/code&gt; hook that copies your session id to clipboard and shows instructions to run &lt;code&gt;aichat resume &amp;lt;pasted-session-id&amp;gt;&lt;/code&gt; , which will present 3 ways to continue your work (see below).&lt;/li&gt; &lt;li&gt;If you know which session id to continue work from, use &lt;code&gt;aichat resume &amp;lt;session-id&amp;gt;&lt;/code&gt;&lt;/li&gt; &lt;li&gt;If you need to search for past sessions, use &lt;code&gt;aichat search&lt;/code&gt; which launches a super-fast Rust/Tantivy-based full-text session search TUI with filters (unlike Claude-Code &lt;code&gt;--resume&lt;/code&gt; which only searches session titles).&lt;/li&gt; &lt;li&gt;In a Claude-Code or Codex-CLI session, you can have the agent (or preferably a sub-agent) search for context on prior work using &lt;code&gt;aichat search ... --json&lt;/code&gt; which returns JSONL-formatted results ideal for querying/filtering with &lt;code&gt;jq&lt;/code&gt; that agents excel at. In the aichat plugin, there is a corresponding &lt;strong&gt;session-search skill&lt;/strong&gt; and (for Claude-Code) a &lt;strong&gt;session-searcher sub-agent.&lt;/strong&gt; You can say something like, &lt;em&gt;&amp;quot;use the session-searcher sub-agent to extract context of how we connected the Rust TUI to the Node-based menus&amp;quot;&lt;/em&gt;&lt;/li&gt; &lt;li&gt;There are 3 ways to continue work from a session: (a) &lt;strong&gt;blind trim&lt;/strong&gt;, i.e. clone session + truncate large tool calls/results + older assistant messages, (b) &lt;strong&gt;smart-trim,&lt;/strong&gt; similar but uses headless agent to decide what to truncate, (c) &lt;strong&gt;rollover&lt;/strong&gt; (I use this the most), which creates a new session, injects session-file &lt;strong&gt;lineage&lt;/strong&gt; (back-pointer to parent session, parent's parent and so on) into the first user message, plus optional instructions to extract summary of latest work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Install:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Step 1: Python package uv tool install claude-code-tools # Step 2: Rust search engine (pick one) brew install pchalasani/tap/aichat-search # Homebrew cargo install aichat-search # Cargo # Or download binary from Releases # Step 3: Install Claude Code plugins (for &amp;gt;resume hook, session search related skill, agent, etc) claude plugin marketplace add pchalasani/claude-code-tools claude plugin install &amp;quot;aichat@cctools-plugins&amp;quot; # or from within Claude Code: /plugin marketplace add pchalasani/claude-code-tools /plugin install aichat@cctools-plugins &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Background&lt;/h1&gt; &lt;p&gt;For those curious, I'm outlining the thought process underlying this tool, hoping it helps explain what the &lt;code&gt;aichat&lt;/code&gt; tool does and why it might be useful to you.&lt;/p&gt; &lt;h1&gt;Compaction is lossy: instead, clone the session and truncate long tool-results or older assistant messages&lt;/h1&gt; &lt;p&gt;There are very often situations where compaction loses important details, so I wanted to find ways to continue my work without compaction. A typical scenario: I am at 90% context usage, and I wish I can go on a bit longer to finish the current work-phase. So I thought,&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;I wish I could &lt;strong&gt;truncate&lt;/strong&gt; some long tool results (e.g. file reads or API results) or older assistant messages (can include write/edit tool-calls) and clear out some context to continue my work.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This lead to the &lt;a href="https://github.com/pchalasani/claude-code-tools#three-resume-strategies"&gt;&lt;code&gt;aichat trim&lt;/code&gt;&lt;/a&gt; utility. It provides two variants:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a &amp;quot;blind&amp;quot; &lt;a href="https://github.com/pchalasani/claude-code-tools#three-resume-strategies"&gt;&lt;code&gt;trim&lt;/code&gt;&lt;/a&gt; mode that truncates all tool-results longer than a threshold (default 500 chars), and optionally all-but-recent assistant messages -- all user-configurable. This can free up 40-60% context, depending on what's been going on in the session.&lt;/li&gt; &lt;li&gt;a &lt;a href="https://github.com/pchalasani/claude-code-tools#three-resume-strategies"&gt;&lt;code&gt;smart-trim&lt;/code&gt;&lt;/a&gt; mode that uses a headless Claude/Codex agent to determine which messages can be safely truncated in order to continue the current work. The precise truncation criteria can be customized (e.g. the user may want to continue some prior work rather than the current task).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both of these modes &lt;em&gt;clone&lt;/em&gt; the current session before truncation, and inject two types of &lt;a href="https://github.com/pchalasani/claude-code-tools#lineage-nothing-is-lost"&gt;&lt;em&gt;lineage&lt;/em&gt;&lt;/a&gt; (essentially, back-pointers):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;Session-lineage&lt;/em&gt; is injected into the first user message: a chronological listing of sessions from which the current session was derived. This allows the (sub-) agent to extract needed context from ancestor sessions, either when prompted by the user, or on its own initiative.&lt;/li&gt; &lt;li&gt;Each truncated message also carries a &lt;em&gt;pointer&lt;/em&gt; to the specific message index in the parent session so full details can always be looked up if needed.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;A cleaner alternative: Start new session with lineage and context summary&lt;/h1&gt; &lt;p&gt;Session trimming can be a quick way to clear out context in order to continue the current task for a bit longer, but after a couple of trims, does not yield as much benefit. But the lineage-injection lead to a different idea to avoid compaction:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Create a fresh session, inject parent-session lineage into the first user message, along with instructions to extract (using sub-agents if available) context of the latest task from the parent session, or skip context extraction and leave it to the user to extract context once the session starts.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is the idea behind the &lt;a href="https://github.com/pchalasani/claude-code-tools#three-resume-strategies"&gt;&lt;code&gt;aichat rollover&lt;/code&gt;&lt;/a&gt; functionality, which is the variant I use the most frequently, instead of first trimming a session (though the blind-trimming can still be useful to continue the current work for a bit longer). I usually choose to skip the summarization (this is the &lt;code&gt;quick&lt;/code&gt; rollover option in the TUI) so that the new session starts quickly and I can instruct Claude-Code/Codex-CLI to extract needed context (usually from the latest chat session shown in the lineage), as shown in the demo video below.&lt;/p&gt; &lt;h1&gt;A hook to simplify continuing work from a session&lt;/h1&gt; &lt;p&gt;I wanted to make it seamless to pick any of the above three task continuation modes, when inside a Claude Code session, so I set up a &lt;code&gt;UserPromptSubmit&lt;/code&gt; &lt;a href="https://github.com/pchalasani/claude-code-tools#resume-options"&gt;hook&lt;/a&gt; (via the &lt;code&gt;aichat&lt;/code&gt; plugin) that is triggered when the user types &lt;code&gt;&amp;gt;resume&lt;/code&gt; (or &lt;code&gt;&amp;gt;continue&lt;/code&gt; or &lt;code&gt;&amp;gt;handoff&lt;/code&gt;). When I am close to full context usage, I type &lt;code&gt;&amp;gt;resume&lt;/code&gt;, and the hook script copies the current session id into the clipboard and shows instructions asking the user to run &lt;code&gt;aichat resume &amp;lt;pasted-session-id&amp;gt;&lt;/code&gt;; this launches a TUI that offering options to choose one of the above &lt;a href="https://github.com/pchalasani/claude-code-tools#three-resume-strategies"&gt;session resumption modes&lt;/a&gt;, see demo video above.&lt;/p&gt; &lt;h1&gt;Fast full-text session search for humans/agents to find prior work context&lt;/h1&gt; &lt;p&gt;The above session resumption methods are useful to continue your work from the &lt;em&gt;current&lt;/em&gt; session, but often you want to continue work that was done in an &lt;em&gt;older&lt;/em&gt; Claude-Code/Codex-CLI session. This is why I added this:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Super-fast Rust/Tantivy-based &lt;a href="https://github.com/pchalasani/claude-code-tools#aichat-search--find-and-select-sessions"&gt;full-text search&lt;/a&gt; of all sessions across Claude-Code and Codex-CLI, with a pleasant self-explanatory TUI for humans, and a CLI mode for Agents to find past work. (The Rust/Tantivy-based search and TUI was inspired by the excellent TUI in the &lt;a href="https://github.com/zippoxer/recall"&gt;zippoxer/recall&lt;/a&gt; repo).&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Users can launch the search TUI using &lt;a href="https://github.com/pchalasani/claude-code-tools#aichat-search--find-and-select-sessions"&gt;&lt;code&gt;aichat search ...&lt;/code&gt;&lt;/a&gt; and (sub-) &lt;a href="https://github.com/pchalasani/claude-code-tools#agent-access-to-history-the-session-searcher-sub-agent"&gt;agents can run&lt;/a&gt; &lt;code&gt;aichat search ... --json&lt;/code&gt; and get results in JSONL format for quick analysis and filtering using &lt;code&gt;jq&lt;/code&gt; which of course CLI agents are great at using. There is a corresponding &lt;em&gt;skill&lt;/em&gt; called &lt;code&gt;session-search&lt;/code&gt; and a &lt;em&gt;sub-agent&lt;/em&gt; called &lt;code&gt;session-searcher&lt;/code&gt;, both available via the &lt;code&gt;aichat&lt;/code&gt; &lt;a href="https://github.com/pchalasani/claude-code-tools#claude-code-plugins"&gt;plugin&lt;/a&gt;. For example in Claude Code, users can recover context of some older work by simply saying something like:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Use your session-searcher sub-agent to recover the context of how we worked on connecting the Rust search TUI with the node-based Resume Action menus.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SatoshiNotMe"&gt; /u/SatoshiNotMe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylwsa/aichat_claudecodecodexcli_tool_for_fast_fulltext/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylwsa/aichat_claudecodecodexcli_tool_for_fast_fulltext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylwsa/aichat_claudecodecodexcli_tool_for_fast_fulltext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyqgi5</id>
    <title>Llama 3.2 3B fMRI (updated findings)</title>
    <updated>2025-12-29T16:19:13+00:00</updated>
    <author>
      <name>/u/Due_Hunter_4891</name>
      <uri>https://old.reddit.com/user/Due_Hunter_4891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm building a local interpretability tool that lets me visualize hidden-state activity and &lt;strong&gt;intervene on individual hidden dimensions during inference&lt;/strong&gt; (via forward hooks). While scanning attn_out, I identified a persistent hidden dimension (dim 3039) that appeared repeatedly across prompts. I'll spare you all the Gradio screenshots, there are quite a few.&lt;/p&gt; &lt;p&gt;Initial probing suggested a loose ‚Äúexpressive vs constrained‚Äù effect, but that interpretation didn‚Äôt hold up under tighter controls. I then ran more systematic tests across:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;multiple prompt types (social, procedural, factual, preference-based)&lt;/li&gt; &lt;li&gt;early / mid / late layers&lt;/li&gt; &lt;li&gt;both positive and negative intervention&lt;/li&gt; &lt;li&gt;long generations (1024 tokens)&lt;/li&gt; &lt;li&gt;repeated runs when results were ambiguous&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Across all of these conditions, &lt;strong&gt;the only stable, cross-prompt effect&lt;/strong&gt; was a change in the model‚Äôs &lt;em&gt;degree of commitment&lt;/em&gt; to its current generative trajectory.&lt;/p&gt; &lt;p&gt;Specifically:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Increasing intervention magnitude (regardless of sign) caused the model to respond &lt;strong&gt;more confidently and decisively&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;This did &lt;strong&gt;not&lt;/strong&gt; correlate with improved factual accuracy&lt;/li&gt; &lt;li&gt;In some cases (especially early-layer intervention), higher intervention increased &lt;strong&gt;confident hallucination&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Constrained procedural prompts (e.g. PB&amp;amp;J instructions) showed minimal variation, while open-ended prompts (e.g. greetings, blog-style responses) showed much larger stylistic and tonal shifts&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The effect appears to modulate &lt;strong&gt;how strongly the model commits to whatever path it has already sampled&lt;/strong&gt;, rather than influencing &lt;em&gt;which&lt;/em&gt; path is chosen. This shows up as:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;reduced hedging&lt;/li&gt; &lt;li&gt;increased assertiveness&lt;/li&gt; &lt;li&gt;stronger persistence of narrative frame&lt;/li&gt; &lt;li&gt;less self-correction once a trajectory is underway&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Importantly, this dimension does &lt;strong&gt;not&lt;/strong&gt; behave like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;a semantic feature&lt;/li&gt; &lt;li&gt;an emotion representation&lt;/li&gt; &lt;li&gt;a creativity or verbosity knob&lt;/li&gt; &lt;li&gt;a factual reasoning mechanism&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A more accurate framing is that it functions as a &lt;strong&gt;global commitment / epistemic certainty gain&lt;/strong&gt;, influencing how readily the model doubles down on its internal state.&lt;/p&gt; &lt;p&gt;This also explains earlier inconsistencies:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;early-layer interventions affect task framing (sometimes badly)&lt;/li&gt; &lt;li&gt;later-layer interventions affect delivery and tone&lt;/li&gt; &lt;li&gt;highly constrained tasks limit the observable effect&lt;/li&gt; &lt;li&gt;magnitude matters more than direction&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;At this stage, the claim is intentionally narrow:&lt;/p&gt; &lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;Next steps (not yet done) include residual-stream analysis to see whether this feature accumulates across layers, and ablation tests to check whether removing it increases hedging and self-revision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Due_Hunter_4891"&gt; /u/Due_Hunter_4891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyqgi5/llama_32_3b_fmri_updated_findings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyqgi5/llama_32_3b_fmri_updated_findings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyqgi5/llama_32_3b_fmri_updated_findings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T16:19:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyo8ry</id>
    <title>SA-RAG: Using spreading activation to improve multi-hop retrieval in RAG systems</title>
    <updated>2025-12-29T14:53:33+00:00</updated>
    <author>
      <name>/u/JudgmentPale458</name>
      <uri>https://old.reddit.com/user/JudgmentPale458</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across an interesting paper proposing &lt;strong&gt;SA-RAG&lt;/strong&gt;, which applies &lt;em&gt;spreading activation&lt;/em&gt; (from cognitive psychology) to GraphRAG-style retrieval.&lt;/p&gt; &lt;p&gt;Instead of relying on iterative LLM-guided query rewriting, activation propagates automatically through a knowledge graph starting from query-matched entities. This helps surface ‚Äúbridge‚Äù documents that standard RAG often misses in multi-hop reasoning tasks.&lt;/p&gt; &lt;p&gt;A few points that stood out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Retrieval is treated as a &lt;strong&gt;structural graph problem&lt;/strong&gt;, not a prompting problem&lt;/li&gt; &lt;li&gt;Works with &lt;strong&gt;small open-weight models&lt;/strong&gt;, no retraining required&lt;/li&gt; &lt;li&gt;Shows strong gains on multi-hop QA benchmarks (MuSiQue, 2WikiMultiHopQA)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious how people here see this compared to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;agentic / iterative RAG&lt;/li&gt; &lt;li&gt;query-rewrite‚Äìbased retrieval&lt;/li&gt; &lt;li&gt;hybrid graph + vector approaches&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Paper: [&lt;a href="https://arxiv.org/abs/2512.15922%5D()"&gt;https://arxiv.org/abs/2512.15922]()&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JudgmentPale458"&gt; /u/JudgmentPale458 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyo8ry/sarag_using_spreading_activation_to_improve/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyo8ry/sarag_using_spreading_activation_to_improve/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyo8ry/sarag_using_spreading_activation_to_improve/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T14:53:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyiqly</id>
    <title>do MoEoE models stand a chance?</title>
    <updated>2025-12-29T10:15:49+00:00</updated>
    <author>
      <name>/u/ComplexType568</name>
      <uri>https://old.reddit.com/user/ComplexType568</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ive heard about plans for DeepSeek to make their new models surpass 1 trillion parameter territory, and with them doing that, im sure other labs will too (especially labs like InclusionAI, where &amp;quot;scaling is all you need&amp;quot;)&lt;/p&gt; &lt;p&gt;so that begs the question, *would* and MoEoE model work? as in mixture of experts models that manage even more experts instead of parameters? imagine a 2-3 trillion model only having to decide on 128 experts instead of 2048 to keep low activated params? &lt;/p&gt; &lt;p&gt;i dont know enough about LLMs to answer this question, so id like to ask all of you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ComplexType568"&gt; /u/ComplexType568 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyiqly/do_moeoe_models_stand_a_chance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T10:15:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylu7n</id>
    <title>Fine-tuning a Small LM for browser control with GRPO and OpenEnv</title>
    <updated>2025-12-29T13:07:04+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylu7n/finetuning_a_small_lm_for_browser_control_with/"&gt; &lt;img alt="Fine-tuning a Small LM for browser control with GRPO and OpenEnv" src="https://external-preview.redd.it/uTkHDDGDMOR6Vy-vy4ASQJNPUnxM33lqhu9afcVyTD8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ac3ac0b4424152125ee4380c95bfa3bfc0481bd" title="Fine-tuning a Small LM for browser control with GRPO and OpenEnv" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today I want to share with you the write-up of a live 60-minute session I hosted on the&lt;a href="https://discord.gg/tVvcxtkkhv"&gt; Liquid AI Discord Community&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The topic? How to teach Language Models to navigate websites and complete tasks using Reinforcement Learning.&lt;/p&gt; &lt;p&gt;We‚Äôre talking about building browser agents that can click buttons, fill forms, and even book flights, all by learning from trial and error instead of perfect demonstrations.&lt;/p&gt; &lt;p&gt;You‚Äôll see how to build the complete training pipeline with GRPO, BrowserGym, and LFM2-350M, starting with a simple ‚Äúclick-test‚Äù task and scaling up from there.&lt;/p&gt; &lt;p&gt;Let me know if you have questions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://paulabartabajo.substack.com/p/fine-tuning-lfm2-350m-for-browser"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylu7n/finetuning_a_small_lm_for_browser_control_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylu7n/finetuning_a_small_lm_for_browser_control_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:07:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyk3jg</id>
    <title>Help me build a (reasonable) 4GPU low-cost LLM machine, is ASUS WS X299 PRO/SE still good?</title>
    <updated>2025-12-29T11:35:12+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I kind of exhausted what could be done with my fast. but VRAM poor, 4090 OC edition, so I was dreaming of designing an openframe 4 GPU machine that can drive with acceptable speed 4 GPUs.&lt;/p&gt; &lt;p&gt;My preliminary research found rather acceptable priced WS X299 PRO/SE workstation motherboards that paired with an 48-Lane CPU may just do the trick, also the 64GB DDR4 for it is really price acceptable. &lt;/p&gt; &lt;p&gt;So are there any better mobo/CPU combo under lesr than 1000EUR capable of driving 4 GPUS (proven solutions are getting a super thanks) , please share your experiences and thoughts, thanks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyk3jg/help_me_build_a_reasonable_4gpu_lowcost_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:35:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjohv</id>
    <title>LM Studio alternative for images / Videos / Audio ?</title>
    <updated>2025-12-29T11:11:00+00:00</updated>
    <author>
      <name>/u/mouseofcatofschrodi</name>
      <uri>https://old.reddit.com/user/mouseofcatofschrodi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With LM Studio (and others alike) it is super easy to run LLMs locally. Ist there anything as easy to create pictures, videos and audios locally using open models?&lt;/p&gt; &lt;p&gt;I tried ComfyUI but didn't find it as easy. With LM Studio I can search for models, see if they will run fast/good with my specs (M3 Pro, 36GB Unified) before downloading them, and in general it is super straight forward.&lt;/p&gt; &lt;p&gt;Two extra questions:&lt;br /&gt; 1. Which models would you recommend for this specs?&lt;br /&gt; 2. For LLMs in Mac, the mlx format makes a huge difference. Is there anything similar for image/video/audio models?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mouseofcatofschrodi"&gt; /u/mouseofcatofschrodi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjohv/lm_studio_alternative_for_images_videos_audio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:11:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pydegt</id>
    <title>Benchmarking local llms for speed with CUDA and vulkan, found an unexpected speedup for select models</title>
    <updated>2025-12-29T05:09:41+00:00</updated>
    <author>
      <name>/u/Amazing_Athlete_2265</name>
      <uri>https://old.reddit.com/user/Amazing_Athlete_2265</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was benchmarking my local llm collection to get an idea of tokens rates. I thought it might be interesting to compare CUDA vs Vulkan on my 3080 10GB. As expected, in almost all cases CUDA was the better option as far as token rate However, I found one suprise that affects a small number of models.&lt;/p&gt; &lt;p&gt;Disclaimer: take the following results with a pinch of salt. I'm not a statistician nor mathmetician. I have been programming for some decades but this test code is mostly deslopped jive code. YMMV.&lt;/p&gt; &lt;p&gt;The main findings is that when running certain models partially offloaded to GPU, some models perform much better on Vulkan than CUDA:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GLM4 9B Q6 had a 2.2x speedup on PP, and 1.7x speedup on TG&lt;/li&gt; &lt;li&gt;Qwen3 8B Q6 had a 1.5x speedup on PP, and 1.1x speedup on PP (meh)&lt;/li&gt; &lt;li&gt;and Ministral3 14B 2512 Q4 had a 4.4x speedup on PP, and a 1.6x speedup on TG&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;edit: should add my setup: using latest llama.cpp build. Most ggufs are Unsloth UD. I primarily target models that can produce at least 20t/s. Ryzen 5 something or other, 32GB cheapest DDR4 RAM.&lt;/h2&gt; &lt;p&gt;The following tables only show models that are partially offloaded onto GPU:&lt;/p&gt; &lt;h3&gt;Token generation (tg) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;13.2&lt;/td&gt; &lt;td&gt;-12.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;25.4&lt;/td&gt; &lt;td&gt;44.0&lt;/td&gt; &lt;td&gt;+18.6&lt;/td&gt; &lt;td&gt;1.73x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;40.4&lt;/td&gt; &lt;td&gt;21.6&lt;/td&gt; &lt;td&gt;-18.9&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;36.1&lt;/td&gt; &lt;td&gt;57.1&lt;/td&gt; &lt;td&gt;+21.0&lt;/td&gt; &lt;td&gt;1.58x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;23.1&lt;/td&gt; &lt;td&gt;15.9&lt;/td&gt; &lt;td&gt;-7.1&lt;/td&gt; &lt;td&gt;0.69x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;23.7&lt;/td&gt; &lt;td&gt;25.8&lt;/td&gt; &lt;td&gt;+2.1&lt;/td&gt; &lt;td&gt;1.09x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;104.3&lt;/td&gt; &lt;td&gt;61.4&lt;/td&gt; &lt;td&gt;-42.9&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;30.4&lt;/td&gt; &lt;td&gt;22.4&lt;/td&gt; &lt;td&gt;-8.0&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;16.4&lt;/td&gt; &lt;td&gt;12.9&lt;/td&gt; &lt;td&gt;-3.5&lt;/td&gt; &lt;td&gt;0.79x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;30.6&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-14.3&lt;/td&gt; &lt;td&gt;0.53x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;46.1&lt;/td&gt; &lt;td&gt;23.4&lt;/td&gt; &lt;td&gt;-22.7&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h3&gt;Prompt processing (pp) - CUDA vs vulkan&lt;/h3&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;CUDA (t/s)&lt;/th&gt; &lt;th&gt;Vulkan (t/s)&lt;/th&gt; &lt;th&gt;Diff (t/s)&lt;/th&gt; &lt;th&gt;Speedup&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;ERNIE4.5 21B-A3B Q6&lt;/td&gt; &lt;td&gt;24.5&lt;/td&gt; &lt;td&gt;13.3&lt;/td&gt; &lt;td&gt;-11.2&lt;/td&gt; &lt;td&gt;0.54x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;GLM4 9B Q6&lt;/td&gt; &lt;td&gt;34.0&lt;/td&gt; &lt;td&gt;75.6&lt;/td&gt; &lt;td&gt;+41.6&lt;/td&gt; &lt;td&gt;2.22x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ling-lite-i1 Q6&lt;/td&gt; &lt;td&gt;37.0&lt;/td&gt; &lt;td&gt;20.2&lt;/td&gt; &lt;td&gt;-16.8&lt;/td&gt; &lt;td&gt;0.55x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ministral3 14B 2512 Q4&lt;/td&gt; &lt;td&gt;58.1&lt;/td&gt; &lt;td&gt;255.4&lt;/td&gt; &lt;td&gt;+197.2&lt;/td&gt; &lt;td&gt;4.39x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3 30B-A3B 2507 Q6&lt;/td&gt; &lt;td&gt;21.4&lt;/td&gt; &lt;td&gt;14.0&lt;/td&gt; &lt;td&gt;-7.3&lt;/td&gt; &lt;td&gt;0.66x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Qwen3-8B Q6&lt;/td&gt; &lt;td&gt;30.3&lt;/td&gt; &lt;td&gt;46.0&lt;/td&gt; &lt;td&gt;+15.8&lt;/td&gt; &lt;td&gt;1.52x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Ring-mini-2.0-i1 Q6&lt;/td&gt; &lt;td&gt;88.4&lt;/td&gt; &lt;td&gt;55.6&lt;/td&gt; &lt;td&gt;-32.8&lt;/td&gt; &lt;td&gt;0.63x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Trinity-Mini 26B-A3B Q6&lt;/td&gt; &lt;td&gt;28.2&lt;/td&gt; &lt;td&gt;20.9&lt;/td&gt; &lt;td&gt;-7.4&lt;/td&gt; &lt;td&gt;0.74x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;granite-4.0-h-small Q4&lt;/td&gt; &lt;td&gt;72.3&lt;/td&gt; &lt;td&gt;42.5&lt;/td&gt; &lt;td&gt;-29.8&lt;/td&gt; &lt;td&gt;0.59x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Kanana 1.5 15B-A3B instruct Q6&lt;/td&gt; &lt;td&gt;29.1&lt;/td&gt; &lt;td&gt;16.3&lt;/td&gt; &lt;td&gt;-12.8&lt;/td&gt; &lt;td&gt;0.56x&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 20B Q6&lt;/td&gt; &lt;td&gt;221.9&lt;/td&gt; &lt;td&gt;112.1&lt;/td&gt; &lt;td&gt;-109.8&lt;/td&gt; &lt;td&gt;0.51x&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazing_Athlete_2265"&gt; /u/Amazing_Athlete_2265 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pydegt/benchmarking_local_llms_for_speed_with_cuda_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T05:09:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyn4ny</id>
    <title>Kimi k2 thinking vs glm 4.7</title>
    <updated>2025-12-29T14:05:53+00:00</updated>
    <author>
      <name>/u/Worried_Goat_8604</name>
      <uri>https://old.reddit.com/user/Worried_Goat_8604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys for agentic coding using opencode , which ai model is better? - Kimi k2 thinking or glm 4.7? Its mainly python coding.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Worried_Goat_8604"&gt; /u/Worried_Goat_8604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyn4ny/kimi_k2_thinking_vs_glm_47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyn4ny/kimi_k2_thinking_vs_glm_47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyn4ny/kimi_k2_thinking_vs_glm_47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T14:05:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pykkqx</id>
    <title>Looking back at end of 2024 vs now</title>
    <updated>2025-12-29T12:02:18+00:00</updated>
    <author>
      <name>/u/Main-Fisherman-2075</name>
      <uri>https://old.reddit.com/user/Main-Fisherman-2075</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been rebuilding a few agent systems recently, and I kept having this vague feeling that everything already feels outdated, even compared to the middle of this year.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Models&lt;/strong&gt;&lt;br /&gt; GPT-4o ‚Üí o3 ‚Üí GPT-5.2&lt;br /&gt; Claude 3.5 ‚Üí Claude 3.7 ‚Üí Claude 4.5&lt;br /&gt; Gemini 1.5 ‚Üí Gemini 2.5 ‚Üí Gemini 3&lt;br /&gt; DeepSeek v2 ‚Üí DeepSeek R1 ‚Üí DeepSeek v3&lt;br /&gt; ...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Agent logic&lt;/strong&gt;&lt;br /&gt; single prompt loop ‚Üí planner / executor split ‚Üí long-running agent with state&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RAG / retrieval&lt;/strong&gt;&lt;br /&gt; top-k doc chunks ‚Üí hybrid retrieve + rerank ‚Üí implicit context reads&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;br /&gt; chat history only ‚Üí session + long-term memory ‚Üí stateful memory across runs&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tool use&lt;/strong&gt;&lt;br /&gt; function calling JSON ‚Üí structured tool execution ‚Üí permissioned tool calls&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Workflows&lt;/strong&gt;&lt;br /&gt; python scripts / cron ‚Üí visual workflows (agent steps) ‚Üí resumable execution engine&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Observability&lt;/strong&gt;&lt;br /&gt; prompt logs ‚Üí agent + tool traces ‚Üí evals tied to deploys&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Protocols / integration&lt;/strong&gt;&lt;br /&gt; custom tool schema per app ‚Üí MCP-style shared interface ‚Üí standardized interface + security boundaries&lt;/p&gt; &lt;p&gt;Curious if others rebuilding systems recently feel the same.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Main-Fisherman-2075"&gt; /u/Main-Fisherman-2075 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pykkqx/looking_back_at_end_of_2024_vs_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T12:02:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylstj</id>
    <title>Single RTX PRO 6000 - Minimax M2.1 (IQ2_M) speed</title>
    <updated>2025-12-29T13:05:12+00:00</updated>
    <author>
      <name>/u/johannes_bertens</name>
      <uri>https://old.reddit.com/user/johannes_bertens</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylstj/single_rtx_pro_6000_minimax_m21_iq2_m_speed/"&gt; &lt;img alt="Single RTX PRO 6000 - Minimax M2.1 (IQ2_M) speed" src="https://preview.redd.it/n0sxcy3q55ag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a383a6b2fe22a5ff97c3cee75cb4cdbaf506167c" title="Single RTX PRO 6000 - Minimax M2.1 (IQ2_M) speed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;&amp;quot;What's the speed?&amp;quot;. It depends.&lt;/h1&gt; &lt;p&gt;I run the model using &lt;code&gt;llama-server -m ~/models/unsloth/MiniMax-M2.1-GGUF/UD-IQ2_M/MiniMax-M2.1-UD-IQ2_M-00001-of-00002.gguf --jinja -ngl 99 -t 80 -c 160000 -fa 1 -ctv q8_0 -ctk q8_0 --host 0.0.0.0 --port 8080 -cram -1 --log-file ~/m2.1.log&lt;/code&gt;&lt;/p&gt; &lt;p&gt;KV quantized to Q8&lt;/p&gt; &lt;p&gt;160k max context&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total samples:&lt;/strong&gt; 107&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Date generated:&lt;/strong&gt; 2025-12-29 13:27&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Key Statistics&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Metric&lt;/th&gt; &lt;th&gt;Min&lt;/th&gt; &lt;th&gt;Max&lt;/th&gt; &lt;th&gt;Mean&lt;/th&gt; &lt;th&gt;Median&lt;/th&gt; &lt;th&gt;Std Dev&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;prompt_eval_speed&lt;/td&gt; &lt;td&gt;23.09&lt;/td&gt; &lt;td&gt;1695.32&lt;/td&gt; &lt;td&gt;668.78&lt;/td&gt; &lt;td&gt;577.88&lt;/td&gt; &lt;td&gt;317.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;eval_speed&lt;/td&gt; &lt;td&gt;30.02&lt;/td&gt; &lt;td&gt;91.17&lt;/td&gt; &lt;td&gt;47.97&lt;/td&gt; &lt;td&gt;46.36&lt;/td&gt; &lt;td&gt;14.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h2&gt;Key Insights&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Highest prompt eval speed:&lt;/strong&gt; 1695.32 tokens/sec (n_tokens=15276)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lowest prompt eval speed:&lt;/strong&gt; 23.09 tokens/sec (n_tokens=67201)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Highest eval speed:&lt;/strong&gt; 91.17 tokens/sec (n_tokens=15276)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lowest eval speed:&lt;/strong&gt; 30.02 tokens/sec (n_tokens=92160)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;em&gt;So bottom line, bigger context = lower speed (both PP &amp;amp; TG)&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/johannes_bertens"&gt; /u/johannes_bertens &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/n0sxcy3q55ag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylstj/single_rtx_pro_6000_minimax_m21_iq2_m_speed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylstj/single_rtx_pro_6000_minimax_m21_iq2_m_speed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:05:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyao6g</id>
    <title>Meta released RPG, a research plan generation dataset on Hugging Face</title>
    <updated>2025-12-29T02:58:09+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt; &lt;img alt="Meta released RPG, a research plan generation dataset on Hugging Face" src="https://external-preview.redd.it/_Vt3gVwDJIJ3tdTBBf0E6Y1zVMQL8lOjQzN3Hnt2brY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=062e0684599eddb333c0833911a29ff674bc632c" title="Meta released RPG, a research plan generation dataset on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;22k tasks spanning ML, Arxiv and PubMed, complete with evaluation rubrics and Llama-4 reference solutions for training AI co-scientists&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/facebook/research-plan-gen"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyao6g/meta_released_rpg_a_research_plan_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T02:58:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrn9v</id>
    <title>I Finished a Fully Local Agentic RAG Tutorial</title>
    <updated>2025-12-29T17:03:44+00:00</updated>
    <author>
      <name>/u/CapitalShake3085</name>
      <uri>https://old.reddit.com/user/CapitalShake3085</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I‚Äôve just finished a &lt;strong&gt;complete Agentic RAG tutorial + repository&lt;/strong&gt; that shows how to build a fully local, end-to-end system.&lt;/p&gt; &lt;p&gt;No APIs, no cloud, no hidden costs.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üí° What‚Äôs inside&lt;/h3&gt; &lt;p&gt;The tutorial covers the full pipeline, including the parts most examples skip:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF ‚Üí Markdown ingestion&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hierarchical chunking (parent / child)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Hybrid retrieval (dense + sparse)&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Vector store with &lt;strong&gt;Qdrant&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Query rewriting + &lt;strong&gt;human-in-the-loop&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Context summarization&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-agent map-reduce&lt;/strong&gt; with &lt;strong&gt;LangGraph&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Local inference with &lt;strong&gt;Ollama&lt;/strong&gt;&lt;br /&gt;&lt;/li&gt; &lt;li&gt;Simple &lt;strong&gt;Gradio&lt;/strong&gt; UI&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;üéØ Who it‚Äôs for&lt;/h3&gt; &lt;p&gt;If you want to &lt;strong&gt;understand Agentic RAG by building it&lt;/strong&gt;, not just reading theory, this might help.&lt;/p&gt; &lt;hr /&gt; &lt;h3&gt;üîó Repo&lt;/h3&gt; &lt;p&gt;&lt;a href="https://github.com/GiovanniPasq/agentic-rag-for-dummies"&gt;https://github.com/GiovanniPasq/agentic-rag-for-dummies&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CapitalShake3085"&gt; /u/CapitalShake3085 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrn9v/i_finished_a_fully_local_agentic_rag_tutorial/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:03:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyntxz</id>
    <title>BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language</title>
    <updated>2025-12-29T14:36:30+00:00</updated>
    <author>
      <name>/u/AgencyInside407</name>
      <uri>https://old.reddit.com/user/AgencyInside407</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"&gt; &lt;img alt="BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language" src="https://external-preview.redd.it/dWN4dmJ1ZmttNWFnMY4GET1d7wPCVUTfL2kwUQvVU9zlAAxjJ2PRs52epB4h.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=604938b875bebc02ca972dc626bb9367cd1bc819" title="BULaMU-Dream: The First Text-to-Image Model Trained from Scratch for an African Language" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everybody! I hope all is well. I just wanted to share a project that I have been working on for the last several months called BULaMU-Dream. It is the first text to image model in the world that has been trained from scratch to respond to prompts in an African Language (Luganda). The details of how I trained it are &lt;a href="https://zenodo.org/records/18086776"&gt;here&lt;/a&gt; and a demo can be found &lt;a href="https://x.com/mwebazarick/status/2005643851655168146?s=12"&gt;here&lt;/a&gt;. I am open to any feedback that you are willing to share because I am going to continue working on improving BULaMU-Dream. I really believe that tiny conditional diffusion models like this can broaden access to multimodal AI tools by allowing people train and use these models on relatively inexpensive setups, like the M4 Mac Mini. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AgencyInside407"&gt; /u/AgencyInside407 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ty3cvnfkm5ag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyntxz/bulamudream_the_first_texttoimage_model_trained/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T14:36:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyrjke</id>
    <title>Benchmarks for Quantized Models? (for users locally running Q8/Q6/Q2 precision)</title>
    <updated>2025-12-29T17:00:09+00:00</updated>
    <author>
      <name>/u/No-Grapefruit-1358</name>
      <uri>https://old.reddit.com/user/No-Grapefruit-1358</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;Many of us use the quantized Q8/Q6/Q2 model instead of fp16 for obvious reasons. Is there a collection of benchmarks which show SWE, HLE etc on Q8/Q6/Q2 quantized models? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Grapefruit-1358"&gt; /u/No-Grapefruit-1358 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyrjke/benchmarks_for_quantized_models_for_users_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T17:00:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyjjbw</id>
    <title>Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together</title>
    <updated>2025-12-29T11:02:29+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt; &lt;img alt="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" src="https://a.thumbs.redditmedia.com/bB4zUj7vleOqJdnXmwlZ9s4tewjkzGrf2kPk8Dbebv4.jpg" title="Naver (South Korean internet giant), has just launched HyperCLOVA X SEED Think, a 32B open weights reasoning model and HyperCLOVA X SEED 8B Omni, a unified multimodal model that brings text, vision, and speech together" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HyperCLOVA X SEED 32B Think: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HyperCLOVA X SEED 8B Omni: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Collection: &lt;a href="https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed"&gt;https://huggingface.co/collections/naver-hyperclovax/hyperclova-x-seed&lt;/a&gt;&lt;/p&gt; &lt;p&gt;From Artificial Analysis on ùïè: &lt;a href="https://x.com/ArtificialAnlys/status/2005429176615174207"&gt;https://x.com/ArtificialAnlys/status/2005429176615174207&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyjjbw"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyjjbw/naver_south_korean_internet_giant_has_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T11:02:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pylyen</id>
    <title>I was training an AI model and...</title>
    <updated>2025-12-29T13:12:46+00:00</updated>
    <author>
      <name>/u/bapuc</name>
      <uri>https://old.reddit.com/user/bapuc</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylyen/i_was_training_an_ai_model_and/"&gt; &lt;img alt="I was training an AI model and..." src="https://preview.redd.it/06e4wmao75ag1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=000d5dbb19fbb5ad24d3ec228e0a3bc802bd60a1" title="I was training an AI model and..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bapuc"&gt; /u/bapuc &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/06e4wmao75ag1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pylyen/i_was_training_an_ai_model_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pylyen/i_was_training_an_ai_model_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T13:12:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pyg4yt</id>
    <title>Tencent just released WeDLM 8B Instruct on Hugging Face</title>
    <updated>2025-12-29T07:38:43+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt; &lt;img alt="Tencent just released WeDLM 8B Instruct on Hugging Face" src="https://b.thumbs.redditmedia.com/C56gntSOSvM_cfj95m0peqGLfh8p1Tnt02oONhKPwFM.jpg" title="Tencent just released WeDLM 8B Instruct on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/tencent/WeDLM-8B-Instruct"&gt;https://huggingface.co/tencent/WeDLM-8B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A diffusion language model that runs 3-6√ó faster than vLLM-optimized Qwen3-8B on math reasoning tasks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pyg4yt"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pyg4yt/tencent_just_released_wedlm_8b_instruct_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-29T07:38:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
