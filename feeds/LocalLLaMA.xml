<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-05T08:54:18+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oowrkx</id>
    <title>What is the best LLM for long context tasks that can run on 16gb vram and 64gb ram</title>
    <updated>2025-11-05T07:41:10+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Use case: chat history analysis (don‚Äôt wanna use cloud)&lt;/p&gt; &lt;p&gt;Note I can run gpt-OSS with 32k context but idk if 32k is enough. &lt;/p&gt; &lt;p&gt;Any models that are really good for high context? Thanks &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowrkx/what_is_the_best_llm_for_long_context_tasks_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowrkx/what_is_the_best_llm_for_long_context_tasks_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oowrkx/what_is_the_best_llm_for_long_context_tasks_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T07:41:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo4kh7</id>
    <title>Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU</title>
    <updated>2025-11-04T11:05:26+00:00</updated>
    <author>
      <name>/u/CombinationNo780</name>
      <uri>https://old.reddit.com/user/CombinationNo780</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt; &lt;img alt="Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU" src="https://b.thumbs.redditmedia.com/nnPDB-YtTPCthxTtyPVParTYcBmaEE0gf_kWM64QSso.jpg" title="Finetuning DeepSeek 671B locally with only 80GB VRAM and Server CPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, we're the KTransformers team (formerly known for our DeepSeek-V3 local CPU/GPU hybrid inference project).&lt;/p&gt; &lt;p&gt;Today, we're proud to announce full integration with LLaMA-Factory, enabling you to &lt;strong&gt;fine-tune DeepSeek-671B or Kimi-K2-1TB locally with just 4x RTX 4090 GPUs&lt;/strong&gt;!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/dlipq1us28zf1.png?width=2332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92fad09b19f37c76c3f08fe9e326816ad4d533d1"&gt;https://preview.redd.it/dlipq1us28zf1.png?width=2332&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=92fad09b19f37c76c3f08fe9e326816ad4d533d1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/finetuning-deepseek-671b-locally-with-only-80gb-vram-and-v0-24938oydy7zf1.png?width=2246&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=216765e8119e54cc2bdc92bf24b082575f7d1bdc"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/finetuning-deepseek-671b-locally-with-only-80gb-vram-and-v0-w1m1j89jy7zf1.png?width=2570&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bde4b33c857b8fd4c1f4d8c0c4ecc42763f5bbc"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More infomation can be found at&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/kvcache-ai/ktransformers/tree/main/KT-SFT"&gt;https://github.com/kvcache-ai/ktransformers/tree/main/KT-SFT&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CombinationNo780"&gt; /u/CombinationNo780 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo4kh7/finetuning_deepseek_671b_locally_with_only_80gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T11:05:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oohhkr</id>
    <title>How to turn a model's sycophancy against itself</title>
    <updated>2025-11-04T19:56:23+00:00</updated>
    <author>
      <name>/u/autoencoder</name>
      <uri>https://old.reddit.com/user/autoencoder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was trying to analyze a complex social situation as well as my own behavior objectively. The models tended to say I did the right thing, but I thought it may have been biased.&lt;/p&gt; &lt;p&gt;So, in a new conversation, I just rephrased it pretending to be the person I perceived to be the offender, and asked about &amp;quot;that other guy's&amp;quot; behavior (actually mine) and what he should have done.&lt;/p&gt; &lt;p&gt;I find this funny, since it forces you to empathize as well when reframing the prompt from the other person's point of view.&lt;/p&gt; &lt;p&gt;Local models are particularly useful for this, since you completely control their memory, as remote AIs could connect the dots between questions and support your original point of view.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/autoencoder"&gt; /u/autoencoder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oohhkr/how_to_turn_a_models_sycophancy_against_itself/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oohhkr/how_to_turn_a_models_sycophancy_against_itself/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oohhkr/how_to_turn_a_models_sycophancy_against_itself/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T19:56:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oox7pq</id>
    <title>Have we figured out any good solutions around the MoE finetuning issues? (other than GSPO)</title>
    <updated>2025-11-05T08:10:35+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Was wondering if we had a more elegant solution yet for using offline-policy training methods (like dpo and it's variants) for MoE training, other than just not training on the router layer. Last I checked only GSPO training worked well for MoE's, but that's pretty expensive.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oox7pq/have_we_figured_out_any_good_solutions_around_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oox7pq/have_we_figured_out_any_good_solutions_around_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oox7pq/have_we_figured_out_any_good_solutions_around_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T08:10:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oofujk</id>
    <title>Companies Publishing LLM Weights on Hugging Face (2025 Edition)</title>
    <updated>2025-11-04T18:55:55+00:00</updated>
    <author>
      <name>/u/tkpred</name>
      <uri>https://old.reddit.com/user/tkpred</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been mapping which AI labs and companies actually &lt;strong&gt;publish their model weights&lt;/strong&gt; on &lt;a href="https://huggingface.co"&gt;Hugging Face&lt;/a&gt; ‚Äî in today‚Äôs LLM ecosystem.&lt;/p&gt; &lt;p&gt;Below is a list of organizations that currently maintain official hosting open-weight models:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Creator&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/01-ai"&gt;01.AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/ai21labs"&gt;AI21 Labs&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/baidu"&gt;Baidu&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/ByteDance-Seed"&gt;ByteDance Seed&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/CohereLabs"&gt;Cohere&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/databricks"&gt;Databricks&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/deepseek-ai"&gt;DeepSeek&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/google"&gt;Google Research&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/ibm-granite"&gt;IBM Granite&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/inclusionAI"&gt;InclusionAI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/LGAI-EXAONE"&gt;LG AI Research&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/LiquidAI"&gt;Liquid AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/meta-llama"&gt;Meta (Llama)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/microsoft"&gt;Microsoft Azure AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/MiniMaxAI"&gt;MiniMax AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/mistralai"&gt;Mistral AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/moonshotai"&gt;Moonshot AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/NousResearch"&gt;Nous Research&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/nvidia"&gt;NVIDIA&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/openai"&gt;OpenAI (&lt;em&gt;some research artifacts only&lt;/em&gt;)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/openchat"&gt;OpenChat&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/perplexity-ai"&gt;Perplexity AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/Qwen"&gt;Alibaba (Qwen)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/RekaAI"&gt;Reka AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/ServiceNow-AI"&gt;ServiceNow AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/Snowflake"&gt;Snowflake&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/upstage"&gt;Upstage&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/xai-org"&gt;xAI (Elon Musk)&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;a href="https://huggingface.co/zai-org"&gt;Z AI&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h3&gt;Why I‚Äôm Building This List&lt;/h3&gt; &lt;p&gt;I‚Äôm studying different LLM architecture families and how design philosophies vary between research groups ‚Äî things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Attention patterns (dense vs. MoE vs. hybrid routing)&lt;/li&gt; &lt;li&gt;Tokenization schemes (BPE vs. SentencePiece vs. tiktoken variants)&lt;/li&gt; &lt;li&gt;Quantization / fine-tuning strategies&lt;/li&gt; &lt;li&gt;Context length scaling and memory efficiency&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h3&gt;Discussion&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;Which other organizations should be included here?&lt;/li&gt; &lt;li&gt;Which model families have the most distinctive architectures?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tkpred"&gt; /u/tkpred &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oofujk/companies_publishing_llm_weights_on_hugging_face/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oofujk/companies_publishing_llm_weights_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oofujk/companies_publishing_llm_weights_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T18:55:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooms3e</id>
    <title>ClickHouse has acquired LibreChat</title>
    <updated>2025-11-04T23:20:25+00:00</updated>
    <author>
      <name>/u/sdairs_ch</name>
      <uri>https://old.reddit.com/user/sdairs_ch</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooms3e/clickhouse_has_acquired_librechat/"&gt; &lt;img alt="ClickHouse has acquired LibreChat" src="https://external-preview.redd.it/D1khPJHUK-RmeIXMBl75xP3lae4sk4GFEOA-YGLGOU8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b2492ea47a8184191be739ee6d66f00791ca2757" title="ClickHouse has acquired LibreChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sdairs_ch"&gt; /u/sdairs_ch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://clickhouse.com/blog/librechat-open-source-agentic-data-stack"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooms3e/clickhouse_has_acquired_librechat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooms3e/clickhouse_has_acquired_librechat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:20:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo7kqy</id>
    <title>Is GPT-OSS-120B the best llm that fits in 96GB VRAM?</title>
    <updated>2025-11-04T13:48:00+00:00</updated>
    <author>
      <name>/u/GreedyDamage3735</name>
      <uri>https://old.reddit.com/user/GreedyDamage3735</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I wonder if gpt-oss-120b is the best local llm, with respect to the general intelligence(and reasoning ability), that can be run on 96GB VRAM GPU. Do you guys have any suggestions otherwise gpt-oss?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GreedyDamage3735"&gt; /u/GreedyDamage3735 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo7kqy/is_gptoss120b_the_best_llm_that_fits_in_96gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo7kqy/is_gptoss120b_the_best_llm_that_fits_in_96gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo7kqy/is_gptoss120b_the_best_llm_that_fits_in_96gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T13:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oowty9</id>
    <title>Curious about real local LLM workflows: What‚Äôs your setup?</title>
    <updated>2025-11-05T07:45:30+00:00</updated>
    <author>
      <name>/u/rakii6</name>
      <uri>https://old.reddit.com/user/rakii6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone, I‚Äôve been exploring the local LLM ecosystem recently and I‚Äôm fascinated by how far self-hosted models, personal rigs, and open tooling have come. Many of you build and fine-tune models without ever touching a commercial AI platform, and honestly, it‚Äôs impressive.&lt;/p&gt; &lt;p&gt;I‚Äôm here to understand the real workflows and needs of people running LLaMA models locally. I‚Äôm not trying to sell anything, replace your setups, or convince you cloud is better. I get why local matters: privacy, control, ownership, experimentation, and raw geek joy.&lt;/p&gt; &lt;p&gt;I‚Äôd love to learn from this community:&lt;/p&gt; &lt;p&gt;~What tooling do you rely on most? (Ollama, LM Studio, KoboldCPP, text-gen-webui, ExLlamaV2, etc.)&lt;/p&gt; &lt;p&gt;~What do you use for fine-tuning / LoRAs? (Axolotl, GPTQ, QLoRA, transformers, AutoTrain?)&lt;/p&gt; &lt;p&gt;~Preferred runtime stacks? CUDA? ROCm? CPU-only builds? Multi-GPU? GGUF workflows?&lt;/p&gt; &lt;p&gt;~Which UI layers make your daily use better? JSON API? Web UIs? Notebooks? VS Code tooling?&lt;/p&gt; &lt;p&gt;~What are the biggest pain points in local workflows? (install hell, driver issues, VRAM limits, model conversion, dataset prep)&lt;/p&gt; &lt;p&gt;My goal isn't to pitch anything, but to get a real understanding of how local LLM power users think and build so I can respect the space, learn from it, and maybe build tools that don‚Äôt disrupt but support the local-first culture.&lt;/p&gt; &lt;p&gt;Just trying to learn from people who already won their sovereignty badge. Appreciate anyone willing to share their setup or insights. The passion here is inspiring.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rakii6"&gt; /u/rakii6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowty9/curious_about_real_local_llm_workflows_whats_your/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowty9/curious_about_real_local_llm_workflows_whats_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oowty9/curious_about_real_local_llm_workflows_whats_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T07:45:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oowt3g</id>
    <title>Testing local speech-to-speech on 8 GB Vram( RTX 4060).</title>
    <updated>2025-11-05T07:43:56+00:00</updated>
    <author>
      <name>/u/Icy_Gas8807</name>
      <uri>https://old.reddit.com/user/Icy_Gas8807</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowt3g/testing_local_speechtospeech_on_8_gb_vram_rtx_4060/"&gt; &lt;img alt="Testing local speech-to-speech on 8 GB Vram( RTX 4060)." src="https://external-preview.redd.it/aGdvNGU1Z3o0ZXpmMQtV4bEmE7OV1G8smVZqriF4a_nIyLOfzpAjqsBGXsQI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8a03dd69f43190f508d817b13a683765a3f3979" title="Testing local speech-to-speech on 8 GB Vram( RTX 4060)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw the post last week regarding best TTS and STT models, forked the official hugging face repo on s2s -&amp;gt; &lt;a href="https://github.com/reenigne314/speech-to-speech.git"&gt;https://github.com/reenigne314/speech-to-speech.git&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;VAD -&amp;gt; mostly untouched except modified some deprecated package issues.&lt;/p&gt; &lt;p&gt;STT -&amp;gt; Still using whishper, most people preferred parakeet, but I faced some package dependency issues( I'll give it a shot again.) &lt;/p&gt; &lt;p&gt;LLM -&amp;gt; LM Studio(llamacpp) &amp;gt;&amp;gt;&amp;gt;&amp;gt; transformers, &lt;/p&gt; &lt;p&gt;TTS -&amp;gt; modified to Kokoro.&lt;/p&gt; &lt;p&gt;I even tried pushing it to use Granite 4H tiny(felt too professional), Gemma 3n E4B(not very satisfied). I stuck with Qwen3 4B despite it's urge to use emojis in every sentence( instructed not to use emojis twice in system prompt).&lt;/p&gt; &lt;p&gt;PS: I will try to run bigger models in my beelink strix halo and update you guys.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Icy_Gas8807"&gt; /u/Icy_Gas8807 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hu9907gz4ezf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowt3g/testing_local_speechtospeech_on_8_gb_vram_rtx_4060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oowt3g/testing_local_speechtospeech_on_8_gb_vram_rtx_4060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T07:43:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1oocbmd</id>
    <title>Cache-to-Cache (C2C)</title>
    <updated>2025-11-04T16:49:08+00:00</updated>
    <author>
      <name>/u/xXWarMachineRoXx</name>
      <uri>https://old.reddit.com/user/xXWarMachineRoXx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A new framework, Cache-to-Cache (C2C), lets multiple LLMs communicate directly through their KV-caches instead of text, transferring deep semantics without token-by-token generation. &lt;/p&gt; &lt;p&gt;It fuses cache representations via a neural projector and gating mechanism for efficient inter-model exchange.&lt;/p&gt; &lt;p&gt;The payoff: up to 10% higher accuracy, 3‚Äì5% gains over text-based communication, and 2√ó faster responses. Cache-to-Cache: Direct Semantic Communication Between Large Language Models&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/thu-nics/C2C"&gt;https://github.com/thu-nics/C2C&lt;/a&gt; Project: &lt;a href="https://github.com/thu-nics"&gt;https://github.com/thu-nics&lt;/a&gt; Paper: &lt;a href="https://arxiv.org/abs/2510.03215"&gt;https://arxiv.org/abs/2510.03215&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In my opinion: can also probably be used instead of thinking word tokens&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xXWarMachineRoXx"&gt; /u/xXWarMachineRoXx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oocbmd/cachetocache_c2c/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oocbmd/cachetocache_c2c/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oocbmd/cachetocache_c2c/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T16:49:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1oowuqd</id>
    <title>Un-LOCC Wrapper: I built a Python library that compresses your OpenAI chats into images, saving up to 3√ó on tokens! (or even more :D)</title>
    <updated>2025-11-05T07:46:54+00:00</updated>
    <author>
      <name>/u/MaxDev0</name>
      <uri>https://old.reddit.com/user/MaxDev0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: I turned my optical compression research into an &lt;strong&gt;actual Python library&lt;/strong&gt; that wraps the OpenAI SDK. Now you can compress large text contexts into images with a simple &lt;code&gt;compressed: True&lt;/code&gt; flag, achieving &lt;strong&gt;up to 2.8:1 token compression&lt;/strong&gt; while maintaining over &lt;strong&gt;93% accuracy&lt;/strong&gt;. Drop-in replacement for OpenAI client - sync/async support included.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC-Wrapper"&gt;https://github.com/MaxDevv/Un-LOCC-Wrapper&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What this is:&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Un-LOCC Wrapper&lt;/strong&gt; - A Python library that takes my optical compression research and makes it &lt;strong&gt;actually usable&lt;/strong&gt; in your projects today. It's a simple wrapper around the OpenAI SDK that automatically converts text to compressed images when you add a &lt;code&gt;compressed: True&lt;/code&gt; flag.&lt;/p&gt; &lt;h1&gt;How it works:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Render text into optimized images (using research-tested fonts/sizes)&lt;/li&gt; &lt;li&gt;Pass images to Vision-Language Models instead of text tokens&lt;/li&gt; &lt;li&gt;Get the same responses while using WAY fewer tokens&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Code Example - It's this simple:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;from un_locc import UnLOCC client = UnLOCC(api_key=&amp;quot;your-api-key&amp;quot;) # Compress large context with one flag messages = [ {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;Summarize this document:&amp;quot;}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: large_text, &amp;quot;compressed&amp;quot;: True} # ‚Üê That's it! ] response = client.chat.completions.create( model=&amp;quot;gpt-4o&amp;quot;, messages=messages ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Async version too:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from un_locc import AsyncUnLOCC client = AsyncUnLOCC(api_key=&amp;quot;your-api-key&amp;quot;) response = await client.chat.completions.create(...) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;üöÄ &lt;strong&gt;Drop-in replacement&lt;/strong&gt; for OpenAI client&lt;/li&gt; &lt;li&gt;‚ö° &lt;strong&gt;Sync &amp;amp; async&lt;/strong&gt; support&lt;/li&gt; &lt;li&gt;üéØ &lt;strong&gt;Research-backed defaults&lt;/strong&gt; (Atkinson Hyperlegible font, 864√ó864px, etc.)&lt;/li&gt; &lt;li&gt;üîß &lt;strong&gt;Customizable&lt;/strong&gt; - override any compression parameter&lt;/li&gt; &lt;li&gt;üìö &lt;strong&gt;Works with&lt;/strong&gt; chat completions &amp;amp; responses API&lt;/li&gt; &lt;li&gt;üèéÔ∏è &lt;strong&gt;Fast rendering&lt;/strong&gt; - ReportLab + pypdfium2 when available&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why this matters:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pay ~3√ó less&lt;/strong&gt; for context tokens&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extend context windows&lt;/strong&gt; without expensive upgrades&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Perfect for&lt;/strong&gt;: chat history compression, document analysis, large-context workflows&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Zero model changes&lt;/strong&gt; - works with existing VLMs like GPT-4o&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Research Behind It:&lt;/h1&gt; &lt;p&gt;Based on my &lt;a href="https://github.com/MaxDevv/UN-LOCC"&gt;UN-LOCC research&lt;/a&gt; testing 90+ experiments across 6+ VLMs:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Gemini 2.0 Flash Lite&lt;/strong&gt;: 93.65% accuracy @ 2.8:1 compression&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen2.5-VL-72B&lt;/strong&gt;: 99.26% accuracy @ 1.7:1 compression&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-VL-235B&lt;/strong&gt;: 95.24% accuracy @ 2.2:1 compression&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Install &amp;amp; Try:&lt;/h1&gt; &lt;pre&gt;&lt;code&gt;pip install un-locc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The library handles all the complexity - fonts, rendering optimization, content type detection. You just add &lt;code&gt;compressed: True&lt;/code&gt; and watch your token usage plummet.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub repo (stars help a ton!):&lt;/strong&gt; &lt;a href="https://github.com/MaxDevv/Un-LOCC-Wrapper"&gt;https://github.com/MaxDevv/Un-LOCC-Wrapper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick Note&lt;/strong&gt;: While testing the library beyond my original research, I discovered that the compression limits are actually MUCH higher than the conservative 3x I reported. Gemini was consistently understanding text and accurately reading back sentences at &lt;strong&gt;6x compression&lt;/strong&gt; without issues. The 3x figure was just my research cutoff for quantifiable accuracy metrics, but for real-world use cases where perfect character-level retrieval isn't critical, we're looking at, maybe something like... &lt;strong&gt;6-7x compression&lt;/strong&gt; lol :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaxDev0"&gt; /u/MaxDev0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowuqd/unlocc_wrapper_i_built_a_python_library_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oowuqd/unlocc_wrapper_i_built_a_python_library_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oowuqd/unlocc_wrapper_i_built_a_python_library_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T07:46:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oorvd0</id>
    <title>In light of Kimi Linear, reposting Minimax's article on Linear Attention</title>
    <updated>2025-11-05T03:08:13+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;My comments first:&lt;/p&gt; &lt;p&gt;&lt;a href="https://imgur.com/a/IpMMPxE"&gt;https://imgur.com/a/IpMMPxE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Kimi Linear once again showed stronger RULER scores in their paper with lower longbenchv2 scores. The problem which I complained about here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That's disastrous! Of the evals in that image, only LongBenchv2 is remotely similar to real world tests like Fiction.liveBench and it's the only one that's lower. Once again they are being misled by bad evals that will take you into the wrong direction. Multi-hop reasoning is EVERYTHING in real world agents.&lt;/p&gt; &lt;p&gt;Looking on X currently the new minimax is getting a lot of hype as the new hotness while kimi linear is already getting forgotten as far as I can tell.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;MiniMax M2 Tech Blog 3: Why Did M2 End Up as a Full Attention Model?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;On behave of pre-training lead Haohai Sun. (&lt;a href="https://zhihu.com/question/1965302088260104295/answer/1966810157473335067"&gt;https://zhihu.com/question/1965302088260104295/answer/1966810157473335067&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;I. Introduction&lt;/p&gt; &lt;p&gt;As the lead of MiniMax-M2 pretrain, I've been getting many queries from the community on &amp;quot;Why did you turn back the clock and go with full attention with MiniMax M2?&amp;quot; After explaining the backstory in one chat after another, I figured it's time to write down our journey in a blog.&lt;/p&gt; &lt;p&gt;Honestly, I could give you the textbook debate. I could talk all afternoon about why you should build linear/sparse attention. Then, I could turn around and talk all afternoon about why you shouldn't. But what's the point of all that hand-waving? The real question is whether you should actually do it.&lt;/p&gt; &lt;p&gt;So, let's start with the conclusion: We are always working on it. But in a real-world, industrial-grade system, the truth is that efficient attention still has some way to go before it can definitively beat full attention. As LLMs have evolved, the entire stack has become monstrously complex. We serve more scenarios, and the architecture design trade-offs are exploding: &amp;quot;How does it perform on code and math? What about agent scenarios? How does it handle multimodality? Does long-chain CoT still hold up? Can RL scale on top of it? Are there hidden traps with low-precision compute? How do you implement interleaved thinking, caching, or speculative decoding? ... &amp;quot;&lt;/p&gt; &lt;p&gt;In short, there's a vast difference between the promise on paper and its payoff in production. You only get to claim that payoff after satisfying Condition 1...n and solving Problem 1...n.&lt;/p&gt; &lt;p&gt;II. Why Efficient Attention?&lt;/p&gt; &lt;p&gt;Let's do a thought experiment. If you had infinite compute, would you even bother with linear or sparse attention? Some might bring up theoretical arguments about softmax attention &amp;quot;oversmoothing&amp;quot; in an infinite context... but who knows? Under the current compute bound, no model has truly pushed softmax attention to its absolute limit. So, for all practical purposes, the race for efficient attention is a race to save compute.&lt;/p&gt; &lt;p&gt;For our M2 design, could we aim to save tokens ‚Äî achieving the same quality with fewer tokens? Well if you believe in scaling laws, to achieve this goal, you'd probably bet on other paths to get there, not efficient attention.&lt;/p&gt; &lt;p&gt;So, the simple truth is this: Compute is finite. We need an architecture that makes better use of it ‚Äî models that achieve higher performance under the same budget (training &amp;amp; inference).&lt;/p&gt; &lt;p&gt;III. The Real Bottlenecks&lt;/p&gt; &lt;p&gt;To build a model that can practically be deployed and used by the community, we have to start with what users care: Quality, Speed (TPS), and Price. Quality is non-negotiable. A useless model is useless even if it's free. So how do we make a Linear/Sparse/Hybrid Attention model that performs well enough? The biggest challenge here isn‚Äôt the architecture design ‚Äî the real bottleneck is the limitations of evaluation. (As for speed and price, those are heavily influenced by the inference stack‚Äîand great models tend to attract great engineers to optimize them.)&lt;/p&gt; &lt;p&gt;The Evaluation Trap: Goodhart's Law in Action&lt;/p&gt; &lt;p&gt;‚ÄúAs long as you build the benchmark, I‚Äôll find a way to beat it.‚Äù Over the past few years of LLM development, the pace of leaderboard progress is staggering. No matter how hard a benchmark is ‚Äî even if the SOTA score starts in single digits ‚Äî once it catches the industry‚Äôs attention, it‚Äôs usually crushed within a few iterations. But how do you build an evaluation system that is comprehensive and actually reflects a model's true capabilities? That‚Äôs one of the hardest ‚Äî and most critical ‚Äî problems in LLM development, and it becomes even more acute when you start messing with a component as fundamental as attention.&lt;/p&gt; &lt;p&gt;Benchmarks are a Leaky Abstraction&lt;/p&gt; &lt;p&gt;There‚Äôs no free lunch. When you reduce the complexity of attention, you pay a price. The question is, where?&lt;/p&gt; &lt;p&gt;When we were developing MiniMax-Text-01, everyone was still evaluating MMLU, BBH, MATH, and LongBench (all of which are now saturated). From the perspective of a year ago, a hybrid of Lightning Attention and Full Attention looked just as good as pure full attention. Our own small-scale hybrid models confirmed this on the leaderboards. (Did we find a free lunch?)&lt;/p&gt; &lt;p&gt;Not quite. The price paid became obvious at a larger scale: the model had clear deficits in complex, multi-hop reasoning tasks.&lt;/p&gt; &lt;p&gt;Okay, once a problem is exposed, you can fix it. We developed proxy metrics for this specific weakness and iterated until the hybrid model seemed to match MHA. But does that proxy metric still correlate with real-world downstream performance at an even larger scale? Are there other hidden weaknesses? Who knows. We haven't run those experiments yet.&lt;/p&gt; &lt;p&gt;The better the models get, the harder they are to evaluate. But that‚Äôs a must part of the journey ‚Äî keep it up, eval teams!&lt;/p&gt; &lt;p&gt;The High Cost of Knowing Things&lt;/p&gt; &lt;p&gt;For complex reasoning tasks, we can sometimes find early proxy metrics that correlate well with final performance ‚Äî but not for all tasks (at least, not yet). As tasks get harder, the amount of experiment compute required just to get a statistically significant signal on your metric grows astronomically ‚Äî which is ironic, since we study efficient attention because compute is limited.&lt;/p&gt; &lt;p&gt;And beyond the academic benchmarks, optimization issues often only surface at scale. You never really know what‚Äôs going to happen until you scale up. Anyone who read our M1 paper will recall the serious precision issues we hit during RL training ‚Äî problems that would‚Äôve been spotted earlier. Going back and analyzing Lightning Attention's numerical convergence with that experience in hand was incredibly clarifying.&lt;/p&gt; &lt;p&gt;Discovering the real problems is often far harder than solving them.&lt;/p&gt; &lt;p&gt;A Symphony of Variables&lt;/p&gt; &lt;p&gt;There are just too many variables in model training. Different architectures behave very differently on different data distributions and with different optimizers. In a world where our data is constantly being updated, an experiment run on last month's data mix might yield the opposite conclusion today. We can‚Äôt observe everything perfectly ‚Äî but we‚Äôre working on finding more reliable experimental strategies.&lt;/p&gt; &lt;p&gt;Infrastructure: Where Theory Meets Metal&lt;/p&gt; &lt;p&gt;Compared to full attention, the infrastructure for linear and sparse attention is much less mature. To actually get the promised results, there‚Äôs still a lot of groundwork to fill in. Take linear attention for example: If you analyze the compute intensity of existing linear architectures, many of them are memory-bound ‚Äî even during training. Without extreme IO optimization, you‚Äôre basically leaving a huge amount of GPU FLOPs on the table. And inference brings even more challenges than training: How do you deliver a service that is genuinely faster and cheaper? Linear attention has linear compute complexity and constant memory usage. That means there‚Äôs a crossover point where it becomes more efficient than full attention in compute and memory. In theory, that point lies at a few thousand tokens ‚Äî which isn‚Äôt particularly long for today‚Äôs large models.&lt;/p&gt; &lt;p&gt;But that‚Äôs just theory. We need to solve a few key problems to actually approach it:&lt;/p&gt; &lt;p&gt;Low-Precision State Storage: Linear attention is currently far more sensitive to numerical precision than full attention.&lt;/p&gt; &lt;p&gt;Prefix Caching: In real-world applications, the cache-hit rate for conversations is very high. A new architecture must handle this gracefully.&lt;/p&gt; &lt;p&gt;Speculative Decoding: How do you optimize speculative decoding with linear attention backbone? Well fortunately, all of these seem solvable.&lt;/p&gt; &lt;p&gt;IV. What‚Äôs Next&lt;/p&gt; &lt;p&gt;Scaling remains the name of the game, and context scaling is one of the key problems. Longer and longer context length is key in both pre-training and post-training. As GPU compute growth slows while data length keeps increasing, the benefits of linear and sparse attention will gradually emerge. We should start preparing now:&lt;/p&gt; &lt;p&gt;Better Data: More multimodal, information-rich long-context data.&lt;/p&gt; &lt;p&gt;Better Evaluation: More informative evaluation system and experimental paradigms to speed up iteration.&lt;/p&gt; &lt;p&gt;Better Infrastructure: Mature training and inference infrastructure to fully squeeze out GPU potential.&lt;/p&gt; &lt;p&gt;V. Addendum: the SWA code...&lt;/p&gt; &lt;p&gt;We accidentally left the SWA inference code in the open-source release, and some people asked why it wasn‚Äôt used in the final model. Simple answer: the performance wasn't good enough.&lt;/p&gt; &lt;p&gt;That experiment was from quite early on, before GPT-OSS was open-sourced (we were pretty surprised to see its structure, by the way). But I can share a brief summary of our failed attempt. We tried adapting CPT into a Hybrid SWA, testing both inter &amp;amp; intra-layer mixing. The motivation for intra-layer mixing was to balance the compute intensity across all layers, which is friendly to both PP in training and PP or AFD during inference. Unfortunately, neither worked. Performance degraded noticeably as context length grew ‚Äî which is unacceptable in agentic scenarios.&lt;/p&gt; &lt;p&gt;Our analysis showed that many global attention patterns (like retrieval head and induction head) were already established early during pre-training. CPT can hardly adjust those patterns afterwards. You surely can mitigate the issue by using data probes to identify and keep those heads as full attention ‚Äî but unfortunately, it‚Äôs nearly impossible to discover them all from human priors.&lt;/p&gt; &lt;p&gt;(And no, this issue isn‚Äôt related to attention sinks.)&lt;/p&gt; &lt;p&gt;If you're interested in this line of research, I recommend taking a closer look at GPT-OSS, CWM, and Gemma, especially their long-context performance.&lt;/p&gt; &lt;p&gt;Finally, we‚Äôre hiring! If you want to join us, send your resume to &lt;a href="mailto:guixianren@minimaxi.com"&gt;guixianren@minimaxi.com&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;References&lt;/li&gt; &lt;li&gt;MiniMax-01: Scaling Foundation Models with Lightning Attention&lt;/li&gt; &lt;li&gt;MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention&lt;/li&gt; &lt;li&gt;CWM: An Open-Weights LLM for Research on Code Generation with World Models&lt;/li&gt; &lt;li&gt;Qwen3-Next&lt;/li&gt; &lt;li&gt;Gemma 3 Technical Report&lt;/li&gt; &lt;li&gt;gpt-oss-120b &amp;amp; gpt-oss-20b Model Card&lt;/li&gt; &lt;li&gt;Retrieval Head Mechanistically Explains Long-Context Factuality&lt;/li&gt; &lt;li&gt;&lt;a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"&gt;https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://x.com/zpysky1125/status/1983383094607347992"&gt;https://x.com/zpysky1125/status/1983383094607347992&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also I called it last month: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1nfyjv5/cmv_qwen3next_is_an_architectural_deadend_much/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oorvd0/in_light_of_kimi_linear_reposting_minimaxs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oorvd0/in_light_of_kimi_linear_reposting_minimaxs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oorvd0/in_light_of_kimi_linear_reposting_minimaxs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T03:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1oor5oc</id>
    <title>Potential external gpu hack/mod to try with DGX Spark/AI Max</title>
    <updated>2025-11-05T02:34:27+00:00</updated>
    <author>
      <name>/u/Ok_Top9254</name>
      <uri>https://old.reddit.com/user/Ok_Top9254</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oor5oc/potential_external_gpu_hackmod_to_try_with_dgx/"&gt; &lt;img alt="Potential external gpu hack/mod to try with DGX Spark/AI Max" src="https://preview.redd.it/szp97ulmoczf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f828f5308e3316d5ce2abeb5d2e466bd4418f3bb" title="Potential external gpu hack/mod to try with DGX Spark/AI Max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Techically both Strix Halo and DGX Spark have x4 m.2 slots that could be used to connect a gpu on riser (or any other pcie device). For boot you could just use PXE or portable linux through USB.&lt;/p&gt; &lt;p&gt;This could be pretty big since they are only good for MoE models anyway (just offload the top experts), and especially good for AI Max to boost its terrible prompt processing numbers even with the recent fixes.&lt;/p&gt; &lt;p&gt;Sorry if someone already tried, I seriously couldn't find this mentioned anywhere (either I'm really blind or jt got burried).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Top9254"&gt; /u/Ok_Top9254 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/szp97ulmoczf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oor5oc/potential_external_gpu_hackmod_to_try_with_dgx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oor5oc/potential_external_gpu_hackmod_to_try_with_dgx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T02:34:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomy4t</id>
    <title>NanoAgent ‚Äî A 135M Agentic LLM with Tool Calling That Runs on CPU</title>
    <updated>2025-11-04T23:27:16+00:00</updated>
    <author>
      <name>/u/TerribleDisaster0</name>
      <uri>https://old.reddit.com/user/TerribleDisaster0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone! I‚Äôm excited to share &lt;strong&gt;NanoAgent&lt;/strong&gt;, a &lt;strong&gt;135M parameter&lt;/strong&gt;, &lt;strong&gt;8k context&lt;/strong&gt; open-source model fine-tuned for &lt;strong&gt;agentic tasks&lt;/strong&gt; ‚Äî tool calling, instruction following, and lightweight reasoning ‚Äî all while being tiny enough (~135 MB in 8-bit) to run on a &lt;strong&gt;CPU or laptop&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Runs locally on CPU (tested on Mac M1, MLX framework)&lt;/li&gt; &lt;li&gt;Supports structured &lt;strong&gt;tool calling&lt;/strong&gt; (single &amp;amp; multi-tool)&lt;/li&gt; &lt;li&gt;Can parse &amp;amp; answer from web results via tools&lt;/li&gt; &lt;li&gt;Handles &lt;strong&gt;question decomposition&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ideal for &lt;strong&gt;edge AI agents&lt;/strong&gt;, &lt;strong&gt;copilots&lt;/strong&gt;, or &lt;strong&gt;IoT assistants&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/QuwsarOhi/NanoAgent"&gt;github.com/QuwsarOhi/NanoAgent&lt;/a&gt;&lt;br /&gt; Huggingface: &lt;a href="https://huggingface.co/quwsarohi/NanoAgent-135M"&gt;https://huggingface.co/quwsarohi/NanoAgent-135M&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The model is still experimental and it is trained on limited resources. Will be very happy to have comments and feedbacks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TerribleDisaster0"&gt; /u/TerribleDisaster0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomy4t/nanoagent_a_135m_agentic_llm_with_tool_calling/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:27:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oonomc</id>
    <title>Why the Strix Halo is a poor purchase for most people</title>
    <updated>2025-11-04T23:59:12+00:00</updated>
    <author>
      <name>/u/NeverEnPassant</name>
      <uri>https://old.reddit.com/user/NeverEnPassant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen a lot of posts that promote the Strix Halo as a good purchase, and I've often wondered if I should have purchased that myself. I've since learned a lot about how these models are executed. In this post I would like share empircal measurements, where I think those numbers come from, and make the case that few people should be purchasing this system. I hope you find it helpful!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Model under test&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;llama.cpp&lt;/li&gt; &lt;li&gt;Gpt-oss-120b&lt;/li&gt; &lt;li&gt;One the highest quality models that can run on mid range hardware. &lt;/li&gt; &lt;li&gt;Total size for this model is ~59GB and ~57GB of that are expert layers.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Systems under test&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;First system:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;128GB Strix Halo&lt;/li&gt; &lt;li&gt;Quad channel LDDR5-8000&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Second System (my system):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dual channel DDR5-6000 + pcie5 x16 + an rtx 5090&lt;/li&gt; &lt;li&gt;An rtx 5090 with the largest context size requires about 2/3 of the experts (38GB of data) to live in system RAM.&lt;/li&gt; &lt;li&gt;cuda backed&lt;/li&gt; &lt;li&gt;mmap off&lt;/li&gt; &lt;li&gt;batch 4096&lt;/li&gt; &lt;li&gt;ubatch 4096&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Here are user submitted numbers for the Strix Halo:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;997.70 ¬± 0.98&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;46.18 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d20000&lt;/td&gt; &lt;td align="right"&gt;364.25 ¬± 0.82&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d20000&lt;/td&gt; &lt;td align="right"&gt;18.16 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d48000&lt;/td&gt; &lt;td align="right"&gt;183.86 ¬± 0.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="right"&gt;10.80 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;What can we learn from this?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Performance is acceptable only at context 0. As context grows performance drops off a cliff for both prefill and decode.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;And here are numbers from my system:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;4065.77 ¬± 25.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;39.35 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d20000&lt;/td&gt; &lt;td align="right"&gt;3267.95 ¬± 27.74&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d20000&lt;/td&gt; &lt;td align="right"&gt;36.96 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pp4096 @ d48000&lt;/td&gt; &lt;td align="right"&gt;2497.25 ¬± 66.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="right"&gt;35.18 ¬± 0.62&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Wait a second, how are the decode numbers so close at context 0? The strix Halo has memory that is 2.5x faster than my system.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let's look closer at gpt-oss-120b. This model is 59 GB in size. There is roughly 0.76GB of layer data that is read for every single token. Since &lt;em&gt;every&lt;/em&gt; token needs this data, it is kept in VRAM. Each token also needs to read 4 arbitrary experts which is an additional 1.78 GB. Considering we can fit 1/3 of the experts in VRAM, this brings the total split to 1.35GB in VRAM and 1.18GB in system RAM at context 0.&lt;/p&gt; &lt;p&gt;Now VRAM on a 5090 is &lt;em&gt;much&lt;/em&gt; faster than both the Strix Halo unified memory and also dual channel DDR5-6000. When all is said and done, doing ~53% of your reads in ultra fast VRAM and 47% of your reads in somewhat slow system RAM, the decode time is roughly equal (a touch slower) than doing all your reads in Strix Halo's moderately fast memory.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Why does the Strix Halo have such a large slowdown in decode with large context?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's because when your context size grows, decode must also read the KV Cache once per layer. At 20k context, that is an extra ~4GB per token that needs to be read! Simple math (2.54 / 6.54) shows it should be run 0.38x as fast as context 0, and is almost exactly what we see in the chart above.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;And why does my system have a large lead in decode at larger context sizes?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;That's because all the KV Cache is stored in VRAM, which has ultra fast memory read. The decode time is dominated by the slow memory read in system RAM, so this barely moves the needle.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Why do prefill times degrade so quickly on the Strix Halo?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Good question! I would love to know!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Can I just add a GPU to the Strix Halo machine to improve my prefill?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Unfortunately not. The ability to leverage a GPU to improve prefill times depends heavily on the pcie bandwidth and the Strix Halo only offers pcie x4.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Real world measurements of the effect of pcie bandwidth on prefill&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;These tests were performed by changing BIOS settings on my machine.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="right"&gt;config&lt;/th&gt; &lt;th align="right"&gt;prefill tps&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="right"&gt;pcie5 x16&lt;/td&gt; &lt;td align="right"&gt;~4100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pcie4 x16&lt;/td&gt; &lt;td align="right"&gt;~2700&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="right"&gt;pcie4 x4&lt;/td&gt; &lt;td align="right"&gt;~1000&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Why is pci bandwidth so important?&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Here is my best high level understanding of what llama.cpp does with a gpu + cpu moe:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;First it runs the router on all 4096 tokens to determine what experts it needs for each token.&lt;/li&gt; &lt;li&gt;Each token will use 4 of 128 experts, so on average each expert will map to 128 tokens (4096 * 4 / 128).&lt;/li&gt; &lt;li&gt;Then for each expert, upload the weights to the GPU and run on all tokens that need that expert.&lt;/li&gt; &lt;li&gt;This is well worth it because prefill is compute intensive and just running it on the CPU is much slower.&lt;/li&gt; &lt;li&gt;This process is pipelined: you upload the weights for the next token, when running compute for the current.&lt;/li&gt; &lt;li&gt;Now all experts for gpt-oss-120b is ~57GB. That will take ~0.9s to upload using pcie5 x16 at its maximum 64GB/s. That places a ceiling in pp of ~4600tps.&lt;/li&gt; &lt;li&gt;For pcie4 x16 you will only get 32GB/s, so your maximum is ~2300tps. For pcie4 x4 like the Strix Halo via occulink, its 1/4 of this number.&lt;/li&gt; &lt;li&gt;In practice neither will get their full bandwidth, but the absolute ratios hold.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Other benefits of a normal computer with a rtx 5090&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Better cooling&lt;/li&gt; &lt;li&gt;Higher quality case&lt;/li&gt; &lt;li&gt;A 5090 will almost certainly have higher resale value than a Strix Halo machine&lt;/li&gt; &lt;li&gt;More extensible&lt;/li&gt; &lt;li&gt;More powerful CPU&lt;/li&gt; &lt;li&gt;Top tier gaming&lt;/li&gt; &lt;li&gt;Models that fit entirely in VRAM will absolutely &lt;em&gt;fly&lt;/em&gt;&lt;/li&gt; &lt;li&gt;Image generation will be much much faster.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;What is Strix Halo good for&lt;/em&gt;&lt;/strong&gt;*&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Extremely low idle power usage&lt;/li&gt; &lt;li&gt;It's small&lt;/li&gt; &lt;li&gt;Maybe all you care about is chat bots with close to 0 context&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you can afford an extra $1000-1500, you are much better off just building a normal computer with an rtx 5090. The value per dollar is just so much stronger. Even if you don't want to spend that kind of money, you should ask yourself if your use case is actually covered by the Strix Halo. Maybe buy nothing instead.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;Corrections&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Please correct me on anything I got wrong! I am just a novice!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I received a message that maybe llama.cpp + Strix Halo is not (fully?) leveraging it's NPU now, which should improve prefill numbers (but not decode). If anyone knows more about this or has preliminary benchmarks, please share them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Updated numbers from the latest llama someone commented here:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_batch&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp4096&lt;/td&gt; &lt;td align="right"&gt;1012.63 ¬± 0.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128&lt;/td&gt; &lt;td align="right"&gt;52.31 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp4096 @ d20000&lt;/td&gt; &lt;td align="right"&gt;357.27 ¬± 0.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d20000&lt;/td&gt; &lt;td align="right"&gt;32.46 ¬± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;pp4096 @ d48000&lt;/td&gt; &lt;td align="right"&gt;230.60 ¬± 0.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td&gt;ROCm,Vulkan&lt;/td&gt; &lt;td align="right"&gt;99&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;4096&lt;/td&gt; &lt;td align="right"&gt;1&lt;/td&gt; &lt;td align="right"&gt;0&lt;/td&gt; &lt;td align="right"&gt;tg128 @ d48000&lt;/td&gt; &lt;td align="right"&gt;32.76 ¬± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;WOW! The ddr5 kit I purchased in June has &lt;a href="https://camelcamelcamel.com/product/B0DFMFBVYP"&gt;doubled&lt;/a&gt; in price since I bought it. Maybe 50% more is now an underestimate.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeverEnPassant"&gt; /u/NeverEnPassant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oonomc/why_the_strix_halo_is_a_poor_purchase_for_most/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oonomc/why_the_strix_halo_is_a_poor_purchase_for_most/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oonomc/why_the_strix_halo_is_a_poor_purchase_for_most/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:59:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1onzrg9</id>
    <title>Qwen is roughly matching the entire American open model ecosystem today</title>
    <updated>2025-11-04T05:57:18+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt; &lt;img alt="Qwen is roughly matching the entire American open model ecosystem today" src="https://preview.redd.it/zvugibssj6zf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1b76885ebcc9a9fe34b1f3215330df073cc1f12" title="Qwen is roughly matching the entire American open model ecosystem today" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zvugibssj6zf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1onzrg9/qwen_is_roughly_matching_the_entire_american_open/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T05:57:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooxple</id>
    <title>GLM 4.6 AIR is coming....?</title>
    <updated>2025-11-05T08:43:40+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt; &lt;img alt="GLM 4.6 AIR is coming....?" src="https://preview.redd.it/56uu0u1fiezf1.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4748db1df48b7ce94e935ab40a291000e001166f" title="GLM 4.6 AIR is coming....?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;or not yet? What do you think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/56uu0u1fiezf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooxple/glm_46_air_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T08:43:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooi8lk</id>
    <title>I built a leaderboard for Rerankers</title>
    <updated>2025-11-04T20:24:00+00:00</updated>
    <author>
      <name>/u/tifa2up</name>
      <uri>https://old.reddit.com/user/tifa2up</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooi8lk/i_built_a_leaderboard_for_rerankers/"&gt; &lt;img alt="I built a leaderboard for Rerankers" src="https://preview.redd.it/lrdfuzpduazf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43e22daf4653431072cda6072129aec0e4f3f7e9" title="I built a leaderboard for Rerankers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is something that I wish I had when starting out.&lt;/p&gt; &lt;p&gt;When I built my first RAG project, I didn‚Äôt know what a reranker was. When I added one, I was blown away by how much of a quality improvement it added. Just 5 lines of code.&lt;/p&gt; &lt;p&gt;Like most people here, I defaulted to Cohere as it was the most popular.&lt;/p&gt; &lt;p&gt;Turns out there are better rerankers out there (and cheaper).&lt;/p&gt; &lt;p&gt;I built a leaderboard with the top reranking models: elo, accuracy, and latency compared.&lt;/p&gt; &lt;p&gt;I‚Äôll be keeping the leaderboard updated as new rerankers enter the arena. Let me kow if I should add any other ones.&lt;/p&gt; &lt;p&gt;&lt;a href="https://agentset.ai/leaderboard/rerankers"&gt;https://agentset.ai/leaderboard/rerankers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tifa2up"&gt; /u/tifa2up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lrdfuzpduazf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooi8lk/i_built_a_leaderboard_for_rerankers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooi8lk/i_built_a_leaderboard_for_rerankers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T20:24:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oo6226</id>
    <title>Disappointed by dgx spark</title>
    <updated>2025-11-04T12:43:28+00:00</updated>
    <author>
      <name>/u/RockstarVP</name>
      <uri>https://old.reddit.com/user/RockstarVP</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"&gt; &lt;img alt="Disappointed by dgx spark" src="https://preview.redd.it/a1tbzs1dk8zf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=090e0bdb3a3f9757ae6bdbff3964dc951a1361ed" title="Disappointed by dgx spark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just tried Nvidia dgx spark irl&lt;/p&gt; &lt;p&gt;gorgeous golden glow, feels like gpu royalty&lt;/p&gt; &lt;p&gt;‚Ä¶but 128gb shared ram still underperform whenrunning qwen 30b with context on vllm&lt;/p&gt; &lt;p&gt;for 5k usd, 3090 still king if you value raw speed over design&lt;/p&gt; &lt;p&gt;anyway, wont replce my mac anytime soon&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RockstarVP"&gt; /u/RockstarVP &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a1tbzs1dk8zf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oo6226/disappointed_by_dgx_spark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T12:43:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomxt6</id>
    <title>Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)</title>
    <updated>2025-11-04T23:26:53+00:00</updated>
    <author>
      <name>/u/vladlearns</name>
      <uri>https://old.reddit.com/user/vladlearns</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"&gt; &lt;img alt="Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)" src="https://preview.redd.it/tu7jitwzqbzf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=952d82b0d88e6a4bfa60cec0ff55072522800b4c" title="Tencent + Tsinghua just dropped a paper called Continuous Autoregressive Language Models (CALM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;STAY CALM! &lt;a href="https://arxiv.org/abs/2510.27688"&gt;https://arxiv.org/abs/2510.27688&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vladlearns"&gt; /u/vladlearns &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tu7jitwzqbzf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomxt6/tencent_tsinghua_just_dropped_a_paper_called/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:26:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oogvcw</id>
    <title>I implemented GPT-OSS from scratch in pure Python, without PyTorch or a GPU</title>
    <updated>2025-11-04T19:33:07+00:00</updated>
    <author>
      <name>/u/ultimate_code</name>
      <uri>https://old.reddit.com/user/ultimate_code</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have also written a detailed and beginner friendly blog that explains every single concept, from simple modules such as Softmax and RMSNorm, to more advanced ones like Grouped Query Attention. I tried to justify the architectural decision behind every layer as well. &lt;/p&gt; &lt;p&gt;Key concepts: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;Grouped Query Attention: with attention sinks and sliding window.&lt;/li&gt; &lt;li&gt;Mixture of Experts (MoE).&lt;/li&gt; &lt;li&gt;Rotary Position Embeddings (RoPE): with NTK-aware scaling.&lt;/li&gt; &lt;li&gt;Functional Modules: SwiGLU, RMSNorm, Softmax, Linear Layer.&lt;/li&gt; &lt;li&gt;Custom BFloat16 implementation in C++ for numerical precision.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you‚Äôve ever wanted to understand how modern LLMs really work, this repo + blog walk you through everything. I have also made sure that the implementation matches the official one in terms of numerical precision (check the &lt;a href="http://test.py"&gt;test.py&lt;/a&gt; file)&lt;/p&gt; &lt;p&gt;Blog: &lt;a href="https://projektjoe.com/blog/gptoss"&gt;https://projektjoe.com/blog/gptoss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/projektjoe/gpt-oss"&gt;https://github.com/projektjoe/gpt-oss&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love any feedback, ideas for extensions, or just thoughts from others exploring transformers from first principles!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ultimate_code"&gt; /u/ultimate_code &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oogvcw/i_implemented_gptoss_from_scratch_in_pure_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T19:33:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1oomyby</id>
    <title>Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment</title>
    <updated>2025-11-04T23:27:29+00:00</updated>
    <author>
      <name>/u/IonizedRay</name>
      <uri>https://old.reddit.com/user/IonizedRay</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"&gt; &lt;img alt="Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment" src="https://external-preview.redd.it/ZoS_79jqnoHBV3LsnI-z672RSVTI_TXKjzXarTZWduA.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7c34b5423e4b02583d66bdf088e63a71b2cb2167" title="Server DRAM prices surge up to 50% as AI-induced memory shortage hits hyperscaler supply ‚Äî U.S. and Chinese customers only getting 70% order fulfillment" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IonizedRay"&gt; /u/IonizedRay &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/storage/server-dram-prices-surge-50-percent"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oomyby/server_dram_prices_surge_up_to_50_as_aiinduced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T23:27:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oojwpj</id>
    <title>The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency</title>
    <updated>2025-11-04T21:30:05+00:00</updated>
    <author>
      <name>/u/Imakerocketengine</name>
      <uri>https://old.reddit.com/user/Imakerocketengine</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"&gt; &lt;img alt="The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency" src="https://b.thumbs.redditmedia.com/z3kkwZtyFH5pRyg7rq5O0EZiOQNj_2JQdPBRk1aeUeQ.jpg" title="The French Government Launches an LLM Leaderboard Comparable to LMarena, Emphasizing European Languages and Energy Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://comparia.beta.gouv.fr/"&gt;https://comparia.beta.gouv.fr/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imakerocketengine"&gt; /u/Imakerocketengine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oojwpj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oojwpj/the_french_government_launches_an_llm_leaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T21:30:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ooa342</id>
    <title>llama.cpp releases new official WebUI</title>
    <updated>2025-11-04T15:26:30+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"&gt; &lt;img alt="llama.cpp releases new official WebUI" src="https://external-preview.redd.it/3mPqb7hXnKE3QMeOYvmnNH3HEJEfsY-FkGb0pZ8tDhU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23cd274a23b9ffd23182c3f9522388baf8354b97" title="llama.cpp releases new official WebUI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16938"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ooa342/llamacpp_releases_new_official_webui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-04T15:26:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oosnaq</id>
    <title>New Qwen models are unbearable</title>
    <updated>2025-11-05T03:45:53+00:00</updated>
    <author>
      <name>/u/kevin_1994</name>
      <uri>https://old.reddit.com/user/kevin_1994</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using GPT-OSS-120B for the last couple months and recently thought I'd try Qwen3 32b VL and Qwen3 Next 80B. &lt;/p&gt; &lt;p&gt;They honestly might be worse than peak ChatGPT 4o. &lt;/p&gt; &lt;p&gt;Calling me a genius, telling me every idea of mine is brilliant, &amp;quot;this isnt just a great idea‚Äîyou're redefining what it means to be a software developer&amp;quot; type shit&lt;/p&gt; &lt;p&gt;I cant use these models because I cant trust them at all. They just agree with literally everything I say. &lt;/p&gt; &lt;p&gt;Has anyone found a way to make these models more usable? They have good benchmark scores so perhaps im not using them correctly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kevin_1994"&gt; /u/kevin_1994 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oosnaq/new_qwen_models_are_unbearable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-05T03:45:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
</feed>
