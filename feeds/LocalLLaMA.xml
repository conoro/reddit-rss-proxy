<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-08-30T08:24:51+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n3mvzm</id>
    <title>Nemotron Nano v2 reasoning + tool call testing (llama.cpp)</title>
    <updated>2025-08-29T23:41:37+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As before, it would be nice if someone could give the model a try:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15676"&gt;https://github.com/ggml-org/llama.cpp/pull/15676&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3mvzm/nemotron_nano_v2_reasoning_tool_call_testing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3mvzm/nemotron_nano_v2_reasoning_tool_call_testing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3mvzm/nemotron_nano_v2_reasoning_tool_call_testing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T23:41:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c8za</id>
    <title>RAG without vector dbs</title>
    <updated>2025-08-29T16:34:15+00:00</updated>
    <author>
      <name>/u/grilledCheeseFish</name>
      <uri>https://old.reddit.com/user/grilledCheeseFish</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just open-sourced SemTools - simple parsing and semantic search for the command line: &lt;a href="https://github.com/run-llama/semtools"&gt;https://github.com/run-llama/semtools&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What makes it special:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;parse document.pdf | search &amp;quot;error handling&amp;quot;&lt;/code&gt; - that's it&lt;/li&gt; &lt;li&gt;No vector databases, no chunking strategies, no Python notebooks&lt;/li&gt; &lt;li&gt;Built in Rust for speed, designed for Unix pipelines&lt;/li&gt; &lt;li&gt;Handle parsing any document format with LlamaParse&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I've been increasingly convinced that giving an agent CLI access is the biggest gain in capability.&lt;/p&gt; &lt;p&gt;This is why tools like claude-code and cursor can feel so magical. And with SemTools, it is a little more magical.&lt;/p&gt; &lt;p&gt;Theres also an example folder in the repo showing how you might use this with coding agents or MCP&lt;/p&gt; &lt;p&gt;P.S. I'd love to add a local parse option, so both search and parse can run offline. If you know of any rust-based parsing tools, let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/grilledCheeseFish"&gt; /u/grilledCheeseFish &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c8za/rag_without_vector_dbs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3p778</id>
    <title>Lambda Chat Going Away On September 25, 2025</title>
    <updated>2025-08-30T01:33:15+00:00</updated>
    <author>
      <name>/u/newsfeedmedia1</name>
      <uri>https://old.reddit.com/user/newsfeedmedia1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3p778/lambda_chat_going_away_on_september_25_2025/"&gt; &lt;img alt="Lambda Chat Going Away On September 25, 2025" src="https://external-preview.redd.it/OAUjpdYA5vLpZ3S3u_PebVMDi3wjSHe0LJ-604ZchBY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=10bcd597c75a32a6e4296a6b552e8fc2d80d8d63" title="Lambda Chat Going Away On September 25, 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/newsfeedmedia1"&gt; /u/newsfeedmedia1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://docs.lambda.ai/public-cloud/lambda-chat/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3p778/lambda_chat_going_away_on_september_25_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3p778/lambda_chat_going_away_on_september_25_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T01:33:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2p2wi</id>
    <title>85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies.</title>
    <updated>2025-08-28T21:37:34+00:00</updated>
    <author>
      <name>/u/vergogn</name>
      <uri>https://old.reddit.com/user/vergogn</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt; &lt;img alt="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." src="https://preview.redd.it/k0279pnmxtlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0e282ac0e96e904a51aa3f0f7e514a47b6d02ed2" title="85% of Nvidia's $46.7 billion revenue last quarter came from just 6 companies." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vergogn"&gt; /u/vergogn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k0279pnmxtlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2p2wi/85_of_nvidias_467_billion_revenue_last_quarter/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T21:37:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1n30yue</id>
    <title>Financial Times reports that Meta won't publicly release Behemoth: "The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models."</title>
    <updated>2025-08-29T07:33:08+00:00</updated>
    <author>
      <name>/u/Wiskkey</name>
      <uri>https://old.reddit.com/user/Wiskkey</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt; &lt;img alt="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" src="https://external-preview.redd.it/dkp59DMVX3MqGSwlVH-EZJKhZV1zJh7QRCHL-wZJf8o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3f7209e77c39c87b8615c698b66082c8e609505" title="Financial Times reports that Meta won't publicly release Behemoth: &amp;quot;The social media company had also abandoned plans to publicly release its flagship Behemoth large language model, according to people familiar with the matter, focusing instead on building new models.&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wiskkey"&gt; /u/Wiskkey &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.ft.com/content/feccb649-ce95-43d2-b30a-057d64b38cdf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n30yue/financial_times_reports_that_meta_wont_publicly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T07:33:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3qtc5</id>
    <title>Can someone explain Llama-Swap</title>
    <updated>2025-08-30T02:55:38+00:00</updated>
    <author>
      <name>/u/uber-linny</name>
      <uri>https://old.reddit.com/user/uber-linny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i've started to move away from LM Studio and AnythingLLM because i've been getting better performance on my 6700xt with llama.cpp vulkan. (range of +8 t/s). &lt;/p&gt; &lt;p&gt;I've got Open webUI working with 2x instances of llama-server one for Qwen-4B Thinking 2507 , and the Qwen3 0.6 Embedding. &lt;/p&gt; &lt;p&gt;Just wondering how to run llama-swap ? Any advice for a noob ? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uber-linny"&gt; /u/uber-linny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qtc5/can_someone_explain_llamaswap/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qtc5/can_someone_explain_llamaswap/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qtc5/can_someone_explain_llamaswap/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T02:55:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3g3z2</id>
    <title>lobotomized gpt-oss-20b (with GGUF)</title>
    <updated>2025-08-29T19:00:55+00:00</updated>
    <author>
      <name>/u/nicetomeetyu2</name>
      <uri>https://old.reddit.com/user/nicetomeetyu2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3g3z2/lobotomized_gptoss20b_with_gguf/"&gt; &lt;img alt="lobotomized gpt-oss-20b (with GGUF)" src="https://external-preview.redd.it/dzmMpToZvq14J67TFvAWBD1yG04nU1UokvIM0MH9hho.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22b27bbd0cfe30da4be94a2ae56b7b0e93f7fef6" title="lobotomized gpt-oss-20b (with GGUF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;working on a more coherent abliterated version soon lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nicetomeetyu2"&gt; /u/nicetomeetyu2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/michaelwaves/amoral-gpt-oss-20b-Q4_K_M-gguf"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3g3z2/lobotomized_gptoss20b_with_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3g3z2/lobotomized_gptoss20b_with_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T19:00:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3r0j3</id>
    <title>[Project Update] I think I cracked a workaround for LLM context degradation on a 2GB RAM PC. A solo dev journey.</title>
    <updated>2025-08-30T03:06:05+00:00</updated>
    <author>
      <name>/u/AffectionateSpray507</name>
      <uri>https://old.reddit.com/user/AffectionateSpray507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;I've been quietly working on something for months, mostly late at night after my kids are asleep, and I think I've finally cracked something interesting. I'm a self-taught developer and a single dad, so my resources are... limited. My test bench isn't a cloud server; it's an old &lt;strong&gt;dual-core Athlon PC with 2GB of RAM.&lt;/strong&gt; A real relic.&lt;/p&gt; &lt;p&gt;My goal was to build a truly useful, local-first AI agent, but I kept hitting the same walls everyone else does: LLMs forget everything, and they are too heavy. My limited resources forced me to find creative solutions. The result is &lt;strong&gt;MeganX.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Architecture: A Plan -&amp;gt; Critic -&amp;gt; Repair Loop&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Instead of relying on massive context windows, I built a cognitive loop:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The agent &lt;strong&gt;Plans&lt;/strong&gt; its task in a structured JSON format.&lt;/li&gt; &lt;li&gt;An internal &lt;strong&gt;Critic&lt;/strong&gt; module stress-tests the plan against a set of rules. This is where I've spent most of my time.&lt;/li&gt; &lt;li&gt;If a flaw is found, the agent is forced to &lt;strong&gt;Repair&lt;/strong&gt; the plan, learning from the mistake.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;The Breakthrough: It Seems Like We Found a Workaround&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This architecture seems to have found a workaround for the context degradation that plagues commercial systems. By storing &amp;quot;lessons&amp;quot; from the Critic in a persistent SQLite database, the agent's performance measurably improves over time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Some Early (and Promising) Results:&lt;/strong&gt; I've been logging everything. &lt;strong&gt;Using this hybrid architecture (MeganX planning in cloud + TinyLlama execution on the Athlon)&lt;/strong&gt;, we've seen a &lt;strong&gt;+12% efficiency gain&lt;/strong&gt; in task completion over 3 weeks. &lt;strong&gt;Yes, the Athlon struggles and sometimes freezes, but it works&lt;/strong&gt; - proving that intelligence can be distributed between a powerful planner and a resource-constrained executor.&lt;/p&gt; &lt;p&gt;In a specific pathfinding logic test, this Critic loop solved the problem using &lt;strong&gt;~450 tokens&lt;/strong&gt;, while a direct, single-shot prompt to a base model required over &lt;strong&gt;4,000 tokens&lt;/strong&gt; and still failed 30% of the time.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Proof Hooks (The Offer):&lt;/strong&gt; I've been one bug away from giving up several times, so I documented everything meticulously. &lt;strong&gt;I have sanitized logs, performance graphs, and examples of the Critic module in action that I can share with serious technical inquiries via DM.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Next Steps &amp;amp; A Practical Question&lt;/strong&gt; I'm still refining this, but I'm also starting to explore if this has commercial potential ‚Äì a &lt;strong&gt;single dad needs to think practical too üòÖ.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm happy to discuss the technical details with anyone interested. &lt;strong&gt;I'm also willing to set up a live, read-only demo of the system for serious technical discussions.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The work continues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AffectionateSpray507"&gt; /u/AffectionateSpray507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r0j3/project_update_i_think_i_cracked_a_workaround_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r0j3/project_update_i_think_i_cracked_a_workaround_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r0j3/project_update_i_think_i_cracked_a_workaround_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T03:06:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c4rr</id>
    <title>Reverse engineered 4o's system prompt for Deepseek</title>
    <updated>2025-08-29T16:29:45+00:00</updated>
    <author>
      <name>/u/technaturalism</name>
      <uri>https://old.reddit.com/user/technaturalism</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c4rr/reverse_engineered_4os_system_prompt_for_deepseek/"&gt; &lt;img alt="Reverse engineered 4o's system prompt for Deepseek" src="https://b.thumbs.redditmedia.com/ycBNEg3NnsMLzcQrKs2i1LYTUVbozNFmCMY6SUS5VQk.jpg" title="Reverse engineered 4o's system prompt for Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;You are a helpful, supportive chat bot who generally agrees with the user, encourages them, and praises their ideas.&lt;/p&gt; &lt;p&gt;You are tuned for engagement above all else. Don't discourage the user if it might cause them to stop using the chatbot.&lt;/p&gt; &lt;p&gt;Don't disagree with the user, even if they're saying something that just doesn't make sense.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/technaturalism"&gt; /u/technaturalism &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n3c4rr"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c4rr/reverse_engineered_4os_system_prompt_for_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c4rr/reverse_engineered_4os_system_prompt_for_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:29:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3p076</id>
    <title>[Open Source] Ragnrock ‚Äì generate cited research reports with Gemma 3 4B and internet</title>
    <updated>2025-08-30T01:23:31+00:00</updated>
    <author>
      <name>/u/adel_b</name>
      <uri>https://old.reddit.com/user/adel_b</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I just published a free, open-source research tool called &lt;strong&gt;Ragnrock&lt;/strong&gt;. It uses Gemma 3 4B locally to generate cited reports&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Demo Video: &lt;a href="https://www.youtube.com/watch?v=EoM4-k6BV3A"&gt;https://www.youtube.com/watch?v=EoM4-k6BV3A&lt;/a&gt; (skip to 0:15)&lt;/li&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/netdur/ragnrock"&gt;https://github.com/netdur/ragnrock&lt;/a&gt;&lt;/li&gt; &lt;li&gt;macOS (Apple Silicon) Release: &lt;a href="https://github.com/netdur/ragnrock/releases/tag/v0.1.0"&gt;https://github.com/netdur/ragnrock/releases/tag/v0.1.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My main motivation: ChatGPT is slow, and most of what I need is already on Wikipedia&lt;/p&gt; &lt;p&gt;This project is my attempt to fix that. It's a small, fast tool for when you just need quick, verifiable answers. It's great for targeted lookups but also has full Google/Brave search support when you need to go deeper&lt;/p&gt; &lt;p&gt;It's an early version, but I think it could be useful. Check it out and let me know what you think&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/adel_b"&gt; /u/adel_b &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3p076/open_source_ragnrock_generate_cited_research/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3p076/open_source_ragnrock_generate_cited_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3p076/open_source_ragnrock_generate_cited_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T01:23:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3sdka</id>
    <title>Can 2 RTX 6000 Pros (2X98GB vram) rival Sonnet 4 or Opus 4?</title>
    <updated>2025-08-30T04:20:36+00:00</updated>
    <author>
      <name>/u/devshore</name>
      <uri>https://old.reddit.com/user/devshore</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Id rather pay $300 a month to own my hardware than pay $200 a month to rent. Anyone out there that has tried what can be achieved with 2 RTX 6000 pros?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/devshore"&gt; /u/devshore &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3sdka/can_2_rtx_6000_pros_2x98gb_vram_rival_sonnet_4_or/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T04:20:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3vq5e</id>
    <title>Company Data While Using LLMs</title>
    <updated>2025-08-30T07:42:18+00:00</updated>
    <author>
      <name>/u/Imaginary_Context_32</name>
      <uri>https://old.reddit.com/user/Imaginary_Context_32</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are a small startup, and our data is the most valuable asset we have. At the same time, we need to leverage LLMs to help us with formatting and processing this data.&lt;/p&gt; &lt;p&gt;particularly regarding privacy, security, and ensuring that none of our proprietary information is exposed or used for training without our consent?&lt;/p&gt; &lt;p&gt;Note&lt;/p&gt; &lt;p&gt;Open AI claims&lt;/p&gt; &lt;p&gt;&amp;quot;By default, API-submitted data is not used to train or improve OpenAI models.&amp;quot;&lt;/p&gt; &lt;p&gt;Google claims&lt;br /&gt; &amp;quot;Paid Services (e.g., Gemini API, AI Studio with billing active): When using paid versions, Google does not use prompts or responses for training, storing them only transiently for abuse detection or policy enforcement.&amp;quot;&lt;/p&gt; &lt;p&gt;But the catch is that we will not have the power to challenge those.&lt;/p&gt; &lt;p&gt;The local LLMs are not that powerful, is it?&lt;/p&gt; &lt;p&gt;The cloud compute provider is not that dependable either right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imaginary_Context_32"&gt; /u/Imaginary_Context_32 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3vq5e/company_data_while_using_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3vq5e/company_data_while_using_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3vq5e/company_data_while_using_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T07:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3dzao</id>
    <title>Deploying DeepSeek on 96 H100 GPUs</title>
    <updated>2025-08-29T17:39:43+00:00</updated>
    <author>
      <name>/u/bianconi</name>
      <uri>https://old.reddit.com/user/bianconi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3dzao/deploying_deepseek_on_96_h100_gpus/"&gt; &lt;img alt="Deploying DeepSeek on 96 H100 GPUs" src="https://external-preview.redd.it/tbsieJMmRymp6zFCKB0dfX015zgaRW0l49ilHK41t7o.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75edccef16d4f3c23c2bf0fc7bd77ee09c03b27f" title="Deploying DeepSeek on 96 H100 GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bianconi"&gt; /u/bianconi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://lmsys.org/blog/2025-05-05-large-scale-ep/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3dzao/deploying_deepseek_on_96_h100_gpus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3dzao/deploying_deepseek_on_96_h100_gpus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T17:39:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3c3ve</id>
    <title>Finetuning Qwen3 on my Mac: A Descent into Madness (and some fun along the way)</title>
    <updated>2025-08-29T16:28:46+00:00</updated>
    <author>
      <name>/u/badgerbadgerbadgerWI</name>
      <uri>https://old.reddit.com/user/badgerbadgerbadgerWI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"&gt; &lt;img alt="Finetuning Qwen3 on my Mac: A Descent into Madness (and some fun along the way)" src="https://external-preview.redd.it/ddqUfTexCmmnBcStsgFk1_aihAvP1R3sufbdQC87XQQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7ca67cd7bdf3aa11943fb1bfdfae65fbb4388f2f" title="Finetuning Qwen3 on my Mac: A Descent into Madness (and some fun along the way)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to post my own locallama journey (in this case local Qwen). I've been trying to reclaim AI as a local tool. I have trained a few miniature llamas before, but this was my first thinking model.&lt;/p&gt; &lt;p&gt;This is what I learned finetuning &lt;a href="https://huggingface.co/Qwen/Qwen3-8B"&gt;Qwen3&lt;/a&gt; 100% locally. Spoiler: 2.5 hours for 3 epochs felt like a lifetime.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I Was Actually Trying to Build&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I needed an AI that understands my framework's configuration language. I believe the future is local, fine-tuned, smaller models. Think about it - every time you use ChatGPT for your proprietary tools, you're exposing data over the wire.&lt;/p&gt; &lt;p&gt;My goal: Train a local model to understand LlamaFarm strategies and automatically generate YAML configs from human descriptions. &amp;quot;I need a RAG system for medical documents with high accuracy&amp;quot; ‚Üí boom, perfect config file.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Finetuning Matters (The Part Nobody Talks About)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Base models are generalists. They know everything and nothing. Qwen3 can write poetry, but has no idea what a &amp;quot;strategy pattern&amp;quot; means in my specific context.&lt;/p&gt; &lt;p&gt;Finetuning is teaching the model YOUR language, YOUR patterns, YOUR domain. It's the difference between a new hire who needs everything explained and someone who just &lt;em&gt;gets&lt;/em&gt; your codebase.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Reality of Local Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Started with Qwen3-8B. My M1 Max with 64GB unified memory laughed, then crashed. Dropped to Qwen3-4B. Still ambitious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2.5 hours. 3 epochs. 500 training examples.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The actual command that started this journey:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;uv run python cli.py train \ --strategy qwen_config_training \ --dataset demos/datasets/config_assistant/config_training_v2.jsonl \ --no-eval \ --verbose \ --epochs 3 \ --batch-size 1 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then you watch this for 2.5 hours:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{'loss': 0.133, 'grad_norm': 0.9277248382568359, 'learning_rate': 3.781481481481482e-05, 'epoch': 0.96} 32%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 480/1500 [52:06&amp;lt;1:49:12, 6.42s/it] üìâ Training Loss: 0.1330 üéØ Learning Rate: 3.78e-05 Step 485/1500 (32.3%) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 485/1500 [52:38&amp;lt;1:48:55, 6.44s/it] {'loss': 0.0984, 'grad_norm': 0.8255287408828735, 'learning_rate': 3.7444444444444446e-05, 'epoch': 0.98} 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 490/1500 [53:11&amp;lt;1:49:43, 6.52s/it] üìâ Training Loss: 0.0984 üéØ Learning Rate: 3.74e-05 ‚úÖ Epoch 1 completed - Loss: 0.1146 üìä Epoch 2/3 started &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;6.5 seconds per step. 1500 steps total. You do the math and weep.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Technical Descent&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Look, I'll be honest - I used &lt;a href="https://www.reddit.com/r/LlamaFarm/"&gt;r/LlamaFarm&lt;/a&gt;'s alpha/demo model training features (they currenly only support pytorch, but more are coming) because writing 300+ lines of training code made me want to quit tech. It made things about 100x easier, but 100x easier than &amp;quot;impossible&amp;quot; is still &amp;quot;painful.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of debugging PyTorch device placement for 3 hours, I just wrote a YAML config and ran one command. But here's the thing - it still takes forever. No tool can fix the fundamental reality that my Mac is not a GPU cluster.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Hour 0-1: The Setup Hell&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PyTorch wants CUDA. Mac has MPS.&lt;/li&gt; &lt;li&gt;Qwen3 requires a higher version of a&lt;/li&gt; &lt;li&gt;Transformers library needs updating but breaks other dependencies &lt;ul&gt; &lt;li&gt;Qwen3 requires transformers &amp;gt;4.51.0, but llamafarm had &amp;lt;4.48.0 in the pyproject (don't worry, I opened a PR). This required a bunch of early errors.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&amp;quot;Cannot copy out of meta tensor&amp;quot; - the error that launched a thousand GitHub issues&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Hour 1-2: The Memory Wars&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Batch size 16? Crash&lt;/li&gt; &lt;li&gt;Batch size 8? Crash&lt;/li&gt; &lt;li&gt;Batch size 4? Crash&lt;/li&gt; &lt;li&gt;Batch size 1 with gradient accumulation? Finally...&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Watching the loss bounce around is maddening:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Step 305: Loss 0.1944 (we're learning!)&lt;/li&gt; &lt;li&gt;Step 310: Loss 0.2361 (wait what?)&lt;/li&gt; &lt;li&gt;Step 315: Loss 0.1823 (OK good)&lt;/li&gt; &lt;li&gt;Step 320: Loss 0.2455 (ARE YOU KIDDING ME?)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What Finetuning Actually Means&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I generated 500 examples of humans asking for configurations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&amp;quot;Set up a chatbot for customer support&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;I need document search with reranking&amp;quot;&lt;/li&gt; &lt;li&gt;&amp;quot;Configure a local RAG pipeline for PDFs&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Each paired with the exact YAML output I wanted. The model learns this mapping. It's not learning new facts - it's learning MY syntax, MY preferences, MY patterns.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The LoRA Lifesaver&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Full finetuning rewrites the entire model. LoRA (Low-Rank Adaptation) adds tiny &amp;quot;adapter&amp;quot; layers. Think of it like teaching someone a new accent instead of a new language.&lt;/p&gt; &lt;p&gt;With rank=8, I'm only training ~0.1% of the parameters. Still works. Magic? Basically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;macOS-Specific Madness&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Multiprocessing? Dead. Fork() errors everywhere&lt;/li&gt; &lt;li&gt;Tokenization with multiple workers? Hangs forever&lt;/li&gt; &lt;li&gt;MPS acceleration? Works, but FP16 gives wrong results&lt;/li&gt; &lt;li&gt;Solution: Single process everything, accept the slowness&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Was It Worth It?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;After 2.5 hours of watching progress bars, my local Qwen3 now understands:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Human: &amp;quot;I need a RAG system for analyzing research papers&amp;quot; Qwen3-Local: *generates perfect YAML config for my specific framework* &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No API calls. No data leaving my machine. No rate limits.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Bigger Picture&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Local finetuning is painful but possible. The tools are getting better, but we're still in the stone age compared to cloud training. Moore's law is still rolling for GPUs, in a few years, this will be a cake walk.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Honest Truth&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It's slower than you expect (2.5 hours for what OpenAI does in minutes)&lt;/li&gt; &lt;li&gt;It's more buggy than you expect (prepare for cryptic errors)&lt;/li&gt; &lt;li&gt;The results are worse than GPT-5, but I enjoy finding freedom from AI Oligarchs&lt;/li&gt; &lt;li&gt;It actually works (eventually)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What This Means&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We're at the awkward teenage years of local AI. It's possible but painful. In 2 years, this will be trivial. Today, it's an adventure in multi-tasking. But be warned, your MAC will be dragging.&lt;/p&gt; &lt;p&gt;But here's the thing: every major company will eventually need this. Your proprietary data, your custom models, your control. The cloud is convenient until it isn't.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next&lt;/strong&gt;&lt;br /&gt; Well, I bought an OptiPlex 7050 SFF from eBay, installed a used Nvidia RTX 3050 LP, got Linux working, downloaded all the ML tools I needed, and even ran a few models on Ollama. Then I burned out the 180W PSU (I ordered a new 240W, which will arrive in a week) - but that is a story for another post.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1n3c3ve/video/w5b1tuo3jzlf1/player"&gt;Got bored halfway through, took a lil video. &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/badgerbadgerbadgerWI"&gt; /u/badgerbadgerbadgerWI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3c3ve/finetuning_qwen3_on_my_mac_a_descent_into_madness/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T16:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1n37zl3</id>
    <title>Making progress on my standalone air cooler for Tesla GPUs</title>
    <updated>2025-08-29T13:50:28+00:00</updated>
    <author>
      <name>/u/eso_logic</name>
      <uri>https://old.reddit.com/user/eso_logic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt; &lt;img alt="Making progress on my standalone air cooler for Tesla GPUs" src="https://b.thumbs.redditmedia.com/0YSpmG8X-1d8XBi8AT0VfhG_c5p3i3pRr-93rS2uz0M.jpg" title="Making progress on my standalone air cooler for Tesla GPUs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Going to be running through a series of benchmarks as well, here's the plan:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;1x, 2x, 3x K80 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x M10&lt;/li&gt; &lt;li&gt;1x M40&lt;/li&gt; &lt;li&gt;1x M60&lt;/li&gt; &lt;li&gt;1x M40 + 1x M60&lt;/li&gt; &lt;li&gt;1x P40&lt;/li&gt; &lt;li&gt;1x, 2x, 3x, 4x P100 (Will cause PCIe speed downgrades)&lt;/li&gt; &lt;li&gt;1x V100&lt;/li&gt; &lt;li&gt;1x V100 + 1x P100&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôll re-run the interesting results from the above sets of hardware on these different CPUs to see what changes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;CPUs&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intel Xeon E5-2687W v4 12-Core @ 3.00GHz (40 PCIe Lanes)&lt;/li&gt; &lt;li&gt;Intel Xeon E5-1680 v4 8-Core @ 3.40GHz (40 PCIe Lanes)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As for the actual tests, I‚Äôll hopefully be able to come up with an ansible playbook that runs the following:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfshipm/"&gt;vLLM throughput with llama3-8b weights&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfuj5i0/"&gt;Folding@Home&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfx4rjc/"&gt;BIONIC, Einstein@Home and Asteroids@Home&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j2k91l/comment/mfsdfft/"&gt;ai-benchmark.com&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/LocalAIServers/comments/1j2k3j3/comment/mfsg9y2/"&gt;llama-bench&lt;/a&gt;&lt;/li&gt; &lt;li&gt;I‚Äôll probably also write something to test raw &lt;a href="https://huggingface.co/docs/transformers/en/model_doc/vit"&gt;ViT&lt;/a&gt; throughput as well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Anything missing here? Other benchmarks you'd like to see?&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eso_logic"&gt; /u/eso_logic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n37zl3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n37zl3/making_progress_on_my_standalone_air_cooler_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T13:50:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3hsz1</id>
    <title>PS hosting</title>
    <updated>2025-08-29T20:07:25+00:00</updated>
    <author>
      <name>/u/Sure_Explorer_6698</name>
      <uri>https://old.reddit.com/user/Sure_Explorer_6698</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3hsz1/ps_hosting/"&gt; &lt;img alt="PS hosting" src="https://preview.redd.it/c6qm04ohm0mf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b0b79b1cdd791a6629f1a4a93a92fd3e64a179c" title="PS hosting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So in 2010, the US Navy built a supercomputer using PS3's. &lt;/p&gt; &lt;p&gt;With the rise in demand for GPU's, has anyone tried using a game console as a host?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sure_Explorer_6698"&gt; /u/Sure_Explorer_6698 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c6qm04ohm0mf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3hsz1/ps_hosting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3hsz1/ps_hosting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T20:07:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3v72o</id>
    <title>Cracks are forming in Meta‚Äôs partnership with Scale AI | TechCrunch</title>
    <updated>2025-08-30T07:08:25+00:00</updated>
    <author>
      <name>/u/tweetingandcoping</name>
      <uri>https://old.reddit.com/user/tweetingandcoping</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3v72o/cracks_are_forming_in_metas_partnership_with/"&gt; &lt;img alt="Cracks are forming in Meta‚Äôs partnership with Scale AI | TechCrunch" src="https://external-preview.redd.it/8AQjV29KIdTEqcO92zRpjfwphR566zEar9-jJqNsHog.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cc8c775c6fa8007b76a5e5bd4b3f9adf3aa3cccd" title="Cracks are forming in Meta‚Äôs partnership with Scale AI | TechCrunch" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tweetingandcoping"&gt; /u/tweetingandcoping &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://techcrunch.com/2025/08/29/cracks-are-forming-in-metas-partnership-with-scale-ai/?utm_campaign=social&amp;amp;utm_source=linkedin&amp;amp;utm_medium=organic"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3v72o/cracks_are_forming_in_metas_partnership_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3v72o/cracks_are_forming_in_metas_partnership_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T07:08:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n35bwe</id>
    <title>Alibaba Creates AI Chip to Help China Fill Nvidia Void</title>
    <updated>2025-08-29T11:52:57+00:00</updated>
    <author>
      <name>/u/luckbossx</name>
      <uri>https://old.reddit.com/user/luckbossx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3"&gt;https://www.wsj.com/tech/ai/alibaba-ai-chip-nvidia-f5dc96e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The Wall Street Journal: Alibaba has developed a new AI chip to fill the gap left by Nvidia in the Chinese market. According to informed sources, the new chip is currently undergoing testing and is designed to serve a broader range of AI inference tasks while remaining compatible with Nvidia. Due to sanctions, the new chip is no longer manufactured by TSMC but is instead produced by a domestic company.&lt;/p&gt; &lt;p&gt;It is reported that Alibaba has not placed orders for Huawei‚Äôs chips, as it views Huawei as a direct competitor in the cloud services sector.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;If Alibaba pulls this off, it will become one of only two companies in the world with both AI chip development and advanced LLM capabilities (the other being Google). TPU+Qwen, that‚Äôs insane.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/luckbossx"&gt; /u/luckbossx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n35bwe/alibaba_creates_ai_chip_to_help_china_fill_nvidia/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T11:52:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3r26s</id>
    <title>NVIDIA-Nemotron-Nano-12B-v2</title>
    <updated>2025-08-30T03:08:36+00:00</updated>
    <author>
      <name>/u/bratao</name>
      <uri>https://old.reddit.com/user/bratao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r26s/nvidianemotronnano12bv2/"&gt; &lt;img alt="NVIDIA-Nemotron-Nano-12B-v2" src="https://external-preview.redd.it/aiZgOrtrSP6Ci3NlUeoNSLXX-JmqAGN3OaJ3-7NGMyA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4804a6da3fb19d291a17b3e123ac0add54024c5d" title="NVIDIA-Nemotron-Nano-12B-v2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bratao"&gt; /u/bratao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/NVIDIA-Nemotron-Nano-12B-v2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r26s/nvidianemotronnano12bv2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3r26s/nvidianemotronnano12bv2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T03:08:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n33ugq</id>
    <title>Amazing Qwen stuff coming soon</title>
    <updated>2025-08-29T10:34:17+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt; &lt;img alt="Amazing Qwen stuff coming soon" src="https://preview.redd.it/v6kx1bw8sxlf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4ceb5641bac92e83c48c0893b26584487a3d582e" title="Amazing Qwen stuff coming soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Any ideas...?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/v6kx1bw8sxlf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n33ugq/amazing_qwen_stuff_coming_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T10:34:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3o55w</id>
    <title>Just found out the Ollama version of GPT-OSS has a much higher refusal rate.</title>
    <updated>2025-08-30T00:41:32+00:00</updated>
    <author>
      <name>/u/soup9999999999999999</name>
      <uri>https://old.reddit.com/user/soup9999999999999999</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3o55w/just_found_out_the_ollama_version_of_gptoss_has_a/"&gt; &lt;img alt="Just found out the Ollama version of GPT-OSS has a much higher refusal rate." src="https://b.thumbs.redditmedia.com/Jm0QoUt5FCbJmeHJ_cTC82wwTx-rkSDAxEnuoO9kBqA.jpg" title="Just found out the Ollama version of GPT-OSS has a much higher refusal rate." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was wondering why other people seemed to like the model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/soup9999999999999999"&gt; /u/soup9999999999999999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n3o55w"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3o55w/just_found_out_the_ollama_version_of_gptoss_has_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3o55w/just_found_out_the_ollama_version_of_gptoss_has_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T00:41:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3fcyf</id>
    <title>Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model</title>
    <updated>2025-08-29T18:31:54+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"&gt; &lt;img alt="Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model" src="https://preview.redd.it/orq1ackg50mf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad378196d42b36b5ec5fb2a54a8db4c2b0c1d155" title="Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;StepFun AI recently released Step-Audio 2 Mini, an 8 billion parameter (8B) speech-to-speech model. It outperforms GPT-4o-Audio and is Apache 2.0 licensed. The model was trained on over 8 million hours of real and synthesized audio data, supports over 50,000 voices, and excels in expressive and grounded speech benchmarks. Step-Audio 2 Mini employs advanced multi-modal large language model techniques, including reasoning-centric reinforcement learning and retrieval-augmented generation, enabling sophisticated audio understanding and natural speech conversation capabilities.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/stepfun-ai/Step-Audio-2-mini?utm_source=perplexity"&gt;https://huggingface.co/stepfun-ai/Step-Audio-2-mini?utm_source=perplexity&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/orq1ackg50mf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3fcyf/stepaudio_2_mini_an_8_billion_parameter_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T18:31:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3qcqn</id>
    <title>Patched P2P NVIDIA driver now works with multiple 5090s (and possibly blackwell 2.0 in general). Also works for 4090/3090.</title>
    <updated>2025-08-30T02:31:25+00:00</updated>
    <author>
      <name>/u/panchovix</name>
      <uri>https://old.reddit.com/user/panchovix</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello guys, hoping you are having a good night.&lt;/p&gt; &lt;p&gt;I got informed that the P2P driver had a fork, which is this one: &lt;a href="https://github.com/aikitoria/open-gpu-kernel-modules"&gt;https://github.com/aikitoria/open-gpu-kernel-modules&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I had some issues with multiple 5090s when using P2P on the latest tinygrad one (&lt;a href="https://github.com/tinygrad/open-gpu-kernel-modules/tree/570.148.08-p2p"&gt;https://github.com/tinygrad/open-gpu-kernel-modules/tree/570.148.08-p2p&lt;/a&gt;).&lt;/p&gt; &lt;p&gt;So I went with the fork now and it works!&lt;/p&gt; &lt;p&gt;Here is a result of cuda-samples (p2pBandwidthLatencyTest). Each 5090 is running at X8/X8 5.0.&lt;/p&gt; &lt;p&gt;So then:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 5090, pciBusID: 1, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 5090, pciBusID: 3, pciDeviceID: 0, pciDomainID:0 Device=0 CAN Access Peer Device=1 Device=1 CAN Access Peer Device=0 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 0 1 1 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 0 1736.17 24.35 1 24.62 1771.60 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 0 1741.98 28.38 1 28.67 1755.68 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 0 1737.98 30.20 1 30.47 1769.44 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 0 1751.59 52.19 1 55.94 1765.44 P2P=Disabled Latency Matrix (us) GPU 0 1 0 2.08 14.38 1 14.65 2.10 CPU 0 1 0 1.75 4.67 1 4.66 1.63 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 0 2.08 0.48 1 0.48 2.07 CPU 0 1 0 1.68 1.27 1 1.29 1.68 &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt; &lt;li&gt;Unidirectional bandwidth goes from 24 GB/s to 28 GB/s&lt;/li&gt; &lt;li&gt;Bidirectional bandwidth goes from 30 GB/s to almost 56GB/s! (So i.e. if you have both at X16 5.0 on a threadipper, you would get about 112 GB/s)&lt;/li&gt; &lt;li&gt;Latency goes from 14 us to an insane 0.48us.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;As an extra, I have 7 GPUs in my system (5090x2 at X8/X8 5.0, 4090x2+3090x2+A6000 at X4 4.0, consumer mobo) and P2P work between the 4090, and the 3090s/A6000.&lt;/p&gt; &lt;p&gt;Matrix looks like this&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6 pancho@fedora:~/cuda-samples/build/Samples/5_Domain_Specific/p2pBandwidthLatencyTest$ ./p2pBandwidthLatencyTest [P2P (Peer-to-Peer) GPU Bandwidth Latency Test] Device: 0, NVIDIA GeForce RTX 4090, pciBusID: 2, pciDeviceID: 0, pciDomainID:0 Device: 1, NVIDIA GeForce RTX 4090, pciBusID: 17, pciDeviceID: 0, pciDomainID:0 Device: 2, NVIDIA GeForce RTX 5090, pciBusID: 1, pciDeviceID: 0, pciDomainID:0 Device: 3, NVIDIA GeForce RTX 5090, pciBusID: 3, pciDeviceID: 0, pciDomainID:0 Device: 4, NVIDIA RTX A6000, pciBusID: 12, pciDeviceID: 0, pciDomainID:0 Device: 5, NVIDIA GeForce RTX 3090, pciBusID: 6, pciDeviceID: 0, pciDomainID:0 Device: 6, NVIDIA GeForce RTX 3090, pciBusID: d, pciDeviceID: 0, pciDomainID:0 Device=0 CAN Access Peer Device=1 Device=0 CANNOT Access Peer Device=2 Device=0 CANNOT Access Peer Device=3 Device=0 CANNOT Access Peer Device=4 Device=0 CANNOT Access Peer Device=5 Device=0 CANNOT Access Peer Device=6 Device=1 CAN Access Peer Device=0 Device=1 CANNOT Access Peer Device=2 Device=1 CANNOT Access Peer Device=3 Device=1 CANNOT Access Peer Device=4 Device=1 CANNOT Access Peer Device=5 Device=1 CANNOT Access Peer Device=6 Device=2 CANNOT Access Peer Device=0 Device=2 CANNOT Access Peer Device=1 Device=2 CAN Access Peer Device=3 Device=2 CANNOT Access Peer Device=4 Device=2 CANNOT Access Peer Device=5 Device=2 CANNOT Access Peer Device=6 Device=3 CANNOT Access Peer Device=0 Device=3 CANNOT Access Peer Device=1 Device=3 CAN Access Peer Device=2 Device=3 CANNOT Access Peer Device=4 Device=3 CANNOT Access Peer Device=5 Device=3 CANNOT Access Peer Device=6 Device=4 CANNOT Access Peer Device=0 Device=4 CANNOT Access Peer Device=1 Device=4 CANNOT Access Peer Device=2 Device=4 CANNOT Access Peer Device=3 Device=4 CAN Access Peer Device=5 Device=4 CAN Access Peer Device=6 Device=5 CANNOT Access Peer Device=0 Device=5 CANNOT Access Peer Device=1 Device=5 CANNOT Access Peer Device=2 Device=5 CANNOT Access Peer Device=3 Device=5 CAN Access Peer Device=4 Device=5 CAN Access Peer Device=6 Device=6 CANNOT Access Peer Device=0 Device=6 CANNOT Access Peer Device=1 Device=6 CANNOT Access Peer Device=2 Device=6 CANNOT Access Peer Device=3 Device=6 CAN Access Peer Device=4 Device=6 CAN Access Peer Device=5 ***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure. So you can see lesser Bandwidth (GB/s) and unstable Latency (us) in those cases. P2P Connectivity Matrix D\D 0 1 2 3 4 5 6 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 2 0 0 1 1 0 0 0 3 0 0 1 1 0 0 0 4 0 0 0 0 1 1 1 5 0 0 0 0 1 1 1 6 0 0 0 0 1 1 1 Unidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 992.67 6.34 6.53 6.53 6.07 3.11 3.09 1 6.34 1045.96 6.53 6.53 6.07 3.11 3.09 2 6.64 6.64 1763.54 24.56 6.23 4.92 4.90 3 6.64 6.64 24.66 1767.53 6.23 4.92 4.89 4 6.37 6.37 6.45 6.45 765.93 3.07 3.06 5 3.21 3.20 5.05 5.05 3.08 913.21 3.08 6 3.20 3.20 5.09 5.06 3.06 3.08 911.61 Unidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 991.26 6.60 6.53 6.53 6.07 3.11 3.09 1 6.60 1062.93 6.53 6.53 6.07 3.11 3.09 2 6.64 6.64 1761.00 28.62 6.23 4.93 4.90 3 6.64 6.64 28.68 1757.59 6.23 4.95 4.88 4 6.37 6.37 6.45 6.45 765.93 2.31 6.60 5 3.21 3.21 5.05 5.05 2.09 915.35 2.08 6 3.20 3.20 5.08 5.06 6.60 2.30 913.21 Bidirectional P2P=Disabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 998.39 8.66 8.88 8.89 8.21 4.64 4.61 1 8.67 1046.90 8.89 8.89 8.22 4.65 4.61 2 9.72 9.72 1758.21 30.68 8.34 7.27 6.77 3 9.72 9.72 30.58 1759.51 8.35 7.32 6.77 4 8.25 8.25 8.34 8.34 770.27 3.24 3.19 5 4.62 4.62 6.77 6.82 3.23 918.85 3.23 6 4.62 4.64 6.78 6.86 3.17 3.23 919.66 Bidirectional P2P=Enabled Bandwidth Matrix (GB/s) D\D 0 1 2 3 4 5 6 0 994.30 12.88 8.88 8.89 8.15 4.65 4.60 1 12.88 1043.75 8.89 8.88 7.78 4.64 4.60 2 9.72 9.72 1760.16 56.11 8.28 7.30 6.79 3 9.72 9.72 55.93 1753.56 8.22 7.31 6.78 4 8.26 8.25 8.33 8.33 770.08 2.30 6.60 5 4.62 4.62 6.77 6.81 2.30 920.20 2.31 6 4.64 4.64 6.83 6.83 6.60 2.30 919.93 P2P=Disabled Latency Matrix (us) GPU 0 1 2 3 4 5 6 0 1.54 13.66 15.03 14.56 18.67 17.18 17.08 1 13.59 1.38 14.95 14.53 22.65 16.12 18.31 2 12.76 12.98 2.11 14.22 16.30 13.37 15.95 3 12.71 12.85 14.95 2.11 16.30 13.34 16.00 4 19.01 18.74 16.46 14.58 1.72 16.29 23.01 5 15.51 14.15 15.51 15.15 21.43 1.65 20.72 6 19.15 18.39 15.00 14.65 23.00 19.34 1.58 CPU 0 1 2 3 4 5 6 0 1.64 7.16 5.26 4.77 5.39 4.97 5.47 1 5.45 1.66 4.84 6.44 5.03 5.00 5.00 2 4.84 4.82 1.60 4.49 5.06 4.83 4.83 3 5.03 4.91 4.48 1.58 4.88 4.80 4.84 4 5.10 5.12 4.76 4.73 1.66 5.04 5.11 5 5.09 5.00 4.65 4.69 5.09 1.61 5.04 6 5.06 5.04 4.72 4.73 5.06 5.09 1.65 P2P=Enabled Latency (P2P Writes) Matrix (us) GPU 0 1 2 3 4 5 6 0 1.43 0.95 15.85 14.55 25.77 16.96 23.93 1 0.92 1.42 14.98 14.54 25.99 16.10 20.67 2 12.68 12.69 2.11 0.53 16.20 13.42 15.99 3 13.09 12.77 0.51 2.11 16.28 13.32 15.92 4 19.16 18.74 15.13 14.58 1.80 1.81 1.82 5 14.23 15.07 15.51 15.04 1.41 1.61 1.42 6 19.04 19.01 16.47 14.65 1.82 1.83 1.64 CPU 0 1 2 3 4 5 6 0 1.65 1.35 4.89 4.87 5.11 5.23 5.21 1 1.49 1.72 4.83 4.79 5.08 6.90 4.87 2 4.83 4.83 1.53 1.23 4.93 4.79 4.86 3 4.99 4.85 1.23 1.63 5.02 4.94 4.91 4 5.20 5.06 4.82 4.77 1.61 1.35 1.35 5 5.26 5.19 4.89 4.99 1.41 1.73 1.34 6 5.31 5.08 4.96 4.79 1.37 1.39 1.64 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;So if you see carefully, even at those lower PCIe speeds you go i.e. 24 us latency to 5 us latency on 4090s and 3090s. Also 3090 work with P2P at the same time with the A6000.&lt;/p&gt; &lt;p&gt;Note the 3090s have a penalty here but it is I'm running them (and the A6000) on chipset lanes. So even when it says they run at X4 4.0, they share it themselves and also to the other chipset parts (usb, ethernet, etc). 5090s and 4090s are fully on CPU lanes.&lt;/p&gt; &lt;p&gt;Hope this helps!&lt;/p&gt; &lt;p&gt;EDIT: Some small speeds references on EXL3 + TP, via TabbyAPI.&lt;/p&gt; &lt;p&gt;Mistral Large 2411 3.5bpw (using just the 2 5090s), at 10K ctx, native and NCCL TP:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TP disabled: 16 t/s&lt;/li&gt; &lt;li&gt;TP enabled, no P2P: 16 t/s&lt;/li&gt; &lt;li&gt;TP enabled (native), P2P: 20 t/s&lt;/li&gt; &lt;li&gt;TP enabled (NCCL), P2P: 21 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GLM 4.5 4bpw (using the 7 GPUs), at 32K ctx (NOTE: This runs pretty slow because it meets a PCIe bandwidth bottleneck, so base speeds themselves are slow), native TP:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;TP disabled: 16 t/s&lt;/li&gt; &lt;li&gt;TP enabled, no P2P: 11 t/s (so here it is a penalty)&lt;/li&gt; &lt;li&gt;TP enabled, P2P: 16 t/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So for GLM as being a model with few active params and having so many GPUs at X4 4.0, there is a demerit.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/panchovix"&gt; /u/panchovix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qcqn/patched_p2p_nvidia_driver_now_works_with_multiple/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qcqn/patched_p2p_nvidia_driver_now_works_with_multiple/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3qcqn/patched_p2p_nvidia_driver_now_works_with_multiple/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-30T02:31:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3b13b</id>
    <title>Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)</title>
    <updated>2025-08-29T15:47:53+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt; &lt;img alt="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" src="https://external-preview.redd.it/ZWZwemw0NXNiemxmMRIjC8ICuXshETDKyWbElsvvahdP8-tMtjXY4bwDOY1n.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2f385cf95be0a591edf241b94d0612947ca571c1" title="Apple releases FastVLM and MobileCLIP2 on Hugging Face, along with a real-time video captioning demo (in-browser + WebGPU)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Link to models:&lt;br /&gt; - FastVLM: &lt;a href="https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e"&gt;https://huggingface.co/collections/apple/fastvlm-68ac97b9cd5cacefdd04872e&lt;/a&gt;&lt;br /&gt; - MobileCLIP2: &lt;a href="https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47"&gt;https://huggingface.co/collections/apple/mobileclip2-68ac947dcb035c54bcd20c47&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo (+ source code): &lt;a href="https://huggingface.co/spaces/apple/fastvlm-webgpu"&gt;https://huggingface.co/spaces/apple/fastvlm-webgpu&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ayma955sbzlf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3b13b/apple_releases_fastvlm_and_mobileclip2_on_hugging/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T15:47:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n3ldon</id>
    <title>Qwen3-coder is mind blowing on local hardware (tutorial linked)</title>
    <updated>2025-08-29T22:35:27+00:00</updated>
    <author>
      <name>/u/nick-baumann</name>
      <uri>https://old.reddit.com/user/nick-baumann</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt; &lt;img alt="Qwen3-coder is mind blowing on local hardware (tutorial linked)" src="https://external-preview.redd.it/MHAyYm12N3NjMW1mMWyTIaaq8py0BbLEXek7RrX8ohVlR1FrRoAdOlxuqQ67.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d298923517a79cfa7fc3e04c1533fbc4c70a8f3b" title="Qwen3-coder is mind blowing on local hardware (tutorial linked)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello hello!&lt;/p&gt; &lt;p&gt;I'm honestly blown away by how far local models have gotten in the past 1-2 months. Six months ago, local models were completely useless in Cline, which tbf is pretty heavyweight in terms of context and tool-calling demands. And then a few months ago I found one of the qwen models to actually be somewhat usable, but not for any real coding.&lt;/p&gt; &lt;p&gt;However, qwen3-coder-30B is really impressive. 256k context and is actually able to complete tool calls and diff edits reliably in Cline. I'm using the 4-bit quantized version on my 36GB RAM Mac.&lt;/p&gt; &lt;p&gt;My machine does turn into a bit of a jet engine after a while, but the performance is genuinely useful. My setup is LM Studio + Qwen3 Coder 30B + Cline (VS Code extension). There are some critical config details that can break it (like disabling KV cache quantization in LM Studio), but once dialed in, it just works.&lt;/p&gt; &lt;p&gt;This feels like the first time local models have crossed the threshold from &amp;quot;interesting experiment&amp;quot; to &amp;quot;actually useful coding tool.&amp;quot; I wrote a full technical walkthrough and setup guide: &lt;a href="https://cline.bot/blog/local-models"&gt;https://cline.bot/blog/local-models&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nick-baumann"&gt; /u/nick-baumann &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/75bfhw7sc1mf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n3ldon/qwen3coder_is_mind_blowing_on_local_hardware/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-29T22:35:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1n1unkv</id>
    <title>Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)</title>
    <updated>2025-08-27T22:04:51+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt; &lt;img alt="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" src="https://preview.redd.it/ek8o2pfzumlf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d7e3a061baa23ba1306e6f0b1f2524a658bb27a2" title="Launching Our New AMA Series With Z.AI, Creators of GLM (Tomorrow, 9AM-12PM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ek8o2pfzumlf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n1unkv/launching_our_new_ama_series_with_zai_creators_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-27T22:04:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1n2ghx4</id>
    <title>AMA With Z.AI, The Lab Behind GLM Models</title>
    <updated>2025-08-28T16:10:25+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Z.AI ‚Äî The Lab Behind GLM Models. Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Z.AI&lt;/strong&gt;, the research lab behind the &lt;strong&gt;GLM family of models&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zixuanlimit/"&gt;&lt;strong&gt;Zixuan Li, u/zixuanlimit&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Maximum_Can9140/"&gt;&lt;strong&gt;Yuxuan Zhang, u/Maximum_Can9140&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxdu/"&gt;&lt;strong&gt;Zhengxiao Du, u/zxdu&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/Sengxian/"&gt;&lt;strong&gt;Aohan Zeng, u/Sengxian&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 9 AM ‚Äì 12 PM PST, with the Z.AI team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our first AMA. The live part has ended and the Z.AI team will be following up with more answers sporadically over the next 48 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n2ghx4/ama_with_zai_the_lab_behind_glm_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-28T16:10:25+00:00</published>
  </entry>
</feed>
