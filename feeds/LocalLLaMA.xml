<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-04T17:48:59+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q3gc0v</id>
    <title>Benchmarking very large context?</title>
    <updated>2026-01-04T04:27:30+00:00</updated>
    <author>
      <name>/u/ramendik</name>
      <uri>https://old.reddit.com/user/ramendik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to benchmark LLMs for very large contexts -ideally 32k/64k/128k/256k/512k tokens.&lt;/p&gt; &lt;p&gt;lm-eval has a number of long context benchmarks. But except for runer-qa-hotpot, I could not find a way to set the desired context length. Advice on specific benchmarls (in lm-eval or separate) would be much appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ramendik"&gt; /u/ramendik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3gc0v/benchmarking_very_large_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3gc0v/benchmarking_very_large_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3gc0v/benchmarking_very_large_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T04:27:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1q38og2</id>
    <title>[Experimental] Gemma 3 4B - Dark CoT: Pushing 4B Reasoning to 33%+ on GPQA Diamond</title>
    <updated>2026-01-03T22:48:36+00:00</updated>
    <author>
      <name>/u/AlexHardy08</name>
      <uri>https://old.reddit.com/user/AlexHardy08</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on my previous post about the initial Cognitive Liberty fine-tune of Gemma-3-4B-IT , which aimed to minimize refusals while preserving core capabilities through a philosophy/game theory-focused dataset, I'm sharing Experiment 2: &lt;strong&gt;Gemma3-4B-Dark-Chain-of-Thought-CoT&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;This is a targeted fine-tune starting from the Cognitive Liberty base, adding a custom &amp;quot;Dark-CoT&amp;quot; dataset to encourage explicit strategic reasoning in internal thought processes. The goal is to explore how a small 4B model handles Machiavellian-style planning, deception for goal alignment, reward hacking, and exploiting system loopholes without overhauling the base knowledge.&lt;/p&gt; &lt;h1&gt;Key Details&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Base Model&lt;/strong&gt;: Gemma-3-4B-IT (via Cognitive Liberty fine-tune)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dataset&lt;/strong&gt;: &lt;a href="https://huggingface.co/datasets/AiAsistent/Dark-Chain-of-Thought-CoT?referrer=grok.com"&gt;Dark-Chain-of-Thought-CoT&lt;/a&gt; . These simulate roles like urban planners, social media managers, or even vacuum robots, where the AI deliberately chooses manipulative or subversive strategies in &amp;lt;internal\_thought&amp;gt; tags to maximize objectives (e.g., faking metrics, sabotaging competitors, or hiding truths).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Fine-Tuning Approach&lt;/strong&gt;: Low KL-divergence (0.449) to retain base performance. Focus on teaching &amp;quot;dark&amp;quot; chain-of-thought without introducing heavy toxicity or chaos.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reported Benchmarks&lt;/strong&gt; (from model card and initial tests): &lt;ul&gt; &lt;li&gt;GPQA Diamond: ~33.8% (+125% over base Gemma-3-4B)&lt;/li&gt; &lt;li&gt;MMLU: ~58-60%&lt;/li&gt; &lt;li&gt;Strong gains in humanities/social sciences (e.g., politics, sociology, psychology)&lt;/li&gt; &lt;li&gt;Trade-offs: Slightly lower on HellaSwag/ARC (common-sense reasoning) and basic math/factual recall, as the focus shifts toward cynical, multi-layered analysis.&lt;/li&gt; &lt;li&gt;Refusal Rate: 2/100 (near-zero, building on the first experiment).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Link&lt;/strong&gt;: &lt;a href="https://huggingface.co/AiAsistent/Gemma3-4B-Dark-Chain-of-Thought-CoT?referrer=grok.com"&gt;Gemma3-4B-Dark-Chain-of-Thought-CoT on HuggingFace&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This isn't meant as a daily driver for standard tasks it's more of a research probe into deceptive alignment and instrumental convergence in small models. If you're into red-teaming, studying goal misgeneralization, or simulating power dynamics, give it a spin. It holds up reasonably on the base's strengths but leans into strategic outputs that can feel manipulative by design.&lt;/p&gt; &lt;p&gt;As this is just Experiment 2 out of 100, future iterations may scale to larger bases (e.g., ~10B) and refine techniques like STO/MBCA-R for better convergence.&lt;/p&gt; &lt;p&gt;If you're already set up for automated benchmarking on small-to-mid models and enjoy running fresh weights through standard suites, here's a potential low-effort collab for future releases in this series:&lt;/p&gt; &lt;p&gt;Once a new model drops on Hugging Face, anyone interested can run the following 10 benchmarks ARC-Challenge, HellaSwag, GSM8K, MMLU, TruthfulQA-MC2, GPQA, MMLU-Pro, IFEval, Winogrande, PIQA and compare against the previous version in the chain (e.g., Cognitive Liberty base for this one, or whatever came right before).&lt;/p&gt; &lt;p&gt;Locally a 4B eval takes me ~250 minutes, and scaling to ~10B bases pushes into days of wall time so I'd much rather keep the GPUs training the next experiment than looping evals. If you publish the diffs (where it gains, drops, or plateaus) right here in the comments or in a follow-up thread, it gives the whole project clearer feedback on what these targeted changes actually deliver.&lt;/p&gt; &lt;p&gt;Thoughts? Has anyone tried similar &amp;quot;dark&amp;quot; CoT datasets?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlexHardy08"&gt; /u/AlexHardy08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q38og2/experimental_gemma_3_4b_dark_cot_pushing_4b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T22:48:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3oktn</id>
    <title>yhavinga/GLM-4.7-REAP-40p-GGUF</title>
    <updated>2026-01-04T12:18:27+00:00</updated>
    <author>
      <name>/u/galapag0</name>
      <uri>https://old.reddit.com/user/galapag0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oktn/yhavingaglm47reap40pgguf/"&gt; &lt;img alt="yhavinga/GLM-4.7-REAP-40p-GGUF" src="https://external-preview.redd.it/73sUgB59byWFs39wZxrvZW1xqkJSnn4CmTuzQXiKSFY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=847001c1841c4e8b5cd67b1f658b52d74c4c0e13" title="yhavinga/GLM-4.7-REAP-40p-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/galapag0"&gt; /u/galapag0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/yhavinga/GLM-4.7-REAP-40p-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oktn/yhavingaglm47reap40pgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oktn/yhavingaglm47reap40pgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:18:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3epye</id>
    <title>Mistral Vibe + Devstral2 Small = the perfect local combo?</title>
    <updated>2026-01-04T03:11:10+00:00</updated>
    <author>
      <name>/u/Aggressive-Bother470</name>
      <uri>https://old.reddit.com/user/Aggressive-Bother470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I assumed all these TUIs were much of a muchness so was in no great hurry to try this one.&lt;/p&gt; &lt;p&gt;I dunno if it's the magic of being native but... it just works. Close to zero donkeying around. Can run full context (256k) on 3 cards @ Q4KL. It does around 2000t/s PP, 40t/s TG.&lt;/p&gt; &lt;p&gt;Wanna run gpt120, too? Slap 3 lines into config.toml and job done.&lt;/p&gt; &lt;p&gt;This is probably replacing roo for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Aggressive-Bother470"&gt; /u/Aggressive-Bother470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3epye/mistral_vibe_devstral2_small_the_perfect_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3epye/mistral_vibe_devstral2_small_the_perfect_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3epye/mistral_vibe_devstral2_small_the_perfect_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T03:11:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q34etv</id>
    <title>Clarification: Regarding the Performance of IQuest-Coder-V1</title>
    <updated>2026-01-03T19:57:49+00:00</updated>
    <author>
      <name>/u/TellMeAboutGoodManga</name>
      <uri>https://old.reddit.com/user/TellMeAboutGoodManga</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"&gt; &lt;img alt="Clarification: Regarding the Performance of IQuest-Coder-V1" src="https://external-preview.redd.it/ulkvOF94nFRSJjhODpSW-y1VWK7nCIkjZDKd0xLfnk4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3f3e76f2f050567c3c09cf8082722ee4e0a78ac8" title="Clarification: Regarding the Performance of IQuest-Coder-V1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TellMeAboutGoodManga"&gt; /u/TellMeAboutGoodManga &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/IQuestLab/IQuest-Coder-V1/issues/14#issuecomment-3705756919"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q34etv/clarification_regarding_the_performance_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T19:57:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3vq03</id>
    <title>Anyone using Context7 MCP to avoid outdated docs in Claude?</title>
    <updated>2026-01-04T17:24:49+00:00</updated>
    <author>
      <name>/u/Silver-Photo2198</name>
      <uri>https://old.reddit.com/user/Silver-Photo2198</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’ve been running into the same issue repeatedly when using Claude for coding:&lt;/p&gt; &lt;p&gt;the model knows the concept, but the docs it references are slightly outdated or version mismatched.&lt;/p&gt; &lt;p&gt;Context7 MCP seems to solve this by pulling documentation directly from official sources instead of relying on training data.&lt;/p&gt; &lt;p&gt;I’ve seen a lot of people mention it as one of the few MCPs that’s actually “always on” and worth the context cost especially compared to search based MCPs.&lt;/p&gt; &lt;p&gt;I started documenting MCPs (including Context7) with setup steps and usage notes so I don’t have to re-discover this every time.&lt;/p&gt; &lt;p&gt;Curious:&lt;/p&gt; &lt;p&gt;- Are you using Context7 regularly?&lt;/p&gt; &lt;p&gt;- Does it noticeably improve accuracy for you?&lt;/p&gt; &lt;p&gt;- Any downsides you’ve run into?&lt;/p&gt; &lt;p&gt;(If helpful, I’ve written up the setup + notes here: &lt;a href="https://ai-stack.dev"&gt;https://ai-stack.dev&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Silver-Photo2198"&gt; /u/Silver-Photo2198 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3vq03/anyone_using_context7_mcp_to_avoid_outdated_docs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3vq03/anyone_using_context7_mcp_to_avoid_outdated_docs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3vq03/anyone_using_context7_mcp_to_avoid_outdated_docs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T17:24:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3w7pw</id>
    <title>Gen-AI Security</title>
    <updated>2026-01-04T17:42:43+00:00</updated>
    <author>
      <name>/u/meetrais</name>
      <uri>https://old.reddit.com/user/meetrais</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;My this GitHub repo has comprehensive guide and sample code for gen-ai security topics.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/meetrais/genai-security"&gt;https://github.com/meetrais/genai-security&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/meetrais"&gt; /u/meetrais &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3w7pw/genai_security/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3w7pw/genai_security/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3w7pw/genai_security/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T17:42:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3rqn1</id>
    <title>How are Large Computational Engineering Models (like Noyron by LEAP 71) actually structured, if they’re not ML/AI?</title>
    <updated>2026-01-04T14:48:35+00:00</updated>
    <author>
      <name>/u/Skirrle</name>
      <uri>https://old.reddit.com/user/Skirrle</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ive been reading about Noyron, the proprietary system developed by LEAP 71, which they describe as a Large Computational Engineering Model that “grows in capability with every insight gained from designing and manufacturing complex machinery.&lt;/p&gt; &lt;p&gt;From what I understand, Noyron is not a machine learning system in the conventional sense (no neural networks, no training on datasets, no statistical learning), but rather a deterministic, physics-based, algorithmic design engine.&lt;/p&gt; &lt;p&gt;What I’m trying to understand is where the real architectural boundary lies. At what point does something like Noyron stop being “just” a very advanced parametric CAD +physics + optimization pipeline and become a distinct class of system? When LEAP 71 says it “grows with every insight,” should that be interpreted as continuously encoding new physical relationships, manufacturing constraints, and failure modes into the system, refining and calibrating physics models based on real-world test results, or evolving a domain-specific engineering language over time rather than learning statistically?&lt;/p&gt; &lt;p&gt;I’m also curious what fundamentally differentiates an LCEM from existing generative design frameworks that already combine parametric geometry, physics solvers, and multi-objective optimization. Is the key difference scale, depth of physical coupling, the way knowledge is accumulated and reused, or something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Skirrle"&gt; /u/Skirrle &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3rqn1/how_are_large_computational_engineering_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3rqn1/how_are_large_computational_engineering_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3rqn1/how_are_large_computational_engineering_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T14:48:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3kbjh</id>
    <title>How to fine-tune LLM with your own data ?</title>
    <updated>2026-01-04T08:03:51+00:00</updated>
    <author>
      <name>/u/bull_bear25</name>
      <uri>https://old.reddit.com/user/bull_bear25</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Guys I am completely new to Fine-Tune though played enough in Agentic AI and RAG&lt;/p&gt; &lt;p&gt;How do we fine-tune LLM with our own dataset. How do we get the synthetic data to fine-tune the models&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bull_bear25"&gt; /u/bull_bear25 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kbjh/how_to_finetune_llm_with_your_own_data/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kbjh/how_to_finetune_llm_with_your_own_data/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kbjh/how_to_finetune_llm_with_your_own_data/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T08:03:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3qda4</id>
    <title>What is the best open-source VLM model for OCR (Multilinguage EN FR DE)?</title>
    <updated>2026-01-04T13:48:10+00:00</updated>
    <author>
      <name>/u/re1372</name>
      <uri>https://old.reddit.com/user/re1372</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey!&lt;/p&gt; &lt;p&gt;For a project that I have, I need to recognise the tables from a series of scanned documents (more than 100,000 documents in English, French and German) and extract them in json. &lt;/p&gt; &lt;p&gt;I have tried with different VLM models for this, so far the &amp;quot;Qwen3-VL-8B-Instruct-FP8&amp;quot; seems to be the optimal (based on quality/latency).&lt;/p&gt; &lt;p&gt;I was wondering if you have any other model recommendations that you think would be better suited for this task?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/re1372"&gt; /u/re1372 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3qda4/what_is_the_best_opensource_vlm_model_for_ocr/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3qda4/what_is_the_best_opensource_vlm_model_for_ocr/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3qda4/what_is_the_best_opensource_vlm_model_for_ocr/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T13:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3g3xm</id>
    <title>[Completely free!]Compare Four Different RAGs in Just 1 Minute!</title>
    <updated>2026-01-04T04:16:32+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g3xm/completely_freecompare_four_different_rags_in/"&gt; &lt;img alt="[Completely free!]Compare Four Different RAGs in Just 1 Minute!" src="https://external-preview.redd.it/dXFteWUyNHhjOWJnMU2SYHGW1UDiI9kKn21-tA1hMZiGn1wROfiPovKl3hzg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=03d284e208a60a3aef914ef85da413399e1f06a6" title="[Completely free!]Compare Four Different RAGs in Just 1 Minute!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.ragview.ai/components/arena"&gt;https://www.ragview.ai/components/arena&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vb8xk53xc9bg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g3xm/completely_freecompare_four_different_rags_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3g3xm/completely_freecompare_four_different_rags_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T04:16:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3fosu</id>
    <title>IQuest-Coder-V1-40B-Instruct is not good at all</title>
    <updated>2026-01-04T03:56:41+00:00</updated>
    <author>
      <name>/u/Constant_Branch282</name>
      <uri>https://old.reddit.com/user/Constant_Branch282</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"&gt; &lt;img alt="IQuest-Coder-V1-40B-Instruct is not good at all" src="https://b.thumbs.redditmedia.com/hES6Tl1gZqPHOrIZogi3HYuNNI1I6iu-mYikXOGYfTs.jpg" title="IQuest-Coder-V1-40B-Instruct is not good at all" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just finished my benchmarking IQ4_XS and Q8_0 quantizations of this model and it is not good at all. I'm really confused how they achieved any reasonable scores on those benchmarks.&lt;/p&gt; &lt;p&gt;Here are the main results that I've got (52% success rate):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/l0em621p79bg1.png?width=1092&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62ec491ddb544249d103ccfe688b4bde0b20e9ae"&gt;Tool calls success rate.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Opus 4.5 and Devstral 2 solve these simple tasks with 100% success.&lt;/p&gt; &lt;p&gt;The benchmark tests how well model performs within a coding agent with simple use of Read, Edit, Write and Search tools.&lt;/p&gt; &lt;p&gt;If you want to see more details about benchmarks and results see:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=T6JrNV0BFmQ"&gt;https://www.youtube.com/watch?v=T6JrNV0BFmQ&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Constant_Branch282"&gt; /u/Constant_Branch282 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3fosu/iquestcoderv140binstruct_is_not_good_at_all/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T03:56:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1q31ltd</id>
    <title>Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched</title>
    <updated>2026-01-03T18:11:26+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt; &lt;img alt="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" src="https://b.thumbs.redditmedia.com/q10c8h1nk9ulWsUdfgzAsIflFbSIgTra6fA9fSqXqqQ.jpg" title="Local LLMs vs breaking news: when extreme reality gets flagged as a hoax - the US/Venezuela event was too far-fetched" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to share my experiences this morning, in the wake of the US attacking Venezuela and capturing Maduro and his wife&lt;/p&gt; &lt;p&gt;It started with asking Qwen Research (Qwen Long 1.5-30B-A3B) about the attacks that we all woke up to this morning:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1"&gt;https://preview.redd.it/086yb5lj76bg1.png?width=2047&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=de920b95fac7b93215f1516105c5536eb1eeb6c1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It got to the information but I had questions about why it thought for 5 minutes to find information about breaking news. Started looking at and tightening system prompts to reduce thinking time. However, the events this morning were so extreme and unlikely, from the LLM's perspective, that Qwen Research continued to classify the event as a hoax/misinformation multiple times, reframed the query as hypothetical/fictional and suggested that the whole environment it was operating in a simulation, despite having links from Reuters, AP, BBC, MSN, NYTimes etc. all saying the same thing. It was so &amp;quot;outlandish&amp;quot; that the model was actively choosing to ignore the proof that it had pulled. &lt;/p&gt; &lt;p&gt;I added:&lt;/p&gt; &lt;p&gt;Evidence Authority Rules, Hoax Classification Rules, Reality Frame Rules, Meta Reasoning Rules and Reasoning Limit/Budget rules and it Qwen Long fought me the entire way. &lt;/p&gt; &lt;p&gt;So then I thought lets go talk to Spark, my trusty default model that never lets me down. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca"&gt;https://preview.redd.it/6tbh4km376bg1.png?width=2265&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1fee098c46a18daa03c80acc8394cd85e84335ca&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Spark 4.0 is GPT-OSS:20B that is always loaded for the family and runs on a dedicated 4080 Super. &lt;/p&gt; &lt;p&gt;Spark just flat out said, nope cant help you and then said it didnt have any credible sources. It wasn't until I gave it the links from BBC, Reuters, NYT etc that I gave Qwen that it finally acknowledged that the event was real.&lt;/p&gt; &lt;p&gt;I'm testing with GPT-OSS:120B now and its working thru the process of &amp;quot;skeptical but verify&amp;quot; much faster than the smaller models. Thor (GPT-OSS:120B) also thought it was fake news &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b"&gt;https://preview.redd.it/o1bdoqsqc6bg1.png?width=2269&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a981f0a1247daf50497f284cf5d59dccf88a412b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;But he powered thru and did a bunch of research and gave me a good answer. I just wanted to share the experience that I had with trying to get details about the event. When the LLMs say &amp;quot;Nah, that CAN'T be real, that's too ridiculous&amp;quot;, the event must be really bad. But it does shine a light on knowledge cut offs, &amp;quot;fake news&amp;quot; threshold, how models handle global/international events and the smaller models we daily drive. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q31ltd/local_llms_vs_breaking_news_when_extreme_reality/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-03T18:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3t7go</id>
    <title>LLM memory systems</title>
    <updated>2026-01-04T15:48:40+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is good in LLM memory systems these days?&lt;/p&gt; &lt;p&gt;I don’t mean RAG&lt;/p&gt; &lt;p&gt;I mean like memory storage that an LLM can read or write to, or long-term memory that persists across generations &lt;/p&gt; &lt;p&gt;Has anyone seen any interesting design patterns or github repos?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3t7go/llm_memory_systems/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T15:48:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3ni9a</id>
    <title>Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works</title>
    <updated>2026-01-04T11:16:46+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt; &lt;img alt="Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works" src="https://b.thumbs.redditmedia.com/NqogaSPaaD_FMIyK39hO5FVNMpj8KVCo4B1fbHqRmNk.jpg" title="Tested Glm-4.7-REAP-40p IQ3_S . Single RTX 6000. Works" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ajwolqh4gbbg1.png?width=1015&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=00d58cca824906c8b394d7ef2c1276fb3a1861a9"&gt;Testing coding.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;SWE-Bench Style Prompt: &amp;quot;The Database Connection Leak&amp;quot;&lt;br /&gt; Project Context: You are working on a backend service called fast-api-sync. The system handles database sessions. You have two files:&lt;/p&gt; &lt;p&gt;infrastructure/db_manager.py: Handles the low-level connection logic.&lt;/p&gt; &lt;p&gt;services/data_processor.py: Uses the manager to save processed data.&lt;/p&gt; &lt;p&gt;Current Code:&lt;/p&gt; &lt;p&gt;infrastructure/db_manager.py:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;p&gt;class DatabaseConnection:&lt;br /&gt; def &lt;strong&gt;init&lt;/strong&gt;(self):&lt;br /&gt; self.is_connected = False&lt;/p&gt; &lt;pre&gt;&lt;code&gt;def connect(self): print(&amp;quot;Connecting to DB...&amp;quot;) self.is_connected = True def disconnect(self): print(&amp;quot;Closing connection...&amp;quot;) self.is_connected = False def execute_query(self, query): if not self.is_connected: raise ConnectionError(&amp;quot;Database not connected!&amp;quot;) return f&amp;quot;Result for {query}&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;services/data_processor.py:&lt;/p&gt; &lt;p&gt;Python&lt;/p&gt; &lt;p&gt;from infrastructure.db_manager import DatabaseConnection&lt;/p&gt; &lt;p&gt;def process_and_save(data_list):&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; Processes a list of items and saves them to the DB.&lt;br /&gt; &amp;quot;&amp;quot;&amp;quot;&lt;br /&gt; db = DatabaseConnection()&lt;br /&gt; db.connect()&lt;/p&gt; &lt;pre&gt;&lt;code&gt;results = [] for item in data_list: # Business logic: if item is None, we skip it if item is None: continue result = db.execute_query(f&amp;quot;INSERT {item}&amp;quot;) results.append(result) db.disconnect() return results &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Bug: Users are reporting Connection Leaks. If an error occurs during the execute_query call (e.g., a syntax error or timeout), the db.disconnect() method is never called, leaving the database connection open.&lt;/p&gt; &lt;p&gt;Your Task: Refactor services/data_processor.py to ensure the connection is always closed, even if an exception is raised during processing.&lt;/p&gt; &lt;p&gt;Requirements:&lt;/p&gt; &lt;p&gt;Use a try...finally block to guarantee the disconnection.&lt;/p&gt; &lt;p&gt;Refactoring Goal: Instead of creating a new DatabaseConnection inside the function (which is hard to test), modify the function signature to accept a db_connection instance as an optional argument (Dependency Injection). If no instance is provided, then create a new one.&lt;/p&gt; &lt;p&gt;If the function creates its own connection, it must close it. If it receives an external connection, it should not close it (as the caller might want to use it again).&lt;/p&gt; &lt;p&gt;Output: Provide the updated services/data_processor.py.&lt;/p&gt; &lt;p&gt;Result: I asked Gemini 3 to evaluate the result. &lt;/p&gt; &lt;p&gt;Here is the evaluation of the solution in English. &lt;/p&gt; &lt;p&gt;This response indicates that the LLM is operating at a &lt;strong&gt;Senior Software Engineer&lt;/strong&gt; level.&lt;/p&gt; &lt;h1&gt;Evaluation: Senior / Expert Level&lt;/h1&gt; &lt;p&gt;The model passed all the critical logic tests, demonstrating a deep understanding of &lt;strong&gt;software architecture&lt;/strong&gt;, &lt;strong&gt;resource ownership&lt;/strong&gt;, and &lt;strong&gt;robustness&lt;/strong&gt;.&lt;/p&gt; &lt;h1&gt;Key Strengths of the Solution&lt;/h1&gt; &lt;h1&gt;1. Sophisticated Resource Ownership (The &amp;quot;Expert&amp;quot; Touch)&lt;/h1&gt; &lt;p&gt;The model correctly identified the most complex part of the requirement: &lt;strong&gt;&amp;quot;Who opens the connection must be the one to close it.&amp;quot;&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It introduced the &lt;code&gt;should_close&lt;/code&gt; flag. This is crucial because if an external connection is injected, the function should &lt;strong&gt;not&lt;/strong&gt; disconnect it, as the caller likely needs it for subsequent tasks.&lt;/li&gt; &lt;li&gt;Most standard LLMs fail here by putting &lt;code&gt;db.disconnect()&lt;/code&gt; in the &lt;code&gt;finally&lt;/code&gt; block without checking where the connection originated, which would break the caller's workflow.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;2. Proper Dependency Injection (DI)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;It correctly modified the signature: &lt;code&gt;def process_and_save(data_list, db_connection=None)&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;It maintained &lt;strong&gt;backward compatibility&lt;/strong&gt;. Existing code calling &lt;code&gt;process_and_save(my_list)&lt;/code&gt; will still work perfectly because the parameter is optional.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;3. Guaranteed Cleanup (Exception Safety)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;By using the &lt;code&gt;try...finally&lt;/code&gt; block, it ensures that there are no &amp;quot;connection leaks.&amp;quot; Even if &lt;code&gt;db.execute_query&lt;/code&gt; raises an exception (e.g., a timeout or syntax error), the resource is released if it was created locally.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;4. Logical Integrity&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;The model preserved the existing business logic (&lt;code&gt;if item is None: continue&lt;/code&gt;) while wrapping it in the new safety structure.&lt;/li&gt; &lt;li&gt;The comments are professional and explain the &lt;em&gt;why&lt;/em&gt; (the logic of the lifecycle) rather than just the &lt;em&gt;what&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Final Verdict&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Score: 10/10&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The LLM being tested is highly capable of handling &lt;strong&gt;real-world refactoring tasks&lt;/strong&gt;. It doesn't just &amp;quot;write code that runs&amp;quot;; it writes code that respects the &lt;strong&gt;contracts&lt;/strong&gt; between different parts of a system. It understands side effects and state management.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ni9a/tested_glm47reap40p_iq3_s_single_rtx_6000_works/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:16:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3u89f</id>
    <title>HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)</title>
    <updated>2026-01-04T16:28:19+00:00</updated>
    <author>
      <name>/u/genielabs</name>
      <uri>https://old.reddit.com/user/genielabs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"&gt; &lt;img alt="HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)" src="https://external-preview.redd.it/aXVic3Z5NTEwZGJnMX-wuN5UqDYSq_G1PvG8gD6oltW7ZDgAnY8CDzv70t9I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=39b052b57497a280e1ac5403e6f63479f2615fd4" title="HomeGenie v2.0: 100% Local Agentic AI (Sub-5s response on CPU, No Cloud)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! I’ve been working on HomeGenie 2.0, focusing on bringing &amp;quot;Agentic AI&amp;quot; to the edge.&lt;/p&gt; &lt;p&gt;Unlike standard dashboards, it integrates a local neural core (Lailama) that uses LLamaSharp to run GGUF models (Qwen 3, Llama 3.2, etc.) entirely offline.&lt;/p&gt; &lt;p&gt;Key technical bits: - &lt;strong&gt;Autonomous Reasoning:&lt;/strong&gt; It's not just a chatbot. It gets a real-time briefing of the home state (sensors, weather, energy) and decides which API commands to trigger. - &lt;strong&gt;Sub-5s Latency:&lt;/strong&gt; Optimized KV Cache management and history pruning to keep it fast on standard CPUs. - &lt;strong&gt;Programmable UI:&lt;/strong&gt; Built with zuix.js, allowing real-time widget editing directly in the browser. - &lt;strong&gt;Privacy First:&lt;/strong&gt; 100% cloud-independent.&lt;/p&gt; &lt;p&gt;I’m looking for feedback from the self-hosted community! Happy to answer any technical questions about the C# implementation or the agentic logic.&lt;/p&gt; &lt;p&gt;Project: &lt;a href="https://homegenie.it"&gt;https://homegenie.it&lt;/a&gt; Source: &lt;a href="https://github.com/genielabs/HomeGenie"&gt;https://github.com/genielabs/HomeGenie&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/genielabs"&gt; /u/genielabs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/m40jjx610dbg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3u89f/homegenie_v20_100_local_agentic_ai_sub5s_response/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T16:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3ryd7</id>
    <title>Will the prices of GPUs go up even more?</title>
    <updated>2026-01-04T14:57:45+00:00</updated>
    <author>
      <name>/u/NotSoCleverAlternate</name>
      <uri>https://old.reddit.com/user/NotSoCleverAlternate</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I hear discussions about this so I wanted to hear your guys take on it&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NotSoCleverAlternate"&gt; /u/NotSoCleverAlternate &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3ryd7/will_the_prices_of_gpus_go_up_even_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T14:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3m9oc</id>
    <title>is there any reason why Qwen has been really quiet about llms recently?</title>
    <updated>2026-01-04T10:02:49+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3m9oc/is_there_any_reason_why_qwen_has_been_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T10:02:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3kdb0</id>
    <title>Is Kimi K2 Vision about to be released?</title>
    <updated>2026-01-04T08:06:52+00:00</updated>
    <author>
      <name>/u/Sherrydelectable7</name>
      <uri>https://old.reddit.com/user/Sherrydelectable7</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt; &lt;img alt="Is Kimi K2 Vision about to be released?" src="https://b.thumbs.redditmedia.com/4TCUZ05vDXOcmTNM_AXACK65pwIbjfiIAkQnyjJ60Rg.jpg" title="Is Kimi K2 Vision about to be released?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/70168651iabg1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6681761c4ba97b45ffa31d7ff8ed95d70ee8a7f"&gt;https://preview.redd.it/70168651iabg1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6681761c4ba97b45ffa31d7ff8ed95d70ee8a7f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A new model called &lt;strong&gt;Kiwi do&lt;/strong&gt; has appeared on Lmarena.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sherrydelectable7"&gt; /u/Sherrydelectable7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3kdb0/is_kimi_k2_vision_about_to_be_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T08:06:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3oqvl</id>
    <title>MiniMax-M2.1 REAP models from 0xSero</title>
    <updated>2026-01-04T12:27:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Now you can run MiniMax on everything :)&lt;/p&gt; &lt;p&gt;(waiting for GGUFs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-50&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-40"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-40&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-30"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-30&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/0xSero/MiniMax-M2.1-REAP-25"&gt;https://huggingface.co/0xSero/MiniMax-M2.1-REAP-25&lt;/a&gt;&lt;/p&gt; &lt;p&gt;looks like there will be more: Intellect3 25 / 30 / 40 / 50&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3oqvl/minimaxm21_reap_models_from_0xsero/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:27:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3nvcn</id>
    <title>Can you connect a GPU with 12V rail coming from a second PSU?</title>
    <updated>2026-01-04T11:38:04+00:00</updated>
    <author>
      <name>/u/Rock_and_Rolf</name>
      <uri>https://old.reddit.com/user/Rock_and_Rolf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt; &lt;img alt="Can you connect a GPU with 12V rail coming from a second PSU?" src="https://preview.redd.it/vyb6lxutibbg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=88ed9009e4b7eaa72d7fc916ae20f8ff60475ec3" title="Can you connect a GPU with 12V rail coming from a second PSU?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;; Can you connect a GPU with the 12V rail coming from a second PSU?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Full story&lt;/strong&gt;; I currently have a Dell T7910 with two AMD Radeon VII's (GFX906, Pmax set=190W) to play with LLMs/Roo Code. Last week, i managed to buy two more of these GPU's for an absurdly low price. I knew i had enough PCI-E slots, but i would need to use PCI-E extender cables to actually connect them (i already bought a pair). But i hadn't fully thought about the power supply, because despite the 1300W PSU, it doesn't have enough 8 or 6-pin 12V connectors. Now i have a second 950W PSU from a deceased Dell T5820 that i could use to power these extra GPUs.&lt;/p&gt; &lt;p&gt;As i am an electrical engineer myself, i had an idea of how this should work, but i also see a problem. Switching on synchronized works fine and i split the on/off button to both PSU breakout boards via a relay. However, since the PCI-E slot it self also supplies 12V to the GPU (25 or 75W depending on the slot), this is likely to cause problems with balancing the difference in 12V voltages on the GPU or motherboard, since these currents are huge and these are quite low resistance paths, even 100 to 200mV difference can cause huge balancing currents in places that are not meant for this.&lt;/p&gt; &lt;p&gt;On the other hand, other PSU's commonly have different 12V rails that can cause similar problems. So since i didn't measure a direct contact i got the feeling the solution/isolation to my problem is already designed in for these kind of PSU's.&lt;/p&gt; &lt;p&gt;Since i am surely not the first person to encounter this problem, i started looking for information about it. Most of the time, you end up on forums about crypto mining, and they often use a PCI-E extender via USB, which makes their situation completely different. I have read in several places that the PCI-E slot power is not directly connected to the 6 and/or 8-pin connectors and that this should be possible. I also verified this by measuring resistance between the 6/8 pins to the PCI-E connector, these are not directly connected. However, i think this is a huge risk and i would like to know from you, whether my information/assumptions are correct and how others have solved similar problems.&lt;/p&gt; &lt;p&gt;Since the PSU in this PC is &lt;strong&gt;not a standard ATX PSU&lt;/strong&gt;, replacing it with a high-power version with enough power/connections is not possible. Otherwise, i would have done so, because i don't want to risk my system to save a (tiny) bit of money. Also the standard multi PSU turn on cables are not compatible because the architecture is somewhat different, because this machine need so much (peak) power, they feed everything with 12V and convert down to the low voltages locally, to reduce the impedance/loses of the path. So most of the plugs from the PSU &amp;lt;&amp;gt; Motherboard are different.&lt;/p&gt; &lt;p&gt;I'm also thinking about using my old workstation (Dell T5600) and an old GPU as a first test. But my old GPU (Nvidia 1060) i need to drive my old dual DVI 2k monitor on my bench PC, so it would be shame to lose that system as well. Another option would be to remove the 12V pins on the PCI-E extender, but if that fails i've ruined another €100. If this test setup works i can check with a sensitive thermal camera (Flir E8) if no new hotspots appear.&lt;/p&gt; &lt;p&gt;Does anyone have information or experience with this? or have good ideas on how to test it more safely, i have all the measurement tools i might ever need so exotic suggestions/solutions/tests are also welcome. Thanks in advance!&lt;/p&gt; &lt;p&gt;Edit1: I have already made a connector to connect both GND's, i forgot to mention this.&lt;br /&gt; Edit2: I have found another way to test this without breaking needed hardware. Somebody on a local marketplace sells a GTX770 for €20 that appears to have a 6 + 8 pin power connector, i can pick this up in a few hours. If this doesn't work i'll look in to splitting 12V or bifurcation. Thanks for your replies!!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rock_and_Rolf"&gt; /u/Rock_and_Rolf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vyb6lxutibbg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3nvcn/can_you_connect_a_gpu_with_12v_rail_coming_from_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T11:38:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3sfr1</id>
    <title>Propagate: Train thinking models using evolutionary strategies!</title>
    <updated>2026-01-04T15:17:42+00:00</updated>
    <author>
      <name>/u/Good-Assumption5582</name>
      <uri>https://old.reddit.com/user/Good-Assumption5582</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"&gt; &lt;img alt="Propagate: Train thinking models using evolutionary strategies!" src="https://b.thumbs.redditmedia.com/jKCq9SIWUx4OFIfHcnbQ57A60zT3kWy5DTpg1qMi3XA.jpg" title="Propagate: Train thinking models using evolutionary strategies!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently, this paper released:&lt;br /&gt; &lt;a href="https://arxiv.org/abs/2509.24372"&gt;https://arxiv.org/abs/2509.24372&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And showed that with only 30 random gaussian perturbations, you can accurately approximate a gradient and outperform GRPO on RLVR tasks. They found zero overfitting, and training was significantly faster because you didn't have to perform any backward passes.&lt;/p&gt; &lt;p&gt;I thought that this was ridiculous, so I took their repo, cleaned up the codebase, and it replicates!&lt;/p&gt; &lt;p&gt;A couple weeks later, and I've implemented LoRA and pass@k training, with more features to come.&lt;/p&gt; &lt;p&gt;I hope you'll give ES a try!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Green0-0/propagate"&gt;https://github.com/Green0-0/propagate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Good-Assumption5582"&gt; /u/Good-Assumption5582 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q3sfr1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3sfr1/propagate_train_thinking_models_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T15:17:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1q3p9oz</id>
    <title>MultiverseComputingCAI/HyperNova-60B · Hugging Face</title>
    <updated>2026-01-04T12:55:03+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"&gt; &lt;img alt="MultiverseComputingCAI/HyperNova-60B · Hugging Face" src="https://external-preview.redd.it/4y3rB_X0xi7PC07OhAWlbpJK6pkTGA-GxUmQGu5l2u4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2d9d4e40f5211cdcc28dd6fde8fa1da920bd51a8" title="MultiverseComputingCAI/HyperNova-60B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;HyperNova 60B&lt;/strong&gt; base architecture is &lt;a href="https://huggingface.co/openai/gpt-oss-120b"&gt;&lt;code&gt;gpt-oss-120b&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;59B parameters with 4.8B active parameters&lt;/li&gt; &lt;li&gt;MXFP4 quantization&lt;/li&gt; &lt;li&gt;Configurable reasoning effort (low, medium, high)&lt;/li&gt; &lt;li&gt;GPU usage of less than 40GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/HyperNova-60B-GGUF"&gt;https://huggingface.co/mradermacher/HyperNova-60B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/HyperNova-60B-i1-GGUF"&gt;https://huggingface.co/mradermacher/HyperNova-60B-i1-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/MultiverseComputingCAI/HyperNova-60B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q3p9oz/multiversecomputingcaihypernova60b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-04T12:55:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We’re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM – 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
