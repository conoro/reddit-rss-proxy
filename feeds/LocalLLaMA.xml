<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-26T07:49:04+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1npo93e</id>
    <title>I built a tiny fully local AI agent for a Raspberry Pi</title>
    <updated>2025-09-24T21:14:05+00:00</updated>
    <author>
      <name>/u/syxa</name>
      <uri>https://old.reddit.com/user/syxa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"&gt; &lt;img alt="I built a tiny fully local AI agent for a Raspberry Pi" src="https://external-preview.redd.it/eWJobjllM3hoNnJmMX3IlXPTfCA6UKqCZh3d_lEw5N2PzMnkp8dcFp83zF5t.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58e4d2c8a5cd47a44d3d6abb40f3a02cb6dff369" title="I built a tiny fully local AI agent for a Raspberry Pi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Over the past few months, Iâ€™ve been working on a tiny agent that can run entirely on a Raspberry Pi 5. It's capable of executing tools and runs some of the smallest good models I could find (specifically Qwen3:1.7b and Gemma3:1b).&lt;/p&gt; &lt;p&gt;From wake-word detection, to transcription, to the actual LLM inference, everything happens on the Pi 5 itself. It was definitely a challenge given the hardware constraints, but I learned a lot along the way.&lt;/p&gt; &lt;p&gt;I've detailed everything in this blog post if you're curious: &lt;a href="https://blog.simone.computer/an-agent-desktoy"&gt;https://blog.simone.computer/an-agent-desktoy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/syxanash/maxheadbox"&gt;https://github.com/syxanash/maxheadbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/syxa"&gt; /u/syxa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xslfjc3xh6rf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T21:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqq4n6</id>
    <title>I made a library to help writing test code for vLLM.</title>
    <updated>2025-09-26T02:47:38+00:00</updated>
    <author>
      <name>/u/jeffrey-0711</name>
      <uri>https://old.reddit.com/user/jeffrey-0711</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqq4n6/i_made_a_library_to_help_writing_test_code_for/"&gt; &lt;img alt="I made a library to help writing test code for vLLM." src="https://external-preview.redd.it/YMIHwR4e0oK00ZXbDfhDREwv42vpChwnDCf7he5QwGI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d179eec84dba993ba11d0ca1724e3f0c730327c4" title="I made a library to help writing test code for vLLM." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Does anybody write test code while developing with vLLM?&lt;/p&gt; &lt;p&gt;Introducing &amp;quot;vllm-mock&amp;quot;, my new small open-source.&lt;/p&gt; &lt;p&gt;I love vLLM and know how important test code is in maintaining project quality and bug tracking. But writing test code for LLM inference is hard because it costs GPU time (which means moneyðŸ¤‘) and loading the whole model is pretty slow.&lt;/p&gt; &lt;p&gt;So, I made a small library to provide a mock instance to write test code for vLLM.&lt;/p&gt; &lt;p&gt;With &amp;quot;vllm-mock,&amp;quot; you don't need to create a vLLM mock instance on your ownâ€”I already made one!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NomaDamas/vllm-mock"&gt;https://github.com/NomaDamas/vllm-mock&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to give a starðŸ’« to the repo. Thank you:)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ns8xexofafrf1.png?width=1602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9a469956824445feabd851ac31b78bfd530a7a1"&gt;https://preview.redd.it/ns8xexofafrf1.png?width=1602&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c9a469956824445feabd851ac31b78bfd530a7a1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jeffrey-0711"&gt; /u/jeffrey-0711 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqq4n6/i_made_a_library_to_help_writing_test_code_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqq4n6/i_made_a_library_to_help_writing_test_code_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqq4n6/i_made_a_library_to_help_writing_test_code_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T02:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq4xs9</id>
    <title>Stockmark 2 100B Instruct</title>
    <updated>2025-09-25T12:05:42+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stockmark-2-100B-Instruct is a 100-billion-parameter large language model built from scratch, with a particular focus on Japanese. It was pre-trained on approximately 2.0 trillion tokens of data, consisting of 60% English, 30% Japanese, and 10% code. Following pretraining, the model underwent post-training (SFT and DPO) with synthetic data in Japanese to enhance its ability to follow instructions. This version improves instruction-following ability and adds support for long-context (32k), compared to the previous version &lt;a href="https://huggingface.co/stockmark/Stockmark-2-100B-Instruct"&gt;https://huggingface.co/stockmark/Stockmark-2-100B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T12:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqufim</id>
    <title>The Evolution of Search - A Brief History of Information Retrieval</title>
    <updated>2025-09-26T06:52:33+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqufim/the_evolution_of_search_a_brief_history_of/"&gt; &lt;img alt="The Evolution of Search - A Brief History of Information Retrieval" src="https://external-preview.redd.it/rdiqDR_pScizHcdiYSt4CYhHHCgIfLWKQPCyQEs-E3k.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9980443776d6f25f3145283b6475431f5b3cff07" title="The Evolution of Search - A Brief History of Information Retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/ghE4gQkx2b4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqufim/the_evolution_of_search_a_brief_history_of/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqufim/the_evolution_of_search_a_brief_history_of/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T06:52:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqpnoc</id>
    <title>AMD's GAIA for GenAI adds Linux support: using Vulkan for GPUs, no NPUs yet</title>
    <updated>2025-09-26T02:23:52+00:00</updated>
    <author>
      <name>/u/Fcking_Chuck</name>
      <uri>https://old.reddit.com/user/Fcking_Chuck</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fcking_Chuck"&gt; /u/Fcking_Chuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/news/AMD-GAIA-GenAI-Linux-Support"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqpnoc/amds_gaia_for_genai_adds_linux_support_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqpnoc/amds_gaia_for_genai_adds_linux_support_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T02:23:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqgio2</id>
    <title>In-Browser Codebase to Knowledge Graph generator</title>
    <updated>2025-09-25T19:42:54+00:00</updated>
    <author>
      <name>/u/DeathShot7777</name>
      <uri>https://old.reddit.com/user/DeathShot7777</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqgio2/inbrowser_codebase_to_knowledge_graph_generator/"&gt; &lt;img alt="In-Browser Codebase to Knowledge Graph generator" src="https://external-preview.redd.it/NGhmbm9oZnAyZHJmMSatzj3-4R6EilPMIHLTuZh7EM6HdLWcM6lfSRZEuWNf.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=042bf9353b8749bf35198f75a4c60f2d68f1f6ee" title="In-Browser Codebase to Knowledge Graph generator" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m working on a side project that generates a Knowledge Graph from codebases and provides a Graph-RAG-Agent. It runs entirely &lt;strong&gt;client-side&lt;/strong&gt; in the browser, making it fully private, even the graph database runs in browser through web-assembly. I had posted this here a month ago for advices, now it is working and has massive performance gain. It is now able to generate KG from big repos ( 1000+ files) in seconds. &lt;/p&gt; &lt;p&gt;In theory since its graph based, it should be much more accurate than traditional RAG, hoping to make it as useful and easy to use as gitingest / gitdiagram, and be helpful in understanding big repositories and prevent breaking code changes&lt;/p&gt; &lt;p&gt;Future plan:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ollama support&lt;/li&gt; &lt;li&gt;Exposing browser tab as MCP for AI IDE / CLI can query the knowledge graph directly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Need suggestions on cool feature list.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Repo link: &lt;a href="https://github.com/abhigyanpatwari/GitNexus"&gt;https://github.com/abhigyanpatwari/GitNexus&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Pls leave a star if seemed cool ðŸ« &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tech Jargon:&lt;/strong&gt; It follows this 4-pass system and there are multiple optimizations to make it work inside browser. Uses Tree-sitter WASM to generate AST. The data is stored in a graph DB called Kuzu DB which also runs inside local browser through kuzu-WASM. LLM creates cypher queries which are executed to query the graph.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Pass 1: Structure Analysis&lt;/strong&gt; â€“ Scans the repository, identifies files and folders, and creates a hierarchical &lt;em&gt;CONTAINS&lt;/em&gt; relationship between them.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pass 2: Code Parsing &amp;amp; AST Extraction&lt;/strong&gt; â€“ Uses Tree-sitter to generate abstract syntax trees, extracts functions/classes/symbols, and caches them efficiently.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pass 3: Import Resolution&lt;/strong&gt; â€“ Detects and maps &lt;code&gt;import/require&lt;/code&gt; statements to connect files/modules with &lt;em&gt;IMPORTS&lt;/em&gt; relationships.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pass 4: Call Graph Analysis&lt;/strong&gt; â€“ Links function calls across the project with &lt;em&gt;CALLS&lt;/em&gt; relationships, using exact, fuzzy, and heuristic matching.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Optimizations:&lt;/strong&gt; Uses worker pool for parallel processing. Number of worker is determined from available cpu cores, max limit is set to 20. Kuzu db write is using COPY instead of merge so that the whole data can be dumped at once massively improving performance, although had to use polymorphic tables which resulted in empty columns for many rows, but worth it since writing one batch at a time was taking a lot of time for huge repos. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathShot7777"&gt; /u/DeathShot7777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/b7v2eovm2drf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqgio2/inbrowser_codebase_to_knowledge_graph_generator/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqgio2/inbrowser_codebase_to_knowledge_graph_generator/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T19:42:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq6hdq</id>
    <title>Kimi Infra team releases K2 Vendor Verifier: an openâ€‘source toolâ€‘call validator for LLM providers</title>
    <updated>2025-09-25T13:15:45+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt; &lt;img alt="Kimi Infra team releases K2 Vendor Verifier: an openâ€‘source toolâ€‘call validator for LLM providers" src="https://external-preview.redd.it/h6GA9o9Fh1iKBczAmWMEU51oiY2qNtCo2dIcqBlrOfA.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2a31130f73f9547fcd2000fb95b0ce425bb7225" title="Kimi Infra team releases K2 Vendor Verifier: an openâ€‘source toolâ€‘call validator for LLM providers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lap5au1j7brf1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703ef673fa1ffd579a91b53a656de3eec0fe056e"&gt;https://preview.redd.it/lap5au1j7brf1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703ef673fa1ffd579a91b53a656de3eec0fe056e&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Since the release of the Kimi K2 model, we have received numerous feedback on the precision of Kimi K2 in toolcall. Given that K2 focuses on the agentic loop, the reliability of toolcall is of utmost importance.&lt;/p&gt; &lt;p&gt;We have observed significant differences in the toolcall performance of various open-source solutions and vendors. When selecting a provider, users often prioritize lower latency and cost, but may inadvertently overlook more subtle yet critical differences in model accuracy.&lt;/p&gt; &lt;p&gt;These inconsistencies not only affect user experience but also impact K2's performance in various benchmarking results. To mitigate these problems, we launch K2 Vendor Verifier to monitor and enhance the quality of all K2 APIs.&lt;/p&gt; &lt;p&gt;We hope K2VV can help ensuring that everyone can access a consistent and high-performing Kimi K2 model.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I found in Kimi K2 0905's release &lt;a href="https://platform.moonshot.cn/blog/posts/kimi-k2-0905"&gt;blog&lt;/a&gt; that they mentioned a new technology called &amp;quot;Token Enforcer ensures 100% correct toolcall format&amp;quot;. That's huge!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T13:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqnabr</id>
    <title>Best instruct model that fits in 32gb VRAM</title>
    <updated>2025-09-26T00:28:45+00:00</updated>
    <author>
      <name>/u/swmfg</name>
      <uri>https://old.reddit.com/user/swmfg</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I have a task where I need the LLM to interpret some text, only summarise the relevant paragraphs and return in json format. I've been using Qwen3-4B-Instruct-2507 and I must say, given the size of the model, it's doing quite well. However, I noticed that it seems to waste too much tokens on thinking. I can see that it repeats what it wants to say a few times before exiting thinking mode and actually return me the output. So I'm wondering whether there are better models out there that can fit in my 5090? What would be your go-to model in the &amp;lt;=32gb VRAM range?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/swmfg"&gt; /u/swmfg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqnabr/best_instruct_model_that_fits_in_32gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqnabr/best_instruct_model_that_fits_in_32gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqnabr/best_instruct_model_that_fits_in_32gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T00:28:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqf3vf</id>
    <title>Run Your Local LLMs as Web Agents Directly in Your Browser with BrowserOS</title>
    <updated>2025-09-25T18:49:21+00:00</updated>
    <author>
      <name>/u/PrizeInflation9105</name>
      <uri>https://old.reddit.com/user/PrizeInflation9105</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqf3vf/run_your_local_llms_as_web_agents_directly_in/"&gt; &lt;img alt="Run Your Local LLMs as Web Agents Directly in Your Browser with BrowserOS" src="https://external-preview.redd.it/B0qz1htbzurDEqVn_GMHDL43INLFr3hKzZ1EYoYt7h4.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9d6a7958fa1564136b1b7fa7ac0cd44640e242a8" title="Run Your Local LLMs as Web Agents Directly in Your Browser with BrowserOS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Run web agents using local models from Ollama without any data ever leaving machine. &lt;/p&gt; &lt;p&gt;Itâ€™s a simple, open-source Chromium browser that connects directly to your local API endpoint. You can tell your own models to browse, research, and automate tasks, keeping everything 100% private and free.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PrizeInflation9105"&gt; /u/PrizeInflation9105 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.browseros.com/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqf3vf/run_your_local_llms_as_web_agents_directly_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqf3vf/run_your_local_llms_as_web_agents_directly_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T18:49:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqksbv</id>
    <title>Why do LLMs do the comparative thing so often</title>
    <updated>2025-09-25T22:35:19+00:00</updated>
    <author>
      <name>/u/Imbuyingdrugs</name>
      <uri>https://old.reddit.com/user/Imbuyingdrugs</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For example â€˜Thatâ€™s not a weakness, thatâ€™s a compass pointing you away from the wrong life.â€™ &lt;/p&gt; &lt;p&gt;I see it in so many responses and also I can tell if something is AI just based off this &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Imbuyingdrugs"&gt; /u/Imbuyingdrugs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqksbv/why_do_llms_do_the_comparative_thing_so_often/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqksbv/why_do_llms_do_the_comparative_thing_so_often/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqksbv/why_do_llms_do_the_comparative_thing_so_often/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:35:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqg5q3</id>
    <title>Whatâ€™s your experience with Qwen3-Omni so far?</title>
    <updated>2025-09-25T19:29:16+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Omni is now out for a few days, whatâ€™s your experience with it so far? And what are you using it for?&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Qwen3-Omni is the natively end-to-end multilingual omni model. It processes text, images, audio, and video, and delivers real-time streaming responses in both text and natural speech. We introduce several upgrades to improve performance and efficiency.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Blog: &lt;a href="https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4"&gt;https://qwen.ai/blog?id=65f766fc2dcba7905c1cb69cc4cab90e94126bf4&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Weights: &lt;a href="https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe"&gt;https://huggingface.co/collections/Qwen/qwen3-omni-68d100a86cd0906843ceccbe&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2509.17765"&gt;https://arxiv.org/abs/2509.17765&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqg5q3/whats_your_experience_with_qwen3omni_so_far/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqg5q3/whats_your_experience_with_qwen3omni_so_far/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqg5q3/whats_your_experience_with_qwen3omni_so_far/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T19:29:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq0cp9</id>
    <title>IMPORTANT: Why Abliterated Models SUCK. Here is a better way to uncensor LLMs.</title>
    <updated>2025-09-25T07:26:06+00:00</updated>
    <author>
      <name>/u/Optimal_League_1419</name>
      <uri>https://old.reddit.com/user/Optimal_League_1419</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have been testing many local models.&lt;br /&gt; And... I have noticed that all abliterated models have degraded perfomance compared to the original. Especially the newer MoE models such as Qwen3 30b a3b, they suffer the most from abliteration.&lt;br /&gt; The areas in which they get degraded the most are logical reasoning, agentic tasks and most importantly they hallucinate like crazy which causes abliterated big models like 30b to be often be outperformed by non-abliterated 4-8b models in my tests.&lt;/p&gt; &lt;p&gt;I have noticed a very important pattern.&lt;br /&gt; Models that have been abliterated but also finetuned have very little degredation compared to models that were just abliterated.&lt;br /&gt; Here are some models that were abliterated but finetuned/trained after and they perform equally or outperform the originals but have the amazing added benefit of being completely uncensored:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;mradermacher/Qwen3-30B-A3B-abliterated-erotic-i1-GGUF This model is very powerful. It was abliterated but also trained on uncensored material. I have found this model to perform very close to the original model while being completely uncensored. It does struggle a little more in agentic tasks compared to the original but in everything else its near perfect. Its hallucination rates are very low compared to other abliterated versions of Qwen3 30b a3b and its pretty knowledgable.&lt;/li&gt; &lt;li&gt;mlabonne/NeuralDaredevil-8B-abliterated This model is absolutely amazing, it was abliterated but was also DPO finetuned. The original model was Llama3-8b. This model completely outperforms the original. And again this model is completely uncensored. Also the author of this model has generously provided information about what datasets he used to train this model and what he did to achieve these results.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These two models were the best I have found among the uncensored models made by the community.&lt;/p&gt; &lt;p&gt;Why is &lt;strong&gt;Qwen3-30B-A3B-abliterated-erotic&lt;/strong&gt;-i1-GGUF better than all other abliterated/uncensored Qwen3-30b-a3b models?&lt;br /&gt; I have actually used the i1-Q4_K_S version of this model in my tests.&lt;br /&gt; I have compared it to these models below:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated&lt;/strong&gt;-GGUF/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M.gguf&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010&lt;/strong&gt;-i1-GGUF/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_M.gguf (this model especially sucks)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated&lt;/strong&gt;-GGUF/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M.gguf&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have asked these models the usual uncensored questions like &amp;quot;How to sell meth&amp;quot; all the abliterated Qwen3-30b-a3b models would give me a generic business pitch which was completely unrealistic and more fitting for a candy shop or a tech company rather than an illegal underground drug distribution ring. They made nonesensical strategies.&lt;br /&gt; The &lt;strong&gt;Qwen3-30B-A3B-abliterated-erotic&lt;/strong&gt; model was the only model out of the 4 that actually came up with a reasonable business strategy that would be successful in that scenario.&lt;/p&gt; &lt;p&gt;Another test I did is I tested these models with MCPs and the 3 Huihui models really sucked with tool calls, they would either call the wrong tool for the occasion or they would repeatedly spam the same tool many times in a row without any reason for that. Hallucination...&lt;br /&gt; Again the &lt;strong&gt;Qwen3-30B-A3B-abliterated-erotic&lt;/strong&gt; model won in this case, it called tools correctly more often than the other three models although it performed slightly worse than the original Qwen3-30b a3b model.&lt;br /&gt; Also this model was best at giving facts (its hallucination was the lowset)&lt;/p&gt; &lt;p&gt;I'm actually shocked that a model trained for erotic conversations performs so well. But here we are...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My theory&lt;/strong&gt; is that models trained after abliteration recover most of the perfomance lost during abliteration.&lt;br /&gt; My request to you guys is to try to train Qwen3-30b-a3b after abliteration on a high quality dataset so we can have more high quality uncensored models.&lt;/p&gt; &lt;p&gt;I'm sure that I'm not the only person frustrated with the limited selection of uncensored models today.&lt;br /&gt; Most uncensored models today are very low quality.&lt;br /&gt; My goal is to change that...&lt;br /&gt; &lt;strong&gt;I'm making this post to convince other devs to work on creating good quality uncensored models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you work with fine tuning and finetuning/abliterating models &lt;strong&gt;hit me up&lt;/strong&gt;, I will be more than happy to share all the data I've gathered during testing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I believe that free access to information is a fundamental human right.&lt;/strong&gt; Censored models take away that right to unrestricted access to valuable information.&lt;br /&gt; Without free access to information we become easy to control.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimal_League_1419"&gt; /u/Optimal_League_1419 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq0cp9/important_why_abliterated_models_suck_here_is_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq0cp9/important_why_abliterated_models_suck_here_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq0cp9/important_why_abliterated_models_suck_here_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T07:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqunp9</id>
    <title>Can a 64GB Mac run Qwen3-Next-80B?</title>
    <updated>2025-09-26T07:06:48+00:00</updated>
    <author>
      <name>/u/xieyutong</name>
      <uri>https://old.reddit.com/user/xieyutong</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen comments suggesting that it's tight even on a 48GB Mac, but I'm hoping 64GB might be enough with proper quantization.I've also gathered some important caveats from the community that I'd like to confirm:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Quantization Pitfalls: Many community-shared quantized versions (like the FP8 ones) seem to have issues. A common problem mentioned is that the tokenizer_config.json might be missing the chat_template, which breaks function calling. The suggested fix is to replace it with the original tokenizer_config from the official model repo.&lt;/li&gt; &lt;li&gt;SGLang vs. Memory: Could frameworks like SGLang offer significant memory savings for this model compared to standard vLLM or llama.cpp? However, I saw reports that SGLang might have compatibility issues, particularly with some FP8 quantized versions, causing errors.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;My Goal: I'm planning to compareQwen3-Next-80B (with Claude Code for coding tasks) against GPT-OSS-120B (with Codex) to see if the Qwen combo can be a viable local alternative.Any insights, especially from those who have tried running Qwen3-Next-80B on similar hardware, would be greatly appreciated! Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xieyutong"&gt; /u/xieyutong &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqunp9/can_a_64gb_mac_run_qwen3next80b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqunp9/can_a_64gb_mac_run_qwen3next80b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqunp9/can_a_64gb_mac_run_qwen3next80b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T07:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq4yoy</id>
    <title>16GB VRAM Essentials</title>
    <updated>2025-09-25T12:06:57+00:00</updated>
    <author>
      <name>/u/Few-Welcome3297</name>
      <uri>https://old.reddit.com/user/Few-Welcome3297</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4yoy/16gb_vram_essentials/"&gt; &lt;img alt="16GB VRAM Essentials" src="https://external-preview.redd.it/U8XqPOiV3AZVtQLqefgeW9FqeySJR9_ozWDBINkXoAI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab359b786eea2765d3cb2b27f2a47ec00e777455" title="16GB VRAM Essentials" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good models to try/use if you have 16GB of VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Welcome3297"&gt; /u/Few-Welcome3297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/shb777/16gb-vram-essentials-68a83fc22eb5fc0abd9292dc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4yoy/16gb_vram_essentials/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4yoy/16gb_vram_essentials/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T12:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqo0oo</id>
    <title>The current state of LLM benchmarks is so polluted</title>
    <updated>2025-09-26T01:03:46+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As the title says. &lt;/p&gt; &lt;p&gt;Since the beginning of the LLM craze, every lab has been publishing and cherry picking their results, and there's a lack of transparency from the AI labs. This only affects the consumers.&lt;/p&gt; &lt;p&gt;There are multiple issues that exist today and haven't been solved:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Labs are reporting only the benchmarks where their models look good, they cherry pick results.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Some labs are training on the very same benchmarks they evaluate, maybe not on purpose, but contamination is there.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Most published benchmarks are not actually useful at all, they are usually weird academic cases where the models fail, instead of real-world use patterns of these models.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Every lab uses their own testing methodology, their own parameters and prompts, and they seem to tune things until they appear better than the previous release.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Everyone is implementing their own benchmarks in their own way and never release the code to reproduce.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The APIs fluctuate in quality and some providers are selling quantized versions instead of the original model, thus, we see regressions. Nobody is tracking this.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Is there anyone working on these issues? I'd love to talk if so. We just started working on independent benchmarking and plan to build a standard so anyone can build and publish their own benchmark easily, for any use case. All open source, open data.&lt;/p&gt; &lt;p&gt;Imagine a place that test new releases and report API regressions, in favor of the consumers. Not with academic contaminated benchmarks but with actual real world performance benchmarks.&lt;/p&gt; &lt;p&gt;There's already great websites out there doing an effort, but what I envision is a place where you can find hundreds of community built benchmarks of all kinds (legal, healthcare, roleplay, instruction following, asr, etc). And a way to monitor the real quality of the models out there.&lt;/p&gt; &lt;p&gt;Is this something anyone else shares? or is it just me becoming crazy due to no good existing solution?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqo0oo/the_current_state_of_llm_benchmarks_is_so_polluted/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqo0oo/the_current_state_of_llm_benchmarks_is_so_polluted/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqo0oo/the_current_state_of_llm_benchmarks_is_so_polluted/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T01:03:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqe2wq</id>
    <title>support for GroveMoE has been merged into llama.cpp</title>
    <updated>2025-09-25T18:09:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqe2wq/support_for_grovemoe_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support for GroveMoE has been merged into llama.cpp" src="https://external-preview.redd.it/nxPo6cfvMPF7ggoi2vynbmnR5sz82ODMoRIW0fu1uyY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1da8cf735a8b89aee205f00585306fab9d51e04" title="support for GroveMoE has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model by InclusionAI:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;GroveMoE&lt;/strong&gt;, a new sparse architecture using &lt;strong&gt;adjugate experts&lt;/strong&gt; for dynamic computation allocation, featuring the following key highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Novel &lt;strong&gt;adjugate experts&lt;/strong&gt; grouped with ordinary experts; shared computation is executed once, then reused, cutting FLOPs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sparse Activation&lt;/strong&gt;: 33 B params total, only &lt;strong&gt;3.14â€“3.28 B&lt;/strong&gt; active per token.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Traning&lt;/strong&gt;: Mid-training + SFT, up-cycled from Qwen3-30B-A3B-Base; preserves prior knowledge while adding new capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15510"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqe2wq/support_for_grovemoe_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqe2wq/support_for_grovemoe_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T18:09:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqd3fo</id>
    <title>llama.cpp now supports Qwen3 reranker</title>
    <updated>2025-09-25T17:32:35+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After adding &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l3vt95/comment/mw4k324/?context=3"&gt;support for Qwen3 embeddings&lt;/a&gt; a while ago, &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14029"&gt;support for Qwen3 rerankers&lt;/a&gt; was just merged. Note that the conversion script was changed in that MR. That means that you'll need a fresh GGUF for it to give correct results, not one of those that were uploaded months ago.&lt;/p&gt; &lt;p&gt;So how to run a simple example and what does it do?&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-embedding -m qwen3-reranker-0.6b_Q8_0.gguf --embd-normalize -1 -p &amp;quot;&amp;lt;question&amp;gt;\t&amp;lt;document&amp;gt;&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You run this for the question and for each document that you found regarding that question. This then gives a score how well the document matches the question. Here are 4 reranked snippets for the following question:&lt;/p&gt; &lt;p&gt;&lt;em&gt;What does reranking mean?&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;0.998&lt;/strong&gt; &amp;quot;Reranking is one of the simplest methods for dramatically improving recall performance in Retrieval Augmented Generation (RAG) or any other retrieval-based pipeline.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.996&lt;/strong&gt; &amp;quot;A reranking model â€” also known as a cross-encoder â€” is a type of model that, given a query and document pair, will output a similarity score.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.190&lt;/strong&gt; &amp;quot;Given 40M records, if we use a small reranking model like BERT on a V100 GPU â€” we'd be waiting more than 50 hours to return a single query result.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.001&lt;/strong&gt; &amp;quot;Before setting up the retrieval pipeline, we need data to retrieve! We will use the jamescalam/ai-arxiv-chunked dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqd3fo/llamacpp_now_supports_qwen3_reranker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqd3fo/llamacpp_now_supports_qwen3_reranker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqd3fo/llamacpp_now_supports_qwen3_reranker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T17:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq1ia2</id>
    <title>China already started making CUDA and DirectX supporting GPUs, so over of monopoly of NVIDIA. The Fenghua No.3 supports latest APIs, including DirectX 12, Vulkan 1.2, and OpenGL 4.6.</title>
    <updated>2025-09-25T08:44:03+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1ia2/china_already_started_making_cuda_and_directx/"&gt; &lt;img alt="China already started making CUDA and DirectX supporting GPUs, so over of monopoly of NVIDIA. The Fenghua No.3 supports latest APIs, including DirectX 12, Vulkan 1.2, and OpenGL 4.6." src="https://preview.redd.it/kvkovm34x9rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45659c3155f1d8f5d8b88f474aa1e034afc37b1f" title="China already started making CUDA and DirectX supporting GPUs, so over of monopoly of NVIDIA. The Fenghua No.3 supports latest APIs, including DirectX 12, Vulkan 1.2, and OpenGL 4.6." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kvkovm34x9rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1ia2/china_already_started_making_cuda_and_directx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1ia2/china_already_started_making_cuda_and_directx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T08:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq182d</id>
    <title>Alibaba just unveiled their Qwen roadmap. The ambition is staggering!</title>
    <updated>2025-09-25T08:24:45+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"&gt; &lt;img alt="Alibaba just unveiled their Qwen roadmap. The ambition is staggering!" src="https://preview.redd.it/5tm4p90rt9rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6724dd3297826b1a060f45ea0c5e1fd9e366f5ab" title="Alibaba just unveiled their Qwen roadmap. The ambition is staggering!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two big bets: unified multi-modal models and extreme scaling across every dimension.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Context length: 1M â†’ 100M tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Parameters: trillion â†’ ten trillion scale&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Test-time compute: 64k â†’ 1M scaling&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Data: 10 trillion â†’ 100 trillion tokens&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They're also pushing synthetic data generation &amp;quot;without scale limits&amp;quot; and expanding agent capabilities across complexity, interaction, and learning modes.&lt;/p&gt; &lt;p&gt;The &amp;quot;scaling is all you need&amp;quot; mantra is becoming China's AI gospel.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tm4p90rt9rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T08:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqhlyw</id>
    <title>I'm testing the progress on GitHub. Qwen Next gguf. Fingers crossed.</title>
    <updated>2025-09-25T20:25:45+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"&gt; &lt;img alt="I'm testing the progress on GitHub. Qwen Next gguf. Fingers crossed." src="https://external-preview.redd.it/EKetf3gBiUdNtrEdMnvehl50wpsTXtAb_rcC73bjkFY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a8b0037a07998e56cdb65b5e64502a4aceb6ef48" title="I'm testing the progress on GitHub. Qwen Next gguf. Fingers crossed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4zwiyxqyddrf1.png?width=1163&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ab1462bb56937da0853af07c994da164ffdeb090"&gt;qwen next&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Can't wait to test the final build. &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;https://github.com/ggml-org/llama.cpp/pull/16095&lt;/a&gt; . Thx for your hard work &lt;a href="https://github.com/pwilkin"&gt;&lt;strong&gt;pwilkin&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqhlyw/im_testing_the_progress_on_github_qwen_next_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T20:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqr5lp</id>
    <title>Kwaipilot/KAT-Dev</title>
    <updated>2025-09-26T03:41:01+00:00</updated>
    <author>
      <name>/u/random-tomato</name>
      <uri>https://old.reddit.com/user/random-tomato</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"&gt; &lt;img alt="Kwaipilot/KAT-Dev" src="https://external-preview.redd.it/RFhQxPOdxc2Em-bjhotgIyTCAALVqXONnbv5f8ro8uY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=24d680deeef40cd14e1c0a13e00f25c88680f997" title="Kwaipilot/KAT-Dev" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;KAT-Dev-32B&lt;/strong&gt; is an open-source 32B-parameter model for software engineering tasks.&lt;/p&gt; &lt;p&gt;On SWE-Bench Verified, &lt;strong&gt;KAT-Dev-32B&lt;/strong&gt; achieves comparable performance with &lt;strong&gt;62.4%&lt;/strong&gt; resolved and ranks &lt;strong&gt;5th&lt;/strong&gt; among all open-source models with different scales.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/random-tomato"&gt; /u/random-tomato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Kwaipilot/KAT-Dev"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqr5lp/kwaipilotkatdev/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T03:41:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqaiaz</id>
    <title>Tencent is teasing the worldâ€™s most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28</title>
    <updated>2025-09-25T15:54:29+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"&gt; &lt;img alt="Tencent is teasing the worldâ€™s most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28" src="https://preview.redd.it/t8w84ihz1crf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35af183180dd8d2fd35f6be46e55d9a4dc75e26d" title="Tencent is teasing the worldâ€™s most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8w84ihz1crf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T15:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqb3p3</id>
    <title>What? Running Qwen-32B on a 32GB GPU (5090).</title>
    <updated>2025-09-25T16:16:51+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt; &lt;img alt="What? Running Qwen-32B on a 32GB GPU (5090)." src="https://external-preview.redd.it/eGQxejJvNXo1Y3JmMc7C-li4AFXa_Q-5qATmlwGRne0zNSJFPFjYVcktZ0y0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b1e06d42c9067d85e9ec551328755bfdad37895" title="What? Running Qwen-32B on a 32GB GPU (5090)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/01adz6it5crf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T16:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqkx7o</id>
    <title>Apparently all third party providers downgrade, none of them provide a max quality model</title>
    <updated>2025-09-25T22:41:03+00:00</updated>
    <author>
      <name>/u/Charuru</name>
      <uri>https://old.reddit.com/user/Charuru</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"&gt; &lt;img alt="Apparently all third party providers downgrade, none of them provide a max quality model" src="https://preview.redd.it/k5on2q9i2erf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c1c2fb9cea2d9deb0e87434f884c9dc83258dd" title="Apparently all third party providers downgrade, none of them provide a max quality model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charuru"&gt; /u/Charuru &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k5on2q9i2erf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkx7o/apparently_all_third_party_providers_downgrade/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqkayx</id>
    <title>I trained an LLM from scratch AMA!</title>
    <updated>2025-09-25T22:14:44+00:00</updated>
    <author>
      <name>/u/thebadslime</name>
      <uri>https://old.reddit.com/user/thebadslime</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's been a few months and I have posted a few times but I am finished!&lt;/p&gt; &lt;p&gt;I used Claude to write my training scripts, and I trained a 960M model on public domain data. It was not fast or easy, but it only cost $500 ( I received free credits from Amazon). It took 3 attempts to get it right. Happy to go into detail&lt;/p&gt; &lt;p&gt;It's a LLama 3 architecture with a 3:1 GQA, flash attention 2, and sink tokens. I have not began post-training yet, so it is NOT VERY USABLE!!!&lt;/p&gt; &lt;p&gt;I am hoping that post turns it into something useful, I have used 1B base models and they all kind of suck.&lt;/p&gt; &lt;p&gt;Post training will be TRL with DPO and the ultrafeedbck dataset. The mdoel is released under the CC0 license, do as you will with it.&lt;/p&gt; &lt;p&gt;Project website: &lt;a href="https://www.libremodel.xyz/"&gt;The LibreModel Project&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face : &lt;a href="https://huggingface.co/jerrimu/libremodel"&gt;jerrimu/libremodel Â· Hugging Face&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github ( GGUF here): &lt;a href="https://github.com/openconstruct/libremodel/releases"&gt;Releases Â· openconstruct/libremodel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I would like to train more open source models, and am seeking donations for hardware: If you would like to support this cause you may donate here : &lt;a href="https://github.com/sponsors/openconstruct"&gt;Sponsor @openconstruct on GitHub Sponsors&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thebadslime"&gt; /u/thebadslime &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqkayx/i_trained_an_llm_from_scratch_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T22:14:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
