<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-31T18:53:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1okp58l</id>
    <title>vLLM, how does it use empty VRAM region?</title>
    <updated>2025-10-31T08:35:11+00:00</updated>
    <author>
      <name>/u/PlanetMercurial</name>
      <uri>https://old.reddit.com/user/PlanetMercurial</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Trying to understand how vLLM works?&lt;br /&gt; So say if I have single 96GB GPU.&lt;br /&gt; And my model fits in 16GB... that gives me 80GB spare VRAM... &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Now if i send 3 concurrent requests to vLLM each of 10000 tokens, how would vLLM process that? I guess each of those 10000 tokens use up VRAM... and then what magic does vLLM do to get the concurrent processing does.. . does it use up the other spare VRAM to get it done?&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What does batching mean.. is a single request of 10000 tokens considered a batch? Or does batch need to be setup as a separate parameter?&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PlanetMercurial"&gt; /u/PlanetMercurial &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okp58l/vllm_how_does_it_use_empty_vram_region/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okp58l/vllm_how_does_it_use_empty_vram_region/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okp58l/vllm_how_does_it_use_empty_vram_region/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T08:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oksprm</id>
    <title>Is it possible to use vram like ram is multigpu setups?</title>
    <updated>2025-10-31T12:04:40+00:00</updated>
    <author>
      <name>/u/opoot_</name>
      <uri>https://old.reddit.com/user/opoot_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a weird question, but I mean this in terms of using MOE models.&lt;/p&gt; &lt;p&gt;I have 2 MI50s and a 7900 xt, which I have the 7900xt in my gaming PC.&lt;/p&gt; &lt;p&gt;The 7900xt has a far stronger gpu chip while the mi50s have more faster vram.&lt;/p&gt; &lt;p&gt;Given that is is very popular to use a gpu for prompt processing for MOE models while forcing the weights on the system ram, can I do the same thing to use the 7900xt for prompt processing while still leveraging the vram of the mi50s?&lt;/p&gt; &lt;p&gt;Or is there anyway to combine the 3 gpu in a way where I can make more use of the 7900xt‚Äôs strong chip?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/opoot_"&gt; /u/opoot_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oksprm/is_it_possible_to_use_vram_like_ram_is_multigpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oksprm/is_it_possible_to_use_vram_like_ram_is_multigpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oksprm/is_it_possible_to_use_vram_like_ram_is_multigpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T12:04:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol2odj</id>
    <title>I fine tuned a (small) model to help with reasoning backfill on old/non-reasoning datasets</title>
    <updated>2025-10-31T18:39:58+00:00</updated>
    <author>
      <name>/u/joeyzero</name>
      <uri>https://old.reddit.com/user/joeyzero</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2odj/i_fine_tuned_a_small_model_to_help_with_reasoning/"&gt; &lt;img alt="I fine tuned a (small) model to help with reasoning backfill on old/non-reasoning datasets" src="https://external-preview.redd.it/XlylphZC1EXPPsyoBwqV24OccqWD2HchWXqXQLjgX7U.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f6888a387b5da2d855423f2536a91f90ab6dd2e4" title="I fine tuned a (small) model to help with reasoning backfill on old/non-reasoning datasets" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to play around with trying to synthesize reasoning traces for older/chat datasets where reasoning wasn't conventionalized yet. I wasn't able to find a model that could do the job, so I tried throwing one together by moving the logic around from existing reasoning datasets to see if we could infer reasoning from a given input and output without changing the example output.&lt;/p&gt; &lt;p&gt;This model is just a lil guy, but I'm pretty happy with the results so far. I'd love to try applying this same idea to stylized (aka brainrot) models to see if we can generate datasets to train models with highly stylized thinking. I'd also like to try this with a larger model someday to see if we get tracers that are more coherent, but for my use case (just trying to augment conversational datasets). Currently, I feel like this model is really only suitable for bootstrapping reasoning back into a model that has lost its reasoning capability, but I'm still throwing examples at it to see what it can reasonably do.&lt;/p&gt; &lt;p&gt;Anyway... There's a prompt example in the readme. If anyone ends up playing around with it, let me know what you think. I feel like there's still lots of room for improvement, but I'm really surprised with the results so far.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/joeyzero"&gt; /u/joeyzero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/joeyzero/Qwen3-4B-Reasoning-Backfill-v0.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2odj/i_fine_tuned_a_small_model_to_help_with_reasoning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2odj/i_fine_tuned_a_small_model_to_help_with_reasoning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:39:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1okx7kq</id>
    <title>For those who‚Äôve been following my dev journey, the first AgentTrace milestone üëÄ</title>
    <updated>2025-10-31T15:11:46+00:00</updated>
    <author>
      <name>/u/AdVivid5763</name>
      <uri>https://old.reddit.com/user/AdVivid5763</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okx7kq/for_those_whove_been_following_my_dev_journey_the/"&gt; &lt;img alt="For those who‚Äôve been following my dev journey, the first AgentTrace milestone üëÄ" src="https://preview.redd.it/zxfgomk6rgyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3df414230ba8664fcd4ec3bc3a7bd4e9da417dd" title="For those who‚Äôve been following my dev journey, the first AgentTrace milestone üëÄ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For those who‚Äôve been following the process, here‚Äôs the first real visual milestone for AgentTrace, my project to see how AI agents think.&lt;/p&gt; &lt;p&gt;It‚Äôs a Cognitive Flow Visualizer that maps every step of an agent‚Äôs reasoning, so instead of reading endless logs, you can actually see the decision flow:&lt;/p&gt; &lt;p&gt;üß© Nodes for Input, Action, Validation, Output üîÅ Loops showing reasoning divergence üéØ Confidence visualization (color-coded edges) ‚ö†Ô∏è Failure detection for weak reasoning paths&lt;/p&gt; &lt;p&gt;The goal isn‚Äôt to make agents smarter, it‚Äôs to make them understandable.&lt;/p&gt; &lt;p&gt;For the first time, you can literally watch an agent think, correct itself, and return to the user, like seeing the cognitive map behind the chat.&lt;/p&gt; &lt;p&gt;Next phase: integrating real reasoning traces to explain why each step was taken, not just what happened.&lt;/p&gt; &lt;p&gt;Curious how you‚Äôd use reasoning visibility in your own builds, debugging, trust, teaching, or optimization?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdVivid5763"&gt; /u/AdVivid5763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zxfgomk6rgyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okx7kq/for_those_whove_been_following_my_dev_journey_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okx7kq/for_those_whove_been_following_my_dev_journey_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok2lht</id>
    <title>Qwen 3 VL merged into llama.cpp!</title>
    <updated>2025-10-30T15:21:24+00:00</updated>
    <author>
      <name>/u/ervertes</name>
      <uri>https://old.reddit.com/user/ervertes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16780"&gt;https://github.com/ggml-org/llama.cpp/pull/16780&lt;/a&gt;&lt;/p&gt; &lt;p&gt;WE ARE SO BACK!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ervertes"&gt; /u/ervertes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2lht/qwen_3_vl_merged_into_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2lht/qwen_3_vl_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok2lht/qwen_3_vl_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T15:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol0tvf</id>
    <title>Hypothetical: if you had a gb300 nvl72 , what would you do with it?</title>
    <updated>2025-10-31T17:29:21+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hypothetical: Suppose you lived in an alternative world, where gb300 nvl72s were affordable for an avg enthusiast and you had one and a 850kw outlet , what would you do with it? It has 21 tb of hbm memory and 1440 petaflops of fp4 sparse compute and up to ~ 576 TB/s of aggregate bandwidth.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol0tvf/hypothetical_if_you_had_a_gb300_nvl72_what_would/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol0tvf/hypothetical_if_you_had_a_gb300_nvl72_what_would/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol0tvf/hypothetical_if_you_had_a_gb300_nvl72_what_would/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T17:29:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1okx07d</id>
    <title>Future of APUs for local AI?</title>
    <updated>2025-10-31T15:03:57+00:00</updated>
    <author>
      <name>/u/Excellent_Koala769</name>
      <uri>https://old.reddit.com/user/Excellent_Koala769</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What do you think about the future of APUs? Will they become dominant over GPUs for local AI inferencing?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Excellent_Koala769"&gt; /u/Excellent_Koala769 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okx07d/future_of_apus_for_local_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okx07d/future_of_apus_for_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okx07d/future_of_apus_for_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:03:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol1xji</id>
    <title>I'm currently solving a problem I have with Ollama and LM Studio.</title>
    <updated>2025-10-31T18:11:11+00:00</updated>
    <author>
      <name>/u/Sileniced</name>
      <uri>https://old.reddit.com/user/Sileniced</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1xji/im_currently_solving_a_problem_i_have_with_ollama/"&gt; &lt;img alt="I'm currently solving a problem I have with Ollama and LM Studio." src="https://a.thumbs.redditmedia.com/7CIswPRY8uX4FmBUl_hjfcodZvGvvL5-q1_tIqfpZ80.jpg" title="I'm currently solving a problem I have with Ollama and LM Studio." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am currently working on &lt;a href="https://github.com/veighnsche/llama-orch"&gt;rbee (formerly named llama-orch)&lt;/a&gt;. rbee is an Ollama- or LM Studio‚Äìlike program.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How is rbee different?&lt;/strong&gt;&lt;br /&gt; In addition to running on your local machine, it can securely connect to all the GPUs in your local network. You can choose exactly which GPU runs which LLM, image, video, or sound model. In the future, you‚Äôll even be able to choose which GPU to use for gaming and which one to dedicate as an inference server.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works&lt;/strong&gt;&lt;br /&gt; You start with the &lt;em&gt;rbee-keeper&lt;/em&gt;, which provides the GUI. The &lt;em&gt;rbee-keeper&lt;/em&gt; orchestrates the &lt;em&gt;queen-rbee&lt;/em&gt; (which supports an OpenAI-compatible API server) and can also manage &lt;em&gt;rbee-hives&lt;/em&gt; on the local machine or on other machines via secure SSH connections.&lt;/p&gt; &lt;p&gt;&lt;em&gt;rbee-hives&lt;/em&gt; are responsible for handling all operations on a computer, such as starting and stopping &lt;em&gt;worker-rbee&lt;/em&gt; instances on that system. A &lt;em&gt;worker-rbee&lt;/em&gt; is a program that performs the actual LLM inference and sends the results back to the queen or the UI. There are many types of workers, and the system is freely extensible.&lt;/p&gt; &lt;p&gt;The &lt;em&gt;queen-rbee&lt;/em&gt; connects all the hives (computers with GPUs) and exposes them as a single HTTP API. You can fully script the scheduling using &lt;a href="https://github.com/veighnsche/llama-orch/blob/44e3e7d294763cceaf300d696837fe5256c9e32d/.business/stakeholders/RHAI_PROGRAMMABLE_SCHEDULER.md"&gt;Rhai&lt;/a&gt;, allowing you to decide how AI jobs are routed to specific GPUs.&lt;/p&gt; &lt;p&gt;I‚Äôm trying to make this as extensible as possible for the open-source community. It‚Äôs very easy to create your own custom &lt;em&gt;queen-rbee&lt;/em&gt;, &lt;em&gt;rbee-hive&lt;/em&gt;, or &lt;em&gt;worker&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;There are major plans for security, as I want rbee to be approved for EU usage that requires operational auditing.&lt;/p&gt; &lt;p&gt;If you have multiple GPUs or multiple computers with GPUs, rbee can turn them into a cloud-like infrastructure that all comes together under one API endpoint such as &lt;code&gt;/v1/chat&lt;/code&gt;. The &lt;em&gt;queen-rbee&lt;/em&gt; then determines the best GPU to handle the request‚Äîeither automatically or according to your custom rules and policies.&lt;/p&gt; &lt;p&gt;I would really appreciate it if you gave the repo a star. I‚Äôm a passionate software engineer who couldn‚Äôt thrive in the corporate environment and would rather build sustainable open source. Please let me know if this project interests you or if you have potential use cases for it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sileniced"&gt; /u/Sileniced &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol1xji"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1xji/im_currently_solving_a_problem_i_have_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1xji/im_currently_solving_a_problem_i_have_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:11:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1okybza</id>
    <title>8-Pin PCIE (single) to 12VHPWR - Cable problem solved</title>
    <updated>2025-10-31T15:54:43+00:00</updated>
    <author>
      <name>/u/richardbaxter</name>
      <uri>https://old.reddit.com/user/richardbaxter</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okybza/8pin_pcie_single_to_12vhpwr_cable_problem_solved/"&gt; &lt;img alt="8-Pin PCIE (single) to 12VHPWR - Cable problem solved" src="https://b.thumbs.redditmedia.com/0jvjWYpUIDaHKDqLZWfJyJOwO4_8IlAuza1m8Aydv3o.jpg" title="8-Pin PCIE (single) to 12VHPWR - Cable problem solved" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have a Corsair power supply, which uses Type 4 cables in my LLM server. It's an asus WRX80E-SAGE motherboard, so theres 7 pci slots. Ideal for my bootstrapped, single slot Ada rtx gpus. The one problem I've had is not enough ports on the psu to run 6 gpus (which is what I've built). &lt;/p&gt; &lt;p&gt;I'd been looking for a custom power cable that connects from one of the 8-pin PCIE/CPU power ports (I think these pcie/cpu ports are modular and support different pinouts for ATX12V/EPS12V/ PCIE) on the PSU to a 16-pin 12VHPWR connector. &lt;/p&gt; &lt;p&gt;This is to power single ADA RTX4000's (from 1 pcie port only) - they only need around 130w and certainly not the 600w a 12VHPWR plug is rated to. So all in all it felt like a safe bet to try it out. &lt;/p&gt; &lt;p&gt;Anyway, took me a while but I got these from MODDIY, they work and they're nicely made. They even correctly implemented sense pins (SENSEO/SENSEI) to signal the proper power delivery capability to the graphics card.&lt;/p&gt; &lt;p&gt;Hope sharing this solves a similar problem for other folks! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/richardbaxter"&gt; /u/richardbaxter &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1okybza"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okybza/8pin_pcie_single_to_12vhpwr_cable_problem_solved/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okybza/8pin_pcie_single_to_12vhpwr_cable_problem_solved/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1okbrz4</id>
    <title>IBM just released unsloth for finetinuing Granite4.0_350M</title>
    <updated>2025-10-30T21:08:34+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okbrz4/ibm_just_released_unsloth_for_finetinuing/"&gt; &lt;img alt="IBM just released unsloth for finetinuing Granite4.0_350M" src="https://preview.redd.it/vn84zysldbyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c3e80873a766a970e98766336d69701437c1c9c8" title="IBM just released unsloth for finetinuing Granite4.0_350M" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/unslothai/notebooks/blob/main/nb/Granite4.0_350M.ipynb"&gt;https://github.com/unslothai/notebooks/blob/main/nb/Granite4.0_350M.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Big ups for the IBM folks for following up so quickly and thanks to the unsloth guys for working with them. You guys are amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vn84zysldbyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okbrz4/ibm_just_released_unsloth_for_finetinuing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okbrz4/ibm_just_released_unsloth_for_finetinuing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T21:08:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz32u</id>
    <title>What has been your experience with high latency in your AI coding tools?</title>
    <updated>2025-10-31T16:22:57+00:00</updated>
    <author>
      <name>/u/InceptionAI_Tom</name>
      <uri>https://old.reddit.com/user/InceptionAI_Tom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious about everyone‚Äôs experience with high latency in your AI applications.&lt;/p&gt; &lt;p&gt;High latency seems to be a pretty common issue I see talked about here.&lt;/p&gt; &lt;p&gt;What have you tried and what has worked? What hasn‚Äôt worked?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InceptionAI_Tom"&gt; /u/InceptionAI_Tom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz32u/what_has_been_your_experience_with_high_latency/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz32u/what_has_been_your_experience_with_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz32u/what_has_been_your_experience_with_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:22:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz6ko</id>
    <title>What's the difference between f16 and bf16 mmproj GGUF files for Qwen3-VL?</title>
    <updated>2025-10-31T16:26:40+00:00</updated>
    <author>
      <name>/u/windows_error23</name>
      <uri>https://old.reddit.com/user/windows_error23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is a stupid question. Some quant providers upload both, along with f32. Isn't the model originally in bf16? Which is higher quality. Thanks a lot for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/windows_error23"&gt; /u/windows_error23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz6ko/whats_the_difference_between_f16_and_bf16_mmproj/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz6ko/whats_the_difference_between_f16_and_bf16_mmproj/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz6ko/whats_the_difference_between_f16_and_bf16_mmproj/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1okto5m</id>
    <title>Granite-4.0-H-1B as a thesaurus</title>
    <updated>2025-10-31T12:48:50+00:00</updated>
    <author>
      <name>/u/DHasselhoff77</name>
      <uri>https://old.reddit.com/user/DHasselhoff77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okto5m/granite40h1b_as_a_thesaurus/"&gt; &lt;img alt="Granite-4.0-H-1B as a thesaurus" src="https://preview.redd.it/jee57z2h1gyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3c62cb546f07086f857e19cd67d8f579a0883539" title="Granite-4.0-H-1B as a thesaurus" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DHasselhoff77"&gt; /u/DHasselhoff77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jee57z2h1gyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okto5m/granite40h1b_as_a_thesaurus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okto5m/granite40h1b_as_a_thesaurus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T12:48:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1okq6ms</id>
    <title>Anyone else running their whole AI stack as Proxmox LXC containers? Im currently using Open WebUI as front-end, LiteLLM as a router and A vLLM container per model as back-ends</title>
    <updated>2025-10-31T09:44:18+00:00</updated>
    <author>
      <name>/u/AFruitShopOwner</name>
      <uri>https://old.reddit.com/user/AFruitShopOwner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okq6ms/anyone_else_running_their_whole_ai_stack_as/"&gt; &lt;img alt="Anyone else running their whole AI stack as Proxmox LXC containers? Im currently using Open WebUI as front-end, LiteLLM as a router and A vLLM container per model as back-ends" src="https://preview.redd.it/gva8eiw9weyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7d91a864d5dc7462667570db07c5263e8a57532" title="Anyone else running their whole AI stack as Proxmox LXC containers? Im currently using Open WebUI as front-end, LiteLLM as a router and A vLLM container per model as back-ends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not implemented it yet, but I believe it should be possible for LiteLLM to interface with the Proxmox API and dynamically turn on and off vLLM containers depening on what model users select (in Open WebUI). Does anyone have any experience with this?&lt;/p&gt; &lt;p&gt;I want to add a container for n8n for automation workflows (connected to LiteLLM for AI models), a websearch MCP container running something like Searxng (because I find the web search implementation in Open WebUI to be extremely limited) and an (agentic) RAG service. I need robust retrieval over professional/Dutch GAAP/IFRS accounting materials, internal company docs, client data, and relevant laws/regulations. There seem to be a million ways to do RAG; this will be the cornerstone of the system.&lt;/p&gt; &lt;p&gt;I built this AI server/Workstation for the Dutch accounting firm I work at (I have no IT background myself so its been quite the learning proces). Managment wanted everything local and I jumped on the oppertunity to learn something new.&lt;/p&gt; &lt;p&gt;My specs:&lt;br /&gt; CPU - AMD EPYC 9575F&lt;br /&gt; Dual GMI links allowing it to use almost all of the theoretical system memory bandwidth, 5Ghz Boost clock, 64 core, 128 thread beast of a CPU, seems to me like the best choice for an AI exterimentation server. Great as a host for GPU inference, Hybrid Inference (GPU + System memory spillover) and CPU only inference.&lt;/p&gt; &lt;p&gt;RAM - 1.152tb (12x96gb RDIMMs ) ECC DDR5 6.400MT/s RAM (~614gb/s theoretical max bandwidth). Will allow me to run massive MOE models on the CPU, albeit slowly. Also plenty or ram for any other service I want to run.&lt;/p&gt; &lt;p&gt;MOBO - Supermicro H13SSL-N (Rev. 2.01). I have a Supermicro H14SSL-NT on backorder but it could be a couple of weeks before I get that one.&lt;/p&gt; &lt;p&gt;GPU's - 3x Nvidia RTX Pro 6000 Max-Q. I was planning on getting 2 Workstation editions but the supplier kept fucking up my order and sending me the Max Q's. Eventually caved and got a third Max-Q because I had plenty of cooling and power capacity. 3 gpu's is not ideal for tensor parallelism but pipleline- and expert parallelism are decent alternatives when 2x96 gb is not enough. Maybe I'll get a 4th one eventually.&lt;/p&gt; &lt;p&gt;Storage - A bunch of Kioxia CM7 R's.&lt;/p&gt; &lt;p&gt;Gpt-oss 120b is the main 'workhorse' model. It comfortably fits in a single GPU so I can use the other GPU's to run auxiliary models that can assist gpt-oss 120b. Maybe a couple of gpt-oss 20b models in a websearch mcp server, a vision language model like Qwen 3 VL, Deepseek-OCR or Gemma 3 for pictures/files.&lt;/p&gt; &lt;p&gt;As mentioned, I don‚Äôt come from an IT background, so I‚Äôm looking for practical advice and sanity checks. How does this setup look? Is there anything you‚Äôd fundamentally do differently? I followed a bunch of guides (mostly the excellent ones from DigitalSpaceport), got about 90% of the way with ChatGPT 5 Thinking, and figured out the last 10% through trial and error (Proxmox Snapshots make the trail and error approach really easy).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AFruitShopOwner"&gt; /u/AFruitShopOwner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gva8eiw9weyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okq6ms/anyone_else_running_their_whole_ai_stack_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okq6ms/anyone_else_running_their_whole_ai_stack_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T09:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1okp7kq</id>
    <title>Glm Rickrolled meüò≠üò≠üò≠</title>
    <updated>2025-10-31T08:39:36+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okp7kq/glm_rickrolled_me/"&gt; &lt;img alt="Glm Rickrolled meüò≠üò≠üò≠" src="https://preview.redd.it/4m02xmp1teyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9daf0601d1b7d2d29160d60da118845bac56dc56" title="Glm Rickrolled meüò≠üò≠üò≠" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://chat.z.ai/s/1aead084-3e5f-41d4-94ef-e8a0fd8ac04a"&gt;Chat&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.z.ai/space/y0u7899c45v1-art"&gt;Space&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4m02xmp1teyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okp7kq/glm_rickrolled_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okp7kq/glm_rickrolled_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T08:39:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1okguct</id>
    <title>Another dim of scaling? ByteDance drops ‚ÄúOuro‚Äù: 1.4B ‚âà 4B, 2.6B ‚âà/Ôºû 8B</title>
    <updated>2025-10-31T00:45:55+00:00</updated>
    <author>
      <name>/u/RunTop7329</name>
      <uri>https://old.reddit.com/user/RunTop7329</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"&gt; &lt;img alt="Another dim of scaling? ByteDance drops ‚ÄúOuro‚Äù: 1.4B ‚âà 4B, 2.6B ‚âà/Ôºû 8B" src="https://preview.redd.it/5nubsersacyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22934e9d6b42924c165caa81b6e7db406e1bd999" title="Another dim of scaling? ByteDance drops ‚ÄúOuro‚Äù: 1.4B ‚âà 4B, 2.6B ‚âà/Ôºû 8B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;recurrent depth with shared weights + early-exit gates; trained to 7.7T tokens.&lt;/li&gt; &lt;li&gt;2.6B model ‚â• 8B baselines on reasoning (e.g., MMLU-Pro 55.73, BBH 80.46, MATH500 90.85); 1.4B ‚âà 4B.&lt;/li&gt; &lt;li&gt;Gains credited to better reasoning/knowledge manipulation, not more memorized facts.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I guess it is more friendly to individual home users. The logic goes the opposite of MoE. Basically, activated parameters &amp;gt; 100%. Correct me if wrong.&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2510.25741"&gt;Scaling Latent Reasoning via Looped Language Models&lt;/a&gt;, &lt;a href="https://ouro-llm.github.io/"&gt;https://ouro-llm.github.io/&lt;/a&gt;, &lt;a href="https://x.com/tianyu_zh/status/1983784440829522364"&gt;https://x.com/tianyu_zh/status/1983784440829522364&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RunTop7329"&gt; /u/RunTop7329 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5nubsersacyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okguct/another_dim_of_scaling_bytedance_drops_ouro_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T00:45:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol2oxw</id>
    <title>Unbound In-Character Reasoning Model - Apollo-V0.1-4B-Thinking</title>
    <updated>2025-10-31T18:40:35+00:00</updated>
    <author>
      <name>/u/AllThingsIntel</name>
      <uri>https://old.reddit.com/user/AllThingsIntel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An experimental model with many of its creative inhibitions lifted. Its internal reasoning process adapts to the persona you assign (via the system prompt), allowing it to explore a wider spectrum of themes. This is a V0.1 preview for testing. More refined versions (non-reasoning variants as well) are planned. Follow for updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllThingsIntel"&gt; /u/AllThingsIntel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AllThingsIntel/Apollo-V0.1-4B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1oko2ar</id>
    <title>Want to run claude like model on ~$10k budget. Please help me with the machine build. I don't want to spend on cloud.</title>
    <updated>2025-10-31T07:19:40+00:00</updated>
    <author>
      <name>/u/LordSteinggard</name>
      <uri>https://old.reddit.com/user/LordSteinggard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally saved money for this, want to have my own rig. Works that I will be doing:&lt;br /&gt; 1. Want to run Claude like model of course&lt;br /&gt; 2. 3D modeling from very high resolution images, interacting with 3D models. Images are diverse - nanoscale samples to satellite imageries.&lt;/p&gt; &lt;p&gt;Max that I can go is probably 1/2k extra, not more. Please don't ask me to work on cloud! Lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordSteinggard"&gt; /u/LordSteinggard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oko2ar/want_to_run_claude_like_model_on_10k_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oko2ar/want_to_run_claude_like_model_on_10k_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oko2ar/want_to_run_claude_like_model_on_10k_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T07:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol1vdm</id>
    <title>Mergekit has been re-licensed under GNU LGPL v3</title>
    <updated>2025-10-31T18:08:57+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kinda self-promo ? But also feel it's worth shouting out anyways, mergekit is back to LGPL license!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/arcee-ai/mergekit"&gt;https://github.com/arcee-ai/mergekit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.arcee.ai/blog/mergekit-returns-to-its-roots"&gt;https://www.arcee.ai/blog/mergekit-returns-to-its-roots&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok3xie</id>
    <title>200+ pages of Hugging Face secrets on how to train an LLM</title>
    <updated>2025-10-30T16:11:22+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt; &lt;img alt="200+ pages of Hugging Face secrets on how to train an LLM" src="https://preview.redd.it/s12qz4k3w9yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c78fbb2faf8b6857633466eb7cf34609898a57" title="200+ pages of Hugging Face secrets on how to train an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's elie from the hugging face pre-training team! We're very excited to share our new blog (book?) that cover the full pipeline: pre-training, post-training and infra. 200+ pages of what worked, what didn‚Äôt, and how to make it run reliably :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook"&gt;https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope yall will enjoy it, don't hesitate to make feedback on the community tab :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s12qz4k3w9yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T16:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oku9og</id>
    <title>Why the hype around ultra small models like Granite4_350m? What are the actual use cases for these models?</title>
    <updated>2025-10-31T13:14:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get that small models can run on edge devices, but what are people actually planning on using a 350m parameter model for in the real world? I‚Äôm just really curious as to what use cases developers see these fitting into vs. using 1b, 4b, or 8b? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T13:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol006s</id>
    <title>Drummer's Rivermind‚Ñ¢ 24B v1 - A spooky future for LLMs, Happy Halloween!</title>
    <updated>2025-10-31T16:58:28+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt; &lt;img alt="Drummer's Rivermind‚Ñ¢ 24B v1 - A spooky future for LLMs, Happy Halloween!" src="https://external-preview.redd.it/cJYTAXorc18tRHqzBZaT1gjlzMY9WdV4LW05HQehJqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fbf968d35d15b5cb941693631cde3f0e872cd42" title="Drummer's Rivermind‚Ñ¢ 24B v1 - A spooky future for LLMs, Happy Halloween!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The older brother of &lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1"&gt;https://huggingface.co/TheDrummer/Rivermind-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-24B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:58:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oky3un</id>
    <title>Qwen3-VL GGUF!</title>
    <updated>2025-10-31T15:46:07+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have not tried any yet, multiple other Veterans have uploaded GGUF Quants, linking to unsloth for their guide and all available models from 2B-32B.&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth"&gt;Hugging Face Unsloth&lt;/a&gt;&lt;br /&gt; &lt;a href="https://docs.unsloth.ai/models/qwen3-vl-run-and-fine-tune"&gt;Unsloth Guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1okppxs</id>
    <title>Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?</title>
    <updated>2025-10-31T09:13:34+00:00</updated>
    <author>
      <name>/u/Successful-Newt1517</name>
      <uri>https://old.reddit.com/user/Successful-Newt1517</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt; &lt;img alt="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" src="https://preview.redd.it/ej8yokr9zeyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd9469bd25e003a0703a4ea86d079690b75d94f" title="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, what's going on? Are Chinese models saving American startups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Newt1517"&gt; /u/Successful-Newt1517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ej8yokr9zeyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T09:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz8qz</id>
    <title>pewdiepie dropped a video about running local ai</title>
    <updated>2025-10-31T16:28:56+00:00</updated>
    <author>
      <name>/u/topfpflanze187</name>
      <uri>https://old.reddit.com/user/topfpflanze187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt; &lt;img alt="pewdiepie dropped a video about running local ai" src="https://external-preview.redd.it/WddxiFHLc3dMB9LBPGHmNWXXrzglB78uxpSOk1Y4d6E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d41e205151bfdcec37d1be377abc09d05a02773e" title="pewdiepie dropped a video about running local ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topfpflanze187"&gt; /u/topfpflanze187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qw4fDU18RcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0i7q</id>
    <title>AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo</title>
    <updated>2025-10-30T14:00:16+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" src="https://b.thumbs.redditmedia.com/hb9XoRhxPRhv8ljYkjnVbJWgnClXeLGMxbG1TEDCwos.jpg" title="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We‚Äôre super excited to host this week‚Äôs AMA! &lt;/p&gt; &lt;p&gt;Join us and ask your questions directly to the human minds behind all things Liquid: Liquid Foundational Models, the Liquid Edge AI Platform (LEAP) for model customization and deployment, and Apollo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our participants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks &lt;a href="https://www.reddit.com/user/jamarks13/"&gt;u/jamarks13&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith &lt;a href="https://www.reddit.com/user/jimmysmith1919/"&gt;u/jimmysmith1919&lt;/a&gt; (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne &lt;a href="https://www.reddit.com/user/mlabonne/"&gt;u/mlabonne&lt;/a&gt; (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes &lt;a href="https://www.reddit.com/user/Wide-Half-7982/"&gt;u/Wide-Half-7982&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak &lt;a href="https://www.reddit.com/user/ankebananke/"&gt;u/ankebananke&lt;/a&gt; (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur B√∂√∂k &lt;a href="https://www.reddit.com/user/ManWithARedFace/"&gt;u/ManWithARedFace&lt;/a&gt; (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev &lt;a href="https://www.reddit.com/user/ykhrustalev/"&gt;u/ykhrustalev&lt;/a&gt; (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena &lt;a href="https://www.reddit.com/user/humble_pi_314/"&gt;u/humble_pi_314&lt;/a&gt; (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca &lt;a href="https://www.reddit.com/user/Ok-Safe-5316/"&gt;u/Ok-Safe-5316&lt;/a&gt; (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale &lt;a href="https://www.reddit.com/user/anthony-liquidai/"&gt;u/anthony-liquidai&lt;/a&gt; (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo &lt;a href="https://www.reddit.com/user/PauLabartaBajo/"&gt;u/PauLabartaBajo&lt;/a&gt; (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from &lt;strong&gt;10 AM - 1 PM PST&lt;/strong&gt;. The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&amp;gt; &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198"&gt;https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks to everyone who participated in this AMA. It was a pleasure.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://discord.gg/DFU3WQeaYD"&gt;Join the Liquid AI Discord Community&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:00:16+00:00</published>
  </entry>
</feed>
