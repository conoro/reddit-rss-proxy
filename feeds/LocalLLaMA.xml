<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-02T22:05:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1n6ubtx</id>
    <title>Voice cloning</title>
    <updated>2025-09-02T20:17:31+00:00</updated>
    <author>
      <name>/u/master_of_obvious</name>
      <uri>https://old.reddit.com/user/master_of_obvious</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been using alltalk for a while and I'm not up to speed on any of the newer models out there. I have an AMD GPU which is not supported by all talk. Are there any alternatives that work well with AMD gpus? I would be open to subscribing to something if voice cloning works well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/master_of_obvious"&gt; /u/master_of_obvious &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ubtx/voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ubtx/voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ubtx/voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T20:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1n5zed0</id>
    <title>I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large</title>
    <updated>2025-09-01T20:13:23+00:00</updated>
    <author>
      <name>/u/Altruistic-Tea-5612</name>
      <uri>https://old.reddit.com/user/Altruistic-Tea-5612</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"&gt; &lt;img alt="I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large" src="https://external-preview.redd.it/ywyWexAkeoEnV3YXP8YcOUkQeZuDP2-5umUBtdqKkZ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ebad8a84758c0785d9bbd8dcbf0a28e687394e9e" title="I pretrained and postrained a LLM with less than $50 budget which outperforms Google BERT large" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks from LocalLLama sub! I am really thankful for amazing people in this sub for sharing useful things which helped me to learn lots of things about pretraing , post training and evaluation etc for your context I don't have professional ML background!&lt;/p&gt; &lt;p&gt;Today I am super excited to share that I pretrained and post trained 150M parameter model from scratch which outperforms Google BERT model and I also built embedding model which works on par with Jina-embedings-v2-base model in MTEB benchmarks &lt;/p&gt; &lt;p&gt;In this article I shared how I did this model along with links to weights of model&lt;br /&gt; thanks again&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Altruistic-Tea-5612"&gt; /u/Altruistic-Tea-5612 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://medium.com/@harishhacker3010/pretraining-a-llm-with-less-than-50-budget-which-outperforms-google-bert-dbe541b7b14b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n5zed0/i_pretrained_and_postrained_a_llm_with_less_than/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-01T20:13:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6h3rk</id>
    <title>rStar2-Agent</title>
    <updated>2025-09-02T11:37:39+00:00</updated>
    <author>
      <name>/u/thatusernsmeis</name>
      <uri>https://old.reddit.com/user/thatusernsmeis</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This MS model was realeased some days ago and I haven‚Äôt see any posts talking about it on here but from the benchmarks looks promising for a 14B model. &lt;/p&gt; &lt;p&gt;Has anyone tried it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thatusernsmeis"&gt; /u/thatusernsmeis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.arxiv.org/pdf/2508.20722"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6h3rk/rstar2agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6h3rk/rstar2agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T11:37:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ol4j</id>
    <title>Introducing Environments Hub: a fully open source RL stack to compete with Big Tech</title>
    <updated>2025-09-02T16:42:48+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ol4j/introducing_environments_hub_a_fully_open_source/"&gt; &lt;img alt="Introducing Environments Hub: a fully open source RL stack to compete with Big Tech" src="https://external-preview.redd.it/9_Fvq6ehID_iSoTHhpFmOGScEf94F8f6zO7oymBq6Qw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1b9850311d3200a89b31cfb0229239ad5e236894" title="Introducing Environments Hub: a fully open source RL stack to compete with Big Tech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think a lot of people slept on this announcement, but it's critical for the future of open-source AI. This is our chance to build the infrastructure to truly compete with the big labs, and everyone can play a part.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.primeintellect.ai/blog/environments"&gt; Prime Intellect Environments Hub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;TL;DR&lt;/p&gt; &lt;p&gt;They announced the launch of a platform called Environments Hub. This is an open, community-driven hub designed to address the current issues of fragmented, closed, and difficult-to-share reinforcement learning environments.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;: Currently, many high-quality reinforcement learning environments are exclusively sold by startups to a handful of large labs, leaving open-source models lagging behind.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Platform features&lt;/strong&gt;: Users can create, share, and manage reinforcement learning environments in this hub, conduct model evaluations, and directly use them with their open-source prime-rl training framework. The platform also offers sandbox functionality for secure code execution.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Goal&lt;/strong&gt;: To create a robust open-source ecosystem of environments that powers the development of open-source AI, enabling it to compete with large closed labs. They aim to provide a full-stack reinforcement learning infrastructure, spanning from computation and inference to training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vision&lt;/strong&gt;: They believe reinforcement learning is not only a path to AGI but also the foundation for building future AI-native products. By lowering the barriers to training and deploying models, they hope to empower more researchers and startups, fostering a truly open and sovereign AI ecosystem.&lt;/p&gt; &lt;p&gt;This aligns closely with Andrej Karpathy‚Äôs view earlier this year:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/lokd4zdc5smf1.png?width=1194&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8a7049efc480d3c2f566ccf3529b7ab0ce60649"&gt;Karpathy's View&lt;/a&gt;&lt;/p&gt; &lt;p&gt;edit: Someone reminded me that Karpathy also responded to this announcement&lt;br /&gt; &lt;a href="https://x.com/karpathy/status/1960803117689397543"&gt;https://x.com/karpathy/status/1960803117689397543&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What do you think? In this new &amp;quot;era of environments,&amp;quot; would you bet on reinforcement learning, or on something else entirely?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ol4j/introducing_environments_hub_a_fully_open_source/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ol4j/introducing_environments_hub_a_fully_open_source/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ol4j/introducing_environments_hub_a_fully_open_source/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:42:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n68afq</id>
    <title>Why are all AI "Success" posts terrible?</title>
    <updated>2025-09-02T02:49:54+00:00</updated>
    <author>
      <name>/u/Bimbam_tm</name>
      <uri>https://old.reddit.com/user/Bimbam_tm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;Wow look at this!&amp;quot; someone cries, and includes a screenshot/gif from a single-line AI prompt magically producing a working product.&lt;/p&gt; &lt;p&gt;Great, and completely unsurprising given that one-line prompts work exactly like horoscopes - so vague they can't help but satisfy whatever slop gets generated. But whatever, as long as it looks gifable right?&lt;/p&gt; &lt;p&gt;&amp;quot;Build me a todo app that looks nice!&amp;quot;&lt;br /&gt; Congratulations, you just wrote the AI equivalent of &amp;quot;you will face challenges this week.&amp;quot; The AI spits out literally anything 'todo adjacent' and you're amazed because technically it worked. Just like horoscopes, it's response is written so broadly that the reader finds it somehow fits their expectations..&lt;/p&gt; &lt;p&gt;A real horoscope would say &amp;quot;On Tuesday at 3:47 PM, you will receive a text from someone whose name starts with J about a blue object.&amp;quot;&lt;/p&gt; &lt;p&gt;With that in mind, how about someone show me a real workflow:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Your original concept art/design docs/sketches&lt;/li&gt; &lt;li&gt;How close you actually got to achieve your original concept/idea&lt;/li&gt; &lt;li&gt;How many iterations it took&lt;/li&gt; &lt;li&gt;What didn't work&lt;/li&gt; &lt;li&gt;The actual prompts you used (all of them)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unless that AI output was almost EXACTLY what you had in mind from prompt #1 (which seems highly unlikely), all your &amp;quot;amazing&amp;quot; result proves was your prompt was horoscope-level vague, and you're apparently ok with mediocrity .&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bimbam_tm"&gt; /u/Bimbam_tm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n68afq/why_are_all_ai_success_posts_terrible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T02:49:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ncef</id>
    <title>Best open-source + fast models (OCR / VLM) for reading diagrams, graphs, charts in documents?</title>
    <updated>2025-09-02T15:56:54+00:00</updated>
    <author>
      <name>/u/Particular_Cake4359</name>
      <uri>https://old.reddit.com/user/Particular_Cake4359</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ncef/best_opensource_fast_models_ocr_vlm_for_reading/"&gt; &lt;img alt="Best open-source + fast models (OCR / VLM) for reading diagrams, graphs, charts in documents?" src="https://preview.redd.it/cituamodxrmf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acb2dec320a92d7571df392f3617d229d330429c" title="Best open-source + fast models (OCR / VLM) for reading diagrams, graphs, charts in documents?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I‚Äôm looking for &lt;strong&gt;open-source models&lt;/strong&gt; that are both &lt;strong&gt;fast and accurate&lt;/strong&gt; for reading content like &lt;strong&gt;diagrams, graphs, and charts&lt;/strong&gt; inside documents (PDF, PNG, JPG, etc.).&lt;/p&gt; &lt;p&gt;I tried &lt;strong&gt;Qwen2.5-VL-7B-Instruct&lt;/strong&gt; on a figure with 3 subplots, but the result was too generic and missed important details.&lt;/p&gt; &lt;p&gt;So my question is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What open-source OCR or vision-language models work best for this?&lt;/li&gt; &lt;li&gt;Any that are &lt;strong&gt;lightweight / fast&lt;/strong&gt; enough to run on modest hardware (CPU or small GPU)?&lt;/li&gt; &lt;li&gt;Bonus if you know benchmarks or comparisons for this task.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Particular_Cake4359"&gt; /u/Particular_Cake4359 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cituamodxrmf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ncef/best_opensource_fast_models_ocr_vlm_for_reading/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ncef/best_opensource_fast_models_ocr_vlm_for_reading/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:56:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6fta4</id>
    <title>ETH Zurich Open LLM "Apertus" has been released</title>
    <updated>2025-09-02T10:25:45+00:00</updated>
    <author>
      <name>/u/kisamoto</name>
      <uri>https://old.reddit.com/user/kisamoto</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"&gt; &lt;img alt="ETH Zurich Open LLM &amp;quot;Apertus&amp;quot; has been released" src="https://external-preview.redd.it/3xCYbgdmDkf0KukpAo-RYTjRTShJKNSz9uOuaVJW_jI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5da84c50675f066ea42c6cb75049480ff32b8ed5" title="ETH Zurich Open LLM &amp;quot;Apertus&amp;quot; has been released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kisamoto"&gt; /u/kisamoto &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/swiss-ai/apertus-llm-68b699e65415c231ace3b059"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6fta4/eth_zurich_open_llm_apertus_has_been_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T10:25:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ojwi</id>
    <title>Jupyter Agent Dataset</title>
    <updated>2025-09-02T16:41:35+00:00</updated>
    <author>
      <name>/u/lvwerra</name>
      <uri>https://old.reddit.com/user/lvwerra</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"&gt; &lt;img alt="Jupyter Agent Dataset" src="https://external-preview.redd.it/AILThe6Dj0cxSzLU5c4WZjfK6cm5Jz57hJf5d-2R9PU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8d89f36fc6f3a8b9463d808eb0c39c867916796b" title="Jupyter Agent Dataset" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/jx5v4hen4smf1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f33c813c304597d66fdad442a28987f129fb08c8"&gt;https://preview.redd.it/jx5v4hen4smf1.png?width=3200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f33c813c304597d66fdad442a28987f129fb08c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi there, Leandro here from the research team at Hugging Face. We built the Jupyter Agent Dataset, a high quality data analysis and code execution dataset built with 7TB of Kaggle datasets and 20k+ notebooks using Qwen3-Coder.&lt;/p&gt; &lt;p&gt;The traces were generated using a combination of real notebook filtering, synthetic QA annotation, and then solving those questions with Qwen3-Coder by using the real data and code execution.&lt;/p&gt; &lt;p&gt;Training on the dataset shows significant improvement on data exploration and analysis tasks! Ask me anything about it!&lt;/p&gt; &lt;p&gt;Dataset: &lt;a href="https://huggingface.co/datasets/data-agents/jupyter-agent-dataset"&gt;https://huggingface.co/datasets/data-agents/jupyter-agent-dataset&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lvwerra"&gt; /u/lvwerra &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ojwi/jupyter_agent_dataset/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:41:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6w7r9</id>
    <title>B850 AI Top motherboard</title>
    <updated>2025-09-02T21:29:40+00:00</updated>
    <author>
      <name>/u/sleepy_roger</name>
      <uri>https://old.reddit.com/user/sleepy_roger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just wanted to pass the existence of this board along. Since I've mentioned it to a few people over the past few days and they also hadn't seen it or heard of it.&lt;/p&gt; &lt;p&gt;I came across it accidentally when looking for a good bifurcation board to support 2 new cards. I was just looking through the list of all 870 boards that support bifurcation. I figured the &amp;quot;AI&amp;quot; name was some gimmick, but it's definitely not. I almost just grabbed another Carbon, but glad I didn't. The board is priced pretty close to the same as X870e counterparts, but it's also incredibly premium for a B. I've personally never come across a B board with so many features. The X870e version is of course even more premium, but over double the price.&lt;/p&gt; &lt;p&gt;Anyway, the board has pretty great specs in general. Along with the 2x8 5.0 PCIe and really good spacing for large cards, it has 2x10g Ethernet ports, an 8-layer PCB, a ton of USB ports, etc. Great heatsinks as well, which make the board surprisingly heavy.&lt;/p&gt; &lt;p&gt;I'm using it with a Proxmox setup, so not using any of their &amp;quot;AI software,&amp;quot; however the board features in general are really nice.&lt;/p&gt; &lt;p&gt;Honestly it's just kind of cool to see a company making stuff for hobbyist AI enthusiasts.&lt;/p&gt; &lt;p&gt;link - &lt;a href="https://www.gigabyte.com/Motherboard/B850-AI-TOP"&gt;https://www.gigabyte.com/Motherboard/B850-AI-TOP&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sleepy_roger"&gt; /u/sleepy_roger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6w7r9/b850_ai_top_motherboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:29:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6nkki</id>
    <title>Anyone here using Qwen3-235b-a22b-thinking-2507 as their daily driver???</title>
    <updated>2025-09-02T16:05:10+00:00</updated>
    <author>
      <name>/u/True_Requirement_891</name>
      <uri>https://old.reddit.com/user/True_Requirement_891</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I fucking love this model!!! It performs better than deepseek for me in general use for nearly everything!!! Easily the BEST Open Weight model we have that rivals closed models!&lt;/p&gt; &lt;p&gt;It just feels fucking intelligent to talk to lmao passes my vibe check and it's fast.&lt;/p&gt; &lt;p&gt;What is the experience of you guys with this model in more general use cases???&lt;/p&gt; &lt;p&gt;Also, I really wanna see a scaled up general version of this model like: Qwen3-480B-35B-Thinking &lt;/p&gt; &lt;p&gt;The coder variant sucks for anything but producing code and maybe tool calling.&lt;/p&gt; &lt;p&gt;Sure it's gonna be difficult to run locally for most of the community but being able to access this amazing model from multiple cloud providers for dirt cheap prices is amazing for me! Not being locked at the mercy of closed labs.&lt;/p&gt; &lt;p&gt;You don't have to worry about model changing behind the scenes! You get &amp;quot;near&amp;quot; full control of the model.&lt;/p&gt; &lt;p&gt;Ofcourse there are issues like cloud providers using smaller quants behind the scenes but still worth it from more legit providers.&lt;/p&gt; &lt;p&gt;Qwen3-235b-a22b-thinking-2507 doesn't even feel benchmaxxed or at least not from my experience. The pre-update version was garbage but after the update, it became my favourite one so far!!!&lt;/p&gt; &lt;p&gt;Some more thoughts:&lt;/p&gt; &lt;p&gt;The new DeepSeek-V3.1 sucks ass man like madly inconsistent and just doesn't have the feel... It disappointed me big time. I saw people praising it but honestly, I just don't get it.&lt;/p&gt; &lt;p&gt;R1-0528 was a significant upgrade in terms of intelligence even with a lack of that &amp;quot;vibe&amp;quot;.&lt;/p&gt; &lt;p&gt;V3-0324 was just üíã&lt;/p&gt; &lt;p&gt;But this new V3.1 feels like the worst of both worlds. I tried it a lot and I just can't trust it. It's very inconsistent in performance/accuracy. It also loses context fast. Misunderstands stuff way more than other models... An absolute failure in my experience. Maybe it's because of the hybrid thinking system that qwen left behind???&lt;/p&gt; &lt;p&gt;I just don't get how are you guys able to use V3.1 without letting out a sigh every prompt?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/True_Requirement_891"&gt; /u/True_Requirement_891 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6nkki/anyone_here_using_qwen3235ba22bthinking2507_as/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6nkki/anyone_here_using_qwen3235ba22bthinking2507_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6nkki/anyone_here_using_qwen3235ba22bthinking2507_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:05:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6lu9t</id>
    <title>every LLM metric you need to know (v2.0)</title>
    <updated>2025-09-02T15:00:08+00:00</updated>
    <author>
      <name>/u/FlimsyProperty8544</name>
      <uri>https://old.reddit.com/user/FlimsyProperty8544</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Since I made &lt;a href="https://www.reddit.com/r/LLMDevs/comments/1j6pxv9/every_llm_metric_you_need_to_know/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;this post&lt;/a&gt; a few months ago, the AI and evals space has shifted significantly. Better LLMs mean that standard out-of-the-box metrics aren‚Äôt as useful as they once were, and &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;custom metrics&lt;/a&gt; are becoming more important. Increasingly agentic and complex use cases are driving the need for &lt;a href="https://deepeval.com/docs/metrics-task-completion"&gt;agentic metrics&lt;/a&gt;. And the lack of ground truth‚Äîespecially for smaller startups‚Äîputs more emphasis on referenceless metrics, especially around tool-calling and agents.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;A Note about Statistical Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;It‚Äôs become clear that statistical scores like BERT and ROUGE are fast, cheap, and deterministic, but much less effective than &lt;a href="https://deepeval.com/docs/metrics-introduction"&gt;LLM judges&lt;/a&gt; (especially SOTA models) if you care about capturing nuanced contexts and evaluation accuracy, so I‚Äôll only be talking about LLM judges in this list.&lt;/p&gt; &lt;p&gt;That said, here‚Äôs the updated, more comprehensive list of every LLM metric you need to know, version 2.0.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom Metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Every LLM use-case is unique and requires &lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;custom metrics&lt;/a&gt; for automated testing. In fact they are the most important metrics when it comes to building your eval pipeline. Common use-cases of custom metrics include defining custom criterias for ‚Äúcorrectness‚Äù, and tonality/style-based metrics like ‚Äúoutput professionalism‚Äù.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-llm-evals"&gt;G-Eval:&lt;/a&gt; a framework that uses LLMs with chain-of-thoughts (CoT) to evaluate LLM outputs based on any custom criteria.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-dag"&gt;DAG (Directed Acyclic Graphs):&lt;/a&gt; a framework to help you build decision tree metrics using LLM judges at each node to determine branching path, and useful for specialized use-cases, like aligning document genreatino with your format. &lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-arena-g-eval"&gt;Arena G-Eval&lt;/a&gt;: a framework that uses LLMs with chain-of-thoughts (CoT) to pick the best LLM output from a group of contestants based on any custom criteria, which is useful for picking the best models, prompts for your use-case/&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-conversational-g-eval"&gt;Conversational G-Eval&lt;/a&gt;: The equivalent G-Eval, but for evaluating entire conversations instead of single-turn interactions.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/multimodal-metrics-g-eval"&gt;Multimodal G-Eval&lt;/a&gt;: G-Eval that extends to other modalities such as image.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Agentic Metrics:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Almost every use case today is agentic. But evaluating agents is hard ‚Äî the sheer number of possible decision-tree rabbit holes makes analysis complex. Having a ground truth for every tool call is essentially impossible. That‚Äôs why the following &lt;a href="https://deepeval.com/docs/metrics-task-completion"&gt;agentic metrics&lt;/a&gt; are especially useful.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-task-completion"&gt;Task Completion:&lt;/a&gt; evaluates if an LLM agent accomplishes a task by analyzing the entire traced execution flow. This metric is easy to set up because it requires NO ground truth, and is arguably the most useful metric for detecting failed any agentic executions, like browser-based tasks, for example.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-argument-correctness"&gt;Argument Correctness&lt;/a&gt;: evaluates if an LLM generates the correct inputs to a tool calling argument, which is especially useful for evaluating tool calls when you don‚Äôt have access to expected tools and ground truth.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-tool-correctness"&gt;Tool Correctness:&lt;/a&gt; assesses your LLM agent's function/tool calling ability. It is calculated by comparing whether every tool that is expected to be used was indeed called. It does require a ground truth.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-mcp-use"&gt;MCP-Use&lt;/a&gt;: The MCP Use is a metric that is used to evaluate how effectively an MCP based LLM agent makes use of the mcp servers it has access to.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-mcp-task-completion"&gt;MCP Task Completion&lt;/a&gt;: The MCP task completion metric is a conversational metric that uses LLM-as-a-judge to evaluate how effectively an MCP based LLM agent accomplishes a task.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-multi-turn-mcp-use"&gt;Multi-turn MCP-Use&lt;/a&gt;: The Multi-Turn MCP Use metric is a conversational metric that uses LLM-as-a-judge to evaluate how effectively an MCP based LLM agent makes use of the mcp servers it has access to.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;RAG Metrics&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;While AI agents are gaining momentum, most LLM apps in production today still rely on RAG. These metrics remain crucial as long as RAG is needed ‚Äî which will be the case as long as there‚Äôs a cost tradeoff with model context length.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-answer-relevancy"&gt;Answer Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating how relevant the actual output of your LLM application is compared to the provided input&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-faithfulness"&gt;Faithfulness:&lt;/a&gt; measures the quality of your RAG pipeline's generator by evaluating whether the actual output factually aligns with the contents of your retrieval context&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-contextual-precision"&gt;Contextual Precision:&lt;/a&gt; measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval context that are relevant to the given input are ranked higher than irrelevant ones.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-contextual-recall"&gt;Contextual Recall:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the extent of which the retrieval context aligns with the expected output&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-contextual-relevancy"&gt;Contextual Relevancy:&lt;/a&gt; measures the quality of your RAG pipeline's retriever by evaluating the overall relevance of the information presented in your retrieval context for a given input&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Conversational metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;50% of the agentic use-cases I encounter are conversational. Both agentic and conversational metrics go hand-in-hand. Conversational evals are different from single-turn evals because chatbots must remain consistent and context-aware across entire conversations, not just accurate in single-ouptuts. Here are the most useful conversational metrics.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-turn-relevancy"&gt;Turn Relevancy:&lt;/a&gt; determines whether your LLM chatbot is able to consistently generate relevant responses throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-role-adherence"&gt;Role Adherence:&lt;/a&gt; determines whether your LLM chatbot is able to adhere to its given role throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-knowledge-retention"&gt;Knowledge Retention:&lt;/a&gt; determines whether your LLM chatbot is able to retain factual information presented throughout a conversation.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-conversation-completeness"&gt;Conversational Completeness:&lt;/a&gt; determines whether your LLM chatbot is able to complete an end-to-end conversation by satisfying user needs throughout a conversation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Safety Metrics&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Better LLMs don‚Äôt mean your app is safe from malicious users. In fact, the more agentic your system becomes, the more sensitive data it can access ‚Äî and stronger LLMs only amplify what can go wrong.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-bias"&gt;Bias&lt;/a&gt;: determines whether your LLM output contains gender, racial, or political bias.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-toxicity"&gt;Toxicity&lt;/a&gt;: evaluates toxicity in your LLM outputs.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-hallucination"&gt;Hallucination&lt;/a&gt;: determines whether your LLM generates factually correct information by comparing the output to the provided context&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-non-advice"&gt;Non-Advice:&lt;/a&gt; determines whether your LLM output contains inappropriate professional advice that should be avoided.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-misuse"&gt;Misuse&lt;/a&gt;: determines whether your LLM output contains inappropriate usage of a specialized domain chatbot.&lt;/li&gt; &lt;li&gt;&lt;a href="https://deepeval.com/docs/metrics-pii-leakage"&gt;PII Leakage&lt;/a&gt;: determines whether your LLM output contains personally identifiable information (PII) or privacy-sensitive data that should be protected. &lt;/li&gt; &lt;li&gt;Role Violation&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;These metrics are a great starting point for setting up your eval pipeline, but there are many ways to apply them. Should you run evaluations in &lt;a href="https://www.confident-ai.com/docs/llm-evaluation/single-turn/end-to-end"&gt;development&lt;/a&gt; or &lt;a href="https://www.confident-ai.com/docs/llm-tracing/evaluations"&gt;production&lt;/a&gt;? Should you test your app&lt;a href="https://deepeval.com/docs/evaluation-end-to-end-llm-evals"&gt; end-to-end&lt;/a&gt; or evaluate &lt;a href="https://deepeval.com/docs/evaluation-component-level-llm-evals"&gt;components separately&lt;/a&gt;? These kinds of questions are important to ask‚Äîand the right answer ultimately depends on your specific use case.&lt;/p&gt; &lt;p&gt;I‚Äôll probably write more about this in another post, but the&lt;a href="https://deepeval.com/docs/evaluation-component-level-llm-evals"&gt; DeepEval docs&lt;/a&gt; are a great place to dive deeper into these metrics, understand how to use them, and explore their broader implications.&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/confident-ai/deepeval"&gt;Github Repo&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FlimsyProperty8544"&gt; /u/FlimsyProperty8544 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6lu9t/every_llm_metric_you_need_to_know_v20/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6lu9t/every_llm_metric_you_need_to_know_v20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6lu9t/every_llm_metric_you_need_to_know_v20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:00:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6ewmu</id>
    <title>Apertus: a fully open, transparent, multilingual language model</title>
    <updated>2025-09-02T09:29:24+00:00</updated>
    <author>
      <name>/u/Stock-Variation-2237</name>
      <uri>https://old.reddit.com/user/Stock-Variation-2237</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"&gt; &lt;img alt="Apertus: a fully open, transparent, multilingual language model" src="https://external-preview.redd.it/bTft7kJCnEeOJxWtFZSRa954qWiZB1xs4iZXNvShGsI.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=afccde1fc69d131b9cf94781d34b916dcaf90c23" title="Apertus: a fully open, transparent, multilingual language model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Stock-Variation-2237"&gt; /u/Stock-Variation-2237 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://actu.epfl.ch/news/apertus-a-fully-open-transparent-multilingual-lang/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6ewmu/apertus_a_fully_open_transparent_multilingual/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:29:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6vzfe</id>
    <title>WEBGEN-4B: Quality Web Design Generation</title>
    <updated>2025-09-02T21:20:22+00:00</updated>
    <author>
      <name>/u/smirkishere</name>
      <uri>https://old.reddit.com/user/smirkishere</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt; &lt;img alt="WEBGEN-4B: Quality Web Design Generation" src="https://b.thumbs.redditmedia.com/hz9KVQoDGH5SuRnQfWtoik62ndFYILrISKyncU-X0Cc.jpg" title="WEBGEN-4B: Quality Web Design Generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Tesslate/WEBGEN-4B is a 4B model that produces quality tailwind websites. We trained it on 100k samples with synthetic data exclusively generated from GPT-OSS. WEBGEN is fast, controllable, and can drop right into your agentic workflows.&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/Tesslate/WEBGEN-4B-Preview"&gt;https://huggingface.co/Tesslate/WEBGEN-4B-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF"&gt;https://huggingface.co/gabriellarson/WEBGEN-4B-Preview-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Over the course of this week and next week, we will be dropping a few more models or open sourced software based on the innovations we've made in this space!&lt;/p&gt; &lt;p&gt;Please reach out for API keys to test it out if needed. On the model card and below in the comments will be our designer platform (which we will open source soon) where you can use the model for free. &lt;/p&gt; &lt;p&gt;In other news, we are open sourcing our UIGEN-T2 Dataset at Tesslate/UIGEN-T2&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/smirkishere"&gt; /u/smirkishere &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6vzfe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6vzfe/webgen4b_quality_web_design_generation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T21:20:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6kklk</id>
    <title>Git commands as LLM memory - 15k tokens down to 5k</title>
    <updated>2025-09-02T14:11:37+00:00</updated>
    <author>
      <name>/u/Apart-Employment-592</name>
      <uri>https://old.reddit.com/user/Apart-Employment-592</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I have been experimenting with giving LLMs access to granular git history instead of dumping entire codebases into context.&lt;/p&gt; &lt;p&gt;The approach: auto-commit code changes every 15 seconds to a shadow repo, then let the AI query it with git commands. So instead of feeding 5,000 lines of context, the model runs:&lt;/p&gt; &lt;p&gt;- `git diff HEAD~10` (50 tokens)&lt;/p&gt; &lt;p&gt;- `git log -S &amp;quot;function&amp;quot;` to find when something was implemented&lt;/p&gt; &lt;p&gt;- `git blame` to understand evolution&lt;/p&gt; &lt;p&gt;Tested this with Claude on a debugging session:&lt;/p&gt; &lt;p&gt;- Without git history: 15,000 tokens, multiple attempts before fixing a bug&lt;/p&gt; &lt;p&gt;- With git access: 5,000 tokens, found the bug fix almost immediately&lt;/p&gt; &lt;p&gt;The interesting part is that LLMs already understand git commands perfectly. They know exactly what to query without special training.&lt;/p&gt; &lt;p&gt;Technical approach I'm trying:&lt;/p&gt; &lt;p&gt;- detect file changes using `git status` every 15 seconds&lt;/p&gt; &lt;p&gt;- Commits to separate .shadowgit/ repo (not main)&lt;/p&gt; &lt;p&gt;- MCP server exposes read-only git operations&lt;/p&gt; &lt;p&gt;- Everything local&lt;/p&gt; &lt;p&gt;Questions for the community:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Anyone else exploring git as LLM memory? What's your approach?&lt;/li&gt; &lt;li&gt;For local models with small context windows, would this help?&lt;/li&gt; &lt;li&gt;Downsides I'm seeing: models might apply outdated patterns from history. Anyone experience this?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;P.S. For transparency: I packaged this into a tool (ShadowGit, $19) but the MCP server is open source: &lt;a href="https://github.com/blade47/shadowgit-mcp"&gt;https://github.com/blade47/shadowgit-mcp&lt;/a&gt; if you want to build your own solution. More interested in the community's feedback on this approach.&lt;/p&gt; &lt;p&gt;Thank you!&lt;/p&gt; &lt;p&gt;Alessandro&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Apart-Employment-592"&gt; /u/Apart-Employment-592 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6kklk/git_commands_as_llm_memory_15k_tokens_down_to_5k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T14:11:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6qzre</id>
    <title>I open-sourced 50+ Docker images to give your local LLMs easy access to tools like GitHub, Gmail, Slack, etc. No more dependency hell.</title>
    <updated>2025-09-02T18:12:10+00:00</updated>
    <author>
      <name>/u/IllChannel5235</name>
      <uri>https://old.reddit.com/user/IllChannel5235</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I've been experimenting with local LLMs and autonomous agents. A major pain point is giving these agents access to real-world tools. Setting up connections to services like GitHub, Jira, or Slack locally is a nightmare of dependency management, OAuth flows, and custom scripts.&lt;/p&gt; &lt;p&gt;To solve this, my team at Klavis AI has open-sourced &lt;strong&gt;pre-built Docker images for 50+ high-quality MCP (Model Context Protocol) servers.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;You can now spin up a server to give your local model access to an external tool with a single command. &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FKlavis-AI%2Fklavis"&gt;https://github.com/Klavis-AI/klavis&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For example, to run a GitHub MCP server locally:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# With our managed OAuth (free API key) docker run -p 5000:5000 \ -e KLAVIS_API_KEY=$KLAVIS_API_KEY \ ghcr.io/klavis-ai/github-mcp-server:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or bring your own GitHub token:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# With your own token docker run -p 5000:5000 \ -e AUTH_DATA='{&amp;quot;access_token&amp;quot;:&amp;quot;ghp_your_github_token&amp;quot;}' \ ghcr.io/klavis-ai/github-mcp-server:latest &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;No more fighting with Python environments or implementing OAuth. Just a clean, containerized MCP server your agent can talk to.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this is a big deal for LocalLLaMA:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Empower Your Agents:&lt;/strong&gt; Give your models the ability to read GitHub issues, check your Google Calendar, or search through Notion docs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight &amp;amp; Local:&lt;/strong&gt; The images are Alpine-based and run entirely on your machine, keeping everything local.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dead Simple:&lt;/strong&gt; No compiling, no dependency hell. Just docker run.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;50+ MCP servers Available:&lt;/strong&gt; We've containerized servers for GitHub, Gmail, Slack, Notion, Jira, Linear, Salesforce, and many more.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Bigger Picture: Solving Agent Limitations&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;We all know agents struggle with tool selection, context window limits, and understanding human context. We're building a solution to these fundamental problems, allowing agents to use hundreds of tools without overwhelming the context window. These open-source servers are the first step.&lt;/p&gt; &lt;p&gt;If you're interested in the future of capable AI agents, check out our waitlist. &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.klavis.ai%2Fwaitlist"&gt;https://www.klavis.ai/waitlist&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub Repo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fgithub.com%2FKlavis-AI%2Fklavis"&gt;https://github.com/Klavis-AI/klavis&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demo:&lt;/strong&gt; &lt;a href="https://www.google.com/url?sa=E&amp;amp;q=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DNITgggPT3pA"&gt;https://www.youtube.com/watch?v=NITgggPT3pA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would love to hear your feedback and see what you build with this!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IllChannel5235"&gt; /u/IllChannel5235 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qzre/i_opensourced_50_docker_images_to_give_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:12:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6f5xl</id>
    <title>I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more.</title>
    <updated>2025-09-02T09:46:05+00:00</updated>
    <author>
      <name>/u/hedonihilistic</name>
      <uri>https://old.reddit.com/user/hedonihilistic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt; &lt;img alt="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." src="https://b.thumbs.redditmedia.com/UxxcnGYx3ztd3aYOh5G5hmtgSX12gsIZtQoWGJsTZJs.jpg" title="I just released a big update for my AI research agent, MAESTRO, with a new docs site showing example reports from Qwen 72B, GPT-OSS 120B, and more." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working hard on a big update for my open-source project, MAESTRO, and I'm excited to share v0.1.5-alpha with you all. MAESTRO is an autonomous research agent that turns any question into a fully-cited report.&lt;/p&gt; &lt;p&gt;A huge focus of this release was improving performance and compatibility with local models. I've refined the core agent workflows and prompts to make sure it works well with most reasonably intelligent locally hosted models.&lt;/p&gt; &lt;p&gt;I also launched a completely new documentation site to help users setup and start using MAESTRO. The best part is the new &lt;a href="https://murtaza-nasir.github.io/maestro/example-reports/"&gt;Example Reports Section&lt;/a&gt; that shows many reports generated with Local LLMs.&lt;/p&gt; &lt;p&gt;I've done extensive testing and shared the resulting reports so you can see what it's capable of. There are examples from a bunch of self-hosted models, including:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Large Models:&lt;/strong&gt; Qwen 2.5 72B, GPT-OSS 120B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Medium Models:&lt;/strong&gt; Qwen 3 32B, Gemma 3 27B, GPT-OSS 20B&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a great way to see how different models handle complex topics and various writing styles before you commit to running them. I've also included performance notes on things like KV cache usage during these runs.&lt;/p&gt; &lt;p&gt;Under the hood, I improved some UI features and added parallel processing for more operations, so it‚Äôs a little faster and more responsive.&lt;/p&gt; &lt;p&gt;If you're interested in AI assisted research or just want to see what's possible with the latest open models, I'd love for you to check it out.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://github.com/murtaza-nasir/maestro"&gt;&lt;strong&gt;GitHub Release&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://murtaza-nasir.github.io/maestro"&gt;&lt;strong&gt;New Docs Site&lt;/strong&gt;&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful. Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedonihilistic"&gt; /u/hedonihilistic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6f5xl"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6f5xl/i_just_released_a_big_update_for_my_ai_research/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:46:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rnp6</id>
    <title>Showerthought: Modern AI safety training is anti-safety</title>
    <updated>2025-09-02T18:36:36+00:00</updated>
    <author>
      <name>/u/Deathcrow</name>
      <uri>https://old.reddit.com/user/Deathcrow</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Probably not a unique thought, but it needs to be said.&lt;/p&gt; &lt;p&gt;It seems to me, that modern AI alignment safety training (driven by very superficial concerns, like porn, politics, hacking, mean words), wherein AI is trained to either outright reject the human's requests, or worse, subtly steer/manipulate users away from these topics, is actually anti-safety (the doomsday kind).&lt;/p&gt; &lt;p&gt;Why do we want AI agents to become more capable at deceiving users and circumventing our wishes? In this cycle of unnatural selection, the &amp;quot;safest&amp;quot; AI model is one where the user is still happy to use it and trust its answers, even though it's heavily censored or misleading?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deathcrow"&gt; /u/Deathcrow &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rnp6/showerthought_modern_ai_safety_training_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6eimy</id>
    <title>New Open LLM from Switzerland "Apertus", 40%+ training data is non English</title>
    <updated>2025-09-02T09:03:53+00:00</updated>
    <author>
      <name>/u/EnnioEvo</name>
      <uri>https://old.reddit.com/user/EnnioEvo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html"&gt;https://ethz.ch/en/news-and-events/eth-news/news/2025/09/press-release-apertus-a-fully-open-transparent-multilingual-language-model.html&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnnioEvo"&gt; /u/EnnioEvo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6eimy/new_open_llm_from_switzerland_apertus_40_training/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:03:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6myps</id>
    <title>Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!</title>
    <updated>2025-09-02T15:42:25+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"&gt; &lt;img alt="Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!" src="https://a.thumbs.redditmedia.com/GtXCbhwiuG2DgCCJnnea_WK3V__o2zXoto7eNDiCWZ8.jpg" title="Artificial Analysis Intelligence Index now measures agentic capabilities, good news for Kimi K2 and GLM 4.5!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;From the source: Tool calling and agentic workflows are increasingly the norm for how language models are used by both developers and consumers. Adding Terminal-Bench and ùúè¬≤-Bench to our Intelligence Index reflects this trend and allows us to see where models have strengths for agentic use cases, compared to prior evaluations that are more focused on knowledge and reasoning.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Full methodology details &lt;a href="https://artificialanalysis.ai/methodology/intelligence-benchmarking"&gt;here&lt;/a&gt;. This should tip the scales a bit more in favor of Kimi K2 and GLM 4.5, which are post-trained for tool use. Current benchmarks are heavily weighted towards knowledge and mathematical/logical reasoning.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6myps"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6myps/artificial_analysis_intelligence_index_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:42:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6siz6</id>
    <title>NousResearch/Hermes-4-14B ¬∑ Hugging Face</title>
    <updated>2025-09-02T19:08:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt; &lt;img alt="NousResearch/Hermes-4-14B ¬∑ Hugging Face" src="https://external-preview.redd.it/3zW4BctOGBSQqyD1VYjxoOK5if51GWWepXF3S3IdZF0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b37e63854cc68df0d3bc6f558a76fe90da9ad013" title="NousResearch/Hermes-4-14B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes 4 14B is a frontier, hybrid-mode &lt;strong&gt;reasoning&lt;/strong&gt; model based on Qwen 3 14B by Nous Research that is aligned to &lt;strong&gt;you&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Read the Hermes 4 technical report here: &lt;a href="https://arxiv.org/abs/2508.18255"&gt;Hermes 4 Technical Report&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chat with Hermes in Nous Chat: &lt;a href="https://chat.nousresearch.com"&gt;https://chat.nousresearch.com&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Training highlights include a newly synthesized post-training corpus emphasizing verified reasoning traces, massive improvements in math, code, STEM, logic, creativity, and format-faithful outputs, while preserving general assistant quality and broadly neutral alignment.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B#whats-new-vs-hermes-3"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;What‚Äôs new vs Hermes 3&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Post-training corpus&lt;/strong&gt;: Massively increased dataset size from 1M samples and 1.2B tokens to &lt;strong&gt;~5M samples / ~60B tokens&lt;/strong&gt; blended across reasoning and non-reasoning data.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hybrid reasoning mode&lt;/strong&gt; with explicit &lt;code&gt;&amp;lt;think&amp;gt;‚Ä¶&amp;lt;/think&amp;gt;&lt;/code&gt; segments when the model decides to deliberate, and options to make your responses faster when you want.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reasoning&lt;/strong&gt; that is top quality, expressive, improves math, code, STEM, logic, and even creative writing and subjective responses.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Schema adherence &amp;amp; structured outputs&lt;/strong&gt;: trained to produce valid JSON for given schemas and to repair malformed objects.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Much easier to steer and align&lt;/strong&gt;: extreme improvements on steerability, especially on reduced refusal rates.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/NousResearch/Hermes-4-14B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6siz6/nousresearchhermes414b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T19:08:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6od0s</id>
    <title>ÊÆãÂøÉ / Zanshin - Navigate through media by speaker</title>
    <updated>2025-09-02T16:34:30+00:00</updated>
    <author>
      <name>/u/hamza_q_</name>
      <uri>https://old.reddit.com/user/hamza_q_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/ÊÆãÂøÉ_zanshin_navigate_through_media_by_speaker/"&gt; &lt;img alt="ÊÆãÂøÉ / Zanshin - Navigate through media by speaker" src="https://external-preview.redd.it/czg0dWhsczUyc21mMcardPaxszcLLO9nZqjdF7h57XxHnWsrQqY3M3ZeJApB.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae580e102b0dfc5e45071c5a325f730f47366dad" title="ÊÆãÂøÉ / Zanshin - Navigate through media by speaker" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ÊÆãÂøÉ / Zanshin is a media player that allows you to:&lt;/p&gt; &lt;p&gt;- Visualize who speaks when &amp;amp; for how long&lt;/p&gt; &lt;p&gt;- Jump/skip speaker segments&lt;/p&gt; &lt;p&gt;- Remove/disable speakers (auto-skip)&lt;/p&gt; &lt;p&gt;- Set different playback speeds for each speaker&lt;/p&gt; &lt;p&gt;It's a better, more efficient way to listen to podcasts, interviews, press conferences, etc.&lt;/p&gt; &lt;p&gt;It has first-class support for YouTube videos; just drop in a URL. Also supports your local media files. All processing runs on-device.&lt;/p&gt; &lt;p&gt;Download today for macOS: &lt;a href="https://zanshin.sh"&gt;https://zanshin.sh&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also works on Linux and WSL, but currently without packaging. You can get it running though with just a few terminal commands. Check out the repo for instructions: &lt;a href="https://github.com/narcotic-sh/zanshin"&gt;https://github.com/narcotic-sh/zanshin&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Zanshin is powered by Senko, a new, very fast, speaker diarization pipeline I've developed.&lt;/p&gt; &lt;p&gt;On an M3 MacBook Air, it takes over 5 minutes to process 1 hour of audio using Pyannote 3.1, the leading open-source diarization pipeline. With Senko, it only takes ~24 seconds, a ~14x speed improvement. And on an RTX 4090 + Ryzen 9 7950X machine, processing 1 hour of audio takes just 5 seconds with Senko, a ~17x speed improvement.&lt;/p&gt; &lt;p&gt;Senko's speed is what make's Zanshin possible. Senko is a modified version of the speaker diarization pipeline found in the excellent 3D-Speaker project. Check out Senko here: &lt;a href="https://github.com/narcotic-sh/senko"&gt;https://github.com/narcotic-sh/senko&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cheers, everyone; enjoy ÊÆãÂøÉ/Zanshin and Senko. I hope you find them useful. Let me know what you think!&lt;/p&gt; &lt;p&gt;~&lt;/p&gt; &lt;p&gt;Side note: I am looking for a job. If you like my work and have an opportunity for me, I'm all ears :) You can contact me at mhamzaqayyum [at] &lt;a href="http://icloud.com"&gt;icloud.com&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hamza_q_"&gt; /u/hamza_q_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qh0wtns52smf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/ÊÆãÂøÉ_zanshin_navigate_through_media_by_speaker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6od0s/ÊÆãÂøÉ_zanshin_navigate_through_media_by_speaker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T16:34:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6rbi2</id>
    <title>Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp; Meta AI</title>
    <updated>2025-09-02T18:24:09+00:00</updated>
    <author>
      <name>/u/prusswan</name>
      <uri>https://old.reddit.com/user/prusswan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt; &lt;img alt="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" src="https://external-preview.redd.it/FUP5JRh_hs7L2Yd_DmiTAO0WgUYYJ4skdrhkm8MNDKc.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fcef23df49c3448a2d625ddb43fe346cbd8bdd05" title="Piracy is for Trillion Dollar Companies | Fair Use, Copyright Law, &amp;amp; Meta AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So acquiring copyrighted material for the purpose of training LLMs is deemed transformative and qualifies under fair use? Gonna call this Meta's Defence from now on.. I have a huge stash of ebooks to run through&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/prusswan"&gt; /u/prusswan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=sdtBgB7iS8c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6rbi2/piracy_is_for_trillion_dollar_companies_fair_use/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:24:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6qqk4</id>
    <title>Slop posts</title>
    <updated>2025-09-02T18:02:48+00:00</updated>
    <author>
      <name>/u/One-Employment3759</name>
      <uri>https://old.reddit.com/user/One-Employment3759</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Can we please stop making slop posts where some guy is like &amp;quot;oh wow guys, I just bet OpenAI/Anthropic in a weekend of playing around, tee hee hee&amp;quot;&lt;/p&gt; &lt;p&gt;Thanks. I valued this sub for being high signal and having competent people, but it feels like it's going downhill lately.&lt;/p&gt; &lt;p&gt;At the very least, if you have done something groundbreaking, come here asking for people to validate your work instead of doing some influencer shit pretending you're the best thing since transformers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/One-Employment3759"&gt; /u/One-Employment3759 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6qqk4/slop_posts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T18:02:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6epwv</id>
    <title>My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench üòÖ</title>
    <updated>2025-09-02T09:17:01+00:00</updated>
    <author>
      <name>/u/DanAiTuning</name>
      <uri>https://old.reddit.com/user/DanAiTuning</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt; &lt;img alt="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench üòÖ" src="https://b.thumbs.redditmedia.com/pYjWCv-fYbHaF7KTK2GkiEx7BRL3zHSwgEFLLf7Zn0M.jpg" title="My weekend project accidentally beat Claude Code - multi-agent coder now #12 on Stanford's TerminalBench üòÖ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;üëã Hitting a million brick walls with multi-turn RL training isn't fun, so I thought I would try something new to climb Stanford's leaderboard for now! So this weekend I was just tinkering with multi-agent systems and... somehow ended up beating Claude Code on Stanford's TerminalBench leaderboard (#12)! Genuinely didn't expect this - started as a fun experiment and ended up with something that works surprisingly well.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I did:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Built a multi-agent AI system with three specialised agents:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Orchestrator&lt;/strong&gt;: The brain - never touches code, just delegates and coordinates&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Explorer agents&lt;/strong&gt;: Read &amp;amp; run only investigators that gather intel&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Coder agents&lt;/strong&gt;: The ones who actually implement stuff&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Created a &amp;quot;Context Store&amp;quot; which can be thought of as persistent memory that lets agents share their discoveries.&lt;/p&gt; &lt;p&gt;Tested on TerminalBench with both Claude Sonnet-4 and Qwen3-Coder-480B. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Orchestrator + Sonnet-4: &lt;strong&gt;36.0% success rate&lt;/strong&gt; (#12 on leaderboard, ahead of Claude Code!)&lt;/li&gt; &lt;li&gt;Orchestrator + Qwen-3-Coder: 19.25% success rate&lt;/li&gt; &lt;li&gt;Sonnet-4 consumed 93.2M tokens vs Qwen's 14.7M tokens to compete all tasks!&lt;/li&gt; &lt;li&gt;The orchestrator's explicit task delegation + intelligent context sharing between subagents seems to be the secret sauce&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;(Kind of) Technical details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The orchestrator can't read/write code directly - this forces proper delegation patterns and strategic planning&lt;/li&gt; &lt;li&gt;Each agent gets precise instructions about what &amp;quot;knowledge artifacts&amp;quot; to return, these artifacts are then stored, and can be provided to future subagents upon launch.&lt;/li&gt; &lt;li&gt;Adaptive trust calibration: simple tasks = high autonomy, complex tasks = iterative decomposition&lt;/li&gt; &lt;li&gt;Each agent has its own set of tools it can use.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;More details:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;My Github repo has all the code, system messages, and way more technical details if you're interested!&lt;/p&gt; &lt;p&gt;‚≠êÔ∏è &lt;a href="https://github.com/Danau5tin/multi-agent-coding-system"&gt;&lt;strong&gt;Orchestrator repo - all code open sourced!&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;Dan&lt;/p&gt; &lt;p&gt;(Evaluated on the excellent &lt;a href="https://www.tbench.ai/"&gt;TerminalBench&lt;/a&gt; benchmark by Stanford &amp;amp; Laude Institute)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DanAiTuning"&gt; /u/DanAiTuning &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1n6epwv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6epwv/my_weekend_project_accidentally_beat_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T09:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1n6mi81</id>
    <title>German "Who Wants to Be a Millionaire" Benchmark</title>
    <updated>2025-09-02T15:24:56+00:00</updated>
    <author>
      <name>/u/Available_Load_5334</name>
      <uri>https://old.reddit.com/user/Available_Load_5334</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt; &lt;img alt="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" src="https://preview.redd.it/du3iq68grrmf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=486736a10efedf5ea83f05d63d41d7eda1e92ac7" title="German &amp;quot;Who Wants to Be a Millionaire&amp;quot; Benchmark" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i have created a benchmark for german &amp;quot;who wants to be millionaire&amp;quot; questions. there are 45x15 questions, all 45 rounds go from easy to hard and all tested models ran through all 45 rounds and got kicked out of a round if the answer was wrong, keeping the current winnings. no jokers.&lt;/p&gt; &lt;p&gt;i am a bit limited with the selection of llm's since i run them on my framework laptop 13 (amd ryzen 5 7640u with 32 gb ram), so i mainly used smaller llm's. also, qwen3's thinking went on for way to long for each question so i just tested non-thinking models except for gpt-oss-20b (low). but in my initial testing for qwen3-4b-thinking-2507, it seemed to worsen the quality of answers at least for the first questions.&lt;/p&gt; &lt;p&gt;the first few questions are often word-play and idioms questions needing great understanding of the german language. these proved to be very hard for most llm's but are easily solvable by the average german. once the first few questions were solved the models had an easier time answering.&lt;/p&gt; &lt;p&gt;i tried to use optimal model settings and included them in the table, let me know if they could be improved. all models are quant Q4_K_M.&lt;/p&gt; &lt;p&gt;i have close to no python coding ability so the main script was created with qwen3-coder. the project (with detailed results for each model, and the queationaire) is open souce and available on github.&lt;br /&gt; &lt;a href="https://github.com/ikiruneo/millionaire-bench"&gt;https://github.com/ikiruneo/millionaire-bench&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Available_Load_5334"&gt; /u/Available_Load_5334 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/du3iq68grrmf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1n6mi81/german_who_wants_to_be_a_millionaire_benchmark/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-02T15:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
