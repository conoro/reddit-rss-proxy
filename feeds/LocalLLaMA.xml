<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-10T13:28:31+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pj2g59</id>
    <title>Open sourced a LLM powered draw.io live editor</title>
    <updated>2025-12-10T13:23:14+00:00</updated>
    <author>
      <name>/u/JerryKwan</name>
      <uri>https://old.reddit.com/user/JerryKwan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj2g59/open_sourced_a_llm_powered_drawio_live_editor/"&gt; &lt;img alt="Open sourced a LLM powered draw.io live editor" src="https://preview.redd.it/zn848zmsnd6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=569a52e33b9f76b6af27e325c0cbd2808c8088bd" title="Open sourced a LLM powered draw.io live editor" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have open sourced a LLM powerd drawio live editor, it supports fully local deployment, and bidirectional Interoperability.&lt;br /&gt; Feel free to check the codes from &lt;a href="https://github.com/JerryKwan/drawio-live-editor"&gt;https://github.com/JerryKwan/drawio-live-editor&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/JerryKwan"&gt; /u/JerryKwan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zn848zmsnd6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj2g59/open_sourced_a_llm_powered_drawio_live_editor/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj2g59/open_sourced_a_llm_powered_drawio_live_editor/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T13:23:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pizn9w</id>
    <title>Tested MiniMax M2 for boilerplate, bug fixes, API tweaks and docs ‚Äì surprisingly decent</title>
    <updated>2025-12-10T10:52:47+00:00</updated>
    <author>
      <name>/u/alokin_09</name>
      <uri>https://old.reddit.com/user/alokin_09</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizn9w/tested_minimax_m2_for_boilerplate_bug_fixes_api/"&gt; &lt;img alt="Tested MiniMax M2 for boilerplate, bug fixes, API tweaks and docs ‚Äì surprisingly decent" src="https://b.thumbs.redditmedia.com/W80CXrV6jgFuUCVTSWWvfiPABAUSrng34hFhMhgAnig.jpg" title="Tested MiniMax M2 for boilerplate, bug fixes, API tweaks and docs ‚Äì surprisingly decent" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been testing MiniMax M2 as a ‚Äúcheap implementation model‚Äù next to the usual frontier suspects, and wanted to share some actual numbers instead of vibes.&lt;/p&gt; &lt;p&gt;We ran it through four tasks inside Kilo Code:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Boilerplate generation&lt;/strong&gt; - building a Flask API from scratch&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Bug detection&lt;/strong&gt; - finding issues in Go code with concurrency and logic bugs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code extension&lt;/strong&gt; - adding features to an existing Node.js/Express project&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt; - generating READMEs and JSDoc for complex code&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;1. Flask API from scratch&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Prompt: &lt;em&gt;Create a Flask API with 3 endpoints for a todo app with GET, POST, DELETE, plus input validation and error handling.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Result: full project with &lt;a href="http://app.py"&gt;&lt;code&gt;app.py&lt;/code&gt;&lt;/a&gt;, &lt;code&gt;requirements.txt&lt;/code&gt;, and a 234-line &lt;a href="http://README.md"&gt;&lt;code&gt;README.md&lt;/code&gt;&lt;/a&gt; in under 60 seconds, at zero cost on the current free tier. Code followed Flask conventions and even added a health check and query filters we didn‚Äôt explicitly ask for.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Bug detection in Go&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Prompt: &lt;em&gt;Review this Go code and identify any bugs, potential crashes, or concurrency issues. Explain each problem and how to fix it.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;The result: MiniMax M2 found all 4 bugs.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tested-minimax-m2-for-boilerplate-bug-fixes-api-tweaks-and-v0-bhln7zjorc6g1.png?width=1456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5480fc1ccfd4ad6f0f93fdc1f8e51a08b413d2ce"&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wrhcuk9dxc6g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62de352fca2b5ed4d5899cea10c44789b48110e5"&gt;https://preview.redd.it/wrhcuk9dxc6g1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62de352fca2b5ed4d5899cea10c44789b48110e5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Extending a Node/TS API&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This test had two parts.&lt;/p&gt; &lt;p&gt;First, we asked MiniMax M2 to create a bookmark manager API. Then we asked it to extend the implementation with new features.&lt;/p&gt; &lt;p&gt;Step 1 prompt: ‚ÄúCreate a Node.js Express API with TypeScript for a simple bookmark manager. Include GET /bookmarks, POST /bookmarks, and DELETE /bookmarks/:id with in-memory storage, input validation, and error handling.‚Äù&lt;/p&gt; &lt;p&gt;Step 2 prompt: ‚ÄúNow extend the bookmark API with GET /bookmarks/:id, PUT /bookmarks/:id, GET /bookmarks/search?q=term, add a favorites boolean field, and GET /bookmarks/favorites. Make sure the new endpoints follow the same patterns as the existing code.‚Äù&lt;/p&gt; &lt;p&gt;Results: MiniMax M2 generated a proper project structure and the service layer shows clean separation of concerns:&lt;/p&gt; &lt;p&gt;When we asked the model to extend the API, it followed the existing patterns precisely. It extended the project without trying to ‚Äúrewrite‚Äù everything, kept the same validation middleware, error handling, and response format.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Docs/JSDoc&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Prompt: &lt;em&gt;Add comprehensive JSDoc documentation to this TypeScript function. Include descriptions for all parameters, return values, type definitions, error handling behavior, and provide usage examples showing common scenarios&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Result: The output included documentation for every type, parameter descriptions with defaults, error-handling notes, and five different usage examples. MiniMax M2 understood the function‚Äôs purpose, identified all three patterns it implements, and generated examples that demonstrate realistic use cases.&lt;/p&gt; &lt;p&gt;Takeaways so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M2 is very good when you already know what you want (build X with these endpoints, find bugs, follow existing patterns, document this function).&lt;/li&gt; &lt;li&gt;It‚Äôs not trying to ‚Äúoverthink‚Äù like Opus / GPT when you just need code written.&lt;/li&gt; &lt;li&gt;At regular pricing it‚Äôs &amp;lt;10% of Claude Sonnet 4.5, and right now it‚Äôs free inside Kilo Code, so you can hammer it for boilerplate-type work.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full write-up with prompts, screenshots, and test details is here if you want to dig in:&lt;/p&gt; &lt;p&gt;‚Üí &lt;a href="https://blog.kilo.ai/p/putting-minimax-m2-to-the-test-boilerplate"&gt;https://blog.kilo.ai/p/putting-minimax-m2-to-the-test-boilerplate&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alokin_09"&gt; /u/alokin_09 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizn9w/tested_minimax_m2_for_boilerplate_bug_fixes_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizn9w/tested_minimax_m2_for_boilerplate_bug_fixes_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pizn9w/tested_minimax_m2_for_boilerplate_bug_fixes_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T10:52:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1piw1sw</id>
    <title>Best small LLM for general advice?</title>
    <updated>2025-12-10T06:59:14+00:00</updated>
    <author>
      <name>/u/Qxz3</name>
      <uri>https://old.reddit.com/user/Qxz3</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not as a coding assistant or puzzle solver, but for general discussions about life, health, relationships etc. &lt;/p&gt; &lt;p&gt;So far my best bet has been Gemma 3. Have fiddled a bit with Ministral 3 but it tends to produce answers that are long, lack focus, rely too much on bullet points and speaks the dreaded AI slop language. Perhaps better prompting would help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Qxz3"&gt; /u/Qxz3 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw1sw/best_small_llm_for_general_advice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw1sw/best_small_llm_for_general_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piw1sw/best_small_llm_for_general_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T06:59:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi8z74</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:35:50+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/h9d1fvb7w66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd2f0eef8f43dfd10528e31cb2b9efd46b87bfb" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9d1fvb7w66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj0jdy</id>
    <title>is there htop for vulkan? htop for vram?</title>
    <updated>2025-12-10T11:45:39+00:00</updated>
    <author>
      <name>/u/cranberrie_sauce</name>
      <uri>https://old.reddit.com/user/cranberrie_sauce</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there htop for vulkan? htop for vram? &lt;/p&gt; &lt;p&gt;I find its near impossible to know what is the current strix halo vram utilization.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cranberrie_sauce"&gt; /u/cranberrie_sauce &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj0jdy/is_there_htop_for_vulkan_htop_for_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj0jdy/is_there_htop_for_vulkan_htop_for_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj0jdy/is_there_htop_for_vulkan_htop_for_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T11:45:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1piw3mp</id>
    <title>VSCode Copilot Autocomplete with local / custom models</title>
    <updated>2025-12-10T07:02:05+00:00</updated>
    <author>
      <name>/u/mter24</name>
      <uri>https://old.reddit.com/user/mter24</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey there,&lt;/p&gt; &lt;p&gt;I am the creator of this issue: &lt;a href="https://github.com/microsoft/vscode/issues/263535"&gt;https://github.com/microsoft/vscode/issues/263535&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It is basically a feature request that allows developers to use their own LLMs for autocomplete.&lt;/p&gt; &lt;p&gt;Now I need now &lt;strong&gt;your help&lt;/strong&gt;. If you think this could be a useful feature please &lt;strong&gt;upvote this issue&lt;/strong&gt;.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mter24"&gt; /u/mter24 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw3mp/vscode_copilot_autocomplete_with_local_custom/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piw3mp/vscode_copilot_autocomplete_with_local_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piw3mp/vscode_copilot_autocomplete_with_local_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T07:02:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pil53r</id>
    <title>AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision</title>
    <updated>2025-12-09T22:19:37+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"&gt; &lt;img alt="AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision" src="https://b.thumbs.redditmedia.com/3AFqcVzBcIqkShsFZeKC1B1EUCe3mUgvy1_R1nSDzVc.jpg" title="AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twice as fast at running 8-bit transformers than the previous generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pil53r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T22:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj1a7c</id>
    <title>vLLM supports the new Devstral 2 coding models</title>
    <updated>2025-12-10T12:25:39+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj1a7c/vllm_supports_the_new_devstral_2_coding_models/"&gt; &lt;img alt="vLLM supports the new Devstral 2 coding models" src="https://preview.redd.it/br78ujzbdd6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ea9b92040ddda20f4696a2bafe051470316248e4" title="vLLM supports the new Devstral 2 coding models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Devstral 2 is SOTA open model for code agents with a fraction of the parameters of its competitors and achieving 72.2% on SWE-bench Verified.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/br78ujzbdd6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj1a7c/vllm_supports_the_new_devstral_2_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj1a7c/vllm_supports_the_new_devstral_2_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T12:25:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1piz6vx</id>
    <title>Devstral-Small-2-24B q6k entering loop (both Unsloth and Bartowski) (llama.cpp)</title>
    <updated>2025-12-10T10:24:12+00:00</updated>
    <author>
      <name>/u/relmny</name>
      <uri>https://old.reddit.com/user/relmny</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying both:&lt;/p&gt; &lt;p&gt;Unsloth: Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL.gguf&lt;br /&gt; and&lt;br /&gt; Bartowki: mistralai_Devstral-Small-2-24B-Instruct-2512-Q6_K_L.gguf&lt;/p&gt; &lt;p&gt;and with a context of 24k (still have enough VRAM available) for a 462 tokens prompt, it enters a loop after a few tokens.&lt;/p&gt; &lt;p&gt;I tried different options with llama-server (llama.cpp), which I started with the Unsloth's recommended one and then I started making some changes, leaving it as clean as possible, but I still get a loop.&lt;/p&gt; &lt;p&gt;I managed to get an answer, once, with Bartowski one with the very basic settings (flags) but although it didn't enter a loop, it did repeated the same line 3 times.&lt;/p&gt; &lt;p&gt;The cleaner one was (also tried temp: 0.15):&lt;/p&gt; &lt;p&gt;--threads -1 --cache-type-k q8_0 --n-gpu-layers 99 --temp 0.2 -c 24786&lt;/p&gt; &lt;p&gt;Is Q6 broken? or are there any new flags that need to be added?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/relmny"&gt; /u/relmny &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piz6vx/devstralsmall224b_q6k_entering_loop_both_unsloth/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piz6vx/devstralsmall224b_q6k_entering_loop_both_unsloth/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piz6vx/devstralsmall224b_q6k_entering_loop_both_unsloth/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T10:24:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir03u</id>
    <title>New ASR modelÔºöGLM-ASR-Nano-2512 1.5B Supports Mandarin/English/Cantonese and more</title>
    <updated>2025-12-10T02:34:29+00:00</updated>
    <author>
      <name>/u/Terrible_Scar_9890</name>
      <uri>https://old.reddit.com/user/Terrible_Scar_9890</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"&gt; &lt;img alt="New ASR modelÔºöGLM-ASR-Nano-2512 1.5B Supports Mandarin/English/Cantonese and more" src="https://b.thumbs.redditmedia.com/cXXf5ug_0MFMT80R9gQEIam1ZEahJsu9lpAYtVW8efI.jpg" title="New ASR modelÔºöGLM-ASR-Nano-2512 1.5B Supports Mandarin/English/Cantonese and more" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/j5os8wcdga6g1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c976a30e633fb6c7dfcdc9bf2cdceaaf3798438"&gt;https://preview.redd.it/j5os8wcdga6g1.png?width=1200&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9c976a30e633fb6c7dfcdc9bf2cdceaaf3798438&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-ASR-Nano-2512"&gt;https://huggingface.co/zai-org/GLM-ASR-Nano-2512&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GLM-ASR-Nano-2512&lt;/strong&gt;&lt;br /&gt; 1.5B&lt;br /&gt; Supports Mandarin/English/Cantonese and more&lt;br /&gt; Clearly recognizes whisper/quiet speech&lt;br /&gt; Excels in noisy, overlapping environments&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terrible_Scar_9890"&gt; /u/Terrible_Scar_9890 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir03u/new_asr_modelglmasrnano2512_15b_supports/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:34:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1piabn8</id>
    <title>Devstral-Small-2-24B-Instruct-2512 on Hugging Face</title>
    <updated>2025-12-09T15:29:19+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt; &lt;img alt="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" src="https://external-preview.redd.it/9AtiZkI9TGGX4HUjb1yXt2_pTwLgjJScmnM7q3ZVgpw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98ba88a60b440a866a778f583a15081cf6838a4" title="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1piq11p</id>
    <title>Mac with 64GB? Try Qwen3-Next!</title>
    <updated>2025-12-10T01:50:12+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried qwen3-next-80b-a3b-thinking-4bit using mlx-lm on my M3 Max with 64GB, and the quality is excellent with very reasonable speed.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing: 7123 tokens at 1015.80 tokens per second&lt;/li&gt; &lt;li&gt;Text generation: 1253 tokens at 65.84 tokens per second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I can also fully load 33.5K context only using 49.8GB, and I can push and allocate up to 58 of 64 GB without any freezing.&lt;/p&gt; &lt;p&gt;I think this model might be the best model so far that pushes a 64 GB Mac to its limits in the best way!&lt;/p&gt; &lt;p&gt;I also tried qwen3-next-80b-a3b-thinking-q4_K_M.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prompt processing: 7122 tokens at 295.24 tokens per second&lt;/li&gt; &lt;li&gt;Text generation: 1222 tokens at 10.99 tokens per second&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;People mentioned in the comment that Qwen3-next is not optimized for speed with gguf yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piq11p/mac_with_64gb_try_qwen3next/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piq11p/mac_with_64gb_try_qwen3next/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piq11p/mac_with_64gb_try_qwen3next/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T01:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1piykjp</id>
    <title>Built a visual debugger for my local agents because I was lost in JSON, would you use this?</title>
    <updated>2025-12-10T09:44:56+00:00</updated>
    <author>
      <name>/u/AdVivid5763</name>
      <uri>https://old.reddit.com/user/AdVivid5763</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"&gt; &lt;img alt="Built a visual debugger for my local agents because I was lost in JSON, would you use this?" src="https://preview.redd.it/ymvtn22clc6g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75a1585a27e4b916d4d1714f215f2080267441e6" title="Built a visual debugger for my local agents because I was lost in JSON, would you use this?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I run local LLM agents with tools / RAG. When a run broke, my workflow was basically: &lt;/p&gt; &lt;p&gt;rerun with more logging, diff JSON, and guess which step actually screwed things up. Slow and easy to miss.&lt;/p&gt; &lt;p&gt;So I hacked a small tool for myself: it takes a JSON trace and shows the run as a graph + timeline. &lt;/p&gt; &lt;p&gt;Each step is a node with the prompt / tool / result, and there‚Äôs a basic check that highlights obvious logic issues (like using empty tool results as if they were valid). &lt;/p&gt; &lt;p&gt;It‚Äôs already way faster for me than scrolling logs.&lt;/p&gt; &lt;p&gt;Long-term, I‚Äôd like this to become a proper ‚Äúcognition debugger‚Äù layer on top of whatever logs/traces you already have, especially for non-deterministic agents where ‚Äúwhat happened?‚Äù is not obvious.&lt;/p&gt; &lt;p&gt;It‚Äôs model-agnostic as long as the agent can dump a trace.&lt;/p&gt; &lt;p&gt;I‚Äôm mostly curious if anyone else here hits the same pain. &lt;/p&gt; &lt;p&gt;If this sounds useful, tell me what a debugger like this must show for you to actually use it. &lt;/p&gt; &lt;p&gt;I‚Äôll drop a demo link in the comments üîó.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdVivid5763"&gt; /u/AdVivid5763 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ymvtn22clc6g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piykjp/built_a_visual_debugger_for_my_local_agents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T09:44:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pigb3i</id>
    <title>DeepSeek-V3.2-REAP: 508B and 345B checkpoints</title>
    <updated>2025-12-09T19:14:58+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, to get us all in the holiday mood we're continuing to REAP models, this time we got DeepSeek-V3.2 for you at 25% and 50% compression: &lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're pretty excited about this one and are working to get some agentic evals for coding and beyond on these checkpoints soon. Enjoy and stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T19:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj12o6</id>
    <title>We basically have GLM 4.6 Air, without vision</title>
    <updated>2025-12-10T12:14:56+00:00</updated>
    <author>
      <name>/u/LegacyRemaster</name>
      <uri>https://old.reddit.com/user/LegacyRemaster</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"&gt; &lt;img alt="We basically have GLM 4.6 Air, without vision" src="https://b.thumbs.redditmedia.com/N6-RjB_vDOlUTxDCH2IFVKNBxBmsxRH0AWNuhj77qNs.jpg" title="We basically have GLM 4.6 Air, without vision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/exy3lba7cd6g1.png?width=2075&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4cf64faf530b641c9a81d37e1af555bbc372a568"&gt;glm 4.6 air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tested and working in LM Studio. Thanks for the GGUF!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LegacyRemaster"&gt; /u/LegacyRemaster &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12o6/we_basically_have_glm_46_air_without_vision/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T12:14:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pihu16</id>
    <title>bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF</title>
    <updated>2025-12-09T20:10:40+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt; &lt;img alt="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" src="https://external-preview.redd.it/Y9-VSUeByMali_oSJcuRXft1g3dj7X6u-O2vcI7YtII.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9d7830dcda85560752ed0db90867edc36dddee1" title="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T20:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir8jc</id>
    <title>3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)</title>
    <updated>2025-12-10T02:45:16+00:00</updated>
    <author>
      <name>/u/Electronic-Fly-6465</name>
      <uri>https://old.reddit.com/user/Electronic-Fly-6465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"&gt; &lt;img alt="3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)" src="https://preview.redd.it/nzlqosj6ia6g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c781c9e64faa5592f78eccce290897f05ec44256" title="3D visualisation of GPT-2's layer-by-layer transformations (prototype ‚ÄúLLM oscilloscope‚Äù)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a visualisation tool that displays the internal layer dynamics of GPT-2 Small during a single forward pass.&lt;/p&gt; &lt;p&gt;It renders:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;per-head vector deltas&lt;/li&gt; &lt;li&gt;PCA-3 residual stream projections&lt;/li&gt; &lt;li&gt;angle + magnitude differences between heads&lt;/li&gt; &lt;li&gt;stabilisation behaviour in early layers&lt;/li&gt; &lt;li&gt;the sharp directional transition around layers 9‚Äì10&lt;/li&gt; &lt;li&gt;the consistent ‚Äúanchoring / braking‚Äù effect in layer 11&lt;/li&gt; &lt;li&gt;two-prompt comparison mode (‚ÄúI like X‚Äù vs ‚ÄúI like Y‚Äù)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything in the video is generated from real measurements ‚Äî no mock data or animation shortcuts.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Demo video (22 min raw walkthrough):&lt;/strong&gt;&lt;br /&gt; &lt;a href="https://youtu.be/dnWikqNAQbE"&gt;https://youtu.be/dnWikqNAQbE&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Just sharing the prototype.&lt;br /&gt; If anyone working on interpretability or visualisation wants to discuss it, I‚Äôm around.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electronic-Fly-6465"&gt; /u/Electronic-Fly-6465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nzlqosj6ia6g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir8jc/3d_visualisation_of_gpt2s_layerbylayer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:45:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1piumvw</id>
    <title>bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face</title>
    <updated>2025-12-10T05:37:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"&gt; &lt;img alt="bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face" src="https://external-preview.redd.it/0jH917Owmr7iKrMXvyA0r05fWobE4kYASAkKFjbuamg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d8285c76b1c0f5aa8e695fdb89fac6a270b922a8" title="bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;it was gated before, finally it's available&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/ServiceNow-AI_Apriel-1.6-15b-Thinker-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piumvw/bartowskiservicenowai_apriel1615bthinkergguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T05:37:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9q3t</id>
    <title>Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI</title>
    <updated>2025-12-09T15:05:54+00:00</updated>
    <author>
      <name>/u/YanderMan</name>
      <uri>https://old.reddit.com/user/YanderMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt; &lt;img alt="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YanderMan"&gt; /u/YanderMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pj12ix</id>
    <title>Hands-on review of Mistral Vibe on large python project</title>
    <updated>2025-12-10T12:14:43+00:00</updated>
    <author>
      <name>/u/Avienir</name>
      <uri>https://old.reddit.com/user/Avienir</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just spent some time testing Mistral Vibe on real use cases and I must say I‚Äôm impressed. For context: I'm a dev working on a fairly big Python codebase (~40k LOC) with some niche frameworks (Reflex, etc.), so I was curious how it handles real-world existing projects rather than just spinning up new toys from scratch.&lt;/p&gt; &lt;p&gt;UI/Features: Looks really clean and minimal ‚Äì nice themes, feels polished for a v1.0.5. Missing some QoL stuff that's standard in competitors: no conversation history/resume, no checkpoints, no planning mode, no easy AGENTS.md support for project-specific config. Probably coming soon since it's super fresh.&lt;/p&gt; &lt;p&gt;The good (coding performance): Tested on two tasks in my existing repo:&lt;/p&gt; &lt;p&gt;Simple one: Shrink text size in a component. It nailed it ‚Äì found the right spot, checked other components to gauge scale, deduced the right value. Felt smart. 10/10.&lt;/p&gt; &lt;p&gt;Harder: Fix a validation bug in time-series models with multiple series. Solved it exactly as asked, wrote its own temp test to verify, cleaned up after. Struggled a bit with running the app (my project uses uv, not plain python run), and needed a few iterations on integration tests, but ended up with solid, passing tests and even suggested extra e2e ones. 8/10.&lt;/p&gt; &lt;p&gt;Overall: Fast, good context search, adapts to project style well, does exactly what you ask without hallucinating extras.&lt;/p&gt; &lt;p&gt;The controversial bit: 100k token context limit Yeah, it's capped there (compresses beyond?). Won't build huge apps from zero or refactor massive repos in one go. But... is that actually a dealbreaker? My harder task fit in ~75k. For day-to-day feature adds/bug fixes in real codebases, it feels reasonable ‚Äì forces better planning and breaking things down. Kinda natural discipline? Summary pros/cons:&lt;/p&gt; &lt;p&gt;Pros:&lt;/p&gt; &lt;p&gt;Speed Smart context handling Sticks to instructions Great looking terminal UI&lt;/p&gt; &lt;p&gt;Cons:&lt;/p&gt; &lt;p&gt;100k context cap Missing features (history, resume, etc.)&lt;/p&gt; &lt;p&gt;Definitely worth trying if you're into CLI agents or want a cheaper/open alternative. Curious what others think ‚Äì anyone else messed with it yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Avienir"&gt; /u/Avienir &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12ix/handson_review_of_mistral_vibe_on_large_python/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12ix/handson_review_of_mistral_vibe_on_large_python/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pj12ix/handson_review_of_mistral_vibe_on_large_python/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T12:14:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pizl8t</id>
    <title>Built a GGUF memory &amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL</title>
    <updated>2025-12-10T10:49:12+00:00</updated>
    <author>
      <name>/u/ittaboba</name>
      <uri>https://old.reddit.com/user/ittaboba</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"&gt; &lt;img alt="Built a GGUF memory &amp;amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL" src="https://external-preview.redd.it/cnpqZXU4dXV0YzZnMYh73P_j0pnSesQyyRb8l_QLx5gX0RNmxMe-sw-YRlmA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edc6af8aed5100a84c198cee502219f22065dba7" title="Built a GGUF memory &amp;amp; tok/sec calculator for inference requirements ‚Äì Drop in any HF GGUF URL" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi there,&lt;/p&gt; &lt;p&gt;Built a small utility that estimates how much memory you need to run GGUF models locally, plus an approximate tok/sec based on your machine (Apple Silicon only atm, more hardware soon) and task (e.g. ask a generic question, write a draft, etc.).&lt;/p&gt; &lt;p&gt;You can select a model from a dropdown or paste any direct GGUF URL from HF. The tool parses the model metadata (size, layers, hidden dimensions, KV cache, etc.) and uses that to estimate:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Total memory needed for weights + KV cache + activations + overhead&lt;/li&gt; &lt;li&gt;Expected latency and generation speed (tok/sec)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Demo: &lt;a href="https://manzoni.app/llm_calculator"&gt;https://manzoni.app/llm_calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code + formulas: &lt;a href="https://github.com/gems-platforms/gguf-memory-calculator"&gt;https://github.com/gems-platforms/gguf-memory-calculator&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback, edge cases, or bug reports (e.g. comparisons against your actual tokens/sec to tighten the estimates). &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ittaboba"&gt; /u/ittaboba &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/qahbzltutc6g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pizl8t/built_a_gguf_memory_toksec_calculator_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T10:49:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1piux9z</id>
    <title>Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters</title>
    <updated>2025-12-10T05:54:13+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt; &lt;img alt="Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters" src="https://b.thumbs.redditmedia.com/02J_w_E-jjdBogT1atigLYYoW24wILytNHbOE75U0FI.jpg" title="Z.ai release GLM-ASR-Nano: an open-source ASR model with 1.5B parameters" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4hs2rkx0gb6g1.png?width=1600&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a1005ca9567e6c31bb0b23f8a3e9473959507757"&gt;Benchmark&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Designed for real-world complexity, it outperforms OpenAI Whisper V3 on multiple benchmarks while maintaining a compact size.&lt;/p&gt; &lt;p&gt;Key capabilities include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Exceptional Dialect Support:&lt;/strong&gt; Beyond standard Mandarin and English, the model is highly optimized for &lt;strong&gt;Cantonese&lt;/strong&gt; and other dialects, effectively bridging the gap in dialectal speech recognition.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Low-Volume Speech Robustness:&lt;/strong&gt; Specifically trained for &lt;strong&gt;&amp;quot;Whisper/Quiet Speech&amp;quot;&lt;/strong&gt; scenarios. It captures and accurately transcribes extremely low-volume audio that traditional models often miss.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SOTA Performance:&lt;/strong&gt; Achieves the &lt;strong&gt;lowest average error rate (4.10)&lt;/strong&gt; among comparable open-source models, showing significant advantages in Chinese benchmarks (Wenet Meeting, Aishell-1, etc..)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huggingface: &lt;a href="https://huggingface.co/zai-org/GLM-ASR-Nano-2512"&gt;https://huggingface.co/zai-org/GLM-ASR-Nano-2512&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piux9z/zai_release_glmasrnano_an_opensource_asr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T05:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pir555</id>
    <title>So what's the closest open-source thing to claude code?</title>
    <updated>2025-12-10T02:40:56+00:00</updated>
    <author>
      <name>/u/According-Ebb917</name>
      <uri>https://old.reddit.com/user/According-Ebb917</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;just wondering which coding agent/multi-agent system out there is the closest to claude code? Particularly in terms of good scaffolding (subagents, skills, proper context engineering, etc...) and works well with a set of models? I feel like there's a new one everyday but I can't seem to figure out which work and which don't&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/According-Ebb917"&gt; /u/According-Ebb917 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pir555/so_whats_the_closest_opensource_thing_to_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T02:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1piwx9u</id>
    <title>Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores</title>
    <updated>2025-12-10T07:54:13+00:00</updated>
    <author>
      <name>/u/Sumanth_077</name>
      <uri>https://old.reddit.com/user/Sumanth_077</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt; &lt;img alt="Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores" src="https://external-preview.redd.it/G7Gcft3BKg57j9czqWCQwa5R5JjWhPW-BbTK-PcJb1k.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f0aad896057c96dc1a1c9470d0d19ea461ad37b1" title="Trinity Mini: a 26B OpenWeight MoE model with a 3B active and strong reasoning scores" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Arcee AI quietly dropped a pretty interesting model last week: Trinity Mini, a 26B-parameter sparse MoE with only 3B active parameters&lt;/p&gt; &lt;p&gt;A few things that actually stand out beyond the headline numbers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;128 experts, 8 active + 1 shared expert&lt;/strong&gt;. Routing is noticeably more stable than typical 2/4-expert MoEs, especially on math and tool-calling tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;10T curated tokens&lt;/strong&gt;, built on top of the Datology dataset stack. The math/code additions seem to actually matter, the model holds state across multi-step reasoning better than most mid-size MoEs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;128k context&lt;/strong&gt; without the ‚Äúfalls apart after 20k tokens‚Äù behavior a lot of open models still suffer from.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Strong zero-shot scores&lt;/strong&gt;: &lt;ul&gt; &lt;li&gt;&lt;strong&gt;84.95% MMLU (ZS)&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;92.10% Math-500&lt;/strong&gt; These would be impressive even for a 70B dense model. For a 3B-active MoE, it‚Äôs kind of wild.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you want to experiment with it, it‚Äôs available via &lt;a href="https://clarifai.com/arcee_ai/AFM/models/trinity-mini"&gt;Clarifai&lt;/a&gt; and also &lt;a href="https://openrouter.ai/arcee-ai/trinity-mini"&gt;OpenRouter&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Curious what you all think after trying it?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1m97sj3f0c6g1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddc01b2fd25dddd2c9f1e45965cbff3e58cccdf"&gt;https://preview.redd.it/1m97sj3f0c6g1.png?width=4800&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4ddc01b2fd25dddd2c9f1e45965cbff3e58cccdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sumanth_077"&gt; /u/Sumanth_077 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piwx9u/trinity_mini_a_26b_openweight_moe_model_with_a_3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-10T07:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
