<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-29T16:47:45+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qpneiq</id>
    <title>AMD Strix Halo GMTEK 128GB Unified ROCKS!</title>
    <updated>2026-01-28T20:43:57+00:00</updated>
    <author>
      <name>/u/MSBStudio</name>
      <uri>https://old.reddit.com/user/MSBStudio</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running a MAX+ 395 as my daily workstation ‚Äî the unified memory architecture &lt;/p&gt; &lt;p&gt;is a game-changer for AI/ML workloads. Being able to allocate 96GB+ to the GPU without the PCIe bottleneck makes local LLM. DeepSeek 70B *12 tokens/s, gpt-oss faster, comfyui with LTX2 12 s/it this is a game changer...no quants not hassle. In if you need check out my GIT I have step by step &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/bkpaine1"&gt;https://github.com/bkpaine1&lt;/a&gt; have some comfyui nodes for AMD and walk throughs to get beast cranking! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MSBStudio"&gt; /u/MSBStudio &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpneiq/amd_strix_halo_gmtek_128gb_unified_rocks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T20:43:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq9n5f</id>
    <title>QWEN3 on the SBC (Orange pi 6 plus)</title>
    <updated>2026-01-29T14:13:34+00:00</updated>
    <author>
      <name>/u/Desperate-Sir-5088</name>
      <uri>https://old.reddit.com/user/Desperate-Sir-5088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry for my bad English, and I worte this article by the helping of local LLM :(&lt;/p&gt; &lt;p&gt;Week ago, I bought Orange Pi 6 Plus from Aliexpress to try running LLM on SBC.&lt;/p&gt; &lt;p&gt;It has a 32GB of unified LPDDR5 RAM!!! and is almost identical to Radax Orion O6&lt;/p&gt; &lt;p&gt;The spec of Orange Pi 6 32GB (ARM-9v 12-Cores Architecture)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;SoC:&lt;/strong&gt; CIX CD8160 (12-core 64-bit ARMv9: 4x A72 + 4x A72 + 4x A52).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;AI Performance:&lt;/strong&gt; ~45 TOPS (combined CPU/GPU/NPU).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 16GB, 32GB, or 64GB LPDDR5.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Unfortunately, O/S and Driver support of Orange Pi series were really notorious.&lt;/p&gt; &lt;p&gt;On latest release, Ubuntu 24.04 + 6.8 Kernel with dedicated GPU drive support Vulkan 1.4.&lt;/p&gt; &lt;p&gt;But, It was painfully slow and unstable for the general usage.&lt;/p&gt; &lt;p&gt;Finally, I was able to achieve satisfactory performance with this combination :&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ik_llama.cpp + QWEN3-30B-A3B (IQ4_XS quant)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Personally, I strongly advise against buying an Orange Pi 6 for LLM purposes. &lt;/p&gt; &lt;p&gt;However, I would be leaving a few hints here for friends who might repeat this foolish mistake.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. Compile ik_llama with Arm9v flags with GCC 12&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;sudo add-apt-repository ppa:ubuntu-toolchain-&lt;a href="/r/test"&gt;r/test&lt;/a&gt;&lt;br /&gt; sudo apt update&lt;br /&gt; sudo apt install -y gcc-12 g++-12&lt;/p&gt; &lt;p&gt;cmake -B build \&lt;/p&gt; &lt;p&gt;-DGGML_CPU_ALL_VARIANTS=OFF \&lt;br /&gt; -DGGML_ARCH_FLAGS=&amp;quot;-march=armv9-a+dotprod+fp16&amp;quot;&lt;/p&gt; &lt;p&gt;cmake --build build --config Release -j$(nproc)&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Do not try using GPU/NPU - just depends on Big core (4cores) with -ngl 0 flag&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I'm not familar with Linux &amp;amp; ARM devices, and can't guarantee No. of Big cores &lt;/p&gt; &lt;p&gt;in other boards. So, please use btop or other apps to get exact information of your board.&lt;/p&gt; &lt;p&gt;Here is my final setting to load QWEN3-30B Instruct model with usable performence&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;taskset -c 0,1,10,11 ./llama-bench -m /home/LLM_test/Qwen3-VL-30B-A3B-Instruct-IQ4_XS.gguf -ngl 0 --mmap 0 -ctk q8_0 -ctv q8_0&lt;/p&gt; &lt;p&gt;| model | size | params | backend |threads|type_k|type_v|mmap| test | t/s |&lt;/p&gt; &lt;p&gt;| ------------------------------ | ---------: | ---------: | ---------- | ------: | -----: | -----: | ---: | ------------: | ---------------: |&lt;/p&gt; &lt;p&gt;===================================== llama_new_context_with_model: f16&lt;/p&gt; &lt;p&gt;======================================= HAVE_FANCY_SIMD is NOT defined&lt;/p&gt; &lt;p&gt;| qwen3vlmoe 30B.A3B IQ4_XS - 4.25 bpw | 15.25 GiB | 30.53 B | CPU | 12 | q8_0 | q8_0 | 0 | pp512 | 52.82 ¬± 0.42 |&lt;/p&gt; &lt;p&gt;===================================== llama_new_context_with_model: f16&lt;/p&gt; &lt;p&gt;| qwen3vlmoe 30B.A3B IQ4_XS - 4.25 bpw | 15.25 GiB | 30.53 B | CPU | 12 | q8_0 | q8_0 | 0 | tg128 | 8.35 ¬± 0.00 |&lt;/p&gt; &lt;p&gt;build: 69fdd041 (4149)&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qq9n5f/video/llym7f8jqagg1/player"&gt;https://reddit.com/link/1qq9n5f/video/llym7f8jqagg1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Sir-5088"&gt; /u/Desperate-Sir-5088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq9n5f/qwen3_on_the_sbc_orange_pi_6_plus/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq9n5f/qwen3_on_the_sbc_orange_pi_6_plus/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq9n5f/qwen3_on_the_sbc_orange_pi_6_plus/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T14:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqcze7</id>
    <title>chat : add parsing for solar-open-100b by aldehir ¬∑ Pull Request #18540 ¬∑ ggml-org/llama.cpp</title>
    <updated>2026-01-29T16:17:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqcze7/chat_add_parsing_for_solaropen100b_by_aldehir/"&gt; &lt;img alt="chat : add parsing for solar-open-100b by aldehir ¬∑ Pull Request #18540 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/3TwT5w3u0FeWYIlu8vGj_s0Ycol7t-njdumAgpPxGvM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8189be2f746d94e5222c57541f7ac42211203dd8" title="chat : add parsing for solar-open-100b by aldehir ¬∑ Pull Request #18540 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;code&gt;reasoning_effort: &amp;quot;minimal&amp;quot; | &amp;quot;low&amp;quot; | &amp;quot;medium&amp;quot; | &amp;quot;high&amp;quot; = &amp;quot;high&amp;quot;&lt;/code&gt; - Set reasoning effort. When set to &lt;code&gt;low&lt;/code&gt; or &lt;code&gt;minimal&lt;/code&gt;, reasoning is disabled.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18540"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqcze7/chat_add_parsing_for_solaropen100b_by_aldehir/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqcze7/chat_add_parsing_for_solaropen100b_by_aldehir/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T16:17:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp87tk</id>
    <title>Kimi K2.5 is the best open model for coding</title>
    <updated>2026-01-28T10:54:13+00:00</updated>
    <author>
      <name>/u/npc_gooner</name>
      <uri>https://old.reddit.com/user/npc_gooner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt; &lt;img alt="Kimi K2.5 is the best open model for coding" src="https://preview.redd.it/unxlhercm2gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23f59fb1153598db9b9c8a3b5ce9067435dfba28" title="Kimi K2.5 is the best open model for coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;they really cooked&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/npc_gooner"&gt; /u/npc_gooner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/unxlhercm2gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp87tk/kimi_k25_is_the_best_open_model_for_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T10:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1qppjo4</id>
    <title>Assistant_Pepe_8B, 1-M context, zero slop</title>
    <updated>2026-01-28T22:03:49+00:00</updated>
    <author>
      <name>/u/Sicarius_The_First</name>
      <uri>https://old.reddit.com/user/Sicarius_The_First</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;/blockquote&gt; &lt;p&gt;This is a project that was a long time in the making because I wanted to get it right. I'm still not fully satisfied, as there are some rough corners to sand, but for now, this would do.&lt;/p&gt; &lt;p&gt;The goal was to &lt;strong&gt;maximize shitpostness&lt;/strong&gt; along with &lt;strong&gt;helpfulness&lt;/strong&gt;, without glazing the user for every retarded idea. Not an easy needle to thread.&lt;/p&gt; &lt;p&gt;This amphibious AI has learned the ways of /g/, and speaks &lt;strong&gt;fluent brainrot&lt;/strong&gt;, but will also help you out with just about anything you'll need, and won't be ashamed to roast you while at it.&lt;/p&gt; &lt;p&gt;For those who remember &lt;a href="https://huggingface.co/SicariusSicariiStuff/Oni_Mitsubishi_12B"&gt;Oni_Mitsubishi_12B&lt;/a&gt; - it was &lt;strong&gt;so overtly toxic&lt;/strong&gt; that it made me worry at first (only to quickly be verified as not even that uncensored). I could do better. So now I did.&lt;/p&gt; &lt;p&gt;This model is a &lt;strong&gt;significant refinement&lt;/strong&gt; of the idea, with a cleaned dataset, better curation, and with much more intelligence (also &lt;strong&gt;one million tokens of contexts&lt;/strong&gt;, theoretically).&lt;/p&gt; &lt;p&gt;It is much less (overtly) toxic, and much smarter, while also being very helpful (and imo much more funny too, because the skies are blue due to the chemtrails and neurlink that feeds this simulation)&lt;/p&gt; &lt;h1&gt;But why?&lt;/h1&gt; &lt;p&gt;It's now late &lt;strong&gt;January&lt;/strong&gt;, &lt;strong&gt;2026&lt;/strong&gt;, open source is crushing closed frontier (&lt;a href="https://huggingface.co/moonshotai/Kimi-K2.5"&gt;Kimi K2.5&lt;/a&gt; was recently released, &lt;strong&gt;1T&lt;/strong&gt; params that &lt;strong&gt;beats frontier models&lt;/strong&gt;), but has anyone released a &lt;strong&gt;helpful shitposting AI yet?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Yeah, didn't think so.&lt;/p&gt; &lt;p&gt;If it &lt;strong&gt;shitposts too hard&lt;/strong&gt;, it is often not that &lt;strong&gt;helpful&lt;/strong&gt;; if it's '&lt;strong&gt;helpful enough&lt;/strong&gt;, the &lt;strong&gt;shitposting ability is often lacking&lt;/strong&gt;. You just couldn't win. &lt;strong&gt;Until now&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Oh, and &lt;strong&gt;no system prompt is needed&lt;/strong&gt;. Just don't let it get stuck in a greentext loop. I might have overcooked the frog a tad bit too fast in the pot for this one.&lt;/p&gt; &lt;p&gt;P.S It writes &lt;strong&gt;HILARIOUS STORIES&lt;/strong&gt;, nothing like a typical AI assistant, see the examples below for details.&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Top tier shitposting&lt;/strong&gt; absolutely unhinged, funny, and witty. Sometimes cringe too; nothing is perfect.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Helpful!&lt;/strong&gt; will actually get shit done.&lt;/li&gt; &lt;li&gt;Will &lt;strong&gt;100% roast you&lt;/strong&gt; for being dumb, thanks to a subtle &lt;strong&gt;negativity bias infusion&lt;/strong&gt;. Very &lt;strong&gt;refreshing!&lt;/strong&gt; ü§å&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep insights&lt;/strong&gt; (when it doesn't delve into absolutely unhinged conspiracy theories about how the water makes the frogs gay).&lt;/li&gt; &lt;li&gt;Built on my &lt;a href="https://huggingface.co/SicariusSicariiStuff/Llama-3.1-Nemotron-8B-UltraLong-1M-Instruct_Abliterated"&gt;UltraLong-1M-Instruct_Abliterated&lt;/a&gt; model, fulfill your dream of a &lt;strong&gt;million-token-long&lt;/strong&gt; shitpost.&lt;/li&gt; &lt;li&gt;Say goodbye to &lt;strong&gt;GPT-isms&lt;/strong&gt; and say hello to &lt;strong&gt;truly creative stories!&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Ships code.&lt;/li&gt; &lt;li&gt;Inclusive toward amphibians.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B"&gt;https://huggingface.co/SicariusSicariiStuff/Assistant_Pepe_8B&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sicarius_The_First"&gt; /u/Sicarius_The_First &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qppjo4/assistant_pepe_8b_1m_context_zero_slop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:03:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8z7z</id>
    <title>Field Report: What leadership actually *treats AI as (Notes from a Dev)</title>
    <updated>2026-01-29T13:46:29+00:00</updated>
    <author>
      <name>/u/MitsotakiShogun</name>
      <uri>https://old.reddit.com/user/MitsotakiShogun</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: Hype. Hype &amp;gt; Substance. In order to woo stockholders. That's it.&lt;/p&gt; &lt;p&gt;Hi fellow llamas,&lt;/p&gt; &lt;p&gt;I read &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1qpsgzr/field_report_what_leadership_actually_thinks_ai/"&gt;this pretty decent post&lt;/a&gt; and while I do agree with lots of the views in that post (even though it's not meant for hobbyists), I thought I'd chime in with a few more thoughts about leadership, and stuff. But before that, let me share some background.&lt;/p&gt; &lt;p&gt;I work at a big company (top 500 by market cap, world), one that actually used AI (which its different names, like statistical/machine learning, NLP, etc) from the early '90s in high-impact domains (adjacent to finance or law, but not quite). The first department head had a published paper on Bayesian statistics for NLP before I was born, and I don't think I understand all of it even now. Decades of NLP work created quite a few useful products, most of which had narrow scope for the AI parts, and the rest was mostly engineering effort and human expert work (reviewing/fixing stuff). We had text-generation models in production at least 4-5 months before ChatGPT (not sure how much more, that's when I transferred from a different business unit).&lt;/p&gt; &lt;p&gt;Fast-forward to today, and management is basically a joke. The last capable (aka engineer/scientist) department head was fired ~3 years ago by the young CTO (who was a Consulting Boy‚Ñ¢), and the interim department heads were also incapable and had short tenures. The current CTO does seem capable and knowledgeable (another engineer), but the middle layers of management are still the same, with most capable people leaving to the bigger firms, and the less capable getting promoted. So let's view &lt;em&gt;how&lt;/em&gt; this happens.&lt;/p&gt; &lt;p&gt;Last year I've been in probably a thousand meetings (like most tech folk, I guess) with managers of all levels, from CTO to managers-in-name only (e.g. directors without any (in)direct reports), to talk about our ongoing AI projects, planned projects, project proposals. The proposals that went through were all about &amp;quot;agents&amp;quot;. If something contained the word, it's probability of getting approved was 418967936.71% higher. I remember a meeting when a scientist and an engineer presented what was essentially an LLM-assisted exhaustive search (multiple data sources) and generation implementation with planning, refinement, draft, human feedback, and final output... and management (CTO, department head, and a couple director) was asking why they didn't use &amp;quot;deep search&amp;quot; and how it can be made agentic. Zero questions about potential issues, zero questions about costs, zero questions about quality. The scientist was so perplexed with those questions, not understand why you would let the LLM decide &lt;em&gt;if it wants&lt;/em&gt; to use search or which databases to query (rather than being forced to use it, and query all databases).&lt;/p&gt; &lt;p&gt;Of course, the problem doesn't stop with management not understanding, and thus promoting the wrong projects and focusing on the wrong metrics (&amp;quot;AI adoption&amp;quot; instead of &amp;quot;revenue increase&amp;quot; / &amp;quot;cost reduction&amp;quot; / ...). This also enables a culture that lets engineers give in to their bad habits and temptations. I know because I've been there too, and it basically boils down to: &amp;quot;Oh look, a shiny new framework! Let's replace all our battle-tested, well-documented tools with this thingy that a single person created in a few months, because it's popular and might be in demand for new jobs and I can put it on my CV&amp;quot;. The newest CTO is trying to curb this trend with a bigger focus on products (which sadly disproportionately affected research output, e.g. publications, open-sourcing), but the middle managers are also trying to showcase the work their teams are doing and thus aim for the flashy stuff that they don't really understand. I've lost track of how many times I've heard my manager speak of using AI in ways that simply don't make any sense.&lt;/p&gt; &lt;p&gt;Perhaps the easiest way to tell is the number of new projects that were started versus what made it in production versus what has &amp;gt;10 users after a year. All AI/ML projects had low success rates (at least for individual experiments, if you hacked at a problem for months and collected data then the rate was much higher), but last year the number of employees trended downwards, the number of projects shot up, and the number of projects that get discarded (decommissioned, merged into others, etc) is also higher than ever.&lt;/p&gt; &lt;p&gt;So when that other post said to not over-engineer solutions when &amp;quot;a script will do&amp;quot;, it wasn't just fluff, it's a real issue that in the past was kept in check by management that &lt;del&gt;didn't butt in too much&lt;/del&gt; trusted its experts, and senior engineers that were too &lt;del&gt;grumpy&lt;/del&gt; uhm... &lt;del&gt;lazy to try to anything new&lt;/del&gt; no, wait... focused on what mattered. You don't need a fucking observability platform and AI code reviews / automated PRs when you cannot even use the &lt;code&gt;logging&lt;/code&gt; library. You don't need the most expensive LLM agents when your prompts writer doesn't even know what templating is, and instead of using structured generation or function calling he asks the LLM to reply with &lt;code&gt;&amp;lt;answer&amp;gt;yes|no&amp;lt;/answer&amp;gt;&lt;/code&gt; which is then parsed without even using regex. And I don't need to come back after a two week vacation to see half my code &amp;quot;refactored&amp;quot; by a dude vibe-coding everything four weeks before the production release deadline.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Sorry, this turned into a rant quicker than I realize. To re-iterate: * upper management tries to appeal to stockholders with hype chasing * middle management tries to appeal to upper management with hype chasing * all management focuses on wrong metrics (e.g. usage of AI copilot, how many products had AI integrated into them) * engineers try to appeal to middle management with hype chasing and also play with new fancy tech * talented folks are leaving for bigger/better companies while the &amp;quot;meh&amp;quot; people remain and get promoted to higher roles and management * proper engineering culture takes a back seat because nobody cares anymore since no incentives promote it&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;AI disclaimer: 100% of this post was hand-typed. Because &lt;del&gt;I'm stupid and like to waste my time on Reddit&lt;/del&gt; thoughts matter more than formatting, but I know how much y'all love your emojis, so here's your daily dosage: ‚úÖüåàü¶Ñüå∏üå∫üåªüåºüå∑üåπüçÄüå¥üåµüå≤üå≥üçéüçèüçêüçäüçãüçåüçâüçáüçìü´êüçàüçíüçëü•≠üççü••ü•ùüçÖüçÜü•ëü•¶ü•¨ü•íüå∂Ô∏èü´ëüåΩü•ïü´íüßÑüßÖü•îüç†ü•êü•Øüçûü•ñü•®üßÄü•ö‚ú®&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MitsotakiShogun"&gt; /u/MitsotakiShogun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8z7z/field_report_what_leadership_actually_treats_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8z7z/field_report_what_leadership_actually_treats_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8z7z/field_report_what_leadership_actually_treats_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:46:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpmay0</id>
    <title>I just got my Dell DGX Spark GB10 that I won from the hackathon!</title>
    <updated>2026-01-28T20:03:20+00:00</updated>
    <author>
      <name>/u/brandon-i</name>
      <uri>https://old.reddit.com/user/brandon-i</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"&gt; &lt;img alt="I just got my Dell DGX Spark GB10 that I won from the hackathon!" src="https://preview.redd.it/af2u39y6a5gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d772153c2ea4d039aff7359c493d9106f7a82aae" title="I just got my Dell DGX Spark GB10 that I won from the hackathon!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Please don't mind the breadcrumbs... &lt;/p&gt; &lt;p&gt;But they pretty much overnighted the Dell DGX Spark GB10.&lt;/p&gt; &lt;p&gt;I think the first thing I am going to try and do is figure out how to get a robot arm to do some sort of shape matching using transfer learning to stick particular shapes in the correct holes. I think that might be easy enough? (I am naive because I haven't done transfer learning or physical AI yet)&lt;/p&gt; &lt;p&gt;I also want to try using LTX and see if it can recreate the ending for How I Met Your Mother or Game of Thrones (if it is able to do that). Might honestly be difficult because I haven't worked with vision models other than image creation using Fal.ai. I wonder if this machine can handle it.&lt;/p&gt; &lt;p&gt;Otherwise, I am going to keep hammering at figuring out better ways of solving the Social Determinants of Health problem. There are a lot of correlations that I wasn't able to completely finish within the limited amount of time for example:&lt;/p&gt; &lt;p&gt;Crime, lack of parks, and food insecurity increases chronic disease risk because people do not feel safe to leave their homes and exercise or walk and often times default to junk food as there are no other culturally sensitive alternatives leading to obesity and higher cardiovascular.&lt;/p&gt; &lt;p&gt;It would be also great if my AI Agents can go through some research paper and identify some of the most crucial ones that I can at least bake into the platform as a baseline that might be effecting other cities.&lt;/p&gt; &lt;p&gt;Also since I have 4 TB SSD I can potentially add the data from a bunch of different cities and start doing some pattern matching/correlation detection between this generally siloed data and see if I could suggest specific campaigns for the cities that would help unrepresented people get better access to care. &lt;/p&gt; &lt;p&gt;One of my passions (and I know this sounds really nerdy) is to create really good multi-turn evaluation harnesses that can use Process Supervised Reward Models to better train complex AI agents and self-heal.&lt;/p&gt; &lt;p&gt;If anyone has advice on any of this I would love to hear it.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brandon-i"&gt; /u/brandon-i &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/af2u39y6a5gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpmay0/i_just_got_my_dell_dgx_spark_gb10_that_i_won_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T20:03:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqakid</id>
    <title>Anyone see the new Acree models?</title>
    <updated>2026-01-29T14:49:48+00:00</updated>
    <author>
      <name>/u/EuphoricPenguin22</name>
      <uri>https://old.reddit.com/user/EuphoricPenguin22</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/arcee-ai/Trinity-Large-Preview"&gt;https://huggingface.co/arcee-ai/Trinity-Large-Preview&lt;/a&gt;&lt;/p&gt; &lt;p&gt;400B w/ 13B active for the large preview model. Free right now via API on OpenRouter (or the Apache 2.0 weights on HuggingFace).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EuphoricPenguin22"&gt; /u/EuphoricPenguin22 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqakid/anyone_see_the_new_acree_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqakid/anyone_see_the_new_acree_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqakid/anyone_see_the_new_acree_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T14:49:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpfse6</id>
    <title>Run Kimi K2.5 Locally</title>
    <updated>2026-01-28T16:17:45+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt; &lt;img alt="Run Kimi K2.5 Locally" src="https://preview.redd.it/rxqfj5os74gg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2606f30079a77f14bb28c31413be651c092abaa9" title="Run Kimi K2.5 Locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi-K2.5 achieves SOTA performance in vision, coding, agentic and chat tasks. &lt;/p&gt; &lt;p&gt;The 1T parameter hybrid reasoning model requires 600GB of disk space, while the quantized &lt;strong&gt;Unsloth Dynamic 1.8-bit&lt;/strong&gt; version reduces this to &lt;strong&gt;240GB (-60% size).&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Model:&lt;/strong&gt; &lt;a href="https://huggingface.co/unsloth/Kimi-K2.5-GGUF"&gt;&lt;strong&gt;Kimi-K2.5-GGUF&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Official Guide:&lt;/strong&gt; &lt;a href="https://unsloth.ai/docs/models/kimi-k2.5"&gt;&lt;strong&gt;https://unsloth.ai/docs/models/kimi-k2.5&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rxqfj5os74gg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpfse6/run_kimi_k25_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T16:17:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpqlfj</id>
    <title>768Gb "Mobile" AI Server Follow-Up Part 1, Look Inside</title>
    <updated>2026-01-28T22:44:03+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"&gt; &lt;img alt="768Gb &amp;quot;Mobile&amp;quot; AI Server Follow-Up Part 1, Look Inside" src="https://external-preview.redd.it/b3IzbXRvY3BwNWdnMU2mkuU7oHD8qNQyskshMOF3z-mD-pRqGUpyoVD2VUXQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8b6cba6ca164006c230852989fb885c68feec072" title="768Gb &amp;quot;Mobile&amp;quot; AI Server Follow-Up Part 1, Look Inside" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey Y'all,&lt;/p&gt; &lt;p&gt;The post I made about the AI server got a lot of buzz, so I decided to do a follow up with some video on the project. Because of reddit's video upload restrictions, I'll have to upload them in separate posts with slightly different focuses, but I've uploaded the full (and higher quality) version to Youtube. Taking the video from 1080p to 720p to meet reddit's video size requirements kinda messed up visibility on the screen record in one of the later parts, so I'll leave a link to the full video here for convenience, otherwise the other parts should get posted here shortly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://youtu.be/TJOKEFdCkv0"&gt;https://youtu.be/TJOKEFdCkv0&lt;/a&gt; &lt;/p&gt; &lt;p&gt;This part primarily focuses on providing some background context on how we came to the W200 in the first place, what it solved for us, and a look inside the unit.&lt;/p&gt; &lt;p&gt;Spec summary:&lt;/p&gt; &lt;p&gt;512Gb DDR4, 256GB VRAM (8x3090+2x5090), 64 core Threadripper Pro 3995WX&lt;/p&gt; &lt;p&gt;Case: Core W200&lt;/p&gt; &lt;p&gt;Appreciate all of the comments and responses on the last post, I've never done anything like this before so I apologize if things are not more polished, attention normally isn't my thing so while the volume of feedback was a little overwhelming the interest was very much encouraging. It seems like every other day we see people post builds here composed of top of the line enterprise hardware with sunken costs reaching tens of thousands of dollars, so I think it can make a difference to just highlight what can be possible with a little ingenuity, consumer grade components, and a more relatively &amp;quot;realistic&amp;quot; budget (in this case, around ~17k usd). Keep this figure in mind when comparing cost:value to these other workstations and their specs/performance capability/creative potential, because I do think this illustrates that effective AI hosting can be more than just throwing money at the problem. Whether someone is working with 100$ or 100k$, focusing on innovative problem solving, pushing optimization limits, and just seeing what can be possible with what's currently available is an order of magnitude more exciting and interesting to see than a squeaky clean $50,000 supercomputer with specialized hardware that very few people will ever get to see in-person within their lifetime posted by someone asking the same question asked since the dawn of time, &amp;quot;what should I do with this?&amp;quot;. Ultimately the interest for experimentation and trying new approaches is what keeps this hobby (local AI) alive and relevant, and imo will be our best counterbalance to the complications that closed-model AI companies impose as we move forward.&lt;/p&gt; &lt;p&gt;Questions welcome.&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/trvmg2cpp5gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpqlfj/768gb_mobile_ai_server_followup_part_1_look_inside/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T22:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpyxfk</id>
    <title>Reasoning Devstral 2</title>
    <updated>2026-01-29T04:42:29+00:00</updated>
    <author>
      <name>/u/Front_Eagle739</name>
      <uri>https://old.reddit.com/user/Front_Eagle739</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fun fact! You can actually make devstral 2 123B &amp;amp; Devstral 24B reason! Accidently had a reasoning forcing jinja template on for another model when I started testing the mlx version of this thing with a couple of reasoning effot = extra high statements in my system prompt because I really wanted more reasoning out of the last model I was using and havving forgotten about that tried devstral 2 and got 2 minutes of reasoning before it answered my test question.&lt;/p&gt; &lt;p&gt;Turns out they are both hybrid reasoners if you put {%- set reasoning_content = 'High' %} in the jinja. Nice clean logical reasoning as well. That's actually fixed my main issue with these models, sometimes you just really need that extra consistency.&lt;/p&gt; &lt;p&gt;Did everybody else know this and I just missed it somehow?&lt;/p&gt; &lt;p&gt;Edit. Seems the smaller one may have some difficulty exiting the thinking, at least with some sampler settings. Big one seems fine though. Quality of response is definitely going way up.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Front_Eagle739"&gt; /u/Front_Eagle739 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpyxfk/reasoning_devstral_2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T04:42:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq0qut</id>
    <title>I built an open-source, multi-agent alternative to OpenAI Prism for research workflows (Verification Agent + LaTeX + PDF)</title>
    <updated>2026-01-29T06:14:18+00:00</updated>
    <author>
      <name>/u/Inside-Scratch4</name>
      <uri>https://old.reddit.com/user/Inside-Scratch4</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working on an open-source project called &lt;strong&gt;Prismer&lt;/strong&gt; to tackle the mess that is the current academic workflow.&lt;/p&gt; &lt;p&gt;Like many of you, I found that using generic LLMs for research often leads to hallucinations, especially with citations. And relying on closed ecosystems like OpenAI‚Äôs Prism wasn‚Äôt ideal for privacy or customization.&lt;/p&gt; &lt;p&gt;So I built &lt;strong&gt;Prismer&lt;/strong&gt;, an all-in-one platform that integrates: &lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;AI-Native PDF Reader&lt;/strong&gt;: With bi-directional citation graphs. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Citation Verification Agent&lt;/strong&gt;: Uses multiple agents to cross-check references against real databases (arXiv, etc.) to prevent LLM hallucinations. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;Jupyter Integration&lt;/strong&gt;: For data analysis right next to your writing. &lt;/li&gt; &lt;li&gt;&lt;strong&gt;LaTeX Editor&lt;/strong&gt;: With real-time preview.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It‚Äôs completely open-source (MIT License). The goal is to have a modular system where you can swap in your own models or agents.&lt;/p&gt; &lt;p&gt;I‚Äôd love to get some feedback from this community on the agent orchestration part specifically.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/Prismer-AI/Prismer"&gt;https://github.com/Prismer-AI/Prismer&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Let me know what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Inside-Scratch4"&gt; /u/Inside-Scratch4 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq0qut/i_built_an_opensource_multiagent_alternative_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T06:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqc1fx</id>
    <title>Run Local LLMs with Claude Code &amp; OpenAI Codex</title>
    <updated>2026-01-29T15:43:54+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqc1fx/run_local_llms_with_claude_code_openai_codex/"&gt; &lt;img alt="Run Local LLMs with Claude Code &amp;amp; OpenAI Codex" src="https://preview.redd.it/46s35meo6bgg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a5c4d381bee94911400914067a46722f3b88a4a" title="Run Local LLMs with Claude Code &amp;amp; OpenAI Codex" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This step-by-step guide shows you how to connect open LLMs to Claude Code and Codex entirely locally. &lt;/p&gt; &lt;p&gt;Run using any open model like DeepSeek, Qwen, Gemma etc.&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://unsloth.ai/docs/basics/claude-codex"&gt;https://unsloth.ai/docs/basics/claude-codex&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/46s35meo6bgg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqc1fx/run_local_llms_with_claude_code_openai_codex/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqc1fx/run_local_llms_with_claude_code_openai_codex/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T15:43:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqbkk4</id>
    <title>GLM 4.7 flash Q6 thought for 1400 minutes. 2000 lines of thoughts, had to be stopped.</title>
    <updated>2026-01-29T15:27:00+00:00</updated>
    <author>
      <name>/u/regjoe13</name>
      <uri>https://old.reddit.com/user/regjoe13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqbkk4/glm_47_flash_q6_thought_for_1400_minutes_2000/"&gt; &lt;img alt="GLM 4.7 flash Q6 thought for 1400 minutes. 2000 lines of thoughts, had to be stopped." src="https://b.thumbs.redditmedia.com/JgX3rA4cwLQskKgGIifQhm7pes24EQPx7ALGs87m29A.jpg" title="GLM 4.7 flash Q6 thought for 1400 minutes. 2000 lines of thoughts, had to be stopped." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tryed this model for the first time. Asked a simple question, and forgot about it. Today morning I still see it thinking. Thankfully I stopped it before it became sentient.&lt;br /&gt; 3090, 3060 dual, 96GB RAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regjoe13"&gt; /u/regjoe13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qqbkk4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqbkk4/glm_47_flash_q6_thought_for_1400_minutes_2000/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqbkk4/glm_47_flash_q6_thought_for_1400_minutes_2000/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T15:27:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq64rx</id>
    <title>Kimi K2.5, a Sonnet 4.5 alternative for a fraction of the cost</title>
    <updated>2026-01-29T11:31:03+00:00</updated>
    <author>
      <name>/u/Grand-Management657</name>
      <uri>https://old.reddit.com/user/Grand-Management657</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yes you read the title correctly. Kimi K2.5 is THAT good.&lt;/p&gt; &lt;p&gt;I would place it around Sonnet 4.5 level quality. It‚Äôs great for agentic coding and uses structured to-do lists similar to other frontier models, so it‚Äôs able to work autonomously like Sonnet or Opus.&lt;/p&gt; &lt;p&gt;It's thinking is very methodical and highly logical, so its not the best at creative writing but the tradeoff is that it is very good for agentic use.&lt;/p&gt; &lt;p&gt;The move from K2 -&amp;gt; K2.5 brought multimodality, which means that you can drive it to self-verify changes. Prior to this, I used antigravity almost exclusively because of its ability to drive the browser agent to verify its changes. This is now a core agentic feature of K2.5. It can build the app, open it in a browser, take a screenshot to see if it rendered correctly, and then loop back to fix the UI based on what it &amp;quot;saw&amp;quot;. Hookup playwright or vercel's browser-agent and you're good to go.&lt;/p&gt; &lt;p&gt;Now like I said before, I would still classify Opus 4.5 as superior outside of JS or TS environments. If you are able to afford it you should continue using Opus, especially for complex applications. &lt;/p&gt; &lt;p&gt;But for many workloads the best economical and capable pairing would be Opus as an orchestrator/planner + Kimi K2.5 as workers/subagents. This way you save a ton of money while getting 99% of the performance (depending on your workflow).&lt;/p&gt; &lt;p&gt;+ You don't have to be locked into a single provider for it to work.&lt;/p&gt; &lt;p&gt;+ Screw closed source models.&lt;/p&gt; &lt;p&gt;+ Spawn hundreds of parallel agents like you've always wanted WITHOUT despawning your bank account.&lt;/p&gt; &lt;p&gt;&lt;em&gt;Btw this is coming from someone who very much disliked GLM 4.7 and thought it was benchmaxxed to the moon&lt;/em&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Grand-Management657"&gt; /u/Grand-Management657 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq64rx/kimi_k25_a_sonnet_45_alternative_for_a_fraction/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq64rx/kimi_k25_a_sonnet_45_alternative_for_a_fraction/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq64rx/kimi_k25_a_sonnet_45_alternative_for_a_fraction/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:31:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq29ab</id>
    <title>Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description.</title>
    <updated>2026-01-29T07:40:59+00:00</updated>
    <author>
      <name>/u/VirtualJamesHarrison</name>
      <uri>https://old.reddit.com/user/VirtualJamesHarrison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"&gt; &lt;img alt="Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description." src="https://external-preview.redd.it/NGZpZjMyeWJwOGdnMSyVzMY88rGIMLP8wkCsphE6OdlDVcwcn9ECGq-UAL8f.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3fc37bfffb290e2fd1af99cd3cb5f3e90281514f" title="Using a LLM to procedurally generate spells for a VR prototype. Oh and Stick based sound track (listen to the lyrics). Full tech details in description." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The system works by having a pool of 200 spell components like explosive or change color. A LLM then converts each word into a set of component instructions.&lt;/p&gt; &lt;p&gt;For example &amp;quot;explode&amp;quot; = explosive + change color + apply force.&lt;/p&gt; &lt;p&gt;This means we can have a system that can generate a spell for literally any word.&lt;/p&gt; &lt;p&gt;Stick based music was made with Suno.&lt;/p&gt; &lt;p&gt;It's still early Alpha, but if you want to help me break it or try to find hidden spells, come join the Discord: &lt;a href="https://discord.com/invite/VjZQcjtfDq"&gt;https://discord.com/invite/VjZQcjtfDq&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/VirtualJamesHarrison"&gt; /u/VirtualJamesHarrison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hbq4wsxbp8gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq29ab/using_a_llm_to_procedurally_generate_spells_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T07:40:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qqadna</id>
    <title>My humble GLM 4.7 Flash appreciation post</title>
    <updated>2026-01-29T14:42:32+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"&gt; &lt;img alt="My humble GLM 4.7 Flash appreciation post" src="https://preview.redd.it/jh83y5tqqagg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=84c7118a2d6ca354bab71459f8fd90766a909deb" title="My humble GLM 4.7 Flash appreciation post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was impressed by GLM 4.7 Flash performance, but not surprised, because I knew they could make an outstanding model that will leave most of the competitor models around the same size in the dust.&lt;/p&gt; &lt;p&gt;However I was wondering how good it really is, so I got an idea to use Artificial Analysis to put together all the similar sized open weight models I could think of at that time (or at least the ones available there for selection) and check out their benchmarks against each other to see how are they all doing.&lt;/p&gt; &lt;p&gt;To make things more interesting, I decided to throw in some of the best Gemini models for comparison and well... I knew the model was good, but this good? I don't think we can appreciate this little gem enough, just look who's there daring to get so close to the big guys. üòâ&lt;/p&gt; &lt;p&gt;This graph makes me wonder - Could it be that 30B-A3B or similar model sizes might eventually be enough to compete with today's big models? Because to me it looks that way and I have a strong belief that ZAI has what it takes to get us there and I think it's amazing that we have a model of this size and quality at home now.&lt;/p&gt; &lt;p&gt;Thank you, ZAI! ‚ù§&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/jh83y5tqqagg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qqadna/my_humble_glm_47_flash_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T14:42:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq401x</id>
    <title>I built an open-source, local-first voice cloning studio (Qwen3-TTS + Whisper)</title>
    <updated>2026-01-29T09:26:48+00:00</updated>
    <author>
      <name>/u/jamiepine</name>
      <uri>https://old.reddit.com/user/jamiepine</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been working on an open-source project called Voicebox.&lt;/p&gt; &lt;p&gt;Qwen3-TTS blew my mind when it dropped, crazy good cloning from seconds of audio, low latency, and open. I started playing around, but got annoyed re-cloning the same voices every session. So I built a quick saver for profiles... and it snowballed into &lt;strong&gt;Voicebox&lt;/strong&gt;, my attempt at the &amp;quot;Ollama for voice.&amp;quot;&lt;/p&gt; &lt;p&gt;It's a native desktop app (Tauri/Rust/Python, super lightweight‚Äîno Electron bloat or Python setup for users). Everything local, private, offline.&lt;/p&gt; &lt;p&gt;Main bits:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clone voices instantly with Qwen3-TTS (single or multi-sample for better quality)&lt;/li&gt; &lt;li&gt;DAW-like multi-track timeline to compose conversations/podcasts/narratives&lt;/li&gt; &lt;li&gt;In-app system audio/mic recording + Whisper transcription&lt;/li&gt; &lt;li&gt;REST API + one-click local server for integrating into games/apps/agents&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MIT open-source, early stage (v0.1.x).&lt;br /&gt; Repo: &lt;a href="https://github.com/jamiepine/voicebox"&gt;https://github.com/jamiepine/voicebox&lt;/a&gt;&lt;br /&gt; Downloads: &lt;a href="https://voicebox.sh/"&gt;https://voicebox.sh&lt;/a&gt; (macOS/Windows now; Linux soon)&lt;/p&gt; &lt;p&gt;Planning XTTS, Bark, etc. next. What models do you want most? Any feedback if you try it‚Äîbugs, missing features, workflow pains?&lt;/p&gt; &lt;p&gt;Give it a spin and lmk what you think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jamiepine"&gt; /u/jamiepine &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq401x/i_built_an_opensource_localfirst_voice_cloning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T09:26:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8rpu</id>
    <title>[News] ACE-Step 1.5 Preview - Now requires &lt;4GB VRAM, 100x faster generation</title>
    <updated>2026-01-29T13:37:50+00:00</updated>
    <author>
      <name>/u/ExcellentTrust4433</name>
      <uri>https://old.reddit.com/user/ExcellentTrust4433</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8rpu/news_acestep_15_preview_now_requires_4gb_vram/"&gt; &lt;img alt="[News] ACE-Step 1.5 Preview - Now requires &amp;lt;4GB VRAM, 100x faster generation" src="https://preview.redd.it/9x51tk85kagg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=040668466faf2131a4ff169fa2fe22f761361864" title="[News] ACE-Step 1.5 Preview - Now requires &amp;lt;4GB VRAM, 100x faster generation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Fresh from the ACE-Step Discord - preview of the v1.5 README!&lt;/p&gt; &lt;p&gt;Key improvements:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;**&amp;lt;4GB VRAM** (down from 8GB in v1!) - true consumer hardware&lt;/li&gt; &lt;li&gt;**100x faster** than pure LM architectures&lt;/li&gt; &lt;li&gt;Hybrid LM + DiT architecture with Chain-of-Thought&lt;/li&gt; &lt;li&gt;10-minute compositions, 50+ languages&lt;/li&gt; &lt;li&gt;Cover generation, repainting, vocal-to-BGM&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Release should be imminent!&lt;/p&gt; &lt;p&gt;Also check &lt;a href="/r/ACEStepGen"&gt;r/ACEStepGen&lt;/a&gt; for dedicated discussions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExcellentTrust4433"&gt; /u/ExcellentTrust4433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9x51tk85kagg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8rpu/news_acestep_15_preview_now_requires_4gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8rpu/news_acestep_15_preview_now_requires_4gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:37:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq8e2x</id>
    <title>Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face</title>
    <updated>2026-01-29T13:21:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face" src="https://external-preview.redd.it/7bBjSbi8Jb_ZIxPLdQlxsAX41TayP_Nw4jr5gGuqpXw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=807a024098d7c29e7df6eb6cac205fdc9b7cdeb4" title="Qwen/Qwen3-ASR-1.7B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Qwen3-ASR family includes Qwen3-ASR-1.7B and Qwen3-ASR-0.6B, which support language identification and ASR for 52 languages and dialects. Both leverage large-scale speech training data and the strong audio understanding capability of their foundation model, Qwen3-Omni. Experiments show that the 1.7B version achieves state-of-the-art performance among open-source ASR models and is competitive with the strongest proprietary commercial APIs. Here are the main features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;All-in-one&lt;/strong&gt;: Qwen3-ASR-1.7B and Qwen3-ASR-0.6B support language identification and speech recognition for 30 languages and 22 Chinese dialects, so as to English accents from multiple countries and regions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Excellent and Fast&lt;/strong&gt;: The Qwen3-ASR family ASR models maintains high-quality and robust recognition under complex acoustic environments and challenging text patterns. Qwen3-ASR-1.7B achieves strong performance on both open-sourced and internal benchmarks. While the 0.6B version achieves accuracy-efficient trade-off, it reaches 2000 times throughput at a concurrency of 128. They both achieve streaming / offline unified inference with single model and support transcribe long audio.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Novel and strong forced alignment Solution&lt;/strong&gt;: We introduce Qwen3-ForcedAligner-0.6B, which supports timestamp prediction for arbitrary units within up to 5 minutes of speech in 11 languages. Evaluations show its timestamp accuracy surpasses E2E based forced-alignment models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Comprehensive inference toolkit&lt;/strong&gt;: In addition to open-sourcing the architectures and weights of the Qwen3-ASR series, we also release a powerful, full-featured inference framework that supports vLLM-based batch inference, asynchronous serving, streaming inference, timestamp prediction, and more.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-ASR-1.7B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq8e2x/qwenqwen3asr17b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T13:21:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq5zdr</id>
    <title>I built an 80M parameter LLM from scratch using the same architecture as Llama 3 - here's what I learned</title>
    <updated>2026-01-29T11:22:46+00:00</updated>
    <author>
      <name>/u/Routine-Thanks-572</name>
      <uri>https://old.reddit.com/user/Routine-Thanks-572</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share Mini-LLM, a complete implementation of a modern transformer language model built entirely from scratch.&lt;/p&gt; &lt;h1&gt;What makes this different from most educational projects?&lt;/h1&gt; &lt;p&gt;Most tutorials use outdated techniques (learned position embeddings, LayerNorm, character-level tokenization). Mini-LLM implements the &lt;strong&gt;exact same components as Llama 3&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;RoPE&lt;/strong&gt; (Rotary Position Embeddings) - scales to longer sequences&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RMSNorm&lt;/strong&gt; - faster and more stable than LayerNorm&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SwiGLU&lt;/strong&gt; - state-of-the-art activation function&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Grouped Query Attention&lt;/strong&gt; - efficient inference&lt;/li&gt; &lt;li&gt;&lt;strong&gt;SentencePiece BPE&lt;/strong&gt; - real-world tokenization with 32K vocab&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Complete Pipeline&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Custom tokenizer ‚Üí Data processing ‚Üí Training ‚Üí Inference&lt;/li&gt; &lt;li&gt;Memory-mapped data loading (TB-scale ready)&lt;/li&gt; &lt;li&gt;Mixed precision training with gradient accumulation&lt;/li&gt; &lt;li&gt;KV caching for fast generation&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;80M parameters trained on 361M tokens&lt;/li&gt; &lt;li&gt;5 hours on single A100, final loss ~3.25&lt;/li&gt; &lt;li&gt;Generates coherent text with proper grammar&lt;/li&gt; &lt;li&gt;200-500 tokens/sec inference speed&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try it yourself&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/Ashx098/Mini-LLM"&gt;https://github.com/Ashx098/Mini-LLM&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;HuggingFace:&lt;/strong&gt; &lt;a href="https://huggingface.co/Ashx098/Mini-LLM"&gt;https://huggingface.co/Ashx098/Mini-LLM&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is clean, well-documented, and designed for learning. Every component has detailed explanations of the &amp;quot;why&amp;quot; not just the &amp;quot;how&amp;quot;.&lt;/p&gt; &lt;p&gt;Perfect for students wanting to understand modern LLM architecture without drowning in billion-parameter codebases!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Routine-Thanks-572"&gt; /u/Routine-Thanks-572 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq5zdr/i_built_an_80m_parameter_llm_from_scratch_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:22:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq67io</id>
    <title>OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion</title>
    <updated>2026-01-29T11:35:08+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"&gt; &lt;img alt="OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion" src="https://external-preview.redd.it/anhiOGswbTh5OWdnMQLRfJQU73a9pcHTIBGMkMlYp-rLlT5zwChrnU104y5M.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=75356aa649008b072137cfe5ab634a7b54be4fd5" title="OpenMOSS just released MOVA (MOSS-Video-and-Audio) - Fully Open-Source - 18B Active Params (MoE Architecture, 32B in total) - Day-0 support for SGLang-Diffusion" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: MOVA: Towards Scalable and Synchronized Video‚ÄìAudio Generation: &lt;a href="https://github.com/OpenMOSS/MOVA"&gt;https://github.com/OpenMOSS/MOVA&lt;/a&gt;&lt;br /&gt; MOVA-360: &lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-360p"&gt;https://huggingface.co/OpenMOSS-Team/MOVA-360p&lt;/a&gt;&lt;br /&gt; MOVA-720p: &lt;a href="https://huggingface.co/OpenMOSS-Team/MOVA-720p"&gt;https://huggingface.co/OpenMOSS-Team/MOVA-720p&lt;/a&gt;&lt;br /&gt; From OpenMOSS on ùïè: &lt;a href="https://x.com/Open_MOSS/status/2016820157684056172"&gt;https://x.com/Open_MOSS/status/2016820157684056172&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6n89xfl8y9gg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq67io/openmoss_just_released_mova_mossvideoandaudio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:35:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qq6n3t</id>
    <title>GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week.</title>
    <updated>2026-01-29T11:58:22+00:00</updated>
    <author>
      <name>/u/Distinct-Expression2</name>
      <uri>https://old.reddit.com/user/Distinct-Expression2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"&gt; &lt;img alt="GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week." src="https://preview.redd.it/uf2m03ak2agg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=02bc2552e04fbb090e9eb5df2979a536c39ef524" title="GitHub trending this week: half the repos are agent frameworks. 90% will be dead in 1 week." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It this the js framework hell moment of ai?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Distinct-Expression2"&gt; /u/Distinct-Expression2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/uf2m03ak2agg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qq6n3t/github_trending_this_week_half_the_repos_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-29T11:58:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qp46za</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)</title>
    <updated>2026-01-28T06:54:28+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" src="https://preview.redd.it/y2qj7ancf1gg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bb1df11c9d46ca94be0db3438449dc28e2dd48e" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2.5 SoTA Model (Wednesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; üëã&lt;/p&gt; &lt;p&gt;We're excited for Wednesday's guests, &lt;strong&gt;The Moonshot AI Lab Team!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kicking things off Wednesday, Jan. 28th, 8 AM‚Äì11 AM PST&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;‚ö†Ô∏è &lt;strong&gt;Note:&lt;/strong&gt; The AMA itself will be hosted in a &lt;strong&gt;separate thread,&lt;/strong&gt; please don‚Äôt post questions here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/y2qj7ancf1gg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qp46za/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T06:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
