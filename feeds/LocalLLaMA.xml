<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-28T21:24:21+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pxvp4t</id>
    <title>Anyone running 4x RTX Pro 6000s stacked directly on top of each other?</title>
    <updated>2025-12-28T16:37:35+00:00</updated>
    <author>
      <name>/u/Comfortable-Plate467</name>
      <uri>https://old.reddit.com/user/Comfortable-Plate467</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"&gt; &lt;img alt="Anyone running 4x RTX Pro 6000s stacked directly on top of each other?" src="https://a.thumbs.redditmedia.com/ORhdMeatICbcQ3aVXDfxgI0ZMabVn4Bvql6IkdtE5D4.jpg" title="Anyone running 4x RTX Pro 6000s stacked directly on top of each other?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ovmd5a522z9g1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ec42305e26873179763f77c1d3d2a1bf972623e4"&gt;https://preview.redd.it/ovmd5a522z9g1.jpg?width=4000&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ec42305e26873179763f77c1d3d2a1bf972623e4&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Is anyone here actually running a quad RTX Pro 6000 setup with the cards sandwiched together? I‚Äôve got two right now and I‚Äôm looking to add two more. My thinking is that since the cool air should flow from bottom to top through each card, the thermals might be manageable. Has anyone tried this? I really want to avoid using riser cables‚Äîthey‚Äôre such a mess and a total pain to deal with&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Plate467"&gt; /u/Comfortable-Plate467 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxvp4t/anyone_running_4x_rtx_pro_6000s_stacked_directly/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T16:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1py1pao</id>
    <title>Minimax Non Api Account Issue</title>
    <updated>2025-12-28T20:34:07+00:00</updated>
    <author>
      <name>/u/ballshuffington</name>
      <uri>https://old.reddit.com/user/ballshuffington</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I was wondering if any of you guys know, one of you guys must know someone on the Minimax team or be able to get a hold of them. I would love to use their chat models without, you know, having to use the API (which I already use) . However, I, and most likely a whole bunch of other users, tend to stay away from Google because of its privacy practices. The only way to create an account via their chat platform is to have a Google account. I think it would be a win-win if they set up an email password sign-in as well. So then a whole bunch of other users could use their platform without letting Google see everything that we do and just being less, less reliant on it in general. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ballshuffington"&gt; /u/ballshuffington &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1pao/minimax_non_api_account_issue/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1pao/minimax_non_api_account_issue/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py1pao/minimax_non_api_account_issue/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T20:34:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxbg4x</id>
    <title>SOCAMM2 - new(ish), screwable (replaceable, non soldered) LPDDR5X RAM standard intended for AI data centers.</title>
    <updated>2025-12-27T23:10:47+00:00</updated>
    <author>
      <name>/u/-InformalBanana-</name>
      <uri>https://old.reddit.com/user/-InformalBanana-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Samsung introduces SOCAMM2 LPDDR5X memory module for AI data centers ‚Äî new standard set to offer reduced power consumption and &lt;strong&gt;double the bandwidth&lt;/strong&gt; versus DDR5 RDIMMs.&lt;/p&gt; &lt;p&gt;The SOCAMM2 LPDDR5X-based module is being positioned as a standardized, serviceable alternative to soldered memory as AI servers chase higher bandwidth.&lt;/p&gt; &lt;p&gt;Hopefully this gets represented and used more in the consumer market.&lt;/p&gt; &lt;p&gt;More info:&lt;/p&gt; &lt;p&gt;&lt;a href="https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/"&gt;https://semiconductor.samsung.com/news-events/tech-blog/introducing-samsungs-socamm2-new-lpddr-memory-module-empowering-next-generation-ai-infrastructure/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers"&gt;https://www.tomshardware.com/tech-industry/samsung-introduces-socamm2-lpddr5x-memory-module-for-ai-data-centers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/-InformalBanana-"&gt; /u/-InformalBanana- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxbg4x/socamm2_newish_screwable_replaceable_non_soldered/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T23:10:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1py294m</id>
    <title>GLM 4.5 Air and agentic CLI tools/TUIs?</title>
    <updated>2025-12-28T20:56:33+00:00</updated>
    <author>
      <name>/u/bfroemel</name>
      <uri>https://old.reddit.com/user/bfroemel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I revisited GLM 4.5 Air and at least on llama.cpp I am able to get stable tool calls with unsloth's UD_Q4_K_XL (unsloth updated the weights on HF a couple of days ago); that's probably thanks to: &lt;a href="https://github.com/ggml-org/llama.cpp/pull/16932"&gt;https://github.com/ggml-org/llama.cpp/pull/16932&lt;/a&gt; and maybe unsloth (there is no changelog/reason why they recently updated the weights).&lt;/p&gt; &lt;p&gt;Unfortunately with codex-cli sometimes the model becomes stuck at constantly doing the same tool call; maybe it was just bad luck in combination with the set of MCPs, quantization related instability, bad sampling parameters, or there could be some functionality within codex-cli missing to properly engage with GLM 4.5 Air.&lt;/p&gt; &lt;p&gt;Is anyone seriously using GLM 4.5 Air locally for agentic coding (e.g., having it reliably do 10 to 50 tool calls in a single agent round) and has some hints regarding well-working coding TUIs? (ofc I am not expecting that GLM 4.5 Air can solve all tasks, but it imo shouldn't get stuck in tool-calling loops and/or I might be just spoiled by other models not doing that.)&lt;/p&gt; &lt;p&gt;p.s., relevant llama.cpp parameters (derived from unsloth's GLM 4.6V flash docs (no GLM 4.5 Air docs) and temperature recommendation from zai labs):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--ctx-size 128000 --temp 0.6 --top-p 0.6 --top-k 2 --min-p 0.0 --jinja &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bfroemel"&gt; /u/bfroemel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py294m/glm_45_air_and_agentic_cli_toolstuis/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py294m/glm_45_air_and_agentic_cli_toolstuis/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py294m/glm_45_air_and_agentic_cli_toolstuis/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T20:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxp9rg</id>
    <title>Seeking advice from developers building apps with ML/DL integration</title>
    <updated>2025-12-28T11:32:29+00:00</updated>
    <author>
      <name>/u/hemahariharansamson</name>
      <uri>https://old.reddit.com/user/hemahariharansamson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I am planning to build apps and websites that solve real-world problems. My goal is not just to create normal CRUD or UI-focused apps, but also to gradually integrate my own machine learning and deep learning models into these products and services.&lt;/p&gt; &lt;p&gt;I‚Äôve been experimenting with AI-assisted development tools like Cursor to speed up design and coding, but I want to learn from the community about what works best in practice.&lt;/p&gt; &lt;p&gt;I‚Äôd love to hear from you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What is your go-to AI tool for development like Cursor?&lt;/li&gt; &lt;li&gt;What subscription plan or setup do you use?&lt;/li&gt; &lt;li&gt;Any tips for integrating custom ML/DL models into real apps?&lt;/li&gt; &lt;li&gt;Recommended tech stacks, workflows, or common pitfalls for beginners building production-ready apps&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Looking forward to your advice. Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hemahariharansamson"&gt; /u/hemahariharansamson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxp9rg/seeking_advice_from_developers_building_apps_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T11:32:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxr47l</id>
    <title>MCP servers are hard to debug and impossible to test, so I built Syrin</title>
    <updated>2025-12-28T13:16:33+00:00</updated>
    <author>
      <name>/u/hack_the_developer</name>
      <uri>https://old.reddit.com/user/hack_the_developer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I‚Äôve been building MCP servers and kept running into the same issues:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;No visibility into why an LLM picked a tool&lt;/li&gt; &lt;li&gt;Tool calls looping or failing silently&lt;/li&gt; &lt;li&gt;No deterministic way to test MCP behaviour&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So I built &lt;strong&gt;Syrin,&lt;/strong&gt; a local-first &lt;strong&gt;CLI debugger and test runner for MCP servers&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does (v1.0.0):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;CLI commands: &lt;code&gt;syrin init&lt;/code&gt;, &lt;code&gt;doctor&lt;/code&gt;, &lt;code&gt;test&lt;/code&gt;, &lt;code&gt;list&lt;/code&gt;, &lt;code&gt;dev&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Full MCP protocol support (tools, resources, prompts, validation)&lt;/li&gt; &lt;li&gt;Multi-LLM support: OpenAI, Claude, Ollama (auto-manages Ollama)&lt;/li&gt; &lt;li&gt;Safe-by-default execution (preview mode + full event tracing)&lt;/li&gt; &lt;li&gt;YAML config, HTTP + stdio transport&lt;/li&gt; &lt;li&gt;TypeScript, npm package, npx-friendly&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What I‚Äôm working on next:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Deterministic &lt;strong&gt;unit tests for tools&lt;/strong&gt; (was it called? with what args?)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Workflow testing&lt;/strong&gt; for multi-step tool chains with dependencies&lt;/li&gt; &lt;li&gt;Assertions on runtime events, not model text&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ankan-labs/syrin"&gt;&lt;strong&gt;https://github.com/ankan-labs/syrin&lt;/strong&gt;&lt;/a&gt;&lt;br /&gt; &lt;strong&gt;NPM:&lt;/strong&gt; &lt;a href="https://www.npmjs.com/package/@ankan-ai/syrin"&gt;&lt;strong&gt;https://www.npmjs.com/package/@ankan-ai/syrin&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you‚Äôre building MCP servers, I‚Äôd love feedback or contributors.&lt;br /&gt; If this is the wrong approach, tell me why.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hack_the_developer"&gt; /u/hack_the_developer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxr47l/mcp_servers_are_hard_to_debug_and_impossible_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T13:16:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxs4us</id>
    <title>What's a good small model for generating tags from text content?</title>
    <updated>2025-12-28T14:05:34+00:00</updated>
    <author>
      <name>/u/ghulamalchik</name>
      <uri>https://old.reddit.com/user/ghulamalchik</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm using Karakeep which is a bookmark system for links. They offer using Ollama/Open Router/OpenAI for auto generating tag.&lt;/p&gt; &lt;p&gt;First of all, are tiny models capable of doing this task? By tiny I mean maybe 200m, 500m. If not, what could be the best smallest option? I'm currently using Mistral 7b, it's not the best, but it's not bad either.&lt;/p&gt; &lt;p&gt;I wonder if I can get better results with another model, and if it can be smaller too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ghulamalchik"&gt; /u/ghulamalchik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxs4us/whats_a_good_small_model_for_generating_tags_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxs4us/whats_a_good_small_model_for_generating_tags_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxs4us/whats_a_good_small_model_for_generating_tags_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:05:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxl1k1</id>
    <title>I built a frontend for stable-diffusion.cpp for local image generation</title>
    <updated>2025-12-28T07:06:16+00:00</updated>
    <author>
      <name>/u/fabricio3g</name>
      <uri>https://old.reddit.com/user/fabricio3g</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a Front End of stable-diffusion-cpp to run localy Z-Image Turbo on my old vulkan compatible integrated GPU using stable-diffusion.cpp. &lt;/p&gt; &lt;p&gt;The code is a messy but works for my needs. Some features aren‚Äôt fully tested due to my weak GPU. The project is open source and open to contributions.&lt;/p&gt; &lt;p&gt;Currently: Run with npm start&lt;/p&gt; &lt;p&gt;Windows build not working &lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/fabricio3g/FlaxeoUI"&gt;https://github.com/fabricio3g/FlaxeoUI&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fabricio3g"&gt; /u/fabricio3g &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxl1k1/i_built_a_frontend_for_stablediffusioncpp_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T07:06:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxj4lv</id>
    <title>Day 20: 21 Days of Building a Small Language Model: Activation Functions</title>
    <updated>2025-12-28T05:18:48+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Welcome to Day 20 of 21 Days of Building a Small Language Model. The topic for today is activation functions, the components that give neural networks their ability to learn complex, non-linear patterns. Yesterday we explored residual connections and how they enable deep networks. Today, we'll discover how activation functions work, why they're essential, and how modern choices like SwiGLU have become the standard in state-of-the-art language models.&lt;/p&gt; &lt;h1&gt;Why Activation Functions matter&lt;/h1&gt; &lt;p&gt;Before we dive into specific activation functions, let's understand why they're essential. A neural network without activation functions is just a series of matrix multiplications. No matter how many layers you stack, the result is always a linear transformation. This means the network can only learn linear relationships, which is extremely limiting.&lt;/p&gt; &lt;p&gt;Activation functions introduce non-linearity, allowing networks to learn complex patterns. They're what enable neural networks to approximate any function, recognize images, understand language, and solve problems that linear models cannot. Without activation functions, neural networks would be no more powerful than simple linear regression.&lt;/p&gt; &lt;p&gt;But not all activation functions are created equal. The choice of activation function affects training stability, convergence speed, gradient flow, and final model performance. This is why understanding activation functions is crucial for building effective language models.&lt;/p&gt; &lt;h1&gt;Evolution: From ReLU to SwiGLU&lt;/h1&gt; &lt;p&gt;The history of activation functions in deep learning shows a clear evolution toward smoother, more effective functions. Let's trace this journey:&lt;/p&gt; &lt;h1&gt;ReLU&lt;/h1&gt; &lt;p&gt;ReLU (Rectified Linear Unit) was the breakthrough that made deep learning practical. It's defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;ReLU(x) = max(0, x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;ReLU is simple: if the input is positive, output the input; if negative, output zero. This simplicity made it fast to compute and helped with the vanishing gradient problem that plagued earlier activation functions like sigmoid and tanh.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why ReLU worked:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fast computation (just a max operation)&lt;/li&gt; &lt;li&gt;Helps with vanishing gradients (gradient is 1 for positive inputs)&lt;/li&gt; &lt;li&gt;Sparse activations (many neurons output zero, creating sparsity)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Dead neurons: Once a neuron outputs zero, it may never recover (dying ReLU problem)&lt;/li&gt; &lt;li&gt;Not smooth: The function has a sharp corner at zero, which can cause issues&lt;/li&gt; &lt;li&gt;Zero gradient for negative inputs: No learning happens for negative values&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;GELU&lt;/h1&gt; &lt;p&gt;GELU (Gaussian Error Linear Unit) addressed some of ReLU's limitations by being smooth and differentiable everywhere. It's defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GELU(x) = x √ó Œ¶(x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;where Œ¶(x) is the cumulative distribution function of the standard normal distribution. GELU is smooth, meaning it has no sharp corners, which helps with gradient flow.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why GELU became popular:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth and differentiable everywhere&lt;/li&gt; &lt;li&gt;Better gradient flow than ReLU&lt;/li&gt; &lt;li&gt;Works well in transformers (used in BERT, GPT-2)&lt;/li&gt; &lt;li&gt;More stable training, especially for language models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;GELU's characteristics:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth transition instead of sharp cutoff&lt;/li&gt; &lt;li&gt;Allows small negative values to pass through (unlike ReLU)&lt;/li&gt; &lt;li&gt;Better for tasks requiring fine-grained control&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Swish/SiLU&lt;/h1&gt; &lt;p&gt;Swish (also called SiLU, Sigmoid Linear Unit) is defined as:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Swish(x) = x √ó sigmoid(x) = x √ó (1 / (1 + e^(-x)))&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Swish is smooth like GELU but has been shown to work better in many applications. It's non-monotonic (can decrease for negative inputs), which gives it more flexibility than ReLU.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why Swish works well:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Smooth and differentiable everywhere&lt;/li&gt; &lt;li&gt;Non-monotonic behavior provides more expressiveness&lt;/li&gt; &lt;li&gt;Better gradient flow than ReLU&lt;/li&gt; &lt;li&gt;Proven effective in modern language models&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;SwiGLU&lt;/h1&gt; &lt;p&gt;SwiGLU (Swish-Gated Linear Unit) takes Swish and adds a gating mechanism. Instead of just applying Swish to a transformation, SwiGLU uses two parallel paths:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SwiGLU(x) = Swish(W_gate √ó x) ‚äô (W_up √ó x)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;where ‚äô is element-wise multiplication. The key innovation is the gating mechanism: one path gets activated (the gate), and the other doesn't (the up projection). The gate controls how much of the unactivated path passes through.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why SwiGLU is powerful:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gating mechanism allows more complex transformations&lt;/li&gt; &lt;li&gt;The gate can selectively pass or block information&lt;/li&gt; &lt;li&gt;More expressive than simple activation functions&lt;/li&gt; &lt;li&gt;Has become the standard in modern models like Qwen, LLaMA, and GPT&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;My Experience&lt;/h1&gt; &lt;p&gt;From working with different activation functions in practice, here's what I've learned:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;For small models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GELU is often the safe, reliable choice. It provides good stability and performance without the extra parameters of gated variants.&lt;/li&gt; &lt;li&gt;SwiGLU can provide better performance but requires more parameters. For small models where every parameter counts, the trade-off isn't always worth it.&lt;/li&gt; &lt;li&gt;ReLU can work but is less stable, especially early in training. I avoid it unless I have a specific reason.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;For Larger models:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;SwiGLU has become the standard. The extra parameters are worth it for the improved expressiveness and performance.&lt;/li&gt; &lt;li&gt;The gating mechanism provides significant benefits in larger models where parameter count is less constrained.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Training Stability:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I've discovered that activation function choice can dramatically affect training stability. GELU and Swish provide better stability than ReLU, especially for small models.&lt;/li&gt; &lt;li&gt;The smoothness of these functions helps with gradient flow, which is critical for stable training.&lt;/li&gt; &lt;li&gt;I've had training runs that failed with ReLU but succeeded with GELU, even with identical architectures and hyperparameters.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;My Decision Framework:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For most small models, I use GELU it's the safe, reliable choice that works well.&lt;/li&gt; &lt;li&gt;If I have parameter budget and want to maximize performance, I use SwiGLU.&lt;/li&gt; &lt;li&gt;I only consider alternatives like ReLU if I have a specific reason or constraint.&lt;/li&gt; &lt;li&gt;Activation function isn't usually the bottleneck for small models, so I don't spend too much time optimizing it. GELU works, and that's often enough.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Summary&lt;/h1&gt; &lt;p&gt;Today we explored activation functions, the components that give neural networks their non-linear power. We learned how activation functions evolved from simple ReLU to sophisticated SwiGLU, and how they affect training stability and model performance.&lt;/p&gt; &lt;p&gt;Understanding activation functions is crucial because they're fundamental to how neural networks learn. The choice of activation function can mean the difference between a model that trains stably and one that fails, between a model that converges quickly and one that struggles. In the context of language models, activation functions work together with normalization, residual connections, and attention mechanisms to create the powerful architectures we use today.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxj4lv/day_20_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T05:18:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxxs46</id>
    <title>Local text to speech in your browser</title>
    <updated>2025-12-28T17:59:52+00:00</updated>
    <author>
      <name>/u/s3rgio0</name>
      <uri>https://old.reddit.com/user/s3rgio0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxs46/local_text_to_speech_in_your_browser/"&gt; &lt;img alt="Local text to speech in your browser" src="https://external-preview.redd.it/MTJxeDRoeWhoejlnMeaSxd9Clcsv5yxjX3jxtAIOssFrOjLFHIM2LHVFh0Aq.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec57ba95f567986fa73e0c92e0fc8799e669c561" title="Local text to speech in your browser" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The audio quality is much better on Desktop devices using Safari or Chrome compared to Android and iOS. It uses open source TTS models: &lt;/p&gt; &lt;p&gt;- &lt;a href="https://huggingface.co/spaces/hexgrad/Kokoro-TTS"&gt;https://huggingface.co/spaces/hexgrad/Kokoro-TTS&lt;/a&gt; (Desktop devices on Chrome, Safari and Edge)&lt;/p&gt; &lt;p&gt;- &lt;a href="https://github.com/rhasspy/piper"&gt;https://github.com/rhasspy/piper&lt;/a&gt; (Anything else such as iOS, Android, Firefox)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;On first use it can download up to 300MB into your borwser storage, but does it only once.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://desktop.with.audio/reader/new"&gt;https://desktop.with.audio/reader/new&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It also works very well with Github repos. Just paste the Github repo URL and get listen to the README in that page.&lt;/p&gt; &lt;p&gt;Check it out and let me know what you think. If you are interested in more details there is also a blog post about this: &lt;a href="https://blog.with.audio/posts/web-reader-tts"&gt;https://blog.with.audio/posts/web-reader-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How much do you think you'd use this? Any feedback?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/s3rgio0"&gt; /u/s3rgio0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/87e5y2yhhz9g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxs46/local_text_to_speech_in_your_browser/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxs46/local_text_to_speech_in_your_browser/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T17:59:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1py1nui</id>
    <title>Anyone been using local GLM-4.5-Air-IQ2_KL.gguf with Claude Code?</title>
    <updated>2025-12-28T20:32:25+00:00</updated>
    <author>
      <name>/u/xSNYPSx777</name>
      <uri>https://old.reddit.com/user/xSNYPSx777</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has 5090 + 48gigs of ram, constantly usage of ram is about 15-20 gigs, so available memory for 2-3 bit quants. Any tips how to use it ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xSNYPSx777"&gt; /u/xSNYPSx777 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1nui/anyone_been_using_local_glm45airiq2_klgguf_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1nui/anyone_been_using_local_glm45airiq2_klgguf_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py1nui/anyone_been_using_local_glm45airiq2_klgguf_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T20:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1py1xaa</id>
    <title>Triple GPU LLM benchmarks with --n-cpu-moe help</title>
    <updated>2025-12-28T20:43:14+00:00</updated>
    <author>
      <name>/u/tabletuser_blogspot</name>
      <uri>https://old.reddit.com/user/tabletuser_blogspot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1xaa/triple_gpu_llm_benchmarks_with_ncpumoe_help/"&gt; &lt;img alt="Triple GPU LLM benchmarks with --n-cpu-moe help" src="https://b.thumbs.redditmedia.com/HjkjA93ha8R3KgxW-QPTTRqs8j2MwGdksn3xZkyM3Dk.jpg" title="Triple GPU LLM benchmarks with --n-cpu-moe help" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here we have three Nvidia &lt;a href="https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840"&gt;GTX-1070 &lt;/a&gt;8GB cards running a few LLM that sit right on the edge of the available 24GB VRAM. Down below you can see how to get LLM to work if it exceeds VRAM limit.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/krumdzhea0ag1.jpg?width=3055&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=36d84a35c99829dfe4c199f99472490bcd3791c8"&gt;AM4 running triple GTX 1070 with Riser assist.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;System: &lt;/p&gt; &lt;p&gt;AMD Ryzen 5 3600 CPU, 32GB DDR4 RAM, Kubuntu 25.10 Kernel 6.17 OS, Triple GTX 1070 (8GB) 24GB VRAM GPUs. Power limits set to 333 watts for GPUs. &lt;/p&gt; &lt;p&gt;Llama.cpp Ubuntu Vulkan build: 06705fdcb (7552) &lt;/p&gt; &lt;h1&gt;Gemma-3-27b-it.Q5_K_M.gguf&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;(t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma3 27B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.94 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;55.63 ¬± 0.63&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Gemma3 27B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;17.94 GiB&lt;/td&gt; &lt;td align="left"&gt;27.01 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.45 ¬± 0.15&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Qwen3-Coder-30B-A3B-Instruct-UD-Q5_K_XL.gguf&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;(t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3Moe 30B.A3B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;20.24 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;84.43 ¬± 0.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3Moe 30B.A3B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;20.24 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;48.16 ¬± 1.89&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Nemotron-3-Nano-30B-A3B-UD-Q4_K_XL.gguf&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;(t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron H MoE 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.26 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;78.35 ¬± 1.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Nemotron H MoE 31B.A3.5B Q4_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.26 GiB&lt;/td&gt; &lt;td align="left"&gt;31.58 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;39.56 ¬± 0.34&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Olmo-3-32B-Think-UD-Q5_K_XL.gguf&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;(t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Olmo2 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.23 GiB&lt;/td&gt; &lt;td align="left"&gt;32.23 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;45.74 ¬± 0.45&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Olmo2 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.23 GiB&lt;/td&gt; &lt;td align="left"&gt;32.23 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.04 ¬± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;DeepSeek-R1-Distill-Qwen-32B-Q5_K_M.gguf&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;(t/s)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.66 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;44.83 ¬± 0.37&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen2 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.66 GiB&lt;/td&gt; &lt;td align="left"&gt;32.76 B&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;5.04 ¬± 0.00&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;LLM Granite 4.0 must be right outside the 24GB VRAM limit so lets see if we can get it working.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;In &lt;code&gt;llama.cpp&lt;/code&gt;, the command-line argument &lt;code&gt;--n-cpu-moe N&lt;/code&gt; (or &lt;code&gt;-ncmoe N&lt;/code&gt;) is a performance tuning option used to offload the Mixture of Experts (MoE) weights of the first N layers from the GPU to the CPU. &lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;*&lt;strong&gt;Granite-4.0-h-small-UD-Q5_K_XL\&lt;/strong&gt;*: ErrorOutOfDeviceMemory&lt;/p&gt; &lt;p&gt;First we find what is best &lt;code&gt;-ngl&lt;/code&gt; value.&lt;/p&gt; &lt;p&gt;Granite-4.0-h-small-UD-Q5_K_XL.gguf &lt;code&gt;-ngl 39&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;38.91 ¬± 0.24&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;9.11 ¬± 0.99&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Then we try different &lt;code&gt;-ncmoe&lt;/code&gt; values and settled with&lt;/p&gt; &lt;p&gt;Granite-4.0-h-small-UD-Q5_K_XL.gguf &lt;code&gt;-ngl 39 --n-cpu-moe 1&lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;model&lt;/th&gt; &lt;th align="left"&gt;size&lt;/th&gt; &lt;th align="left"&gt;params&lt;/th&gt; &lt;th align="left"&gt;backend&lt;/th&gt; &lt;th align="left"&gt;ngl&lt;/th&gt; &lt;th align="left"&gt;n_cpu_moe&lt;/th&gt; &lt;th align="left"&gt;test&lt;/th&gt; &lt;th align="left"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;41.24 ¬± 0.52&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;granitehybrid 32B Q5_K - Medium&lt;/td&gt; &lt;td align="left"&gt;21.53 GiB&lt;/td&gt; &lt;td align="left"&gt;32.21 B&lt;/td&gt; &lt;td align="left"&gt;Vulkan&lt;/td&gt; &lt;td align="left"&gt;39&lt;/td&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;14.52 ¬± 0.27&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tabletuser_blogspot"&gt; /u/tabletuser_blogspot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1xaa/triple_gpu_llm_benchmarks_with_ncpumoe_help/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1xaa/triple_gpu_llm_benchmarks_with_ncpumoe_help/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py1xaa/triple_gpu_llm_benchmarks_with_ncpumoe_help/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T20:43:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxz7uz</id>
    <title>Which coding tool with Minimax M2.1?</title>
    <updated>2025-12-28T18:55:37+00:00</updated>
    <author>
      <name>/u/Leflakk</name>
      <uri>https://old.reddit.com/user/Leflakk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With llama.cpp and model loaded in vram (Q4 K M on 6x3090) it seems quite long with claude code. Which Minimax quant &amp;amp; coding agent/tool do you use and how is your experience (quality, speed)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Leflakk"&gt; /u/Leflakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7uz/which_coding_tool_with_minimax_m21/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7uz/which_coding_tool_with_minimax_m21/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7uz/which_coding_tool_with_minimax_m21/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T18:55:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1py1rpg</id>
    <title>[Tool Release] Skill Seekers v2.5.0 - Convert any documentation into structured markdown skills for local/remote LLMs</title>
    <updated>2025-12-28T20:36:54+00:00</updated>
    <author>
      <name>/u/Critical-Pea-8782</name>
      <uri>https://old.reddit.com/user/Critical-Pea-8782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey üëã&lt;/p&gt; &lt;p&gt;Released &lt;strong&gt;Skill Seekers v2.5.0&lt;/strong&gt; with universal LLM support - convert any documentation into structured markdown skills.&lt;/p&gt; &lt;p&gt;## What It Does&lt;/p&gt; &lt;p&gt;Automatically scrapes documentation websites and converts them into organized, categorized reference files with extracted code examples. Works with any LLM (local or remote).&lt;/p&gt; &lt;p&gt;## New in v2.5.0: Universal Format Support&lt;/p&gt; &lt;ul&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Generic Markdown export&lt;/strong&gt; - works with ANY LLM&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Claude AI&lt;/strong&gt; format (if you use Claude)&lt;/li&gt; &lt;li&gt;‚úÖ &lt;strong&gt;Google Gemini&lt;/strong&gt; format (with grounding)&lt;/li&gt; &lt;li&gt;&lt;p&gt;‚úÖ &lt;strong&gt;OpenAI ChatGPT&lt;/strong&gt; format (with vector search)&lt;/p&gt; &lt;h2&gt;Why This Matters for Local LLMs&lt;/h2&gt; &lt;p&gt;Instead of context-dumping entire docs, you get:&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Organized structure&lt;/strong&gt;: Categorized by topic (getting-started, API, examples, etc.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Extracted patterns&lt;/strong&gt;: Code examples pulled from docs with syntax highlighting&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Portable format&lt;/strong&gt;: Pure markdown ZIP - use with Ollama, llama.cpp, or any local model&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;strong&gt;Reusable&lt;/strong&gt;: Build once, use with any LLM&lt;/p&gt; &lt;h2&gt;Quick Example&lt;/h2&gt; &lt;p&gt;```bash&lt;/p&gt; &lt;h1&gt;Install&lt;/h1&gt; &lt;p&gt;pip install skill-seekers&lt;/p&gt; &lt;h1&gt;Scrape any documentation&lt;/h1&gt; &lt;p&gt;skill-seekers scrape --config configs/react.json&lt;/p&gt; &lt;h1&gt;Export as universal markdown&lt;/h1&gt; &lt;p&gt;skill-seekers package output/react/ --target markdown&lt;/p&gt; &lt;h1&gt;Result: react-markdown.zip with organized .md files&lt;/h1&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The output is just structured markdown files - perfect for feeding to local models or adding to your RAG pipeline.&lt;/p&gt; &lt;p&gt;Features&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;üìÑ Documentation scraping with smart categorization&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;üêô GitHub repository analysis&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;üìï PDF extraction (for PDF-based docs)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;üîÄ Multi-source unified (docs + code + PDFs in one skill)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;üéØ 24 preset configs (React, Vue, Django, Godot, etc.)&lt;/p&gt; &lt;p&gt;Links&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/yusufkaraaslan/Skill_Seekers"&gt;https://github.com/yusufkaraaslan/Skill_Seekers&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;PyPI: &lt;a href="https://pypi.org/project/skill-seekers/"&gt;https://pypi.org/project/skill-seekers/&lt;/a&gt;&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Release: &lt;a href="https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.5.0"&gt;https://github.com/yusufkaraaslan/Skill_Seekers/releases/tag/v2.5.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT licensed, contributions welcome! Would love to hear what documentation you'd like to see supported.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Critical-Pea-8782"&gt; /u/Critical-Pea-8782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1rpg/tool_release_skill_seekers_v250_convert_any/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1py1rpg/tool_release_skill_seekers_v250_convert_any/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1py1rpg/tool_release_skill_seekers_v250_convert_any/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T20:36:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxm29c</id>
    <title>Unsloth GLM-4.7-GGUF?</title>
    <updated>2025-12-28T08:08:13+00:00</updated>
    <author>
      <name>/u/UnknownDude360</name>
      <uri>https://old.reddit.com/user/UnknownDude360</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all! I‚Äôm really excited to test out GLM-4.7 and I‚Äôve been specifically waiting for Unsloth‚Äôs quants because they always cook!&lt;/p&gt; &lt;p&gt;Well, I‚Äôm a little confused. Which is ‚Äútechnically‚Äù better? I mean higher average bits? Less lossy. &lt;/p&gt; &lt;p&gt;Q3_K_M @ 171GB vs Q3_K_XL @ 159GB?&lt;/p&gt; &lt;p&gt;I have 48GB VRAM + 128GB DDR4 = 176GB absolute maximum ideally. &lt;/p&gt; &lt;p&gt;I would expect it be obvious, the _XL should be better than the _M‚Ä¶ right?&lt;/p&gt; &lt;p&gt;However the more lossy quant is somehow bigger? Can someone help me reconcile this discrepancy? I feel kinda dumb overthinking this‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/UnknownDude360"&gt; /u/UnknownDude360 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxm29c/unsloth_glm47gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T08:08:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxo9y5</id>
    <title>What non-Asian based models do you recommend at the end of 2025?</title>
    <updated>2025-12-28T10:31:00+00:00</updated>
    <author>
      <name>/u/thealliane96</name>
      <uri>https://old.reddit.com/user/thealliane96</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Background:&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;Building agentic stuff so tool calling has to be good (gpt oss has been the most reliable one in my, admittedly anecdotal, experience)&lt;/li&gt; &lt;li&gt;Work with and do work for certain organizations where I can‚Äôt:&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;- Use frontier models (or any hosted models for that matter)&lt;/p&gt; &lt;p&gt;- Use models released by Chinese, Taiwanese, etc based companies (maybe it‚Äôs dumb, okay it‚Äôs probably dumb, but unfortunately I don‚Äôt make the rules lol)&lt;/p&gt; &lt;p&gt;So I come to yall ask for your recommendations going into 2026.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;I‚Äôm aware there‚Äôs some other similar posts but since they‚Äôre somewhat dated and a lot has happened since, I figured it wouldn‚Äôt be&lt;/em&gt; &lt;strong&gt;&lt;em&gt;too&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;egregious to throw mine up. Hope it‚Äôs okay &amp;lt;3&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;While I am hoping to get recs for models I haven‚Äôt considered that will actually be effective, I‚Äôm also hoping just to find some new stuff to try regardless &amp;lt;3&lt;/em&gt;&lt;/p&gt; &lt;h1&gt;Models Tried&lt;/h1&gt; &lt;p&gt;- llama3.1 8B&lt;/p&gt; &lt;p&gt;- mistral Nemo&lt;/p&gt; &lt;p&gt;- Nemo fine tuned on my dataset&lt;/p&gt; &lt;p&gt;- mistral small 3.1 / 3.2 24b&lt;/p&gt; &lt;p&gt;- gpt-oss 20b and 120b&lt;/p&gt; &lt;p&gt;- several other mistral and devstral variants&lt;/p&gt; &lt;p&gt;- some phi models&lt;/p&gt; &lt;p&gt;- Gemma 3 27B (been so long and didn‚Äôt try it as much as the others)&lt;/p&gt; &lt;h1&gt;Unorganized Thoughts Regarding Models Tried&lt;/h1&gt; &lt;p&gt;From my experience testing them:&lt;/p&gt; &lt;p&gt;- All are generally good with raw text output (except Nemo, Nemo just sucks ass in my opinion)&lt;/p&gt; &lt;p&gt;- Tool calling wise **gpt-oss** is leagues ahead of all the others, at least in my experience using them&lt;/p&gt; &lt;p&gt;- llama3.1 8B is surprising good for raw text output and summarization and it has a oddly pleasing writing style? Maybe that‚Äôs just me.&lt;/p&gt; &lt;p&gt;- Mistral models in general never fail to be underwhelming for me. Quite liked Small 3.2, but when I slotted it into a (honestly) quite simple agent setup it got stuck in loops and would fuck up tool calls whereas gpt-oss-20b did it perfectly fine.&lt;/p&gt; &lt;p&gt;- devstral, mixtral, all those mistral variants I‚Äôve found to also be incredibly underwhelming&lt;/p&gt; &lt;p&gt;- Phi models were, in my experience, utterly useless&lt;/p&gt; &lt;p&gt;- Gemma 3 honestly don‚Äôt remember, planning to try it out again soon&lt;/p&gt; &lt;h1&gt;On GPT-OSS&lt;/h1&gt; &lt;p&gt;While the answer is somewhat obviously ‚Äújust use gpt oss‚Äù, there‚Äôs 2 negatives I find with it, neither are really deal breaking but they can be annoying plus sometimes you just want to try different stuff.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Negative 1:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I sometimes find it can maybe be a bit &lt;em&gt;too good&lt;/em&gt; at following instructions?&lt;/p&gt; &lt;p&gt;It‚Äôll kind of, well, follow them to the letter including making things up to produce an output I‚Äôve asked for.&lt;/p&gt; &lt;p&gt;I‚Äôve gotten around this by instructing it to only output things it‚Äôs seen directly in tool results or directly from some external context it was given and that‚Äôs worked quite well but still.&lt;/p&gt; &lt;p&gt;It also suffers from what I like to call &lt;em&gt;context window snowballing&lt;/em&gt; where it gets stuck on one path and becomes very narrow minded (all the previous tokens influencing the next token basically, so without some type of intervention it‚Äôll snowball down that same path).&lt;/p&gt; &lt;p&gt;Again I have ways getting around this where I‚Äôll intentionally stop it after a certain percentage of the context window is full and then have it break down what it did and what the next steps should be and then I‚Äôll throw that into a new run with a clear context window and instructing to rethink through the task and what it‚Äôs next steps should be. It‚Äôs a lot of work around but it works decently well.&lt;/p&gt; &lt;p&gt;I also haven‚Äôt found 120b to really be all that better than 20b, honestly sometimes 120b‚Ä¶ kinda performs &lt;em&gt;worse&lt;/em&gt;?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Negative Number 2:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;For the work I‚Äôm doing I have to abliterate it (de-censor it).&lt;/p&gt; &lt;p&gt;It‚Äôd get stuck in a reasoning loop of trying to decide whether it could answer or not until eventually it‚Äôd just time out or I‚Äôd kill it. And what I‚Äôm asking it to do is not even against policy, it‚Äôs just been so heavily censored.&lt;/p&gt; &lt;p&gt;This isn‚Äôt that big of a deal as it‚Äôs been made quite easy by heretic, but still one of those annoyances where you just kind of wish you didn‚Äôt have to do it.&lt;/p&gt; &lt;p&gt;‚Äî-&lt;/p&gt; &lt;p&gt;Anyway enough of my rambling, anyone who read through it all, you‚Äôre a real one!&lt;/p&gt; &lt;h1&gt;TL;DR&lt;/h1&gt; &lt;p&gt;Can‚Äôt use models from either Chinese or other Asia-based companies/orgs.&lt;/p&gt; &lt;p&gt;Looking for recommendations for American/Canadian/European models that are good at tool calling that aren‚Äôt within the list of ones I‚Äôve already tried.&lt;/p&gt; &lt;p&gt;Edit:&lt;/p&gt; &lt;p&gt;Guess markdown formatting isn‚Äôt supported on mobile lol&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thealliane96"&gt; /u/thealliane96 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxo9y5/what_nonasian_based_models_do_you_recommend_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T10:31:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxsdnm</id>
    <title>XiaomiMiMo/MiMo-V2-Flash Under-rated?</title>
    <updated>2025-12-28T14:17:17+00:00</updated>
    <author>
      <name>/u/SlowFail2433</name>
      <uri>https://old.reddit.com/user/SlowFail2433</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;XiaomiMiMo/MiMo-V2-Flash has 310B param and top benches.&lt;/p&gt; &lt;p&gt;Seems to compete well with KimiK2Thinking, GLM4.7, MinimaxM2.1, Deepseek3.2&lt;/p&gt; &lt;p&gt;What do you think of this model?&lt;/p&gt; &lt;p&gt;Any use-cases welcome but particularly math, coding and agentic&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SlowFail2433"&gt; /u/SlowFail2433 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxsdnm/xiaomimimomimov2flash_underrated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxsdnm/xiaomimimomimov2flash_underrated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxsdnm/xiaomimimomimov2flash_underrated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:17:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxtwn2</id>
    <title>Which is the best embedding model for production use?</title>
    <updated>2025-12-28T15:24:55+00:00</updated>
    <author>
      <name>/u/Hari-Prasad-12</name>
      <uri>https://old.reddit.com/user/Hari-Prasad-12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've done my research for embedding models for a critical production job. I've read a lot about bge m3 since I can't use a closed source model like text emmedings 3 or something properitry I'm seeking your experience working with these open source models. &lt;/p&gt; &lt;p&gt;To put it simply, which one of these works the best in production:&lt;br /&gt; 1. bge m3&lt;br /&gt; 2. embeddinggemma-300m&lt;br /&gt; 3. qwen3-embedding-0.6b&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hari-Prasad-12"&gt; /u/Hari-Prasad-12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxtwn2/which_is_the_best_embedding_model_for_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T15:24:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxxuib</id>
    <title>Which are the best coding + tooling agent models for vLLM for 128GB memory?</title>
    <updated>2025-12-28T18:02:08+00:00</updated>
    <author>
      <name>/u/jinnyjuice</name>
      <uri>https://old.reddit.com/user/jinnyjuice</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel a lot of the coding models jump from ~30B class to ~120B to &amp;gt;200B. Is there anything ~100B and a bit under that performs well?&lt;/p&gt; &lt;p&gt;Or are ~120B models ok with GGUF or AWQ compression (or maybe 16 FP or Q8_K_XL?)?&lt;/p&gt; &lt;p&gt;Bonus question -- generally if the models are about the same or heavier than the RAM in terms of storage space required for the model (e.g. 120 GB storage), they won't work, right?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jinnyjuice"&gt; /u/jinnyjuice &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxuib/which_are_the_best_coding_tooling_agent_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxuib/which_are_the_best_coding_tooling_agent_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxxuib/which_are_the_best_coding_tooling_agent_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T18:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxz7f0</id>
    <title>Plamo3 (2B/8B/31B) support has been merged into llama.cpp</title>
    <updated>2025-12-28T18:55:09+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"&gt; &lt;img alt="Plamo3 (2B/8B/31B) support has been merged into llama.cpp" src="https://external-preview.redd.it/addr5Q-exN6mOW2m8NxyWDisrtP7qSIOjIojUWESLhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=044fee01a3568867e7f945c50edee5a2529bd629" title="Plamo3 (2B/8B/31B) support has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;PLaMo 3 NICT 31B Base is a 31B model pre-trained on English and Japanese datasets, developed by Preferred Networks, Inc. collaborative with National Institute of Information and Communications Technology, NICT.&lt;/p&gt; &lt;p&gt;PLaMo 3 NICT models adapt a hybrid architecture with Sliding Window Attention (SWA) and Traditional Attetntion layers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17304"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxz7f0/plamo3_2b8b31b_support_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T18:55:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxuk38</id>
    <title>Fix for Nvidia Nemotron Nano 3's forced thinking ‚Äì now it can be toggled on and off!</title>
    <updated>2025-12-28T15:51:54+00:00</updated>
    <author>
      <name>/u/Substantial_Swan_144</name>
      <uri>https://old.reddit.com/user/Substantial_Swan_144</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, everyone,&lt;/p&gt; &lt;p&gt;if you downloaded NVidia Nemotron 3, you are probably aware that the instruction 'detailed thinking off' doesn't work. This is because the automatic Jinja template on Lmstudio has a bug that forces thinking.&lt;/p&gt; &lt;p&gt;However, I'm postining a workaround here: this template has a bugfix which makes thinking on by default, but it can be toggled off by typing /nothink at the system prompt (like you do with Qwen). I pasted it on Pastebin to make this post clean: &lt;a href="https://pastebin.com/y5g3X2Ex"&gt;https://pastebin.com/y5g3X2Ex&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Swan_144"&gt; /u/Substantial_Swan_144 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxuk38/fix_for_nvidia_nemotron_nano_3s_forced_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T15:51:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxad0k</id>
    <title>NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux</title>
    <updated>2025-12-27T22:22:21+00:00</updated>
    <author>
      <name>/u/HumanDrone8721</name>
      <uri>https://old.reddit.com/user/HumanDrone8721</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt; &lt;img alt="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" src="https://external-preview.redd.it/Z1W-jCS5853m4eyzALlzqsbFjQ8v2fOj2tdMfCsU0J8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=232d0328fa3a116bc0a1917deae0e1763f4b6c47" title="NVIDIA Drops Pascal Support On Linux, Causing Chaos On Arch Linux" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HumanDrone8721"&gt; /u/HumanDrone8721 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://hackaday.com/2025/12/26/nvidia-drops-pascal-support-on-linux-causing-chaos-on-arch-linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxad0k/nvidia_drops_pascal_support_on_linux_causing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-27T22:22:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pxss0m</id>
    <title>Senator in Tennessee introduces bill to felonize making AI "act as a companion" or "mirror human interactions"</title>
    <updated>2025-12-28T14:35:58+00:00</updated>
    <author>
      <name>/u/CanineAssBandit</name>
      <uri>https://old.reddit.com/user/CanineAssBandit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Call (202) 224-3121 for the Capitol switchboard to contact your representative.&lt;/p&gt; &lt;p&gt;The bill:&lt;br /&gt; &lt;a href="https://legiscan.com/TN/bill/SB1493/2025"&gt;https://legiscan.com/TN/bill/SB1493/2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quotes from the bill (emphasis mine):&lt;/p&gt; &lt;p&gt;It is an offense for a person to knowingly train artificial intelligence to:&lt;br /&gt; (3) Provide emotional support, &lt;strong&gt;including through open-ended conversations&lt;/strong&gt; with a user;&lt;br /&gt; (4) Develop an emotional relationship with, or otherwise &lt;strong&gt;act as a companion&lt;/strong&gt; to, an individual;&lt;br /&gt; (6) Otherwise act as a sentient human or &lt;strong&gt;mirror interactions that a human user might have with another human user&lt;/strong&gt;, such that an individual would feel that the individual could develop a friendship or other relationship with the artificial intelligence;&lt;br /&gt; (8) &lt;strong&gt;Simulate a human being&lt;/strong&gt;, including in appearance, voice, or other mannerisms.&lt;/p&gt; &lt;p&gt;&amp;quot;Train&amp;quot;:&lt;br /&gt; (A) Means utilizing sets of data and other information to teach an artificial intelligence system to perceive, interpret, and learn from data, such that the A.I. will later be capable of &lt;strong&gt;making decisions based on information or other inputs&lt;/strong&gt; provided to the A.I.&lt;br /&gt; (B) Includes development of a large language model when the person developing the large language model knows that the model will be used to teach the A.I.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CanineAssBandit"&gt; /u/CanineAssBandit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pxss0m/senator_in_tennessee_introduces_bill_to_felonize/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-28T14:35:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
