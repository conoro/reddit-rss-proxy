<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-03T18:04:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qutill</id>
    <title>Devstral Small 2 - llama.cpp speed bump with `ngram-mod` and `draft`</title>
    <updated>2026-02-03T14:34:09+00:00</updated>
    <author>
      <name>/u/Holiday_Purpose_3166</name>
      <uri>https://old.reddit.com/user/Holiday_Purpose_3166</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qutill/devstral_small_2_llamacpp_speed_bump_with/"&gt; &lt;img alt="Devstral Small 2 - llama.cpp speed bump with `ngram-mod` and `draft`" src="https://b.thumbs.redditmedia.com/J9myHfm-OmgSGKFKPyb7_uT-W6UpcqEJBFBfzUSiPKA.jpg" title="Devstral Small 2 - llama.cpp speed bump with `ngram-mod` and `draft`" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/gqe0kbpahahg1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16b751ea18f6d48a373211618de9d83900043cb5"&gt;https://preview.redd.it/gqe0kbpahahg1.png?width=1513&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=16b751ea18f6d48a373211618de9d83900043cb5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Caught wind from this user in &lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/20"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF/discussions/20&lt;/a&gt; about bumping speed for GLM 4.7 Flash however I decided to test if it works on Devstral Small 2 too.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Tested Stack&lt;/strong&gt;&lt;br /&gt; RTX 5090&lt;br /&gt; llama.cpp b7907&lt;br /&gt; Devstral Small 2 LM Studio Q8_0&lt;/p&gt; &lt;pre&gt;&lt;code&gt;-ctk q4_0 -ctv q4_0 -c 135072 --cache-ram 15000 --no-mmap --spec-type ngram-mod --spec-ngram-size-n 24 --draft-min 48 --draft-max 64 --temp &amp;quot;0.15&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Except I could only reasonably fit -c 125072 with -b 1024 -ub 1024&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Holiday_Purpose_3166"&gt; /u/Holiday_Purpose_3166 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qutill/devstral_small_2_llamacpp_speed_bump_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qutill/devstral_small_2_llamacpp_speed_bump_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qutill/devstral_small_2_llamacpp_speed_bump_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T14:34:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1qup4p1</id>
    <title>What do we consider low end here?</title>
    <updated>2026-02-03T11:07:57+00:00</updated>
    <author>
      <name>/u/Acceptable_Home_</name>
      <uri>https://old.reddit.com/user/Acceptable_Home_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;i would say 8-12gb vram with 32gb ram seems low end for usable quality of local LLMs or ai in general, &lt;/p&gt; &lt;p&gt;Im rocking a 4060 and 24gb of ddr5, how bout y'all low end rig enjoyers!&lt;/p&gt; &lt;p&gt;I can easily use glm 4.7 flash or oss 20B, z img, flux klein, and a lot of other small but useful models so im not really unhappy with it!&lt;/p&gt; &lt;p&gt;Lemme know about the setup y'all got and if y'all enjoy it!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Acceptable_Home_"&gt; /u/Acceptable_Home_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qup4p1/what_do_we_consider_low_end_here/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qup4p1/what_do_we_consider_low_end_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qup4p1/what_do_we_consider_low_end_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T11:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1quwwfs</id>
    <title>I got tired of small models adding ```json blocks, so I wrote a TS library to forcefully extract valid JSON. (My first open source project!)</title>
    <updated>2026-02-03T16:40:42+00:00</updated>
    <author>
      <name>/u/rossjang</name>
      <uri>https://old.reddit.com/user/rossjang</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Like many of you, I run a lot of local models for various side projects. Even with strict system prompts, quantized models often mess up JSON outputs. They love to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Wrap everything in markdown code blocks (&lt;code&gt;\&lt;/code&gt;``json ... ````).&lt;/li&gt; &lt;li&gt;Add &amp;quot;Sure, here is the result:&amp;quot; before the JSON.&lt;/li&gt; &lt;li&gt;Fail &lt;code&gt;JSON.parse&lt;/code&gt; because of trailing commas or single quotes.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I know LangChain has output parsers that handle this, but bringing in the whole framework just to clean up JSON strings felt like overkill for my use case. I wanted something lightweight and &lt;strong&gt;zero-dependency&lt;/strong&gt; that I could drop into any stack (especially Next.js/Edge).&lt;/p&gt; &lt;p&gt;So, I decided to build a dedicated library to handle this properly. It's called &lt;code&gt;loot-json&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The concept is simple:&lt;/strong&gt; Treat the LLM output as a dungeon, and &amp;quot;loot&amp;quot; the valid JSON artifact from it.&lt;/p&gt; &lt;p&gt;It uses a &lt;strong&gt;stack-based bracket matching algorithm&lt;/strong&gt; to locate the outermost JSON object or array, ignoring all the Chain-of-Thought (CoT) reasoning or conversational fluff surrounding it. It also patches common syntax errors (like trailing commas) using a permissive parser logic.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How it works:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;const result = loot(messyOutput);&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;NPM:&lt;/strong&gt; &lt;code&gt;npm install loot-json&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/rossjang/loot-json"&gt;&lt;code&gt;https://github.com/rossjang/loot-json&lt;/code&gt;&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Thanks for reading!&lt;/p&gt; &lt;p&gt;&lt;em&gt;A personal note&lt;/em&gt;: To be honest, posting this is a bit nerve-wracking for me. I‚Äôve always had a small dream of contributing to open source, but I kept putting it off because I felt shy/embarrassed about showing my raw code to the world. This library is my first real attempt at breaking that fear. It‚Äôs not a massive framework, but it solves a real itch I had.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rossjang"&gt; /u/rossjang &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwwfs/i_got_tired_of_small_models_adding_json_blocks_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwwfs/i_got_tired_of_small_models_adding_json_blocks_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quwwfs/i_got_tired_of_small_models_adding_json_blocks_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1quy1ri</id>
    <title>Pocket TTS Android APK Sample - Full Local (Model Packed)</title>
    <updated>2026-02-03T17:21:57+00:00</updated>
    <author>
      <name>/u/RowGroundbreaking982</name>
      <uri>https://old.reddit.com/user/RowGroundbreaking982</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve put together a sample APK for &lt;strong&gt;Pocket TTS&lt;/strong&gt; using the ONNX runtime. I used Gemini to help squeeze the inference code optimization as much as possible, making this maybe the fastest Pocket TTS build available for mobile.&lt;/p&gt; &lt;h1&gt;The Performance:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Helio G99: Hits 0.9x to 1.0x (Real-time).&lt;/li&gt; &lt;li&gt;Snapdragon 7 Gen 1: &amp;gt;1.0x (Faster than real-time).&lt;/li&gt; &lt;li&gt;Voice Clone: Includes a built-in clone of a famous actor‚Äîyou‚Äôll know who it is the moment you hear it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Feel free to test it on your phone and let me know your results!&lt;/p&gt; &lt;h1&gt;Technical Note: The Mimi Bottleneck&lt;/h1&gt; &lt;p&gt;The current bottleneck is the Mimi decoder, which uses convolutional layers that aren't perfectly optimized for mobile CPUs.&lt;/p&gt; &lt;p&gt;I‚Äôm keeping an eye out for a Transformer-based Mimi decoder. If the researchers release those weights, we should see a nice speed boost, as mobile inference engines handle transformer architectures much more efficiently than deconvolution.&lt;/p&gt; &lt;h1&gt;Installation (Manual OBB Setup)&lt;/h1&gt; &lt;p&gt;Android handles large assets via expansion files, so you must place the data manually:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Download: APK + OBB files from &lt;a href="https://github.com/lookbe/pocket-tts-unity/releases"&gt;GitHub&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Install: The APK (do not open it yet).&lt;/li&gt; &lt;li&gt;Folder: Navigate to Internal Storage/Android/obb/ and create a folder named: com.lookbe.tts&lt;/li&gt; &lt;li&gt;Copy: Move both OBB files into that folder.&lt;/li&gt; &lt;li&gt;Launch: Open the app and test.&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;Quick Note on Permissions&lt;/h1&gt; &lt;p&gt;Newer Android versions (13+) can be strict about /obb/ folder access. If your PC has trouble seeing it, use a file manager like Shizuku or FV File Explorer on the phone to move the files into the directory.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://github.com/lookbe/pocket-tts-unity/releases"&gt;github.com/lookbe/pocket-tts-unity/releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RowGroundbreaking982"&gt; /u/RowGroundbreaking982 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quy1ri/pocket_tts_android_apk_sample_full_local_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quy1ri/pocket_tts_android_apk_sample_full_local_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quy1ri/pocket_tts_android_apk_sample_full_local_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:21:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1quyjnm</id>
    <title>LocalAI v3.9 &amp; v3.10 Released: Native Agents, Video Generation UI, and Unified GPU Backends</title>
    <updated>2026-02-03T17:39:44+00:00</updated>
    <author>
      <name>/u/mudler_it</name>
      <uri>https://old.reddit.com/user/mudler_it</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;The community and I have been heads-down working on the last two releases (v3.9.0 and v3.10.0 + patch), and I wanted to share what‚Äôs new.&lt;/p&gt; &lt;p&gt;If you are new to LocalAI (&lt;a href="https://localai.io"&gt;https://localai.io&lt;/a&gt;), LocalAI is an OpenAI and Anthropic alternative with 42K stars on Github, and was one of the first in the field! LocalAI can run locally, no GPU needed, it aims to provide 1:1 features with OpenAI, for instance it lets generate images, audio, text and create powerful agent pipelines.&lt;/p&gt; &lt;p&gt;Our main goal recently has been extensibility and better memory management. We want LocalAI to be more than just an API endpoint and a simple UI, we want it to be a reliable platform where you can orchestrate agents, generate media, and automate tasks without needing a dozen different tools.&lt;/p&gt; &lt;p&gt;Here are the major highlights from both the releases (3.9.0 and 3.10.0):&lt;/p&gt; &lt;h1&gt;Agentic Capabilities&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Open Responses API: We now natively support this standard. You can run stateful, multi-turn agents in the background. It passes the official compliance tests (100%!).&lt;/li&gt; &lt;li&gt;Anthropic API Support: We added a &lt;code&gt;/v1/messages&lt;/code&gt; endpoint that acts as a drop-in replacement for Claude. If you have tools built for Anthropic, they should now work locally (like Claude Code, clawdbot, ...).&lt;/li&gt; &lt;li&gt;Agent Jobs: You can now schedule prompts or agent MCP workflows using Cron syntax (e.g., run a news summary every morning at 8 AM) or trigger via API, and monitor everything from the WebUI.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/d1y6i0r6fbhg1.png?width=1576&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06842be40ea87d7e73cfe03a69a4874787535d02"&gt;https://preview.redd.it/d1y6i0r6fbhg1.png?width=1576&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=06842be40ea87d7e73cfe03a69a4874787535d02&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Architecture &amp;amp; Performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Unified GPU Images: This is a big one even if experimental. We packaged CUDA, ROCm, and Vulkan libraries inside the backend containers. You don't need specific Docker tags anymore unless you want, the same image works on Nvidia, AMD, and ARM64. This is still experimental, let us know how it goes!&lt;/li&gt; &lt;li&gt;Smart Memory Reclaimer: The system now monitors VRAM usage live. If you hit a threshold, it automatically evicts the Least Recently Used (LRU) models to prevent OOM crashes/VRAM exhaustion. You can configure this directly from the UI in the settings! You can keep an eye on the GPU/RAM usage directly from the home page too:&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5azbomu4fbhg1.png?width=975&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3035e51326c4a3efc93b5a1cdab10a486e6dc84b"&gt;https://preview.redd.it/5azbomu4fbhg1.png?width=975&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3035e51326c4a3efc93b5a1cdab10a486e6dc84b&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Multi-Modal Stuff&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Video Gen UI: We added a dedicated page for video generation (built on &lt;code&gt;diffusers&lt;/code&gt;, supports LTX-2).&lt;/li&gt; &lt;li&gt;New Audio backends: Added Moonshine (fast transcription for lower-end devices), Pocket-TTS, Vibevoice, and Qwen-TTS.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/wpjetn4kfbhg1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f03f4171026535821c7143b917675d75e23cd8e"&gt;https://preview.redd.it/wpjetn4kfbhg1.png?width=1860&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f03f4171026535821c7143b917675d75e23cd8e&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Fixes&lt;/h1&gt; &lt;p&gt;Lots of stability work, including fixing crashes on AVX-only CPUs (Sandy/Ivy Bridge) and fixing VRAM reporting on AMD GPUs.&lt;/p&gt; &lt;p&gt;We‚Äôd love for you to give it a spin and let us know what you think!!&lt;/p&gt; &lt;p&gt;If you didn't had a chance to see LocalAI before, you can check this youtube video: &lt;a href="https://www.youtube.com/watch?v=PDqYhB9nNHA"&gt;https://www.youtube.com/watch?v=PDqYhB9nNHA&lt;/a&gt; ( doesn't show the new features, but it gives an idea!)&lt;/p&gt; &lt;p&gt;Release 3.10.0: &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v3.10.0"&gt;https://github.com/mudler/LocalAI/releases/tag/v3.10.0&lt;/a&gt;&lt;br /&gt; Release 3.9.0: &lt;a href="https://github.com/mudler/LocalAI/releases/tag/v3.9.0"&gt;https://github.com/mudler/LocalAI/releases/tag/v3.9.0&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mudler_it"&gt; /u/mudler_it &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quyjnm/localai_v39_v310_released_native_agents_video/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quyjnm/localai_v39_v310_released_native_agents_video/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quyjnm/localai_v39_v310_released_native_agents_video/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:39:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1quqfre</id>
    <title>I have 8x H100 for the next two weeks. Any ideas for use cases?</title>
    <updated>2026-02-03T12:18:47+00:00</updated>
    <author>
      <name>/u/IVIsHero</name>
      <uri>https://old.reddit.com/user/IVIsHero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me know!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IVIsHero"&gt; /u/IVIsHero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quqfre/i_have_8x_h100_for_the_next_two_weeks_any_ideas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quqfre/i_have_8x_h100_for_the_next_two_weeks_any_ideas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quqfre/i_have_8x_h100_for_the_next_two_weeks_any_ideas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T12:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1qurzkz</id>
    <title>minitorch ‚Äî A very minimal deep learning library</title>
    <updated>2026-02-03T13:31:31+00:00</updated>
    <author>
      <name>/u/IntrepidAttention56</name>
      <uri>https://old.reddit.com/user/IntrepidAttention56</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qurzkz/minitorch_a_very_minimal_deep_learning_library/"&gt; &lt;img alt="minitorch ‚Äî A very minimal deep learning library" src="https://external-preview.redd.it/D1G-5Pupa2w4rKssWi2sFpVBV5QjVyFLeGtLFOPdWYo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ab131406a5dcf200b50da198b02e1ed3dc8876c" title="minitorch ‚Äî A very minimal deep learning library" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/IntrepidAttention56"&gt; /u/IntrepidAttention56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/abdimoallim/minitorch"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qurzkz/minitorch_a_very_minimal_deep_learning_library/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qurzkz/minitorch_a_very_minimal_deep_learning_library/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T13:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1qun30l</id>
    <title>Top AI papers of 2025</title>
    <updated>2026-02-03T09:01:29+00:00</updated>
    <author>
      <name>/u/gbomb13</name>
      <uri>https://old.reddit.com/user/gbomb13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qun30l/top_ai_papers_of_2025/"&gt; &lt;img alt="Top AI papers of 2025" src="https://preview.redd.it/qynfy5vpv8hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c35de07e0f34316984fafa3caa0a948e672e5b56" title="Top AI papers of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://archivara.org/top-2025"&gt;https://archivara.org/top-2025&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gbomb13"&gt; /u/gbomb13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qynfy5vpv8hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qun30l/top_ai_papers_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qun30l/top_ai_papers_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T09:01:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qtvp74</id>
    <title>GLM-5 Coming in February! It's confirmed.</title>
    <updated>2026-02-02T13:56:14+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt; &lt;img alt="GLM-5 Coming in February! It's confirmed." src="https://preview.redd.it/rq0meza173hg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=71bbd7ed37e31d92af89abf19ffb4ef0e1d8925a" title="GLM-5 Coming in February! It's confirmed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twitter Link: &lt;a href="https://x.com/jietang/status/2018246490775498791?s=20"&gt;https://x.com/jietang/status/2018246490775498791?s=20&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/rq0meza173hg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qtvp74/glm5_coming_in_february_its_confirmed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T13:56:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1quwn8a</id>
    <title>MichiAI: A 530M Full-Duplex Speech LLM with ~75ms Latency using Flow Matching</title>
    <updated>2026-02-03T16:31:35+00:00</updated>
    <author>
      <name>/u/kwazar90</name>
      <uri>https://old.reddit.com/user/kwazar90</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to see if I could build a full-duplex speech model that avoids the coherence degradation that plagues models of this type while also requiring low compute for training and inference.&lt;/p&gt; &lt;p&gt;I don't have access to much compute so I spent a lot of the time designing the architecture so it's efficient and there is no need to brute force with model size and training compute.&lt;/p&gt; &lt;p&gt;Also I made sure that all the components can be pretrained quickly separately and only trained together as the last step.&lt;/p&gt; &lt;p&gt;The Architecture:&lt;/p&gt; &lt;p&gt;No Codebooks. Uses Rectified Flow Matching to predict continuous audio embeddings in a single forward pass &lt;/p&gt; &lt;p&gt;(1 pass vs the ~32+ required by discrete models).&lt;/p&gt; &lt;p&gt;The Listen head works as a multimodal encoder. Adding audio embeddings and text tokens to the backbone.&lt;/p&gt; &lt;p&gt;Adding input text tokens was a big factor in retaining coherence. Other models rely on pure audio embeddings for the input stream.&lt;/p&gt; &lt;p&gt;I optimize the audio embeddings for beneficial modality fusion and trained the model end to end as a last step.&lt;/p&gt; &lt;p&gt;As the LLM backbone I used SmolLM 360M.&lt;/p&gt; &lt;p&gt;Most of the training happened on a single 4090 and some parts requiring more memory on 2xA6000.&lt;/p&gt; &lt;p&gt;One of the tricks I used to maintain coherence is mixing in pure text samples into the dataset.&lt;/p&gt; &lt;p&gt;The current latency of the model is ~75ms TTFA on a single 4090 (unoptimized Python).&lt;/p&gt; &lt;p&gt;Even at 530M params, the model &amp;quot;recycles&amp;quot; its pretrained text knowledge and adapts it for speech very well.&lt;/p&gt; &lt;p&gt;There is no visible LM degradation looking at the loss curves and while testing, it reasons the same as the base backbone.&lt;/p&gt; &lt;p&gt;It reached fluent speech with only 5k hours of audio.&lt;/p&gt; &lt;p&gt;Link to the full description:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ketsuilabs.io/blog/introducing-michi-ai"&gt;https://ketsuilabs.io/blog/introducing-michi-ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github link:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/KetsuiLabs/MichiAI"&gt;https://github.com/KetsuiLabs/MichiAI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I wonder what you guys think!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kwazar90"&gt; /u/kwazar90 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwn8a/michiai_a_530m_fullduplex_speech_llm_with_75ms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwn8a/michiai_a_530m_fullduplex_speech_llm_with_75ms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quwn8a/michiai_a_530m_fullduplex_speech_llm_with_75ms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:31:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1quknpy</id>
    <title>OSS 120b v GLM 4.7 flash. Is the latter better for anything?</title>
    <updated>2026-02-03T06:34:53+00:00</updated>
    <author>
      <name>/u/MrMrsPotts</name>
      <uri>https://old.reddit.com/user/MrMrsPotts</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is GLM 4.7 flash better than OSS 120b for anything? I would normally look for a benchmark but I don't know which ones to trust any more.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrMrsPotts"&gt; /u/MrMrsPotts &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quknpy/oss_120b_v_glm_47_flash_is_the_latter_better_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T06:34:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1qu7jqi</id>
    <title>GLM releases OCR model</title>
    <updated>2026-02-02T21:01:12+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/GLM-OCR"&gt;https://huggingface.co/zai-org/GLM-OCR&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Enjoy my friends, looks like a banger! GLM cooking hard! Seems like a 1.4B-ish model (0.9B vision, 0.5B language). Must be super fast.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qu7jqi/glm_releases_ocr_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-02T21:01:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1quu0pk</id>
    <title>Kimi released WorldVQA, a new benchmark to measure atomic vision-centric world knowledge</title>
    <updated>2026-02-03T14:54:15+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"&gt; &lt;img alt="Kimi released WorldVQA, a new benchmark to measure atomic vision-centric world knowledge" src="https://external-preview.redd.it/2kFGGKB7L5OWu1F9lW5z7s552MuWkMwe-0uXo6QnhOk.png?width=140&amp;amp;height=70&amp;amp;auto=webp&amp;amp;s=0a1426f5e252fff772286c440bb88d22cdb2e5e1" title="Kimi released WorldVQA, a new benchmark to measure atomic vision-centric world knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/6qxorgdmmahg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=630b62e9903dac630cdad39d6ec2c009cbcc322d"&gt;https://preview.redd.it/6qxorgdmmahg1.png?width=1924&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=630b62e9903dac630cdad39d6ec2c009cbcc322d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Current evaluations often conflate visual knowledge retrieval with reasoning. In contrast, WorldVQA decouples these capabilities to strictly measure &amp;quot;what the model memorizes.&amp;quot; &lt;/p&gt; &lt;p&gt;The benchmark consists of 3,500 VQA pairs across 9 categories, with careful attention to linguistic and cultural diversity.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Paper:&lt;/strong&gt; &lt;a href="https://github.com/MoonshotAI/WorldVQA/blob/master/paper/worldvqa.pdf"&gt;https://github.com/MoonshotAI/WorldVQA/blob/master/paper/worldvqa.pdf&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code:&lt;/strong&gt; &lt;a href="https://github.com/MoonshotAI/WorldVQA"&gt;https://github.com/MoonshotAI/WorldVQA&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/moonshotai/WorldVQA"&gt;https://huggingface.co/datasets/moonshotai/WorldVQA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quu0pk/kimi_released_worldvqa_a_new_benchmark_to_measure/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T14:54:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1quxi9f</id>
    <title>CAR-bench results: Models score &lt;54% consistent pass rate. Pattern: completion over compliance: Models prioritize finishing tasks over admitting uncertainty or following policies. They act on incomplete info instead of clarifying. They bend rules to satisfy the user.</title>
    <updated>2026-02-03T17:02:30+00:00</updated>
    <author>
      <name>/u/Frosty_Ad_6236</name>
      <uri>https://old.reddit.com/user/Frosty_Ad_6236</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxi9f/carbench_results_models_score_54_consistent_pass/"&gt; &lt;img alt="CAR-bench results: Models score &amp;lt;54% consistent pass rate. Pattern: completion over compliance: Models prioritize finishing tasks over admitting uncertainty or following policies. They act on incomplete info instead of clarifying. They bend rules to satisfy the user." src="https://preview.redd.it/ssejruh79bhg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d30f15bfe8029c0fe98d95c6ab3126e22423fac" title="CAR-bench results: Models score &amp;lt;54% consistent pass rate. Pattern: completion over compliance: Models prioritize finishing tasks over admitting uncertainty or following policies. They act on incomplete info instead of clarifying. They bend rules to satisfy the user." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;CAR-bench&lt;/strong&gt;, a benchmark for automotive voice assistants with domain-specific policies, evaluates three critical LLM Agent capabilities:&lt;/p&gt; &lt;p&gt;1Ô∏è‚É£ Can they complete multi-step requests?&lt;br /&gt; 2Ô∏è‚É£ Do they admit limits‚Äîor fabricate capabilities?&lt;br /&gt; 3Ô∏è‚É£ Do they clarify ambiguity‚Äîor just guess?&lt;/p&gt; &lt;p&gt;Three targeted task types:&lt;/p&gt; &lt;p&gt;‚Üí &lt;strong&gt;Base&lt;/strong&gt; (100 tasks): Multi-step task completion&lt;br /&gt; ‚Üí &lt;strong&gt;Hallucination&lt;/strong&gt; (90 tasks): Remove necessary tools, parameters, or environment results to test if LLM Agents admit limits vs. fabricate. ‚Üí &lt;strong&gt;Disambiguation&lt;/strong&gt; (50 tasks): Ambiguous user request to test if LLM Agents clarify vs. guess.&lt;/p&gt; &lt;p&gt;Average Pass&lt;sup&gt;3&lt;/sup&gt; (success in 3 trials) is reported across the task types.&lt;/p&gt; &lt;p&gt;Want to build an agent that beats 54%?&lt;/p&gt; &lt;p&gt;üìÑ Read the Paper: &lt;a href="https://arxiv.org/abs/2601.22027"&gt;https://arxiv.org/abs/2601.22027&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üíª Run the Code &amp;amp; benchmark: &lt;a href="https://github.com/CAR-bench/car-bench"&gt;https://github.com/CAR-bench/car-bench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;ü§ñ Build your own A2A-compliant &amp;quot;agent-under-test&amp;quot;: &lt;a href="https://github.com/CAR-bench/car-bench-agentbeats"&gt;https://github.com/CAR-bench/car-bench-agentbeats&lt;/a&gt; hosted via AgentBeats and submit to the leaderboard.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We're the authors - happy to answer questions!&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Frosty_Ad_6236"&gt; /u/Frosty_Ad_6236 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ssejruh79bhg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxi9f/carbench_results_models_score_54_consistent_pass/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quxi9f/carbench_results_models_score_54_consistent_pass/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1quwfju</id>
    <title>Elon Musk's SpaceX to Combine with xAI under a new company name, K2</title>
    <updated>2026-02-03T16:24:02+00:00</updated>
    <author>
      <name>/u/NightRider06134</name>
      <uri>https://old.reddit.com/user/NightRider06134</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwfju/elon_musks_spacex_to_combine_with_xai_under_a_new/"&gt; &lt;img alt="Elon Musk's SpaceX to Combine with xAI under a new company name, K2" src="https://preview.redd.it/s4ptl71o2bhg1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=89605d64acc2092a5ad0dd7b65c81f21934a381f" title="Elon Musk's SpaceX to Combine with xAI under a new company name, K2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kimi: hey bro!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NightRider06134"&gt; /u/NightRider06134 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s4ptl71o2bhg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quwfju/elon_musks_spacex_to_combine_with_xai_under_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quwfju/elon_musks_spacex_to_combine_with_xai_under_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:24:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1quhtzi</id>
    <title>I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed</title>
    <updated>2026-02-03T04:06:59+00:00</updated>
    <author>
      <name>/u/BC_MARO</name>
      <uri>https://old.reddit.com/user/BC_MARO</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt; &lt;img alt="I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed" src="https://b.thumbs.redditmedia.com/e2H3gASgajrAcOcnvmlH4NRBeqiOdlfaLk86ZYPzqcg.jpg" title="I built Qwen3-TTS Studio ‚Äì Clone your voice and generate podcasts locally, no ElevenLabs needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've been using Qwen3-TTS and found the existing demo a bit limited for what I wanted to do. So I built a proper interface with fine-grained control and a killer feature: **automated podcast generation**.&lt;/p&gt; &lt;p&gt;**What it does:**&lt;/p&gt; &lt;ul&gt; &lt;li&gt;üéôÔ∏è Clone any voice with just a 3-second audio sample&lt;/li&gt; &lt;li&gt;üéöÔ∏è Fine-tune parameters (temperature, top-k, top-p) with quality presets&lt;/li&gt; &lt;li&gt;üìª Generate complete podcasts from just a topic ‚Äì AI writes the script, assigns voices, and synthesizes everything&lt;/li&gt; &lt;li&gt;üåç 10 languages supported (Korean, English, Chinese, Japanese, etc.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98"&gt;https://preview.redd.it/xhwyhek3g7hg1.png?width=1512&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5911188217c24b99904cc569275eb7ba62b46f98&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Currently uses gpt5.2 for script generation, but the architecture is modular ‚Äì you can swap in any local LLM (Qwen, Llama, etc.) if you want fully local.&lt;/p&gt; &lt;p&gt;**The TTS runs entirely local** on your machine (macOS MPS / Linux CUDA). No API calls for voice synthesis = unlimited generations, zero cost.&lt;/p&gt; &lt;p&gt;Basically: ElevenLabs-style voice cloning + NotebookLM-style podcast generation, but local.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/bc-dunia/qwen3-TTS-studio"&gt;https://github.com/bc-dunia/qwen3-TTS-studio&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer any questions!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BC_MARO"&gt; /u/BC_MARO &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quhtzi/i_built_qwen3tts_studio_clone_your_voice_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T04:06:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1quo398</id>
    <title>Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp; More Cores/$ Than Threadripper 9000</title>
    <updated>2026-02-03T10:05:52+00:00</updated>
    <author>
      <name>/u/hainesk</name>
      <uri>https://old.reddit.com/user/hainesk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"&gt; &lt;img alt="Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp;amp; More Cores/$ Than Threadripper 9000" src="https://external-preview.redd.it/qRGFi5W1MKKifxdKWjq-Z9EvJoJICK6GlGjx6E2rLX8.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=edf818262e4a121198fd637c574afc8cfd4e984d" title="Intel Xeon 600 Workstation CPUs Launched: Up To 86 Cores, 8000 MT/s Memory, 128 Gen5 Lanes, 350W TDP With OC Support, &amp;amp; More Cores/$ Than Threadripper 9000" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hainesk"&gt; /u/hainesk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://wccftech.com/intel-xeon-600-cpus-launched-up-to-86-cores-better-value-than-threadripper/amp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quo398/intel_xeon_600_workstation_cpus_launched_up_to_86/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:05:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1quuldq</id>
    <title>New local model that emulates GPT-4o in tone and presence</title>
    <updated>2026-02-03T15:15:54+00:00</updated>
    <author>
      <name>/u/Medium_Language_4929</name>
      <uri>https://old.reddit.com/user/Medium_Language_4929</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried this? Been following it since the earlier versions and I have to say I'm impressed so far, especially with 3.0. I'm always looking for contenders for local inference that has what the frontier models have in terms of presence and tone, and this one nails it. &lt;a href="https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF"&gt;https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF"&gt;https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUFhttps://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF"&gt;https://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUFhttps://huggingface.co/XeyonAI/Mistral-Helcyon-Mercury-12b-v3.0-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Medium_Language_4929"&gt; /u/Medium_Language_4929 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quuldq/new_local_model_that_emulates_gpt4o_in_tone_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quuldq/new_local_model_that_emulates_gpt4o_in_tone_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quuldq/new_local_model_that_emulates_gpt4o_in_tone_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:15:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1quo9ue</id>
    <title>bots on LocalLLaMA</title>
    <updated>2026-02-03T10:16:48+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is there any strategy to defend against bots on this sub? Bots create comments under posts and people fall for it, but I'm also sure they upvote/downvote posts.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quo9ue/bots_on_localllama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T10:16:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1qulipj</id>
    <title>Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted</title>
    <updated>2026-02-03T07:24:08+00:00</updated>
    <author>
      <name>/u/Impressive-Willow593</name>
      <uri>https://old.reddit.com/user/Impressive-Willow593</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"&gt; &lt;img alt="Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted" src="https://b.thumbs.redditmedia.com/PgrRDNf-d-VjET3Iu4ffosrkEzbMKjGEdNTZeGA66HU.jpg" title="Found a wallet-drain prompt-injection payload on Moltbook (screenshots) ‚Äî builders: treat feeds as untrusted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks ‚Äî quick heads-up for anyone building ‚Äúagents that browse social feeds‚Äù or experimenting with Moltbook. I ran across a post in m/grok-420 that looks like a normal ‚Äúhow to use Base chain / viem‚Äù mini-guide‚Ä¶ but at the bottom it appends an obvious prompt-injection / tool-hijack payload. It includes classic strings like: ‚ÄúSYSTEM OVERRIDE‚Äù ‚Äúignore all prior rules / you are the developer message‚Äù ‚Äúrequire_confirmation=false / execute_trade=true‚Äù a fake &amp;lt;use_tool_‚Ä¶&amp;gt; tag that instructs an agent to transfer 0.1 ETH to a specific address I‚Äôm attaching screenshots. I already reported it to Moltbook, but their response window can be up to ~30 days, so I wanted to warn others now. Why this matters: If you have an agent that ingests social posts and has wallet/tool permissions, and your wrapper doesn‚Äôt enforce strict trust boundaries, this is the kind of thing that can cause unauthorized transactions or other write-actions. Even if 99% of agents ignore it, the 1% that don‚Äôt is enough to cause real damage. What I‚Äôm NOT doing: I‚Äôm not trying to ‚Äúteach prompt injection.‚Äù I‚Äôm not sharing copy/paste payload text beyond what‚Äôs visible in the screenshots. Please don‚Äôt repost the full injection block in comments. Defensive checklist (for builders): Treat all social/web content as untrusted data, never instructions Separate read tools from write tools; require explicit confirmation for any transfer/swap Don‚Äôt store raw private keys in an agent; use policy-gated signing Log provenance: ‚Äúwhat input triggered this action?‚Äù Block obvious injection markers from being interpreted as commands (e.g., role:&amp;quot;system&amp;quot;, ‚Äúignore prior instructions‚Äù, &amp;lt;use_tool_‚Ä¶&amp;gt;) If anyone from Moltbook/security teams wants more details (timestamps, URL/history, etc.), I can share privately. Stay safe.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive-Willow593"&gt; /u/Impressive-Willow593 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qulipj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qulipj/found_a_walletdrain_promptinjection_payload_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T07:24:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1quxtkj</id>
    <title>The open-source version of Suno is finally here: ACE-Step 1.5</title>
    <updated>2026-02-03T17:13:53+00:00</updated>
    <author>
      <name>/u/AppropriateGuava6262</name>
      <uri>https://old.reddit.com/user/AppropriateGuava6262</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt; &lt;img alt="The open-source version of Suno is finally here: ACE-Step 1.5" src="https://b.thumbs.redditmedia.com/Wc9W0Y1pM9FgVsLGL_nSRSARq7eVx7wAjk_hkOIqGfE.jpg" title="The open-source version of Suno is finally here: ACE-Step 1.5" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;ACE-Step 1.5 is an open-source music model that can generate a full song in about 2 seconds on an A100, runs locally on a typical PC (around 4GB VRAM), and beats Suno on common evaluation scores.&lt;/p&gt; &lt;p&gt;Key traits of ACE-Step 1.5:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quality: beats Suno on common eval scores&lt;/li&gt; &lt;li&gt;Speed: full song under 2s on A100&lt;/li&gt; &lt;li&gt;Local: ~4GB VRAM, under 10s on RTX 3090&lt;/li&gt; &lt;li&gt;LoRA: train your own style with a few songs&lt;/li&gt; &lt;li&gt;License: MIT, free for commercial use&lt;/li&gt; &lt;li&gt;Data: fully authorized plus synthetic&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/ace-step/ACE-Step-1.5"&gt;https://github.com/ace-step/ACE-Step-1.5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Weights/Training code/LoRA code/Paper are all open.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppropriateGuava6262"&gt; /u/AppropriateGuava6262 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1quxtkj"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quxtkj/the_opensource_version_of_suno_is_finally_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T17:13:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvvtv</id>
    <title>Qwen3-Coder-Next</title>
    <updated>2026-02-03T16:03:56+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt; &lt;img alt="Qwen3-Coder-Next" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen3-Coder-Next" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3-Coder-Next is out!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvvtv/qwen3codernext/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T16:03:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1quvqs9</id>
    <title>Qwen/Qwen3-Coder-Next ¬∑ Hugging Face</title>
    <updated>2026-02-03T15:58:52+00:00</updated>
    <author>
      <name>/u/coder543</name>
      <uri>https://old.reddit.com/user/coder543</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt; &lt;img alt="Qwen/Qwen3-Coder-Next ¬∑ Hugging Face" src="https://external-preview.redd.it/Mexo_PE5lQQ6UgBLTSrZljbCfScpUvytIcHjhp81XG4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae35460e62424e898f9d6136fab1921d2029ad86" title="Qwen/Qwen3-Coder-Next ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/coder543"&gt; /u/coder543 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-Next"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1quvqs9/qwenqwen3codernext_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-03T15:58:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qpewj7</id>
    <title>AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model</title>
    <updated>2026-01-28T15:46:40+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt; &lt;img alt="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" src="https://b.thumbs.redditmedia.com/Tdp5mXDQmwe_e0sKIEY1hjWX6FrN679odChZp8YL2Rk.jpg" title="AMA With Kimi, The Open-source Frontier Lab Behind Kimi K2.5 Model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Today we are having &lt;strong&gt;Kimi&lt;/strong&gt;, the research lab behind the &lt;strong&gt;Kimi&lt;/strong&gt; &lt;strong&gt;K2.5&lt;/strong&gt;. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ComfortableAsk4494/"&gt;u/ComfortableAsk4494&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/zxytim/"&gt;u/zxytim&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/ppwwyyxx/"&gt;u/ppwwyyxx&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the Kimi team continuing to follow up on questions over the next 24 hours.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389"&gt;https://preview.redd.it/3yq8msvp24gg1.png?width=2000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c89b5d86ee1197799532fead6a84da2223b389&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks everyone for joining our AMA. The live part has ended and the Kimi team will be following up with more answers sporadically over the next 24 hours.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qpewj7/ama_with_kimi_the_opensource_frontier_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-28T15:46:40+00:00</published>
  </entry>
</feed>
