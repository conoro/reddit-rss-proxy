<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-02T16:41:28+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1q1bxci</id>
    <title>support for IQuest-Coder-V1-40B has been merged into llama.cpp</title>
    <updated>2026-01-01T19:00:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bxci/support_for_iquestcoderv140b_has_been_merged_into/"&gt; &lt;img alt="support for IQuest-Coder-V1-40B has been merged into llama.cpp" src="https://external-preview.redd.it/9ahEimrF0NVJzd5JMsSv-jhsKUqTcPwMInlAFk_SJHU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=766f07b197f35e4577b070c65f9edf022eb3283f" title="support for IQuest-Coder-V1-40B has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(one line PR!)&lt;/p&gt; &lt;p&gt;IQuest-Coder-V1 is a new family of code large language models (LLMs) designed to advance autonomous software engineering and code intelligence. Built on the innovative code-flow multi-stage training paradigm, IQuest-Coder-V1 captures the dynamic evolution of software logic, delivering state-of-the-art performance across critical dimensions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;State-of-the-Art Performance&lt;/strong&gt;: Achieves leading results on SWE-Bench Verified (81.4%), BigCodeBench (49.9%), LiveCodeBench v6 (81.1%), and other major coding benchmarks, surpassing competitive models across agentic software engineering, competitive programming, and complex tool use.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code-Flow Training Paradigm&lt;/strong&gt;: Moving beyond static code representations, our models learn from repository evolution patterns, commit transitions, and dynamic code transformations to understand real-world software development processes.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Dual Specialization Paths&lt;/strong&gt;: Bifurcated post-training delivers two specialized variantsâ€”Thinking models (utilizing reasoning-driven RL for complex problem-solving) and Instruct models (optimized for general coding assistance and instruction-following).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient Architecture&lt;/strong&gt;: The IQuest-Coder-V1-Loop variant introduces a recurrent mechanism that optimizes the trade-off between model capacity and deployment footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Native Long Context&lt;/strong&gt;: All models natively support up to 128K tokens without requiring additional scaling techniques.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18524"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bxci/support_for_iquestcoderv140b_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1bxci/support_for_iquestcoderv140b_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T19:00:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1x6e0</id>
    <title>Vibevoice how can I run it locally, without comfyui?</title>
    <updated>2026-01-02T12:17:43+00:00</updated>
    <author>
      <name>/u/ResponsibleTruck4717</name>
      <uri>https://old.reddit.com/user/ResponsibleTruck4717</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know I can run with comfyui but I had problem installing it, any other frameworks / apps?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ResponsibleTruck4717"&gt; /u/ResponsibleTruck4717 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1x6e0/vibevoice_how_can_i_run_it_locally_without_comfyui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1x6e0/vibevoice_how_can_i_run_it_locally_without_comfyui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1x6e0/vibevoice_how_can_i_run_it_locally_without_comfyui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T12:17:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1g7pp</id>
    <title>Solar-Open-100B-GGUF is here!</title>
    <updated>2026-01-01T21:51:33+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1g7pp/solaropen100bgguf_is_here/"&gt; &lt;img alt="Solar-Open-100B-GGUF is here!" src="https://external-preview.redd.it/NhmgyNgn5k9aKcKZk_ELLO4LdarQGlwMfU3zdGmwpqM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4a8af6055f2721b2fd950e489426819702e5c350" title="Solar-Open-100B-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Solar Open&lt;/strong&gt; is a massive &lt;strong&gt;102B-parameter&lt;/strong&gt; Mixture-of-Experts (MoE) model trained from scratch on &lt;strong&gt;19.7 trillion tokens&lt;/strong&gt;. It uses only &lt;strong&gt;12B active parameters&lt;/strong&gt; during inference.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/Solar-Open-100B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1g7pp/solaropen100bgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1g7pp/solaropen100bgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T21:51:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1k9g3</id>
    <title>LFM2 2.6B-Exp on Android: 40+ TPS and 32K context</title>
    <updated>2026-01-02T00:43:10+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm hugely impressed with LiquidAI's new LFM2 2.6B-Exp model, performing at GPT-4 levels across a wide variety of benchmarks (many but perhaps not quite most), plus reasoning too. Try the cloud version here: &lt;a href="https://playground.liquid.ai/chat?model=cmjdu187p00013b6o7tttjvlw"&gt;https://playground.liquid.ai/chat?model=cmjdu187p00013b6o7tttjvlw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;LFM2 uses a hybrid design (gated convolutions and grouped query attention), so it has a tiny KV cache footprint. This makes it capable of super smart, high speed, long context local inference on phones.&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="https://huggingface.co/LiquidAI/LFM2-2.6B-Exp-GGUF"&gt;https://huggingface.co/LiquidAI/LFM2-2.6B-Exp-GGUF&lt;/a&gt; with llama.cpp:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Download &lt;a href="https://huggingface.co/LiquidAI/LFM2-2.6B-Exp-GGUF/resolve/main/LFM2-2.6B-Exp-Q4_K_M.gguf"&gt;LFM2-2.6B-Exp-Q4_K_M.gguf&lt;/a&gt; (~1.6GB);&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Get &lt;a href="https://play.google.com/store/apps/details?id=com.pocketpalai"&gt;PocketPal AI&lt;/a&gt; or &lt;a href="https://play.google.com/store/apps/details?id=com.danemadsen.maid"&gt;Maid&lt;/a&gt; from the Google Play Store or GitHub&lt;a href="https://github.com/a-ghorbani/pocketpal-ai"&gt;[1]&lt;/a&gt;&lt;a href="https://github.com/Mobile-Artificial-Intelligence/maid"&gt;[2]&lt;/a&gt;. Or better, install Termux and compile llama.cpp with OpenCL support to utilize your phone's GPU (&lt;a href="https://github.com/JackZeng0208/llama.cpp-android-tutorial"&gt;tutorial for Adreno support&lt;/a&gt;. Get Termux from &lt;a href="https://f-droid.org/packages/com.termux/"&gt;F-Droid&lt;/a&gt; or &lt;a href="https://github.com/termux/termux-app/releases"&gt;GitHub&lt;/a&gt;, NOT the Google Play Store -- the Play Store version is outdated and will fail to compile current llama.cpp code.)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Import the local model file using these sampler settings recommended by Liquid AI:&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;ul&gt; &lt;li&gt;Temperature: 0.3&lt;/li&gt; &lt;li&gt;Min-P: 0.15&lt;/li&gt; &lt;li&gt;Repetition Penalty: 1.05&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Those values support the &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt; tag for reasoning. If &lt;code&gt;--jinja&lt;/code&gt; on the command line (optionally after building with &lt;code&gt;--reasoning-format none&lt;/code&gt; to show all the reasoning tokens) doesn't get you reasoning, this system prompt will:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;You are a helpful AI assistant. You always reason before responding, using the following format: &amp;lt;think&amp;gt; your internal reasoning &amp;lt;/think&amp;gt; your external response.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;PocketPal has GPU support on iOS using Apple's &amp;quot;Metal&amp;quot; API, but I don't have an iPhone so I can't vouch for whether it achieves the 40+ tokens/second you can get with the Termux method compiling llama.cpp with &lt;code&gt;GGML_OPENCL=ON&lt;/code&gt; on Android.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1k9g3/lfm2_26bexp_on_android_40_tps_and_32k_context/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1k9g3/lfm2_26bexp_on_android_40_tps_and_32k_context/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1k9g3/lfm2_26bexp_on_android_40_tps_and_32k_context/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T00:43:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1vgqc</id>
    <title>Which is the best available open source model for TTS + cloning?</title>
    <updated>2026-01-02T10:40:07+00:00</updated>
    <author>
      <name>/u/GeekoGeek</name>
      <uri>https://old.reddit.com/user/GeekoGeek</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have been searching for TTS and voice-cloning related models. I found f5-tts to be very good at cloning, but it has license restrictions.&lt;br /&gt; I want to convert large PDFs, docx, and books to audiobooks with cloned voices. I would appreciate any help. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/GeekoGeek"&gt; /u/GeekoGeek &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1vgqc/which_is_the_best_available_open_source_model_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1vgqc/which_is_the_best_available_open_source_model_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1vgqc/which_is_the_best_available_open_source_model_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T10:40:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1k2al</id>
    <title>LM Studio MCP</title>
    <updated>2026-01-02T00:34:24+00:00</updated>
    <author>
      <name>/u/Serious_Molasses313</name>
      <uri>https://old.reddit.com/user/Serious_Molasses313</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1k2al/lm_studio_mcp/"&gt; &lt;img alt="LM Studio MCP" src="https://external-preview.redd.it/MGFkbmw3c3Z6dGFnMYMsEIwVA3CjPImT0TqNaeMWo1Z4CqRfpk_ys7vjHmKG.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ce2d3be57645a246d9305c0446bda24b68b57e3d" title="LM Studio MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TITLE: Local AI Agent: Daily News Automation with GPT-OSS 20B&lt;/p&gt; &lt;p&gt;OVERVIEW: I just automated my entire &amp;quot;Daily Instagram News&amp;quot; pipeline using a single prompt and GPT-OSS 20B running locally. No subscriptions, no API feesâ€”just raw open-source power interacting with my local machine.&lt;/p&gt; &lt;p&gt;THE STACK: - Model: GPT-OSS 20B (Local) - Environment: LM Studio / Local Agent Framework - Capabilities: Web scraping, Google Search, and Local File I/O&lt;/p&gt; &lt;p&gt;THE ONE-PROMPT WORKFLOW: &amp;quot;Scrape my Instagram feed for the latest 10 posts, cross-reference trends (SpaceX, Wall Street) via Google, and save a professional Markdown briefing to my 'World News' folder.&amp;quot;&lt;/p&gt; &lt;p&gt;LOGIC CHAIN EXECUTION: 1. SCRAPE: Headless browser pulls top IG captions &amp;amp; trends. 2. RESEARCH: Fetches broader context (e.g., SpaceX valuation) via Google. 3. SYNTHESIZE: Summarizes data into a clean, professional news format. 4. DEPLOY: Writes .md file directly to the local project directory.&lt;/p&gt; &lt;p&gt;WHY LOCAL 20B IS A GAME-CHANGER: - Privacy: My Instagram data and local file paths never touch a corporate cloud. - Reasoning: The 20B parameter size is the &amp;quot;sweet spot&amp;quot;â€”small enough to run on consumer GPUs, but smart enough to handle complex tool-calling. - Zero Cost: Unlimited runs without worrying about token costs or rate limits.&lt;/p&gt; &lt;p&gt;PRO-TIPS FOR LOCAL AGENTS: - Handle Cooldowns: Build a &amp;quot;wait_cooldown&amp;quot; function into your search tool to avoid IP blocks. - Strict Pathing: Hard-code &amp;quot;allowed&amp;quot; directories in your Python tools for better security.&lt;/p&gt; &lt;p&gt;TL;DR: Open-source models have reached the point where they can act as autonomous personal assistants.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;6GB Vram 32GBddr5 &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Serious_Molasses313"&gt; /u/Serious_Molasses313 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/hi8l2yrvztag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1k2al/lm_studio_mcp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1k2al/lm_studio_mcp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T00:34:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ge7u</id>
    <title>Youtu-LLM-2B-GGUF is here!</title>
    <updated>2026-01-01T21:58:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ge7u/youtullm2bgguf_is_here/"&gt; &lt;img alt="Youtu-LLM-2B-GGUF is here!" src="https://external-preview.redd.it/Y37xpjhN-nCEndZ3yq8G_giypCpe4DWVeooeM-2eJFQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa31ea451fa2270258ecf921a02d20fd0e93ef9e" title="Youtu-LLM-2B-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;Youtu-LLM-2B&lt;/strong&gt; is a highly efficient 1.96B parameter model featuring a Dense MLA architecture and a native &lt;strong&gt;128K context window&lt;/strong&gt;. Despite its small size, it supports &lt;strong&gt;Agentic capabilities&lt;/strong&gt; and &amp;quot;Reasoning Mode&amp;quot; (Chain of Thought), outperforming many larger models in STEM, coding, and agentic benchmarks.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/Youtu-LLM-2B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ge7u/youtullm2bgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ge7u/youtullm2bgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T21:58:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1q230h2</id>
    <title>student seeking feedback</title>
    <updated>2026-01-02T16:25:25+00:00</updated>
    <author>
      <name>/u/Fragrant_Basis_5648</name>
      <uri>https://old.reddit.com/user/Fragrant_Basis_5648</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hey folks,&lt;/p&gt; &lt;p&gt;iâ€™m a cs student and i built a small open-source tool called &lt;strong&gt;basis router&lt;/strong&gt;. it routes large data (s3, postgres, mongodb, etc.) to llms across providers (openai / anthropic / gemini) with chunking + aggregation handled for you.&lt;/p&gt; &lt;p&gt;before i invest more time: is this something youâ€™d actually use in your projects or work? if not, whatâ€™s missing or unconvincing?&lt;/p&gt; &lt;p&gt;github repo: &lt;a href="https://github.com/Jity01/basis-2"&gt;https://github.com/Jity01/basis-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;--&lt;/p&gt; &lt;p&gt;edit: one more question actually, would adding local models be helpful?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fragrant_Basis_5648"&gt; /u/Fragrant_Basis_5648 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q230h2/student_seeking_feedback/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q230h2/student_seeking_feedback/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q230h2/student_seeking_feedback/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T16:25:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1q20npx</id>
    <title>Just got an RTX Pro 6000 - need recommendations for processing a massive dataset with instruction following</title>
    <updated>2026-01-02T14:55:47+00:00</updated>
    <author>
      <name>/u/Sensitive_Sweet_1850</name>
      <uri>https://old.reddit.com/user/Sensitive_Sweet_1850</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, so I recently picked up an RTX Pro 6000 and I'm looking to put it to good use. I have a pretty large dataset that needs processing - we're talking around 300 million tokens here. The tricky part is that I need the model to follow very specific instructions while processing this data, so instruction following capability is crucial for my use case.&lt;/p&gt; &lt;p&gt;I've been doing some research but honestly there are so many open-weight models out there right now that it's hard to keep track of what's actually good for this kind of workload. I'm not looking for the biggest model necessarily, just something that can handle instruction following really well while being efficient enough to churn through this much data without taking forever.&lt;/p&gt; &lt;p&gt;What would you guys recommend? Has anyone here done something similar with large-scale dataset processing? I'm open to suggestions on model choice, quantization options, or any tips on optimizing throughput. Would really appreciate any insights from people who've actually battle-tested these models on serious workloads.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sensitive_Sweet_1850"&gt; /u/Sensitive_Sweet_1850 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q20npx/just_got_an_rtx_pro_6000_need_recommendations_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q20npx/just_got_an_rtx_pro_6000_need_recommendations_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q20npx/just_got_an_rtx_pro_6000_need_recommendations_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T14:55:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1986x</id>
    <title>IQuestCoder - new 40B dense coding model</title>
    <updated>2026-01-01T17:12:51+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"&gt; &lt;img alt="IQuestCoder - new 40B dense coding model" src="https://external-preview.redd.it/puEaI60nzHUbVlmNCXfE1sl9fmhvVgJHgKO3FYQHywY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7035b6af6efe8e8b4c47de0ff3945cb68bc2973c" title="IQuestCoder - new 40B dense coding model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As usual, benchmarks claim it's absolutely SOTA and crushes the competition. Since I'm willing to verify it, I've adapted it to GGUF. It's basically Llama arch (reportedly was supposed to be using SWA, but it didn't get used in the final version), so works out of the box with Llama.cpp.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ilintar/IQuest-Coder-V1-40B-Instruct-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1986x/iquestcoder_new_40b_dense_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-01T17:12:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1p6cy</id>
    <title>Which is the current best ERP model ~8b?</title>
    <updated>2026-01-02T04:34:12+00:00</updated>
    <author>
      <name>/u/spritefanty</name>
      <uri>https://old.reddit.com/user/spritefanty</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Let me goon guys ðŸ˜­&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spritefanty"&gt; /u/spritefanty &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p6cy/which_is_the_current_best_erp_model_8b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p6cy/which_is_the_current_best_erp_model_8b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p6cy/which_is_the_current_best_erp_model_8b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T04:34:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1sq7x</id>
    <title>Upstage Solar-Open Validation Session.l</title>
    <updated>2026-01-02T07:50:12+00:00</updated>
    <author>
      <name>/u/Desperate-Sir-5088</name>
      <uri>https://old.reddit.com/user/Desperate-Sir-5088</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.youtube.com/live/2YY9aAUSo_w?si=C_j7CcgR0c1kqexf"&gt;https://www.youtube.com/live/2YY9aAUSo_w?si=C_j7CcgR0c1kqexf&lt;/a&gt;&lt;/p&gt; &lt;p&gt;CEO Mr. Sung Kim explained a model archtecture &amp;amp; opened WanDB logs.&lt;/p&gt; &lt;p&gt;c.f All sessions were conducted in Korean. If necessary, converting them with notebookLM for prefer language in later. It almost may preserve the original nuance in English&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Desperate-Sir-5088"&gt; /u/Desperate-Sir-5088 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1sq7x/upstage_solaropen_validation_sessionl/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1sq7x/upstage_solaropen_validation_sessionl/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1sq7x/upstage_solaropen_validation_sessionl/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T07:50:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1mycx</id>
    <title>The Optimal Architecture for Small Language Models</title>
    <updated>2026-01-02T02:46:36+00:00</updated>
    <author>
      <name>/u/asankhs</name>
      <uri>https://old.reddit.com/user/asankhs</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1mycx/the_optimal_architecture_for_small_language_models/"&gt; &lt;img alt="The Optimal Architecture for Small Language Models" src="https://external-preview.redd.it/MbzQ4EvxKebUzYSzc2CTx7R3Iko3eawCPnYZllcCxz0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb69d67dc82ba8be99004df843dff727cdc1ba0f" title="The Optimal Architecture for Small Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/asankhs"&gt; /u/asankhs &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/codelion/optimal-model-architecture"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1mycx/the_optimal_architecture_for_small_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1mycx/the_optimal_architecture_for_small_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T02:46:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1qepk</id>
    <title>Deep Research Agent, an autonomous research agent system</title>
    <updated>2026-01-02T05:37:27+00:00</updated>
    <author>
      <name>/u/martian7r</name>
      <uri>https://old.reddit.com/user/martian7r</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1qepk/deep_research_agent_an_autonomous_research_agent/"&gt; &lt;img alt="Deep Research Agent, an autonomous research agent system" src="https://external-preview.redd.it/cnQ2bWFnMjBpdmFnMWx1yzDmXrZrGTnA-76VDcIJKHxAik455r_NvogMYYoY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c82a051dca0471a20056357f59ee10610cb23cb5" title="Deep Research Agent, an autonomous research agent system" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/tarun7r/deep-research-agent"&gt;https://github.com/tarun7r/deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most AI research agents simply summarize the first few search results and present them as analysis. I wanted something more rigorous, something closer to how a human analyst would plan, verify, and synthesize information.&lt;/p&gt; &lt;p&gt;How It Works (Architecture)&lt;/p&gt; &lt;p&gt;Instead of relying on a single LLM loop, this system coordinates four specialized agents:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Planner&lt;/strong&gt; â€“ Analyzes the topic and creates a strategic research plan&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Searcher&lt;/strong&gt; â€“ Autonomously determines what to query and retrieves deeper, high-value content&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Synthesizer&lt;/strong&gt; â€“ Aggregates findings and prioritizes sources using a credibility scoring mechanism&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Writer&lt;/strong&gt; â€“ Produces a structured research report with citations (APA, MLA, IEEE) and self-corrects weak sections&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Credibility Scoring: The Key Differentiator&lt;/p&gt; &lt;p&gt;Hallucinations are one of the biggest challenges in AI-assisted research. To reduce misinformation, the system assigns each source a credibility score (0â€“100) before content is summarized. Scoring considers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Domain authority (.edu, .gov, peer-reviewed publications, reputable institutions)&lt;/li&gt; &lt;li&gt;Academic writing indicators&lt;/li&gt; &lt;li&gt;Structural trust signals&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This ensures low-quality sources are filtered out before they influence results.&lt;/p&gt; &lt;p&gt;Built With: Python, LangGraph and LangChain, Chainlit&lt;/p&gt; &lt;p&gt;If you are interested, feel free to explore the code, star the project, and contribute.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/martian7r"&gt; /u/martian7r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kv7vji10ivag1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1qepk/deep_research_agent_an_autonomous_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1qepk/deep_research_agent_an_autonomous_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T05:37:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ntkh</id>
    <title>I built a simple Web UI for training and running LLM experiments on your local computer! Inspired by minGPT project.</title>
    <updated>2026-01-02T03:27:06+00:00</updated>
    <author>
      <name>/u/Maxwell10206</name>
      <uri>https://old.reddit.com/user/Maxwell10206</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ntkh/i_built_a_simple_web_ui_for_training_and_running/"&gt; &lt;img alt="I built a simple Web UI for training and running LLM experiments on your local computer! Inspired by minGPT project." src="https://b.thumbs.redditmedia.com/NiQ7WKE58qMIxYT2YbL_iLhmCuWAxahmtcA-k3N_GMw.jpg" title="I built a simple Web UI for training and running LLM experiments on your local computer! Inspired by minGPT project." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was playing around with the open source project called minGPT. And started to build a ton of scripts and running many different training experiments using different datasets I was either download or generating. It became a huge mess quickly and lost track of a lot of things. So I got inspired to build my own local web ui for building datasets, configuration files, running training experiments and inspecting the outputs of LLMs. Thought I would share it here to see what everyone thought or if anything similar exists already xD&lt;/p&gt; &lt;p&gt;You can find it on GitHub here &lt;a href="https://github.com/MaxHastings/llm-madness"&gt;https://github.com/MaxHastings/llm-madness&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Maxwell10206"&gt; /u/Maxwell10206 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q1ntkh"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ntkh/i_built_a_simple_web_ui_for_training_and_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ntkh/i_built_a_simple_web_ui_for_training_and_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T03:27:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1w2tg</id>
    <title>88% vs 76%: Multimodal outperforms text embeddings on visual docs in RAG</title>
    <updated>2026-01-02T11:16:15+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Building a RAG system for docs with mixed content: text, tables, charts. I wanted to know if multimodal embeddings are worth it or if text would be just fine.&lt;/p&gt; &lt;p&gt;Decided to test it out. I had two approaches:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;Convert everything to text, use text embeddings&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Keep images as images, use multimodal embeddings&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;After running 150 queries on identical setups across DocVQA (text + tables), ChartQA (charts), and AI2D (diagrams):&lt;/p&gt; &lt;p&gt;Results of Recall@1:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Tables = multimodal 88%, text 76% (12-point gap)&lt;/li&gt; &lt;li&gt;Charts = multimodal 92%, text 90% (small edge)&lt;/li&gt; &lt;li&gt; Pure text = text 96%, multimodal 92% (text wins)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Takeaway: for dealing with visual docs, multimodal seem to be the better default. But for pure text, text embeddings would be enough. &lt;/p&gt; &lt;p&gt;(posted a write-up of full breakdown here: &lt;a href="https://agentset.ai/blog/multimodal-vs-text-embeddings"&gt;https://agentset.ai/blog/multimodal-vs-text-embeddings&lt;/a&gt; )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w2tg/88_vs_76_multimodal_outperforms_text_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w2tg/88_vs_76_multimodal_outperforms_text_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w2tg/88_vs_76_multimodal_outperforms_text_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T11:16:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1lgb7</id>
    <title>TIL you can allocate 128 GB of unified memory to normal AMD iGPUs on Linux via GTT</title>
    <updated>2026-01-02T01:37:11+00:00</updated>
    <author>
      <name>/u/1ncehost</name>
      <uri>https://old.reddit.com/user/1ncehost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I am training a 1B model right now on my 7900 XTX with some custom kernels I wrote, and while it is training I wanted to optimize the kernels at the same time. However, my VRAM is nearly maxed doing training, so its not ideal.&lt;/p&gt; &lt;p&gt;Then I realized maybe my 2 CU Raphael iGPU might be able to help since I only need to run some limited samples and the speed isn't as important for optimization as it is for training. After doing some research, it turned out that not only does ROCm recognize the iGPU, but a Linux feature called Graphics Translation Table (GTT) for AMD iGPUs can use up to 128 GB of system memory as VRAM. It even allocates it dynamically, so it isn't removed from your CPU's memory pool until it is allocated. I think a lot of people running Strix Halo are probably using the bios setting, but if you are running Linux you should check to see if GTT works for you since its dynamically allocated.&lt;/p&gt; &lt;p&gt;This isn't very useful for most people:&lt;/p&gt; &lt;p&gt;1) It isn't going to be good for inference because iGPUs are very very slow, and usually the CPU itself is faster for inference.&lt;/p&gt; &lt;p&gt;2) I'm accessing ROCm directly via C++ / HIP kernels, so I can avoid all the support issues ROCm has for iGPUs in the python stack&lt;/p&gt; &lt;p&gt;However, for development it is actually pretty awesome. I allocated 24 GB of GTT so now the iGPU can load a full training run that my main GPU can run so I can profile it. Meanwhile my main GPU is doing long term loss convergence tests in parallel. Since RDNA iGPUs have been around for a while now, this enables big memory AMD GPU kernel development for cheap.&lt;/p&gt; &lt;p&gt;Also it might be interesting for developing hybrid CPU/GPU architectures. The MI300A does exist which has unified HBM tied to a CPU and giant iGPU. A standard ryzen laptop could kind of sort of simulate it for cheap. Stuff like vector indexing on the CPU into big GEMMs on the GPU could be done without PCIE overhead.&lt;/p&gt; &lt;p&gt;I thought it was cool enough to post. Probably a &amp;quot;Cool story bro&amp;quot; moment for most of you though haha.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1ncehost"&gt; /u/1ncehost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1lgb7/til_you_can_allocate_128_gb_of_unified_memory_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T01:37:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21wqw</id>
    <title>A deep dive in DeepSeek's mHC: They improved things everyone else thought didnâ€™t need improving</title>
    <updated>2026-01-02T15:44:21+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;The Context&lt;/h1&gt; &lt;p&gt;Since ResNet (2015), the Residual Connection (x_{l+1} = x_l + F(x_l)) has been the untouchable backbone of deep learning (from CNN to Transformer, from BERT to GPT). It solves the vanishing gradient problem by providing an &amp;quot;identity mapping&amp;quot; fast lane. For 10 years, almost no one questioned it.&lt;/p&gt; &lt;h1&gt;The Problem&lt;/h1&gt; &lt;p&gt;However, this standard design forces a rigid 1:1 ratio between the input and the new computation, preventing the model from dynamically adjusting how much it relies on past layers versus new information.&lt;/p&gt; &lt;h1&gt;The Innovation&lt;/h1&gt; &lt;p&gt;ByteDace tried to break this rule with &amp;quot;Hyper-Connections&amp;quot; (HC), allowing the model to learn the connection weights instead of using a fixed ratio.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;The potential:&lt;/strong&gt; Faster convergence and better performance due to flexible information routing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The issue:&lt;/strong&gt; It was incredibly unstable. Without constraints, signals were amplified by &lt;strong&gt;3000x&lt;/strong&gt; in deep networks, leading to exploding gradients.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;The Solution: Manifold-Constrained Hyper-Connections (mHC)&lt;/h1&gt; &lt;p&gt;In their new paper, DeepSeek solved the instability by constraining the learnable matrices to be &amp;quot;Double Stochastic&amp;quot; (all elements â‰§ 0, rows/cols sum to 1).&lt;/p&gt; &lt;p&gt;Mathematically, this forces the operation to act as a weighted average (convex combination). It guarantees that signals are never amplified beyond control, regardless of network depth.&lt;/p&gt; &lt;h1&gt;The Results&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; Max gain magnitude dropped from &lt;strong&gt;3000 to 1.6&lt;/strong&gt; (3 orders of magnitude improvement).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Performance:&lt;/strong&gt; mHC beats both the standard baseline and the unstable HC on benchmarks like GSM8K and DROP.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Cost:&lt;/strong&gt; Only adds ~6% to training time due to heavy optimization (kernel fusion).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Why it matters&lt;/h1&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f"&gt;https://preview.redd.it/ybux3x1wgyag1.png?width=1206&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=daafe17d3a61d387adf952ad756eb70af3bc445f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As hinted in the attached tweet, we are seeing a fascinating split in the AI world. While the industry frenzy focuses on commercialization and AI Agentsâ€”exemplified by Meta spending $2 Billion to acquire Manusâ€”labs like DeepSeek and Moonshot (Kimi) are playing a different game.&lt;/p&gt; &lt;p&gt;Despite resource constraints, they are digging into the deepest levels of macro-architecture and optimization. They have the audacity to question what we took for granted: &lt;strong&gt;Residual Connections&lt;/strong&gt; (challenged by DeepSeek's mHC) and &lt;strong&gt;AdamW&lt;/strong&gt; (challenged by Kimi's Muon). Just because these have been the standard for 10 years doesn't mean they are the optimal solution.&lt;/p&gt; &lt;p&gt;Crucially, instead of locking these secrets behind closed doors for commercial dominance, they are &lt;strong&gt;open-sourcing&lt;/strong&gt; these findings for the advancement of humanity. This spirit of relentless self-doubt and fundamental reinvention is exactly how we evolve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21wqw/a_deep_dive_in_deepseeks_mhc_they_improved_things/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:44:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1q21zql</id>
    <title>Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued â€” Spectrum Sourcing</title>
    <updated>2026-01-02T15:47:29+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt; &lt;img alt="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued â€” Spectrum Sourcing" src="https://external-preview.redd.it/hL9IlSGjDxXkUJI7Ss74xpCqhzcKCtv4f-p8hzWkQ_U.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=704790efb261d2a51abf0f339a4ace74d00ec32c" title="Industry Update: Supermicro Policy on Standalone Motherboards Sales Discontinued â€” Spectrum Sourcing" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This isn't new, but somehow I missed it, and figure many in this community might also not be aware of this.&lt;/p&gt; &lt;p&gt;The TLDR, as the title says: Supermicro is stopping standalone motherboard sales and now selling only entire servers. As if things weren't already bad enough...&lt;/p&gt; &lt;p&gt;I had noticed an uptick in used board prices on ebay, local ads, and tech forums but didn't have an explanation for it. This explains why.&lt;/p&gt; &lt;p&gt;While most discussions in this community center around consumer boards, workstation and server boards offer so many more features and functionality, and used to be much cheaper than their desktop counterparts.&lt;/p&gt; &lt;p&gt;Supermicro was arguably the largest supplier of such boards, and with them stopping motherboard sales, all workstation and server boards in standard industry form-factor (EATX, ATX, MATX, IT, and SSE variants) will have a sharp drop in availability in the foreseeable future.&lt;/p&gt; &lt;p&gt;Add to that the sharp increase in RAM prices, and you can see why many businesses will be hesitant to move to newer DDR5 server platforms and instead choose to stock to DDR4 platforms to reuse their existing memory. I suspect many will consolidate their existing DDR4 based Xeon and early Epyc (Naples) to Epyc Milan servers using existing market supply of servers and boards.&lt;/p&gt; &lt;p&gt;We're barely in 2026, but it's looking like this year will squeeze us, consumer, even more than 2025 has.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.spectrumsourcing.com/spectrum-news-feed/industry-update-supermicro-policy-on-standalone-motherboards-sales-discontinued"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q21zql/industry_update_supermicro_policy_on_standalone/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T15:47:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1ura1</id>
    <title>[IQuestLab/IQuest-Coder-V1] SWE-bench score is compromised because environment setup was wrong</title>
    <updated>2026-01-02T09:57:34+00:00</updated>
    <author>
      <name>/u/nullmove</name>
      <uri>https://old.reddit.com/user/nullmove</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;TL;DR is that they didn't clean the repo (.git/ folder), model just reward hacked its way to look up future commits with fixes. Credit goes to everyone in this thread for solving this: &lt;a href="https://xcancel.com/xeophon/status/2006969664346501589"&gt;https://xcancel.com/xeophon/status/2006969664346501589&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(given that IQuestLab published their SWE-Bench Verified trajectory data, I want to be charitable and assume genuine oversight rather than &amp;quot;benchmaxxing&amp;quot;, probably an easy to miss thing if you are new to benchmarking)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nullmove"&gt; /u/nullmove &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1ura1/iquestlabiquestcoderv1_swebench_score_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T09:57:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1p5q5</id>
    <title>Getting ready to train in Intel arc</title>
    <updated>2026-01-02T04:33:19+00:00</updated>
    <author>
      <name>/u/hasanismail_</name>
      <uri>https://old.reddit.com/user/hasanismail_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"&gt; &lt;img alt="Getting ready to train in Intel arc" src="https://a.thumbs.redditmedia.com/iosPXZzRPv_Wd4_Y3UYcEyKFQKwEaHWvSBwjyGk3uo8.jpg" title="Getting ready to train in Intel arc" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just waiting on pcie risers can't wait to start training on Intel arc I'm not sure in anyone else is attempting the same thing yet so I though I would share &lt;/p&gt; &lt;p&gt;PS. I am not causing a GPU shortage pls dont comment about this I am not open ai or google believe me there would have been signs on my other posts gamers would say sh*t like this so before u comment please educate yourselves&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hasanismail_"&gt; /u/hasanismail_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1q1p5q5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1p5q5/getting_ready_to_train_in_intel_arc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T04:33:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1uyf6</id>
    <title>New Models from South Korea's Sovereign AI Foundation Model Project</title>
    <updated>2026-01-02T10:09:28+00:00</updated>
    <author>
      <name>/u/chibop1</name>
      <uri>https://old.reddit.com/user/chibop1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've seen posts with individual models here and there, but not together in one post. Also I'm including some English articles I found about the project.&lt;/p&gt; &lt;p&gt;It's bit old news, but the South Korean government funded the Sovereign AI Foundation Model Project, and the five selected teams released their initial models and presented on December 30, 2025.&lt;/p&gt; &lt;p&gt;Below are the repos I was able to track down on Huggingface, but please let me know if I missed or included wrong repo.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Naver Cloud: &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Omni-8B"&gt;HyperCLOVAX-SEED-Omni-8B &lt;/a&gt;, &lt;a href="https://huggingface.co/naver-hyperclovax/HyperCLOVAX-SEED-Think-32B"&gt;HyperCLOVAX-SEED-Think-32B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Upstage: &lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;Solar-Open-102B-A12B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;SK Telecom: &lt;a href="https://huggingface.co/skt/A.X-K1"&gt;A.X-K1-519B-A33B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LG AI Research: &lt;a href="https://huggingface.co/LGAI-EXAONE/K-EXAONE-236B-A23B"&gt;K-EXAONE-236B-A23B&lt;/a&gt;&lt;/li&gt; &lt;li&gt;NC AI: &lt;a href="https://huggingface.co/NC-AI-consortium-VAETKI/VAETKI"&gt;VAETKI-112B-A10B&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It looks like MSIT is backing the project with funding, GPUs, and datasets. Teams will be evaluated and eliminated through 2026 and into mid 2027 until two finalists. You can read more about the project below:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.msit.go.kr/eng/bbs/view.do?bbsSeqNo=42&amp;amp;mId=4&amp;amp;nttSeqNo=1152&amp;amp;sCode=eng"&gt;https://www.msit.go.kr/eng/bbs/view.do?bbsSeqNo=42&amp;amp;mId=4&amp;amp;nttSeqNo=1152&amp;amp;sCode=eng&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.upi.com/Top_News/World-News/2025/12/30/ai-model-national-project/7441767133090/"&gt;https://www.upi.com/Top_News/World-News/2025/12/30/ai-model-national-project/7441767133090/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.koreatimes.co.kr/business/tech-science/20251230/consortia-unveil-models-for-national-ai-project"&gt;https://www.koreatimes.co.kr/business/tech-science/20251230/consortia-unveil-models-for-national-ai-project&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chibop1"&gt; /u/chibop1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1uyf6/new_models_from_south_koreas_sovereign_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T10:09:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1q1w1qj</id>
    <title>Most optimal vram/performance per price and advice for Shenzhen GPU market</title>
    <updated>2026-01-02T11:14:30+00:00</updated>
    <author>
      <name>/u/notafakename10</name>
      <uri>https://old.reddit.com/user/notafakename10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt; &lt;img alt="Most optimal vram/performance per price and advice for Shenzhen GPU market" src="https://preview.redd.it/4nfcarq96xag1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=34f4fbc2fba6317c3d5435a92332540815eb3714" title="Most optimal vram/performance per price and advice for Shenzhen GPU market" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Iâ€™m in Shanghai at the moment and heading to Shenzhen soon - Iâ€™ve got around $1500-3000 USD to get the most optimal setup possible. The people I am with are great at negotiating (natives, speak the language) I just need to figure out what I wantâ€¦ &lt;/p&gt; &lt;p&gt;I main use local models I would want at least 48gb vram, ideally closer to 96gb an at least some grunt for the odd PyTorch model training run. Iâ€™m open to modded cards (one of my current front runners is 4x 3080 20gb cards) open to both AMD and domestic / enterprise cards. &lt;/p&gt; &lt;p&gt;Prices are best estimates from deep seek - could be wildly wrong, anyone had experience navigating the GPU markets? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notafakename10"&gt; /u/notafakename10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4nfcarq96xag1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1q1w1qj/most_optimal_vramperformance_per_price_and_advice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-02T11:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. Weâ€™re excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM â€“ 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1pwh0q9</id>
    <title>Best Local LLMs - 2025</title>
    <updated>2025-12-26T22:31:28+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Year end thread for the best LLMs of 2025!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;2025 is almost done! Its been &lt;strong&gt;a wonderful year&lt;/strong&gt; for us Open/Local AI enthusiasts. And its looking like Xmas time brought some great gifts in the shape of Minimax M2.1 and GLM4.7 that are touting frontier model performance. Are we there already? are we at parity with proprietary models?!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The standard spiel:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Only open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;em&gt;Please thread your responses in the top level comments for each Application below to enable readability&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;General&lt;/strong&gt;: Includes practical guidance, how to, encyclopedic QnA, search engine replacement/augmentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Agentic/Agentic Coding/Tool Use/Coding&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Creative Writing/RP&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Speciality&lt;/strong&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;If a category is missing, please create a top level comment under the Speciality comment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Notes&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Useful breakdown of how folk are using LLMs: &lt;a href="https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d"&gt;https://preview.redd.it/i8td7u8vcewf1.png?width=1090&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=423fd3fe4cea2b9d78944e521ba8a39794f37c8d&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A good suggestion for last time, breakdown/classify your recommendation by model memory footprint: (you can and should be using multiple models in each size range for different tasks)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Unlimited: &amp;gt;128GB VRAM &lt;/li&gt; &lt;li&gt;Medium: 8 to 128GB VRAM&lt;/li&gt; &lt;li&gt;Small: &amp;lt;8GB VRAM&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pwh0q9/best_local_llms_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-26T22:31:28+00:00</published>
  </entry>
</feed>
