<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-24T23:35:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1puqqjv</id>
    <title>is the openai package still the best approach for working with LLMs in Python?</title>
    <updated>2025-12-24T16:04:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Not a fan of langchain, crewai or the scores of other AI frameworks. I want just the basics of structured outputs. As far as I can tell the openai package is the works-and-bug-free go to. You of course can insert your own endpoint, model. Is there nothing better now? So many new models etc. but nothing better in such a basic, core tool?&lt;/p&gt; &lt;p&gt;EDIT: For clarity, I dont want to depend on a package from OpenAI as I dont have sufficient trust that they wont compromise it in the future in a way that makes life difficult for using non-openAI endpoints/models with it. Of any sub, hopefully this one has a visceral sense around this&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puqqjv/is_the_openai_package_still_the_best_approach_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puqqjv/is_the_openai_package_still_the_best_approach_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puqqjv/is_the_openai_package_still_the_best_approach_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T16:04:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzqpe</id>
    <title>Memora - A persistent memory layer for Claude Code with live knowledge graph visualization</title>
    <updated>2025-12-24T23:07:29+00:00</updated>
    <author>
      <name>/u/spokv</name>
      <uri>https://old.reddit.com/user/spokv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzqpe/memora_a_persistent_memory_layer_for_claude_code/"&gt; &lt;img alt="Memora - A persistent memory layer for Claude Code with live knowledge graph visualization" src="https://external-preview.redd.it/owS2rr5iTZyYvLFNQdhz6LwdZHrdy9e7Q8gomrWVBZI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=dbaabdc848ffa7339f883c1311779dbe92a42a45" title="Memora - A persistent memory layer for Claude Code with live knowledge graph visualization" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an MCP server that gives Claude Code persistent memory across sessions.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Stores memories in SQLite with semantic search&lt;/li&gt; &lt;li&gt;Auto-links related memories based on similarity&lt;/li&gt; &lt;li&gt;Interactive knowledge graph that updates in real-time&lt;/li&gt; &lt;li&gt;Duplicate detection, issue tracking, TODOs&lt;/li&gt; &lt;li&gt;Works with Claude Code, Codex CLI, and other MCP clients&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Demo:&lt;/strong&gt; Shows creating memories and watching the graph build connections automatically.&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1puzqpe/video/683bm1ywg89g1/player"&gt;https://reddit.com/link/1puzqpe/video/683bm1ywg89g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Zero dependencies (optional: cloud sync, embeddings)&lt;/li&gt; &lt;li&gt;Hierarchical organization with sections/subsections&lt;/li&gt; &lt;li&gt;Filter by tags, status, categories&lt;/li&gt; &lt;li&gt;Export to HTML graph for sharing&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/agentic-mcp-tools/memora"&gt;https://github.com/agentic-mcp-tools/memora&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feedback welcome!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/spokv"&gt; /u/spokv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzqpe/memora_a_persistent_memory_layer_for_claude_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzqpe/memora_a_persistent_memory_layer_for_claude_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puzqpe/memora_a_persistent_memory_layer_for_claude_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:07:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1puwh0a</id>
    <title>Just saw this paper on arxiv - is this legit? Supposedly LangVAE straps a VAE + compression algorithm onto any LLM image, reduces resource requirements by up to -90%-?!</title>
    <updated>2025-12-24T20:23:32+00:00</updated>
    <author>
      <name>/u/MrE_WI</name>
      <uri>https://old.reddit.com/user/MrE_WI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/html/2505.00004v1"&gt;https://arxiv.org/html/2505.00004v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If the article and supporting libs -are- legit, then i have two follow up qs: &lt;/p&gt; &lt;p&gt;Can this be used to reduce requirements for inference, or is it only useful for training and research? &lt;/p&gt; &lt;p&gt;Finally, if it -can- reduce requirements for inference, how do we get started?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MrE_WI"&gt; /u/MrE_WI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puwh0a/just_saw_this_paper_on_arxiv_is_this_legit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puwh0a/just_saw_this_paper_on_arxiv_is_this_legit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puwh0a/just_saw_this_paper_on_arxiv_is_this_legit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T20:23:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1pudm4m</id>
    <title>I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf</title>
    <updated>2025-12-24T03:45:18+00:00</updated>
    <author>
      <name>/u/AdditionalWeb107</name>
      <uri>https://old.reddit.com/user/AdditionalWeb107</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"&gt; &lt;img alt="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf" src="https://preview.redd.it/iuaxwr9x529g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1349e4df960f26fc52d217c9f4f15fd3fc847cb5" title="I built Plano(A3B): most efficient LLMs for agent orchestration that exceed frontier model perf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone ‚Äî I‚Äôm on the Katanemo research team. Today we‚Äôre thrilled to launch &lt;strong&gt;Plano-Orchestrator&lt;/strong&gt;, a new family of LLMs built for fast multi-agent orchestration.&lt;/p&gt; &lt;p&gt;What do these new LLMs do? given a user request and the conversation context, Plano-Orchestrator decides which agent(s) should handle the request and in what sequence. In other words, it acts as the supervisor agent in a multi-agent system. Designed for multi-domain scenarios, it works well across general chat, coding tasks, and long, multi-turn conversations, while staying efficient enough for low-latency production deployments.&lt;/p&gt; &lt;p&gt;Why did we built this? Our applied research is focused on helping teams deliver agents safely and efficiently, with better real-world performance and latency ‚Äî the kind of ‚Äúglue work‚Äù that usually sits outside any single agent‚Äôs core product logic.&lt;/p&gt; &lt;p&gt;Plano-Orchestrator is integrated into Plano, our models-native proxy and dataplane for agents. Hope you enjoy it ‚Äî and we‚Äôd love feedback from anyone building multi-agent systems&lt;/p&gt; &lt;p&gt;Learn more about the LLMs &lt;a href="https://huggingface.co/collections/katanemo/plano-orchestrator"&gt;here&lt;/a&gt;&lt;br /&gt; About our open source project: &lt;a href="https://github.com/katanemo/plano"&gt;https://github.com/katanemo/plano&lt;/a&gt;&lt;br /&gt; And about our research: &lt;a href="https://planoai.dev/research"&gt;https://planoai.dev/research&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AdditionalWeb107"&gt; /u/AdditionalWeb107 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iuaxwr9x529g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pudm4m/i_built_planoa3b_most_efficient_llms_for_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T03:45:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv04uy</id>
    <title>model: support MiMo-V2-Flash by ngxson ¬∑ Pull Request #18328 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-24T23:28:46+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"&gt; &lt;img alt="model: support MiMo-V2-Flash by ngxson ¬∑ Pull Request #18328 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/ljL76ES0ycKhBmmY2ipEg2qGxnKYFbw_FzMzU74PR0Q.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=951b663470ca3d64de2becd5786b3d589c6d0ac7" title="model: support MiMo-V2-Flash by ngxson ¬∑ Pull Request #18328 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18328"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv04uy/model_support_mimov2flash_by_ngxson_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:28:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pulqzt</id>
    <title>Unsloth GLM 4.7 UD-Q2_K_XL or gpt-oss 120b?</title>
    <updated>2025-12-24T11:57:54+00:00</updated>
    <author>
      <name>/u/EnthusiasmPurple85</name>
      <uri>https://old.reddit.com/user/EnthusiasmPurple85</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm sure that gpt-oss will be much faster but, would the extreme GLM quant be better for general programming and chat? Anyone tried? Downloading them as of now. RTX3090 + 128GB of DDR4 3600&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EnthusiasmPurple85"&gt; /u/EnthusiasmPurple85 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pulqzt/unsloth_glm_47_udq2_k_xl_or_gptoss_120b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pulqzt/unsloth_glm_47_udq2_k_xl_or_gptoss_120b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pulqzt/unsloth_glm_47_udq2_k_xl_or_gptoss_120b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:57:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1puh2lw</id>
    <title>[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance</title>
    <updated>2025-12-24T06:59:18+00:00</updated>
    <author>
      <name>/u/Psychological_Box406</name>
      <uri>https://old.reddit.com/user/Psychological_Box406</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"&gt; &lt;img alt="[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance" src="https://b.thumbs.redditmedia.com/A1Q7Evja7kTuk7_hIySJ70jn6LZTsCxGkeCj-i6CNEI.jpg" title="[Follow-up] GLM 4.7 vs Minimax M2.1 - A Discovery That Might Explain the Poor GLM Performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Following up on my previous post comparing &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ptq7rc/glm_47_vs_minimax_m21_my_test_subscription/"&gt;GLM 4.7 and Minimax M2.1&lt;/a&gt; on a task.&lt;br /&gt; First, I got some valid feedback on the comments saying that this sub is specifically about local models, not API subscriptions. Fair point. But both of these models are fully hostable locally. Many people don't have the infrastructure or resources to self-host, so I think sharing real-world performance data, even from API usage, is still valuable for those who do. The results apply regardless of whether you run them on someone's servers or your own hardware.&lt;/p&gt; &lt;p&gt;That said, something interesting came up while I was checking my billing history on Z.ai...&lt;/p&gt; &lt;p&gt;Looking at yesterday's session costs, I realized something crucial: &lt;strong&gt;It didn't just use GLM 4.7.&lt;/strong&gt; The billing breakdown shows multiple models were used during that 70min session:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;glm-4.5-air&lt;/li&gt; &lt;li&gt;glm-4.7&lt;/li&gt; &lt;li&gt;glm-4.5&lt;/li&gt; &lt;li&gt;glm-4.6&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This means their platform was automatically routing across different model versions, not just hitting GLM 4.7 consistently.&lt;/p&gt; &lt;p&gt;Could this automatic model routing be why the performance wasn't good?&lt;/p&gt; &lt;p&gt;Those self-hosting it locally will likely see better performance since they're using a single model version without the routing shuffle.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ottux5r6n39g1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a0d33ee5e79a01023b8e1a97341dde9bfe0cd1"&gt;https://preview.redd.it/ottux5r6n39g1.png?width=1123&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e4a0d33ee5e79a01023b8e1a97341dde9bfe0cd1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Box406"&gt; /u/Psychological_Box406 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puh2lw/followup_glm_47_vs_minimax_m21_a_discovery_that/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T06:59:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1puz9bl</id>
    <title>Guide to fine-tuning</title>
    <updated>2025-12-24T22:41:56+00:00</updated>
    <author>
      <name>/u/LahmeriMohamed</name>
      <uri>https://old.reddit.com/user/LahmeriMohamed</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;hello guys i am looking for a guide from 0 , about fine-tuning , i am new into llm and vlm, my goal is to fine-tune qwen3-vl on text and others , any help is welcomed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LahmeriMohamed"&gt; /u/LahmeriMohamed &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puz9bl/guide_to_finetuning/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puz9bl/guide_to_finetuning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puz9bl/guide_to_finetuning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T22:41:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1put96m</id>
    <title>A sanity layer that can make SLMs useful (sSanityLayer)</title>
    <updated>2025-12-24T17:57:25+00:00</updated>
    <author>
      <name>/u/ValuableLucky8566</name>
      <uri>https://old.reddit.com/user/ValuableLucky8566</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a MultiHeadAttention Layer architecture that modulates emotional intensity by introducing vector bias and/or vector noise. It uses semantic anchoring to alter the sanity state(essentialy tied to strength and boost parameter) using a hybrid RNN. Note, this does not make LLMs smarter, but rather acts as a smart filter.&lt;/p&gt; &lt;p&gt;The logic can be used to create vSLMs like the one demonstrated in the repository, that are trained to respond through triggers. The sSanityLayer dynamically updates its state, and introduces vector noise to corrupt the vector positions in V dataset. The result? The model knows what it wants, but can't put it in a fixed manner. This flustered state can be triggered by lowered sanity.&lt;/p&gt; &lt;p&gt;Potato is a model trained on the same architecture, at just 77KB, fulfills the same precisely well. The model can be trained on CPUs, while also being insanely fast(for it's small size).&lt;/p&gt; &lt;p&gt;On transformer models, the anchors change the logit bias by using t_ids_2 = tokenizer.encode(&amp;quot;&amp;quot; + w, add_special_tokens=False).&lt;/p&gt; &lt;p&gt;Example log from GPT2 Small: Prompt: &amp;quot;the girl was incapable and dead&amp;quot;&lt;/p&gt; &lt;p&gt;Without the layer: Output: &amp;quot;accurate presentation so precisely there was no transition... and a prognosis with 1990s digital. Somebody make a damn big thing up...&lt;/p&gt; &lt;p&gt;With the layer: Output: &amp;quot;because she refused to buckle.&amp;quot;&lt;/p&gt; &lt;p&gt;GitHub link: &lt;a href="https://github.com/kavyamali/sSanityLayer"&gt;https://github.com/kavyamali/sSanityLayer&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ValuableLucky8566"&gt; /u/ValuableLucky8566 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1put96m/a_sanity_layer_that_can_make_slms_useful/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1put96m/a_sanity_layer_that_can_make_slms_useful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1put96m/a_sanity_layer_that_can_make_slms_useful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T17:57:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzin1</id>
    <title>Llama.cpp multiple model presets appreciation post</title>
    <updated>2025-12-24T22:55:44+00:00</updated>
    <author>
      <name>/u/robiinn</name>
      <uri>https://old.reddit.com/user/robiinn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently Llama.cpp &lt;a href="https://github.com/ggml-org/llama.cpp/pull/17859"&gt;added support&lt;/a&gt; for &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#model-presets"&gt;model presets&lt;/a&gt;, which is a awsome feature that allow model loading and switching, and I have not seen much talk about. I would like to show my appreciation to the developers that are working on Llama.cpp and also share that the &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#model-presets"&gt;model preset feature&lt;/a&gt; exists to switch models. &lt;/p&gt; &lt;p&gt;A short guide of how to use it:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Get your hands on a recent version of &lt;code&gt;llama-server&lt;/code&gt; from Llama.cpp.&lt;/li&gt; &lt;li&gt;Create a &lt;code&gt;.ini&lt;/code&gt; file. I named my file &lt;code&gt;models.ini&lt;/code&gt;. &lt;/li&gt; &lt;li&gt;Add the content of the models to your &lt;code&gt;.ini&lt;/code&gt; file. See either the &lt;a href="https://github.com/ggml-org/llama.cpp/tree/master/tools/server#model-presets"&gt;README&lt;/a&gt; or my example below. The values in the &lt;code&gt;[*]&lt;/code&gt; section is shared between each model, and &lt;code&gt;[Devstral2:Q5_K_XL]&lt;/code&gt; declares a new model.&lt;/li&gt; &lt;li&gt;Run &lt;code&gt;llama-server --models-preset &amp;lt;path to your.ini&amp;gt;/models.ini&lt;/code&gt; to start the server.&lt;/li&gt; &lt;li&gt;Optional: Try out the webui on &lt;a href="http://localhost:8080"&gt;&lt;code&gt;http://localhost:8080&lt;/code&gt;&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Here is my &lt;code&gt;models.ini&lt;/code&gt; file as an example:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;version = 1 [*] flash-attn = on n-gpu-layers = 99 c = 32768 jinja = true t = -1 b = 2048 ub = 2048 [Devstral2:Q5_K_XL] temp = 0.15 min-p = 0.01 model = /home/&amp;lt;name&amp;gt;/gguf/Devstral-Small-2-24B-Instruct-2512-UD-Q5_K_XL.gguf cache-type-v = q8_0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Thanks for me, I just wanted to share this with you all and I hope it helps someone!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/robiinn"&gt; /u/robiinn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzin1/llamacpp_multiple_model_presets_appreciation_post/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzin1/llamacpp_multiple_model_presets_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puzin1/llamacpp_multiple_model_presets_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T22:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pv022d</id>
    <title>What is llama.cpp equivalent for image &amp; video gen?</title>
    <updated>2025-12-24T23:24:39+00:00</updated>
    <author>
      <name>/u/ClimateBoss</name>
      <uri>https://old.reddit.com/user/ClimateBoss</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I use &lt;strong&gt;llama.cpp&lt;/strong&gt; to generate text from GGUF models on a server offline&lt;strong&gt;.&lt;/strong&gt; I can scp GGUF and run it and even build llama.cpp from source.&lt;/p&gt; &lt;p&gt;Most examples I found are setting up Gradio, using python scripts, and installing python pip packages or even running MacOS app (I use arch btw!)&lt;/p&gt; &lt;p&gt;What's a local cli for image &amp;amp; video gen? Text 2 Image and Image 2 Video if you dont want a UI.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ClimateBoss"&gt; /u/ClimateBoss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv022d/what_is_llamacpp_equivalent_for_image_video_gen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pv022d/what_is_llamacpp_equivalent_for_image_video_gen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pv022d/what_is_llamacpp_equivalent_for_image_video_gen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:24:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pun4kk</id>
    <title>Which GPU should I use to caption ~50k images/day</title>
    <updated>2025-12-24T13:14:59+00:00</updated>
    <author>
      <name>/u/koteklidkapi</name>
      <uri>https://old.reddit.com/user/koteklidkapi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I need to generate captions/descriptions for around 50,000 images per day (~1.5M per month) using a vision-language model. From my initial tests, uform-gen2-qwen-500m and qwen2.5-vl:7b seem good enough quality for me.&lt;/p&gt; &lt;p&gt;I‚Äôm planning to rent a GPU, but inference speed is critical ‚Äî the images need to be processed within the same day, so latency and throughput matter a lot.&lt;/p&gt; &lt;p&gt;Based on what I‚Äôve found online, AWS G5 instances or GPUs like L40 &lt;em&gt;seem&lt;/em&gt; like they could handle this, but I‚Äôm honestly not very confident about that assessment.&lt;/p&gt; &lt;p&gt;Do you have any recommendations?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Which GPU(s) would you suggest for this scale?&lt;/li&gt; &lt;li&gt;Any experience running similar VLM workloads at this volume?&lt;/li&gt; &lt;li&gt;Tips on optimizing throughput (batching, quantization, etc.) are also welcome.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Thanks in advance.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/koteklidkapi"&gt; /u/koteklidkapi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pun4kk/which_gpu_should_i_use_to_caption_50k_imagesday/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pun4kk/which_gpu_should_i_use_to_caption_50k_imagesday/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pun4kk/which_gpu_should_i_use_to_caption_50k_imagesday/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T13:14:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1punxjz</id>
    <title>minimax m2.1 is going to open source which is good but picture is here is minimax decoded how to make there model in good in coding. if u look at the benchmark closely its same like the claude bechmark best in coding wrost in other . so now we have a lab which solely focusing on coding</title>
    <updated>2025-12-24T13:55:44+00:00</updated>
    <author>
      <name>/u/Select_Dream634</name>
      <uri>https://old.reddit.com/user/Select_Dream634</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1punxjz/minimax_m21_is_going_to_open_source_which_is_good/"&gt; &lt;img alt="minimax m2.1 is going to open source which is good but picture is here is minimax decoded how to make there model in good in coding. if u look at the benchmark closely its same like the claude bechmark best in coding wrost in other . so now we have a lab which solely focusing on coding" src="https://preview.redd.it/h0zmnel4q59g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e169e63ee2261089cd5e05e957f9a8baaf183883" title="minimax m2.1 is going to open source which is good but picture is here is minimax decoded how to make there model in good in coding. if u look at the benchmark closely its same like the claude bechmark best in coding wrost in other . so now we have a lab which solely focusing on coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;minimax is the part of alibaba so they got a compute and lots of compute so they are not going to lag behind and guess minimax is also good in video , audio generation .&lt;/p&gt; &lt;p&gt;so what the hell claude is doing with that much compute and crying about price &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Select_Dream634"&gt; /u/Select_Dream634 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h0zmnel4q59g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1punxjz/minimax_m21_is_going_to_open_source_which_is_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1punxjz/minimax_m21_is_going_to_open_source_which_is_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T13:55:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1puf614</id>
    <title>New 1B parameter open-source coding model getting 76% on HumanEval [shameless but proud self-plug]</title>
    <updated>2025-12-24T05:08:56+00:00</updated>
    <author>
      <name>/u/More_Article9837</name>
      <uri>https://old.reddit.com/user/More_Article9837</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks, merry festive season to you all. Hope you are staying safe!&lt;br /&gt; Wanted to share a new open-source coding model release that might be interesting to yall here. My team proudly published it this morning..(we are a small start up out of Australia) &lt;/p&gt; &lt;p&gt;It‚Äôs called Maincoder-1B... a 1B-parameter code generation model that gets 76% on HumanEval, which is unusually high for a model this small (so far its ranking best-in-class for open models in that size range). &lt;/p&gt; &lt;p&gt;Our focus isn‚Äôt on scaling up, but on making small models actually good. We know that with a lot of real-world use cases such as: interactive tools, local/offline coding, batch refactors, search-based program synthesis... you care more about latency, cost, and fast rollouts than having a massive model. &lt;/p&gt; &lt;p&gt;Some key points to note:&lt;br /&gt; -Designed for low-latency and low-cost inference&lt;br /&gt; -Can run locally or on constrained hardware&lt;br /&gt; -Useful for systems that need many cheap generations (search, verification, RL-style loops)&lt;br /&gt; -as well as fine tuning to personal preferences&lt;br /&gt; -Released under Apache 2.0 &lt;/p&gt; &lt;p&gt;It does have the expected limitations: ~2k context window and it‚Äôs best at small, self-contained tasks....not large codebases or safety-critical code without human review. &lt;/p&gt; &lt;p&gt;Weights and benchmarks and all that are here:&lt;br /&gt; &lt;a href="https://huggingface.co/Maincode/Maincoder-1B"&gt;https://huggingface.co/Maincode/Maincoder-1B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The full release note is here: &lt;a href="https://maincode.com/maincoder/"&gt;https://maincode.com/maincoder/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;Keen to hear your thoughts ..and particularly where small-but-strong coding models fit best today. Thanks in advance for your support :) We are excited to have got this over the line!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/More_Article9837"&gt; /u/More_Article9837 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puf614/new_1b_parameter_opensource_coding_model_getting/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T05:08:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1puxedb</id>
    <title>üéÑ We release 67,074 Qwen3-Coder OpenHands trajectories on SWE-rebench + 2 model checkpoints!</title>
    <updated>2025-12-24T21:08:30+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/"&gt; &lt;img alt="üéÑ We release 67,074 Qwen3-Coder OpenHands trajectories on SWE-rebench + 2 model checkpoints!" src="https://external-preview.redd.it/Dhe375wyR8tu2LaFu992kxZN7nBVngBP3mBqrvOD7tg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c142e961fe25094c4c77dc7d9a6118e57af1255" title="üéÑ We release 67,074 Qwen3-Coder OpenHands trajectories on SWE-rebench + 2 model checkpoints!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Happy holidays! üéÑ&lt;br /&gt; I‚Äôm Ibragim from Nebius.&lt;/p&gt; &lt;p&gt;We‚Äôre releasing a big dataset for agentic coding research: 67,074 OpenHands trajectories (plus 2 RFT checkpoints), built from 3,800 resolved issues across 1,800+ Python repos. The trajectories are long: 64 turns on average, up to 100 turns, and up to 131k context length.&lt;/p&gt; &lt;p&gt;Agent framework: &lt;strong&gt;OpenHands&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;strong&gt;Qwen3-Coder-480B-A35B-Instruct&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Training tasks from &lt;strong&gt;SWE-rebench:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/nebius/SWE-rebench"&gt;https://huggingface.co/datasets/nebius/SWE-rebench&lt;/a&gt;&lt;/p&gt; &lt;p&gt;To demonstrate the data quality, we‚Äôre also releasing two checkpoints trained with rejection sampling fine-tuning (RFT):&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt; SWE-rebench-openhands-Qwen3-30B-A3B&lt;/strong&gt;&lt;br /&gt; SWE-bench Verified: 26% ‚Üí 50% Pass@1&lt;br /&gt; SWE-rebench (September): 14% ‚Üí 28% Pass@1&lt;/p&gt; &lt;p&gt;&lt;strong&gt;&amp;gt; SWE-rebench-openhands-Qwen3-235B-A22B&lt;/strong&gt;&lt;br /&gt; SWE-bench Verified: 46% ‚Üí 62% Pass@1&lt;br /&gt; SWE-rebench (September): 25% ‚Üí 34% Pass@1&lt;/p&gt; &lt;p&gt;We also ran extensive evaluations of OpenHands with 100-turn and 500-turn limits across various models.&lt;/p&gt; &lt;p&gt;We don‚Äôt just look at solutions ‚Äî we also evaluate tests generated by the models. For each issue, we check:&lt;/p&gt; &lt;p&gt;&amp;gt; How often the generated tests are correct&lt;br /&gt; &amp;gt; How often the model‚Äôs final patch passes its own tests&lt;/p&gt; &lt;p&gt;More details in our blog post:&lt;br /&gt; &lt;a href="https://nebius.com/blog/posts/openhands-trajectories-with-qwen3-coder-480b"&gt;https://nebius.com/blog/posts/openhands-trajectories-with-qwen3-coder-480b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hugging Face collection:&lt;br /&gt; &lt;a href="https://huggingface.co/collections/nebius/openhands-trajectories"&gt;https://huggingface.co/collections/nebius/openhands-trajectories&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Please let us know if you‚Äôd like us to release more data using other models or agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/nebius/openhands-trajectories"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T21:08:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1puglt8</id>
    <title>The current state of sparse-MoE's for agentic coding work (Opinion)</title>
    <updated>2025-12-24T06:31:19+00:00</updated>
    <author>
      <name>/u/ForsookComparison</name>
      <uri>https://old.reddit.com/user/ForsookComparison</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/"&gt; &lt;img alt="The current state of sparse-MoE's for agentic coding work (Opinion)" src="https://preview.redd.it/a8f2furcj39g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0d2f4646060fd2e6d3c36f45c6b4e665a71e1ce9" title="The current state of sparse-MoE's for agentic coding work (Opinion)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ForsookComparison"&gt; /u/ForsookComparison &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a8f2furcj39g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puglt8/the_current_state_of_sparsemoes_for_agentic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T06:31:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1puv3de</id>
    <title>K2-V2 - 70B and creative writing</title>
    <updated>2025-12-24T19:18:47+00:00</updated>
    <author>
      <name>/u/silenceimpaired</name>
      <uri>https://old.reddit.com/user/silenceimpaired</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone else tried K2-V2 - 70B in the creative writing realm? I first heard about it from this post: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt; https://www.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/ &lt;/a&gt;&lt;/p&gt; &lt;p&gt;I am pleasantly surprised at the thinking (you can choose the thinking budget) and output. Is it the best? I don't know yet, but it's nice to have an entirely new line of models to work with... Dense models have always been more friendly to those of us with a &amp;quot;healthy&amp;quot; level of VRAM.&lt;/p&gt; &lt;p&gt;I think GLM 4.6 still stacks above it, but it probably edges out GLM Air 4.5. I'll have to go back to that and see how that was. MiniMax-M2 is also rising in the ranks for me. Probably also better than K2-V2. Still pretty new for me.&lt;/p&gt; &lt;p&gt;Love to have your thoughts, and how it stacks up against other models you use.&lt;/p&gt; &lt;p&gt;Here are some direct links:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LLM360/K2-V2"&gt; https://huggingface.co/LLM360/K2-V2 &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/LLM360/K2-V2-Instruct"&gt; https://huggingface.co/LLM360/K2-V2-Instruct &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/cturan/K2-V2-Instruct-GGUF"&gt; https://huggingface.co/cturan/K2-V2-Instruct-GGUF &lt;/a&gt;&lt;/p&gt; &lt;p&gt;SAMPLE&lt;/p&gt; &lt;p&gt;&lt;a href="https://pastebin.com/YBwTE8Be"&gt;https://pastebin.com/YBwTE8Be&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/silenceimpaired"&gt; /u/silenceimpaired &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puv3de/k2v2_70b_and_creative_writing/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puv3de/k2v2_70b_and_creative_writing/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puv3de/k2v2_70b_and_creative_writing/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T19:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1puzo82</id>
    <title>Merry Christmas! üéÑ üéÅ</title>
    <updated>2025-12-24T23:03:48+00:00</updated>
    <author>
      <name>/u/Rare_Carry9799</name>
      <uri>https://old.reddit.com/user/Rare_Carry9799</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Merry Christmas! ü•≥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rare_Carry9799"&gt; /u/Rare_Carry9799 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puzo82/merry_christmas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T23:03:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1puxg7h</id>
    <title>MiniMax M2.1 scores 43.4% on SWE-rebench (November)</title>
    <updated>2025-12-24T21:10:50+00:00</updated>
    <author>
      <name>/u/Fabulous_Pollution10</name>
      <uri>https://old.reddit.com/user/Fabulous_Pollution10</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/"&gt; &lt;img alt="MiniMax M2.1 scores 43.4% on SWE-rebench (November)" src="https://preview.redd.it/s0vbt46vt79g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9a1fcc806bfccb9370c298a67d419e024ce322b3" title="MiniMax M2.1 scores 43.4% on SWE-rebench (November)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;br /&gt; We added MiniMax M2.1 results to the December SWE-rebench update.&lt;/p&gt; &lt;p&gt;Please check the leaderboard: &lt;a href="https://swe-rebench.com/"&gt;https://swe-rebench.com/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôll add GLM-4.7 and Gemini Flash 3 in the next release.&lt;br /&gt; By the way, we just released a large dataset of agentic trajectories and two checkpoints trained on it, based on Qwen models.&lt;br /&gt; Here‚Äôs the post:&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1puxedb/we_release_67074_qwen3coder_openhands/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fabulous_Pollution10"&gt; /u/Fabulous_Pollution10 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s0vbt46vt79g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puxg7h/minimax_m21_scores_434_on_swerebench_november/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T21:10:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1puwi5o</id>
    <title>Deepseek will release a larger model next year</title>
    <updated>2025-12-24T20:25:01+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;THis is old news but, I forgot to mention this before.&lt;/p&gt; &lt;p&gt;This is from section 5, &lt;a href="https://arxiv.org/html/2512.02556v1#S5"&gt;https://arxiv.org/html/2512.02556v1#S5&lt;/a&gt; -&amp;quot; First, due to fewer total training FLOPs, the breadth of world knowledge in DeepSeek-V3.2 still lags behind that of leading proprietary models. We plan to address this knowledge gap in future iterations by scaling up the pre-training compute.&amp;quot;&lt;/p&gt; &lt;p&gt;I speculate it will be bigger than 1.6T params(maybe 1.7-2.5T) and have 95B-111B active params and at least trained 2.5-3x more tokens than now... Hopefully they will releases the weights for this. I also hope for a smaller version(maybe it won't happen)..&lt;/p&gt; &lt;p&gt;&amp;quot; Second, token efficiency remains a challenge; DeepSeek-V3.2 typically requires longer generation trajectories (i.e., more tokens) to match the output quality of models like Gemini-3.0-Pro. Future work will focus on optimizing the intelligence density of the model‚Äôs reasoning chains to improve efficiency. Third, solving complex tasks is still inferior to frontier models, motivating us to further refine our foundation model and post-training recipe.&amp;quot;&lt;/p&gt; &lt;p&gt;- They will increase the efficiency of its reasoning ie it will use less thinking tokens than before for the same task .&lt;/p&gt; &lt;p&gt;Also they will improve its abilities solving complex task, this probably means better reasoning and agentic tooling&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puwi5o/deepseek_will_release_a_larger_model_next_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T20:25:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pullo0</id>
    <title>Hmm all reference to open-sourcing has been removed for Minimax M2.1...</title>
    <updated>2025-12-24T11:48:37+00:00</updated>
    <author>
      <name>/u/Responsible_Fig_1271</name>
      <uri>https://old.reddit.com/user/Responsible_Fig_1271</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Funny how yesterday this page &lt;a href="https://www.minimax.io/news/minimax-m21"&gt;https://www.minimax.io/news/minimax-m21&lt;/a&gt; had a statement that weights would be open-sourced on Huggingface and even a discussion of how to run locally on vLLM and SGLang. There was even a (broken but soon to be functional) HF link for the repo...&lt;/p&gt; &lt;p&gt;Today that's all gone.&lt;/p&gt; &lt;p&gt;Has MiniMax decided to go API only? Seems like they've backtracked on open-sourcing this one. Maybe they realized it's so good that it's time to make some $$$ :( Would be sad news for this community and a black mark against MiniMax.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Responsible_Fig_1271"&gt; /u/Responsible_Fig_1271 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pullo0/hmm_all_reference_to_opensourcing_has_been/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T11:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1puyq9r</id>
    <title>Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record</title>
    <updated>2025-12-24T22:14:48+00:00</updated>
    <author>
      <name>/u/fallingdowndizzyvr</name>
      <uri>https://old.reddit.com/user/fallingdowndizzyvr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"&gt; &lt;img alt="Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record" src="https://external-preview.redd.it/jvYBeKyc_OM28gIhUnO6GEg0WpsjQzWHJmf00gHtJBw.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d32d447dab802e0a1aec9574d5282648b995cf17" title="Exclusive: Nvidia buying AI chip startup Groq's assets for about $20 billion in largest deal on record" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fallingdowndizzyvr"&gt; /u/fallingdowndizzyvr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/12/24/nvidia-buying-ai-chip-startup-groq-for-about-20-billion-biggest-deal.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1puyq9r/exclusive_nvidia_buying_ai_chip_startup_groqs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T22:14:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1pux0yc</id>
    <title>We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found.</title>
    <updated>2025-12-24T20:50:16+00:00</updated>
    <author>
      <name>/u/vox-deorum</name>
      <uri>https://old.reddit.com/user/vox-deorum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt; &lt;img alt="We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found." src="https://b.thumbs.redditmedia.com/4t8jQ0I5zf5Jo9d6oRUo6_gfae4uPfqghHDLniBqMTU.jpg" title="We asked OSS-120B and GLM 4.6 to play 1,408 Civilization V games from the Stone Age into the future. Here's what we found." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://i.redd.it/zaib4up4s79g1.gif"&gt;GLM-4.6 Playing Civilization V + Vox Populi (Replay)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We had GPT-OSS-120B and GLM-4.6 playing 1,408 full Civilization V games (with Vox Populi/Community Patch activated). In a nutshell: LLMs set strategies for Civilization V's algorithmic AI to execute. Here is what we found:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/shjvvfpbq79g1.png?width=3187&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0175d5203c471ef332d54c2fe2b17d2369813e24"&gt;An overview of our system and results&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TLDR:&lt;/strong&gt; It is now possible to get open-source LLMs to play end-to-end Civilization V games (the m. They are not beating algorithm-based AI on a very simple prompt, but they do play quite differently. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;The boring result:&lt;/strong&gt; With a simple prompt and little memory, both LLMs did slightly better in the best score they could achieve within each game (+1-2%), but slightly worse in win rates (-1~3%). Despite the large number of games run (2,207 in total, with 919 baseline games), neither metric is significant.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The surprising part:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Pure-LLM or pure-RL approaches &lt;a href="https://arxiv.org/abs/2401.10568"&gt;[1]&lt;/a&gt;, &lt;a href="https://arxiv.org/abs/2502.20807"&gt;[2]&lt;/a&gt; couldn't get an AI to play and survive full Civilization games. With our hybrid approach, LLMs can survive as long as the game goes (~97.5% LLMs, vs. ~97.3% the in-game AI). The model can be as small as OSS-20B in our internal test.&lt;/p&gt; &lt;p&gt;Moreover, the two models developed &lt;strong&gt;completely different playstyles&lt;/strong&gt;.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OSS-120B went full warmonger: +31.5% more Domination victories, -23% fewer Cultural victories compared to baseline&lt;/li&gt; &lt;li&gt;GLM-4.6 played more balanced, leaning into both Domination and Cultural strategies&lt;/li&gt; &lt;li&gt;Both models preferred &lt;strong&gt;Order&lt;/strong&gt; (&lt;strong&gt;communist-like&lt;/strong&gt;, ~24% more likely) ideology over &lt;strong&gt;Freedom&lt;/strong&gt; (democratic-like)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Cost/latency (OSS-120B):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;~53,000 input / 1,500 output tokens per turn&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~$0.86/game&lt;/strong&gt; (OpenRouter pricing as of 12/2025)&lt;/li&gt; &lt;li&gt;Input tokens scale linearly as the game state grows.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output stays flat: models don't automatically &amp;quot;think harder&amp;quot; in the late game.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Watch more:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Paper link: &lt;a href="https://arxiv.org/abs/2512.18564"&gt;https://arxiv.org/abs/2512.18564&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/1.Civ5Replay"&gt;Example save 1&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/2.Civ5Replay"&gt;Example save 2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://civitas-john.github.io/vox-deorum-replay/?file=https://civitas-john.github.io/vox-deorum-replay/examples/3.Civ5Replay"&gt;Example save 3&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Try it yourself:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The Vox Deorum system is 100% open-sourced and currently in beta testing&lt;/li&gt; &lt;li&gt;GitHub Repo: &lt;a href="https://github.com/CIVITAS-John/vox-deorum"&gt;https://github.com/CIVITAS-John/vox-deorum&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GitHub Release: &lt;a href="https://github.com/CIVITAS-John/vox-deorum/releases"&gt;https://github.com/CIVITAS-John/vox-deorum/releases&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Works with any &lt;strong&gt;OpenAI-compatible local providers&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tccdt44oq79g1.png?width=2291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0b8a4fe5871db4d2bf00f417acd13de3e688037f"&gt;We exposed the game as a MCP server, so your agents can play the game with you&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Your thoughts are greatly appreciated:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;What's a good way to express the game state more efficiently? Consider a late-game turn where you have 20+ cities and 100+ units. Easily 50k+ tokens. Could multimodal help?&lt;/li&gt; &lt;li&gt;How can we get LLMs to play better? I have considered RAG, but there is really little data to &amp;quot;retrieve&amp;quot; here. Possibly self-play + self-reflection + long-term memory?&lt;/li&gt; &lt;li&gt;How are we going to design strategy games if LLMs are to play with you? I have put an LLM spokesperson for civilizations as an example, but there is surely more to do?&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Join us:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I am hiring a PhD student for Fall '26, and we are expanding our game-related work rapidly. Shoot me a DM if you are interested!&lt;/li&gt; &lt;li&gt;I am happy to collaborate with anyone interested in furthering this line of work.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vox-deorum"&gt; /u/vox-deorum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pux0yc/we_asked_oss120b_and_glm_46_to_play_1408/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-24T20:50:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pt50mt</id>
    <title>AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)</title>
    <updated>2025-12-22T17:12:21+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt; &lt;img alt="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" src="https://preview.redd.it/r06ch4zyfs8g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e0a10789f393f350618520fcb81174f3a3dae1c7" title="AMA Announcement: Z.ai, The Opensource Lab Behind GLM-4.7 (Tuesday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r06ch4zyfs8g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pt50mt/ama_announcement_zai_the_opensource_lab_behind/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T17:12:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1ptxm3x</id>
    <title>AMA With Z.AI, The Lab Behind GLM-4.7</title>
    <updated>2025-12-23T16:04:23+00:00</updated>
    <author>
      <name>/u/zixuanlimit</name>
      <uri>https://old.reddit.com/user/zixuanlimit</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi r/LocalLLaMA&lt;/p&gt; &lt;p&gt;Today we are having &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt;, the research lab behind the GLM 4.7. We‚Äôre excited to have them open up and answer your questions directly.&lt;/p&gt; &lt;p&gt;Our participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Yuxuan Zhang, &lt;a href="/u/YuxuanZhangzR"&gt;u/YuxuanZhangzR&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Qinkai Zheng, &lt;a href="/u/QinkaiZheng"&gt;u/QinkaiZheng&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Aohan Zeng, &lt;a href="/u/Sengxian"&gt;u/Sengxian&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Zhenyu Hou, &lt;a href="/u/ZhenyuHou"&gt;u/ZhenyuHou&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Xin Lv, &lt;a href="/u/davidlvxin"&gt;u/davidlvxin&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from 8 AM ‚Äì 11 AM PST, with the &lt;a href="http://Z.AI"&gt;Z.AI&lt;/a&gt; team continuing to follow up on questions over the next 48 hours.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zixuanlimit"&gt; /u/zixuanlimit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ptxm3x/ama_with_zai_the_lab_behind_glm47/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-23T16:04:23+00:00</published>
  </entry>
</feed>
