<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-20T18:54:38+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qi098r</id>
    <title>I built a Graph-Based Agent to automate my PhD research "trial-and-error" loops (because existing tools were too rigid)</title>
    <updated>2026-01-20T12:50:34+00:00</updated>
    <author>
      <name>/u/New-Weekend3503</name>
      <uri>https://old.reddit.com/user/New-Weekend3503</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"&gt; &lt;img alt="I built a Graph-Based Agent to automate my PhD research &amp;quot;trial-and-error&amp;quot; loops (because existing tools were too rigid)" src="https://b.thumbs.redditmedia.com/5zqrKq9xvh4UA3qyWZcgdgFDtjCS_pscE-hwLfbsIpw.jpg" title="I built a Graph-Based Agent to automate my PhD research &amp;quot;trial-and-error&amp;quot; loops (because existing tools were too rigid)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m a Physics PhD student (working on ML applications in Astrophysics).&lt;/p&gt; &lt;p&gt;We all know the pain of research: you have a hypothesis, you write code, you run the experiment, check the error, modify the code, and repeat. I wanted to automate this loop.&lt;/p&gt; &lt;p&gt;I tried existing solutions like OpenEvolve and Microsoft's RD-Agent, but they didn't fit my workflow:&lt;/p&gt; &lt;p&gt;OpenEvolve focuses heavily on &amp;quot;population-based&amp;quot; evolution. I didn't need a swarm of agents; I needed a agent to iterate deeply on a highly customized research strategy, working like another me.&lt;/p&gt; &lt;p&gt;RD-Agent is powerful but felt like a &amp;quot;black box.&amp;quot; It was hard to customize the specific steps of my research process (e.g., &amp;quot;If accuracy &amp;gt; 80%, do X; else, search web for Y&amp;quot;).&lt;/p&gt; &lt;p&gt;So I built AgentCommander.&lt;/p&gt; &lt;p&gt;What it is: It’s a visual, graph-based workflow engine that wraps around the Gemini CLI (and Qwen) to orchestrate long-running, self-improving experiments.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uqadonhk0ieg1.png?width=1489&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2787341f2c1575d5c1b3171dd8eb7ff6f18e4ec5"&gt;Project Introduction&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8pbtc8lt0ieg1.png?width=3045&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=98c6e51bc4ce1315d3912759548e42ff7f03893b"&gt;Control Panel&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key Engineering Features:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Customizable &amp;quot;Graph&amp;quot; Workflow: Instead of a fixed pipeline, you can design your research steps visually (like a flow chart). There's even an in-editor AI assistant to help modify the graph on the fly.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0749326p0ieg1.png?width=3063&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=2564eca4baf4081283992f0d69ba333212f302ff"&gt;Visual Workflow Editor with AI Assistant.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;Best-of-N&amp;quot; Inheritance: It doesn't just blindly scroll forward. It maintains an Evolutionary Tree, ensuring the agent always inherits from the historically best-performing branch (Strategy A -&amp;gt; Strategy A.1 -&amp;gt; Best!).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/h7hxff7w0ieg1.png?width=3059&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=754ed59bbd341d5a2f5ab5f2609087cf59908d6b"&gt;The Evolutionary Tree tracking the best code branches.&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Strict Snapshot Security: This was critical. LLMs love to &amp;quot;cheat&amp;quot; by modifying the evaluation script to get a perfect score. AgentCommander takes a file snapshot before execution. If the Agent tries to touch unauthorized files (like evaluator.py), it instantly reverts the changes.&lt;/p&gt; &lt;p&gt;CLI-First: It uses the Gemini CLI directly, which I found offers better file-permission isolation than other SDK-based approaches.&lt;/p&gt; &lt;p&gt;I’ve been using it to automate my ML tasks for the past month, and it feels like having a clone of myself doing the grunt work.&lt;/p&gt; &lt;p&gt;It's open source (Apache 2.0). I’d love to hear your comments!&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/mx-Liu123/AgentCommander"&gt;https://github.com/mx-Liu123/AgentCommander&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/New-Weekend3503"&gt; /u/New-Weekend3503 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi098r/i_built_a_graphbased_agent_to_automate_my_phd/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:50:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi9qi3</id>
    <title>Windows 11 + RX 7900 XT: vLLM 0.13 running on ROCm (TheRock) with TRITON_ATTN — first working run + benchmark (~3.4 tok/s)</title>
    <updated>2026-01-20T18:49:03+00:00</updated>
    <author>
      <name>/u/Former-University905</name>
      <uri>https://old.reddit.com/user/Former-University905</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;Windows 11 + RX 7900 XT + ROCm TheRock PyTorch nightly (torch 2.11 ROCm 7.11)&lt;/li&gt; &lt;li&gt;vLLM 0.13.0, triton-windows 3.5.1.post23&lt;/li&gt; &lt;li&gt;&lt;code&gt;VLLM_ATTENTION_BACKEND=TRITON_ATTN&lt;/code&gt; gives ~3.4 tok/s (cold run; varies cold vs warm)&lt;/li&gt; &lt;li&gt;&lt;code&gt;VLLM_USE_TRITON_FLASH_ATTN=1&lt;/code&gt; slower/unstable for me&lt;/li&gt; &lt;li&gt;&lt;code&gt;ROCM_ATTN&lt;/code&gt; crashes on my setup; &lt;code&gt;TRITON_ATTN&lt;/code&gt; works&lt;/li&gt; &lt;li&gt;Still hacky: missing compiled ops → Python fallbacks / glue&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/r/ROCm/comments/1qi8jcn/windows_11_rx_7900_xt_vllm_013_running_on_rocm/"&gt;Full logs + full setup details (r/ROCm)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Former-University905"&gt; /u/Former-University905 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi9qi3/windows_11_rx_7900_xt_vllm_013_running_on_rocm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi9qi3/windows_11_rx_7900_xt_vllm_013_running_on_rocm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi9qi3/windows_11_rx_7900_xt_vllm_013_running_on_rocm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T18:49:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhjhlh</id>
    <title>GLM-4.7-Flash-GGUF is here!</title>
    <updated>2026-01-19T22:49:59+00:00</updated>
    <author>
      <name>/u/KvAk_AKPlaysYT</name>
      <uri>https://old.reddit.com/user/KvAk_AKPlaysYT</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt; &lt;img alt="GLM-4.7-Flash-GGUF is here!" src="https://external-preview.redd.it/xaz8me0jAeBOkTb7mKUXdYdIdr8aoSsiwENwulyOJmI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6f21f70be7ae2e1b3f10f33471dbfc4c47ba6518" title="GLM-4.7-Flash-GGUF is here!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KvAk_AKPlaysYT"&gt; /u/KvAk_AKPlaysYT &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AaryanK/GLM-4.7-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhjhlh/glm47flashgguf_is_here/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhz5fz</id>
    <title>GLM 4.7 Flash is endlessly reasoning in chinese</title>
    <updated>2026-01-20T11:55:35+00:00</updated>
    <author>
      <name>/u/xenydactyl</name>
      <uri>https://old.reddit.com/user/xenydactyl</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just downloaded the UD-Q4_K_XL unsloth quant of GLM 4.7 Flash and used the recommended settings &lt;code&gt;--temp 0.2 --top-k 50 --top-p 0.95 --min-p 0.01 --dry-multiplier 1.1&lt;/code&gt;. I pulled and compiled the latest llama.cpp and ran the model and tried using it in kilo code. The entire reasoning block is in chinese and filled with nonsense numbers all over the place. It also seemingly won't stop reasoning. I've encountered this problem with GLM 4.6V Flash too. Does anyone know how to solve this? Am I doing something wrong?&lt;/p&gt; &lt;p&gt;EDIT:&lt;br /&gt; Solution: If you are using vulkan, add the &lt;code&gt;--no-direct-io&lt;/code&gt; flag to the command. After going through the github issues of llama.cpp, I found &lt;a href="https://github.com/ggml-org/llama.cpp/issues/18835"&gt;this&lt;/a&gt; issue. Seems to be a vulkan related issue.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenydactyl"&gt; /u/xenydactyl &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz5fz/glm_47_flash_is_endlessly_reasoning_in_chinese/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz5fz/glm_47_flash_is_endlessly_reasoning_in_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhz5fz/glm_47_flash_is_endlessly_reasoning_in_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T11:55:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi8nm8</id>
    <title>Research SWA and synthetic training protect attention heads under alignment — GQA shows ~5,800× higher noise sensitivity than MHA</title>
    <updated>2026-01-20T18:11:34+00:00</updated>
    <author>
      <name>/u/Fantastic_Art_4948</name>
      <uri>https://old.reddit.com/user/Fantastic_Art_4948</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi8nm8/research_swa_and_synthetic_training_protect/"&gt; &lt;img alt="Research SWA and synthetic training protect attention heads under alignment — GQA shows ~5,800× higher noise sensitivity than MHA" src="https://b.thumbs.redditmedia.com/s4VA20MDhdV9MxX-6vrJId8zU7xrVcbD-QUnxWtuFOI.jpg" title="Research SWA and synthetic training protect attention heads under alignment — GQA shows ~5,800× higher noise sensitivity than MHA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I’m sharing results from a systematic empirical analysis of how alignment&lt;/p&gt; &lt;p&gt;(RLHF / DPO / synthetic training) affects attention head specialization&lt;/p&gt; &lt;p&gt;across open-source LLM families. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;This is not a single-model case study:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;– 25+ open models&lt;/p&gt; &lt;p&gt;– 8 vendor families (Meta, Mistral, Google, Alibaba, Microsoft, etc.)&lt;/p&gt; &lt;p&gt;– standardized protocol (bfloat16, 3 random seeds)&lt;/p&gt; &lt;p&gt;– all results fully reproducible (code + JSONs)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/usn1qbjtojeg1.png?width=900&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e70dd311cf5e69010a25d0a5c8961b043976dc4c"&gt;GQA vs MHA noise sensitivity (log scale).At matched scale, GQA shows ~5,800× higher sensitivity to random attentionnoise than MHA (measured across 3 seeds).&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What we observed (empirical patterns, not causal claims):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;• Sliding Window Attention (e.g. Mistral, Gemma-2) preserves or even increases&lt;/p&gt; &lt;p&gt;attention specialization under alignment, while comparable non-SWA models&lt;/p&gt; &lt;p&gt;show large specialization collapse.&lt;/p&gt; &lt;p&gt;• Synthetic-data training (Phi family) yields near scale-invariant&lt;/p&gt; &lt;p&gt;specialization (SI ≈ 0.33) across a ~10× parameter range.&lt;/p&gt; &lt;p&gt;• Grouped Query Attention shows ~5,800× higher sensitivity to random&lt;/p&gt; &lt;p&gt;attention noise than Multi-Head Attention at matched scale, yet appears&lt;/p&gt; &lt;p&gt;more resilient under structured recursive alignment pressure.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Concrete example:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;– Mistral-7B-Instruct: +4.2% SI vs base&lt;/p&gt; &lt;p&gt;– LLaMA-3.1-8B-Instruct: −56.3% SI vs base&lt;/p&gt; &lt;p&gt;To disambiguate “low specialization = suppression” vs “low specialization =&lt;/p&gt; &lt;p&gt;optimization”, we introduce a simple perturbation-based diagnostic that&lt;/p&gt; &lt;p&gt;distinguishes pathological vs healthy low-SI states via noise response.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this might matter for local models:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;– Architecture choices (GQA vs MHA vs SWA) can strongly affect alignment robustness.&lt;/p&gt; &lt;p&gt;– Training heritage appears more predictive than raw parameter count.&lt;/p&gt; &lt;p&gt;– Some internal failure modes don’t show up in benchmarks, but do show up under noise.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I’d especially appreciate feedback on:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;– alternative explanations for the SWA / synthetic-training effects&lt;/p&gt; &lt;p&gt;– failure modes or confounders I might have missed&lt;/p&gt; &lt;p&gt;– similar internal diagnostics people use for attention / KV behavior&lt;/p&gt; &lt;p&gt;– whether SI is a reasonable proxy for attention diversity at scale&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Paper (Zenodo, CC-BY):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://zenodo.org/records/18316488"&gt;https://zenodo.org/records/18316488&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Code + full reproducibility (MIT):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/buk81/uniformity-asymmetry"&gt;https://github.com/buk81/uniformity-asymmetry&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or share additional plots if useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fantastic_Art_4948"&gt; /u/Fantastic_Art_4948 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi8nm8/research_swa_and_synthetic_training_protect/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi8nm8/research_swa_and_synthetic_training_protect/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi8nm8/research_swa_and_synthetic_training_protect/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T18:11:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhx44i</id>
    <title>Compiled awesome reranker resources into one list</title>
    <updated>2026-01-20T10:00:21+00:00</updated>
    <author>
      <name>/u/midamurat</name>
      <uri>https://old.reddit.com/user/midamurat</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt; &lt;img alt="Compiled awesome reranker resources into one list" src="https://a.thumbs.redditmedia.com/rTzqbjRv7RniD0ivDYw-0Ay72od8XMAyCbeLHfr3dx8.jpg" title="Compiled awesome reranker resources into one list" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/55s7lzc59heg1.png?width=1700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa05cd747a7065b96cd34e6499be0bcb78c1069d"&gt;https://preview.redd.it/55s7lzc59heg1.png?width=1700&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=aa05cd747a7065b96cd34e6499be0bcb78c1069d&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Been building RAG systems for a few months. Info on rerankers was scattered everywhere - docs, papers, Reddit threads. &lt;/p&gt; &lt;p&gt;Put it all in one place: &lt;a href="https://github.com/agentset-ai/awesome-rerankers"&gt;https://github.com/agentset-ai/awesome-rerankers&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Quick start code (works out of the box)&lt;/li&gt; &lt;li&gt;Model comparison table&lt;/li&gt; &lt;li&gt;Local options (FlashRank runs on CPU, ~4MB)&lt;/li&gt; &lt;li&gt;Framework integrations&lt;/li&gt; &lt;li&gt;Live benchmarks with ELO scores&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Rerankers give you a solid 15-40% accuracy boost over just vector search. But figuring out which one to use or whether you can run it locally was a pain.&lt;/p&gt; &lt;p&gt;This covers it. If you're building RAG, might save you some time.&lt;/p&gt; &lt;p&gt;Let me know if I missed anything useful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/midamurat"&gt; /u/midamurat &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx44i/compiled_awesome_reranker_resources_into_one_list/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:00:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi9hmc</id>
    <title>llama.cpp: Anthropic Messages API</title>
    <updated>2026-01-20T18:40:23+00:00</updated>
    <author>
      <name>/u/pablines</name>
      <uri>https://old.reddit.com/user/pablines</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anthropic Messages API was recently merged into llama.cpp, allowing tools like Claude Code to connect directly to a local llama.cpp server.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full Messages API&lt;/strong&gt;: &lt;code&gt;POST /v1/messages&lt;/code&gt; for chat completions with streaming support&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token counting&lt;/strong&gt;: &lt;code&gt;POST /v1/messages/count_tokens&lt;/code&gt; to count tokens without generating&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tool use&lt;/strong&gt;: Function calling with &lt;code&gt;tool_use&lt;/code&gt; and &lt;code&gt;tool_result&lt;/code&gt; content blocks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Vision&lt;/strong&gt;: Image inputs via base64 or URL (requires multimodal model)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extended thinking&lt;/strong&gt;: Support for reasoning models via the &lt;code&gt;thinking&lt;/code&gt; parameter&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Streaming&lt;/strong&gt;: Proper Anthropic SSE event types (&lt;code&gt;message_start&lt;/code&gt;, &lt;code&gt;content_block_delta&lt;/code&gt;, etc.)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;# Start server with a capable model&lt;/p&gt; &lt;p&gt;llama-server -hf unsloth/Qwen3-Next-80B-A3B-Instruct-GGUF:Q4_K_M&lt;/p&gt; &lt;p&gt;# Run Claude Code with local endpoint&lt;/p&gt; &lt;p&gt;ANTHROPIC_BASE_URL=&lt;a href="http://127.0.0.1:8080"&gt;http://127.0.0.1:8080&lt;/a&gt; claude&lt;/p&gt; &lt;p&gt;For best results with agentic workloads, use specialized agentic coding models like Nemotron, Qwen3 Coder, Kimi K2, or MiniMax M2.&lt;/p&gt; &lt;p&gt;link: &lt;a href="https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp"&gt;https://huggingface.co/blog/ggml-org/anthropic-messages-api-in-llamacpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pablines"&gt; /u/pablines &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi9hmc/llamacpp_anthropic_messages_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi9hmc/llamacpp_anthropic_messages_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi9hmc/llamacpp_anthropic_messages_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T18:40:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qh5wdq</id>
    <title>zai-org/GLM-4.7-Flash · Hugging Face</title>
    <updated>2026-01-19T14:40:27+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt; &lt;img alt="zai-org/GLM-4.7-Flash · Hugging Face" src="https://external-preview.redd.it/Qs0t4y5eLm-uwORWdP6T0dcwW2T6VJyQFBUSY70CTF8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8700f4a43fe16a1031ccda94b517fd709573a5c3" title="zai-org/GLM-4.7-Flash · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/zai-org/GLM-4.7-Flash"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qh5wdq/zaiorgglm47flash_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T14:40:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi3d4c</id>
    <title>Local LLMs + Desktop Agents: An open source Claude Cowork</title>
    <updated>2026-01-20T15:01:36+00:00</updated>
    <author>
      <name>/u/Manga_m</name>
      <uri>https://old.reddit.com/user/Manga_m</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone!&lt;/p&gt; &lt;p&gt;For the past six months, we’ve been building an open-source local agent called Eigent, an open-source alternative of Cowork and was #1 on GitHub Trending! It supports BYOK （Gemini 3 pro/gpt 5.2/ Z.ai GLM-4.7/MiniMax M2 and more）and local LLMs via Ollama, vLLM, SGLang, and LM Studio. Can help you to organize local files, automate browsers end-to-end.&lt;/p&gt; &lt;p&gt;Why we chose to build a local desktop agent？Even though the web has a much larger traffic entry point, but we believe the first principle should be the upper bound of what the agent can actually do.&lt;/p&gt; &lt;p&gt;The main reasons are:&lt;/p&gt; &lt;p&gt;Context: only a desktop agent can seamlessly access the user’s real context.&lt;/p&gt; &lt;p&gt;Permissions: agents need permissions. On desktop, an agent can operate local file systems, software, system-level calls, and even interact with hardware.&lt;/p&gt; &lt;p&gt;Coverage: a desktop agent can do everything a web agent can do, either through an embedded Chromium browser (e.g. Electron) or via browser extensions.&lt;/p&gt; &lt;p&gt;At the core is CAMEL’s Workforce system, which is inspired by distributing systems: a root node for task planning and coordination, worker nodes for execution, and an asynchronous task channel. It also supports failure tolerance and recursive workers for long-horizon tasks. All of this is open source.&lt;/p&gt; &lt;p&gt;For browser automation, Eigent uses a two-layer architecture:&lt;/p&gt; &lt;p&gt;a Python layer for agent reasoning and orchestration&lt;/p&gt; &lt;p&gt;a TypeScript layer (built on Playwright) for native browser control (DOM ops, SoM markers, occlusion handling)&lt;/p&gt; &lt;p&gt;These two layers communicate asynchronously via WebSockets to keep things low-latency and avoid the limits of Python-only automation. This stack is also open source.&lt;/p&gt; &lt;p&gt;That said, the hardest problems we face today is the local desktop runtime. Supporting multiple operating systems, versions, and package mirrors has been extremely painful. Our desktop agent installs Python and TypeScript dependencies on first launch, and supporting this reliably across macOS and Windows has been more complex than we initially expected.&lt;/p&gt; &lt;p&gt;After looking into a VM-based approach that uses Apple’s Virtualization framework to run Ubuntu on macOS, we started wondering whether a similar setup could help.&lt;/p&gt; &lt;p&gt;Could this kind of VM-based runtime or something equivalent realistically solve the cross-platform issues across both macOS and Windows?&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/eigent-ai/eigent"&gt;https://github.com/eigent-ai/eigent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or exchange notes!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Manga_m"&gt; /u/Manga_m &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3d4c/local_llms_desktop_agents_an_open_source_claude/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3d4c/local_llms_desktop_agents_an_open_source_claude/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3d4c/local_llms_desktop_agents_an_open_source_claude/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:01:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhx6u1</id>
    <title>no problems with GLM-4.7-Flash</title>
    <updated>2026-01-20T10:04:33+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"&gt; &lt;img alt="no problems with GLM-4.7-Flash" src="https://b.thumbs.redditmedia.com/RyfGOsESx4YsXYbZlTJQbs2Km0GhT7KhWobtlXZeH9A.jpg" title="no problems with GLM-4.7-Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I saw many comments that GLM-4.7-Flash doesn't work correctly, could you show specific prompts? I am not doing anything special, all settings are default&lt;/p&gt; &lt;p&gt;!!! UPDATE !!! - check the comments from &lt;a href="https://www.reddit.com/user/shokuninstudio/"&gt;shokuninstudio&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qhx6u1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhx6u1/no_problems_with_glm47flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:04:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhlnsv</id>
    <title>Unsloth GLM 4.7-Flash GGUF</title>
    <updated>2026-01-20T00:17:58+00:00</updated>
    <author>
      <name>/u/Wooden-Deer-1276</name>
      <uri>https://old.reddit.com/user/Wooden-Deer-1276</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/unsloth/GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wooden-Deer-1276"&gt; /u/Wooden-Deer-1276 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhlnsv/unsloth_glm_47flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T00:17:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhqzsi</id>
    <title>Mosquito - 7.3M parameter tiny knowledge model</title>
    <updated>2026-01-20T04:16:20+00:00</updated>
    <author>
      <name>/u/Lopsided-Repair-3638</name>
      <uri>https://old.reddit.com/user/Lopsided-Repair-3638</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A mosquito brain size model (7.3M params) that can answer surprisingly many general knowledge questions. Demo: &lt;a href="https://huggingface.co/spaces/ag14850/Mosquito-Demo"&gt;https://huggingface.co/spaces/ag14850/Mosquito-Demo&lt;/a&gt; Model: &lt;a href="https://huggingface.co/ag14850/Mosquito"&gt;https://huggingface.co/ag14850/Mosquito&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Lopsided-Repair-3638"&gt; /u/Lopsided-Repair-3638 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhqzsi/mosquito_73m_parameter_tiny_knowledge_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T04:16:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi78u3</id>
    <title>I think Giga Potato:free in Kilo Code is Deepseek V4</title>
    <updated>2026-01-20T17:22:33+00:00</updated>
    <author>
      <name>/u/quantier</name>
      <uri>https://old.reddit.com/user/quantier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking for a new free model in Kilo Code after Minimax M2.1 was removed as a free model.&lt;/p&gt; &lt;p&gt;Searched for free and found Giga Potato:free and Googled it (yes the AI models don’t usually have the most recent stuff in their search)&lt;/p&gt; &lt;p&gt;I found this blog article: &lt;a href="https://blog.kilo.ai/p/announcing-a-powerful-new-stealth"&gt;https://blog.kilo.ai/p/announcing-a-powerful-new-stealth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have now tested it and am mindblown it performs like Sonnet 4.5 and maybe even like Opus 4.5. I can give it very short poor prompts and it reasons itself to amazing results! &lt;/p&gt; &lt;p&gt;Whatever open source model this is…..it’s crazy! Honestly!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/quantier"&gt; /u/quantier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi78u3/i_think_giga_potatofree_in_kilo_code_is_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T17:22:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhitrj</id>
    <title>GLM 4.7 Flash official support merged in llama.cpp</title>
    <updated>2026-01-19T22:24:24+00:00</updated>
    <author>
      <name>/u/ayylmaonade</name>
      <uri>https://old.reddit.com/user/ayylmaonade</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt; &lt;img alt="GLM 4.7 Flash official support merged in llama.cpp" src="https://external-preview.redd.it/AVP8Isc32PMjAyVGtAipaav3x8aU8JY8Lx1bZ_yPak0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=43081fb39d8cfd3c8faeeb3516b7513654ed8fce" title="GLM 4.7 Flash official support merged in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ayylmaonade"&gt; /u/ayylmaonade &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/18936"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhitrj/glm_47_flash_official_support_merged_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:24:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhpima</id>
    <title>Bartowski comes through again. GLM 4.7 flash GGUF</title>
    <updated>2026-01-20T03:07:33+00:00</updated>
    <author>
      <name>/u/RenewAi</name>
      <uri>https://old.reddit.com/user/RenewAi</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.7-Flash-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RenewAi"&gt; /u/RenewAi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhpima/bartowski_comes_through_again_glm_47_flash_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T03:07:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhii5v</id>
    <title>My gpu poor comrades, GLM 4.7 Flash is your local agent</title>
    <updated>2026-01-19T22:12:06+00:00</updated>
    <author>
      <name>/u/__Maximum__</name>
      <uri>https://old.reddit.com/user/__Maximum__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried many MoE models at 30B or under and all of them failed sooner or later in an agentic framework. If z.ai is not redirecting my requests to another model, then GLM 4.7 Flash is finally the reliable (soon local) agent that I desperately wanted.&lt;/p&gt; &lt;p&gt;I am running it since more than half an hour on opencode and it produced hundreds of thousands tokens in one session (with context compacting obviously) without any tool calling errors. It clones github repos, it runs all kind of commands, edits files, commits changes, all perfect, not a single error yet.&lt;/p&gt; &lt;p&gt;Can't wait for GGUFs to try this locally.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__Maximum__"&gt; /u/__Maximum__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhii5v/my_gpu_poor_comrades_glm_47_flash_is_your_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-19T22:12:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhwfe0</id>
    <title>How to run and fine-tune GLM-4.7-Flash locally</title>
    <updated>2026-01-20T09:19:00+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt; &lt;img alt="How to run and fine-tune GLM-4.7-Flash locally" src="https://preview.redd.it/g5y2icqg1heg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=97e9539a968badc4795c9185a6384ef02c6b8c01" title="How to run and fine-tune GLM-4.7-Flash locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;GLM-4.7-Flash is Z.ai’s new 30B MoE reasoning model built for local deployment, delivering best-in-class performance for coding, agentic workflows, and chat. &lt;/li&gt; &lt;li&gt;The model uses ~3.6B parameters, supports 200K context, and leads SWE-Bench, GPQA, and reasoning/chat benchmarks.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Official guide - &lt;a href="https://unsloth.ai/docs/models/glm-4.7-flash"&gt;https://unsloth.ai/docs/models/glm-4.7-flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g5y2icqg1heg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhwfe0/how_to_run_and_finetune_glm47flash_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T09:19:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi06kp</id>
    <title>One of the DeepSeek repositories got updated with a reference to a new “model1” model.</title>
    <updated>2026-01-20T12:46:56+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"&gt; &lt;img alt="One of the DeepSeek repositories got updated with a reference to a new “model1” model." src="https://preview.redd.it/j3ifa4kn2ieg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9aab1dc0b54e56da3161e03846e6cdc3fb3e15f1" title="One of the DeepSeek repositories got updated with a reference to a new “model1” model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source DeepSeek on GitHub: FlashMLA: flash_mla/flash_mla_interface.py: &lt;a href="https://github.com/deepseek-ai/FlashMLA/blob/main/flash_mla/flash_mla_interface.py"&gt;https://github.com/deepseek-ai/FlashMLA/blob/main/flash_mla/flash_mla_interface.py&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j3ifa4kn2ieg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi06kp/one_of_the_deepseek_repositories_got_updated_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T12:46:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi0xro</id>
    <title>GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)</title>
    <updated>2026-01-20T13:21:34+00:00</updated>
    <author>
      <name>/u/LayerHot</name>
      <uri>https://old.reddit.com/user/LayerHot</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt; &lt;img alt="GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)" src="https://b.thumbs.redditmedia.com/I-hI61zDUgcyCW4A5C3NZVA2PraY7bDNdxBTnVt-GkY.jpg" title="GLM-4.7-Flash benchmarks: 4,398 tok/s on H200, 112 tok/s on RTX 6000 Ada (GGUF)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran some benchmarks with the new GLM-4.7-Flash model with vLLM and also tested llama.cpp with Unsloth dynamic quants&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GPUs are from&lt;/strong&gt; &lt;a href="http://jarvislabs.ai"&gt;&lt;strong&gt;jarvislabs.ai&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Sharing some results here.&lt;/p&gt; &lt;h1&gt;vLLM on single H200 SXM&lt;/h1&gt; &lt;p&gt;Ran this with 64K context, 500 prompts from InstructCoder dataset.&lt;/p&gt; &lt;p&gt;- Single user: 207 tok/s, 35ms TTFT&lt;/p&gt; &lt;p&gt;- At 32 concurrent users: 2,267 tok/s, 85ms TTFT&lt;/p&gt; &lt;p&gt;- Peak throughput (no concurrency limit): 4,398 tok/s&lt;/p&gt; &lt;p&gt;All of the benchmarks were done with &lt;a href="https://docs.vllm.ai/en/latest/benchmarking/cli/"&gt;vLLM benchmark CLI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Concurrent&lt;/th&gt; &lt;th align="left"&gt;Decode tok/s&lt;/th&gt; &lt;th align="left"&gt;TTFT (median)&lt;/th&gt; &lt;th align="left"&gt;TTFT (P99)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;207&lt;/td&gt; &lt;td align="left"&gt;35ms&lt;/td&gt; &lt;td align="left"&gt;42ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;348&lt;/td&gt; &lt;td align="left"&gt;44ms&lt;/td&gt; &lt;td align="left"&gt;55ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;547&lt;/td&gt; &lt;td align="left"&gt;53ms&lt;/td&gt; &lt;td align="left"&gt;66ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;882&lt;/td&gt; &lt;td align="left"&gt;61ms&lt;/td&gt; &lt;td align="left"&gt;161ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;1,448&lt;/td&gt; &lt;td align="left"&gt;69ms&lt;/td&gt; &lt;td align="left"&gt;187ms&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;32&lt;/td&gt; &lt;td align="left"&gt;2,267&lt;/td&gt; &lt;td align="left"&gt;85ms&lt;/td&gt; &lt;td align="left"&gt;245ms&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Fits fine on single H200 at 64K. For full context (200k) we will need 2xH200.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/a9tzl54z7ieg1.png?width=4291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a246dd4a6b53b58c42106e476e8e14a2c76becd3"&gt;https://preview.redd.it/a9tzl54z7ieg1.png?width=4291&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a246dd4a6b53b58c42106e476e8e14a2c76becd3&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;llama.cpp GGUF on RTX 6000 Ada (48GB)&lt;/h1&gt; &lt;p&gt;Ran the Unsloth dynamic quants with 16k context length and guide by &lt;a href="https://unsloth.ai/docs/models/glm-4.7"&gt;Unsloth&lt;/a&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quant&lt;/th&gt; &lt;th align="left"&gt;Generation tok/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;112&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q6_K_XL&lt;/td&gt; &lt;td align="left"&gt;100&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Q8_K_XL&lt;/td&gt; &lt;td align="left"&gt;91&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1qi0xro/video/h3damlpb8ieg1/player"&gt;https://reddit.com/link/1qi0xro/video/h3damlpb8ieg1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In my initial testing this is really capable and good model for its size.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LayerHot"&gt; /u/LayerHot &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi0xro/glm47flash_benchmarks_4398_toks_on_h200_112_toks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T13:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhs2sd</id>
    <title>It's been one year since the release of Deepseek-R1</title>
    <updated>2026-01-20T05:08:29+00:00</updated>
    <author>
      <name>/u/Recoil42</name>
      <uri>https://old.reddit.com/user/Recoil42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt; &lt;img alt="It's been one year since the release of Deepseek-R1" src="https://preview.redd.it/cin706z9tfeg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=65fbe53bfb15712186113b0e795fc46c050d0d13" title="It's been one year since the release of Deepseek-R1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Recoil42"&gt; /u/Recoil42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cin706z9tfeg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhs2sd/its_been_one_year_since_the_release_of_deepseekr1/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T05:08:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1qhxlgy</id>
    <title>glm-4.7-flash has the best thinking process with clear steps, I love it</title>
    <updated>2026-01-20T10:28:16+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I tested several personal prompts like &lt;code&gt;imagine you are in a farm, what is your favorite barn color?&lt;/code&gt;&lt;/li&gt; &lt;li&gt;although the prompt is short, glm can analyze the prompt and give clear thinking process&lt;/li&gt; &lt;li&gt;without my instruction in the prompt, glm mostly thinks in these steps: &lt;ol&gt; &lt;li&gt;request/goal analysis&lt;/li&gt; &lt;li&gt;brainstorm&lt;/li&gt; &lt;li&gt;draft response&lt;/li&gt; &lt;li&gt;refine response: gives option1, option2, option3...&lt;/li&gt; &lt;li&gt;revise response/plan&lt;/li&gt; &lt;li&gt;polish&lt;/li&gt; &lt;li&gt;final response&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;so the glm thinking duration(110s) is really long compared to nemotron-nano(19s), but the thinking content is my favorite of all the small models. the final response is also clear &lt;ul&gt; &lt;li&gt;thinking process like this seems to be perfect for data analysis (waiting for a fine-tune)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;overall, i love glm-4.7-flash, and will try to replace qwen3-30b and nemotron-nano.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;del&gt;but GLM-4.7-Flash-mlx-4bit is very&lt;/del&gt; &lt;strong&gt;&lt;del&gt;slow&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;at&lt;/del&gt; &lt;strong&gt;&lt;del&gt;19 token/s&lt;/del&gt;&lt;/strong&gt; &lt;del&gt;compared to nemotron-anno-mlx-4bit&lt;/del&gt; &lt;strong&gt;&lt;del&gt;30+ token/s&lt;/del&gt;&lt;/strong&gt;&lt;del&gt;. i donnot understand.&lt;/del&gt;&lt;/p&gt; &lt;p&gt;I'm using &lt;a href="https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit"&gt;https://huggingface.co/lmstudio-community/GLM-4.7-Flash-MLX-4bit&lt;/a&gt; on my m4 macbook air. with default config, the model often goes into loop. with the following config, it finally works for me&lt;/p&gt; &lt;ul&gt; &lt;li&gt;temperature 1.0&lt;/li&gt; &lt;li&gt;repeat penalty: 1.1&lt;/li&gt; &lt;li&gt;top-p: 0.95&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;is there any trick to make the thinking process faster? Thinking can be toggled on/off through lmstudio ui, but i donnot want to disable it, how to make thinking faster?&lt;/p&gt; &lt;ul&gt; &lt;li&gt;lowering the temperature helps. tried 1.0/0.8/0.6&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;EDIT&lt;/strong&gt;:&lt;br /&gt; - 🐛 I tried several more prompts. sometimes the thinking content does not comply to the flow above, for these situations, the model often goes into loops.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qhxlgy/glm47flash_has_the_best_thinking_process_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T10:28:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi3nmd</id>
    <title>Over 6K novels with reasoning traces to train full book writing LLMs</title>
    <updated>2026-01-20T15:12:25+00:00</updated>
    <author>
      <name>/u/XMasterDE</name>
      <uri>https://old.reddit.com/user/XMasterDE</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt; &lt;img alt="Over 6K novels with reasoning traces to train full book writing LLMs" src="https://b.thumbs.redditmedia.com/2qE82dS5QMmSeeQNcY514CvEOcapkm7SNym4UUAFIwE.jpg" title="Over 6K novels with reasoning traces to train full book writing LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/zzxy8r31tieg1.jpg?width=5504&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb966352c2548369a731f0bff03a131c8ec4a1b2"&gt;https://preview.redd.it/zzxy8r31tieg1.jpg?width=5504&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fb966352c2548369a731f0bff03a131c8ec4a1b2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We’re releasing an update to our &lt;strong&gt;LongPage&lt;/strong&gt; dataset.&lt;/p&gt; &lt;p&gt;LongPage is a dataset of &lt;strong&gt;full-length novels paired with reasoning traces&lt;/strong&gt;: each book includes a &lt;strong&gt;hierarchical planning trace&lt;/strong&gt; that breaks the story down from high-level outline into chapters/scenes to support training &lt;strong&gt;full-book writing LLMs&lt;/strong&gt;. The previous release contained ~300 books; this update expands the dataset to &lt;strong&gt;6K+ novels&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;We’re also currently training a &lt;strong&gt;full-book writing model&lt;/strong&gt; on LongPage. We already have early checkpoints running internally, and we plan to release the model as soon as the output quality reaches an acceptable level.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HF Link:&lt;/strong&gt; &lt;a href="https://huggingface.co/datasets/Pageshift-Entertainment/LongPage"&gt;https://huggingface.co/datasets/Pageshift-Entertainment/LongPage&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to follow our journey as we build world-class storytelling models, you can find us here:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Website: &lt;a href="https://pageshift-entertainment.ai/"&gt;https://pageshift-entertainment.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;X (Twitter): &lt;a href="https://x.com/pageshiftAI"&gt;https://x.com/pageshiftAI&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hugging Face: &lt;a href="https://huggingface.co/Pageshift-Entertainment"&gt;https://huggingface.co/Pageshift-Entertainment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LinkedIn: &lt;a href="https://www.linkedin.com/company/pageshift-ai/"&gt;https://www.linkedin.com/company/pageshift-ai/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterDE"&gt; /u/XMasterDE &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi3nmd/over_6k_novels_with_reasoning_traces_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:12:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi512t</id>
    <title>Liquid AI released the best thinking Language Model Under 1GB</title>
    <updated>2026-01-20T16:02:42+00:00</updated>
    <author>
      <name>/u/PauLabartaBajo</name>
      <uri>https://old.reddit.com/user/PauLabartaBajo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"&gt; &lt;img alt="Liquid AI released the best thinking Language Model Under 1GB" src="https://preview.redd.it/nazcfmti1jeg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d85adead4df4e2a21194a3e6ae29b3752731c529" title="Liquid AI released the best thinking Language Model Under 1GB" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Liquid AI released LFM2.5-1.2B-Thinking, a reasoning model that runs entirely on-device. &lt;/p&gt; &lt;p&gt;What needed a data centre two years ago now runs on any phone with 900 MB of memory. &lt;/p&gt; &lt;p&gt;-&amp;gt; Trained specifically for concise reasoning&lt;br /&gt; -&amp;gt; Generates internal thinking traces before producing answers&lt;br /&gt; -&amp;gt; Enables systematic problem-solving at edge-scale latency&lt;br /&gt; -&amp;gt; Shines on tool use, math, and instruction following&lt;br /&gt; -&amp;gt; Matches or exceeds Qwen3-1.7B (thinking mode) acrross most performance benchmarks, despite having 40% less parameters. &lt;/p&gt; &lt;p&gt;At inference time, the gap widens further, outperforming both pure transformer models and hybrid architectures in speed and memory efficiency. &lt;/p&gt; &lt;p&gt;LFM2.5-1.2B-Thinking is available today: with broad, day-one support across the on-device ecosystem.&lt;br /&gt; Hugging Face: &lt;a href="https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking"&gt;https://huggingface.co/LiquidAI/LFM2.5-1.2B-Thinking&lt;/a&gt;&lt;br /&gt; LEAP: &lt;a href="https://leap.liquid.ai/models?model=lfm2.5-1.2b-thinking"&gt;https://leap.liquid.ai/models?model=lfm2.5-1.2b-thinking&lt;/a&gt;&lt;br /&gt; Liquid AI Playground: &lt;a href="https://playground.liquid.ai/login?callbackUrl=%2F"&gt;https://playground.liquid.ai/login?callbackUrl=%2F&lt;/a&gt; &lt;/p&gt; &lt;p&gt;At&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PauLabartaBajo"&gt; /u/PauLabartaBajo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nazcfmti1jeg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi512t/liquid_ai_released_the_best_thinking_language/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T16:02:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qi4uj2</id>
    <title>768Gb Fully Enclosed 10x GPU Mobile AI Build</title>
    <updated>2026-01-20T15:56:13+00:00</updated>
    <author>
      <name>/u/SweetHomeAbalama0</name>
      <uri>https://old.reddit.com/user/SweetHomeAbalama0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt; &lt;img alt="768Gb Fully Enclosed 10x GPU Mobile AI Build" src="https://b.thumbs.redditmedia.com/IFwD006aQ7uS94rhW8Tb5SMKqOvtmvGGWhQOsclMVOE.jpg" title="768Gb Fully Enclosed 10x GPU Mobile AI Build" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I haven't seen a system with this format before but with how successful the result was I figured I might as well share it.&lt;/p&gt; &lt;p&gt;Specs:&lt;br /&gt; Threadripper Pro 3995WX w/ ASUS WS WRX80e-sage wifi ii&lt;/p&gt; &lt;p&gt;512Gb DDR4&lt;/p&gt; &lt;p&gt;256Gb GDDR6X/GDDR7 (8x 3090 + 2x 5090)&lt;/p&gt; &lt;p&gt;EVGA 1600W + Asrock 1300W PSU's&lt;/p&gt; &lt;p&gt;Case: Thermaltake Core W200&lt;/p&gt; &lt;p&gt;OS: Ubuntu&lt;/p&gt; &lt;p&gt;Est. expense: ~$17k&lt;/p&gt; &lt;p&gt;The objective was to make a system for running extra large MoE models (Deepseek and Kimi K2 specifically), that is also capable of lengthy video generation and rapid high detail image gen (the system will be supporting a graphic designer). The challenges/constraints: The system should be easily movable, and it should be enclosed. The result technically satisfies the requirements, with only one minor caveat. Capital expense was also an implied constraint. We wanted to get the most potent system possible with the best technology currently available, without going down the path of needlessly spending tens of thousands of dollars for diminishing returns on performance/quality/creativity potential. Going all 5090's or 6000 PRO's would have been unfeasible budget-wise and in the end likely unnecessary, two 6000's alone could have eaten the cost of the entire amount spent on the project, and if not for the two 5090's the final expense would have been much closer to ~$10k (still would have been an extremely capable system, but this graphic artist would really benefit from the image/video gen time savings that only a 5090 can provide).&lt;/p&gt; &lt;p&gt;The biggest hurdle was the enclosure problem. I've seen mining frames zip tied to a rack on wheels as a solution for mobility, but not only is this aesthetically unappealing, build construction and sturdiness quickly get called into question. This system would be living under the same roof with multiple cats, so an enclosure was almost beyond a nice-to-have, the hardware will need a physical barrier between the expensive components and curious paws. Mining frames were quickly ruled out altogether after a failed experiment. Enter the W200, a platform that I'm frankly surprised I haven't heard suggested before in forum discussions about planning multi-GPU builds, and is the main motivation for this post. The W200 is intended to be a dual-system enclosure, but when the motherboard is installed upside-down in its secondary compartment, this makes a perfect orientation to connect risers to mounted GPU's in the &amp;quot;main&amp;quot; compartment. If you don't mind working in dense compartments to get everything situated (the sheer density overall of the system is among its only drawbacks), this approach reduces the jank from mining frame + wheeled rack solutions significantly. A few zip ties were still required to secure GPU's in certain places, but I don't feel remotely as anxious about moving the system to a different room or letting cats inspect my work as I would if it were any other configuration.&lt;/p&gt; &lt;p&gt;Now the caveat. Because of the specific GPU choices made (3x of the 3090's are AIO hybrids), this required putting one of the W200's fan mounting rails on the main compartment side in order to mount their radiators (pic shown with the glass panel open, but it can be closed all the way). This means the system technically should not run without this panel at least slightly open so it doesn't impede exhaust, but if these AIO 3090's were blower/air cooled, I see no reason why this couldn't run fully closed all the time as long as fresh air intake is adequate.&lt;/p&gt; &lt;p&gt;The final case pic shows the compartment where the actual motherboard is installed (it is however very dense with risers and connectors so unfortunately it is hard to actually see much of anything) where I removed one of the 5090's. Airflow is very good overall (I believe 12x 140mm fans were installed throughout), GPU temps remain in good operation range under load, and it is surprisingly quiet when inferencing. Honestly, given how many fans and high power GPU's are in this thing, I am impressed by the acoustics, I don't have a sound meter to measure db's but to me it doesn't seem much louder than my gaming rig.&lt;/p&gt; &lt;p&gt;I typically power limit the 3090's to 200-250W and the 5090's to 500W depending on the workload.&lt;/p&gt; &lt;p&gt;.&lt;/p&gt; &lt;p&gt;Benchmarks&lt;/p&gt; &lt;p&gt;Deepseek V3.1 Terminus Q2XXS (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 2338 tokens&lt;/p&gt; &lt;p&gt;Time to first token - 1.38s&lt;/p&gt; &lt;p&gt;Token gen rate - 24.92tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;GLM 4.6 Q4KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 4096&lt;/p&gt; &lt;p&gt;Time to first token - 0.76s&lt;/p&gt; &lt;p&gt;Token gen rate - 26.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Kimi K2 TQ1 (87% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 1664&lt;/p&gt; &lt;p&gt;Time to first token - 2.59s&lt;/p&gt; &lt;p&gt;Token gen rate - 19.61tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Hermes 4 405b Q3KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - was so underwhelmed by the response quality I forgot to record lol&lt;/p&gt; &lt;p&gt;Time to first token - 1.13s&lt;/p&gt; &lt;p&gt;Token gen rate - 3.52tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;Qwen 235b Q6KXL (100% GPU offload)&lt;/p&gt; &lt;p&gt;Tokens generated - 3081&lt;/p&gt; &lt;p&gt;Time to first token - 0.42s&lt;/p&gt; &lt;p&gt;Token gen rate - 31.54tps&lt;/p&gt; &lt;p&gt;__________________________&lt;/p&gt; &lt;p&gt;I've thought about doing a cost breakdown here, but with price volatility and the fact that so many components have gone up since I got them, I feel like there wouldn't be much of a point and may only mislead someone. Current RAM prices alone would completely change the estimate cost of doing the same build today by several thousand dollars. Still, I thought I'd share my approach on the off chance it inspires or is interesting to someone.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SweetHomeAbalama0"&gt; /u/SweetHomeAbalama0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qi4uj2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qi4uj2/768gb_fully_enclosed_10x_gpu_mobile_ai_build/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-20T15:56:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
