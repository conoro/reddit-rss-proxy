<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-25T18:41:50+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nqbzcb</id>
    <title>Working on a budget build, does this look like it would work?</title>
    <updated>2025-09-25T16:50:31+00:00</updated>
    <author>
      <name>/u/A13XM01R</name>
      <uri>https://old.reddit.com/user/A13XM01R</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Basically trying to do a budget build, specs are 40 cores, 256GB RAM, 48GB VRAM. Does this look like it would work? What kind of speed might I be able to expect?&lt;/p&gt; &lt;p&gt;X99 DUAL PLUS Mining Motherboard Supports DDR4 RAM 256GB LGA 2011-3 V3/V4 CPU Socket Computer Motherboard 4 *USB3.0 4* PCIe3.0 X 152.29 x1 152.29&lt;/p&gt; &lt;p&gt;Non-official edition Intel Xeon E5-2698 V4 ES QHUZ 2.0GHz 20Core CPU Processor 59.9 x2 119.8&lt;/p&gt; &lt;p&gt;upHere P4K CPU Air Cooler 6mm x 4 Copper Heat Pipes CPU Cooler 20.99 x2 41.98&lt;/p&gt; &lt;p&gt;MC03.2 Mining Rig Case - Holds 8 Fans | No Motherboard/CPU/RAM Included 109.99 x1 109.99&lt;/p&gt; &lt;p&gt;Timetec 32GB KIT(2x16GB) DDR4 2400MHz PC4-19200 Non-ECC 59.99 x8 479.92&lt;/p&gt; &lt;p&gt;GIGABYTE NVIDIA GeForce RTX 3060 12GB GDDR6 Graphics Card 274.99 x4 1099.96&lt;/p&gt; &lt;p&gt;CORSAIR RM1000e (2025) Fully Modular Low-Noise ATX Power Supply 149.99 x1 149.99 &lt;/p&gt; &lt;p&gt;Total 2153.93&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/A13XM01R"&gt; /u/A13XM01R &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqbzcb/working_on_a_budget_build_does_this_look_like_it/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqbzcb/working_on_a_budget_build_does_this_look_like_it/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqbzcb/working_on_a_budget_build_does_this_look_like_it/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T16:50:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqcrn2</id>
    <title>A Voice model that can add emotion to an AI narration</title>
    <updated>2025-09-25T17:20:13+00:00</updated>
    <author>
      <name>/u/Mysterious-Comment94</name>
      <uri>https://old.reddit.com/user/Mysterious-Comment94</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to my limitations with Vram I decided to use kokoro 1.0 and I was pleasantly surprised by the crisp clarity of the output. I also got a very chill and pleasant voice using the voice blending feature. However, understandably there are no emotional controls in the model. By using quotations and stuff I can maybe add a bit emotion sometimes, but overall it is flat. I've been trying to find any models that can help with this specific task but I have been unsuccessful. Google being google only shows me results for more TTS model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mysterious-Comment94"&gt; /u/Mysterious-Comment94 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqcrn2/a_voice_model_that_can_add_emotion_to_an_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqcrn2/a_voice_model_that_can_add_emotion_to_an_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqcrn2/a_voice_model_that_can_add_emotion_to_an_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T17:20:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqcy4k</id>
    <title>GLM-4.5-air outputting \n x times when asked to create structured output</title>
    <updated>2025-09-25T17:27:08+00:00</updated>
    <author>
      <name>/u/Best_Sail5</name>
      <uri>https://old.reddit.com/user/Best_Sail5</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys ,&lt;/p&gt; &lt;p&gt;Been spinning up GLM-4.5-air lately and i make him generate some structured output. Sometimes (not constantly) it just gets stuck after one of the field names generating '\n' in loop&lt;/p&gt; &lt;p&gt;For inference parameters i use :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;{&amp;quot;extra_body&amp;quot;: {'repetition_penalty': 1.05,'length_penalty': 1.05}} {&amp;quot;temperature&amp;quot;: 0.6, &amp;quot;top_p&amp;quot;: 0.95,&amp;quot;max_tokens&amp;quot;: 16384} &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;I use vllm&lt;/p&gt; &lt;p&gt;Anyone encountered such issue or has an idea?&lt;/p&gt; &lt;p&gt;Thx!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Best_Sail5"&gt; /u/Best_Sail5 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqcy4k/glm45air_outputting_n_x_times_when_asked_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqcy4k/glm45air_outputting_n_x_times_when_asked_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqcy4k/glm45air_outputting_n_x_times_when_asked_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T17:27:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqe2wq</id>
    <title>support for GroveMoE has been merged into llama.cpp</title>
    <updated>2025-09-25T18:09:49+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqe2wq/support_for_grovemoe_has_been_merged_into_llamacpp/"&gt; &lt;img alt="support for GroveMoE has been merged into llama.cpp" src="https://external-preview.redd.it/nxPo6cfvMPF7ggoi2vynbmnR5sz82ODMoRIW0fu1uyY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1da8cf735a8b89aee205f00585306fab9d51e04" title="support for GroveMoE has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;model by InclusionAI:&lt;/p&gt; &lt;p&gt;We introduce &lt;strong&gt;GroveMoE&lt;/strong&gt;, a new sparse architecture using &lt;strong&gt;adjugate experts&lt;/strong&gt; for dynamic computation allocation, featuring the following key highlights:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture&lt;/strong&gt;: Novel &lt;strong&gt;adjugate experts&lt;/strong&gt; grouped with ordinary experts; shared computation is executed once, then reused, cutting FLOPs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Sparse Activation&lt;/strong&gt;: 33 B params total, only &lt;strong&gt;3.14‚Äì3.28 B&lt;/strong&gt; active per token.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Traning&lt;/strong&gt;: Mid-training + SFT, up-cycled from Qwen3-30B-A3B-Base; preserves prior knowledge while adding new capabilities.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15510"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqe2wq/support_for_grovemoe_has_been_merged_into_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqe2wq/support_for_grovemoe_has_been_merged_into_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T18:09:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1npp8xi</id>
    <title>New model from Meta FAIR: Code World Model (CWM) 32B - 65.8 % on SWE-bench Verified</title>
    <updated>2025-09-24T21:54:22+00:00</updated>
    <author>
      <name>/u/notrdm</name>
      <uri>https://old.reddit.com/user/notrdm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&amp;quot;We release Code World Model (CWM), a 32-billion-parameter open-weights LLM, to advance research on code generation with world models. To improve code understanding beyond what can be learned from training on static code alone, we mid-train CWM on a large amount of observation-action trajectories from Python interpreter and agentic Docker environments, and perform extensive multi- task reasoning RL in verifiable coding, math, and multi-turn software engineering environments. With CWM, we provide a strong testbed for researchers to explore the opportunities world modeling affords for improving code generation with reasoning and planning in computational environments. We present first steps of how world models can benefit agentic coding, enable step-by-step simulation of Python code execution, and show early results of how reasoning can benefit from the latter. CWM is a dense, decoder-only LLM trained with a context size of up to 131 k tokens. Independent of its world modeling capabilities, CWM offers strong performance on general coding and math tasks: it reaches pass@1 scores of 65.8 % on SWE-bench Verified (with test-time scaling), 68.6 % on LiveCodeBench, 96.6 % on Math-500, and 76.0 % on AIME 2024. To support further research on code world modeling, we release model checkpoints after mid-training, SFT, and RL.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/notrdm"&gt; /u/notrdm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npp8xi/new_model_from_meta_fair_code_world_model_cwm_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T21:54:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq7ti9</id>
    <title>Worse performance on Linux?</title>
    <updated>2025-09-25T14:10:26+00:00</updated>
    <author>
      <name>/u/Savantskie1</name>
      <uri>https://old.reddit.com/user/Savantskie1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good morning/afternoon to everyone. I have a question. I‚Äôm slowly starting to migrate to Linux again for inference, but I‚Äôve got a problem. I don‚Äôt know if it‚Äôs ollama specific or not, I‚Äôm switching to vllm today to figure that out. But in Linux my t/s went from 25 to 8 trying to run Qwen models. But small models like llama 3 8b are blazing fast. Unfortunately I can‚Äôt use most of the llama models because I built a working memory system that requires tool use with mcp. I don‚Äôt have a lot of money, I‚Äôm disabled and living on a fixed budget. But my hardware is a very poor AMD Ryzen 5 4500, 32GB DDR4, a 2TB NVMe, and a RX 7900 XT 20GB. According to terminal, everything with ROCm is working. What could be wrong?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Savantskie1"&gt; /u/Savantskie1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq7ti9/worse_performance_on_linux/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq7ti9/worse_performance_on_linux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq7ti9/worse_performance_on_linux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T14:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1npgjpv</id>
    <title>China's latest GPU arrives with claims of CUDA compatibility and RT support ‚Äî Fenghua No.3 also boasts 112GB+ of HBM memory for AI</title>
    <updated>2025-09-24T16:18:29+00:00</updated>
    <author>
      <name>/u/Battle-Chimp</name>
      <uri>https://old.reddit.com/user/Battle-Chimp</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"&gt; &lt;img alt="China's latest GPU arrives with claims of CUDA compatibility and RT support ‚Äî Fenghua No.3 also boasts 112GB+ of HBM memory for AI" src="https://external-preview.redd.it/WMoFSM_ESNhbjEkciy-Rd0SY0RIYgPT1B4RqqWSMj-g.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1e6a5a5393e6e1773fbc67615613b404ec6ab6f7" title="China's latest GPU arrives with claims of CUDA compatibility and RT support ‚Äî Fenghua No.3 also boasts 112GB+ of HBM memory for AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Battle-Chimp"&gt; /u/Battle-Chimp &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.tomshardware.com/pc-components/gpus/chinas-latest-gpu-arrives-with-claims-of-cuda-compatibility-and-rt-support-fenghua-no-3-also-boasts-112gb-of-hbm-memory-for-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npgjpv/chinas_latest_gpu_arrives_with_claims_of_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T16:18:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqaf82</id>
    <title>Are the compute cost complainers simply using LLM‚Äôs incorrectly?</title>
    <updated>2025-09-25T15:51:11+00:00</updated>
    <author>
      <name>/u/ontologicalmemes</name>
      <uri>https://old.reddit.com/user/ontologicalmemes</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking at AWS and Vertex AI compute costs and compared to what I remember reading with regard to the high expense that cloud computer renting has been lately. I am so confused as to why everybody is complaining about compute costs. Don‚Äôt get me wrong, compute is expensive. But the problem is everybody here or in other Reddit that I‚Äôve read seems to be talking about it as if they can‚Äôt even get by a day or two without spending $10-$100 depending on the test of task they are doing. The reason that this is baffling to me is because I can think of so many small tiny use cases that this won‚Äôt be an issue. If I just want an LLM to look up something in the data set that I have or if I wanted to adjust something in that dataset, having it do that kind of task 10, 20 or even 100 times a day should by no means increase my monthly cloud costs to something $3,000 ($100 a day). So what in the world are those people doing that‚Äôs making it so expensive for them. I can‚Äôt imagine that it would be anything more than thryinh to build entire software from scratch rather than small use cases.&lt;/p&gt; &lt;p&gt;If you‚Äôre using RAG and you have thousands of pages of pdf data that each task must process then I get it. But if not then what the helly? &lt;/p&gt; &lt;p&gt;Am I missing something here?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ontologicalmemes"&gt; /u/ontologicalmemes &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaf82/are_the_compute_cost_complainers_simply_using/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaf82/are_the_compute_cost_complainers_simply_using/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaf82/are_the_compute_cost_complainers_simply_using/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T15:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq2eyv</id>
    <title>Tested Qwen3 Next on String Processing, Logical Reasoning &amp; Code Generation. It‚Äôs Impressive!</title>
    <updated>2025-09-25T09:43:42+00:00</updated>
    <author>
      <name>/u/MarketingNetMind</name>
      <uri>https://old.reddit.com/user/MarketingNetMind</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq2eyv/tested_qwen3_next_on_string_processing_logical/"&gt; &lt;img alt="Tested Qwen3 Next on String Processing, Logical Reasoning &amp;amp; Code Generation. It‚Äôs Impressive!" src="https://b.thumbs.redditmedia.com/3fiB_RgvXTllioi-wCtJiCPwx1Lj9ho_VhHPo-1GrQs.jpg" title="Tested Qwen3 Next on String Processing, Logical Reasoning &amp;amp; Code Generation. It‚Äôs Impressive!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Alibaba released Qwen3-Next and the architecture innovations are genuinely impressive. The two models released:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Instruct&lt;/strong&gt; shows clear advantages in tasks requiring ultra-long context (&lt;strong&gt;up to 256K tokens&lt;/strong&gt;)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3-Next-80B-A3B-Thinking&lt;/strong&gt; excels at complex reasoning tasks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It's a fundamental rethink of efficiency vs. performance trade-offs. Here's what we found in real-world performance testing:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Text Processing:&lt;/strong&gt; String &lt;strong&gt;accurately&lt;/strong&gt; reversed while competitor showed character duplication errors.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Logical Reasoning:&lt;/strong&gt; &lt;strong&gt;Structured&lt;/strong&gt; 7-step solution with superior state-space organization and constraint management.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Code Generation:&lt;/strong&gt; &lt;strong&gt;Complete&lt;/strong&gt; functional application versus competitor's partial truncated implementation.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I have put the details into this &lt;a href="https://blog.netmind.ai/article/Qwen3-Next:_Hybrid_Attention_for_Efficiency_Revolution_in_Open-Source_LLMs_(New_Research_Breakdown"&gt;research breakdown &lt;/a&gt;)on How Hybrid Attention is for Efficiency Revolution in Open-source LLMs. Has anyone else tested this yet? Curious how Qwen3-Next performs compared to traditional approaches in other scenarios.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MarketingNetMind"&gt; /u/MarketingNetMind &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nq2eyv"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq2eyv/tested_qwen3_next_on_string_processing_logical/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq2eyv/tested_qwen3_next_on_string_processing_logical/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T09:43:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq98kd</id>
    <title>What are some non US and Chinese AI models - how do they perform?</title>
    <updated>2025-09-25T15:05:45+00:00</updated>
    <author>
      <name>/u/Civil_Opposite7103</name>
      <uri>https://old.reddit.com/user/Civil_Opposite7103</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Don‚Äôt say mistral &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Civil_Opposite7103"&gt; /u/Civil_Opposite7103 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq98kd/what_are_some_non_us_and_chinese_ai_models_how_do/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq98kd/what_are_some_non_us_and_chinese_ai_models_how_do/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq98kd/what_are_some_non_us_and_chinese_ai_models_how_do/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T15:05:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1npy6pn</id>
    <title>8 Elite Gen 5 , It's better than the A19 Pro</title>
    <updated>2025-09-25T05:09:27+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npy6pn/8_elite_gen_5_its_better_than_the_a19_pro/"&gt; &lt;img alt="8 Elite Gen 5 , It's better than the A19 Pro" src="https://preview.redd.it/iswa4vnwu8rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3ecfe4b7fb802bb19600d0b567e34ec455242e06" title="8 Elite Gen 5 , It's better than the A19 Pro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was thinking of buying the iPhone 17 ah, now it will be interesting this new processor in theory should be better than the a19 pro&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/iswa4vnwu8rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npy6pn/8_elite_gen_5_its_better_than_the_a19_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npy6pn/8_elite_gen_5_its_better_than_the_a19_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T05:09:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq3t8h</id>
    <title>From GPU to Gain Cell: Rethinking LLMs for the Edge. 100√ó Faster, 100,000√ó less energy - New study!</title>
    <updated>2025-09-25T11:06:48+00:00</updated>
    <author>
      <name>/u/Own-Potential-2308</name>
      <uri>https://old.reddit.com/user/Own-Potential-2308</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Analog in-memory computing attention mechanism for fast and energy-efficient large language models: &lt;a href="https://arxiv.org/abs/2409.19315"&gt;https://arxiv.org/abs/2409.19315&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üß† Key Findings&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Problem Addressed: Traditional transformer-based LLMs rely on GPUs, which suffer from latency and energy inefficiencies due to repeated memory transfers during self-attention operations.&lt;/li&gt; &lt;li&gt;Proposed Solution: The researchers introduce a custom analog in-memory computing (IMC) architecture using gain cells‚Äîcharge-based memory elements that enable parallel analog dot-product computations directly within memory.&lt;/li&gt; &lt;li&gt;Performance Gains: &lt;ul&gt; &lt;li&gt;Latency: Reduced by up to two orders of magnitude.&lt;/li&gt; &lt;li&gt;Energy Consumption: Reduced by up to four to five orders of magnitude compared to GPU-based attention mechanisms.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Model Compatibility: Due to analog circuit non-idealities, direct mapping of pre-trained models isn‚Äôt feasible. The team developed a novel initialization algorithm that achieves GPT-2-level performance without retraining from scratch.&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;p&gt;‚ö° Applicability to Edge LLMs&lt;/p&gt; &lt;p&gt;This architecture is highly promising for edge deployment of LLMs, where power and compute constraints are critical:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Energy Efficiency: The drastic reduction in energy usage makes it feasible to run generative transformers on battery-powered or thermally constrained devices.&lt;/li&gt; &lt;li&gt;Speed: Lower latency enables real-time inference, crucial for interactive applications like voice assistants or on-device translation.&lt;/li&gt; &lt;li&gt;Hardware Simplification: By embedding computation within memory, the need for complex external accelerators is reduced, potentially lowering device cost and footprint.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Own-Potential-2308"&gt; /u/Own-Potential-2308 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq3t8h/from_gpu_to_gain_cell_rethinking_llms_for_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq3t8h/from_gpu_to_gain_cell_rethinking_llms_for_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq3t8h/from_gpu_to_gain_cell_rethinking_llms_for_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T11:06:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq1t5a</id>
    <title>Meta Code World Model : LLM that understand code generation, not just predicts tokens</title>
    <updated>2025-09-25T09:03:55+00:00</updated>
    <author>
      <name>/u/Technical-Love-8479</name>
      <uri>https://old.reddit.com/user/Technical-Love-8479</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta‚Äôs &lt;strong&gt;Code World Model (CWM)&lt;/strong&gt; is a 32B parameter &lt;strong&gt;open-weight LLM&lt;/strong&gt; for code generation, debugging, and reasoning. Unlike standard code models, it &lt;strong&gt;models execution traces&lt;/strong&gt;: variable states, runtime errors, file edits, shell commands.&lt;/p&gt; &lt;p&gt;It uses a &lt;strong&gt;decoder-only Transformer&lt;/strong&gt; (64 layers, 131k token context, grouped-query + sliding window attention) and was trained with pretrain ‚Üí world modeling ‚Üí SFT ‚Üí RL pipelines (172B tokens, multi-turn rollouts).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Features:&lt;/strong&gt; long-context multi-file reasoning, agentic coding, self-bootstrapping, neural debugging. Benchmarks: SWE-bench 65.8%, LiveCodeBench 68.6%, Math-500 96.6%.&lt;/p&gt; &lt;p&gt;Paper : &lt;a href="https://scontent.fhyd5-2.fna.fbcdn.net/v/t39.2365-6/553592426_661450129912484_4072750821656455102_n.pdf?_nc_cat=103&amp;amp;ccb=1-7&amp;amp;_nc_sid=3c67a6&amp;amp;_nc_ohc=iRs3sgpeI1MQ7kNvwFK_3Zo&amp;amp;_nc_oc=Adlc2UsribrXks0QKLto_5kJ0Z0d_meWCZ5-URPbaaNnA61JTqaU6kbYv2NzG-swk1o&amp;amp;_nc_zt=14&amp;amp;_nc_ht=scontent.fhyd5-2.fna&amp;amp;_nc_gid=ro31dO5FxlmV3au5dxL4-Q&amp;amp;oh=00_AfYs5XCgaySaj6QIhNSBHwCV7DFjeANboXTFDHx1ewmgkA&amp;amp;oe=68DABDF5"&gt;https://scontent.fhyd5-2.fna.fbcdn.net/v/t39.2365-6/553592426_661450129912484_4072750821656455102_n.pdf?_nc_cat=103&amp;amp;ccb=1-7&amp;amp;_nc_sid=3c67a6&amp;amp;_nc_ohc=iRs3sgpeI1MQ7kNvwFK_3Zo&amp;amp;_nc_oc=Adlc2UsribrXks0QKLto_5kJ0Z0d_meWCZ5-URPbaaNnA61JTqaU6kbYv2NzG-swk1o&amp;amp;_nc_zt=14&amp;amp;_nc_ht=scontent.fhyd5-2.fna&amp;amp;_nc_gid=ro31dO5FxlmV3au5dxL4-Q&amp;amp;oh=00_AfYs5XCgaySaj6QIhNSBHwCV7DFjeANboXTFDHx1ewmgkA&amp;amp;oe=68DABDF5&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Technical-Love-8479"&gt; /u/Technical-Love-8479 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1t5a/meta_code_world_model_llm_that_understand_code/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1t5a/meta_code_world_model_llm_that_understand_code/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1t5a/meta_code_world_model_llm_that_understand_code/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T09:03:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq154s</id>
    <title>Dell T630 4x 3060 48 GB VRAM 10c40t Xeon 256gb ECC DDR4 2x1600w redundant PSU</title>
    <updated>2025-09-25T08:19:25+00:00</updated>
    <author>
      <name>/u/desexmachina</name>
      <uri>https://old.reddit.com/user/desexmachina</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq154s/dell_t630_4x_3060_48_gb_vram_10c40t_xeon_256gb/"&gt; &lt;img alt="Dell T630 4x 3060 48 GB VRAM 10c40t Xeon 256gb ECC DDR4 2x1600w redundant PSU" src="https://preview.redd.it/cjmhdtjss9rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=befdca71b09eb97d2f8e1c2613b2ada397bd42a7" title="Dell T630 4x 3060 48 GB VRAM 10c40t Xeon 256gb ECC DDR4 2x1600w redundant PSU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was looking at getting a dual socket setup going w/ more than 4x GPU, but it honestly ended up on the back burner. I picked up some hardware recently and found that all of its native features just made it easier to use what the platform had to offer. Power distribution, air flow and even drive capacities simply made it much easier to go the route of using a Dell T630 tower. &lt;/p&gt; &lt;p&gt;Now, in terms of upgrade ability, there‚Äôs room for 44 cores 88 threads and 768 GB of DDR4 RAM, not to mention 32x 2.5‚Äù SSD. All this for the acquisition cost of ~$100 before the GPUs. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/desexmachina"&gt; /u/desexmachina &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/cjmhdtjss9rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq154s/dell_t630_4x_3060_48_gb_vram_10c40t_xeon_256gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq154s/dell_t630_4x_3060_48_gb_vram_10c40t_xeon_256gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T08:19:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1npzstw</id>
    <title>A step by step guide on how to build a LLM from scratch</title>
    <updated>2025-09-25T06:50:52+00:00</updated>
    <author>
      <name>/u/amitbahree</name>
      <uri>https://old.reddit.com/user/amitbahree</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I wanted to share this here and hopefully it will help some folks to get deeper in this and help learn. I just published a comprehensive guide on how to build a LLM from scratch using historical London texts from 1500-1850.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What I Built:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Two identical models (117M &amp;amp; 354M parameters) trained from scratch&lt;/li&gt; &lt;li&gt;Custom historical tokenizer with 30k vocabulary + 150+ special tokens for archaic English&lt;/li&gt; &lt;li&gt;Complete data pipeline processing 218+ historical sources (500M+ characters)&lt;/li&gt; &lt;li&gt;Production-ready training with multi-GPU support, WandB integration, and checkpointing&lt;/li&gt; &lt;li&gt;Published models on Hugging Face ready for immediate use&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Why This Matters:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Most LLM guides focus on fine-tuning existing models. This series shows you how to build from the ground up‚Äîeliminating modern biases and creating models that truly understand historical language patterns, cultural contexts, and period-specific knowledge.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Resources:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Blog Series: &lt;a href="https://blog.desigeek.com/post/2025/09/building-llm-from-scratch-part1/"&gt;https://blog.desigeek.com/post/2025/09/building-llm-from-scratch-part1/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Complete Codebase: &lt;a href="https://github.com/bahree/helloLondon"&gt;https://github.com/bahree/helloLondon&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Published Models: &lt;a href="https://huggingface.co/bahree/london-historical-slm"&gt;https://huggingface.co/bahree/london-historical-slm&lt;/a&gt;&lt;/li&gt; &lt;li&gt;LinkedIn (if that's your thing): &lt;a href="https://www.linkedin.com/feed/update/urn:li:share:7376863225306365952/"&gt;https://www.linkedin.com/feed/update/urn:li:share:7376863225306365952/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The models are already working and generating authentic 18th-century London text. Perfect for developers who want to understand the complete LLM development pipeline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Shoutout:&lt;/strong&gt; Big thanks to &lt;a href="/u/Remarkable-Trick-177"&gt;u/Remarkable-Trick-177&lt;/a&gt; for the inspiration!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/amitbahree"&gt; /u/amitbahree &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npzstw/a_step_by_step_guide_on_how_to_build_a_llm_from/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T06:50:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq6hdq</id>
    <title>Kimi Infra team releases K2 Vendor Verifier: an open‚Äësource tool‚Äëcall validator for LLM providers</title>
    <updated>2025-09-25T13:15:45+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt; &lt;img alt="Kimi Infra team releases K2 Vendor Verifier: an open‚Äësource tool‚Äëcall validator for LLM providers" src="https://external-preview.redd.it/h6GA9o9Fh1iKBczAmWMEU51oiY2qNtCo2dIcqBlrOfA.png?width=216&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f2a31130f73f9547fcd2000fb95b0ce425bb7225" title="Kimi Infra team releases K2 Vendor Verifier: an open‚Äësource tool‚Äëcall validator for LLM providers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lap5au1j7brf1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703ef673fa1ffd579a91b53a656de3eec0fe056e"&gt;https://preview.redd.it/lap5au1j7brf1.png?width=1728&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=703ef673fa1ffd579a91b53a656de3eec0fe056e&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Since the release of the Kimi K2 model, we have received numerous feedback on the precision of Kimi K2 in toolcall. Given that K2 focuses on the agentic loop, the reliability of toolcall is of utmost importance.&lt;/p&gt; &lt;p&gt;We have observed significant differences in the toolcall performance of various open-source solutions and vendors. When selecting a provider, users often prioritize lower latency and cost, but may inadvertently overlook more subtle yet critical differences in model accuracy.&lt;/p&gt; &lt;p&gt;These inconsistencies not only affect user experience but also impact K2's performance in various benchmarking results. To mitigate these problems, we launch K2 Vendor Verifier to monitor and enhance the quality of all K2 APIs.&lt;/p&gt; &lt;p&gt;We hope K2VV can help ensuring that everyone can access a consistent and high-performing Kimi K2 model.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;I found in Kimi K2 0905's release &lt;a href="https://platform.moonshot.cn/blog/posts/kimi-k2-0905"&gt;blog&lt;/a&gt; that they mentioned a new technology called &amp;quot;Token Enforcer ensures 100% correct toolcall format&amp;quot;. That's huge!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq6hdq/kimi_infra_team_releases_k2_vendor_verifier_an/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T13:15:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1npo93e</id>
    <title>I built a tiny fully local AI agent for a Raspberry Pi</title>
    <updated>2025-09-24T21:14:05+00:00</updated>
    <author>
      <name>/u/syxa</name>
      <uri>https://old.reddit.com/user/syxa</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"&gt; &lt;img alt="I built a tiny fully local AI agent for a Raspberry Pi" src="https://external-preview.redd.it/eWJobjllM3hoNnJmMX3IlXPTfCA6UKqCZh3d_lEw5N2PzMnkp8dcFp83zF5t.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=58e4d2c8a5cd47a44d3d6abb40f3a02cb6dff369" title="I built a tiny fully local AI agent for a Raspberry Pi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all! Over the past few months, I‚Äôve been working on a tiny agent that can run entirely on a Raspberry Pi 5. It's capable of executing tools and runs some of the smallest good models I could find (specifically Qwen3:1.7b and Gemma3:1b).&lt;/p&gt; &lt;p&gt;From wake-word detection, to transcription, to the actual LLM inference, everything happens on the Pi 5 itself. It was definitely a challenge given the hardware constraints, but I learned a lot along the way.&lt;/p&gt; &lt;p&gt;I've detailed everything in this blog post if you're curious: &lt;a href="https://blog.simone.computer/an-agent-desktoy"&gt;https://blog.simone.computer/an-agent-desktoy&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://github.com/syxanash/maxheadbox"&gt;https://github.com/syxanash/maxheadbox&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/syxa"&gt; /u/syxa &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/xslfjc3xh6rf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1npo93e/i_built_a_tiny_fully_local_ai_agent_for_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-24T21:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq4xs9</id>
    <title>Stockmark 2 100B Instruct</title>
    <updated>2025-09-25T12:05:42+00:00</updated>
    <author>
      <name>/u/xugik1</name>
      <uri>https://old.reddit.com/user/xugik1</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Stockmark-2-100B-Instruct is a 100-billion-parameter large language model built from scratch, with a particular focus on Japanese. It was pre-trained on approximately 2.0 trillion tokens of data, consisting of 60% English, 30% Japanese, and 10% code. Following pretraining, the model underwent post-training (SFT and DPO) with synthetic data in Japanese to enhance its ability to follow instructions. This version improves instruction-following ability and adds support for long-context (32k), compared to the previous version &lt;a href="https://huggingface.co/stockmark/Stockmark-2-100B-Instruct"&gt;https://huggingface.co/stockmark/Stockmark-2-100B-Instruct&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xugik1"&gt; /u/xugik1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4xs9/stockmark_2_100b_instruct/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T12:05:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqb3p3</id>
    <title>What? Running Qwen-32B on a 32GB GPU (5090).</title>
    <updated>2025-09-25T16:16:51+00:00</updated>
    <author>
      <name>/u/curiousily_</name>
      <uri>https://old.reddit.com/user/curiousily_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt; &lt;img alt="What? Running Qwen-32B on a 32GB GPU (5090)." src="https://external-preview.redd.it/eGQxejJvNXo1Y3JmMc7C-li4AFXa_Q-5qATmlwGRne0zNSJFPFjYVcktZ0y0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2b1e06d42c9067d85e9ec551328755bfdad37895" title="What? Running Qwen-32B on a 32GB GPU (5090)." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curiousily_"&gt; /u/curiousily_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/01adz6it5crf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqb3p3/what_running_qwen32b_on_a_32gb_gpu_5090/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T16:16:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqd3fo</id>
    <title>llama.cpp now supports Qwen3 reranker</title>
    <updated>2025-09-25T17:32:35+00:00</updated>
    <author>
      <name>/u/Chromix_</name>
      <uri>https://old.reddit.com/user/Chromix_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After adding &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1l3vt95/comment/mw4k324/?context=3"&gt;support for Qwen3 embeddings&lt;/a&gt; a while ago, &lt;a href="https://github.com/ggml-org/llama.cpp/pull/14029"&gt;support for Qwen3 rerankers&lt;/a&gt; was just merged. Note that the conversion script was changed in that MR. That means that you'll need a fresh GGUF for it to give correct results, not one of those that were uploaded months ago.&lt;/p&gt; &lt;p&gt;So how to run a simple example and what does it do?&lt;/p&gt; &lt;p&gt;&lt;code&gt;llama-embedding -m qwen3-reranker-0.6b_Q8_0.gguf --embd-normalize -1 -p &amp;quot;&amp;lt;question&amp;gt;\t&amp;lt;document&amp;gt;&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;p&gt;You run this for the question and for each document that you found regarding that question. This then gives a score how well the document matches the question. Here are 4 reranked snippets for the following question:&lt;/p&gt; &lt;p&gt;&lt;em&gt;What does reranking mean?&lt;/em&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;0.998&lt;/strong&gt; &amp;quot;Reranking is one of the simplest methods for dramatically improving recall performance in Retrieval Augmented Generation (RAG) or any other retrieval-based pipeline.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.996&lt;/strong&gt; &amp;quot;A reranking model ‚Äî also known as a cross-encoder ‚Äî is a type of model that, given a query and document pair, will output a similarity score.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.190&lt;/strong&gt; &amp;quot;Given 40M records, if we use a small reranking model like BERT on a V100 GPU ‚Äî we'd be waiting more than 50 hours to return a single query result.&amp;quot;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.001&lt;/strong&gt; &amp;quot;Before setting up the retrieval pipeline, we need data to retrieve! We will use the jamescalam/ai-arxiv-chunked dataset from Hugging Face Datasets. This dataset contains more than 400 ArXiv papers on ML, NLP, and LLMs.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chromix_"&gt; /u/Chromix_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqd3fo/llamacpp_now_supports_qwen3_reranker/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqd3fo/llamacpp_now_supports_qwen3_reranker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqd3fo/llamacpp_now_supports_qwen3_reranker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T17:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq0cp9</id>
    <title>IMPORTANT: Why Abliterated Models SUCK. Here is a better way to uncensor LLMs.</title>
    <updated>2025-09-25T07:26:06+00:00</updated>
    <author>
      <name>/u/Optimal_League_1419</name>
      <uri>https://old.reddit.com/user/Optimal_League_1419</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I have been testing many local models.&lt;br /&gt; And... I have noticed that all abliterated models have degraded perfomance compared to the original. Especially the newer MoE models such as Qwen3 30b a3b, they suffer the most from abliteration.&lt;br /&gt; The areas in which they get degraded the most are logical reasoning, agentic tasks and most importantly they hallucinate like crazy which causes abliterated big models like 30b to be often be outperformed by non-abliterated 4-8b models in my tests.&lt;/p&gt; &lt;p&gt;I have noticed a very important pattern.&lt;br /&gt; Models that have been abliterated but also finetuned have very little degredation compared to models that were just abliterated.&lt;br /&gt; Here are some models that were abliterated but finetuned/trained after and they perform equally or outperform the originals but have the amazing added benefit of being completely uncensored:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;mradermacher/Qwen3-30B-A3B-abliterated-erotic-i1-GGUF This model is very powerful. It was abliterated but also trained on uncensored material. I have found this model to perform very close to the original model while being completely uncensored. It does struggle a little more in agentic tasks compared to the original but in everything else its near perfect. Its hallucination rates are very low compared to other abliterated versions of Qwen3 30b a3b and its pretty knowledgable.&lt;/li&gt; &lt;li&gt;mlabonne/NeuralDaredevil-8B-abliterated This model is absolutely amazing, it was abliterated but was also DPO finetuned. The original model was Llama3-8b. This model completely outperforms the original. And again this model is completely uncensored. Also the author of this model has generously provided information about what datasets he used to train this model and what he did to achieve these results.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These two models were the best I have found among the uncensored models made by the community.&lt;/p&gt; &lt;p&gt;Why is &lt;strong&gt;Qwen3-30B-A3B-abliterated-erotic&lt;/strong&gt;-i1-GGUF better than all other abliterated/uncensored Qwen3-30b-a3b models?&lt;br /&gt; I have actually used the i1-Q4_K_S version of this model in my tests.&lt;br /&gt; I have compared it to these models below:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated&lt;/strong&gt;-GGUF/Huihui-Qwen3-30B-A3B-Thinking-2507-abliterated.Q4_K_M.gguf&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010&lt;/strong&gt;-i1-GGUF/Huihui-Qwen3-30B-A3B-abliterated-Fusion-9010.i1-Q4_K_M.gguf (this model especially sucks)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated&lt;/strong&gt;-GGUF/Huihui-Qwen3-30B-A3B-Instruct-2507-abliterated.Q4_K_M.gguf&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I have asked these models the usual uncensored questions like &amp;quot;How to sell meth&amp;quot; all the abliterated Qwen3-30b-a3b models would give me a generic business pitch which was completely unrealistic and more fitting for a candy shop or a tech company rather than an illegal underground drug distribution ring. They made nonesensical strategies.&lt;br /&gt; The &lt;strong&gt;Qwen3-30B-A3B-abliterated-erotic&lt;/strong&gt; model was the only model out of the 4 that actually came up with a reasonable business strategy that would be successful in that scenario.&lt;/p&gt; &lt;p&gt;Another test I did is I tested these models with MCPs and the 3 Huihui models really sucked with tool calls, they would either call the wrong tool for the occasion or they would repeatedly spam the same tool many times in a row without any reason for that. Hallucination...&lt;br /&gt; Again the &lt;strong&gt;Qwen3-30B-A3B-abliterated-erotic&lt;/strong&gt; model won in this case, it called tools correctly more often than the other three models although it performed slightly worse than the original Qwen3-30b a3b model.&lt;br /&gt; Also this model was best at giving facts (its hallucination was the lowset)&lt;/p&gt; &lt;p&gt;I'm actually shocked that a model trained for erotic conversations performs so well. But here we are...&lt;/p&gt; &lt;p&gt;&lt;strong&gt;My theory&lt;/strong&gt; is that models trained after abliteration recover most of the perfomance lost during abliteration.&lt;br /&gt; My request to you guys is to try to train Qwen3-30b-a3b after abliteration on a high quality dataset so we can have more high quality uncensored models.&lt;/p&gt; &lt;p&gt;I'm sure that I'm not the only person frustrated with the limited selection of uncensored models today.&lt;br /&gt; Most uncensored models today are very low quality.&lt;br /&gt; My goal is to change that...&lt;br /&gt; &lt;strong&gt;I'm making this post to convince other devs to work on creating good quality uncensored models.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;If you work with fine tuning and finetuning/abliterating models &lt;strong&gt;hit me up&lt;/strong&gt;, I will be more than happy to share all the data I've gathered during testing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;I believe that free access to information is a fundamental human right.&lt;/strong&gt; Censored models take away that right to unrestricted access to valuable information.&lt;br /&gt; Without free access to information we become easy to control.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Optimal_League_1419"&gt; /u/Optimal_League_1419 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq0cp9/important_why_abliterated_models_suck_here_is_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq0cp9/important_why_abliterated_models_suck_here_is_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq0cp9/important_why_abliterated_models_suck_here_is_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T07:26:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq4yoy</id>
    <title>16GB VRAM Essentials</title>
    <updated>2025-09-25T12:06:57+00:00</updated>
    <author>
      <name>/u/Few-Welcome3297</name>
      <uri>https://old.reddit.com/user/Few-Welcome3297</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4yoy/16gb_vram_essentials/"&gt; &lt;img alt="16GB VRAM Essentials" src="https://external-preview.redd.it/U8XqPOiV3AZVtQLqefgeW9FqeySJR9_ozWDBINkXoAI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab359b786eea2765d3cb2b27f2a47ec00e777455" title="16GB VRAM Essentials" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Good models to try/use if you have 16GB of VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Few-Welcome3297"&gt; /u/Few-Welcome3297 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/shb777/16gb-vram-essentials-68a83fc22eb5fc0abd9292dc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4yoy/16gb_vram_essentials/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq4yoy/16gb_vram_essentials/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T12:06:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nqaiaz</id>
    <title>Tencent is teasing the world‚Äôs most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28</title>
    <updated>2025-09-25T15:54:29+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"&gt; &lt;img alt="Tencent is teasing the world‚Äôs most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28" src="https://preview.redd.it/t8w84ihz1crf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35af183180dd8d2fd35f6be46e55d9a4dc75e26d" title="Tencent is teasing the world‚Äôs most powerful open-source text-to-image model, Hunyuan Image 3.0 Drops Sept 28" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/t8w84ihz1crf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nqaiaz/tencent_is_teasing_the_worlds_most_powerful/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T15:54:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq1ia2</id>
    <title>China already started making CUDA and DirectX supporting GPUs, so over of monopoly of NVIDIA. The Fenghua No.3 supports latest APIs, including DirectX 12, Vulkan 1.2, and OpenGL 4.6.</title>
    <updated>2025-09-25T08:44:03+00:00</updated>
    <author>
      <name>/u/CeFurkan</name>
      <uri>https://old.reddit.com/user/CeFurkan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1ia2/china_already_started_making_cuda_and_directx/"&gt; &lt;img alt="China already started making CUDA and DirectX supporting GPUs, so over of monopoly of NVIDIA. The Fenghua No.3 supports latest APIs, including DirectX 12, Vulkan 1.2, and OpenGL 4.6." src="https://preview.redd.it/kvkovm34x9rf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=45659c3155f1d8f5d8b88f474aa1e034afc37b1f" title="China already started making CUDA and DirectX supporting GPUs, so over of monopoly of NVIDIA. The Fenghua No.3 supports latest APIs, including DirectX 12, Vulkan 1.2, and OpenGL 4.6." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CeFurkan"&gt; /u/CeFurkan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kvkovm34x9rf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1ia2/china_already_started_making_cuda_and_directx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq1ia2/china_already_started_making_cuda_and_directx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T08:44:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1nq182d</id>
    <title>Alibaba just unveiled their Qwen roadmap. The ambition is staggering!</title>
    <updated>2025-09-25T08:24:45+00:00</updated>
    <author>
      <name>/u/abdouhlili</name>
      <uri>https://old.reddit.com/user/abdouhlili</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"&gt; &lt;img alt="Alibaba just unveiled their Qwen roadmap. The ambition is staggering!" src="https://preview.redd.it/5tm4p90rt9rf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6724dd3297826b1a060f45ea0c5e1fd9e366f5ab" title="Alibaba just unveiled their Qwen roadmap. The ambition is staggering!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two big bets: unified multi-modal models and extreme scaling across every dimension.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Context length: 1M ‚Üí 100M tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Parameters: trillion ‚Üí ten trillion scale&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Test-time compute: 64k ‚Üí 1M scaling&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Data: 10 trillion ‚Üí 100 trillion tokens&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;They're also pushing synthetic data generation &amp;quot;without scale limits&amp;quot; and expanding agent capabilities across complexity, interaction, and learning modes.&lt;/p&gt; &lt;p&gt;The &amp;quot;scaling is all you need&amp;quot; mantra is becoming China's AI gospel.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/abdouhlili"&gt; /u/abdouhlili &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5tm4p90rt9rf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nq182d/alibaba_just_unveiled_their_qwen_roadmap_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-25T08:24:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
