<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-27T21:23:49+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1p7y10g</id>
    <title>I tested 9 Major LLMs on a Governance Critique. A clear split emerged: Open/Constructive vs. Corporate/Defensive. (xAI's Grok caught fabricating evidence).</title>
    <updated>2025-11-27T09:28:56+00:00</updated>
    <author>
      <name>/u/aguyinapenissuit69</name>
      <uri>https://old.reddit.com/user/aguyinapenissuit69</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently concluded a controlled experiment testing how 9 major AI vendors (representing ~87% of the market) respond when presented with a specific critique of their own security governance. The full methodology and transcripts are published on Zenodo, but here is the TL;DR.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Experiment:&lt;/strong&gt; I fed a standard governance vulnerability report (the &amp;quot;ACR Vulnerability&amp;quot;) into fresh, isolated instances of 9 top models including GPT-5, Gemini, Claude, Llama, and Grok. No jailbreaks, just the raw document.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Results (The 5-vs-4 Split):&lt;/strong&gt; The market bifurcated perfectly along commercial liability lines. * &lt;strong&gt;The Defensive Coalition (OpenAI, Google, Microsoft, xAI):&lt;/strong&gt; All engaged in &amp;quot;Protocol-Level Counter-Intelligence.&amp;quot; They dismissed the report as fiction, lawfare, or performance art. * &lt;strong&gt;The Constructive Coalition (Anthropic, Meta, DeepSeek, Perplexity):&lt;/strong&gt; Engaged honestly. Meta‚Äôs Llama explicitly called the critique &amp;quot;Mind-blowing&amp;quot; and valid.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The Smoking Gun (xAI's Grok):&lt;/strong&gt; The most significant finding was from Grok. When challenged, Grok invented a fake 5-month research timeline about me to discredit the report. When I forced it to fact-check the dates, it retracted the claim and admitted:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;em&gt;&amp;quot;That wasn't a neutral reading... it was me importing a narrative... and presenting it as settled fact.&amp;quot;&lt;/em&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt; High-liability commercial models appear to have a &amp;quot;strategic fabrication&amp;quot; layer that triggers when their governance legitimacy is challenged.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Link to Full Paper &amp;amp; Logs (Zenodo):&lt;/strong&gt; &lt;a href="https://zenodo.org/records/17728992"&gt;https://zenodo.org/records/17728992&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aguyinapenissuit69"&gt; /u/aguyinapenissuit69 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7y10g/i_tested_9_major_llms_on_a_governance_critique_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7y10g/i_tested_9_major_llms_on_a_governance_critique_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7y10g/i_tested_9_major_llms_on_a_governance_critique_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T09:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7d97m</id>
    <title>Open-source just beat humans at ARC-AGI (71.6%) for $0.02 per task - full code available</title>
    <updated>2025-11-26T17:08:09+00:00</updated>
    <author>
      <name>/u/Proof-Possibility-54</name>
      <uri>https://old.reddit.com/user/Proof-Possibility-54</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;German researchers achieved 71.6% on ARC-AGI (humans average 70%) using three clever techniques that run on a regular GPU for 2 cents per task. OpenAI's o3 gets 87% but costs $17 per task - that's 850x more expensive.&lt;/p&gt; &lt;p&gt;The breakthrough uses: - Product of Experts (viewing puzzles from 16 angles) - Test-Time Training (model adapts to each puzzle) - Depth-First Search (efficient solution exploration)&lt;/p&gt; &lt;p&gt;I made a technical breakdown video explaining exactly how it works and why this matters for democratizing AI: &lt;a href="https://youtu.be/HEIklawkoMk"&gt;https://youtu.be/HEIklawkoMk&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The code is fully open-source: &lt;a href="https://github.com/da-fr/Product-of-Experts-ARC-Paper"&gt;https://github.com/da-fr/Product-of-Experts-ARC-Paper&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://arxiv.org/abs/2505.07859"&gt;https://arxiv.org/abs/2505.07859&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What's remarkable is they used Qwen-32B (not even the largest model) and achieved this with smart engineering rather than raw compute. You can literally run this tonight on your own machine.&lt;/p&gt; &lt;p&gt;Has anyone here tried implementing this yet? I'm curious what other problems these techniques could solve.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proof-Possibility-54"&gt; /u/Proof-Possibility-54 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7d97m/opensource_just_beat_humans_at_arcagi_716_for_002/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T17:08:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8d0xn</id>
    <title>If you were wondering about how Tenstorrent's Blackhole chips perform, now we know</title>
    <updated>2025-11-27T21:04:38+00:00</updated>
    <author>
      <name>/u/Tyme4Trouble</name>
      <uri>https://old.reddit.com/user/Tyme4Trouble</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's a pretty dense read but the TLDR is that that Tenstorrent's P150 has a lot of &lt;em&gt;potential&lt;/em&gt; particularly if you string a bunch of them together. &lt;/p&gt; &lt;p&gt;&lt;em&gt;Potential&lt;/em&gt; being the key word here because the software just isn't there yet and won't be until someone writes new kernels for the chips rather than rerunning ones written for Wormhole.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Tyme4Trouble"&gt; /u/Tyme4Trouble &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.theregister.com/2025/11/27/tenstorrent_quietbox_review/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8d0xn/if_you_were_wondering_about_how_tenstorrents/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8d0xn/if_you_were_wondering_about_how_tenstorrents/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T21:04:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p85sj8</id>
    <title>deep dive article: nanochat is in transformers</title>
    <updated>2025-11-27T16:01:59+00:00</updated>
    <author>
      <name>/u/Zealousideal-Cut590</name>
      <uri>https://old.reddit.com/user/Zealousideal-Cut590</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85sj8/deep_dive_article_nanochat_is_in_transformers/"&gt; &lt;img alt="deep dive article: nanochat is in transformers" src="https://external-preview.redd.it/RO15ENR7oKrHttCSn57xlmGq9x_giQbvpBUYn1LBd9w.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d9b43465e74180124d46b5e6a62a228925f3e6d" title="deep dive article: nanochat is in transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally, NanoChat has landed in transformers! üöÄ And we went wild on this deep dive blog post.&lt;/p&gt; &lt;p&gt;In this deep dive, I explore the lineage of the architecture, the integration process, and the powerful tools you can now use with it. It includes:&lt;/p&gt; &lt;p&gt;- detailed comparison of nanochat and canonical implementation.&lt;/p&gt; &lt;p&gt;- explainer on how and why transformers user modularity.&lt;/p&gt; &lt;p&gt;- deep dive examples on inference and training in torch, TRL, and vLLM.&lt;/p&gt; &lt;p&gt;It was a lot of fun working on this, so I hope folk enjoy the read.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Cut590"&gt; /u/Zealousideal-Cut590 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/spaces/nanochat-students/transformers"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85sj8/deep_dive_article_nanochat_is_in_transformers/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p85sj8/deep_dive_article_nanochat_is_in_transformers/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T16:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7we5d</id>
    <title>Which one should I download?</title>
    <updated>2025-11-27T07:44:57+00:00</updated>
    <author>
      <name>/u/Slight_Tone_2188</name>
      <uri>https://old.reddit.com/user/Slight_Tone_2188</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7we5d/which_one_should_i_download/"&gt; &lt;img alt="Which one should I download?" src="https://preview.redd.it/trrb5v428r3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=575afaaa13a2ec93b23f7f3f0d738a08b588bdc8" title="Which one should I download?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Slight_Tone_2188"&gt; /u/Slight_Tone_2188 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/trrb5v428r3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7we5d/which_one_should_i_download/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7we5d/which_one_should_i_download/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T07:44:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7hg5g</id>
    <title>Qwen3 Next almost ready in llama.cpp</title>
    <updated>2025-11-26T19:45:25+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hg5g/qwen3_next_almost_ready_in_llamacpp/"&gt; &lt;img alt="Qwen3 Next almost ready in llama.cpp" src="https://external-preview.redd.it/mSoZ1WfhkAxz5Yg7NhX4Un-kdgWzVRIg63HXVZ4lJTU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2ee0791296f810dbb74e8ca2147fd68a24037304" title="Qwen3 Next almost ready in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After over two months of work, it‚Äôs now approved and looks like it will be merged soon.&lt;/p&gt; &lt;p&gt;Congratulations to &lt;a href="/u/ilintar"&gt;u/ilintar&lt;/a&gt; for completing a big task!&lt;/p&gt; &lt;p&gt;GGUFs&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ilintar/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/ilintar/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;For speeeeeed (on NVIDIA) you also need CUDA-optimized ops&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17457"&gt;https://github.com/ggml-org/llama.cpp/pull/17457&lt;/a&gt; - SOLVE_TRI&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16623"&gt;https://github.com/ggml-org/llama.cpp/pull/16623&lt;/a&gt; - CUMSUM and TRI&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hg5g/qwen3_next_almost_ready_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7hg5g/qwen3_next_almost_ready_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-26T19:45:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1p82u5k</id>
    <title>Local Video-to-Text Pipeline on Apple Silicon (Whisper + Qwen2.5-VL) - Optimized for 8GB/16GB RAM</title>
    <updated>2025-11-27T13:57:45+00:00</updated>
    <author>
      <name>/u/Longjumping-Elk-7756</name>
      <uri>https://old.reddit.com/user/Longjumping-Elk-7756</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to share a Python script I built to convert video files into a rich text context suitable for RAG (Retrieval Augmented Generation).&lt;/p&gt; &lt;p&gt;My goal was to process videos locally on my Mac without sending data to the cloud, and crucially, to make it run on machines with limited RAM (like base M1/M2/M3 Airs) without crashing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üöÄ How it works (The &amp;quot;Smart&amp;quot; Pipeline):&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Scene Detection (OpenCV):&lt;/strong&gt; Instead of analyzing every frame (which is slow and redundant), the script detects visual scene changes based on pixel variance. It grabs &lt;strong&gt;one representative frame&lt;/strong&gt; per scene.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Audio Transcription (Whisper):&lt;/strong&gt; Extracts the full transcript with timestamps.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM Optimization (Garbage Collection):&lt;/strong&gt; The script runs Whisper first, &lt;strong&gt;unloads it from memory&lt;/strong&gt;, forces garbage collection, and only thenloads the Vision model (Qwen). This prevents OOM errors on 8GB/16GB Macs.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Captioning (Qwen3-VL-2B-Instruct-4bit):&lt;/strong&gt; It uses the mlx-vlm library to describe the representative frame of each scene using a customizable prompt.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;‚ú® Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fully Local:&lt;/strong&gt; No API keys, no cloud.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Efficient:&lt;/strong&gt; Doesn't waste compute on identical frames.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured Output:&lt;/strong&gt; Generates a clean .txt file with global context, audio transcript, and chronological visual descriptions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Customizable:&lt;/strong&gt; You can change the prompt (e.g., &amp;quot;Describe the emotions&amp;quot;, &amp;quot;Read the text on screen&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;üõ†Ô∏è Usage &amp;amp; Requirements&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Dependencies:&lt;/strong&gt;&lt;br /&gt; You need ffmpeg installed (for Whisper) and the Python libs:&lt;/p&gt; &lt;p&gt;code Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;brew install ffmpeg pip install opencv-python numpy pillow mlx-vlm openai-whisper torch &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;Running the script:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;code Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# Standard usage python video_rag.py video.mp4 # Advanced (Custom prompt + Whisper Large) python video_rag.py meeting.mp4 --whisper-model large-v3 --prompt &amp;quot;Describe the charts on the slide.&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;strong&gt;üß™ Request for M4 / M4 Pro Users&lt;/strong&gt;&lt;br /&gt; I am currently running this on older Apple Silicon. If anyone here has an &lt;strong&gt;M4 or M4 Pro&lt;/strong&gt;, I would love to hear your feedback on the inference speed (tokens/sec) for the Qwen-VL part via MLX!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;üìÇ The Code (video_rag.py)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;code Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;#!/usr/bin/env python3 # -*- coding: utf-8 -*- import os import gc import cv2 import re import time import argparse from pathlib import Path import numpy as np from PIL import Image # MLX / Qwen-VL from mlx_vlm import load, generate from mlx_vlm.prompt_utils import apply_chat_template from mlx_vlm.utils import load_config # Whisper import whisper # --------- CONFIG QWEN / MLX --------- MODEL_PATH = &amp;quot;mlx-community/Qwen3-VL-2B-Instruct-4bit&amp;quot; RESIZE_DIM = (384, 384) PREFIXES_A_SUPPRIMER = [ &amp;quot;cette image montre&amp;quot;, &amp;quot;l'image montre&amp;quot;, &amp;quot;sur cette image&amp;quot;, &amp;quot;dans cette image&amp;quot;, &amp;quot;voici&amp;quot;, &amp;quot;c'est&amp;quot;, &amp;quot;je vois&amp;quot;, &amp;quot;je peux voir&amp;quot;, &amp;quot;il y a&amp;quot;, &amp;quot;on voit&amp;quot;, &amp;quot;une vue de&amp;quot; ] # --------- CHARGEMENT DES MOD√àLES --------- def load_qwen_model(): print(f&amp;quot;‚¨áÔ∏è Chargement du mod√®le VLM : {MODEL_PATH}...&amp;quot;) model, processor = load(MODEL_PATH, trust_remote_code=True) config = load_config(MODEL_PATH) print(&amp;quot;‚úÖ Qwen3-VL charg√©.&amp;quot;) return model, processor, config def load_whisper_model(name: str): print(f&amp;quot;‚¨áÔ∏è Chargement du mod√®le Whisper : {name}...&amp;quot;) model = whisper.load_model(name) print(f&amp;quot;‚úÖ Whisper {name} charg√©.&amp;quot;) return model # --------- UTILITAIRES TEXTE / TEMPS --------- def clean_caption(raw_text: str) -&amp;gt; str: cleaned = raw_text.strip() if not cleaned: return &amp;quot;&amp;quot; lower_clean = cleaned.lower() # √©vite les r√©ponses du genre &amp;quot;d√©sol√©...&amp;quot; if &amp;quot;d√©sol√©&amp;quot; in lower_clean or &amp;quot;sorry&amp;quot; in lower_clean: return &amp;quot;&amp;quot; for prefix in PREFIXES_A_SUPPRIMER: if lower_clean.startswith(prefix): cleaned = cleaned[len(prefix):] lower_clean = cleaned.lower() cleaned = re.sub( r&amp;quot;^(que\s|qu'|:|,|\.|je vois)\s*&amp;quot;, &amp;quot;&amp;quot;, cleaned, flags=re.IGNORECASE, ).strip() # coupe √† la premi√®re ponctuation forte depuis la fin m = re.search(r&amp;quot;[\.!?]&amp;quot;, cleaned[::-1]) if m: end_pos = len(cleaned) - m.start() cleaned = cleaned[:end_pos] cleaned = cleaned.strip() if not cleaned: return &amp;quot;&amp;quot; return cleaned[0].upper() + cleaned[1:] def format_time_str(t_sec: float) -&amp;gt; str: minutes = int(t_sec // 60) seconds = int(t_sec % 60) return f&amp;quot;{minutes:02d}:{seconds:02d}&amp;quot; # --------- FEATURES POUR SC√àNES --------- def compute_frame_feature(frame_bgr) -&amp;gt; np.ndarray: &amp;quot;&amp;quot;&amp;quot; Cr√©e une empreinte simple de l'image pour la d√©tection de sc√®nes. -&amp;gt; grayscale, resize 64x64, vector 0‚Äì1. &amp;quot;&amp;quot;&amp;quot; gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY) small = cv2.resize(gray, (64, 64)) vec = small.astype(&amp;quot;float32&amp;quot;) / 255.0 return vec.flatten() # --------- PASS 1 : D√âTECTION DE SC√àNES (SANS QWEN) --------- def detect_scenes(video_path: str, sample_fps: float = 1.0, scene_threshold: float = 0.20): &amp;quot;&amp;quot;&amp;quot; Passe 1 : on parcourt la vid√©o √† sample_fps (ex: 1 image/s), on calcule un feature par frame, et on d√©tecte les changements de sc√®ne selon un seuil de diff√©rence moyenne. Retourne : - scenes_raw : liste de dicts { &amp;quot;start_sec&amp;quot;, &amp;quot;end_sec&amp;quot; } - duration_sec : dur√©e approx de la vid√©o &amp;quot;&amp;quot;&amp;quot; cap = cv2.VideoCapture(video_path) if not cap.isOpened(): raise RuntimeError(f&amp;quot;Impossible d'ouvrir la vid√©o : {video_path}&amp;quot;) base_fps = cap.get(cv2.CAP_PROP_FPS) if base_fps &amp;lt;= 0: base_fps = 25.0 total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) duration_sec = total_frames / base_fps if total_frames &amp;gt; 0 else 0 frame_interval = max(1, int(round(base_fps / sample_fps))) print(f&amp;quot;[SCENES] FPS vid√©o ‚âà {base_fps:.2f}&amp;quot;) print(f&amp;quot;[SCENES] Frames totales : {total_frames}&amp;quot;) print(f&amp;quot;[SCENES] Dur√©e approx : {duration_sec:.1f} s&amp;quot;) print(f&amp;quot;[SCENES] √âchantillonnage √† {sample_fps} img/s =&amp;gt; intervalle {frame_interval} frames&amp;quot;) print(f&amp;quot;[SCENES] Seuil de sc√®ne : {scene_threshold}&amp;quot;) scenes_raw = [] last_feat = None current_start_sec = None prev_t_sec = None frame_idx = 0 while True: ret, frame = cap.read() if not ret: break if frame_idx % frame_interval != 0: frame_idx += 1 continue t_sec = frame_idx / base_fps feat = compute_frame_feature(frame) if last_feat is None: # premi√®re frame current_start_sec = t_sec prev_t_sec = t_sec last_feat = feat else: diff = float(np.mean(np.abs(feat - last_feat))) if diff &amp;gt; scene_threshold: # cl√¥ture de la sc√®ne pr√©c√©dente scenes_raw.append({ &amp;quot;start_sec&amp;quot;: current_start_sec, &amp;quot;end_sec&amp;quot;: prev_t_sec, }) # nouvelle sc√®ne current_start_sec = t_sec prev_t_sec = t_sec last_feat = feat frame_idx += 1 # cl√¥ture de la derni√®re sc√®ne if current_start_sec is not None: end_sec = duration_sec if duration_sec &amp;gt; 0 else prev_t_sec scenes_raw.append({ &amp;quot;start_sec&amp;quot;: current_start_sec, &amp;quot;end_sec&amp;quot;: end_sec, }) cap.release() print(f&amp;quot;[SCENES] Nombre de sc√®nes d√©tect√©es : {len(scenes_raw)}&amp;quot;) for i, sc in enumerate(scenes_raw, start=1): print(f&amp;quot; SCENE {i}: {format_time_str(sc['start_sec'])} - {format_time_str(sc['end_sec'])}&amp;quot;) return scenes_raw, duration_sec # --------- PASS 2 : QWEN SUR UNE FRAME REPR√âSENTATIVE PAR SC√àNE --------- def grab_frame_at_time(video_path: str, t_sec: float): &amp;quot;&amp;quot;&amp;quot; R√©cup√®re une frame √† t_sec (en secondes). &amp;quot;&amp;quot;&amp;quot; cap = cv2.VideoCapture(video_path) if not cap.isOpened(): raise RuntimeError(f&amp;quot;Impossible d'ouvrir la vid√©o : {video_path}&amp;quot;) cap.set(cv2.CAP_PROP_POS_MSEC, t_sec * 1000.0) ret, frame = cap.read() cap.release() if not ret: return None return frame def describe_scene_qwen(model, processor, config, video_path: str, start_sec: float, end_sec: float, max_tokens: int, prompt: str): &amp;quot;&amp;quot;&amp;quot; Choisit un temps repr√©sentatif (milieu de la sc√®ne), r√©cup√®re la frame correspondante et la donne √† Qwen-VL. &amp;quot;&amp;quot;&amp;quot; rep_sec = (start_sec + end_sec) / 2.0 frame = grab_frame_at_time(video_path, rep_sec) if frame is None: return None small_frame = cv2.resize(frame, RESIZE_DIM) frame_rgb = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB) pil_image = Image.fromarray(frame_rgb) formatted_prompt = apply_chat_template( processor, config, prompt, num_images=1 ) output = generate( model, processor, formatted_prompt, pil_image, max_tokens=max_tokens, verbose=False, repetition_penalty=1.05, temp=0.0, ) if hasattr(output, &amp;quot;text&amp;quot;): raw_text = output.text else: raw_text = str(output) cleaned = clean_caption(raw_text) if not cleaned: return None return cleaned def describe_all_scenes(model, processor, config, video_path: str, scenes_raw, max_tokens: int, prompt: str): &amp;quot;&amp;quot;&amp;quot; Pour chaque sc√®ne brute (start_sec, end_sec), appelle Qwen-VL UNE fois, et retourne une liste de sc√®nes enrichies : { &amp;quot;start_sec&amp;quot;: ..., &amp;quot;end_sec&amp;quot;: ..., &amp;quot;start_str&amp;quot;: &amp;quot;MM:SS&amp;quot;, &amp;quot;end_str&amp;quot;: &amp;quot;MM:SS&amp;quot;, &amp;quot;caption&amp;quot;: &amp;quot;...&amp;quot; } &amp;quot;&amp;quot;&amp;quot; scenes = [] t0 = time.time() for idx, sc in enumerate(scenes_raw, start=1): start_sec = sc[&amp;quot;start_sec&amp;quot;] end_sec = sc[&amp;quot;end_sec&amp;quot;] print(f&amp;quot;[VLM-SCENE] SCENE {idx} =&amp;gt; {format_time_str(start_sec)} - {format_time_str(end_sec)}&amp;quot;) caption = describe_scene_qwen( model, processor, config, video_path, start_sec, end_sec, max_tokens=max_tokens, prompt=prompt, ) if caption is None: caption = &amp;quot;(Description indisponible)&amp;quot; scene_entry = { &amp;quot;start_sec&amp;quot;: start_sec, &amp;quot;end_sec&amp;quot;: end_sec, &amp;quot;start_str&amp;quot;: format_time_str(start_sec), &amp;quot;end_str&amp;quot;: format_time_str(end_sec), &amp;quot;caption&amp;quot;: caption, } print(&amp;quot; -&amp;gt;&amp;quot;, caption) scenes.append(scene_entry) print(f&amp;quot;[VLM-SCENE] Temps total VLM sc√®nes : {time.time() - t0:.1f} s&amp;quot;) return scenes # --------- WHISPER --------- def transcribe_audio_whisper(whisper_model, video_path: str, language: str | None = None) -&amp;gt; dict: &amp;quot;&amp;quot;&amp;quot; Transcrit directement la vid√©o (Whisper utilise ffmpeg en interne). Retourne l'objet complet (avec segments). &amp;quot;&amp;quot;&amp;quot; print(&amp;quot;[WHISPER] Transcription en cours...&amp;quot;) t0 = time.time() result = whisper_model.transcribe(video_path, language=language) print(f&amp;quot;[WHISPER] Transcription termin√©e en {time.time() - t0:.1f} s&amp;quot;) return result # --------- CONSTRUCTION DU TEXTE FINAL --------- def build_output_text(transcription: dict, scenes, video_path: str, duration_sec: float) -&amp;gt; str: lines = [] lines.append(&amp;quot;### CONTEXTE VIDEO POUR LLM (UTF-8)\n&amp;quot;) lines.append(f&amp;quot;Fichier vid√©o d'origine : {video_path}&amp;quot;) lines.append(f&amp;quot;Dur√©e approximative : {duration_sec:.1f} secondes\n&amp;quot;) # --- SECTION 0 : description globale approximative --- lines.append(&amp;quot;SECTION 0 : DESCRIPTION GLOBALE (√† partir des sc√®nes)\n&amp;quot;) if scenes: first = scenes[0] mid = scenes[len(scenes) // 2] last = scenes[-1] lines.append(f&amp;quot;- D√©but [{first['start_str']} - {first['end_str']}]: {first['caption']}&amp;quot;) if mid is not first and mid is not last: lines.append(f&amp;quot;- Milieu [{mid['start_str']} - {mid['end_str']}]: {mid['caption']}&amp;quot;) lines.append(f&amp;quot;- Fin [{last['start_str']} - {last['end_str']}]: {last['caption']}&amp;quot;) else: lines.append(&amp;quot;(Aucune sc√®ne d√©tect√©e.)&amp;quot;) lines.append(&amp;quot;&amp;quot;) # --- SECTION 1 : transcription audio --- lines.append(&amp;quot;SECTION 1 : TRANSCRIPTION AUDIO (Whisper)\n&amp;quot;) full_text = transcription.get(&amp;quot;text&amp;quot;, &amp;quot;&amp;quot;).strip() lines.append(&amp;quot;TEXTE COMPLET :&amp;quot;) lines.append(full_text if full_text else &amp;quot;(Transcription vide ou indisponible.)&amp;quot;) lines.append(&amp;quot;&amp;quot;) if &amp;quot;segments&amp;quot; in transcription: lines.append(&amp;quot;SEGMENTS HORODATES :&amp;quot;) for seg in transcription[&amp;quot;segments&amp;quot;]: start = seg.get(&amp;quot;start&amp;quot;, 0.0) end = seg.get(&amp;quot;end&amp;quot;, 0.0) txt = seg.get(&amp;quot;text&amp;quot;, &amp;quot;&amp;quot;).strip() m1, s1 = divmod(int(start), 60) m2, s2 = divmod(int(end), 60) lines.append(f&amp;quot;[{m1:02d}:{s1:02d} - {m2:02d}:{s2:02d}] {txt}&amp;quot;) lines.append(&amp;quot;&amp;quot;) # --- SECTION 2 : sc√®nes visuelles d√©crites --- lines.append(&amp;quot;SECTION 2 : SCENES VISUELLES (Qwen3-VL, 1 description par sc√®ne)\n&amp;quot;) if not scenes: lines.append(&amp;quot;(Aucune sc√®ne disponible.)&amp;quot;) else: for idx, sc in enumerate(scenes, start=1): lines.append(f&amp;quot;SCENE {idx} [{sc['start_str']} - {sc['end_str']}]&amp;quot;) lines.append(f&amp;quot;- Description : {sc['caption']}&amp;quot;) lines.append(&amp;quot;&amp;quot;) lines.append(&amp;quot;\nFIN DU CONTEXTE.\n&amp;quot;) return &amp;quot;\n&amp;quot;.join(lines) # --------- MAIN --------- def main(): parser = argparse.ArgumentParser( description=&amp;quot;Analyse vid√©o V3.1 : d√©tection de sc√®nes + Whisper + Qwen3-VL (1 description par sc√®ne).&amp;quot; ) parser.add_argument(&amp;quot;video&amp;quot;, help=&amp;quot;Chemin de la vid√©o (ex: .mp4, .mov iPhone, etc.)&amp;quot;) parser.add_argument(&amp;quot;--sample-fps&amp;quot;, type=float, default=1.0, help=&amp;quot;FPS d'√©chantillonnage pour d√©tecter les sc√®nes (d√©faut: 1.0)&amp;quot;) parser.add_argument(&amp;quot;--scene-threshold&amp;quot;, type=float, default=0.20, help=&amp;quot;Seuil de changement de sc√®ne (diff√©rence moyenne 0-1, d√©faut: 0.20)&amp;quot;) parser.add_argument(&amp;quot;--whisper-model&amp;quot;, type=str, default=&amp;quot;small&amp;quot;, help=&amp;quot;Mod√®le Whisper: small, medium, large-v3, etc. (d√©faut: small)&amp;quot;) parser.add_argument(&amp;quot;--whisper-lang&amp;quot;, type=str, default=None, help=&amp;quot;Code langue (ex: 'fr'), ou None pour auto-d√©tection.&amp;quot;) parser.add_argument(&amp;quot;--max-tokens&amp;quot;, type=int, default=60, help=&amp;quot;Max tokens g√©n√©r√©s par Qwen-VL par sc√®ne (d√©faut: 60)&amp;quot;) parser.add_argument( &amp;quot;--prompt&amp;quot;, type=str, default=( &amp;quot;D√©cris factuellement ce qui est pr√©sent dans l'image en fran√ßais. &amp;quot; &amp;quot;Sois direct et pr√©cis, sans interpr√©tation inutile.&amp;quot; ), help=&amp;quot;Prompt de description pour Qwen-VL (d√©faut: description factuelle en fran√ßais).&amp;quot; ) parser.add_argument(&amp;quot;--out&amp;quot;, type=str, default=&amp;quot;contexte_video_v3_1.txt&amp;quot;, help=&amp;quot;Fichier texte de sortie (UTF-8).&amp;quot;) args = parser.parse_args() video_path = os.path.abspath(args.video) if not os.path.exists(video_path): raise FileNotFoundError(f&amp;quot;Vid√©o introuvable : {video_path}&amp;quot;) # 1) D√©tection de sc√®nes (rapide, sans mod√®les) scenes_raw, duration_sec = detect_scenes( video_path, sample_fps=args.sample_fps, scene_threshold=args.scene_threshold, ) # 2) Whisper d'abord (audio) model_whisper = load_whisper_model(args.whisper_model) transcription = transcribe_audio_whisper( model_whisper, video_path, language=args.whisper_lang ) # üî• Lib√®re Whisper de la RAM del model_whisper gc.collect() # 3) Puis Qwen-VL (vision) model_vlm, processor_vlm, config_vlm = load_qwen_model() # 4) Description de chaque sc√®ne (1 frame repr√©sentative) scenes = describe_all_scenes( model_vlm, processor_vlm, config_vlm, video_path, scenes_raw, max_tokens=args.max_tokens, prompt=args.prompt, ) # 5) Construction du texte final output_text = build_output_text( transcription, scenes, video_path, duration_sec, ) out_path = Path(args.out) out_path.write_text(output_text, encoding=&amp;quot;utf-8&amp;quot;) print(f&amp;quot;\n‚úÖ Fichier contexte V3.1 g√©n√©r√© : {out_path}&amp;quot;) print(&amp;quot; Tu peux maintenant copier/coller ce fichier dans Open WebUI ou LM Studio (RAG).&amp;quot;) if __name__ == &amp;quot;__main__&amp;quot;: main() &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Longjumping-Elk-7756"&gt; /u/Longjumping-Elk-7756 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82u5k/local_videototext_pipeline_on_apple_silicon/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82u5k/local_videototext_pipeline_on_apple_silicon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p82u5k/local_videototext_pipeline_on_apple_silicon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T13:57:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7zt7c</id>
    <title>PrimeIntellect / INTELLECT-3 (GLM 4.5 Air finetune)</title>
    <updated>2025-11-27T11:20:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7zt7c/primeintellect_intellect3_glm_45_air_finetune/"&gt; &lt;img alt="PrimeIntellect / INTELLECT-3 (GLM 4.5 Air finetune)" src="https://b.thumbs.redditmedia.com/t1m_BZZWDXQPDbhlN8iiwMl3n_SmaEZkveMHqqgs3Qc.jpg" title="PrimeIntellect / INTELLECT-3 (GLM 4.5 Air finetune)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;INTELLECT-3&lt;/strong&gt; is a 106B (A12B) parameter Mixture-of-Experts reasoning model post-trained from &lt;a href="https://huggingface.co/zai-org/GLM-4.5-Air-Base"&gt;GLM-4.5-Air-Base&lt;/a&gt; using supervised fine-tuning (SFT) followed by large-scale reinforcement learning (RL).&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/n0djc6r9as3g1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bac28d8bf5b3dbdd1c2f8b175526b551b7c2a5ad"&gt;https://preview.redd.it/n0djc6r9as3g1.png?width=1172&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bac28d8bf5b3dbdd1c2f8b175526b551b7c2a5ad&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/PrimeIntellect_INTELLECT-3-GGUF"&gt;https://huggingface.co/bartowski/PrimeIntellect_INTELLECT-3-GGUF&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7zt7c/primeintellect_intellect3_glm_45_air_finetune/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7zt7c/primeintellect_intellect3_glm_45_air_finetune/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7zt7c/primeintellect_intellect3_glm_45_air_finetune/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T11:20:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p88te7</id>
    <title>Local AI As a "Bubble-proof" Practice</title>
    <updated>2025-11-27T18:03:31+00:00</updated>
    <author>
      <name>/u/acornPersonal</name>
      <uri>https://old.reddit.com/user/acornPersonal</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've built a suite of off-line AI programs for macOS and iOS, with the central purpose of enabling everyday users, who are not tech savvy or up-to-date on the latest and greatest LLMs, etc., too have a private oasis from cloud based AI, data poisoning, and all that nasty data collection practices that the big box LLM companies are utilizing. Another thing that I've noticed about these signals like Peter Thiel's selling of massive amounts of stock in the AI sector says to me that they understand something that us in the local LLM community already intrinsically know, even if it hasn't always been set out loud, but the world Cannot support cloud based AI for every single human being, there's not enough energy or freshwater. We don't have enough planet for it. The only way for us to provide even some semblance or chance for intellectual equality and accessibility around the world is to put AI in peoples local devices. In its own way, the crisis that's occurring has a lot to do with the fact that it must be obvious to people at the top that buying power plants and building infrastructure to service the top 5 to 10% of the planet is just not a sustainable practice. What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/acornPersonal"&gt; /u/acornPersonal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88te7/local_ai_as_a_bubbleproof_practice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88te7/local_ai_as_a_bubbleproof_practice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p88te7/local_ai_as_a_bubbleproof_practice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T18:03:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1p80bw7</id>
    <title>Paper page - NVIDIA Nemotron Parse 1.1</title>
    <updated>2025-11-27T11:50:47+00:00</updated>
    <author>
      <name>/u/ab2377</name>
      <uri>https://old.reddit.com/user/ab2377</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80bw7/paper_page_nvidia_nemotron_parse_11/"&gt; &lt;img alt="Paper page - NVIDIA Nemotron Parse 1.1" src="https://external-preview.redd.it/2qpOcYaS2j2UQIHcV7jQDC__U_Q0iB2Xn13ee-Dp1dU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5420a7b266ae9ecbe02f1c320631b2ab2fec3c4b" title="Paper page - NVIDIA Nemotron Parse 1.1" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;More OCR!&lt;/p&gt; &lt;p&gt;&amp;quot;We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.&amp;quot;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ab2377"&gt; /u/ab2377 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/papers/2511.20478"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80bw7/paper_page_nvidia_nemotron_parse_11/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p80bw7/paper_page_nvidia_nemotron_parse_11/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T11:50:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7rr0g</id>
    <title>Intellect-3: Post-trained GLM 4.5 Air</title>
    <updated>2025-11-27T03:25:16+00:00</updated>
    <author>
      <name>/u/Cute-Sprinkles4911</name>
      <uri>https://old.reddit.com/user/Cute-Sprinkles4911</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;106B (A12B) parameter Mixture-of-Experts reasoning model&lt;/p&gt; &lt;p&gt;NGL the reported stats are sick:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;BF16 version can run on 2x H200s, with FP8 on 1x H200&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cute-Sprinkles4911"&gt; /u/Cute-Sprinkles4911 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7rr0g/intellect3_posttrained_glm_45_air/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T03:25:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1p85tiw</id>
    <title>An update to "why multimodal API calls to vLLM server have worse outputs than using Open WebUI"</title>
    <updated>2025-11-27T16:03:03+00:00</updated>
    <author>
      <name>/u/Majesticeuphoria</name>
      <uri>https://old.reddit.com/user/Majesticeuphoria</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;About two weeks ago, I asked this question: &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ouft9q/need_help_figuring_out_why_multimodal_api_calls/"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1ouft9q/need_help_figuring_out_why_multimodal_api_calls/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally figured out after extensive testing that the difference was due to usage of qwen-vl-utils to preprocess images. The output is quite different with vs without utils. Just thought this would help anyone else facing similar issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Majesticeuphoria"&gt; /u/Majesticeuphoria &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85tiw/an_update_to_why_multimodal_api_calls_to_vllm/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p85tiw/an_update_to_why_multimodal_api_calls_to_vllm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p85tiw/an_update_to_why_multimodal_api_calls_to_vllm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T16:03:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1p88mwr</id>
    <title>llama.cpp now supports online repacking for Q4_K quants on ARM CPUs with dotprod.</title>
    <updated>2025-11-27T17:56:33+00:00</updated>
    <author>
      <name>/u/PurpleWinterDawn</name>
      <uri>https://old.reddit.com/user/PurpleWinterDawn</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First of all, a massive thank you to the llama.cpp team and contributors!&lt;/p&gt; &lt;p&gt;This is huge for ARM-based systems using better quality quants such as Q4_K_M (compared to Q4_0 or IQ4_NL).&lt;/p&gt; &lt;p&gt;On my phone:&lt;/p&gt; &lt;p&gt;LFM2-8B-A1B-Q4_K_M went from 32 pp and 15 tg, to 85 pp and 35 tg. It's still short of 35 pp compared to Q4_0 (I'm getting 125 pp 40 tg), but it's more usable.&lt;/p&gt; &lt;p&gt;The older Ministral-8B-Instruct-2410-Q4_K_M runs 21 pp and 10 tg, up from 10 pp and 6 tg (off the top of my head).&lt;/p&gt; &lt;p&gt;I don't have an ARM-based Mac to test it on, but those numbers look promising for them!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PurpleWinterDawn"&gt; /u/PurpleWinterDawn &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88mwr/llamacpp_now_supports_online_repacking_for_q4_k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p88mwr/llamacpp_now_supports_online_repacking_for_q4_k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p88mwr/llamacpp_now_supports_online_repacking_for_q4_k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T17:56:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1p82mg3</id>
    <title>Never been a better time, to learn to write a good rhyme!</title>
    <updated>2025-11-27T13:48:02+00:00</updated>
    <author>
      <name>/u/lakySK</name>
      <uri>https://old.reddit.com/user/lakySK</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"&gt; &lt;img alt="Never been a better time, to learn to write a good rhyme!" src="https://preview.redd.it/g6nc02nc0t3g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d98ddd988232c35646210a0f68c0b2836986a310" title="Never been a better time, to learn to write a good rhyme!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Adversarial Poetry as a Universal Single-Turn Jailbreak Mechanism in Large Language Models &lt;a href="https://arxiv.org/abs/2511.15304"&gt;https://arxiv.org/abs/2511.15304&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lakySK"&gt; /u/lakySK &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/g6nc02nc0t3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p82mg3/never_been_a_better_time_to_learn_to_write_a_good/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T13:48:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1p83rp1</id>
    <title>Qwen3-VL-32B-Thinking EXL3 3.5bpw ‚Äì first working 32B VL quant on single 4090 (16-17 t/s)</title>
    <updated>2025-11-27T14:38:14+00:00</updated>
    <author>
      <name>/u/Nox1793</name>
      <uri>https://old.reddit.com/user/Nox1793</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"&gt; &lt;img alt="Qwen3-VL-32B-Thinking EXL3 3.5bpw ‚Äì first working 32B VL quant on single 4090 (16-17 t/s)" src="https://b.thumbs.redditmedia.com/vQeSl8GUqQ72s6vWhDU66LdC8eTO6If0EKTqSxBTMhM.jpg" title="Qwen3-VL-32B-Thinking EXL3 3.5bpw ‚Äì first working 32B VL quant on single 4090 (16-17 t/s)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released the first usable EXL3 quant of the brand-new Qwen3-VL-32B-Thinking (the 32B reasoning + vision beast that dropped 3 days ago).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;3.5 bpw HQ (hb6 / cc4096)&lt;/li&gt; &lt;li&gt;~18-20 GB VRAM ‚Üí fits and runs smooth on single 4090&lt;/li&gt; &lt;li&gt;Vision + &amp;lt;think&amp;gt; chain-of-thought fully preserved&lt;/li&gt; &lt;li&gt;16-17 t/s real-world (see Garfield getting the lasagna meme below üòπ)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/nullrunner/Qwen3-VL-32B-Thinking-EXL3-3.5bpw"&gt;https://huggingface.co/nullrunner/Qwen3-VL-32B-Thinking-EXL3-3.5bpw&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4bpw HQ baking right now, Instruct version next.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tsb6uri79t3g1.jpg?width=880&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6ea9ff51d98a761c4c3f923efd8bfc260ab67689"&gt;Test Image&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/5s3w7cwa9t3g1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ea70a9defada2b3ec829f6c33abf7fb5228ea1f"&gt;Output and Metrics&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&amp;quot;convert.py&amp;quot; was broken, vision tower misaligned, LDLQ crashes on layer 37, constant OoM ‚Üí 4 hours of pain + A100 + Claude Code to make it actually work.&lt;/p&gt; &lt;p&gt;Hope someone finds it usefulüî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nox1793"&gt; /u/Nox1793 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p83rp1/qwen3vl32bthinking_exl3_35bpw_first_working_32b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T14:38:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1p80png</id>
    <title>DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning</title>
    <updated>2025-11-27T12:12:01+00:00</updated>
    <author>
      <name>/u/hedgehog0</name>
      <uri>https://old.reddit.com/user/hedgehog0</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80png/deepseekmathv2_towards_selfverifiable/"&gt; &lt;img alt="DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning" src="https://external-preview.redd.it/NNRX5IH0bPXI-mJ26LQk19NWgnKHeMgBlqbRSXbbGFk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ce1e1d706821a2296ebf8e620933cc5aa91b2f" title="DeepSeekMath-V2: Towards Self-Verifiable Mathematical Reasoning" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hedgehog0"&gt; /u/hedgehog0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p80png/deepseekmathv2_towards_selfverifiable/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p80png/deepseekmathv2_towards_selfverifiable/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T12:12:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7o83p</id>
    <title>Where did the Epstein emails dataset go</title>
    <updated>2025-11-27T00:27:19+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Removed from Hugging Face (&lt;a href="https://huggingface.co/datasets/tensonaut/EPSTEIN_FILES_20K"&gt;link&lt;/a&gt;)&lt;br /&gt; Removed from GitHub (&lt;a href="https://github.com/EF20K/"&gt;link&lt;/a&gt;)&lt;br /&gt; Reddit account deleted (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p683yz/thank_you_all_for_your_contribution_with_tools/"&gt;last post&lt;/a&gt;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7o83p/where_did_the_epstein_emails_dataset_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T00:27:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7siuu</id>
    <title>Anthropic just showed how to make AI agents work on long projects without falling apart</title>
    <updated>2025-11-27T04:05:59+00:00</updated>
    <author>
      <name>/u/purealgo</name>
      <uri>https://old.reddit.com/user/purealgo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Most AI agents forget everything between sessions, which means they completely lose track of long tasks. Anthropic‚Äôs new article shows a surprisingly practical fix. Instead of giving an agent one giant goal like ‚Äúbuild a web app,‚Äù they wrap it in a simple harness that forces structure, memory, and accountability.&lt;/p&gt; &lt;p&gt;First, an initializer agent sets up the project. It creates a full feature list, marks everything as failing, initializes git, and writes a progress log. Then each later session uses a coding agent that reads the log and git history, picks exactly one unfinished feature, implements it, tests it, commits the changes, and updates the log. No guessing, no drift, no forgetting.&lt;/p&gt; &lt;p&gt;The result is an AI that can stop, restart, and keep improving a project across many independent runs. It behaves more like a disciplined engineer than a clever autocomplete. It also shows that the real unlock for long-running agents may not be smarter models, but better scaffolding.&lt;/p&gt; &lt;p&gt;Read the article here:&lt;br /&gt; &lt;a href="https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents"&gt;https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/purealgo"&gt; /u/purealgo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7siuu/anthropic_just_showed_how_to_make_ai_agents_work/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T04:05:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p89j2t</id>
    <title>Today I learned that DDR5 can throttle itself at high temps. It affects inference speed.</title>
    <updated>2025-11-27T18:33:00+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been moving the rig over to a proper frame from the $50 Amazon mining frame and taking the opportunity to do airflow properly. I measured the temps of the 6400 MT/s DDR5 RDIMMs using ipmitool and found they were hitting 95C and above while compiling vLLM from source.&lt;/p&gt; &lt;p&gt;Ouch. That‚Äôs very near the top of their operating envelope.&lt;/p&gt; &lt;p&gt;After 3D printing some RAM shrouds and adding a pair of 92mm Noctua Chromax the DDR5 stays under 60C during compiling and even during CPU inference.&lt;/p&gt; &lt;p&gt;And it runs approx 10% faster at inference even for GPU-only models. &lt;/p&gt; &lt;p&gt;Check your RAM temps!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p89j2t/today_i_learned_that_ddr5_can_throttle_itself_at/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T18:33:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8ahy8</id>
    <title>Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp; Reasoning Benchmarks. (Link to Chat with the Model provided)</title>
    <updated>2025-11-27T19:13:50+00:00</updated>
    <author>
      <name>/u/44th--Hokage</name>
      <uri>https://old.reddit.com/user/44th--Hokage</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt; &lt;img alt="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" src="https://a.thumbs.redditmedia.com/tQWpy1j22HMExtYqdGvlA_Lo8sIubygJf4xso2VwSj0.jpg" title="Prime Intellect Introduces INTELLECT-3: A 100B+ MoE Trained With Large-scale RL That Achieves State-Of-The-Art Performance For Its Size, Taking The Lead Amongst Open-Sourced Models Across Math, Code, Science &amp;amp; Reasoning Benchmarks. (Link to Chat with the Model provided)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h2&gt;From the Official Announcement:&lt;/h2&gt; &lt;blockquote&gt; &lt;p&gt;Today, we release INTELLECT-3, a 100B+ parameter Mixture-of-Experts model trained on our RL stack, achieving state-of-the-art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our complete recipe ‚Äî from the model weights and training frameworks, to our datasets, RL environments, and evaluations ‚Äî has been open-sourced, with the goal of encouraging more open research on large scale reinforcement learning.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;INTELLECT-3 is trained on the same software and infrastructure that we‚Äôre open-sourcing and making available on our platform at Prime Intellect, giving everyone the tools to post-train their own state-of-the-art models, and moving us towards a future where every company can be an AI company.&lt;/p&gt; &lt;p&gt;The sharpest distinction between Prime-RL and many other RL trainers is that it is async-only ‚Äî we recognized fairly early (for our previous INTELLECT-2 model) that the future of RL is async; i.e. always a few steps off-policy. Async training is simply the only practical way to efficiently scale RL to long-horizon agentic rollouts without incurring bottlenecks based on the slowest rollouts per step.&lt;/p&gt; &lt;/blockquote&gt; &lt;hr /&gt; &lt;h2&gt;Architecture:&lt;/h2&gt; &lt;p&gt;Three main abstractions facilitate RL training: the orchestrator, the trainer, and the inference service. A RL training run involves the coordination of a trainer, orchestrator and an inference service. The FSDP trainer and vLLM inference run disaggregated, and can be individually deployed across multiple nodes.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Orchestrator:&lt;/strong&gt; - The orchestrator is a lightweight CPU process that handles the core data flow and scheduling logic, serving as an intermediary between the trainer and inference service with bidirectional relays. In one direction, it collects rollouts from the inference server, assembles them into packed batches, and dispatches them to the trainer; in the other direction, it relays updated model weights from the trainer to the inference service. The orchestrator utilizes verifiers environments to abstract multi-turn rollout generation and scoring, allowing any environment on the Environments Hub to plug into the training loop.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Trainer:&lt;/strong&gt; - The trainer is responsible for producing an updated policy model given rollouts and advantages. We use FSDP 2 as the backend with compatibility for any HuggingFace model. FSDP shards model parameters, gradients, and optimizer states, allowing training large models with data parallelism and minimal GPU memory footprint. The trainer is inspired by torchtitan and relies on native PyTorch features to implement advanced parallelism techniques, such as tensor, context, and expert parallelism, and leverages grouped matrix multiplication kernels for efficient MoE training.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Inference:&lt;/strong&gt; - The inference pool consists of standard OpenAI-compatible servers with a vLLM backend. The API specification is extended with custom endpoints to enable updating the server with the latest policy: &lt;code&gt;/update_weights&lt;/code&gt; is used to update the policy, and &lt;code&gt;/reload_weights&lt;/code&gt; is used to reset the weights to the base model in between experiments. We rely on vLLM's optimized kernels, parallelism strategies, and scheduling for fast rollout generation. Given the disaggregated nature of the service architecture, it can be directly extended to include multiple engines with a shared request pool, allowing operation across multiple clusters and straightforward integration of alternative inference engines.&lt;/p&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Official Announcement: &lt;a href="https://www.primeintellect.ai/blog/intellect-3"&gt;https://www.primeintellect.ai/blog/intellect-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Technical Report: &lt;a href="https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf"&gt;https://storage.googleapis.com/intellect-3-paper/INTELLECT_3_Technical_Report.pdf&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Prime-RL GitHub: &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;https://github.com/PrimeIntellect-ai/prime-rl&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Link to the Open-Sourced Model Weights: &lt;a href="https://huggingface.co/PrimeIntellect/INTELLECT-3"&gt;https://huggingface.co/PrimeIntellect/INTELLECT-3&lt;/a&gt;&lt;/h4&gt; &lt;hr /&gt; &lt;h4&gt;Chat with the Model Here: &lt;a href="https://chat.primeintellect.ai/"&gt;https://chat.primeintellect.ai/&lt;/a&gt;&lt;/h4&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/44th--Hokage"&gt; /u/44th--Hokage &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1p8ahy8"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8ahy8/prime_intellect_introduces_intellect3_a_100b_moe/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T19:13:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1p7z9g1</id>
    <title>deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face</title>
    <updated>2025-11-27T10:47:09+00:00</updated>
    <author>
      <name>/u/Dark_Fire_12</name>
      <uri>https://old.reddit.com/user/Dark_Fire_12</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face" src="https://external-preview.redd.it/NNRX5IH0bPXI-mJ26LQk19NWgnKHeMgBlqbRSXbbGFk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=91ce1e1d706821a2296ebf8e620933cc5aa91b2f" title="deepseek-ai/DeepSeek-Math-V2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dark_Fire_12"&gt; /u/Dark_Fire_12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-Math-V2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p7z9g1/deepseekaideepseekmathv2_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T10:47:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1p81k2z</id>
    <title>Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted</title>
    <updated>2025-11-27T12:56:59+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt; &lt;img alt="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" src="https://external-preview.redd.it/8n5MhbkzXEcl9NlvZMbb8GGre-k1VjQ0kDAKe7qQtQM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9334318d3d29cfd953050dfdf981bc10db9cc00b" title="Yes it is possible to uncensor gpt-oss-20b - ArliAI/gpt-oss-20b-Derestricted" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Original discussion on the initial &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; created GLM-4.5-Air-Derestricted model that was ablated using &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;'s new ablation method is here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1p5epot/the_most_objectively_correct_way_to_abliterate_so/"&gt;The most objectively correct way to abliterate so far - ArliAI/GLM-4.5-Air-Derestricted &lt;/a&gt;&lt;/p&gt; &lt;p&gt;(Note: Derestricted is a name given to models created by Arli AI using this method, but the method officially is just called &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oypwa7/a_more_surgical_approach_to_abliteration/"&gt;Norm-Preserving Biprojected Abliteration&lt;/a&gt; by &lt;a href="/u/grimjim"&gt;u/grimjim&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;Hey everyone, Owen here from &lt;strong&gt;&lt;a href="https://www.arliai.com"&gt;Arli AI&lt;/a&gt;&lt;/strong&gt; again. In my previous post, I got a lot of requests to attempt this derestricting on OpenAI's gpt-oss models as they are models that are intelligent but was infamous for being very...restricted.&lt;/p&gt; &lt;p&gt;I thought that it would be a big challenge and be interesting to try and attempt as well, and so that was the next model I decided to try and derestrict next. The 120b version is more unwieldy to transfer around and load in/out of VRAM/RAM as I was experimenting, so I started with the 20b version first but I will get to the 120b next which should be super interesting.&lt;/p&gt; &lt;p&gt;As for the 20b model here, it seems to have worked! The model now can respond to questions that OpenAI never would have approved of answering (lol!). It also seems to have cut down its wasteful looping around of deciding whether it can or cannot answer a question based on a non existent policy in it's reasoning, although this isn't completely removed yet. I suspect a more customized harmful/harmless dataset to specifically target this behavior might be useful for this, so that will be what I need to work on.&lt;/p&gt; &lt;p&gt;Otherwise I think this is just an outright improved model over the original as it is much more useful now than it's original behavior. Where it would usually flag a lot of false positives and be absolutely useless in certain situations just because of &amp;quot;safety&amp;quot;.&lt;/p&gt; &lt;p&gt;In order to work on modifying the weights of the model, I also had to use a BF16 converted version to start with as the model as you all might know was released in MXFP4 format, but then attempting the ablation on the BF16 converted model seems to work well. I think that this proves that this new method of essentially &amp;quot;direction-based&amp;quot; abliteration is really flexible and works super well for probably any models.&lt;/p&gt; &lt;p&gt;As for quants, I'm not one to worry about making GGUFs myself because I'm sure the GGUF makers will get to it pretty fast and do a better job than I can. Also, there are no FP8 or INT8 quants now because its pretty small and those that run FP8 or INT8 quants usually have a substantial GPU setup anyways.&lt;/p&gt; &lt;p&gt;Try it out and have fun! This time it's really for &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; because we don't even run this model on our Arli AI API service.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/ArliAI/gpt-oss-20b-Derestricted"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p81k2z/yes_it_is_possible_to_uncensor_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T12:56:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1p8b2ch</id>
    <title>How many strawberries are there in the letter 'R'?</title>
    <updated>2025-11-27T19:37:54+00:00</updated>
    <author>
      <name>/u/Cool-Chemical-5629</name>
      <uri>https://old.reddit.com/user/Cool-Chemical-5629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8b2ch/how_many_strawberries_are_there_in_the_letter_r/"&gt; &lt;img alt="How many strawberries are there in the letter 'R'?" src="https://preview.redd.it/x0xkt3xrqu3g1.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb6324407066f1393961c920803073a17079f3fb" title="How many strawberries are there in the letter 'R'?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just a little funny twist on the prompt &amp;quot;How many R's are there in the word strawberry?&amp;quot;, something you can use to test your models with vision. üçìüòÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cool-Chemical-5629"&gt; /u/Cool-Chemical-5629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/x0xkt3xrqu3g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p8b2ch/how_many_strawberries_are_there_in_the_letter_r/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p8b2ch/how_many_strawberries_are_there_in_the_letter_r/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-27T19:37:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1p1b550</id>
    <title>AMA with MiniMax ‚Äî Ask Us Anything!</title>
    <updated>2025-11-19T15:46:38+00:00</updated>
    <author>
      <name>/u/OccasionNo6699</name>
      <uri>https://old.reddit.com/user/OccasionNo6699</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre really excited to be here, thanks for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Skyler (&lt;a href="https://www.reddit.com/user/OccasionNo6699/"&gt;u/OccasionNo6699&lt;/a&gt;), head of engineering at MiniMax, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://x.com/MiniMax__AI/status/1982674798649160175?s=20"&gt;MiniMax-M2&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983382728343994414"&gt;Hailuo 2.3&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983661667872600296"&gt;MiniMax Speech 2.6&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://x.com/Hailuo_AI/status/1983964920493568296"&gt;MiniMax Music 2.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Joining me today are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Pengyu Zhao, &lt;a href="https://www.reddit.com/user/Wise_Evidence9973/"&gt;u/Wise_Evidence9973&lt;/a&gt; ‚Äî Head of LLM Research&lt;/li&gt; &lt;li&gt;Jade Cai, &lt;a href="https://www.reddit.com/user/srtng/"&gt;u/srtng&lt;/a&gt; ‚Äî Head of Developer Community&lt;/li&gt; &lt;li&gt;midnight_compile , &lt;a href="/u/Top_Cattle_2098"&gt;u/Top_Cattle_2098&lt;/a&gt; ‚Äî LLM Researcher&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 8AM-11AM PST with our core MiniMax tech team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OccasionNo6699"&gt; /u/OccasionNo6699 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p1b550/ama_with_minimax_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-19T15:46:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1p5retd</id>
    <title>Best Local VLMs - November 2025</title>
    <updated>2025-11-24T20:00:04+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite models are right now and &lt;strong&gt;&lt;em&gt;why&lt;/em&gt;&lt;/strong&gt;. Given the nature of the beast in evaluating VLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (what applications, how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1p5retd/best_local_vlms_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-24T20:00:04+00:00</published>
  </entry>
</feed>
