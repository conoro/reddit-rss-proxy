<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-04T06:10:55+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pdhmiw</id>
    <title>Qwen3-Coder-30B-A3B-Instruct[-FP8] config.json updated</title>
    <updated>2025-12-03T21:49:59+00:00</updated>
    <author>
      <name>/u/MutantEggroll</name>
      <uri>https://old.reddit.com/user/MutantEggroll</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct/commit/b2cff646eb4bb1d68355c01b18ae02e7cf42d120"&gt;Update config.json ¬∑ Qwen/Qwen3-Coder-30B-A3B-Instruct at b2cff64&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Unsure of the implications of this, maybe someone here can explain what's changed?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MutantEggroll"&gt; /u/MutantEggroll &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhmiw/qwen3coder30ba3binstructfp8_configjson_updated/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhmiw/qwen3coder30ba3binstructfp8_configjson_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhmiw/qwen3coder30ba3binstructfp8_configjson_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:49:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdhuyy</id>
    <title>Why doesn't deepseek release a smaller air model? Because they are focused at research?</title>
    <updated>2025-12-03T21:58:57+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why doesn't deepseek release a smaller air model like a 120b A10b MoE model or a 32b dense model? It seems like they are mainly focused in research and doesn't frequently release small models unlike GLM and qwen &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdhuyy/why_doesnt_deepseek_release_a_smaller_air_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:58:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdrvh8</id>
    <title>Introducing Lynkr ‚Äî an open-source Claude-style AI coding proxy built specifically for Databricks model endpoints üöÄ</title>
    <updated>2025-12-04T05:32:39+00:00</updated>
    <author>
      <name>/u/Dangerous-Dingo-5169</name>
      <uri>https://old.reddit.com/user/Dangerous-Dingo-5169</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks ‚Äî I‚Äôve been building a small developer tool that I think many Databricks users or AI-powered dev-workflow fans might find useful. It‚Äôs called &lt;strong&gt;Lynkr&lt;/strong&gt;, and it acts as a Claude-Code-style proxy that connects &lt;em&gt;directly&lt;/em&gt; to Databricks model endpoints while adding a lot of developer workflow intelligence on top.&lt;/p&gt; &lt;h1&gt;üîß What exactly is Lynkr?&lt;/h1&gt; &lt;p&gt;Lynkr is a &lt;strong&gt;self-hosted Node.js proxy&lt;/strong&gt; that mimics the Claude Code API/UX but routes all requests to &lt;strong&gt;Databricks-hosted models&lt;/strong&gt;.&lt;br /&gt; If you like the Claude Code workflow (repo-aware answers, tooling, code edits), but want to use &lt;strong&gt;your own Databricks models&lt;/strong&gt;, this is built for you.&lt;/p&gt; &lt;h1&gt;Key features:&lt;/h1&gt; &lt;h1&gt;üß† Repo intelligence&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Builds a lightweight index of your workspace (files, symbols, references).&lt;/li&gt; &lt;li&gt;Helps models ‚Äúunderstand‚Äù your project structure better than raw context dumping.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üõ†Ô∏è Developer tooling (Claude-style)&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Tool call support (sandboxed tasks, tests, scripts).&lt;/li&gt; &lt;li&gt;File edits, ops, directory navigation.&lt;/li&gt; &lt;li&gt;Custom tool manifests plug right in.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üìÑ Git-integrated workflows&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;AI-assisted diff review.&lt;/li&gt; &lt;li&gt;Commit message generation.&lt;/li&gt; &lt;li&gt;Selective staging &amp;amp; auto-commit helpers.&lt;/li&gt; &lt;li&gt;Release note generation.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;‚ö° Prompt caching and performance&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Smart local cache for repeated prompts.&lt;/li&gt; &lt;li&gt;Reduced Databricks token/compute usage.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üéØ Why I built this&lt;/h1&gt; &lt;p&gt;Databricks has become an amazing platform to host and fine-tune LLMs ‚Äî but there wasn‚Äôt a clean way to get a &lt;strong&gt;Claude-like developer agent experience&lt;/strong&gt; using custom models on Databricks.&lt;br /&gt; Lynkr fills that gap:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;You stay inside your company‚Äôs infra (compliance-friendly).&lt;/li&gt; &lt;li&gt;You choose your model (Databricks DBRX, Llama, fine-tunes, anything supported).&lt;/li&gt; &lt;li&gt;You get familiar AI coding workflows‚Ä¶ &lt;em&gt;without the vendor lock-in&lt;/em&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;üöÄ Quick start&lt;/h1&gt; &lt;p&gt;Install via npm:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;npm install -g lynkr &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Set your Databricks environment variables (token, workspace URL, model endpoint), run the proxy, and point your Claude-compatible client to the local Lynkr server.&lt;/p&gt; &lt;p&gt;Full README + instructions:&lt;br /&gt; &lt;a href="https://github.com/vishalveerareddy123/Lynkr?utm_source=chatgpt.com"&gt;https://github.com/vishalveerareddy123/Lynkr&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;üß™ Who this is for&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;Databricks users who want a full AI coding assistant tied to their own model endpoints&lt;/li&gt; &lt;li&gt;Teams that need privacy-first AI workflows&lt;/li&gt; &lt;li&gt;Developers who want repo-aware agentic tooling but must self-host&lt;/li&gt; &lt;li&gt;Anyone experimenting with building AI code agents on Databricks&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôd love feedback from anyone willing to try it out ‚Äî bugs, feature requests, or ideas for integrations.&lt;br /&gt; Happy to answer questions too!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dangerous-Dingo-5169"&gt; /u/Dangerous-Dingo-5169 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrvh8/introducing_lynkr_an_opensource_claudestyle_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrvh8/introducing_lynkr_an_opensource_claudestyle_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdrvh8/introducing_lynkr_an_opensource_claudestyle_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T05:32:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdsf7g</id>
    <title>Mac Mini M4 32gb or NVIDIA Jetson AGX Orin 64GB Developer Kit?</title>
    <updated>2025-12-04T06:02:13+00:00</updated>
    <author>
      <name>/u/Outrageous_Lab_8431</name>
      <uri>https://old.reddit.com/user/Outrageous_Lab_8431</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdsf7g/mac_mini_m4_32gb_or_nvidia_jetson_agx_orin_64gb/"&gt; &lt;img alt="Mac Mini M4 32gb or NVIDIA Jetson AGX Orin 64GB Developer Kit?" src="https://a.thumbs.redditmedia.com/OfSx2V0AEbde7QvgduM7mXfUEAe_UNzvqzQ8G1Hl040.jpg" title="Mac Mini M4 32gb or NVIDIA Jetson AGX Orin 64GB Developer Kit?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I see the NVIDIA Jetson AGX Orin 64GB is currently 50% off at $999&lt;br /&gt; and the Mac mini M4 with 32GB RAM is also around $999.&lt;/p&gt; &lt;p&gt;I want to buy one mainly to run local LLMs like Llama.&lt;br /&gt; I‚Äôm a Linux user, so I don‚Äôt care much about ease of use.&lt;br /&gt; I‚Äôm focused on performance, especially tokens per second.&lt;br /&gt; Which one should I choose?&lt;/p&gt; &lt;p&gt;Thank you&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27ruvecxn45g1.png?width=1020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59a2f0b150d90107b91c39abfc86b2393bac3956"&gt;https://preview.redd.it/27ruvecxn45g1.png?width=1020&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=59a2f0b150d90107b91c39abfc86b2393bac3956&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/64klvr33o45g1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70c9fdb3e6d02842ae4aa1afd14eed843d29a765"&gt;https://preview.redd.it/64klvr33o45g1.png?width=1142&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=70c9fdb3e6d02842ae4aa1afd14eed843d29a765&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Outrageous_Lab_8431"&gt; /u/Outrageous_Lab_8431 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdsf7g/mac_mini_m4_32gb_or_nvidia_jetson_agx_orin_64gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdsf7g/mac_mini_m4_32gb_or_nvidia_jetson_agx_orin_64gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdsf7g/mac_mini_m4_32gb_or_nvidia_jetson_agx_orin_64gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T06:02:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd1yqc</id>
    <title>Hot take: We‚Äôre overselling 'semantic search' in RAG.</title>
    <updated>2025-12-03T11:42:02+00:00</updated>
    <author>
      <name>/u/Raisin_False</name>
      <uri>https://old.reddit.com/user/Raisin_False</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been building some RAG stuff and 'semantic search' feels way more magical in marketing than in reality.&lt;/p&gt; &lt;p&gt;Embeddings are great &lt;strong&gt;fuzzy matchers in meaning space&lt;/strong&gt; - they shine on paraphrases, synonyms, 'something like this' queries. But whenever I need sharper behavior (logic, constraints, dates, 'papers using X on Y after 2019'), plain bi-encoder vector search starts to fall over unless I add extra machinery.&lt;/p&gt; &lt;p&gt;In practice my setups end up looking more like:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;BM25 or dense (or hybrid) &lt;/li&gt; &lt;li&gt;Reranker and/or LLM query rewrite &lt;/li&gt; &lt;li&gt;LLM reasoning also maybe graphs/filters&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;At that point, calling just the first stage 'semantic search' feels a bit misleading, cause it's more like 'dense/vector retrieval' plus a bunch of stuff on top that actually does the reasoning.&lt;/p&gt; &lt;p&gt;So i have 2 questions for you:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Is 'semantic search' a fair name for plain vector similarity, or do you avoid that term?&lt;/li&gt; &lt;li&gt;How far did you get with just embeddings before needing reranking / query rewriting / graphs / filters?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Raisin_False"&gt; /u/Raisin_False &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd1yqc/hot_take_were_overselling_semantic_search_in_rag/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T11:42:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlhyn</id>
    <title>Local ai for music?</title>
    <updated>2025-12-04T00:28:51+00:00</updated>
    <author>
      <name>/u/Rique_Belt</name>
      <uri>https://old.reddit.com/user/Rique_Belt</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There is the Qwen-image-editing and those fancy new VLM that can be run on local hardware without requiring crazy specs. But are there such thing for music input and output? Recently, there was two situations on which I wanted to extract only the piano melody from a song and find similar songs that starts with an specific rhythm like Supernaut from Black Sabbath which sounded really familiar. The first situation I know that there is ai for that for I have used but in that case was for vocals, but the second case I am not sure since it would require a world-knowledge and special training for that.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Rique_Belt"&gt; /u/Rique_Belt &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlhyn/local_ai_for_music/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlhyn/local_ai_for_music/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlhyn/local_ai_for_music/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:28:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqlkp</id>
    <title>Built a Python SDK for tool calling with Ollama (also has a TUI)</title>
    <updated>2025-12-04T04:26:39+00:00</updated>
    <author>
      <name>/u/jrummy16</name>
      <uri>https://old.reddit.com/user/jrummy16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqlkp/built_a_python_sdk_for_tool_calling_with_ollama/"&gt; &lt;img alt="Built a Python SDK for tool calling with Ollama (also has a TUI)" src="https://external-preview.redd.it/H_Xk72f2p-5BMENjkachQUexRhOMhSdq6pGnN1HVmBM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c0ed292d5b1727c8c0a87f5cad09195ad91ce81d" title="Built a Python SDK for tool calling with Ollama (also has a TUI)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Been running Ollama locally and got tired of writing the same boilerplate for tool calling every time. Built Consoul so I can just do:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from consoul import Consoul console = Consoul(provider=&amp;quot;ollama&amp;quot;, model=&amp;quot;llama3.2&amp;quot;) console.chat(&amp;quot;refactor this&amp;quot;, tools=True) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It handles the tool calling loop, file editing, code search, etc. Also works with Claude/GPT if you want to compare responses.&lt;/p&gt; &lt;p&gt;The TUI is actually nice too (Textual-based):&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install 'consoul[tui]' consoul tui &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Added HuggingFace tokenizers for Ollama because calling the API to count tokens was painfully slow. Now it's instant.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/goatbytes/consoul"&gt;https://github.com/goatbytes/consoul&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MIT licensed. Been using it daily, curious what breaks for others.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jrummy16"&gt; /u/jrummy16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/goatbytes/consoul"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqlkp/built_a_python_sdk_for_tool_calling_with_ollama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqlkp/built_a_python_sdk_for_tool_calling_with_ollama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:26:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqvet</id>
    <title>Question for LLM engineers: is there value in a tool that tests prompts at scale and rewrites them until they behave correctly?</title>
    <updated>2025-12-04T04:40:49+00:00</updated>
    <author>
      <name>/u/BulkyAd7044</name>
      <uri>https://old.reddit.com/user/BulkyAd7044</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want feedback from people who work with LLMs on a regular basis.&lt;/p&gt; &lt;p&gt;A lot of prompt development still feels like guesswork. Teams write a few examples, test in a playground, or keep spreadsheets. When a prompt or model changes, it is hard to know what quietly broke. Running batches of tests across multiple providers often requires custom scripts and rate limit workarounds.&lt;/p&gt; &lt;p&gt;Claude or GPT can generate a couple examples, but they do not create diverse synthetic test suites and they do not run evaluations at scale. Most developers end up tweaking prompts by hand until they feel right, even though the behavior may not be validated.&lt;/p&gt; &lt;p&gt;I am exploring whether a tool focused on synthetic test generation and multi-model evaluation would be useful. The idea is to help developers arrive at a prompt that is actually tested and predictable, not something tuned by manual trial and error. The system would generate around 100 realistic and edge-case inputs, evaluate them across models, and then automatically rewrite and refine the prompt until it performs well on the full test set.&lt;/p&gt; &lt;p&gt;Ideas I am considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Generate ~100 realistic and edge-case inputs for a prompt&lt;/li&gt; &lt;li&gt;Run those tests across GPT, Claude, Gemini, etc&lt;/li&gt; &lt;li&gt;Show where outputs diverge&lt;/li&gt; &lt;li&gt;Automatically refine the prompt based on the failures&lt;/li&gt; &lt;li&gt;Give developers more confidence that the final prompt is stable and ready to ship&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This is not a product pitch. I just want to understand the pain points.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Would a tool that generates tests and automatically improves your prompt until it performs well be useful&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BulkyAd7044"&gt; /u/BulkyAd7044 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqvet/question_for_llm_engineers_is_there_value_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqvet/question_for_llm_engineers_is_there_value_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqvet/question_for_llm_engineers_is_there_value_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:40:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdmc7w</id>
    <title>1 week update on ForgeIndex, my directory for local AI tools</title>
    <updated>2025-12-04T01:06:21+00:00</updated>
    <author>
      <name>/u/Equivalent-Ad-9798</name>
      <uri>https://old.reddit.com/user/Equivalent-Ad-9798</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone ‚Äî last week I shared ForgeIndex.ai here, a lightweight directory I‚Äôm building to make it easier to discover open-source local AI tools in one place.&lt;/p&gt; &lt;p&gt;Quick 1-week update: - The index has grown from around 30 projects to 60+ - I added hardware + OS + software requirements for most tools - Improved categories/tags for easier filtering - Fixed UI issues based on feedback - Added more demos + GitHub links - Working on a roadmap for smarter filtering (GPU-friendly, CPU-only, mobile capable, etc.)&lt;/p&gt; &lt;p&gt;The goal is to make ForgeIndex the simplest way to explore local AI tools without digging through GitHub, Reddit threads, Discords, YouTube (like me lol) or newsletters.&lt;/p&gt; &lt;p&gt;If you know projects I should add, or features you‚Äôd like to see (search filters, categories, compatibility flags etc.), let me know. Still really early, but steadily improving.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://forgeindex.ai"&gt;https://forgeindex.ai&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions or get feedback!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Equivalent-Ad-9798"&gt; /u/Equivalent-Ad-9798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdmc7w/1_week_update_on_forgeindex_my_directory_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdmc7w/1_week_update_on_forgeindex_my_directory_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdmc7w/1_week_update_on_forgeindex_my_directory_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T01:06:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdl3q0</id>
    <title>Parameters to run Deepseek R1 671b Q4</title>
    <updated>2025-12-04T00:11:46+00:00</updated>
    <author>
      <name>/u/I_like_fragrances</name>
      <uri>https://old.reddit.com/user/I_like_fragrances</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdl3q0/parameters_to_run_deepseek_r1_671b_q4/"&gt; &lt;img alt="Parameters to run Deepseek R1 671b Q4" src="https://b.thumbs.redditmedia.com/dj_ndZdqEls0WR74zJtsmcO_odY9JVi164RGExix-GE.jpg" title="Parameters to run Deepseek R1 671b Q4" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to run Deepseek R1 671b Q4, I need to offload some to RAM but every config I try it fails to load. How can I get it to load on LMStudio? I attached images of my hardware and the model parameter config options.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/I_like_fragrances"&gt; /u/I_like_fragrances &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pdl3q0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdl3q0/parameters_to_run_deepseek_r1_671b_q4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdl3q0/parameters_to_run_deepseek_r1_671b_q4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd9tgj</id>
    <title>A Technical Tour of the DeepSeek Models from V3 to V3.2</title>
    <updated>2025-12-03T17:03:17+00:00</updated>
    <author>
      <name>/u/seraschka</name>
      <uri>https://old.reddit.com/user/seraschka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt; &lt;img alt="A Technical Tour of the DeepSeek Models from V3 to V3.2" src="https://external-preview.redd.it/Oy9W7OYOeVO8Z6Sl3EWWZR-9AbREkAwoyEei1XJ7yeY.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7a5effd7f132f71b2efdd47cc12daa448023c0bf" title="A Technical Tour of the DeepSeek Models from V3 to V3.2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/seraschka"&gt; /u/seraschka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://sebastianraschka.com/blog/2025/technical-deepseek.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd9tgj/a_technical_tour_of_the_deepseek_models_from_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T17:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd5yxy</id>
    <title>My experiences with the new Ministral 3 14B Reasoning 2512 Q8</title>
    <updated>2025-12-03T14:41:03+00:00</updated>
    <author>
      <name>/u/egomarker</name>
      <uri>https://old.reddit.com/user/egomarker</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt; &lt;img alt="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" src="https://b.thumbs.redditmedia.com/YQNWxn03P5a0Q35GBj3cSIS0Oa0a8pdRn0Pkkl0sUGM.jpg" title="My experiences with the new Ministral 3 14B Reasoning 2512 Q8" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;45 minutes and 33K tokens of thinking about making html tetris (1 line prompt):&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819"&gt;https://preview.redd.it/jzjcom93105g1.png?width=500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8d67b1b895715d2dfbb927db0bc2bc485b28b819&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tool calling breaks all the time:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184"&gt;https://preview.redd.it/02edr424105g1.png?width=314&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=67cccfd1b1fdaa59da095b9bd31ef09f1ec1c184&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Also at some point it stopped using the [think] tags altogether and just started thinking out loud. I'll leave it running for a couple of hours and see if it eventually manages to build the HTML Tetris.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/egomarker"&gt; /u/egomarker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd5yxy/my_experiences_with_the_new_ministral_3_14b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T14:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd3xyp</id>
    <title>Why don't Google and Openai release their old models?</title>
    <updated>2025-12-03T13:19:51+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;GPt 4 and gemini 2 pro are dated, they should release it... Are they afraid of releasing their data and architecture? They released gemma and gpt oss already. Gemini 2 has a large context window, but the quality degrades when it gets large though and it is replicable.. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd3xyp/why_dont_google_and_openai_release_their_old/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T13:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdi8o7</id>
    <title>DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp; CPU Support</title>
    <updated>2025-12-03T22:13:40+00:00</updated>
    <author>
      <name>/u/Dogacel</name>
      <uri>https://old.reddit.com/user/Dogacel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt; &lt;img alt="DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" src="https://external-preview.redd.it/9aiVAPD5fnnzElgE74nQoRx4aoPflI7CwyCPycl2BLA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9f593757a501d49b05111d714ec689f7acf643d1" title="DeepSeek-OCR ‚Äì Apple Metal Performance Shaders (MPS) &amp;amp; CPU Support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently updated DeepSeek-OCR to support Apple Metal (MPS) and CPU acceleration. I wanted to share this in case anyone else has been looking to run it efficiently on macOS.&lt;/p&gt; &lt;p&gt;To make it easier to use, I also forked an existing desktop client and applied the patch. You can check it out here:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Dogacel/deepseek-ocr-client-macos"&gt;https://github.com/Dogacel/deepseek-ocr-client-macos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dogacel"&gt; /u/Dogacel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Dogacel/DeepSeek-OCR-Metal-MPS"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdi8o7/deepseekocr_apple_metal_performance_shaders_mps/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdk4zx</id>
    <title>[Resource] 20,000+ Pages of U.S. House Oversight Epstein Estate Docs (OCR'd &amp; Cleaned for RAG/Analysis)</title>
    <updated>2025-12-03T23:30:44+00:00</updated>
    <author>
      <name>/u/Ok-District-1330</name>
      <uri>https://old.reddit.com/user/Ok-District-1330</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve reuploaded the recent release of 20,000+ pages of documents regarding the Epstein Estate from the U.S. House Oversight Committee. The goal is to make these scattered government files accessible for journalists and researchers using open-source tools.&lt;/p&gt; &lt;p&gt;Note, this was originally shared here, by another user who's account has now been deleted, then uploaded to huggingface. The original huggingface repo has since been removed, as well as the original uploaders account. Credit for the original dataset goes to him/her. This is simply a clone, hosted on Github and my huggingface account, with a gradio app I built for interacting/searching it.&lt;/p&gt; &lt;p&gt;The original release contained mixed file formats and nested folders. This dataset converts images/PDFs to text (via Tesseract OCR) and standardizes them into a single CSV format.&lt;/p&gt; &lt;p&gt;Searchable App: A Gradio browser to search the corpus without downloading the full set.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/theelderemo/epstein-files"&gt;Hugging Face Gradio App and Repo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/theelderemo/Epstein-files"&gt;Github Mirror&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-District-1330"&gt; /u/Ok-District-1330 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdk4zx/resource_20000_pages_of_us_house_oversight/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T23:30:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd2wjt</id>
    <title>DeepSeek V3.2 Technical Report</title>
    <updated>2025-12-03T12:31:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt; &lt;img alt="DeepSeek V3.2 Technical Report" src="https://preview.redd.it/q3rjrhs0gz4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7d2e078ce099142771b5d3999cbb9670fbfc18d8" title="DeepSeek V3.2 Technical Report" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Here is a brief summary of &lt;strong&gt;key breakthroughs of DeepSeek V3.2&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;1. DeepSeek Sparse Attention (DSA)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A new efficient attention mechanism that dramatically reduces computational complexity while preserving performance in long-context scenarios. &lt;/p&gt; &lt;p&gt;It uses a lightning indexer with fine-grained top-k token selection to achieve sparse but effective attention.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;2. Scalable and Stable Reinforcement Learning Framework&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Implements a heavily scaled post-training RL pipeline, with compute exceeding 10% of pretraining cost. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;3. Large-Scale Agentic Task Synthesis Pipeline&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Provides a novel pipeline that programmatically generates large numbers of tool-use environments (1,800+ environments, 85,000+ complex prompts). &lt;/p&gt; &lt;p&gt;This boosts generalization, tool-use ability, and instruction-following in interactive settings.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;4. Unified Reasoning + Agentic RL Training&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Merges reasoning, tool-use, and human-alignment RL into a single stage rather than multi-stage pipelines. &lt;/p&gt; &lt;p&gt;This avoids catastrophic forgetting and improves cross-domain performance simultaneously.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;A high-compute variant trained with relaxed length penalties and enhanced mathematical-reasoning rewards. &lt;/p&gt; &lt;p&gt;This model even surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI).&lt;/p&gt; &lt;p&gt;&lt;a href="https://arxiv.org/abs/2512.02556"&gt;Arxiv paper &lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q3rjrhs0gz4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd2wjt/deepseek_v32_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T12:31:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pd04cn</id>
    <title>Chinese startup founded by Google engineer claims to have developed its own tpu reportedly 1.5 times faster than nvidia a100.</title>
    <updated>2025-12-03T09:51:40+00:00</updated>
    <author>
      <name>/u/Turbulent_Pin7635</name>
      <uri>https://old.reddit.com/user/Turbulent_Pin7635</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient"&gt;https://www.tomshardware.com/tech-industry/chinese-startup-founded-by-google-engineer-claims-to-have-developed-its-own-tpu-reportedly-1-5-times-faster-than-nvidias-a100-gpu-from-2020-42-percent-more-efficient&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Turbulent_Pin7635"&gt; /u/Turbulent_Pin7635 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pd04cn/chinese_startup_founded_by_google_engineer_claims/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T09:51:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdin3b</id>
    <title>The Best Open Weights Coding Models of 2025</title>
    <updated>2025-12-03T22:29:44+00:00</updated>
    <author>
      <name>/u/mr_riptano</name>
      <uri>https://old.reddit.com/user/mr_riptano</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt; &lt;img alt="The Best Open Weights Coding Models of 2025" src="https://external-preview.redd.it/wxWktbYwfvy3j0Y1BhifZGPwnHUnY7JGZEyykx4N_HM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=48a8af816e33acba1ab83ce8086f21346740c37d" title="The Best Open Weights Coding Models of 2025" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm back with uncontaminated evals for DeepSeek-V3.2, Kimi K2 Thinking, and MiniMax M2. (We caught GLM 4.6 last time around.) &lt;/p&gt; &lt;p&gt;If you just want the numbers, you can find them for the finalists &lt;a href="https://brokk.ai/power-ranking?models=dsv3.2%2Cglm4.6-fp8%2Ck2-thinking%2Cm2"&gt;here&lt;/a&gt; and for everyone else &lt;a href="https://brokk.ai/power-ranking?dataset=openround"&gt;here&lt;/a&gt;. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mr_riptano"&gt; /u/mr_riptano &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://blog.brokk.ai/the-best-open-weights-coding-models-of-2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdin3b/the_best_open_weights_coding_models_of_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T22:29:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdlfu3</id>
    <title>Frozen networks show usable early-layer intent: 1370√ó fewer FLOPs and 10√ó faster inference (code + weights)9</title>
    <updated>2025-12-04T00:26:15+00:00</updated>
    <author>
      <name>/u/anima-core</name>
      <uri>https://old.reddit.com/user/anima-core</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with whether a frozen network‚Äôs early activations contain enough ‚Äúsemantic intent‚Äù to skip most of the compute.&lt;/p&gt; &lt;p&gt;I used a standard ResNet-18 trained on CIFAR-10 (87.89 percent accuracy), pulled a single 64-dimensional vector from an early layer, and trained a tiny decoder on top of it.&lt;/p&gt; &lt;p&gt;Results on the same hardware: ‚Ä¢ 72.57 percent accuracy from that early-layer vector ‚Ä¢ ~10√ó faster real latency ‚Ä¢ 1370√ó fewer FLOPs ‚Ä¢ No pruning, distillation, quantization, early exit tricks, or sparsity ‚Ä¢ The full model stayed completely frozen&lt;/p&gt; &lt;p&gt;This means 99.93 percent of the original network‚Äôs compute was not required to recover 82.6 percent of its performance.&lt;/p&gt; &lt;p&gt;Code + one-click run script: &lt;a href="https://github.com/Anima-Core/an1-meaning-engine"&gt;https://github.com/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF demo + pretrained weights: &lt;a href="https://huggingface.co/Anima-Core/an1-meaning-engine"&gt;https://huggingface.co/Anima-Core/an1-meaning-engine&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Runs end to end on almost any GPU or CPU in a few minutes.&lt;/p&gt; &lt;p&gt;Dedicated to my late father, Asad Shamim, whose loss opened the path that led me here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/anima-core"&gt; /u/anima-core &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdlfu3/frozen_networks_show_usable_earlylayer_intent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:26:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdm268</id>
    <title>How Attention Got So Efficient [GQA/MLA/DSA]</title>
    <updated>2025-12-04T00:53:59+00:00</updated>
    <author>
      <name>/u/onil_gova</name>
      <uri>https://old.reddit.com/user/onil_gova</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt; &lt;img alt="How Attention Got So Efficient [GQA/MLA/DSA]" src="https://external-preview.redd.it/4QixmEzxJtTr5ZgAjR4FoJjK4qVPLU4zAuNo-fsPzgM.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5f8badcadc6197f760be212560f8188dc2793fa6" title="How Attention Got So Efficient [GQA/MLA/DSA]" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For anyone trying to understand why Deepseek 3.2 DSA is a milestone in terms of solving long context, I really recommend this video.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/onil_gova"&gt; /u/onil_gova &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/Y-o545eYjXM?si=pt-SxR5anfLNSN8j"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdm268/how_attention_got_so_efficient_gqamladsa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T00:53:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdfk0o</id>
    <title>Hermes 4.3 - 36B Model released</title>
    <updated>2025-12-03T20:30:22+00:00</updated>
    <author>
      <name>/u/crazeum</name>
      <uri>https://old.reddit.com/user/crazeum</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt; &lt;img alt="Hermes 4.3 - 36B Model released" src="https://external-preview.redd.it/thAQxjbw3fpc9fgR1nrJDb-3cDeZ9f7TtJWveW5lCQ4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=49b96ff1b32dfa841362b8c2a0d4449fdd83b1f0" title="Hermes 4.3 - 36B Model released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hermes uncensored line models with apache 2 license. Post trained from Seed-OSS-36B-Base on their psyche network. The cool bit is they also trained it centralized and the distributed psyche trained version outperformed the centrally trained one.&lt;/p&gt; &lt;p&gt;GGUF links: &lt;a href="https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF"&gt;https://huggingface.co/NousResearch/Hermes-4.3-36B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crazeum"&gt; /u/crazeum &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nousresearch.com/introducing-hermes-4-3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdfk0o/hermes_43_36b_model_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T20:30:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdqxbw</id>
    <title>Chinese CXMT unveils DDR5-8000 RAM</title>
    <updated>2025-12-04T04:43:35+00:00</updated>
    <author>
      <name>/u/Ok_Warning2146</name>
      <uri>https://old.reddit.com/user/Ok_Warning2146</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory"&gt;https://www.techpowerup.com/343185/chinese-cxmt-shows-homegrown-ddr5-8000-and-lpddr5x-10667-memory&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Chinese RAM might be the way to buck the trend of rising prices.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Warning2146"&gt; /u/Ok_Warning2146 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdqxbw/chinese_cxmt_unveils_ddr58000_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-04T04:43:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdcytv</id>
    <title>Micron Announces Exit from Crucial Consumer Business</title>
    <updated>2025-12-03T18:54:47+00:00</updated>
    <author>
      <name>/u/FullstackSensei</name>
      <uri>https://old.reddit.com/user/FullstackSensei</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Technically speaking, we're screwed.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullstackSensei"&gt; /u/FullstackSensei &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://investors.micron.com/news-releases/news-release-details/micron-announces-exit-crucial-consumer-business"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdcytv/micron_announces_exit_from_crucial_consumer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T18:54:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1pdh0sm</id>
    <title>8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich</title>
    <updated>2025-12-03T21:26:43+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt; &lt;img alt="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" src="https://external-preview.redd.it/ZzlmajZ6b3gzMjVnMYyMXOA9G9iEfbHd4uR1YsqLbApEsnv66h0V49mXIA5l.png?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=001f3aaa8ebe40ec05d81117c0df8ce6a792a1bd" title="8 local LLMs on a single Strix Halo debating whether a hot dog is a sandwich" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/o8n25oox325g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pdh0sm/8_local_llms_on_a_single_strix_halo_debating/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-03T21:26:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
