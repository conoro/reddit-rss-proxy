<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-19T15:05:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oar481</id>
    <title>What is currently the best model for accurately describing an image ? 19/10/2025</title>
    <updated>2025-10-19T14:31:53+00:00</updated>
    <author>
      <name>/u/Top-Diver-4606</name>
      <uri>https://old.reddit.com/user/Top-Diver-4606</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's all in the title. This post is just meant to serve as a checkpoint.&lt;/p&gt; &lt;p&gt;PS : To make it interesting, specify the associated image description category. Because basically, it's like saying which is the best LLM; you have to be specific about the task. Following your comments, I will put the top list directly in my post.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Top-Diver-4606"&gt; /u/Top-Diver-4606 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar481/what_is_currently_the_best_model_for_accurately/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar481/what_is_currently_the_best_model_for_accurately/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar481/what_is_currently_the_best_model_for_accurately/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:31:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar532</id>
    <title>Phoneme Extraction Failure When Fine-Tuning VITS TTS on Arabic Dataset</title>
    <updated>2025-10-19T14:32:54+00:00</updated>
    <author>
      <name>/u/Batman_255</name>
      <uri>https://old.reddit.com/user/Batman_255</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm fine-tuning &lt;strong&gt;VITS TTS&lt;/strong&gt; on an &lt;strong&gt;Arabic speech dataset&lt;/strong&gt; (audio files + transcriptions), and I encountered the following error during training:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;RuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument. &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;üß© What I Found&lt;/h1&gt; &lt;p&gt;After investigating, I discovered that &lt;strong&gt;all&lt;/strong&gt; &lt;code&gt;.npy&lt;/code&gt; &lt;strong&gt;phoneme cache files&lt;/strong&gt; inside &lt;code&gt;phoneme_cache/&lt;/code&gt; contain only a single integer like:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;int32: 3 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That means &lt;strong&gt;phoneme extraction failed&lt;/strong&gt;, resulting in empty or invalid token sequences.&lt;br /&gt; This seems to be the reason for the empty tensor error during alignment or duration prediction.&lt;/p&gt; &lt;p&gt;When I set:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;use_phonemes = False &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;the model starts training successfully ‚Äî but then I get warnings such as:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Character 'ÿß' not found in the vocabulary &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;(and the same for other Arabic characters).&lt;/p&gt; &lt;h1&gt;‚ùì What I Need Help With&lt;/h1&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;Why did the phoneme extraction fail?&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Is this likely related to my dataset (Arabic text encoding, unsupported characters, or missing phonemizer support)?&lt;/li&gt; &lt;li&gt;How can I fix or rebuild the phoneme cache correctly for Arabic?&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;How can I use phonemes and still avoid the&lt;/strong&gt; &lt;code&gt;min(): Expected reduction dim&lt;/code&gt; &lt;strong&gt;error?&lt;/strong&gt; &lt;ul&gt; &lt;li&gt;Should I delete and regenerate the phoneme cache after fixing the phonemizer?&lt;/li&gt; &lt;li&gt;Are there specific settings or phonemizers I should use for Arabic (e.g., &lt;code&gt;espeak&lt;/code&gt;, &lt;code&gt;mishkal&lt;/code&gt;, or &lt;code&gt;arabic-phonetiser&lt;/code&gt;)? the model automatically uses &lt;code&gt;espeak&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt; &lt;h1&gt;üß† My Current Understanding&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;code&gt;use_phonemes = True&lt;/code&gt;: converts text to phonemes (better pronunciation if it works).&lt;/li&gt; &lt;li&gt;&lt;code&gt;use_phonemes = False&lt;/code&gt;: uses raw characters directly.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Any help on:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Fixing or regenerating the phoneme cache for Arabic&lt;/li&gt; &lt;li&gt;Recommended phonemizer / model setup&lt;/li&gt; &lt;li&gt;Or confirming if this is purely a dataset/phonemizer issue&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;would be greatly appreciated!&lt;/p&gt; &lt;p&gt;Thanks in advance!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Batman_255"&gt; /u/Batman_255 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar532/phoneme_extraction_failure_when_finetuning_vits/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar532/phoneme_extraction_failure_when_finetuning_vits/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar532/phoneme_extraction_failure_when_finetuning_vits/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:32:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar5du</id>
    <title>I made a multi-provider AI coding agent</title>
    <updated>2025-10-19T14:33:16+00:00</updated>
    <author>
      <name>/u/West-Bottle9609</name>
      <uri>https://old.reddit.com/user/West-Bottle9609</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I've been building Binharic, an open-source AI coding assistant that runs in the terminal. It's entirely written in TypeScript and uses the AI SDK from Vercel for its agentic logic, including tool use and workflow management.&lt;/p&gt; &lt;p&gt;It supports models from OpenAI, Google, Anthropic, and local ones through Ollama. It has a built-in keyword-based RAG pipeline and can use external tools via the MCP. Many things about the agent are customizable, including its personality. The default persona is a Tech-Priest (from Warhammer 40k), but this can be changed.&lt;/p&gt; &lt;p&gt;Project's GitHub repo: &lt;a href="https://github.com/CogitatorTech/binharic-cli"&gt;https://github.com/CogitatorTech/binharic-cli&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/West-Bottle9609"&gt; /u/West-Bottle9609 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar5du/i_made_a_multiprovider_ai_coding_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:33:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar69q</id>
    <title>Any resource to understand LLM fine tuning/inference at a medium level to learn about temperature, quanitzation, loss functions, gpu setup?</title>
    <updated>2025-10-19T14:34:15+00:00</updated>
    <author>
      <name>/u/SnooMarzipans2470</name>
      <uri>https://old.reddit.com/user/SnooMarzipans2470</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;is there any resource you found helpful to learn LLM fine tuning at a medium level so. i can start tinkering by knowing what's happening behind the scenes? Thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SnooMarzipans2470"&gt; /u/SnooMarzipans2470 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar69q/any_resource_to_understand_llm_fine/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oar842</id>
    <title>PC hardware questions - RAM/FCLK frequency, PCIx4 wiring</title>
    <updated>2025-10-19T14:36:28+00:00</updated>
    <author>
      <name>/u/Ertata</name>
      <uri>https://old.reddit.com/user/Ertata</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I want to run an LLM locally for no great reason, it's being more of a hobby. Completely new to it. Have a couple of technical questions&lt;/p&gt; &lt;p&gt;To start with I am going to try CPU inference with Ryzen 9700x, in that case should I bother OCing memory from 6000 to 6400 MT/s and FCLK from 2000 to 2133, or it will give less increase in speed than the numbers suggest in which case I probably will not bother stressing my system&lt;/p&gt; &lt;p&gt;Second - I have 1080 (non-Ti) and looking to get a used 3090. I know the fact that bottom PCIe is wired x4 does not matter a great deal, but does it matter it is wired to chipset and not CPU directly if I were to use both cards at the same time ot it's largely the same if I am not looking to do inference all day every day? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ertata"&gt; /u/Ertata &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar842/pc_hardware_questions_ramfclk_frequency_pcix4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oar842/pc_hardware_questions_ramfclk_frequency_pcix4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oar842/pc_hardware_questions_ramfclk_frequency_pcix4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:36:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oarakk</id>
    <title>Environmental Impact</title>
    <updated>2025-10-19T14:39:20+00:00</updated>
    <author>
      <name>/u/Flaky-Werewolf-2563</name>
      <uri>https://old.reddit.com/user/Flaky-Werewolf-2563</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Trying to understand this in regard to local LLMs.&lt;/p&gt; &lt;p&gt;I recently came from a discussion in &lt;a href="/r/aiwars"&gt;r/aiwars&lt;/a&gt; where someone argued that since they run their image generation stuff locally, they &amp;quot;don't use any data centers&amp;quot; and have &amp;quot;zero environmental impact&amp;quot;.&lt;/p&gt; &lt;p&gt;Meanwhile, posts/comments like on &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mymak3/google_new_research_paper_measuring_the/"&gt;this thread&lt;/a&gt; seem to argue that 1) yes, local AI still has an environmental impact and 2) they're actually less efficient.&lt;/p&gt; &lt;p&gt;Also got into an argument about how local just isn't available to everyone, so it's totally reasonable that people go for public LLMs, and got told &amp;quot;get a better PC&amp;quot;. And learn to program apparently, because that seems necessary to get anything to work.&lt;/p&gt; &lt;p&gt;I mainly use Ollama (which everyone says is the worst apparently), and in order to use it I need to turn off every other process on my laptop, and it still crashes frequently and takes 5-10min to generate mediocre responses. I'll still use it on occasion, bust I mostly abandoned AI as &amp;quot;bad&amp;quot;, though I still have some use cases. Recently tried Kobold which doesn't seem to be working, and SillyTavern, which was apparently not local after all.&lt;/p&gt; &lt;p&gt;Otherwise I've been under the impression that privacy is a much more relevant strength for local over public.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Flaky-Werewolf-2563"&gt; /u/Flaky-Werewolf-2563 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oarakk/environmental_impact/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oarakk/environmental_impact/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oarakk/environmental_impact/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:39:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9s60k</id>
    <title>Bee-8B, "fully open 8B Multimodal LLM designed to close the performance gap with proprietary models"</title>
    <updated>2025-10-18T10:21:03+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt; &lt;img alt="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" src="https://external-preview.redd.it/Ncd1u3KqHI3q8cl_bGxSrWsYruQjV7vJ0ceKV7GWN6Y.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ae9ffa18e4d3b03e94f5458c4c02b0fc641812e8" title="Bee-8B, &amp;quot;fully open 8B Multimodal LLM designed to close the performance gap with proprietary models&amp;quot;" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Open-Bee/Bee-8B-RL"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9s60k/bee8b_fully_open_8b_multimodal_llm_designed_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T10:21:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1oarkn3</id>
    <title>I am generally impressed by iPhone 17 GPU</title>
    <updated>2025-10-19T14:50:58+00:00</updated>
    <author>
      <name>/u/Glad-Speaker3006</name>
      <uri>https://old.reddit.com/user/Glad-Speaker3006</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oarkn3/i_am_generally_impressed_by_iphone_17_gpu/"&gt; &lt;img alt="I am generally impressed by iPhone 17 GPU" src="https://external-preview.redd.it/bHkzd2twcGkwM3dmMdYUh878AlpOQDz4eE-1IUAASsN5-1iGTSJV8Fv1Kuhw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=182ac263223acd4366fff9b131c07961a7377440" title="I am generally impressed by iPhone 17 GPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen3 4B runs at ~25t/s on A19 Pro with MLX. This is a massive gain even compared with iPhone 16 pro. Energy efficiency appears to have gotten better too, as my iPhone Air did not get very hot. Finally feels like local AI is going to possible. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Glad-Speaker3006"&gt; /u/Glad-Speaker3006 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/kk520tyi03wf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oarkn3/i_am_generally_impressed_by_iphone_17_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oarkn3/i_am_generally_impressed_by_iphone_17_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:50:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa1zfp</id>
    <title>3x Price Increase on Llama API</title>
    <updated>2025-10-18T17:36:52+00:00</updated>
    <author>
      <name>/u/Player06</name>
      <uri>https://old.reddit.com/user/Player06</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This went pretty under the radar, but a few days ago the 'Meta: Llama 3 70b' model went from 0.13c/M to 0.38c/M.&lt;/p&gt; &lt;p&gt;I noticed because I run one of the apps listed in the top 10 consumers of that model (the one with the weird penguin icon). I cannot find any evidence of this online, except my openrouter bill.&lt;/p&gt; &lt;p&gt;I ditched my local inference last month because the openrouter Llama price looked so good. But now I got rug pulled.&lt;/p&gt; &lt;p&gt;Did anybody else notice this? Or am I crazy and the prices never changed? It feels unusual for a provider to bump their API prices this much.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Player06"&gt; /u/Player06 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa1zfp/3x_price_increase_on_llama_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:36:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa3u2d</id>
    <title>The size difference of gpt-oss-120b vs it's abliterated version</title>
    <updated>2025-10-18T18:48:10+00:00</updated>
    <author>
      <name>/u/iamkucuk</name>
      <uri>https://old.reddit.com/user/iamkucuk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was away from the locally hosted models, so please forgive my ignorance.&lt;/p&gt; &lt;p&gt;Here are two versions of gpt-oss-120b:&lt;/p&gt; &lt;p&gt;&lt;a href="https://ollama.com/library/gpt-oss"&gt;https://ollama.com/library/gpt-oss&lt;/a&gt;&lt;br /&gt; &lt;a href="https://ollama.com/huihui_ai/gpt-oss-abliterated"&gt;https://ollama.com/huihui_ai/gpt-oss-abliterated&lt;/a&gt;&lt;/p&gt; &lt;p&gt;As you can see, one takes 88 GB and the other takes 65 GB, and the difference shows when they are loaded as well. I thought they were both 4-bit. Would someone be able to explain where the discrepancy is coming from? And if any abliterated versions of the original model's quant occupy the same space?&lt;/p&gt; &lt;p&gt;Another question would be, I can see the GGUF versions of gpt-oss. Why would we need GGUF versions, as the model itself already is quantized?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/iamkucuk"&gt; /u/iamkucuk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa3u2d/the_size_difference_of_gptoss120b_vs_its/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T18:48:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa671p</id>
    <title>Could you recommend good LLM models for heavier stories that include NSFW content?</title>
    <updated>2025-10-18T20:19:23+00:00</updated>
    <author>
      <name>/u/Sr_M_Ghost</name>
      <uri>https://old.reddit.com/user/Sr_M_Ghost</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently using Deep Seek R2 0528, but I'd like other models that are better suited to this type of content.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sr_M_Ghost"&gt; /u/Sr_M_Ghost &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa671p/could_you_recommend_good_llm_models_for_heavier/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T20:19:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa8klx</id>
    <title>Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models</title>
    <updated>2025-10-18T21:54:34+00:00</updated>
    <author>
      <name>/u/Ryoiki-Tokuiten</name>
      <uri>https://old.reddit.com/user/Ryoiki-Tokuiten</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt; &lt;img alt="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" src="https://external-preview.redd.it/ZnUwMzR3NXV5eHZmMTNcUS_6HPKChleMDpJ0qQU2p3V675TK2MrxaOnhSll8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=568275d60bed24ec215f8d24a426e506eab3f5d7" title="Open source custom implementation of GPT-5 Pro / Gemini Deepthink now supports local models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ryoiki-Tokuiten"&gt; /u/Ryoiki-Tokuiten &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/jak8lx5uyxvf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa8klx/open_source_custom_implementation_of_gpt5_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T21:54:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaafyq</id>
    <title>3 3090's, room for one more?</title>
    <updated>2025-10-18T23:16:49+00:00</updated>
    <author>
      <name>/u/BusinessBookkeeper63</name>
      <uri>https://old.reddit.com/user/BusinessBookkeeper63</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt; &lt;img alt="3 3090's, room for one more?" src="https://preview.redd.it/4wabpxvjcyvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=abcb88de7be3c5396eac5805691a1be8d3309133" title="3 3090's, room for one more?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I am currently running 3 3090's and was thinking of adding one more. But as you can see, my case Thermaltake CTE750 Air has some free space, but not sure if it can fit another 3090.&lt;/p&gt; &lt;p&gt;I know, I know, I should have had a server rack but I was looking for a Local AI + relatively decent looking case, so this is what I landed on. The CTE 750 is big enough for 3 3090's, but not sure if I should be doing 4 given temps inside a closed case is probably going to rise quick. The third 3090 needs a custom mount and sits on the side of the case in this picture, but it rests on the intake fans and I have screwed the standing with 3 screws. I have no idea, where I could fit the 4th.&lt;/p&gt; &lt;p&gt;Any suggestions on how I could do 4 3090;s in this case or if anyone has done this before?&lt;/p&gt; &lt;p&gt;Also looking for suggestions on my cooling. Currently it has intake from bottom, front, back and sides and outtake on top only. This is somewhat based on the CTE design, but open to other suggestions. Another option, is to eventually do water cooling to save on some space and keep things cooler, but that's a project kept for December.&lt;/p&gt; &lt;p&gt;Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BusinessBookkeeper63"&gt; /u/BusinessBookkeeper63 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4wabpxvjcyvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaafyq/3_3090s_room_for_one_more/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T23:16:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa29de</id>
    <title>Drummer's Cydonia and Magidonia 24B v4.2.0</title>
    <updated>2025-10-18T17:47:49+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt; &lt;img alt="Drummer's Cydonia and Magidonia 24B v4.2.0" src="https://external-preview.redd.it/texRxv_iJ0Ni14pBUMNg-YEbpRERebh0ufaJ753mjSs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ab199da434abf76101631f6569f8bb80838d47f9" title="Drummer's Cydonia and Magidonia 24B v4.2.0" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Magidonia is Cydonia using Magistral 2509 base.&lt;/p&gt; &lt;p&gt;Magidonia variant: &lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Cydonia (Small 3.2) variant: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0"&gt;https://huggingface.co/TheDrummer/Cydonia-24B-v4.2.0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;4.2.0 is an upgrade from 4.1 in regards to creativity. Enjoy!&lt;/p&gt; &lt;p&gt;Does anyone have a base to recommend for finetuning? Waiting for GLM Air 4.6 to come out :^)&lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;By the way, Huggingface has restricted storage in my account and I'm having a harder time doing my open-source work for the community. I'll be all out of space after a few days of work thanks to their storage restriction. &lt;/p&gt; &lt;p&gt;I tried contacting them via [&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:billing@hf.co"&gt;billing@hf.co&lt;/a&gt;) but they told me to make my case to [&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;](mailto:&lt;a href="mailto:models@hf.co"&gt;models@hf.co&lt;/a&gt;) . I haven't received a response from &lt;em&gt;that&lt;/em&gt; team yet. Other employees I've reached out to recommended that I pay around $200 / mo to get the storage I need, I think.&lt;/p&gt; &lt;p&gt;At this point I believe they're not interested in giving me an exception. I got bundled up with those who upload 1T models, I guess? I'm not sure what to do next, but I might have to start deleting models. Let me know if you guys have any ideas!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Magidonia-24B-v4.2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa29de/drummers_cydonia_and_magidonia_24b_v420/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T17:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1oaqr7x</id>
    <title>How to Fine tune a LLM to give it a persona?</title>
    <updated>2025-10-19T14:16:50+00:00</updated>
    <author>
      <name>/u/Bruce_spixky</name>
      <uri>https://old.reddit.com/user/Bruce_spixky</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am trying to fine tune a LLM for a hospital but I don't know how to get started. I want it to know about my hospital details. Also, When asked &amp;quot;Who are you?&amp;quot; It must say &amp;quot;I am a Chatbot of XYZ Hospital&amp;quot; rather than saying about the base model. Can someone tell me how to do it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bruce_spixky"&gt; /u/Bruce_spixky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaqr7x/how_to_fine_tune_a_llm_to_give_it_a_persona/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oaqr7x/how_to_fine_tune_a_llm_to_give_it_a_persona/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oaqr7x/how_to_fine_tune_a_llm_to_give_it_a_persona/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T14:16:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1oapgtj</id>
    <title>I want to have a local llm server for my house - just focused on coding assistant - what would be a reasonable spec for that?</title>
    <updated>2025-10-19T13:19:49+00:00</updated>
    <author>
      <name>/u/gameguy56</name>
      <uri>https://old.reddit.com/user/gameguy56</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I don't need and am not interested in video/image generation - just want something to work with me on coding stuff. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gameguy56"&gt; /u/gameguy56 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oapgtj/i_want_to_have_a_local_llm_server_for_my_house/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T13:19:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1o9xiza</id>
    <title>dgx, it's useless , High latency</title>
    <updated>2025-10-18T14:38:06+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt; &lt;img alt="dgx, it's useless , High latency" src="https://preview.redd.it/wwroq3nbtvvf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d419416ab7812d4f7c564531795007c015a4c85f" title="dgx, it's useless , High latency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ahmad posted a tweet where DGX latency is high : &lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19"&gt;https://x.com/TheAhmadOsman/status/1979408446534398403?t=COH4pw0-8Za4kRHWa2ml5A&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wwroq3nbtvvf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o9xiza/dgx_its_useless_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T14:38:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1oafumx</id>
    <title>Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device</title>
    <updated>2025-10-19T03:51:17+00:00</updated>
    <author>
      <name>/u/phone_radio_tv</name>
      <uri>https://old.reddit.com/user/phone_radio_tv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt; &lt;img alt="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" src="https://external-preview.redd.it/1REAPLhpZknr_BaLbzQgHufo9VWmTuOWut1-PIVgTuo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3c98ea4dc27d76e781b86ebeda6b0c583cc503d" title="Own your AI: Learn how to fine-tune Gemma 3 270M and run it on-device" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/phone_radio_tv"&gt; /u/phone_radio_tv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://developers.googleblog.com/en/own-your-ai-fine-tune-gemma-3-270m-for-on-device/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oafumx/own_your_ai_learn_how_to_finetune_gemma_3_270m/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T03:51:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oa98jf</id>
    <title>Made a website to track 348 benchmarks across 188 models.</title>
    <updated>2025-10-18T22:22:42+00:00</updated>
    <author>
      <name>/u/Odd_Tumbleweed574</name>
      <uri>https://old.reddit.com/user/Odd_Tumbleweed574</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt; &lt;img alt="Made a website to track 348 benchmarks across 188 models." src="https://preview.redd.it/omjxzqi82yvf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df31289ac6e45db697b70c3a9add3e087585f736" title="Made a website to track 348 benchmarks across 188 models." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey all, I've been building a website from a while ago in which we track the benchmark results from the official papers / model cards that the labs publish. &lt;/p&gt; &lt;p&gt;I thought it would be interesting to compile everything in one place to fill in the gaps on each model release.&lt;br /&gt; All the data is open in Github and all scores have references to the original posts.&lt;/p&gt; &lt;p&gt;&lt;a href="https://llm-stats.com/benchmarks"&gt;https://llm-stats.com/benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to provide candid feedback. &lt;/p&gt; &lt;p&gt;---&lt;/p&gt; &lt;p&gt;**We don't think this is the best approach yet**. We're now building a way to replicate the results from the most interesting and useful benchmarks, but we understand that most of them haven't been created yet.&lt;/p&gt; &lt;p&gt;Current benchmarks are too simple and are not testing real capabilities. We're looking to build interesting, real world, independent benchmarks with held out data, but that can be easy to reproduce and extend.&lt;/p&gt; &lt;p&gt;Another thing we're currently doing is benchmarking across different inference providers to monitor and detect changes in quality of their service.&lt;/p&gt; &lt;p&gt;We're currently giving out up to $1k to people that want to explore ideas about new benchmarks / environments. Dm me for more information.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd_Tumbleweed574"&gt; /u/Odd_Tumbleweed574 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/omjxzqi82yvf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oa98jf/made_a_website_to_track_348_benchmarks_across_188/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-18T22:22:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakrdm</id>
    <title>Drop your underrated models you run LOCALLY</title>
    <updated>2025-10-19T08:52:22+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Preferably within the 0.2b -32b range, or MoEs up to 140b&lt;/p&gt; &lt;p&gt;I‚Äôm on a LLM downloading spree, and wanna fill up a 2tb SSD with them. &lt;/p&gt; &lt;p&gt;Can be any use case. Just make sure to mention the use case too &lt;/p&gt; &lt;p&gt;Thank you ‚úåÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakrdm/drop_your_underrated_models_you_run_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:52:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oahpmx</id>
    <title>When you have little money but want to run big models</title>
    <updated>2025-10-19T05:38:21+00:00</updated>
    <author>
      <name>/u/alok_saurabh</name>
      <uri>https://old.reddit.com/user/alok_saurabh</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt; &lt;img alt="When you have little money but want to run big models" src="https://a.thumbs.redditmedia.com/vj3j4Vjr082yd6wYwhFLXQpEt2Zp3s7yg7spOglJoq8.jpg" title="When you have little money but want to run big models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I live in India. Everything is expensive. Importers want hefty margin. Government want hefty tax. Rtx 6000 96gb which is possible to get for 7-8k usd in USA is impossible to find even for 11 lakhs(12-13k usd) in India. So we have a couple of friends 1) Juggad 2) Olx ( indian craigslists) 3) Other similar p2p sites like fb marketplace.&lt;/p&gt; &lt;p&gt;Let me show you what I built. 1) Dell T7910 - it has 7 pci slots. I can only get 5 to work. Found it on fb mp with 256 gb ddr4 2) 5 * 3090 from olx 3) 5 pci raisers amazon. These are hard to find for cheap. 4) 1300 watt additional power supply &lt;/p&gt; &lt;p&gt;There are only 4*3090 in this build 5th slot I am using for nvme extension.&lt;/p&gt; &lt;p&gt;Total cost for this build of 96gb vram is around 3.25 lakhs. ( Around 4.6k usd) This post is just for reference for those who are in a similar boat. Please understand there is a lot of difference between planning and execution. Keep +1 lakhs in hand for things that can go wrong.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/alok_saurabh"&gt; /u/alok_saurabh &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oahpmx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oahpmx/when_you_have_little_money_but_want_to_run_big/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T05:38:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1oamo0k</id>
    <title>Gemma 4</title>
    <updated>2025-10-19T10:53:39+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People are very very excited about the release of gemini 3.0 including me, but im more excited in the gemma family of models since they are based on gemini models and on top of that are open-sourced. And simce Gemini 3.0 is groundbreaking (apparently, like the pelican svg, robot svg, xbox svg, os etc tests), I am very curious about how will the gemma 4 models perform. And also, gemma 4 is going to be a big leap compared to gemma 3 coz It was based on gemini 2.0, not 2.5. So we are getting 2 genarational leaps!&lt;/p&gt; &lt;p&gt;When it will be released??&lt;/p&gt; &lt;p&gt;Gemma 1 was based on gemini 1 and was released ~1-2 months after gemini&lt;/p&gt; &lt;p&gt;Gemma 2 was based on gemini 1.5 and was released ~4 months after gemini 1.5&lt;/p&gt; &lt;p&gt;Gemma 3 was based on gemini 2 and was released ~1-2 months after gemini 2.0&lt;/p&gt; &lt;p&gt;So Gemma 4 might be released ~1-2 months after gemini 3??? Maybe???&lt;/p&gt; &lt;p&gt;What are your thoughts?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oamo0k/gemma_4/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T10:53:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1oak08e</id>
    <title>Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference</title>
    <updated>2025-10-19T08:02:23+00:00</updated>
    <author>
      <name>/u/inkberk</name>
      <uri>https://old.reddit.com/user/inkberk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt; &lt;img alt="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" src="https://a.thumbs.redditmedia.com/h4jhl1-2PSdEVtcHTb5JaJVVUfcXqSVvVdD4T8fo5L0.jpg" title="Apple M5 Max and Ultra will finally break monopoly of NVIDIA for AI interference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;According to &lt;a href="https://opendata.blender.org/benchmarks"&gt;https://opendata.blender.org/benchmarks&lt;/a&gt;&lt;br /&gt; The Apple M5 10-core GPU already scores 1732 - outperforming the M1 Ultra with 64 GPU cores.&lt;br /&gt; With simple math:&lt;br /&gt; Apple M5 Max 40-core GPU will score 7000 - that is league of M3 Ultra&lt;br /&gt; Apple M5 Ultra 80-core GPU will score 14000 on par with RTX 5090 and RTX Pro 6000! &lt;/p&gt; &lt;p&gt;Seems like it will be the best performance/memory/tdp/price deal.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/inkberk"&gt; /u/inkberk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oak08e"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oak08e/apple_m5_max_and_ultra_will_finally_break/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T08:02:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oanpdt</id>
    <title>Qwen3 Next support almost ready üéâ</title>
    <updated>2025-10-19T11:52:59+00:00</updated>
    <author>
      <name>/u/beneath_steel_sky</name>
      <uri>https://old.reddit.com/user/beneath_steel_sky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt; &lt;img alt="Qwen3 Next support almost ready üéâ" src="https://external-preview.redd.it/i7eFNEDuUciRrfCZPE4vDbbnitlKFru9a-LhPWvWNKY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c40ba30707796f926638df0347f891c8e7cb6d0c" title="Qwen3 Next support almost ready üéâ" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/beneath_steel_sky"&gt; /u/beneath_steel_sky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16095#issuecomment-3419600401"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oanpdt/qwen3_next_support_almost_ready/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T11:52:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oakwgs</id>
    <title>Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge</title>
    <updated>2025-10-19T09:01:21+00:00</updated>
    <author>
      <name>/u/igorwarzocha</name>
      <uri>https://old.reddit.com/user/igorwarzocha</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt; &lt;img alt="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" src="https://preview.redd.it/2klkt23e91wf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=017d4a00c64748e6f3b664b4a89abc3602199d49" title="Stanford just dropped 5.5hrs worth of lectures on foundational LLM knowledge" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Enjoy? &lt;a href="https://www.youtube.com/@stanfordonline/videos"&gt;https://www.youtube.com/@stanfordonline/videos&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/igorwarzocha"&gt; /u/igorwarzocha &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2klkt23e91wf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oakwgs/stanford_just_dropped_55hrs_worth_of_lectures_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-19T09:01:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
