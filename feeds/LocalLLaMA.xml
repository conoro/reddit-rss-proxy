<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-22T13:02:57+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1psyxwo</id>
    <title>Kimi K2 Thinking is the least sycophantic open-source AI, according to research by Anthropic</title>
    <updated>2025-12-22T12:58:00+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"&gt; &lt;img alt="Kimi K2 Thinking is the least sycophantic open-source AI, according to research by Anthropic" src="https://b.thumbs.redditmedia.com/iXwcJSAdgqEh3LeXTBc6oZpnQTN5MofqdRMMRSIcCJU.jpg" title="Kimi K2 Thinking is the least sycophantic open-source AI, according to research by Anthropic" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/1qpm2njj6r8g1.png?width=2293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3be1a70055147b1d283b5b49557bfd17f1a24c8"&gt;https://preview.redd.it/1qpm2njj6r8g1.png?width=2293&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c3be1a70055147b1d283b5b49557bfd17f1a24c8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It's very close to my daily experience. Kimi directly points out problems instead of flattering me.&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://alignment.anthropic.com/2025/bloom-auto-evals/"&gt;https://alignment.anthropic.com/2025/bloom-auto-evals/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psyxwo/kimi_k2_thinking_is_the_least_sycophantic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:58:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pssf6u</id>
    <title>Best local model for use with agentic coding frameworks for 3x3090 and 256GB RAM?</title>
    <updated>2025-12-22T06:19:03+00:00</updated>
    <author>
      <name>/u/Amazydayzee</name>
      <uri>https://old.reddit.com/user/Amazydayzee</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I really like Cursor, although I hit my rate limits pretty quickly each month. Then I switch to Copilot in VSCode for the rest of the month, which I don’t like so much.&lt;/p&gt; &lt;p&gt;I’d like to find some local model I can use with Roo Code or something like that. I’m aware that I could bounce around the free tier of many many agentic coding frameworks, and there are so many that it could last me all month, but I honestly want to just use something local.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Amazydayzee"&gt; /u/Amazydayzee &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pssf6u/best_local_model_for_use_with_agentic_coding/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pssf6u/best_local_model_for_use_with_agentic_coding/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pssf6u/best_local_model_for_use_with_agentic_coding/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T06:19:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1psnlm0</id>
    <title>MiniMax-M2 Q3_K_M on Quad V100 32gb llama.cpp testing NVlink</title>
    <updated>2025-12-22T02:06:37+00:00</updated>
    <author>
      <name>/u/MachineZer0</name>
      <uri>https://old.reddit.com/user/MachineZer0</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Almost a year ago I bought a server capable of four SXM2 GPUs. The catch was to hack the OCP power supply. &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/r/homelab/comments/1j0769h/comment/mf8yacv/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;https://www.reddit.com/r/homelab/comments/1j0769h/comment/mf8yacv/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt; &lt;/p&gt; &lt;p&gt;I actually did that properly on first attempt, but didn't torque the screws enough on the V100. It wouldn't boot. I didn't really trouble shoot further since I got busy. The project sat for a year as toyed around with Dual 5090, Quad 3090 and 12x MI50 32gb RPC. I got interested in the V100 again after seeing cheap adapters from China. Bought a boat load of 16gb adapter variants since they sold for a song and started putting together with Turbo adapters. Then with the V100 top of mind, I got four of the 32gb SXM2 and went back to the NVLink build.&lt;/p&gt; &lt;p&gt;tldr. Exactly as mentioned in &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/11485"&gt;how do I enable NVLink / peer transfers? · ggml-org/llama.cpp · Discussion #11485 · GitHub&lt;/a&gt;, Split mode 'row' is not optimized for NVlink.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;--split-mode row &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;About 70 tok/s pp and 20 tok/s out&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 3 | task -1 | sampler chain: logits -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist slot launch_slot_: id 3 | task 6677 | processing task slot update_slots: id 3 | task 6677 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 52 slot update_slots: id 3 | task 6677 | n_tokens = 18, memory_seq_rm [18, end) slot update_slots: id 3 | task 6677 | prompt processing progress, n_tokens = 52, batch.n_tokens = 34, progress = 1.000000 slot update_slots: id 3 | task 6677 | prompt done, n_tokens = 52, batch.n_tokens = 34 slot print_timing: id 3 | task 6677 | prompt eval time = 479.55 ms / 34 tokens ( 14.10 ms per token, 70.90 tokens per second) eval time = 310990.17 ms / 6236 tokens ( 49.87 ms per token, 20.05 tokens per second) total time = 311469.71 ms / 6270 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;--split-mode layer&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Holy crap...&lt;/p&gt; &lt;pre&gt;&lt;code&gt;slot launch_slot_: id 2 | task -1 | sampler chain: logits -&amp;gt; penalties -&amp;gt; dry -&amp;gt; top-n-sigma -&amp;gt; top-k -&amp;gt; typical -&amp;gt; top-p -&amp;gt; min-p -&amp;gt; xtc -&amp;gt; temp-ext -&amp;gt; dist slot launch_slot_: id 2 | task 273 | processing task slot update_slots: id 2 | task 273 | new prompt, n_ctx_slot = 40192, n_keep = 0, task.n_tokens = 52 slot update_slots: id 2 | task 273 | n_tokens = 15, memory_seq_rm [15, end) slot update_slots: id 2 | task 273 | prompt processing progress, n_tokens = 52, batch.n_tokens = 37, progress = 1.000000 slot update_slots: id 2 | task 273 | prompt done, n_tokens = 52, batch.n_tokens = 37 slot print_timing: id 2 | task 273 | prompt eval time = 21.97 ms / 37 tokens ( 0.59 ms per token, 1683.88 tokens per second) eval time = 167754.38 ms / 6476 tokens ( 25.90 ms per token, 38.60 tokens per second) total time = 167776.36 ms / 6513 tokens &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope one day someone decides to optimize NVlink for inference. Unless you plan to train, stick with the RTX 3090 as the SXM2 systems are still highly inflated.&lt;/p&gt; &lt;p&gt;But consider messing with $100 V100 16gb SXM2 with a $50 adapter if you can hack cooling, or a $170 turbo adapter if you want the 5min DIY to assemble.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MachineZer0"&gt; /u/MachineZer0 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psnlm0/minimaxm2_q3_k_m_on_quad_v100_32gb_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psnlm0/minimaxm2_q3_k_m_on_quad_v100_32gb_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psnlm0/minimaxm2_q3_k_m_on_quad_v100_32gb_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T02:06:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1psxr5j</id>
    <title>GLM 4.7 Frontend tests (Source: Chinese Forum）</title>
    <updated>2025-12-22T11:54:28+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"&gt; &lt;img alt="GLM 4.7 Frontend tests (Source: Chinese Forum）" src="https://b.thumbs.redditmedia.com/nxtMRcalkPFmLY1Gt5yVZ5Wfx6yToFaDf_s8dLxsNog.jpg" title="GLM 4.7 Frontend tests (Source: Chinese Forum）" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/oa8cibcjuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d77c4e2b26e2dd279169375f10e03884c475f96"&gt;https://preview.redd.it/oa8cibcjuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=4d77c4e2b26e2dd279169375f10e03884c475f96&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3seg8epkuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bbcb786d3be75c7b5a5775fedf0ce93678eee7e2"&gt;https://preview.redd.it/3seg8epkuq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=bbcb786d3be75c7b5a5775fedf0ce93678eee7e2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/nhk4ystluq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d5e5c30101e075b183da8049d13cf21edd663d1f"&gt;https://preview.redd.it/nhk4ystluq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=d5e5c30101e075b183da8049d13cf21edd663d1f&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/27vfi5k1vq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=02493ae4ba91ade53f26d2e0a33d32b1e6910deb"&gt;https://preview.redd.it/27vfi5k1vq8g1.jpg?width=1380&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=02493ae4ba91ade53f26d2e0a33d32b1e6910deb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1p8w9bogyq8g1.jpg?width=825&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd8ecebcd58a8598c477251cde98e85f006678a9"&gt;https://preview.redd.it/1p8w9bogyq8g1.jpg?width=825&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fd8ecebcd58a8598c477251cde98e85f006678a9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://linux.do/t/1350606"&gt;https://linux.do/t/1350606&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/1350292"&gt;https://linux.do/t/1350292&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/1349948"&gt;https://linux.do/t/1349948&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/1350354"&gt;https://linux.do/t/1350354&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://linux.do/t/topic/1350623"&gt;https://linux.do/t/topic/1350623&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psxr5j/glm_47_frontend_tests_source_chinese_forum/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T11:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ps6w96</id>
    <title>Dataset quality is not improving much</title>
    <updated>2025-12-21T13:46:50+00:00</updated>
    <author>
      <name>/u/rekriux</name>
      <uri>https://old.reddit.com/user/rekriux</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"&gt; &lt;img alt="Dataset quality is not improving much" src="https://external-preview.redd.it/1p_Y2zfHWGdGS1n176QenprBBks4UkO2cWuEEHp6f68.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=521a9f1988c888fe9369f5871e2a57530ec8bd94" title="Dataset quality is not improving much" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am checking public dataset often. And while we have RAG and lots of innovation posted here in &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;, there are rarely breakthrough in datasets creation. While I may be lurking in this sub, I doped out of electronics/computing and studied in other fields and obtained my master in something else, I have been dabbling with AI since 2000. So take this as a my rant. But I do hope some people will start more research on dataset quality and it's creation pipelines.&lt;/p&gt; &lt;p&gt;Buckle up (sorry for spelling, no AI proofread and quick typing)&lt;/p&gt; &lt;p&gt;From my perspectives, the most all rounder datasets for instruction following are :&lt;/p&gt; &lt;p&gt;The Tulu from Allenai [allenai/tulu-3-sft-mixture]([&lt;a href="https://huggingface.co/datasets/allenai/tulu-3-sft-mixture"&gt;https://huggingface.co/datasets/allenai/tulu-3-sft-mixture&lt;/a&gt;) The smoltakl from HG &lt;a href="https://huggingface.co/datasets/HuggingFaceTB/smoltalk2"&gt;HuggingFaceTB/smoltalk2&lt;/a&gt; Hermes 3 from NousResearch [NousResearch/Hermes-3-Dataset]([&lt;a href="https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset"&gt;https://huggingface.co/datasets/NousResearch/Hermes-3-Dataset&lt;/a&gt;)&lt;/p&gt; &lt;p&gt;That's about it. The other good dataset are those that mix other datasets for good variety. Dolphin could be good, but I found it's quality a bit lacking to be included in the above. Openherms was also good for it's time, but now it should be heavily reworked.&lt;/p&gt; &lt;p&gt;Just that ? This is kind of concerning. Every one knows the &amp;quot;**garbage in, garbage out**&amp;quot; phenomena.&lt;/p&gt; &lt;p&gt;I consider 2 dataset breakthrough : &lt;strong&gt;WizzardLM&lt;/strong&gt; and &lt;strong&gt;Magpie&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Since then, we hadn't have any great innovation in dataset or did I miss it ? Yea, deduplication and merging datasets, but that's not brilliant level and over engineered.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;Lately, NVIDIA released SFT datasets. The first one they released is behind a &amp;quot;ASK AUTH&amp;quot; to access it? Well, guess what, I was denied access.&lt;/p&gt; &lt;p&gt;Then came Nano and they gave access to the the INSTRUCT SFT:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1"&gt;nvidia/Nemotron-Instruction-Following-Chat-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So I went away and check a few examples. There are other parts of the dataset like RL pipeline, but I didn't have time to investigate further.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Nemotron&lt;/strong&gt; are a bit of hit and miss. If you tried it, sometimes it feels brilliant in solving something, then the next it feels dumb in answering something simpler. Do you get that feeling ?&lt;/p&gt; &lt;p&gt;Well I think this is related to the SFT they did in the initial stage.&lt;/p&gt; &lt;p&gt;For a quick round up of what I found :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Lots of sycophancy thanks to using GPT-OSS 120B&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;No use of **system** message&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Wasting precious resources without having the llm learn that the system prompt is prioritized over user request, soft vs hard overwrites handling, like UPPERCASE or directives that could mean priority like ALWAYS, NEVER, if... Handling opposing directives. Implementing directives as code (codeagent?) ...&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Aren't most coding agent using very long system messages to give the LLM instructions ?? Well Nemotron is missing out on training on it so there is no way that it will perform well when used by a agent that make MASSIVE list of instructions to follow.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Poor use of multi-turn conversations:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Recall of something that was used a few turns up, like initial directives (or some sort of AGENT.md)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Absence of labeling :&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Each conversation should have :&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt; instructions : the specific instructions list to be learned during this conversation instructions_types : in what major categories does those instructions fit in constraints : the .. constraints ... learned ... constraints_types : in what major categories does those constraints fit in tasks : the specific tasks asked the llm... task_type : in what type of llm task does this belong to (EDITING, CREATIVE, CODING...) skills : the specific skills that should be demonstrated ... skills_types : skills categories user_intent : what are the user intents in this conversation user_intent_categories : ... categories has_context : the user provided the context (RAG, CODE, ) inject_knowledge : this inject knowledge to the model by generating a answer from nothing (ex external source) context_type : what is it : code, rag, instruction.md, pasted text, url to fetch... domain_knowledge : what are the domains of knowledge that this touch uppon mode : are we in a chat with a user, a toolcall, a RP session, a persona (coder, writing assistant), interactive vs one shot tools_provided : did we provide tools to the llm tools_used : did the llm use the provided tools tool_summary : tools used, in what order, tool use evaluation (used right tools but many non productive and didn't use the grep tool that should have done it faster) risks : what are the risks associated with the user request risk_mitigation : what should the llm do to mitigate the risks ? disclaimer, refusal, providing multiple perspectives to the request, ignore risk as unfounded intermediary_steps : add additional steps that force the llm to produce plan of action, summary of important information, recall of what was asked the llm to do system_protection : does the system message ask for it to be protected (no leaks) system_protection\_test : did the system message leak in the assistant responses ... &lt;/code&gt;&lt;/p&gt; &lt;p&gt;The labeling of data is the only way to make sure the dataset is balanced in skills, risk management, task types and diversity of knowledge domains etc.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;How many conversations help the llm learn how to efficiently use RAG context in the conversation and make a summary, extract specific information, process it in a coherent json file ? If you don't have your dataset classified, how can you know if this is under-represented and that is why it's not performing well in **YOUR** agentic use ?&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Once you have a label dataset, it's easy to spot blind spots. Also it would be easy to test all skills, tasks, risks etc. to evaluate how it performs on more complicated evaluation set and see it some should be augmented in the dataset. This should be done regularly in training phase, **so you could balance things by finer adjustment in ratios between checkpoint snapshot.**&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;From my perspective, Nano will perform poorly in many cases just because the instruction set for initial SFT was bad. They used GPT-OSS-120B, Qwen3-235B-A22B-Thinking-2507, and Qwen3-235B-A22B-Instruct-2507 for generation, and that seems like middle of the LLM size. I would have thought that more large open models would have been used, at least for some tasks like handling multiple instructions/constraints at the same time while performing many tasks and using many skills. Also using those mid range llms, they should have time to do review of the dataset by LLMS. Just produce statistics and ask all other 400B models to evaluate your pipeline, output, reasoning in making the dataset and THEY WILL TELL YOU WHERE YOU MISSED OUT.&lt;/p&gt; &lt;p&gt;Now if you where to ask me how to enhance this dataset, I would say&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;classify it to get the idea of current state (the system, user, assistant turns)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;make a list of all large categories and plot distributions -&amp;gt; ANALYZE THIS&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;generate system messages for each conversation, starting with the user requests and looking at user_intent a) use a sort of registry to follow and adjust distribution of instructions, constraints, tasks, skills, tools, number of directives in system b) have clear identification of what this conversation is about : you are a chatbot in some company processing complaints, you are a public chat providing answers to help students, engage in roleplay (RP) with user by impersonating, you are a game master/story teller in a interactive, you are a brainstorming assistant that helps produce detailed exploration plans... c) have varying length of system msg, from 10 to 2k tokens&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Insert RAG content from ultra-fineweb, finepdf, wikipedia, recycling_the_web and ask that answer be based on that context (to prevent too much content injection (that may result in more hallucinations) and work more on skills).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;For cases where RAG is not used, this should be CREATIVE/PROBLEM_SOLVING/PLANNING types of tasks, and those tasks should be well defined in system message or in user, make sure it is&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Regenerate set % of user messages using evolve to include more instructions/constraints and complicate things a bit&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;After each change above, update the classification of the conversation, each modification to the conversation should be a json with : what to modify (system, user_#, assistant_#) and classification modification (+instruct, +constraint, +task, -mode, +mode)&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Review distribution of data, make more adjustments&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;now regenerate the answers, before each assistant turn, produce a intermediary turn, it should be like multiple agents debating about what is the task at hand, what previous information was provided, what are the specific instructions and constraints, enumerate previous conversations that may have content for this, are there any ambiguity or any information missing that could prevent making a informed decision...&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;check that it makes sens, risk management, easy answer or considered multiple angles, did the model consider ambiguity or opposing instructions/constraints... That should use the intermediary_steps.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;fix any issues in answers&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;evaluate dataset on small model with 100b token budget the model performance to check the impact of the changes to the dataset&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;hr /&gt; &lt;p&gt;My gold dataset rule :&lt;/p&gt; &lt;p&gt;Now if you just produce answers without the intermediary steps, this is just distillation and the produced model will never be any better than the reference model (in fact it will be a bit worse, because the model attention is limited and it may have missed something once, then your mode will miss it always). But if you use a few models to reason, explore, summarize, recall previous knowledge and make hypothesis, validate hypothesis beforehand and passing that condensed work to the llm before generating the answer, then you are on the way to developing unique and perhaps enhanced skills for your future model. Simple, generate a distilled response and generate a primed response using the gold intermediary step and compare the 2, you will have your answer.&lt;/p&gt; &lt;p&gt;Every assistant generation should also be checked that it respected the task, that it performed it by following the instructions and constraints, that it stayed in it's 'role' or mode...&lt;/p&gt; &lt;p&gt;This is how we could work on having SOTA datasets to rivalize those held behind closed doors.&lt;/p&gt; &lt;p&gt;Hope this inspire more research and higher quality datasets.&lt;/p&gt; &lt;p&gt;P.S. I would like if you hold datasets that can be anonymized to be shared on HG, this could contribute to more diversity.&lt;/p&gt; &lt;p&gt;Also shout out to Eric Hartford &lt;a href="https://huggingface.co/datasets/QuixiAI/VibeCoding"&gt;QuixiAI/VibeCoding&lt;/a&gt; that is trying to make a open dataset for &amp;quot;collect anonymized client ↔ server message logs from popular AI coding tools and interfaces. These logs will form the basis of an open dataset hosted on Hugging Face and GitHub.&amp;quot; So if any of you wish to contribute, please do so !&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rekriux"&gt; /u/rekriux &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/datasets/nvidia/Nemotron-Instruction-Following-Chat-v1/discussions/1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ps6w96/dataset_quality_is_not_improving_much/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T13:46:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1pswrsw</id>
    <title>What's the point of hosting open source LLM on cloud?</title>
    <updated>2025-12-22T10:56:04+00:00</updated>
    <author>
      <name>/u/Yersyas</name>
      <uri>https://old.reddit.com/user/Yersyas</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get Ollama is super popular to serve as a local driver for open source LLMs. On the other hand, do people actually self host on cloud? It sounds like a sensible idea for companies care about security.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Yersyas"&gt; /u/Yersyas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswrsw/whats_the_point_of_hosting_open_source_llm_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswrsw/whats_the_point_of_hosting_open_source_llm_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pswrsw/whats_the_point_of_hosting_open_source_llm_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T10:56:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstdnb</id>
    <title>Best General-Purpose Model - 12gb vram, 128ram</title>
    <updated>2025-12-22T07:17:22+00:00</updated>
    <author>
      <name>/u/rainegarden</name>
      <uri>https://old.reddit.com/user/rainegarden</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;^ I want a general purpose model - though sometimes being okay at coding might be useful for debugging stuff that can fit within the range of my system. It's old server parts I basically got for free, so that's why it's a titan xp and has 128gb of ddr4 ecc. Can someone point me in the right direction? Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rainegarden"&gt; /u/rainegarden &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstdnb/best_generalpurpose_model_12gb_vram_128ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:17:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1psd4er</id>
    <title>Moore Threads Unveils The Lushan Gaming &amp; Huashan AI GPUs: 15x Gaming Performance Uplift, 50x RT Boost, DX12 Ultimate Support, Launching Next Year</title>
    <updated>2025-12-21T18:18:24+00:00</updated>
    <author>
      <name>/u/Individual_Aside7554</name>
      <uri>https://old.reddit.com/user/Individual_Aside7554</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://wccftech.com/moore-threads-lushan-gaming-huashan-ai-gpus-15x-gaming-uplift-50x-rt-boost-dx12-ultimate-support/"&gt;https://wccftech.com/moore-threads-lushan-gaming-huashan-ai-gpus-15x-gaming-uplift-50x-rt-boost-dx12-ultimate-support/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Individual_Aside7554"&gt; /u/Individual_Aside7554 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd4er/moore_threads_unveils_the_lushan_gaming_huashan/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psd4er/moore_threads_unveils_the_lushan_gaming_huashan/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psd4er/moore_threads_unveils_the_lushan_gaming_huashan/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T18:18:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1psw3ae</id>
    <title>Chroma DB's weak Open Source commitment</title>
    <updated>2025-12-22T10:12:38+00:00</updated>
    <author>
      <name>/u/Primary-Lake7507</name>
      <uri>https://old.reddit.com/user/Primary-Lake7507</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chroma's sparse vector search feature (and hybrid search) are only available on Chroma Cloud. This fact is &amp;quot;sparsly&amp;quot; documented and in direct violation of &lt;a href="https://docs.trychroma.com/cloud/getting-started"&gt;their stated open source committments&lt;/a&gt;:&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Under the hood, it's the exact same Apache 2.0–licensed Chroma—no forks, no divergence, just the open-source engine running at scale.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is such a key feature these days that I would call it a bait-and-switch.&lt;/p&gt; &lt;p&gt;Avoid Chroma, opt for stronger open source alternatives like pgvector or Qdrant.&lt;/p&gt; &lt;p&gt;FYI: I originally made &lt;a href="https://www.reddit.com/r/Rag/comments/1psvu1m/chroma_dbs_open_core_baitandswitch/"&gt;a post&lt;/a&gt; over on &lt;a href="/r/Rag"&gt;r/Rag&lt;/a&gt; showing how Chroma DB's open source committment have become very weak. Since there have been &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1gdqlw7/i_tested_what_small_llms_1b3b_can_actually_do/"&gt;popular posts here utilizing Chroma&lt;/a&gt;, I thought it might be helpful for you guys as well.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Primary-Lake7507"&gt; /u/Primary-Lake7507 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw3ae/chroma_dbs_weak_open_source_commitment/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw3ae/chroma_dbs_weak_open_source_commitment/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psw3ae/chroma_dbs_weak_open_source_commitment/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T10:12:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1psy387</id>
    <title>Qwen3-235B-W4A16 is S tier</title>
    <updated>2025-12-22T12:12:42+00:00</updated>
    <author>
      <name>/u/Sero_x</name>
      <uri>https://old.reddit.com/user/Sero_x</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best model I’ve tried that can fit on my 200GB VRAM is surprisingly this Qwen model.&lt;/p&gt; &lt;p&gt;It is able to navigate complex puzzles, and agentic environments with grace, it’s a good coder, and extremely fast.&lt;/p&gt; &lt;p&gt;I am even comparing this to remotely hosted FP16 models like GLM-4.6 &lt;/p&gt; &lt;p&gt;This might be old news but I wrote of Qwen since I typically dislike how locked down their models feel. Hope this is helpful. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sero_x"&gt; /u/Sero_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy387/qwen3235bw4a16_is_s_tier/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy387/qwen3235bw4a16_is_s_tier/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psy387/qwen3235bw4a16_is_s_tier/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:12:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pslzv6</id>
    <title>Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2</title>
    <updated>2025-12-22T00:48:25+00:00</updated>
    <author>
      <name>/u/biet_roi</name>
      <uri>https://old.reddit.com/user/biet_roi</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"&gt; &lt;img alt="Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2" src="https://external-preview.redd.it/J8tu8oCcj88QnyOGI0-nxwGZljaEo8sk3VorQPTha8k.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c7b7531b384cf525c3c5142217cba6a53acbdba0" title="Revibe is a Rust-rewrite of Mistral Vibe written by Devstral 2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/locallama"&gt;r/locallama&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;This was my project to evaluate Devstral 2 since it's free right now. Overall, I thought it did pretty well! The CLI it made is totally usable and has a bit better performance than the original when actively agenting (not that it really matters since it'll likely be dwarfed by the model). I usually prefer tools like this to be in rust though since it's the language I work in daily.&lt;/p&gt; &lt;p&gt;Unfortunately, the 120b devstral is too big &amp;amp; slow for my hardware, but I might try to finetune the 24b. I hope Mistral and other labs will continue releasing open code models :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/biet_roi"&gt; /u/biet_roi &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/nicksenger/revibe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pslzv6/revibe_is_a_rustrewrite_of_mistral_vibe_written/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T00:48:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1pse7w6</id>
    <title>It ain’t much, but proud of my 2x3090 + a spare 3060 for support</title>
    <updated>2025-12-21T19:03:54+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"&gt; &lt;img alt="It ain’t much, but proud of my 2x3090 + a spare 3060 for support" src="https://b.thumbs.redditmedia.com/d_4ZORlCqJiugxvoE3WiLbJ7wAjH7oBiwe8Id8UqYgY.jpg" title="It ain’t much, but proud of my 2x3090 + a spare 3060 for support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It’s a bit tight, but it fits and I didn’t want to buy a new case just yet. I had a spare computer that I bought first 1x3090, and now a 2nd 3090.&lt;/p&gt; &lt;p&gt;Qwen3-Next-80b is great!&lt;/p&gt; &lt;p&gt;Trying to wrap my head around Clint and using it in VS Code, but still not working properly…&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pse7w6"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pse7w6/it_aint_much_but_proud_of_my_2x3090_a_spare_3060/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T19:03:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1pswyjm</id>
    <title>Spent weekend tuning LLM server to hone my nerdism so you don't have to.</title>
    <updated>2025-12-22T11:07:28+00:00</updated>
    <author>
      <name>/u/ChopSticksPlease</name>
      <uri>https://old.reddit.com/user/ChopSticksPlease</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"&gt; &lt;img alt="Spent weekend tuning LLM server to hone my nerdism so you don't have to." src="https://a.thumbs.redditmedia.com/RkN2WJzc5mi4jYKiq6rriMSd4lwO6wttk0O6KbsEPV0.jpg" title="Spent weekend tuning LLM server to hone my nerdism so you don't have to." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Art:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1rdwk3yykq8g1.jpg?width=2494&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=562c0dcecf89a3227a2627572e902afca5384bfb"&gt;https://preview.redd.it/1rdwk3yykq8g1.jpg?width=2494&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=562c0dcecf89a3227a2627572e902afca5384bfb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tl;dr; I've spent some time setting up local AI server with various models for chat and agentic coding in VS code + Cline. The goal was to replace Ollama with llama.cpp and squeeze as much performance as I can from the hardware (Dual RTX 3090 + CPU). The llama-swap configuration with llama.cpp command and options and some extra information is here in the repo: &lt;a href="https://github.com/cepa/llama-nerd"&gt;https://github.com/cepa/llama-nerd&lt;/a&gt;&lt;/p&gt; &lt;p&gt;You can consider this a sample or a reference, it should work if you have 48+ GB of VRAM but you can scale it up or down by adjusting quant and context size in most models.&lt;/p&gt; &lt;p&gt;I guess that config may be helpful for some of you who want to ditch ollama for good.&lt;/p&gt; &lt;p&gt;The Artist:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kdikr0zgmq8g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c500bd772de3ea9be6e8f1f47d542fcf45d2611"&gt;https://preview.redd.it/kdikr0zgmq8g1.jpg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=6c500bd772de3ea9be6e8f1f47d542fcf45d2611&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The llama-swap config:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# llama-swap-config.yaml # Hardware: # Dell T7910 # GPU: 2x NVIDIA RTX 3090 (Total 48GB VRAM) # CPU: 2x Intel(R) Xeon(R) CPU E5-2673 v4 @ 2.30GHz (2 Sockets x 40 Cores) # RAM: 256GB DDR4 # Virtual Machine: # OS: Ubuntu + Nvidia CUDA Drivers # vCPU: 40 Cores # RAM: 64GB # GPU: 2x NVIDIA RTX 3090 (48GB VRAM) (PCIe Passthrough) # Disk: 1TB NVMe (PCIe Passthrough) # NUMA: To DISABLE NUMA, the VM is pinned to physical CPU0 with 64GB RAM and both GPUs. models: # --------------------------------------------------------------------------- # Coding models # --------------------------------------------------------------------------- # https://huggingface.co/unsloth/Seed-OSS-36B-Instruct-GGUF # https://huggingface.co/magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF # Q6_K_XL with quantized 96k context size to fit in the 48GB VRAM for speed Seed-OSS-36B-Instruct-UD-Q5_K_XL: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Seed-OSS-36B-Instruct-UD-Q5_K_XL.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 1.1 --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - seed-oss # https://docs.unsloth.ai/models/qwen3-coder-how-to-run-locally Qwen3-Coder-30B-A3B-Instruct-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 0.2 --min-p 0.0 --top-p 0.90 --top-k 20 --repeat-penalty 1.05 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - qwen3-coder Qwen2.5-Coder-32B-Instruct-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen2.5-Coder-32B-Instruct-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 0.2 --min-p 0.0 --top-p 0.90 --top-k 20 --repeat-penalty 1.05 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - qwen2.5-coder # https://docs.unsloth.ai/models/devstral-2 Devstral-Small-2-24B-Instruct-2512-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf --mmproj /models/mmproj-Devstral-Small-2-24B-Instruct-2512-F16.gguf --n-gpu-layers 999 --ctx-size 131072 --jinja --temp 0.15 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - devstral-small-2 # Devstral is a dense model, 123b works 2tps or less. #Devstral-2-123B-Instruct-2512-IQ4_XS: # cmd: &amp;gt; # llama-server --port ${PORT} # --model /models/Devstral-2-123B-Instruct-2512-IQ4_XS-00001-of-00002.gguf # --n-gpu-layers 58 # --ctx-size 32768 # --jinja # --temp 0.15 # --cache-type-k q4_0 # --cache-type-v q4_0 # https://docs.unsloth.ai/models/nemotron-3 Nemotron-3-Nano-30B-A3B-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Nemotron-3-Nano-30B-A3B-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --jinja --temp 0.6 --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on aliases: - nemotron-3-nano # --------------------------------------------------------------------------- # SOTA Models # --------------------------------------------------------------------------- # https://docs.unsloth.ai/models/gpt-oss-how-to-run-and-fine-tune # dont use cache quant, seems to impact performance # performance: 25..30tps gpt-oss-120b: cmd: &amp;gt; llama-server --port ${PORT} --model /models/gpt-oss-120b-MXFP4_MOE.gguf --n-gpu-layers 999 --ctx-size 65536 --flash-attn on -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --threads -1 --temp 1.0 --min-p 0.0 --top-p 1.0 --top-k 0.0 --chat-template-kwargs &amp;quot;{\&amp;quot;reasoning_effort\&amp;quot;: \&amp;quot;high\&amp;quot;}&amp;quot; # https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally # For q8_0 cache, the max is 64k context size # For iq4_nl cache, the max is 128k context size GLM-4.5-Air-IQ4_XS: cmd: &amp;gt; llama-server --port ${PORT} --model /models/GLM-4.5-Air-IQ4_XS-00001-of-00002.gguf --jinja --n-gpu-layers 999 -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --ctx-size 65536 --temp 1.0 --min-p 0.0 --top-p 0.95 --top-k 40 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/glm-4.6-how-to-run-locally # With mmproj and iq4_nl, the max is 32k context size but slow # With mmproj and q4_0, the max is 16k context size but is affected by the uploaded image size GLM-4.6V-IQ4_XS: cmd: &amp;gt; llama-server --port ${PORT} --model /models/GLM-4.6V-IQ4_XS-00001-of-00002.gguf --mmproj /models/mmproj-GLM-4.6V-F16.gguf --jinja --n-gpu-layers 999 -ot &amp;quot;.ffn_(up)_exps.=CPU&amp;quot; --ctx-size 32768 --temp 1.0 --min-p 0.0 --top-p 0.95 --top-k 40 --cache-type-k iq4_nl --cache-type-v iq4_nl --flash-attn on # https://docs.unsloth.ai/models/qwen3-next Qwen3-Next-80B-A3B-Thinking-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-Next-80B-A3B-Thinking-Q4_K_M.gguf --n-gpu-layers 999 --n-cpu-moe 2 --ctx-size 65536 --cache-type-k q8_0 --cache-type-v q8_0 --temp 0.6 --min-p 0.0 --top-p 0.80 --top-k 20 --flash-attn on # https://docs.unsloth.ai/models/qwen3-next Qwen3-Next-80B-A3B-Instruct-Q4_K_M: cmd: | llama-server --port ${PORT} --model /models/Qwen3-Next-80B-A3B-Instruct-Q4_K_M.gguf --n-gpu-layers 999 --n-cpu-moe 2 --ctx-size 65536 --temp 0.7 --min-p 0.0 --top-p 0.8 --top-k 20 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/qwen3-vl-how-to-run-and-fine-tune Qwen3-VL-30B-A3B-Instruct-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-VL-30B-A3B-Instruct-Q8_0.gguf --mmproj /models/mmproj-Qwen3-VL-30B-A3B-Instruct-f16.gguf --n-gpu-layers 999 --ctx-size 81920 --top-p 0.8 --top-k 20 --temp 0.7 --min-p 0.0 --presence-penalty 1.5 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/qwen3-vl-how-to-run-and-fine-tune Qwen3-VL-30B-A3B-Thinking-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen3-VL-30B-A3B-Thinking-Q8_0.gguf --mmproj /models/mmproj-Qwen3-VL-30B-A3B-Thinking-f16.gguf --n-gpu-layers 999 --ctx-size 81920 --top-p 0.95 --top-k 20 --temp 1.0 --min-p 0.0 --presence-penalty 0.0 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # --------------------------------------------------------------------------- # Legacy Models # --------------------------------------------------------------------------- QwQ-32B-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/QwQ-32B-Q8_0.gguf --n-gpu-layers 999 --ctx-size 65536 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on Qwen2.5-72B-Instruct-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Qwen2.5-72B-Instruct-Q4_K_M.gguf --n-gpu-layers 81 --ctx-size 16384 --cache-type-k q4_0 --cache-type-v q4_0 --flash-attn on DeepSeek-R1-Distill-Llama-70B-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/DeepSeek-R1-Distill-Llama-70B-Q4_K_M.gguf --n-gpu-layers 999 --ctx-size 32768 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on Llama-3.3-70B-Instruct-Q4_K_M: cmd: &amp;gt; llama-server --port ${PORT} --model /models/Llama-3.3-70B-Instruct-Q4_K_M.gguf --n-gpu-layers 999 --ctx-size 32768 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on # https://docs.unsloth.ai/models/gemma-3-how-to-run-and-fine-tune gemma-3-27b-it-Q8_0: cmd: &amp;gt; llama-server --port ${PORT} --model /models/gemma-3-27b-it-Q8_0.gguf --n-gpu-layers 999 --ctx-size 131072 --temp 1.0 --repeat-penalty 1.0 --min-p 0.01 --top-k 64 --top-p 0.95 --cache-type-k q8_0 --cache-type-v q8_0 --flash-attn on &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Hope you like it :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ChopSticksPlease"&gt; /u/ChopSticksPlease &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pswyjm/spent_weekend_tuning_llm_server_to_hone_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T11:07:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1psy0ag</id>
    <title>Local RAG with small models with hallucination mitigation</title>
    <updated>2025-12-22T12:08:03+00:00</updated>
    <author>
      <name>/u/ljubobratovicrelja</name>
      <uri>https://old.reddit.com/user/ljubobratovicrelja</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy0ag/local_rag_with_small_models_with_hallucination/"&gt; &lt;img alt="Local RAG with small models with hallucination mitigation" src="https://external-preview.redd.it/YTFtcnJ1a2t3cThnMS9Ei5brJy4BdsT9diAS7Z-bKUm9Q2FZKYFGqvrqb-vS.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b78e5c7d415421f97b463e11d575e588641cefc9" title="Local RAG with small models with hallucination mitigation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;I started this as a personal project, aiming to build something fully customizable and suitable to my needs, allowing me to study technical documentation, books and scientific articles locally, privately - therefore allowing me to include larger contexts on proprietary docs within my work. However, I was genuinely surprised by how well it worked, so I decided to make it public and share it here.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; - I built a custom RAG for locally deployed small models, tuned to warn about lack of context and hence mitigate hallucination, when prompted too wide.&lt;/p&gt; &lt;p&gt;Here's the project: &lt;a href="https://github.com/ljubobratovicrelja/tensor-truth"&gt;https://github.com/ljubobratovicrelja/tensor-truth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The video here shows it in action. Just a brief example, loading a single book (Convex Optimization by Boyd and Vandenberghe), asking something I know is inside - model answering comfortably, with clear citing from where it got the info. However, after a couple of prompts, asking something I know the book doesn't cover - backpropagation and ML optimization basis - the model admits the limitation of the context plainly. In the following prompt, I load the book I know it covers this topic (Mathematics for Machine Learning), ask it to now revise its sources and try to answer previous question, and it does so successfully.&lt;/p&gt; &lt;p&gt;I'll be honest - I'm a computer vision engineer with limited LLM experience, so when I started this I tried existing tools like AnythingLLM and Open WebUI first. They're great for basic RAG, but I couldn't get the level of control I needed - specifically around confidence thresholds, synthesis behavior when context is missing, and the ability to dynamically update context and re-evaluate answers. So I ended up building this with streamlit and llama-index, tailoring it as I saw fit.&lt;/p&gt; &lt;p&gt;Limitations:&lt;/p&gt; &lt;p&gt;As it's well known, a small model in a RAG system will nicely fetch the correct info from precisely tuned prompt when context is available, but as soon as it has to put some &amp;quot;glue&amp;quot; between multiple sources, it's prone to hallucination. The confidence threshold warning should pop-up in the UI, but more importantly, some prompt engineering helps a lot - for e.g. focusing the prompt to &amp;quot;list available info&amp;quot;, rather than &amp;quot;tell me about it&amp;quot;, and later on asking it to elaborate on specific topics it listed and cited.&lt;/p&gt; &lt;p&gt;Technical details:&lt;/p&gt; &lt;p&gt;Uses hierarchical node parsing (2048-&amp;gt;512-&amp;gt;256 chunks for papers and smaller docs, 3072-&amp;gt;768-&amp;gt;384 for books) + auto-merging retrieval + BGE reranking with similarity cutoff. I guess this is a standard pipeline, however I tuned the system to aid the synthesizer response and warn within the UI when context is not available within assigned confidence thresholds.&lt;/p&gt; &lt;p&gt;Anyhow, I hope you find this useful, and please, by all means - comment away. I am very happy to receive all kinds of feedback, and learn from you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ljubobratovicrelja"&gt; /u/ljubobratovicrelja &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/cvrl5akkwq8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psy0ag/local_rag_with_small_models_with_hallucination/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psy0ag/local_rag_with_small_models_with_hallucination/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:08:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1psyqha</id>
    <title>upstage/Solar-Open-100B · Hugging Face</title>
    <updated>2025-12-22T12:47:16+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"&gt; &lt;img alt="upstage/Solar-Open-100B · Hugging Face" src="https://external-preview.redd.it/KGZNZzWd5K6vM05pplkzhPrPxHhbMAP1w-s6MnLTkhM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=559420850549bc9f134f2c1908f869fba5d48dbf" title="upstage/Solar-Open-100B · Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...do you remember &lt;a href="https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0"&gt;https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0&lt;/a&gt; from 2024?&lt;/p&gt; &lt;p&gt;It looks like they have something new:&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#solar-open"&gt;&lt;/a&gt;Solar Open&lt;/h1&gt; &lt;p&gt;&lt;strong&gt;Solar Open&lt;/strong&gt; is Upstage's flagship &lt;strong&gt;102B-parameter&lt;/strong&gt; large language model, trained &lt;strong&gt;entirely from scratch&lt;/strong&gt; and released under the &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (see &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;). As a &lt;strong&gt;Mixture-of-Experts (MoE)&lt;/strong&gt; architecture, it delivers enterprise-grade performance in reasoning, instruction-following, and agentic capabilities—all while prioritizing transparency and customization for the open-source community.&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#highlights"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MoE Architecture (102B / 12B):&lt;/strong&gt; Built on a Mixture-of-Experts architecture with &lt;strong&gt;102B total / 12B active parameters&lt;/strong&gt;. This design delivers the knowledge depth of a massive model with the inference speed and cost-efficiency of a much smaller model.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massive Training Scale:&lt;/strong&gt; Pre-trained on &lt;strong&gt;19.7 trillion tokens&lt;/strong&gt;, ensuring broad knowledge coverage and robust reasoning capabilities across various domains.&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B#model-overview"&gt;&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Model Overview&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Model Name:&lt;/strong&gt; Solar Open 100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Hugging Face ID:&lt;/strong&gt; Upstage/Solar-Open-100B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; Mixture-of-Experts (MoE) &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Total Parameters:&lt;/strong&gt; 102.6B&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Active Parameters:&lt;/strong&gt; 12B (per token)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Experts:&lt;/strong&gt; 129 Experts (top 8 among 128 Routed + 1 Shared)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pre-training Tokens:&lt;/strong&gt; 19.7 Trillion&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context Length:&lt;/strong&gt; 128k&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Training Hardware:&lt;/strong&gt; NVIDIA B200 GPUs&lt;/li&gt; &lt;li&gt;&lt;strong&gt;License:&lt;/strong&gt; &lt;strong&gt;Solar-Apache License 2.0&lt;/strong&gt; (See &lt;a href="https://huggingface.co/upstage/Solar-Open-100B/blob/main/LICENSE"&gt;LICENSE&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/upstage/Solar-Open-100B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psyqha/upstagesolaropen100b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T12:47:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1psv6uv</id>
    <title>~1.8× peak throughput for Kimi K2 with EAGLE3 draft model</title>
    <updated>2025-12-22T09:14:33+00:00</updated>
    <author>
      <name>/u/yzlnew</name>
      <uri>https://old.reddit.com/user/yzlnew</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all,&lt;/p&gt; &lt;p&gt;we’ve released &lt;strong&gt;Kimi-K2-Instruct-eagle3&lt;/strong&gt;, an &lt;strong&gt;EAGLE3 draft model&lt;/strong&gt; intended to be used with &lt;strong&gt;Kimi-K2-Instruct&lt;/strong&gt; for speculative decoding.&lt;/p&gt; &lt;p&gt;Model link: &lt;a href="https://huggingface.co/AQ-MedAI/Kimi-K2-Instruct-eagle3"&gt;https://huggingface.co/AQ-MedAI/Kimi-K2-Instruct-eagle3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Kimi-K2-Instruct-eagle3&lt;/strong&gt; is a specialized draft model designed to accelerate the inference of the Kimi-K2-Instruct ecosystem using the &lt;strong&gt;EAGLE3&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Kimi-K2-Instruct with EAGLE3 achieves up to &lt;strong&gt;1.8× peak throughput&lt;/strong&gt; versus the base model, accelerating generation across all 7 benchmarks—from +24% on MT-Bench to +80% on Math500 (configured with bs=8, steps=3, topk=1, num_draft_tokens=4).&lt;/p&gt; &lt;p&gt;More performance details in the link above. Hopefully this is useful — even if getting Kimi-K2 running locally comes with a bit of pain/cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/yzlnew"&gt; /u/yzlnew &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psv6uv/18_peak_throughput_for_kimi_k2_with_eagle3_draft/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psv6uv/18_peak_throughput_for_kimi_k2_with_eagle3_draft/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psv6uv/18_peak_throughput_for_kimi_k2_with_eagle3_draft/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T09:14:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1psh1w2</id>
    <title>1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec.</title>
    <updated>2025-12-21T21:04:59+00:00</updated>
    <author>
      <name>/u/jd_3d</name>
      <uri>https://old.reddit.com/user/jd_3d</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"&gt; &lt;img alt="1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec." src="https://preview.redd.it/fkz64bswfm8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=173d94496a9f94631434a2e6b566db19eb11ba40" title="1 year later and people are still speedrunning NanoGPT. Last time this was posted the WR was 8.2 min. Its now 127.7 sec." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1gmd1a8/are_people_speedrunning_training_gpts_now/"&gt;Previous post&lt;/a&gt; for context. Also note original NanoGPT run from Andrej Karpathy was 45 min. I think this is a great way to understand progress in overall algorithmic speed improvements as I'm sure the big labs are using similar speedup tricks. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jd_3d"&gt; /u/jd_3d &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/fkz64bswfm8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psh1w2/1_year_later_and_people_are_still_speedrunning/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T21:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstuyv</id>
    <title>MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo...</title>
    <updated>2025-12-22T07:48:01+00:00</updated>
    <author>
      <name>/u/BlackRice_hmz</name>
      <uri>https://old.reddit.com/user/BlackRice_hmz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"&gt; &lt;img alt="MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo..." src="https://external-preview.redd.it/ZmtlNnAwcnZtcDhnMbRwrbZjXgs5PA7MM0agSvimAWH_bh1Ie65E3MD0QPIx.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9b343b6e2cdec93caf72ec2c4750c9d49b84ff91" title="MiniMax M2.1 is a straight up beast at UI/UX design. Just saw this demo..." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seriously, I didn't expect MiniMax M2.1 to be this cracked at design. Just saw this post on X (link below) and the UI it generated looks incredibly clean.&lt;/p&gt; &lt;p&gt;Also noticed the vLLM PR for it was just merged, so it’s officially coming. If it can actually code and design like this consistently, I'm switching.&lt;/p&gt; &lt;p&gt;Link to the tweet 👉 &lt;a href="https://x.com/CloudTrader4/status/2002729591451054127"&gt;https://x.com/CloudTrader4/status/2002729591451054127&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlackRice_hmz"&gt; /u/BlackRice_hmz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x7el31rvmp8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstuyv/minimax_m21_is_a_straight_up_beast_at_uiux_design/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:48:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstaoo</id>
    <title>Got me a 32GB RTX 4080 Super</title>
    <updated>2025-12-22T07:12:05+00:00</updated>
    <author>
      <name>/u/Spooknik</name>
      <uri>https://old.reddit.com/user/Spooknik</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"&gt; &lt;img alt="Got me a 32GB RTX 4080 Super" src="https://b.thumbs.redditmedia.com/skSb77iJra6hgwff6TIBJmnw4nMo-uZGyL0qiLRJ9hs.jpg" title="Got me a 32GB RTX 4080 Super" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is maybe slightly off topic, but since people ask about hardware here a lot. &lt;/p&gt; &lt;p&gt;I took a risk and bought a modified RTX 4080 Super from the Chinese market for around 1200 USD / 1000 EUR. Which for me because I live in Europe, the cheapest RTX 5090 I can find is around 2500 USD / 2100 EUR. &lt;/p&gt; &lt;p&gt;It's maybe not the best card for price per GB of VRAM considering the RTX 3090 is dropping a lot, but 32GB on one card for about half the price of a 5090 is nice. I do a lot of Diffusion model stuff, so it's great for that too. &lt;/p&gt; &lt;p&gt;It works with the stock Nvidia driver, no messing around, it was just literally plug and play. Card seems really good quality, metal back plate and metal case. Fan sounds like a small jet engine. &lt;/p&gt; &lt;p&gt;But running it around a month now and zero issues at all. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Spooknik"&gt; /u/Spooknik &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pstaoo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstaoo/got_me_a_32gb_rtx_4080_super/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:12:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1psbx2q</id>
    <title>llama.cpp appreciation post</title>
    <updated>2025-12-21T17:28:24+00:00</updated>
    <author>
      <name>/u/hackiv</name>
      <uri>https://old.reddit.com/user/hackiv</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"&gt; &lt;img alt="llama.cpp appreciation post" src="https://preview.redd.it/asipaua1el8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=87eeb0e85f39e765b810e9ec58e5148346cc419b" title="llama.cpp appreciation post" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/hackiv"&gt; /u/hackiv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/asipaua1el8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psbx2q/llamacpp_appreciation_post/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-21T17:28:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1psw818</id>
    <title>Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks</title>
    <updated>2025-12-22T10:21:02+00:00</updated>
    <author>
      <name>/u/Delicious_Focus3465</name>
      <uri>https://old.reddit.com/user/Delicious_Focus3465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/"&gt; &lt;img alt="Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks" src="https://external-preview.redd.it/aTgxeDFoN2hlcThnMS3fkoPAQ79Dr1Rhop5Txa3dHZ8j7rKJLXSjpsKe44ta.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ec62c0ae7606f90a6ff429f79fbdce528fe005e8" title="Jan-v2-VL-Max: A 30B multimodal model outperforming Gemini 2.5 Pro and DeepSeek R1 on execution-focused benchmarks" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, this is Bach from the Jan team.&lt;/p&gt; &lt;p&gt;We’re releasing Jan-v2-VL-max, a 30B multimodal model built for long-horizon execution.&lt;/p&gt; &lt;p&gt;Jan-v2-VL-max outperforms DeepSeek R1 and Gemini 2.5 Pro on the Illusion of Diminishing Returns benchmark, which measures execution length.&lt;/p&gt; &lt;p&gt;Built on Qwen3-VL-30B-A3B-Thinking, Jan-v2-VL-max scales the Jan-v2-VL base model to 30B parameters and applies LoRA-based RLVR to improve stability and reduce error accumulation across many-step executions.&lt;/p&gt; &lt;p&gt;The model is available on &lt;a href="https://chat.jan.ai/"&gt;https://chat.jan.ai/&lt;/a&gt;, a public interface built on Jan Server. We host the platform ourselves for now so anyone can try the model in the browser. We're going to release the latest Jan Server repo soon.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Try the model here: &lt;a href="https://chat.jan.ai/"&gt;https://chat.jan.ai/&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Run the model locally: &lt;a href="https://huggingface.co/janhq/Jan-v2-VL-max-FP8"&gt;https://huggingface.co/janhq/Jan-v2-VL-max-FP8&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can serve the model locally with vLLM (vLLM 0.12.0, transformers 4.57.1). FP8 inference is supported via llm-compressor, with production-ready serving configs included. It's released under the Apache-2.0 license.&lt;/p&gt; &lt;p&gt;&lt;a href="https://chat.jan.ai/"&gt;https://chat.jan.ai/&lt;/a&gt; doesn't replace Jan Desktop. It complements it by giving the community a shared environment to test larger Jan models.&lt;/p&gt; &lt;p&gt;Happy to answer your questions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Delicious_Focus3465"&gt; /u/Delicious_Focus3465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/80qbna7heq8g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psw818/janv2vlmax_a_30b_multimodal_model_outperforming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T10:21:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1psuy8g</id>
    <title>GLM 4.7 IS COMING!!!</title>
    <updated>2025-12-22T08:59:22+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Zhipu’s next-generation model, GLM-4.7, is about to be released! We are now opening Early Access Beta Permissions specifically for our long-term supporters. We look forward to your feedback we work together to make the GLM model even better!&lt;/p&gt; &lt;p&gt;As the latest flagship of the GLM series, &lt;strong&gt;GLM-4.7 features enhanced coding capabilities, long-range task planning, and tool orchestration specifically optimized for Agentic Coding scenarios. It has already achieved leading performance among open-source models across multiple public benchmarks&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;This Early Access Beta aims to collect feedback from &amp;quot;real-world development scenarios&amp;quot; to continuously improve the model's coding ability, engineering comprehension, and overall user experience.&lt;/p&gt; &lt;p&gt;📌 &lt;strong&gt;Testing Key Points&lt;/strong&gt;:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Freedom of Choice: Feel free to choose the tech stack and development scenarios you are familiar with (e.g., developing from scratch, refactoring, adding features, fixing bugs, etc.).&lt;/li&gt; &lt;li&gt;Focus Areas:Pay attention to code quality, instruction following, and whether the intermediate reasoning/processes meet your expectations.&lt;/li&gt; &lt;li&gt;• Authenticity: There is no need to intentionally cover every type of task; prioritize your actual, real-world usage scenarios.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;⏰ &lt;strong&gt;Beta Period: December 22, 2025 – Official Release&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Feedback Channels: For API errors or integration issues, you can provide feedback directly within the group. If you encounter results that do not meet expectations, please post a &amp;quot;Topic&amp;quot; (including the date, prompt, tool descriptions, expected vs. actual results, and attached local logs). Other developers can brainstorm with you, and our algorithm engineers and architects will be responding to your queries!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current early access form only available for Chinese user&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1psuy8g/glm_47_is_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T08:59:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1pstlas</id>
    <title>major open-source releases this year</title>
    <updated>2025-12-22T07:30:46+00:00</updated>
    <author>
      <name>/u/sahilypatel</name>
      <uri>https://old.reddit.com/user/sahilypatel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt; &lt;img alt="major open-source releases this year" src="https://preview.redd.it/wynfuvk9kp8g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=763bc1a7f949dc4ff18c4a976a10f017205abb54" title="major open-source releases this year" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sahilypatel"&gt; /u/sahilypatel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wynfuvk9kp8g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pstlas/major_opensource_releases_this_year/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-22T07:30:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We’re the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We’re excited to be here to talk all things SAM (sorry, we can’t share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;EDIT: Thanks to everyone who joined the AMA and for all the great conversation. We look forward to the next one!&lt;/strong&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
