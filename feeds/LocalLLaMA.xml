<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-09T12:48:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1osdhon</id>
    <title>Looking for a LLM that is close to gpt 4 for writing or RP</title>
    <updated>2025-11-09T07:48:19+00:00</updated>
    <author>
      <name>/u/Intrepid-Biscotti912</name>
      <uri>https://old.reddit.com/user/Intrepid-Biscotti912</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;Quick question: with 288GB of VRAM, what kind of models could I realistically run? I won‚Äôt go into all the hardware details, but it‚Äôs a Threadripper setup with 256GB of system RAM.&lt;/p&gt; &lt;p&gt;I know it might sound like a basic question, but the biggest I‚Äôve run locally so far was a 13B model using a 3080 and a 4060 Ti. I‚Äôm still pretty new to running local models only tried a couple so far and I‚Äôm just looking for something that works well as a solid all-around model, or maybe a few I can switch between depending on what I‚Äôm doing. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intrepid-Biscotti912"&gt; /u/Intrepid-Biscotti912 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdhon/looking_for_a_llm_that_is_close_to_gpt_4_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdhon/looking_for_a_llm_that_is_close_to_gpt_4_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osdhon/looking_for_a_llm_that_is_close_to_gpt_4_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:48:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1osi4kg</id>
    <title>Vision capabilities in medical and handwritten OCR for Gemini 2.5 Pro vs Gemini 2.5 Flash</title>
    <updated>2025-11-09T12:28:13+00:00</updated>
    <author>
      <name>/u/Extra-Designer9333</name>
      <uri>https://old.reddit.com/user/Extra-Designer9333</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I'm working on medical image analysis application that involves OCR, API cost is a sensitive and important for me, does anyone have experience with comparing 2.5 pro vs flash in the OCR medical domain.&lt;/p&gt; &lt;p&gt;Any experience shared will be appreciatedüôè&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Extra-Designer9333"&gt; /u/Extra-Designer9333 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osi4kg/vision_capabilities_in_medical_and_handwritten/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osi4kg/vision_capabilities_in_medical_and_handwritten/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osi4kg/vision_capabilities_in_medical_and_handwritten/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:28:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1os187j</id>
    <title>My Dual MBP setup for offline LLM coding (w/ Qwen3 Coder 30B A3B)</title>
    <updated>2025-11-08T21:35:45+00:00</updated>
    <author>
      <name>/u/bobaburger</name>
      <uri>https://old.reddit.com/user/bobaburger</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;People here often tout about dual GPUs. And here I am, showing my dual Macbooks setup :P jk jk, stay with me, don't laugh.&lt;/p&gt; &lt;p&gt;The setup:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;M2 Max macbook, with 64GB unified memory for serving LLM via LMStudio&lt;/li&gt; &lt;li&gt;M1 Pro macbook, with 16GB unified memory (doesn't matter), as a client, running Claude Code&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The model I'm using is Qwen3 Coder 30B A3B, Q8 MLX (temp = 0.1, repeat penalty = 1.05, top k = 20, context size = 51200). To my surprise, both the code quality and the stability in Claude Code was so good. &lt;/p&gt; &lt;p&gt;I've been trying 32B models for coding previously when QwQ 32 and Qwen2.5 Coder was still around, and none of them work. With Qwen3, it makes me feel like we finally have some actual-useful offline model that I can be happy working with.&lt;/p&gt; &lt;p&gt;Now back to the dual MBP setup, you may ask, why? The main thing is the 64GB MBP, running in clam shell and its only job is for the LLM inference, not doing anything else, so I can ultilize a bit more memory for the Q8 quant instead of Q4.&lt;/p&gt; &lt;p&gt;You can see in the below screenshot, it takes 27GB memory to sit idle with the model loaded, and 47GB during generation.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/fTxdDRO.png"&gt;https://i.imgur.com/fTxdDRO.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The 2nd macbook is unneccesary, it's just something I have at hand. I can use Claude Code on my phone or a Pi if needed.&lt;/p&gt; &lt;p&gt;Now, on inference performance: If I just chat in LMStudio with Qwen3 Coder, it run really fast. But with Claude Code's fatty system prompt, it took about 2 to 3 seconds for prompt processing per request (not so bad), and token generation was about 56 tok/s, pretty much comfortable to use.&lt;/p&gt; &lt;p&gt;On Qwen3 Coder performance: My main workflow is ask Claude Code to perform some search in the codebase, and answer some of my questions, Qwen3 did very good on this, answer quality usually on par with other frontier LLMs in Cursor. Then I'll write a more detailed instruction for the task and let it edit the code, I find that, the more detailed my prompt, the better Qwen3 generate the code.&lt;/p&gt; &lt;p&gt;The only down side is Claude Code's websearch won't work with this setup. But it can be solved by using MCP, i'm also not relying on web search in CC that much.&lt;/p&gt; &lt;p&gt;When I need to move off the work laptop, I don't know if I want to build a custom PC with a dedicated GPU or just go with a mini PC with unified memory, getting over 24GB VRAM with a dedicated GPU will be costly.&lt;/p&gt; &lt;p&gt;I also heard people say 32B dense model works better than A3B, but slower. I think I will try it at some point, but for now, I'm feel quite comfortable with this setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bobaburger"&gt; /u/bobaburger &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os187j/my_dual_mbp_setup_for_offline_llm_coding_w_qwen3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os187j/my_dual_mbp_setup_for_offline_llm_coding_w_qwen3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os187j/my_dual_mbp_setup_for_offline_llm_coding_w_qwen3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T21:35:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1osigbo</id>
    <title>My Startup Got Mention in First Blog Post ( No Promotion )</title>
    <updated>2025-11-09T12:45:03+00:00</updated>
    <author>
      <name>/u/DarkEngine774</name>
      <uri>https://old.reddit.com/user/DarkEngine774</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys I am Very Excited as My Project Tool-Neuron has now reached new height, as First New Blog Post has created for Tool-Neuron If You Guys like Our app, please try to share it, as more people can join the movement, and i might also get new devs to work with Again Thank You All For Your Support link :: &lt;a href="https://www.nickintheloop.com/post/your-ai-hub-in-your-pocket-the-story-behind-toolneuron"&gt;https://www.nickintheloop.com/post/your-ai-hub-in-your-pocket-the-story-behind-toolneuron&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DarkEngine774"&gt; /u/DarkEngine774 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osigbo/my_startup_got_mention_in_first_blog_post_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osigbo/my_startup_got_mention_in_first_blog_post_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osigbo/my_startup_got_mention_in_first_blog_post_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:45:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1osihy5</id>
    <title>TTS not working in Open-WebUi</title>
    <updated>2025-11-09T12:47:17+00:00</updated>
    <author>
      <name>/u/SailAway1798</name>
      <uri>https://old.reddit.com/user/SailAway1798</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osihy5/tts_not_working_in_openwebui/"&gt; &lt;img alt="TTS not working in Open-WebUi" src="https://b.thumbs.redditmedia.com/HwCuqsKyX32GWiGC1YPlO7BUNzZAIVZhC3ubouxFDDM.jpg" title="TTS not working in Open-WebUi" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have just installed ollama and open-webui in a stock with portainer + nginx proxy manager.&lt;br /&gt; It is awesome so far trying different models. The default STT is working (faster-whisper base model)&lt;/p&gt; &lt;p&gt;Idk how to make the TTS work. I tried the OpenAI engine with Openedai but that did not work at all.&lt;br /&gt; I tried the Transformers (Local) with different models or even leaving a blank but no luck what so ever. It just keep loading like that. &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/fsncjqhr980g1.png?width=659&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f50da6f3525884d179755d576f77ceefda133b5"&gt;https://preview.redd.it/fsncjqhr980g1.png?width=659&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7f50da6f3525884d179755d576f77ceefda133b5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I have already googled, asked ChatGPT, Claud, GoogleAi. Nothing helps.&lt;/p&gt; &lt;p&gt;This is my settings in Open-WebUi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/781ph3p7880g1.png?width=970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97ce88175a6b6a65af9a39774385a5a3462d3f42"&gt;https://preview.redd.it/781ph3p7880g1.png?width=970&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=97ce88175a6b6a65af9a39774385a5a3462d3f42&lt;/a&gt;&lt;/p&gt; &lt;p&gt;PLS Help me'. I have spent more than tow days on this. I am a rookie trying to learn so feel free to give me some advice or stuff to try out. Thank you in advanced!&lt;/p&gt; &lt;p&gt;The log of Open-WebUi container:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;pre&gt;&lt;code&gt; File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py&amp;quot;, line 144, in coro await self.app(scope, receive_or_disconnect, send_no_error) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py&amp;quot;, line 182, in __call__ with recv_stream, send_stream, collapse_excgroups(): File &amp;quot;/usr/local/lib/python3.11/contextlib.py&amp;quot;, line 158, in __exit__ self.gen.throw(typ, value, traceback) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/_utils.py&amp;quot;, line 85, in collapse_excgroups raise exc File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py&amp;quot;, line 184, in __call__ response = await self.dispatch_func(request, call_next) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/app/backend/open_webui/main.py&amp;quot;, line 1256, in dispatch response = await call_next(request) ^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py&amp;quot;, line 159, in call_next raise app_exc File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/base.py&amp;quot;, line 144, in coro await self.app(scope, receive_or_disconnect, send_no_error) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette_compress/ __init__ .py&amp;quot;, line 92, in __call__ return await self._zstd(scope, receive, send) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette_compress/_zstd_legacy.py&amp;quot;, line 100, in __call__ await self.app(scope, receive, wrapper) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/middleware/exceptions.py&amp;quot;, line 63, in __call__ await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py&amp;quot;, line 53, in wrapped_app raise exc File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py&amp;quot;, line 42, in wrapped_app await app(scope, receive, sender) File &amp;quot;/usr/local/lib/python3.11/site-packages/fastapi/middleware/asyncexitstack.py&amp;quot;, line 18, in __call__ await self.app(scope, receive, send) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/routing.py&amp;quot;, line 716, in __call__ await self.middleware_stack(scope, receive, send) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/routing.py&amp;quot;, line 736, in app await route.handle(scope, receive, send) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/routing.py&amp;quot;, line 290, in handle await self.app(scope, receive, send) File &amp;quot;/usr/local/lib/python3.11/site-packages/fastapi/routing.py&amp;quot;, line 123, in app await wrap_app_handling_exceptions(app, request)(scope, receive, send) File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py&amp;quot;, line 53, in wrapped_app raise exc File &amp;quot;/usr/local/lib/python3.11/site-packages/starlette/_exception_handler.py&amp;quot;, line 42, in wrapped_app await app(scope, receive, sender) File &amp;quot;/usr/local/lib/python3.11/site-packages/fastapi/routing.py&amp;quot;, line 109, in app response = await f(request) ^^^^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/fastapi/routing.py&amp;quot;, line 387, in app raw_response = await run_endpoint_function( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/fastapi/routing.py&amp;quot;, line 288, in run_endpoint_function return await dependant.call(**values) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/app/backend/open_webui/routers/audio.py&amp;quot;, line 544, in speech load_speech_pipeline(request) File &amp;quot;/app/backend/open_webui/routers/audio.py&amp;quot;, line 325, in load_speech_pipeline request.app.state.speech_speaker_embeddings_dataset = load_dataset( ^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/datasets/load.py&amp;quot;, line 1392, in load_dataset builder_instance = load_dataset_builder( ^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/datasets/load.py&amp;quot;, line 1132, in load_dataset_builder dataset_module = dataset_module_factory( ^^^^^^^^^^^^^^^^^^^^^^^ File &amp;quot;/usr/local/lib/python3.11/site-packages/datasets/load.py&amp;quot;, line 1031, in dataset_module_factory raise e1 from None File &amp;quot;/usr/local/lib/python3.11/site-packages/datasets/load.py&amp;quot;, line 989, in dataset_module_factory raise RuntimeError(f&amp;quot;Dataset scripts are no longer supported, but found {filename}&amp;quot;) RuntimeError: Dataset scripts are no longer supported, but found cmu-arctic-xvectors.py 2025-11-09 12:20:50.966 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/?page=1 HTTP/1.1&amp;quot; 200 2025-11-09 12:21:09.796 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:21:16.970 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:21:24.967 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:21:33.463 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/?page=1 HTTP/1.1&amp;quot; 200 2025-11-09 12:21:33.472 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/?page=1 HTTP/1.1&amp;quot; 200 2025-11-09 12:21:33.479 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/?page=1 HTTP/1.1&amp;quot; 200 2025-11-09 12:21:38.927 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/all/tags HTTP/1.1&amp;quot; 200 2025-11-09 12:21:38.928 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/05a0cb14-7d84-4f4a-a21b-766f7f2061ee HTTP/1.1&amp;quot; 200 2025-11-09 12:21:38.939 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/all/tags HTTP/1.1&amp;quot; 200 2025-11-09 12:21:38.948 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /api/v1/chats/all/tags HTTP/1.1&amp;quot; 200 2025-11-09 12:22:09.798 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:22:17.967 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:22:24.969 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:23:09.817 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:23:24.966 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:24:09.847 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:24:24.963 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:24:35.043 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:25:09.815 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:25:35.055 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:26:09.826 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:26:24.962 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:26:35.069 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:27:09.836 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:27:24.964 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:27:35.085 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:28:09.846 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:28:35.098 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:29:09.958 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:29:24.960 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 2025-11-09 12:29:35.106 | INFO | uvicorn.protocols.http.httptools_impl:send:476 - MyDomainName:0 - &amp;quot;GET /_app/version.json HTTP/1.1&amp;quot; 200 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;I am using 2x Mi50 32GB. HDD for the data and NVMe the models and the cache.&lt;/p&gt; &lt;p&gt;The yaml file of both Ollama and Open-WebUi:&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;version: '3.8'&lt;/p&gt; &lt;p&gt;networks:&lt;/p&gt; &lt;p&gt;ai:&lt;/p&gt; &lt;p&gt;driver: bridge&lt;/p&gt; &lt;p&gt;nginx_proxy:&lt;/p&gt; &lt;p&gt;name: nginx_proxy_manager_default&lt;/p&gt; &lt;p&gt;external: true&lt;/p&gt; &lt;p&gt;services:&lt;/p&gt; &lt;p&gt;ollama:&lt;/p&gt; &lt;p&gt;image: ollama/ollama:rocm&lt;/p&gt; &lt;p&gt;container_name: ollama&lt;/p&gt; &lt;p&gt;restart: unless-stopped&lt;/p&gt; &lt;p&gt;ports:&lt;/p&gt; &lt;p&gt;- &amp;quot;11434:11434&amp;quot;&lt;/p&gt; &lt;p&gt;devices:&lt;/p&gt; &lt;p&gt;# Only MI50 GPUs - excluding iGPU (renderD130)&lt;/p&gt; &lt;p&gt;- /dev/kfd&lt;/p&gt; &lt;p&gt;- /dev/dri/card1&lt;/p&gt; &lt;p&gt;- /dev/dri/card2&lt;/p&gt; &lt;p&gt;- /dev/dri/renderD128&lt;/p&gt; &lt;p&gt;- /dev/dri/renderD129&lt;/p&gt; &lt;p&gt;volumes:&lt;/p&gt; &lt;p&gt;# Store Ollama models&lt;/p&gt; &lt;p&gt;- /home/sam/nvme/ai/ollama:/root/.ollama&lt;/p&gt; &lt;p&gt;environment:&lt;/p&gt; &lt;p&gt;# MI50 is GFX906 architecture&lt;/p&gt; &lt;p&gt;- HSA_OVERRIDE_GFX_VERSION=9.0.6&lt;/p&gt; &lt;p&gt;- ROCR_VISIBLE_DEVICES=0,1&lt;/p&gt; &lt;p&gt;- OLLAMA_KEEP_ALIVE=30m&lt;/p&gt; &lt;p&gt;group_add:&lt;/p&gt; &lt;p&gt;- video&lt;/p&gt; &lt;p&gt;ipc: host&lt;/p&gt; &lt;p&gt;networks:&lt;/p&gt; &lt;p&gt;- ai&lt;/p&gt; &lt;p&gt;open-webui:&lt;/p&gt; &lt;p&gt;image: &lt;a href="http://ghcr.io/open-webui/open-webui:main"&gt;ghcr.io/open-webui/open-webui:main&lt;/a&gt;&lt;/p&gt; &lt;p&gt;container_name: open-webui&lt;/p&gt; &lt;p&gt;restart: unless-stopped&lt;/p&gt; &lt;p&gt;ports:&lt;/p&gt; &lt;p&gt;- &amp;quot;3000:8080&amp;quot;&lt;/p&gt; &lt;p&gt;volumes:&lt;/p&gt; &lt;p&gt;- /home/sam/nvme/ai/open-webui/cache:/app/backend/data/cache&lt;/p&gt; &lt;p&gt;- /home/sam/data/ai/open-webui:/app/backend/data&lt;/p&gt; &lt;p&gt;environment:&lt;/p&gt; &lt;p&gt;- OLLAMA_BASE_URL=http://ollama:11434&lt;/p&gt; &lt;p&gt;- WEBUI_SECRET_KEY=${WEBUI_SECRET_KEY}&lt;/p&gt; &lt;p&gt;networks:&lt;/p&gt; &lt;p&gt;- ai&lt;/p&gt; &lt;p&gt;- nginx_proxy&lt;/p&gt; &lt;p&gt;depends_on:&lt;/p&gt; &lt;p&gt;- ollama&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SailAway1798"&gt; /u/SailAway1798 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osihy5/tts_not_working_in_openwebui/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osihy5/tts_not_working_in_openwebui/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osihy5/tts_not_working_in_openwebui/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:47:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1orter8</id>
    <title>Added Kimi-K2-Thinking to the UGI-Leaderboard</title>
    <updated>2025-11-08T16:17:13+00:00</updated>
    <author>
      <name>/u/DontPlanToEnd</name>
      <uri>https://old.reddit.com/user/DontPlanToEnd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"&gt; &lt;img alt="Added Kimi-K2-Thinking to the UGI-Leaderboard" src="https://preview.redd.it/9kogdlk5620g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9be57e7458c6ad9fc1ebe6278bbb2b316a0b4ba9" title="Added Kimi-K2-Thinking to the UGI-Leaderboard" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard"&gt;https://huggingface.co/spaces/DontPlanToEnd/UGI-Leaderboard&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DontPlanToEnd"&gt; /u/DontPlanToEnd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9kogdlk5620g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orter8/added_kimik2thinking_to_the_ugileaderboard/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:17:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1os6t6w</id>
    <title>Does AMD AI Max 395+ have 8 channel memory like image says it does?</title>
    <updated>2025-11-09T01:46:48+00:00</updated>
    <author>
      <name>/u/Terminator857</name>
      <uri>https://old.reddit.com/user/Terminator857</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os6t6w/does_amd_ai_max_395_have_8_channel_memory_like/"&gt; &lt;img alt="Does AMD AI Max 395+ have 8 channel memory like image says it does?" src="https://b.thumbs.redditmedia.com/xqmXGN5zLIVYxe4EJLFo0UHDSu-Puo3d7ebE6ZV7FFQ.jpg" title="Does AMD AI Max 395+ have 8 channel memory like image says it does?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/elo13whkz40g1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85bdd43ce8b5876cf1e9ae27c34501947cf0b696"&gt;https://preview.redd.it/elo13whkz40g1.png?width=1500&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=85bdd43ce8b5876cf1e9ae27c34501947cf0b696&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Source: &lt;a href="https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395"&gt;https://www.bosgamepc.com/products/bosgame-m5-ai-mini-desktop-ryzen-ai-max-395&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quote: Onboard 8-channel LPDDR5X RAM clocked at 8000MHz.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Terminator857"&gt; /u/Terminator857 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os6t6w/does_amd_ai_max_395_have_8_channel_memory_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os6t6w/does_amd_ai_max_395_have_8_channel_memory_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os6t6w/does_amd_ai_max_395_have_8_channel_memory_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T01:46:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1orlqmh</id>
    <title>Honey we shrunk MiniMax M2</title>
    <updated>2025-11-08T10:04:06+00:00</updated>
    <author>
      <name>/u/arjunainfinity</name>
      <uri>https://old.reddit.com/user/arjunainfinity</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt; &lt;img alt="Honey we shrunk MiniMax M2" src="https://external-preview.redd.it/ehjUFeUe3VAS2s784qCwiz1JNp5TRqMKwWsfk9mVQNM.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a556feed177eef7b3a5785bd56ea678c06d8ea4e" title="Honey we shrunk MiniMax M2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, we pruned MiniMax M2 from 250B to 192B (~25%) with only ~5% loss in coding quality. We did this with $200 worth of 8XH200 compute. Our 50% pruned model is ETA 5 more days. Would love to hear your feedback and would you want a 50% pruned Kimi K2 Thinking?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/arjunainfinity"&gt; /u/arjunainfinity &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/VibeStudio/thrift"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orlqmh/honey_we_shrunk_minimax_m2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T10:04:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1os2756</id>
    <title>AMD R9700: yea or nay?</title>
    <updated>2025-11-08T22:17:19+00:00</updated>
    <author>
      <name>/u/regional_chumpion</name>
      <uri>https://old.reddit.com/user/regional_chumpion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;RDNA4, 32GB VRAM, decent bandwidth. Is rocm an option for local inference with mid-sized models or Q4 quantizations?&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Item&lt;/th&gt; &lt;th align="left"&gt;Price&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;a href="https://www.newegg.com/asrock-creator-r9700-ct-radeon-ai-pro-r9700-32gb-graphics-card/p/N82E16814930143?item=N82E16814930143&amp;amp;utm_campaign=snc-reddit-_-sr-_-14-930-143-_-11082025&amp;amp;utm_medium=social&amp;amp;utm_source=reddit"&gt;ASRock Creator Radeon AI Pro R9700 R9700 CT 32GB 256-bit GDDR6 PCI Express 5.0 x16 Graphics Card&lt;/a&gt;&lt;/td&gt; &lt;td align="left"&gt;$1,299.99&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/regional_chumpion"&gt; /u/regional_chumpion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os2756/amd_r9700_yea_or_nay/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os2756/amd_r9700_yea_or_nay/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os2756/amd_r9700_yea_or_nay/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T22:17:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1os9sl6</id>
    <title>7900 XT vs 9070 XT (16 vs 20GB vram)</title>
    <updated>2025-11-09T04:17:12+00:00</updated>
    <author>
      <name>/u/the926</name>
      <uri>https://old.reddit.com/user/the926</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Both look to be similarly priced at this time.. I am looking to refresh my pc and want to do a little hobby level stuff with LLMs, image generation, etc. &lt;/p&gt; &lt;p&gt;As of now Im planning to stay AMD but I am wondering if I should go with the 20GB 7900XT model vs the 16GB 9070 XT, knowing that the 7900 XT is an older card and I will miss out on some improvments with the 9070 XT. &lt;/p&gt; &lt;p&gt;Thank in advance for any info or opinions. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/the926"&gt; /u/the926 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os9sl6/7900_xt_vs_9070_xt_16_vs_20gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os9sl6/7900_xt_vs_9070_xt_16_vs_20gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os9sl6/7900_xt_vs_9070_xt_16_vs_20gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T04:17:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1oshrpo</id>
    <title>What am I doing wrong?</title>
    <updated>2025-11-09T12:08:42+00:00</updated>
    <author>
      <name>/u/Suomi422</name>
      <uri>https://old.reddit.com/user/Suomi422</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oshrpo/what_am_i_doing_wrong/"&gt; &lt;img alt="What am I doing wrong?" src="https://b.thumbs.redditmedia.com/IY5jPY3l-KQi_kkgr-zHPycoyNWLhZc0E8f6MJpjx8Y.jpg" title="What am I doing wrong?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Suomi422"&gt; /u/Suomi422 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oshrpo"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oshrpo/what_am_i_doing_wrong/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oshrpo/what_am_i_doing_wrong/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:08:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ordgys</id>
    <title>We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?</title>
    <updated>2025-11-08T02:15:28+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt; &lt;img alt="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" src="https://preview.redd.it/qfahc43zzxzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1975edd06526787672ea84d9ae1d9904b84715e1" title="We got this, we can do it! When is the REAP‚Äôd iQ_001_XXS GGUF dropping?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/qfahc43zzxzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ordgys/we_got_this_we_can_do_it_when_is_the_reapd_iq_001/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T02:15:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1osi8m7</id>
    <title>Trying to break into open-source LLMs in 2 months ‚Äî need roadmap + hardware advice</title>
    <updated>2025-11-09T12:34:15+00:00</updated>
    <author>
      <name>/u/Expert-Highlight-538</name>
      <uri>https://old.reddit.com/user/Expert-Highlight-538</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt; &lt;p&gt;I‚Äôve been working as a full-stack dev and mostly using closed-source LLMs (OpenAI, Anthropic etc) just RAG and prompting nothing deep. Lately I‚Äôve been super interested in the open-source side (Llama, Mistral, Ollama, vLLM etc) and want to actually learn how to do fine-tuning, serving, optimizing and all that.&lt;/p&gt; &lt;p&gt;Found The Smol Training Playbook from Hugging Face (that ~220-page guide to training world-class LLMs) it looks awesome but also a bit over my head right now. Trying to figure out what I should learn first before diving into it.&lt;/p&gt; &lt;p&gt;My setup: ‚Ä¢ Ryzen 7 5700X3D ‚Ä¢ RTX 2060 Super (8GB VRAM) ‚Ä¢ 32 GB DDR4 RAM I‚Äôm thinking about grabbing a used 3090 to play around with local models.&lt;/p&gt; &lt;p&gt;So I‚Äôd love your thoughts on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;A rough 2-month roadmap to get from ‚Äújust prompting‚Äù ‚Üí ‚Äúactually building and fine-tuning open models.‚Äù&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;What technical skills matter most for employability in this space right now.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Any hardware or setup tips for local LLM experimentation.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;And what prereqs I should hit before tackling the Smol Playbook.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Appreciate any pointers, resources or personal tips as I'm trying to go all in for the next two months.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Expert-Highlight-538"&gt; /u/Expert-Highlight-538 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osi8m7/trying_to_break_into_opensource_llms_in_2_months/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osi8m7/trying_to_break_into_opensource_llms_in_2_months/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osi8m7/trying_to_break_into_opensource_llms_in_2_months/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T12:34:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1orsdd9</id>
    <title>Meta‚Äôs AI hidden debt</title>
    <updated>2025-11-08T15:35:16+00:00</updated>
    <author>
      <name>/u/Ok-Breakfast-4676</name>
      <uri>https://old.reddit.com/user/Ok-Breakfast-4676</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"&gt; &lt;img alt="Meta‚Äôs AI hidden debt" src="https://preview.redd.it/a6susixny10g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3e81cc9161b2f3c3a8b79ad468df241b09f83883" title="Meta‚Äôs AI hidden debt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Meta‚Äôs hidden AI debt&lt;/p&gt; &lt;p&gt;Meta has parked $30B in AI infra debt off its balance sheet using SPVs the same financial engineering behind Enron and ‚Äô08.&lt;/p&gt; &lt;p&gt;Morgan Stanley sees tech firms needing $800B in private-credit SPVs by 2028. UBS says AI debt is growing $100B/quarter, raising red flags.&lt;/p&gt; &lt;p&gt;This isn‚Äôt dot-com equity growth it‚Äôs hidden leverage. When chips go obsolete in 3 years instead of 6, and exposure sits in short-term leases, transparency fades and that‚Äôs how bubbles start.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok-Breakfast-4676"&gt; /u/Ok-Breakfast-4676 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/a6susixny10g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orsdd9/metas_ai_hidden_debt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T15:35:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1os5uur</id>
    <title>Locally running LLMs on DGX Spark as an attorney?</title>
    <updated>2025-11-09T01:01:07+00:00</updated>
    <author>
      <name>/u/Viaprato</name>
      <uri>https://old.reddit.com/user/Viaprato</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm an attorney and under our applicable professional rules (non US), I'm not allowed to upload client data to LLM servers to maintain absolute confidentiality.&lt;/p&gt; &lt;p&gt;Is it a good idea to get the Lenovo DGX Spark and run Llama 3.1 70B or Qwen 2.5 72B on it for example to review large amount of documents (e.g. 1000 contracts) for specific clauses or to summarize e.g. purchase prices mentioned in these documents?&lt;/p&gt; &lt;p&gt;Context windows on the device are small (~130,000 tokens which are about 200 pages), but with &amp;quot;RAG&amp;quot; using Open WebUI it seems to still be possible to analyze much larger amounts of data.&lt;/p&gt; &lt;p&gt;I am a heavy user of AI consumer models, but have never used linux, I can't code and don't have much time to set things up.&lt;/p&gt; &lt;p&gt;Also I am concerned with performance since GPT has become much better with GPT-5 and in particular perplexity, seemingly using claude sonnet 4.5, is mostly superior over gpt-5. i can't use these newest models but would have to use llama 3.1 or qwen 3.2. &lt;/p&gt; &lt;p&gt;What do you think, will this work well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Viaprato"&gt; /u/Viaprato &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os5uur/locally_running_llms_on_dgx_spark_as_an_attorney/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os5uur/locally_running_llms_on_dgx_spark_as_an_attorney/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os5uur/locally_running_llms_on_dgx_spark_as_an_attorney/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T01:01:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1osdbxz</id>
    <title>Full Stack Local Deep Research Agent</title>
    <updated>2025-11-09T07:38:12+00:00</updated>
    <author>
      <name>/u/Fun-Wolf-2007</name>
      <uri>https://old.reddit.com/user/Fun-Wolf-2007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/anilsharmay/full-stack-local-deep-research-agent"&gt;https://github.com/anilsharmay/full-stack-local-deep-research-agent&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fun-Wolf-2007"&gt; /u/Fun-Wolf-2007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osdbxz/full_stack_local_deep_research_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:38:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1os8fmn</id>
    <title>Any news about DeepSeek R2?</title>
    <updated>2025-11-09T03:07:01+00:00</updated>
    <author>
      <name>/u/Sorry_Ad191</name>
      <uri>https://old.reddit.com/user/Sorry_Ad191</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"&gt; &lt;img alt="Any news about DeepSeek R2?" src="https://b.thumbs.redditmedia.com/PlhiSr0p8aXYM8RnTcq4VE2UHze85d2iBHWp0VLED0M.jpg" title="Any news about DeepSeek R2?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/xbf0p2vod50g1.png?width=450&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a63ccc2bd5ae577ce29167d3995c88a6fd898f22"&gt;Holiday wish: 300B release for community pls :)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Oh my can't even imagine the joy and enthusiasm when/if released!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sorry_Ad191"&gt; /u/Sorry_Ad191 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1os8fmn/any_news_about_deepseek_r2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T03:07:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1orwvvj</id>
    <title>Another day, another model - But does it really matter to everyday users?</title>
    <updated>2025-11-08T18:36:18+00:00</updated>
    <author>
      <name>/u/PumpkinNarrow6339</name>
      <uri>https://old.reddit.com/user/PumpkinNarrow6339</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"&gt; &lt;img alt="Another day, another model - But does it really matter to everyday users?" src="https://preview.redd.it/8tf3l0pyu20g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a3a59b3b2fd820136d7af927e2e8fb8f5f6c1611" title="Another day, another model - But does it really matter to everyday users?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We see new models dropping almost every week now, each claiming to beat the previous ones on benchmarks. Kimi 2 (the new thinking model from Chinese company Moonshot AI) just posted these impressive numbers on Humanity's Last Exam:&lt;/p&gt; &lt;p&gt;Agentic Reasoning Benchmark: - Kimi 2: 44.9&lt;/p&gt; &lt;p&gt;Here's what I've been thinking: For most regular users, benchmarks don't matter anymore.&lt;/p&gt; &lt;p&gt;When I use an AI model, I don't care if it scored 44.9 or 41.7 on some test. I care about one thing: Did it solve MY problem correctly?&lt;/p&gt; &lt;p&gt;The answer quality matters, not which model delivered it.&lt;/p&gt; &lt;p&gt;Sure, developers and researchers obsess over these numbers - and I totally get why. Benchmarks help them understand capabilities, limitations, and progress. That's their job.&lt;/p&gt; &lt;p&gt;But for us? The everyday users who are actually the end consumers of these models? We just want: - Accurate answers - Fast responses&lt;br /&gt; - Solutions that work for our specific use case&lt;/p&gt; &lt;p&gt;Maybe I'm missing something here, but it feels like we're in a weird phase where companies are in a benchmark arms race, while actual users are just vibing with whichever model gets their work done.&lt;/p&gt; &lt;p&gt;What do you think? Am I oversimplifying this, or do benchmarks really not matter much for regular users anymore?&lt;/p&gt; &lt;p&gt;Source: Moonshot AI's Kimi 2 thinking model benchmark results&lt;/p&gt; &lt;p&gt;TL;DR: New models keep topping benchmarks, but users don't care about scores just whether it solves their problem. Benchmarks are for devs; users just want results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PumpkinNarrow6339"&gt; /u/PumpkinNarrow6339 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8tf3l0pyu20g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orwvvj/another_day_another_model_but_does_it_really/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:36:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1orusbs</id>
    <title>Here comes another bubble (AI edition)</title>
    <updated>2025-11-08T17:12:45+00:00</updated>
    <author>
      <name>/u/Parking-Recipe-9003</name>
      <uri>https://old.reddit.com/user/Parking-Recipe-9003</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"&gt; &lt;img alt="Here comes another bubble (AI edition)" src="https://external-preview.redd.it/NDBoMGRhcjBnMjBnMeOwxEKY_BwUmvv0yJlvuSQnrkHkZJuTTKSVmRt4UrhV.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=50d9634803859b13dbbfac1614c7af5d9f6931e2" title="Here comes another bubble (AI edition)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Parking-Recipe-9003"&gt; /u/Parking-Recipe-9003 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/bnjv3qq0g20g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orusbs/here_comes_another_bubble_ai_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T17:12:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ormxoq</id>
    <title>Kimi K2 Thinking was trained with only $4.6 million</title>
    <updated>2025-11-08T11:16:59+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt; &lt;img alt="Kimi K2 Thinking was trained with only $4.6 million" src="https://a.thumbs.redditmedia.com/L1ScXXMSwvWG0gMuGlVV23jh1shgAs9PlFDsIKJOn94.jpg" title="Kimi K2 Thinking was trained with only $4.6 million" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;OpenAI: &amp;quot;We need government support to cover $1.4 trillion in chips and data centers.&amp;quot;&lt;/p&gt; &lt;p&gt;Kimi:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09"&gt;https://preview.redd.it/03swwvpfo00g1.png?width=1199&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f97e99e5a5aac5bb23a62ff58df048f5db678e09&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ormxoq/kimi_k2_thinking_was_trained_with_only_46_million/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T11:16:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1osd8ie</id>
    <title>PSA Kimi K2 Thinking seems to currently be broken for most agents because of tool calling within it's thinking tags</title>
    <updated>2025-11-09T07:32:05+00:00</updated>
    <author>
      <name>/u/lemon07r</name>
      <uri>https://old.reddit.com/user/lemon07r</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Yeah, just what the title says. If any of you are having issues with coding using K2 thinking it's because of this. Only Kimi CLI really supports it atm. Minimax m2 had a similar issue I think and glm 4.6 too, but this could be worked around by disabling tool_calling in thinking, however this can't be done for K2 thinking, hence all the issues people are having with this model for coding. Hopefully most agents will have this fixed soon. I think this is called interleaved thinking, or is something similar to that? Feel free to shed some light on this in the comments if you're more familiar with what's going on.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/lemon07r"&gt; /u/lemon07r &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osd8ie/psa_kimi_k2_thinking_seems_to_currently_be_broken/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T07:32:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1orw0fz</id>
    <title>I've been trying to make a real production service that uses LLM and it turned into a pure agony. Here are some of my "experiences".</title>
    <updated>2025-11-08T18:02:08+00:00</updated>
    <author>
      <name>/u/DaniyarQQQ</name>
      <uri>https://old.reddit.com/user/DaniyarQQQ</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone. I hope this won't be an off topic, but I want to share my experience in creating real production service. Like a real deal, that will earn money. &lt;/p&gt; &lt;p&gt;For this service I've been using &lt;strong&gt;ChatGPT-5&lt;/strong&gt; and &lt;strong&gt;Claude Haiku 4.5&lt;/strong&gt; but I think this could be suitable for other LLMs too. &lt;/p&gt; &lt;p&gt;The idea was as simple as rock. Make an assistant bot that will communicate with people and make a scheduled appointments to the doctor. &lt;/p&gt; &lt;p&gt;Well in a short time I've implemented everything. The vector database that will inject doctor specific knowledge to the conversation at the right time. Multiple tools that will work with doctors data, and couple other integrations. I've extensively made very detailed system prompt, and each tool call returns instructive results. Each tools' parameters' descriptions were written in very detailed way. After testing for a week we finally deployed on production and started to receive conversations from real people.&lt;/p&gt; &lt;p&gt;And then real life had showed a lot of annoying and downright frustrating caveats of these LLMs.&lt;/p&gt; &lt;p&gt;The first frustrating thing is that &lt;strong&gt;LLMs makes an assumptions&lt;/strong&gt; &lt;strong&gt;without calling required tool, which deceives people.&lt;/strong&gt; It happened like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: Please give me an address where this doctor will be on tomorrow. LLM: Tomorrow is sunday, which is weekend, doctor is unavalable. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There is a tool that explicitly returns that address, and doctor actually works on Sundays. It did not call that tool and made up a bullshit excuse. Then I have emulated this question again by myself:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Me: Give me address where this doctor will be on tomorrow. LLM: &amp;lt;DID NOT CALL THE TOOL&amp;gt;. Tomorrow is sunday, which is weekend, doctor is unavalable. Me: Are you sure about that? LLM: &amp;lt;Finally starts calling the tool which returns address for tomorrow and outputs this address.&amp;gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This happens always. No matter what kind of prompts you write, telling it not make any assumptions without any tool calls it still made ups bullshit, which deceives people. Even if you explicitly inject the constraints, it fights them and keeps its own bullshit. &lt;/p&gt; &lt;p&gt;Another problem is close to the first one. &lt;strong&gt;LLMs always agrees with requests without calling tools which confuses people.&lt;/strong&gt; Which looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow. Is it possible. LLM: Of course you can make an appointment. I've prepared everything. What time? User: At 18:00 LLM: &amp;lt;Makes a tool call which returns negative result that next day is unavailable&amp;gt;. I'm sorry tomorrow is unavailable. User: WTF? &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Instead of asking proper question before agreeing, it agrees and then shits itself, confusing the user. Especially ChatGPT-5 has this problem, Claude is more rarer but still could shit itself.&lt;/p&gt; &lt;p&gt;And another problem is that &lt;strong&gt;LLMs output text which is complete opposite of it's tool results.&lt;/strong&gt; I've seen this only single time, but I'm now getting paranoid that this could be happening for a long time. It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for tomorrow at 18:00. LLM: &amp;lt;Makes a call to the tool that returns that it is impossible for this user to make an appointment, because user has another pending appointment&amp;gt; LLM: Good. I've scheduled your appointment for tomorrow at 18:00. Don't forget about that &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;That was an epic failure, LLM completely lied it's own results. I don't even know what to say about that.&lt;/p&gt; &lt;p&gt;And finally the funny one. &lt;strong&gt;Looks like ChatGPT does not like that tools can return negative results, and it keeps calling until it fully overloads all context and finally shits itself.&lt;/strong&gt; It looks something like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;User: I want an appointment for next friday at 18:00 LLM: &amp;lt;Calls a tool for available window for next friday. No available window&amp;gt; LLM: &amp;lt;Calls again this tool but for the next next friday. No available window&amp;gt; LLM: &amp;lt;Cals AGAIN this tool but for the next next next friday. No available window&amp;gt; ------- And so on and so on | By the way, this doctor does not work on fridays, it was explicitly given in a system prompt, but ChatGPT wants to persevere. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;These problems are fixable. You can make even more detailed prompts. Make tools return better and understandable results. You can tune some of LLM parameters. However it is game of whack-a-mole, frustrating one. You fix one thing, another thing comes out. I think some of these models, at least ChatGPT and Claude, were so overly trained on positivity, that they generate deceiving or downright wrong results.&lt;/p&gt; &lt;p&gt;Currently It seems to be that these LLMs can at mostly do their jobs correctly, but these fails, even if they happen rarely, are completely negating all of their reliability. It is not a wonderful magic thing that can solve everything. It is very finnicky (and sometimes very frustrating) tool, that maybe can do what you want. You think you have prepared it for everything, but users can make it shit itself just with a single sentence. &lt;/p&gt; &lt;p&gt;At least I've learned a lot, from these models.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DaniyarQQQ"&gt; /u/DaniyarQQQ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1orw0fz/ive_been_trying_to_make_a_real_production_service/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T18:02:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1osh24c</id>
    <title>Best coding agent for GLM-4.6 that's not CC</title>
    <updated>2025-11-09T11:28:22+00:00</updated>
    <author>
      <name>/u/Illustrious-Many-782</name>
      <uri>https://old.reddit.com/user/Illustrious-Many-782</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I already use GLM with Opencode, Claude Code, and Codex CLI, but since I have the one-year z.ai mini plan, I want to use GLM more than I am right now, Is there a better option than OpenCode (that's not Claude Code, because it's being used by Claude)?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Many-782"&gt; /u/Illustrious-Many-782 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osh24c/best_coding_agent_for_glm46_thats_not_cc/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:28:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ortopy</id>
    <title>Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs</title>
    <updated>2025-11-08T16:28:21+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt; &lt;img alt="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" src="https://preview.redd.it/s190tdo2720g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a07ae8f05f136602ecbf12323e286c3cca29d84" title="Kimi K2 Thinking 1-bit Unsloth Dynamic GGUFs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone! You can now run Kimi K2 Thinking locally with our Unsloth Dynamic 1bit GGUFs. We also collaborated with the Kimi team on a &lt;strong&gt;fix for K2&lt;/strong&gt; &lt;a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking/discussions/12"&gt;&lt;strong&gt;Thinking's chat template&lt;/strong&gt;&lt;/a&gt; not prepending the default system prompt of &lt;code&gt;You are Kimi, an AI assistant created by Moonshot AI.&lt;/code&gt; on the 1st turn.&lt;/p&gt; &lt;p&gt;We also we &lt;strong&gt;fixed llama.cpp custom jinja separators&lt;/strong&gt; for tool calling - Kimi does &lt;code&gt;{&amp;quot;a&amp;quot;:&amp;quot;1&amp;quot;,&amp;quot;b&amp;quot;:&amp;quot;2&amp;quot;}&lt;/code&gt; and not with extra spaces like &lt;code&gt;{&amp;quot;a&amp;quot;: &amp;quot;1&amp;quot;, &amp;quot;b&amp;quot;: &amp;quot;2&amp;quot;}&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The 1-bit GGUF will run on 247GB RAM. We shrank the 1T model to 245GB (-62%) &amp;amp; the accuracy recovery is comparable to our third-party &lt;a href="https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs/unsloth-dynamic-ggufs-on-aider-polyglot"&gt;DeepSeek-V3.1 Aider Polyglot benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All 1bit, 2bit and other bit width GGUFs are at &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The suggested temp is &lt;code&gt;temperature = 1.0&lt;/code&gt;. We also suggest a &lt;code&gt;min_p = 0.01&lt;/code&gt;. If you do not see &lt;code&gt;&amp;lt;think&amp;gt;&lt;/code&gt;, use &lt;code&gt;--special&lt;/code&gt;. The code for llama-cli is below which offloads MoE layers to CPU RAM, and leaves the rest of the model on GPU VRAM:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export LLAMA_CACHE=&amp;quot;unsloth/Kimi-K2-Thinking-GGUF&amp;quot; ./llama.cpp/llama-cli \ -hf unsloth/Kimi-K2-Thinking-GGUF:UD-TQ1_0 \ --n-gpu-layers 99 \ --temp 1.0 \ --min-p 0.01 \ --ctx-size 16384 \ --seed 3407 \ -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Step-by-step Guide + fix details: &lt;a href="https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally"&gt;https://docs.unsloth.ai/models/kimi-k2-thinking-how-to-run-locally&lt;/a&gt; and GGUFs are &lt;a href="https://huggingface.co/unsloth/Kimi-K2-Thinking-GGUF"&gt;here&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Let us know if you have any questions and hope you have a great weekend!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s190tdo2720g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ortopy/kimi_k2_thinking_1bit_unsloth_dynamic_ggufs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-08T16:28:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1osglws</id>
    <title>Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench</title>
    <updated>2025-11-09T11:01:13+00:00</updated>
    <author>
      <name>/u/ihexx</name>
      <uri>https://old.reddit.com/user/ihexx</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt; &lt;img alt="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" src="https://preview.redd.it/mqize91iq70g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=717d19b768a2a88e4b9a2a6690bbde033a817303" title="Kimi K2 Thinking scores lower than Gemini 2.5 Flash on Livebench" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ihexx"&gt; /u/ihexx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/mqize91iq70g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1osglws/kimi_k2_thinking_scores_lower_than_gemini_25/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-09T11:01:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1olq14f</id>
    <title>[MEGATHREAD] Local AI Hardware - November 2025</title>
    <updated>2025-11-01T15:02:17+00:00</updated>
    <author>
      <name>/u/eck72</name>
      <uri>https://old.reddit.com/user/eck72</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is the monthly thread for sharing your local AI setups and the models you're running.&lt;/p&gt; &lt;p&gt;Whether you're using a single CPU, a gaming GPU, or a full rack, post what you're running and how it performs.&lt;/p&gt; &lt;p&gt;Post in any format you like. The list below is just a guide:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Hardware: CPU, GPU(s), RAM, storage, OS&lt;/li&gt; &lt;li&gt;Model(s): name + size/quant&lt;/li&gt; &lt;li&gt;Stack: (e.g. llama.cpp + custom UI)&lt;/li&gt; &lt;li&gt;Performance: t/s, latency, context, batch etc.&lt;/li&gt; &lt;li&gt;Power consumption&lt;/li&gt; &lt;li&gt;Notes: purpose, quirks, comments&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please share setup pics for eye candy!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Quick reminder&lt;/strong&gt;: You can share hardware purely to ask questions or get feedback. All experience levels welcome.&lt;/p&gt; &lt;p&gt;House rules: no buying/selling/promo.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eck72"&gt; /u/eck72 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1olq14f/megathread_local_ai_hardware_november_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-01T15:02:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oqy1k7</id>
    <title>AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)</title>
    <updated>2025-11-07T15:53:33+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt; &lt;img alt="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" src="https://preview.redd.it/8v2luf5owuzf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b9bc34ec8dddd94422397eaa91e0310250da5ba3" title="AMA Announcement: Moonshot AI, The Opensource Frontier Lab Behind Kimi K2 Thinking SoTA Model (Monday, 8AM-11AM PST)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/8v2luf5owuzf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oqy1k7/ama_announcement_moonshot_ai_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-07T15:53:33+00:00</published>
  </entry>
</feed>
