<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-11-01T02:39:12+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1okq6ms</id>
    <title>Anyone else running their whole AI stack as Proxmox LXC containers? Im currently using Open WebUI as front-end, LiteLLM as a router and A vLLM container per model as back-ends</title>
    <updated>2025-10-31T09:44:18+00:00</updated>
    <author>
      <name>/u/AFruitShopOwner</name>
      <uri>https://old.reddit.com/user/AFruitShopOwner</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okq6ms/anyone_else_running_their_whole_ai_stack_as/"&gt; &lt;img alt="Anyone else running their whole AI stack as Proxmox LXC containers? Im currently using Open WebUI as front-end, LiteLLM as a router and A vLLM container per model as back-ends" src="https://preview.redd.it/gva8eiw9weyf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b7d91a864d5dc7462667570db07c5263e8a57532" title="Anyone else running their whole AI stack as Proxmox LXC containers? Im currently using Open WebUI as front-end, LiteLLM as a router and A vLLM container per model as back-ends" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I have not implemented it yet, but I believe it should be possible for LiteLLM to interface with the Proxmox API and dynamically turn on and off vLLM containers depening on what model users select (in Open WebUI). Does anyone have any experience with this?&lt;/p&gt; &lt;p&gt;I want to add a container for n8n for automation workflows (connected to LiteLLM for AI models), a websearch MCP container running something like Searxng (because I find the web search implementation in Open WebUI to be extremely limited) and an (agentic) RAG service. I need robust retrieval over professional/Dutch GAAP/IFRS accounting materials, internal company docs, client data, and relevant laws/regulations. There seem to be a million ways to do RAG; this will be the cornerstone of the system.&lt;/p&gt; &lt;p&gt;I built this AI server/Workstation for the Dutch accounting firm I work at (I have no IT background myself so its been quite the learning proces). Managment wanted everything local and I jumped on the oppertunity to learn something new.&lt;/p&gt; &lt;p&gt;My specs:&lt;br /&gt; CPU - AMD EPYC 9575F&lt;br /&gt; Dual GMI links allowing it to use almost all of the theoretical system memory bandwidth, 5Ghz Boost clock, 64 core, 128 thread beast of a CPU, seems to me like the best choice for an AI exterimentation server. Great as a host for GPU inference, Hybrid Inference (GPU + System memory spillover) and CPU only inference.&lt;/p&gt; &lt;p&gt;RAM - 1.152tb (12x96gb RDIMMs ) ECC DDR5 6.400MT/s RAM (~614gb/s theoretical max bandwidth). Will allow me to run massive MOE models on the CPU, albeit slowly. Also plenty or ram for any other service I want to run.&lt;/p&gt; &lt;p&gt;MOBO - Supermicro H13SSL-N (Rev. 2.01). I have a Supermicro H14SSL-NT on backorder but it could be a couple of weeks before I get that one.&lt;/p&gt; &lt;p&gt;GPU's - 3x Nvidia RTX Pro 6000 Max-Q. I was planning on getting 2 Workstation editions but the supplier kept fucking up my order and sending me the Max Q's. Eventually caved and got a third Max-Q because I had plenty of cooling and power capacity. 3 gpu's is not ideal for tensor parallelism but pipleline- and expert parallelism are decent alternatives when 2x96 gb is not enough. Maybe I'll get a 4th one eventually.&lt;/p&gt; &lt;p&gt;Storage - A bunch of Kioxia CM7 R's.&lt;/p&gt; &lt;p&gt;Gpt-oss 120b is the main 'workhorse' model. It comfortably fits in a single GPU so I can use the other GPU's to run auxiliary models that can assist gpt-oss 120b. Maybe a couple of gpt-oss 20b models in a websearch mcp server, a vision language model like Qwen 3 VL, Deepseek-OCR or Gemma 3 for pictures/files.&lt;/p&gt; &lt;p&gt;As mentioned, I don’t come from an IT background, so I’m looking for practical advice and sanity checks. How does this setup look? Is there anything you’d fundamentally do differently? I followed a bunch of guides (mostly the excellent ones from DigitalSpaceport), got about 90% of the way with ChatGPT 5 Thinking, and figured out the last 10% through trial and error (Proxmox Snapshots make the trail and error approach really easy).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AFruitShopOwner"&gt; /u/AFruitShopOwner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/gva8eiw9weyf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okq6ms/anyone_else_running_their_whole_ai_stack_as/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okq6ms/anyone_else_running_their_whole_ai_stack_as/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T09:44:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1oko2ar</id>
    <title>Want to run claude like model on ~$10k budget. Please help me with the machine build. I don't want to spend on cloud.</title>
    <updated>2025-10-31T07:19:40+00:00</updated>
    <author>
      <name>/u/LordSteinggard</name>
      <uri>https://old.reddit.com/user/LordSteinggard</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Finally saved money for this, want to have my own rig. Works that I will be doing:&lt;br /&gt; 1. Want to run Claude like model of course&lt;br /&gt; 2. 3D modeling from very high resolution images, interacting with 3D models. Images are diverse - nanoscale samples to satellite imageries.&lt;/p&gt; &lt;p&gt;Max that I can go is probably 1/2k extra, not more. Please don't ask me to work on cloud! Lol.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LordSteinggard"&gt; /u/LordSteinggard &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oko2ar/want_to_run_claude_like_model_on_10k_budget/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oko2ar/want_to_run_claude_like_model_on_10k_budget/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oko2ar/want_to_run_claude_like_model_on_10k_budget/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T07:19:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol74b2</id>
    <title>Milestones in open weights AI: what models shaped your journey?</title>
    <updated>2025-10-31T21:42:16+00:00</updated>
    <author>
      <name>/u/synw_</name>
      <uri>https://old.reddit.com/user/synw_</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;When Llama 1 came out I started using local AI and got a bit fascinated by it running locally: this is where it clicked for me. Over time I tried a lot of models and some really stood out, and stayed in my history book. Here is my list of the best open weights models ever:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Llama 1: where everything started for me&lt;/li&gt; &lt;li&gt;Mistral 7b instruct: first time that I realized models are usable for real work&lt;/li&gt; &lt;li&gt;Deepseek 6.7b: first useful code model&lt;/li&gt; &lt;li&gt;Qwq: first reasoning model&lt;/li&gt; &lt;li&gt;Qwen 30b a3b: first moe model&lt;/li&gt; &lt;li&gt;Qwen 4b: first small model that really works&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I essentially focus on stem models but I also liked some more general or conversational talented models like Mistral Nemo for it's prose (+Large +Small for general usage), Aya for translations, or some surprisingly good old fine tunes from back in the days (when super good fine tunes where popping up almost every day) like the Hermes series. While writing this post I've noticed something new to me: I tried different models to get a clean title for the post (only the title was made using AI, I wrote the post myself and did not submit it to AI even if the english is not that good, I hate having models to write for me) and found that Gemma 4b was interesting because creative for this task while disliking it's strong sycophancy.&lt;/p&gt; &lt;p&gt;What are your best open weights models of all times for your use case?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synw_"&gt; /u/synw_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol74b2/milestones_in_open_weights_ai_what_models_shaped/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol74b2/milestones_in_open_weights_ai_what_models_shaped/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol74b2/milestones_in_open_weights_ai_what_models_shaped/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T21:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9ajs</id>
    <title>A Mobile Strix Halo!!!</title>
    <updated>2025-10-31T23:22:16+00:00</updated>
    <author>
      <name>/u/chisleu</name>
      <uri>https://old.reddit.com/user/chisleu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://videocardz.com/newz/onexplayer-onexfly-apex-ryzen-ai-max-395-handheld-announced-costs-1200-2200-features-85wh-external-battery-and-liquid-cooling"&gt;https://videocardz.com/newz/onexplayer-onexfly-apex-ryzen-ai-max-395-handheld-announced-costs-1200-2200-features-85wh-external-battery-and-liquid-cooling&lt;/a&gt;&lt;/p&gt; &lt;p&gt;All you need is a keyboard!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/chisleu"&gt; /u/chisleu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9ajs/a_mobile_strix_halo/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9ajs/a_mobile_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9ajs/a_mobile_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:22:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol3lp9</id>
    <title>What Qwen version do you want to see in Tiny-Qwen?</title>
    <updated>2025-10-31T19:16:31+00:00</updated>
    <author>
      <name>/u/No-Compote-6794</name>
      <uri>https://old.reddit.com/user/No-Compote-6794</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I previously open sourced this clean PyTorch re-implementation of Qwen inspired by Andrej Karpathy’s nanoGPT. &lt;/p&gt; &lt;p&gt;Repo link: &lt;a href="https://github.com/Emericen/tiny-qwen"&gt;https://github.com/Emericen/tiny-qwen&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I’m adding support for Qwen 3 VL, but am curious what you prefer when you see this type of repo &lt;/p&gt; &lt;p&gt;&lt;a href="https://www.reddit.com/poll/1ol3lp9"&gt;View Poll&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Compote-6794"&gt; /u/No-Compote-6794 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3lp9/what_qwen_version_do_you_want_to_see_in_tinyqwen/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3lp9/what_qwen_version_do_you_want_to_see_in_tinyqwen/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3lp9/what_qwen_version_do_you_want_to_see_in_tinyqwen/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T19:16:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz32u</id>
    <title>What has been your experience with high latency in your AI coding tools?</title>
    <updated>2025-10-31T16:22:57+00:00</updated>
    <author>
      <name>/u/InceptionAI_Tom</name>
      <uri>https://old.reddit.com/user/InceptionAI_Tom</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Curious about everyone’s experience with high latency in your AI applications.&lt;/p&gt; &lt;p&gt;High latency seems to be a pretty common issue I see talked about here.&lt;/p&gt; &lt;p&gt;What have you tried and what has worked? What hasn’t worked?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InceptionAI_Tom"&gt; /u/InceptionAI_Tom &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz32u/what_has_been_your_experience_with_high_latency/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz32u/what_has_been_your_experience_with_high_latency/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz32u/what_has_been_your_experience_with_high_latency/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:22:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz6ko</id>
    <title>What's the difference between f16 and bf16 mmproj GGUF files for Qwen3-VL?</title>
    <updated>2025-10-31T16:26:40+00:00</updated>
    <author>
      <name>/u/windows_error23</name>
      <uri>https://old.reddit.com/user/windows_error23</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Sorry if this is a stupid question. Some quant providers upload both, along with f32. Isn't the model originally in bf16? Which is higher quality. Thanks a lot for any help.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/windows_error23"&gt; /u/windows_error23 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz6ko/whats_the_difference_between_f16_and_bf16_mmproj/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz6ko/whats_the_difference_between_f16_and_bf16_mmproj/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz6ko/whats_the_difference_between_f16_and_bf16_mmproj/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:26:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol8zo5</id>
    <title>I built a privacy focused AI assistant for WearOS that supports locally hosted LLMs</title>
    <updated>2025-10-31T23:07:57+00:00</updated>
    <author>
      <name>/u/tr0picana</name>
      <uri>https://old.reddit.com/user/tr0picana</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built an AI assistant for WearOS called &lt;a href="https://play.google.com/store/apps/details?id=com.mortartribe.hopperai"&gt;Hopper &lt;/a&gt;so I could leave my phone at home and still have productivity tools at my disposal. I’m posting about it here because I think this community will appreciate some of the features.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It supports OpenAI compatible endpoints. So it works perfectly if you self-host models.&lt;/li&gt; &lt;li&gt;Complete privacy. I don’t collect any data except for anonymized crash logs that get uploaded to Firebase.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The WearOS app has a companion phone app to make certain actions like entering your API key less painful.&lt;/p&gt; &lt;p&gt;The Wear OS side is completely standalone and doesn't require your phone to function (outside of providing internet access if you don't have an e-sim).&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Instant voice input. You can configure the app to immediately launch into voice recording mode. I wanted push to talk but this is the best I could do because of platform limitations.&lt;/li&gt; &lt;li&gt;Built-in tools: &lt;ul&gt; &lt;li&gt;Create notes. Try saying, &amp;quot;Write a short horror story and save it to my notes&amp;quot;.&lt;/li&gt; &lt;li&gt;Web search. If Hopper can't answer a question with its own knowledge, it will search Yahoo (don't tase me) for websites and scrape them to get better answers.&lt;/li&gt; &lt;li&gt;Alarms &amp;amp; Reminders. Try saying &amp;quot;Remind me to go for a walk in 3 hours&amp;quot;.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Custom tools. Probably the most powerful feature is that you can wrap any API with a webhook tool, turning the API into tools that Hopper can call. This lets you integrate Hopper with a ton of apps or trigger any n8n/make/IFTTT workflows! I made a simple workflow in n8n that sends me an email and now I can ask Hopper to send me an email with anything.&lt;/li&gt; &lt;li&gt;Remote MCP servers. Using the Hopper companion app you can add remote MCP servers and use the tools from within Hopper. Both open and authenticated servers work!&lt;/li&gt; &lt;li&gt;Tool chaining. This is where it all comes together. Try saying, &amp;quot;Find me a recipe for banana pudding, save it to my notes and then email it to me&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The android app is primarily to make managing advanced settings easy. You can also view saved artifacts on it.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Settings management. You can change various watch settings through the app, but more importantly, you can more easily set your OpenAI compatible endpoint and model on the phone instead of typing it out on your watch's keyboard.&lt;/li&gt; &lt;li&gt;Data sync. The app can pull all your saved notes, chats, and images and display/share them.&lt;/li&gt; &lt;li&gt;Add custom tools. You can wrap any API in a webhook tool. Give it a name (create_tweet), description (Post a tweet for the user), and parameters (tweet_contents) and Hopper will figure out if it should use the tool in response to a question/statement!&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I built Hopper on top of DevEmperor's open-source &lt;a href="https://github.com/DevEmperor/WristAssist"&gt;efforts&lt;/a&gt; so a HUGE thank you to them for building such an awesome app &amp;lt;3&lt;/p&gt; &lt;p&gt;If you give it a try I’d love to get your feedback. I'm also happy to add custom features if they make your life easier :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/tr0picana"&gt; /u/tr0picana &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8zo5/i_built_a_privacy_focused_ai_assistant_for_wearos/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8zo5/i_built_a_privacy_focused_ai_assistant_for_wearos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8zo5/i_built_a_privacy_focused_ai_assistant_for_wearos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9a0j</id>
    <title>The wiki plugin should come pre-install for LM-studio</title>
    <updated>2025-10-31T23:21:34+00:00</updated>
    <author>
      <name>/u/OldEffective9726</name>
      <uri>https://old.reddit.com/user/OldEffective9726</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"&gt; &lt;img alt="The wiki plugin should come pre-install for LM-studio" src="https://b.thumbs.redditmedia.com/QSFjhpfepCS36ds4CnQEAwJzByr0jfThPsaIAms2mOk.jpg" title="The wiki plugin should come pre-install for LM-studio" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It's so helpful. The command line is:&lt;/p&gt; &lt;p&gt;&lt;em&gt;lms get lmstudio/wikipedia&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bsuhcw846jyf1.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a1c7b6269d354920963a9fd92baa8d44081f3cc"&gt;https://preview.redd.it/bsuhcw846jyf1.png?width=1964&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8a1c7b6269d354920963a9fd92baa8d44081f3cc&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/OldEffective9726"&gt; /u/OldEffective9726 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9a0j/the_wiki_plugin_should_come_preinstall_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:21:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol3o4l</id>
    <title>Upcoming Coding Models?</title>
    <updated>2025-10-31T19:19:17+00:00</updated>
    <author>
      <name>/u/pmttyji</name>
      <uri>https://old.reddit.com/user/pmttyji</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Anything coming soon or later? Speculations/rumors?&lt;/p&gt; &lt;p&gt;Nothing from Llama for now. I think same on Microsoft too(or Phi new version coming?).&lt;/p&gt; &lt;p&gt;Would be great to have Coder (Both MOE &amp;amp; Dense) models like below.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;LFM Coder - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok0i7q/comment/nm82eq2/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;We're currently exploring the possibility of small coding models...&lt;/a&gt; &amp;amp; &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1ok0i7q/comment/nm984bv/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;Thanks for the feedback on the demand for the Coding models and FIM models. We are constantly thinking about what makes the most sense to release next.&lt;/a&gt; - LFM @ AMA&lt;/li&gt; &lt;li&gt;Granite Coder 30B - &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1oichb7/comment/nm7n2lc/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button"&gt;It is not currently on the roadmap, but we will pass this request along to the Research team! - IBM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;GPT OSS 2.0 Coder 30B - MXFP4 quant would be 17GB size without quantization(As their 20B model is just 12GB)&lt;/li&gt; &lt;li&gt;Seed OSS Coder 30B - Unfortunately I can't even touch their Seed-OSS-36B model with my 8GB VRAM :(&lt;/li&gt; &lt;li&gt;Gemma Coder 20-30B - It seems many from this sub waiting for Gemma4 release as I found multiple threads in last 2 months from my search.&lt;/li&gt; &lt;li&gt;GLM Coder 30B - So many fans for GLM &amp;amp; GLM Air. Great to have small MOE in 30B size.&lt;/li&gt; &lt;li&gt;Mistral Coder - Their recent Magistral &amp;amp; Devstral using by people on coding/FIM stuff. But not suitable for Poor GPU club as those are Dense models. It's been long time that they released a small model in 12B size. Mistral-Nemo-Instruct-2407 is more than a year old.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Recent models related to Coding we got through this sub:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;internlm/JanusCoder-8B - 8B text model based on Qwen3-8B&lt;/li&gt; &lt;li&gt;internlm/JanusCoder-14B - 14B text model based on Qwen3-14B&lt;/li&gt; &lt;li&gt;internlm/JanusCoderV-7B - 7B multimodal model based on Qwen2.5-VL-7B&lt;/li&gt; &lt;li&gt;internlm/JanusCoderV-8B - 8B multimodal model based on InternVL3.5-8B&lt;/li&gt; &lt;li&gt;nvidia/Qwen3-Nemotron-32B-RLBFF&lt;/li&gt; &lt;li&gt;inference-net/Schematron-3B&lt;/li&gt; &lt;li&gt;Tesslate/UIGEN-FX-Agentic-32B - Trained on Qwen3 32B&lt;/li&gt; &lt;li&gt;Tesslate/WEBGEN-Devstral-24B - Trained on Devstral 24B&lt;/li&gt; &lt;li&gt;Kwaipilot/KAT-Dev&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/pmttyji"&gt; /u/pmttyji &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3o4l/upcoming_coding_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3o4l/upcoming_coding_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol3o4l/upcoming_coding_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T19:19:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok3xie</id>
    <title>200+ pages of Hugging Face secrets on how to train an LLM</title>
    <updated>2025-10-30T16:11:22+00:00</updated>
    <author>
      <name>/u/eliebakk</name>
      <uri>https://old.reddit.com/user/eliebakk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt; &lt;img alt="200+ pages of Hugging Face secrets on how to train an LLM" src="https://preview.redd.it/s12qz4k3w9yf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=44c78fbb2faf8b6857633466eb7cf34609898a57" title="200+ pages of Hugging Face secrets on how to train an LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey it's elie from the hugging face pre-training team! We're very excited to share our new blog (book?) that cover the full pipeline: pre-training, post-training and infra. 200+ pages of what worked, what didn’t, and how to make it run reliably :)&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook"&gt;https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hope yall will enjoy it, don't hesitate to make feedback on the community tab :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/eliebakk"&gt; /u/eliebakk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s12qz4k3w9yf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok3xie/200_pages_of_hugging_face_secrets_on_how_to_train/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T16:11:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol1vdm</id>
    <title>Mergekit has been re-licensed under GNU LGPL v3</title>
    <updated>2025-10-31T18:08:57+00:00</updated>
    <author>
      <name>/u/noneabove1182</name>
      <uri>https://old.reddit.com/user/noneabove1182</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Kinda self-promo ? But also feel it's worth shouting out anyways, mergekit is back to LGPL license!&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/arcee-ai/mergekit"&gt;https://github.com/arcee-ai/mergekit&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.arcee.ai/blog/mergekit-returns-to-its-roots"&gt;https://www.arcee.ai/blog/mergekit-returns-to-its-roots&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/noneabove1182"&gt; /u/noneabove1182 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol1vdm/mergekit_has_been_relicensed_under_gnu_lgpl_v3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:08:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol9cai</id>
    <title>Gerbil: An open source desktop app for running LLMs locally</title>
    <updated>2025-10-31T23:24:38+00:00</updated>
    <author>
      <name>/u/i_got_the_tools_baby</name>
      <uri>https://old.reddit.com/user/i_got_the_tools_baby</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt; &lt;img alt="Gerbil: An open source desktop app for running LLMs locally" src="https://external-preview.redd.it/eno0eDlyNTQ3anlmMYROUy6ynC042_ngFZye_M2RHtEEKYtBWVHWeI_XUuyY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4340ab3f6ed93d4f4c1334de594505f6406eaea" title="Gerbil: An open source desktop app for running LLMs locally" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/i_got_the_tools_baby"&gt; /u/i_got_the_tools_baby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/096u8qj06jyf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol9cai/gerbil_an_open_source_desktop_app_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T23:24:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1oku9og</id>
    <title>Why the hype around ultra small models like Granite4_350m? What are the actual use cases for these models?</title>
    <updated>2025-10-31T13:14:05+00:00</updated>
    <author>
      <name>/u/Porespellar</name>
      <uri>https://old.reddit.com/user/Porespellar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I get that small models can run on edge devices, but what are people actually planning on using a 350m parameter model for in the real world? I’m just really curious as to what use cases developers see these fitting into vs. using 1b, 4b, or 8b? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Porespellar"&gt; /u/Porespellar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oku9og/why_the_hype_around_ultra_small_models_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T13:14:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol2oxw</id>
    <title>Unbound In-Character Reasoning Model - Apollo-V0.1-4B-Thinking</title>
    <updated>2025-10-31T18:40:35+00:00</updated>
    <author>
      <name>/u/AllThingsIntel</name>
      <uri>https://old.reddit.com/user/AllThingsIntel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An experimental model with many of its creative inhibitions lifted. Its internal reasoning process adapts to the persona you assign (via the system prompt), allowing it to explore a wider spectrum of themes. This is a V0.1 preview for testing. More refined versions (non-reasoning variants as well) are planned. Follow for updates.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AllThingsIntel"&gt; /u/AllThingsIntel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/AllThingsIntel/Apollo-V0.1-4B-Thinking"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol2oxw/unbound_incharacter_reasoning_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:40:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol5zx5</id>
    <title>Adding a RTX 5080 into a 2U server with OcuLink</title>
    <updated>2025-10-31T20:54:28+00:00</updated>
    <author>
      <name>/u/king_priam_of_Troy</name>
      <uri>https://old.reddit.com/user/king_priam_of_Troy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"&gt; &lt;img alt="Adding a RTX 5080 into a 2U server with OcuLink" src="https://a.thumbs.redditmedia.com/Vdh4_ZPajSdyshlNSjO227Vq2w1bY7lOkx2ZQxIWik8.jpg" title="Adding a RTX 5080 into a 2U server with OcuLink" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;As my P40 was no more up to the task, I needed a better card in my main server. The main issues were:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;It does not fit (NVidia makes sure of that)&lt;/li&gt; &lt;li&gt;It is really hard to get a correct power cable for these new cards. I was afraid to damage my server motherboard.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So the alternative I found was to setup a OcuLink dock with its own power supply. I used the MINIS FORUM DEG1 (because it was the one I could get overnight at Amazon). I put a 4 port OcuLink card in the server (I can use bifurcation later for more GPU).&lt;/p&gt; &lt;p&gt;Performance are great: 140+ token/s with Mistral.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/king_priam_of_Troy"&gt; /u/king_priam_of_Troy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol5zx5"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol5zx5/adding_a_rtx_5080_into_a_2u_server_with_oculink/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T20:54:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol006s</id>
    <title>Drummer's Rivermind™ 24B v1 - A spooky future for LLMs, Happy Halloween!</title>
    <updated>2025-10-31T16:58:28+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt; &lt;img alt="Drummer's Rivermind™ 24B v1 - A spooky future for LLMs, Happy Halloween!" src="https://external-preview.redd.it/cJYTAXorc18tRHqzBZaT1gjlzMY9WdV4LW05HQehJqQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6fbf968d35d15b5cb941693631cde3f0e872cd42" title="Drummer's Rivermind™ 24B v1 - A spooky future for LLMs, Happy Halloween!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The older brother of &lt;a href="https://huggingface.co/TheDrummer/Rivermind-12B-v1"&gt;https://huggingface.co/TheDrummer/Rivermind-12B-v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Rivermind-24B-v1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol006s/drummers_rivermind_24b_v1_a_spooky_future_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:58:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol6qlk</id>
    <title>MiniMax M2 Llama.cpp support merged</title>
    <updated>2025-10-31T21:25:47+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"&gt; &lt;img alt="MiniMax M2 Llama.cpp support merged" src="https://external-preview.redd.it/zIJQO0qHVJjeSmNiAhS1pkkPDhsRxWN-vwvuIbrWY3s.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=190f42b7e585927328c4767cedf1cc2831910bf7" title="MiniMax M2 Llama.cpp support merged" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Aight, the MiniMax M2 support is officially in. &lt;/p&gt; &lt;p&gt;Remember that there is no support for the chat format yet, and for a good reason - there is currently no easy way to deal with the &amp;quot;interleaved&amp;quot; thinking format of the model. &lt;/p&gt; &lt;p&gt;I'm currently considering the intermediate solution - since the model makers recommend passing the thinking blocks back to the model, I'm thinking of leaving all the thinking tags inside the normal content and letting clients parse it (so no `reasoning_content`), but add parsing for tool calls (and possibly reinject the starting `&amp;lt;think&amp;gt;` tag).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/commit/0de0a01576772032008a689afc4d7c80685074c4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol6qlk/minimax_m2_llamacpp_support_merged/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T21:25:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1oky3un</id>
    <title>Qwen3-VL GGUF!</title>
    <updated>2025-10-31T15:46:07+00:00</updated>
    <author>
      <name>/u/khubebk</name>
      <uri>https://old.reddit.com/user/khubebk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have not tried any yet, multiple other Veterans have uploaded GGUF Quants, linking to unsloth for their guide and all available models from 2B-32B.&lt;br /&gt; &lt;a href="https://huggingface.co/unsloth"&gt;Hugging Face Unsloth&lt;/a&gt;&lt;br /&gt; &lt;a href="https://docs.unsloth.ai/models/qwen3-vl-run-and-fine-tune"&gt;Unsloth Guide&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/khubebk"&gt; /u/khubebk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oky3un/qwen3vl_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T15:46:07+00:00</published>
  </entry>
  <entry>
    <id>t3_1okppxs</id>
    <title>Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?</title>
    <updated>2025-10-31T09:13:34+00:00</updated>
    <author>
      <name>/u/Successful-Newt1517</name>
      <uri>https://old.reddit.com/user/Successful-Newt1517</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt; &lt;img alt="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" src="https://preview.redd.it/ej8yokr9zeyf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd9469bd25e003a0703a4ea86d079690b75d94f" title="Both Cursor and Cognition (Windsurf) new models are speculated to be built on Chinese base models?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, what's going on? Are Chinese models saving American startups?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Successful-Newt1517"&gt; /u/Successful-Newt1517 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ej8yokr9zeyf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okppxs/both_cursor_and_cognition_windsurf_new_models_are/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T09:13:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7vwv</id>
    <title>For any LLM enthusiast in Finland you have decommission Super Computer equipped with 96 Nvidia A100 40Gb Pcie , if you live nearby Kajaani try contact company maybe you get them on discount ;)</title>
    <updated>2025-10-31T22:16:56+00:00</updated>
    <author>
      <name>/u/DeathRabit86</name>
      <uri>https://old.reddit.com/user/DeathRabit86</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/"&gt;https://research.csc.fi/2025/09/25/installation-of-the-roihu-supercomputer-begins/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;“CSC is preparing the end-of-life plans for Mahti and Puhti in line with scientific needs and sustainability principles. In practice, we’ll donate the systems to suitable recipients for continued use or spare parts”, says&lt;/em&gt; &lt;strong&gt;&lt;em&gt;Sebastian von Alfthan&lt;/em&gt;&lt;/strong&gt;*, Development Manager at CSC.*&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DeathRabit86"&gt; /u/DeathRabit86 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7vwv/for_any_llm_enthusiast_in_finland_you_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:16:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol7ri8</id>
    <title>support for Minimax M2 has been merged into llama.cpp</title>
    <updated>2025-10-31T22:11:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt; &lt;img alt="support for Minimax M2 has been merged into llama.cpp" src="https://external-preview.redd.it/I_x1QIcREivfRZWw6RyYObzeaj8mdE6DXTQR3kx1F5I.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47afb9f21e4fd695dd9279346c35a27194d0369b" title="support for Minimax M2 has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16831"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol7ri8/support_for_minimax_m2_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:11:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol8bfx</id>
    <title>New AI workstation</title>
    <updated>2025-10-31T22:36:32+00:00</updated>
    <author>
      <name>/u/faileon</name>
      <uri>https://old.reddit.com/user/faileon</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt; &lt;img alt="New AI workstation" src="https://b.thumbs.redditmedia.com/lEmf1RVi5jrp-eyNGLmi5QJsRATbD0Vj35rKsMda9_Q.jpg" title="New AI workstation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Managed to fit in 4x RTX 3090 to a Phantek Server/Workstation case. Scores each card for roughly 800$. The PCIE riser on picture was too short (30cm) and had to be replaced with a 60cm one. The vertical mount is for Lian LI case, but manages to hook it up in the Phantek too. Mobo is ASRock romed8-2t, CPU is EPYC 7282 from eBay for 75$. So far it's a decent machine especially considering the cost.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/faileon"&gt; /u/faileon &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ol8bfx"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol8bfx/new_ai_workstation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T22:36:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ol30e5</id>
    <title>qwen2.5vl:32b is saving me $1400 from my HOA</title>
    <updated>2025-10-31T18:53:04+00:00</updated>
    <author>
      <name>/u/jedsk</name>
      <uri>https://old.reddit.com/user/jedsk</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Over this year I finished putting together my local LLM machine with a quad 3090 setup. Built a few workflows with it but like most of you, just wanted to experiment with local models and for the sake of burning tokens lol.&lt;/p&gt; &lt;p&gt;Then in July, my ceiling got damaged from an upstairs leak. HOA says &amp;quot;not our problem.&amp;quot; I'm pretty sure they're wrong, but proving it means reading their governing docs (20 PDFs, +1,000 pages total).&lt;/p&gt; &lt;p&gt;Thought this was the perfect opportunity to create an actual useful app and do bulk PDF processing with vision models. Spun up qwen2.5vl:32b on Ollama and built a pipeline:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;PDF → image conversion → markdown&lt;/li&gt; &lt;li&gt;Vision model extraction&lt;/li&gt; &lt;li&gt;Keyword search across everything&lt;/li&gt; &lt;li&gt;Found 6 different sections proving HOA was responsible&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Took about 3-4 hours to process everything locally. Found the proof I needed on page 287 of their Declaration. Sent them the evidence, but ofc still waiting to hear back.&lt;/p&gt; &lt;p&gt;Finally justified the purpose of this rig lol.&lt;/p&gt; &lt;p&gt;Anyone else stumble into unexpectedly practical uses for their local LLM setup? Built mine for experimentation, but turns out it's perfect for sensitive document processing you can't send to cloud services.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jedsk"&gt; /u/jedsk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ol30e5/qwen25vl32b_is_saving_me_1400_from_my_hoa/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T18:53:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1okz8qz</id>
    <title>pewdiepie dropped a video about running local ai</title>
    <updated>2025-10-31T16:28:56+00:00</updated>
    <author>
      <name>/u/topfpflanze187</name>
      <uri>https://old.reddit.com/user/topfpflanze187</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt; &lt;img alt="pewdiepie dropped a video about running local ai" src="https://external-preview.redd.it/WddxiFHLc3dMB9LBPGHmNWXXrzglB78uxpSOk1Y4d6E.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d41e205151bfdcec37d1be377abc09d05a02773e" title="pewdiepie dropped a video about running local ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/topfpflanze187"&gt; /u/topfpflanze187 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=qw4fDU18RcU"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1okz8qz/pewdiepie_dropped_a_video_about_running_local_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-31T16:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1ok0i7q</id>
    <title>AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo</title>
    <updated>2025-10-30T14:00:16+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" src="https://b.thumbs.redditmedia.com/hb9XoRhxPRhv8ljYkjnVbJWgnClXeLGMxbG1TEDCwos.jpg" title="AMA with Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We’re super excited to host this week’s AMA! &lt;/p&gt; &lt;p&gt;Join us and ask your questions directly to the human minds behind all things Liquid: Liquid Foundational Models, the Liquid Edge AI Platform (LEAP) for model customization and deployment, and Apollo.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our participants:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks &lt;a href="https://www.reddit.com/user/jamarks13/"&gt;u/jamarks13&lt;/a&gt; (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith &lt;a href="https://www.reddit.com/user/jimmysmith1919/"&gt;u/jimmysmith1919&lt;/a&gt; (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne &lt;a href="https://www.reddit.com/user/mlabonne/"&gt;u/mlabonne&lt;/a&gt; (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes &lt;a href="https://www.reddit.com/user/Wide-Half-7982/"&gt;u/Wide-Half-7982&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak &lt;a href="https://www.reddit.com/user/ankebananke/"&gt;u/ankebananke&lt;/a&gt; (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur Böök &lt;a href="https://www.reddit.com/user/ManWithARedFace/"&gt;u/ManWithARedFace&lt;/a&gt; (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev &lt;a href="https://www.reddit.com/user/ykhrustalev/"&gt;u/ykhrustalev&lt;/a&gt; (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena &lt;a href="https://www.reddit.com/user/humble_pi_314/"&gt;u/humble_pi_314&lt;/a&gt; (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca &lt;a href="https://www.reddit.com/user/Ok-Safe-5316/"&gt;u/Ok-Safe-5316&lt;/a&gt; (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale &lt;a href="https://www.reddit.com/user/anthony-liquidai/"&gt;u/anthony-liquidai&lt;/a&gt; (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo &lt;a href="https://www.reddit.com/user/PauLabartaBajo/"&gt;u/PauLabartaBajo&lt;/a&gt; (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The AMA will run from &lt;strong&gt;10 AM - 1 PM PST&lt;/strong&gt;. The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;&amp;gt; &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; &amp;gt; &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198"&gt;https://preview.redd.it/uvhnx2j379yf1.png?width=1620&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9638f2940194e4d3cf6c4e79195373908a36c198&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Thanks to everyone who participated in this AMA. It was a pleasure.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;a href="https://discord.gg/DFU3WQeaYD"&gt;Join the Liquid AI Discord Community&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ok0i7q/ama_with_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-30T14:00:16+00:00</published>
  </entry>
</feed>
