<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-01T16:43:58+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pbes68</id>
    <title>I made an architecture for fixing Mamba's long-context forgetting</title>
    <updated>2025-12-01T14:53:49+00:00</updated>
    <author>
      <name>/u/InstructionOk9108</name>
      <uri>https://old.reddit.com/user/InstructionOk9108</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"&gt; &lt;img alt="I made an architecture for fixing Mamba's long-context forgetting" src="https://b.thumbs.redditmedia.com/NAAVJAXhlCgY4eFntwhC-Ka4uVFkZ1Q1JM7ABZgIAxI.jpg" title="I made an architecture for fixing Mamba's long-context forgetting" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The result&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/yo8w4021wl4g1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45de5d9a738065409294056902ce2b19a0b54f8c"&gt;https://preview.redd.it/yo8w4021wl4g1.png?width=3000&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45de5d9a738065409294056902ce2b19a0b54f8c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Benchmark (NIAH @ 32k Context):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Baseline:&lt;/strong&gt; Mamba-2 (130M) ‚Üí Starts forgetting at ~29k (83.3% acc).&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Mine:&lt;/strong&gt; Spectrum-State (85M) ‚Üí &lt;strong&gt;100% Recall&lt;/strong&gt; across the board.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;My model is &lt;strong&gt;smaller&lt;/strong&gt; (85M vs 130M) and was trained on only &lt;strong&gt;1/7th of the data&lt;/strong&gt; used for Mamba-2.&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/jaeha_han_/status/1995481225755038088"&gt;There is a detailed infos on my X.&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InstructionOk9108"&gt; /u/InstructionOk9108 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbes68/i_made_an_architecture_for_fixing_mambas/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T14:53:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb37b7</id>
    <title>Looking for High-Quality Open-Source Local TTS That‚Äôs Faster Than IndexTTS2</title>
    <updated>2025-12-01T04:21:24+00:00</updated>
    <author>
      <name>/u/TomNaughtyy</name>
      <uri>https://old.reddit.com/user/TomNaughtyy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Me and my cousin have been using IndexTTS2 for a while and really like the voice quality, it sounds natural and expressive. The only issue is that it‚Äôs slow. He‚Äôs getting around 1.6 RTF on his 3090, which makes it hard to generate longer audio efficiently (we work with long audio, not real-time use).&lt;/p&gt; &lt;p&gt;We‚Äôve also tried Kokoro TTS and CosyVoice 2. Kokoro is super fast, but most of the voices sound too synthetic or ‚ÄúAI-like‚Äù for our needs. One voice we actually liked was ‚ÄúNicole‚Äù in Kokoro, it has a more natural and calm tone that works well for us. CosyVoice 2 had better expressiveness and sounded promising, but it had a habit of changing words or pronouncing them weirdly, which broke the consistency.&lt;/p&gt; &lt;p&gt;We‚Äôre only interested in open-source models. No commercial or cloud APIs.&lt;/p&gt; &lt;p&gt;A few things to note: We‚Äôre not planning to use emotion vectors, style tokens, or any prompt engineering tricks, just clean, straightforward narration. We‚Äôre on strong hardware (3090 and 4090), so GPU resources aren‚Äôt a problem. Just want something with good voice quality that runs faster than IndexTTS2 and ideally has at least one solid voice that sounds natural.&lt;/p&gt; &lt;p&gt;Any models or voices you recommend?&lt;br /&gt; Thanks&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TomNaughtyy"&gt; /u/TomNaughtyy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb37b7/looking_for_highquality_opensource_local_tts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb37b7/looking_for_highquality_opensource_local_tts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb37b7/looking_for_highquality_opensource_local_tts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T04:21:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pams8b</id>
    <title>nvidia/Orchestrator-8B ¬∑ Hugging Face</title>
    <updated>2025-11-30T16:42:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/"&gt; &lt;img alt="nvidia/Orchestrator-8B ¬∑ Hugging Face" src="https://external-preview.redd.it/Havs9Ap5icNaW5b7G-LM12y5Y2tjxzsA0o3nV6l5p6A.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=73e0546e48c2735a0477acb6fffa02a0e545e6c6" title="nvidia/Orchestrator-8B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Orchestrator-8B is a state-of-the-art 8B parameter orchestration model designed to solve complex, multi-turn agentic tasks by coordinating a diverse set of expert models and tools.&lt;/p&gt; &lt;p&gt;On the Humanity's Last Exam (HLE) benchmark, ToolOrchestrator-8B achieves a score of 37.1%, outperforming GPT-5 (35.1%) while being approximately 2.5x more efficient.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/nvidia_Orchestrator-8B-GGUF"&gt;https://huggingface.co/bartowski/nvidia_Orchestrator-8B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nvidia/Orchestrator-8B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pams8b/nvidiaorchestrator8b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T16:42:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb4pbm</id>
    <title>[2511.23404] LFM2 Technical Report</title>
    <updated>2025-12-01T05:39:01+00:00</updated>
    <author>
      <name>/u/Thrumpwart</name>
      <uri>https://old.reddit.com/user/Thrumpwart</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Thrumpwart"&gt; /u/Thrumpwart &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2511.23404"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb4pbm/251123404_lfm2_technical_report/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb4pbm/251123404_lfm2_technical_report/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T05:39:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbcelg</id>
    <title>[Toolkit] TinyLlama Fine-Tuning + RAG Lab (Full FT / LoRA / QLoRA | T4-friendly | Unified pipeline)</title>
    <updated>2025-12-01T13:11:46+00:00</updated>
    <author>
      <name>/u/sai_ai_lab</name>
      <uri>https://old.reddit.com/user/sai_ai_lab</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just released an early (and still very much WIP) version of &lt;strong&gt;FT-Lab&lt;/strong&gt;, a lightweight toolkit for fine-tuning and retrieval-augmented generation (RAG) with TinyLlama.&lt;/p&gt; &lt;p&gt;üîß &lt;strong&gt;What it does:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Full Fine-Tuning / LoRA / QLoRA support for TinyLlama&lt;/li&gt; &lt;li&gt;Unified preprocessing + tokenizer setup&lt;/li&gt; &lt;li&gt;RAG workflows with LlamaIndex + LangChain&lt;/li&gt; &lt;li&gt;Retrieval metrics: &lt;code&gt;recall@k&lt;/code&gt;, &lt;code&gt;precision@k&lt;/code&gt;, &lt;code&gt;hit_rate@k&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;üéØ &lt;strong&gt;Goal:&lt;/strong&gt; Make TinyLlama fine-tuning and evaluation reproducible and small-GPU-friendly.&lt;/p&gt; &lt;p&gt;üß™ Still a work in progress ‚Äî some parts might not run yet, but feedback is super welcome!&lt;/p&gt; &lt;p&gt;üîó GitHub: &lt;a href="https://github.com/REICHIYAN/ft_lab"&gt;https://github.com/REICHIYAN/ft_lab&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/sai_ai_lab"&gt; /u/sai_ai_lab &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcelg/toolkit_tinyllama_finetuning_rag_lab_full_ft_lora/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcelg/toolkit_tinyllama_finetuning_rag_lab_full_ft_lora/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcelg/toolkit_tinyllama_finetuning_rag_lab_full_ft_lora/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:11:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1pavof6</id>
    <title>[Ministral 3] Add ministral 3 - Pull Request #42498 ¬∑ huggingface/transformers</title>
    <updated>2025-11-30T22:36:36+00:00</updated>
    <author>
      <name>/u/bratao</name>
      <uri>https://old.reddit.com/user/bratao</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pavof6/ministral_3_add_ministral_3_pull_request_42498/"&gt; &lt;img alt="[Ministral 3] Add ministral 3 - Pull Request #42498 ¬∑ huggingface/transformers" src="https://external-preview.redd.it/kvAOuOuPU1hgF-Ezo21UQUe0ThkEwS_Wm4nwhMo6c8c.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5bbd39529d0c356901aca5c2c1d85379cf4fd779" title="[Ministral 3] Add ministral 3 - Pull Request #42498 ¬∑ huggingface/transformers" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bratao"&gt; /u/bratao &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/huggingface/transformers/pull/42498"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pavof6/ministral_3_add_ministral_3_pull_request_42498/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pavof6/ministral_3_add_ministral_3_pull_request_42498/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T22:36:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1paw8u1</id>
    <title>Winter LLM</title>
    <updated>2025-11-30T23:00:42+00:00</updated>
    <author>
      <name>/u/aziham</name>
      <uri>https://old.reddit.com/user/aziham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paw8u1/winter_llm/"&gt; &lt;img alt="Winter LLM" src="https://preview.redd.it/agojdb246h4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bf041f02afcc034c276829a48360b9ceb30e6b70" title="Winter LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aziham"&gt; /u/aziham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/agojdb246h4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paw8u1/winter_llm/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paw8u1/winter_llm/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T23:00:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbg1sl</id>
    <title>Last week in Multimodal AI - Local Edition</title>
    <updated>2025-12-01T15:42:18+00:00</updated>
    <author>
      <name>/u/Vast_Yak_4147</name>
      <uri>https://old.reddit.com/user/Vast_Yak_4147</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbg1sl/last_week_in_multimodal_ai_local_edition/"&gt; &lt;img alt="Last week in Multimodal AI - Local Edition" src="https://a.thumbs.redditmedia.com/E4fiq9i_bRJBgvAUEkI05u-TOmUaEUMerQyGv47ibT4.jpg" title="Last week in Multimodal AI - Local Edition" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I curate a weekly newsletter on multimodal AI. Here are the local/edge highlights from last week:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Z-Image - 6B Commercial-Grade Generation&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 6B parameter model competes with commercial giants for photorealistic images.&lt;br /&gt; ‚Ä¢ Handles bilingual text rendering at quality comparable to paid services, no license fees.&lt;br /&gt; ‚Ä¢ &lt;a href="https://z-image.ai/"&gt;Website&lt;/a&gt; | &lt;a href="https://huggingface.co/Tongyi-MAI/Z-Image-Turbo"&gt;Hugging Face&lt;/a&gt; | &lt;a href="https://comfyanonymous.github.io/ComfyUI_examples/z_image/"&gt;ComfyUI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/8jgc6hra3m4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a76883f218bb21c7f1322278f07b8f363c7d5f5"&gt;https://preview.redd.it/8jgc6hra3m4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=4a76883f218bb21c7f1322278f07b8f363c7d5f5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;HunyuanOCR - 1B SOTA OCR Model&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Beats larger models like Qwen3-VL-4B and commercial APIs with just 1B parameters.&lt;br /&gt; ‚Ä¢ Achieves SOTA results on OCRBench for models under 3B, runs on-device.&lt;br /&gt; ‚Ä¢ &lt;a href="https://github.com/Tencent-Hunyuan/HunyuanOCR/blob/main/HunyuanOCR_Technical_Report.pdf"&gt;Technical Report&lt;/a&gt; | &lt;a href="https://huggingface.co/tencent/HunyuanOCR"&gt;Model&lt;/a&gt; | &lt;a href="https://huggingface.co/spaces/tencent/HunyuanOCR"&gt;Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/mt2ixgwd3m4g1.png?width=2604&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19255e0c95fc7287fa81f74eba6245eb61fd82e3"&gt;https://preview.redd.it/mt2ixgwd3m4g1.png?width=2604&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=19255e0c95fc7287fa81f74eba6245eb61fd82e3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;RynnVLA-002 - Unified Vision-Language-Action Model&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ 97.4% success on LIBERO simulation, 50% boost on real-world LeRobot tasks.&lt;br /&gt; ‚Ä¢ Runs locally for robot action generation and environment dynamics prediction.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/papers/2511.17502"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-002"&gt;Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1pbg1sl/video/u5jni69f3m4g1/player"&gt;https://reddit.com/link/1pbg1sl/video/u5jni69f3m4g1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;GigaWorld-0 - Unified World Model for VLA(2B)&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Trains robots on simulated data that transfers to physical tasks.&lt;br /&gt; ‚Ä¢ Acts as data engine for vision-language-action learning on local hardware.&lt;br /&gt; ‚Ä¢ &lt;a href="https://huggingface.co/papers/2511.19861"&gt;Paper&lt;/a&gt; | &lt;a href="https://huggingface.co/open-gigaai/GigaWorld-0-Video-GR1-2b"&gt;Model&lt;/a&gt; | &lt;a href="https://huggingface.co/open-gigaai/GigaWorld-0-Video-Pretrain-2b"&gt;Pretrain Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cb3bmjyh3m4g1.jpg?width=1708&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=06d9eb9a33c23595ad543f756c20d2fb3a5c0e6c"&gt;https://preview.redd.it/cb3bmjyh3m4g1.jpg?width=1708&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=06d9eb9a33c23595ad543f756c20d2fb3a5c0e6c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vidi2 - 12B Multimodal Video Model&lt;/strong&gt;&lt;br /&gt; ‚Ä¢ Handles video understanding and creation with 12B parameters.&lt;br /&gt; ‚Ä¢ Optimized architecture for local video workflows.&lt;br /&gt; ‚Ä¢ &lt;a href="https://bytedance.github.io/vidi-website/"&gt;Website&lt;/a&gt; | &lt;a href="https://arxiv.org/pdf/2511.19529"&gt;Paper&lt;/a&gt; | &lt;a href="https://github.com/bytedance/vidi/tree/main"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jyga8otk4m4g1.png?width=940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb615c585d0154cfb5476af59f11be67dafeea13"&gt;https://preview.redd.it/jyga8otk4m4g1.png?width=940&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=eb615c585d0154cfb5476af59f11be67dafeea13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Checkout the &lt;a href="https://open.substack.com/pub/thelivingedge/p/multimodal-monday-35-small-models?r=12l7fk&amp;amp;utm_campaign=post&amp;amp;utm_medium=web&amp;amp;showWelcomeOnShare=false"&gt;full newslette&lt;/a&gt;r for more demos, papers, and resources.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Vast_Yak_4147"&gt; /u/Vast_Yak_4147 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbg1sl/last_week_in_multimodal_ai_local_edition/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbg1sl/last_week_in_multimodal_ai_local_edition/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbg1sl/last_week_in_multimodal_ai_local_edition/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T15:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbeg6g</id>
    <title>We built a 1 and 3B local Git agents that turns plain English into correct git commands. They matche GPT-OSS 120B accuracy (gitara)</title>
    <updated>2025-12-01T14:40:08+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbeg6g/we_built_a_1_and_3b_local_git_agents_that_turns/"&gt; &lt;img alt="We built a 1 and 3B local Git agents that turns plain English into correct git commands. They matche GPT-OSS 120B accuracy (gitara)" src="https://preview.redd.it/5321sl8otl4g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0694f90bbe4e81187886cd5619b3f07bf3a56d1a" title="We built a 1 and 3B local Git agents that turns plain English into correct git commands. They matche GPT-OSS 120B accuracy (gitara)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We have been working on tool calling SLMs and how to get the most out of a small model. One of the use cases turned out to be very useful and we hope to get your feedback. You can find more information on the &lt;a href="https://github.com/distil-labs/distil-gitara"&gt;github page&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We trained a &lt;strong&gt;3B function-calling model&lt;/strong&gt; (‚ÄúGitara‚Äù) that converts natural language ‚Üí valid git commands, with accuracy nearly identical to a &lt;strong&gt;120B teacher model&lt;/strong&gt;, that can run on your laptop.&lt;/p&gt; &lt;p&gt;Just type: &lt;em&gt;‚Äúundo the last commit but keep the changes‚Äù ‚Üí&lt;/em&gt; you get: &lt;em&gt;&lt;code&gt;git reset --soft HEAD~1&lt;/code&gt;&lt;/em&gt;.&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;Why we built it&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;We forget to use git flags correctly all the time, so we thought the chance is you do too.&lt;/p&gt; &lt;p&gt;Small models are perfect for &lt;strong&gt;structured tool-calling tasks&lt;/strong&gt;, so this became our testbed.&lt;/p&gt; &lt;p&gt;Our goals:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Runs locally&lt;/strong&gt; (Ollama)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;max. 2-second responses&lt;/strong&gt; on a laptop&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Structured JSON output ‚Üí deterministic git commands&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Match the accuracy of a large model&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;Results&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;Model&lt;/th&gt; &lt;th&gt;Params&lt;/th&gt; &lt;th&gt;Accuracy&lt;/th&gt; &lt;th&gt;Model link&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;GPT-OSS 120B (teacher)&lt;/td&gt; &lt;td&gt;120B&lt;/td&gt; &lt;td&gt;0.92 ¬± 0.02&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;Llama 3.2 3B Instruct (fine-tuned)&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;3B&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.92 ¬± 0.01&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-gitara-v2-Llama-3.2-3B-Instruct"&gt;huggingface&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama 3.2 1B (fine-tuned)&lt;/td&gt; &lt;td&gt;1B&lt;/td&gt; &lt;td&gt;0.90 ¬± 0.01&lt;/td&gt; &lt;td&gt;&lt;a href="https://huggingface.co/distil-labs/Distil-gitara-v2-Llama-3.2-1B-Instruct"&gt;huggingface&lt;/a&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Llama 3.2 3B (base)&lt;/td&gt; &lt;td&gt;3B&lt;/td&gt; &lt;td&gt;0.12 ¬± 0.05&lt;/td&gt; &lt;td&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The fine-tuned &lt;strong&gt;3B model matches the 120B model&lt;/strong&gt; on tool-calling correctness.&lt;/p&gt; &lt;p&gt;Responds &lt;strong&gt;&amp;lt;2 seconds&lt;/strong&gt; on a M4 MacBook Pro.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Examples&lt;/h2&gt; &lt;p&gt;``` ‚Äúwhat's in the latest stash, show diff‚Äù ‚Üí git stash show --patch&lt;/p&gt; &lt;p&gt;‚Äúpush feature-x to origin, override any changes there‚Äù ‚Üí git push origin feature-x --force --set-upstream&lt;/p&gt; &lt;p&gt;‚Äúundo last commit but keep the changes‚Äù ‚Üí git reset --soft HEAD~1&lt;/p&gt; &lt;p&gt;‚Äúshow 8 commits as a graph‚Äù ‚Üí git log -n 8 --graph&lt;/p&gt; &lt;p&gt;‚Äúmerge vendor branch preferring ours‚Äù ‚Üí git merge vendor --strategy ours&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;The model &lt;strong&gt;prints the git command but does NOT execute it&lt;/strong&gt;, by design.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What‚Äôs under the hood&lt;/h2&gt; &lt;p&gt;From the README (summarized):&lt;/p&gt; &lt;ul&gt; &lt;li&gt;We defined all git actions as &lt;strong&gt;OpenAI function-calling schemas&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Created ~100 realistic seed examples&lt;/li&gt; &lt;li&gt;Generated &lt;strong&gt;10,000 validated synthetic examples&lt;/strong&gt; via a teacher model&lt;/li&gt; &lt;li&gt;Fine-tuned Llama 3.2 3B with LoRA&lt;/li&gt; &lt;li&gt;Evaluated by matching generated functions to ground truth&lt;/li&gt; &lt;li&gt;Accuracy matched the teacher at ~0.92&lt;/li&gt; &lt;/ul&gt; &lt;hr /&gt; &lt;h2&gt;Want to try it?&lt;/h2&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/distil-labs/distil-gitara"&gt;https://github.com/distil-labs/distil-gitara&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Quick start (Ollama):&lt;/p&gt; &lt;p&gt;```bash hf download distil-labs/Llama-3_2-gitara-3B --local-dir distil-model cd distil-model ollama create gitara -f Modelfile python gitara.py &amp;quot;your git question here&amp;quot;&lt;/p&gt; &lt;p&gt;```&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Discussion&lt;/h2&gt; &lt;p&gt;Curious to hear from the community:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;How are you using local models in your workflows?&lt;/li&gt; &lt;li&gt;Anyone else experimenting with structured-output SLMs for local workflows?&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5321sl8otl4g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbeg6g/we_built_a_1_and_3b_local_git_agents_that_turns/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbeg6g/we_built_a_1_and_3b_local_git_agents_that_turns/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T14:40:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb8v2u</id>
    <title>Karpathy/reader3 ‚Äî self-hosted EPUB reader for LLM-assisted reading</title>
    <updated>2025-12-01T09:56:10+00:00</updated>
    <author>
      <name>/u/freesysck</name>
      <uri>https://old.reddit.com/user/freesysck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"&gt; &lt;img alt="Karpathy/reader3 ‚Äî self-hosted EPUB reader for LLM-assisted reading" src="https://a.thumbs.redditmedia.com/qKEh6RaOA6QT-c0IcO3cDj9L25ecEnI9yuqdotqbo_4.jpg" title="Karpathy/reader3 ‚Äî self-hosted EPUB reader for LLM-assisted reading" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/vu1hbkj1fk4g1.png?width=962&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a91cc3e4a21891e1eb54d4ff5f4199fdf1505265"&gt;https://preview.redd.it/vu1hbkj1fk4g1.png?width=962&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a91cc3e4a21891e1eb54d4ff5f4199fdf1505265&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Tiny, demo-grade EPUB reader that splits a book into &lt;strong&gt;chapters&lt;/strong&gt; and serves &lt;strong&gt;local static pages&lt;/strong&gt;, so you can copy a chapter straight into an LLM for co-reading/Q&amp;amp;A.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Chapter-by-chapter&lt;/strong&gt; view for easy chunking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Lightweight self-hosting:&lt;/strong&gt; single-file script + simple static server&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LLM-friendly workflow:&lt;/strong&gt; copy/paste each chapter to your model&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Open-source (MIT):&lt;/strong&gt; learning example/personal tool; no long-term maintenance promised&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo: &lt;a href="http://github.com/karpathy/reader3"&gt;&lt;code&gt;github.com/karpathy/reader3&lt;/code&gt;&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/freesysck"&gt; /u/freesysck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb8v2u/karpathyreader3_selfhosted_epub_reader_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T09:56:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1parhxk</id>
    <title>I mapped how language models decide when a pile of sand becomes a ‚Äúheap‚Äù</title>
    <updated>2025-11-30T19:46:42+00:00</updated>
    <author>
      <name>/u/Specialist_Bad_4465</name>
      <uri>https://old.reddit.com/user/Specialist_Bad_4465</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1parhxk/i_mapped_how_language_models_decide_when_a_pile/"&gt; &lt;img alt="I mapped how language models decide when a pile of sand becomes a ‚Äúheap‚Äù" src="https://preview.redd.it/763n9ju87g4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1534d6f16651204ad4fafddb5c126adb160d5ebf" title="I mapped how language models decide when a pile of sand becomes a ‚Äúheap‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This chart compares how three open-weight language models decide when a pile of sand becomes a ‚Äúheap.‚Äù&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;X-axis:&lt;/strong&gt; number of grains of sand, on a log scale from 1 to 100,000,000.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Y-axis:&lt;/strong&gt; probability that the model answers ‚ÄúYes, this is a heap‚Äù given that many grains, P(Yes | n).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What each line shows:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Cyan ‚Äì Mistral-7B:&lt;/strong&gt; starts around 0.25 at 1 grain and climbs smoothly to ~0.8 by 100M grains.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Magenta ‚Äì DeepSeek-7B:&lt;/strong&gt; similar S-shape but consistently lower than Mistral; it crosses the 0.5 line later, so it‚Äôs ‚Äústricter‚Äù about when a heap begins.&lt;br /&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Yellow ‚Äì Llama-3-8B:&lt;/strong&gt; stays noisy in roughly the 0.35‚Äì0.6 band across almost the entire range, from 1 grain to 100M, rarely committing strongly either way.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The shaded band between 0.4 and 0.6 highlights the ‚Äúborderline‚Äù region where the models are most uncertain about heapness.&lt;/p&gt; &lt;p&gt;All three curves come from the same basic setup:&lt;br /&gt; I give the model a few examples (1‚Äì2 grains ‚Üí ‚ÄúNo‚Äù, 999,999‚Äì1,000,000 grains ‚Üí ‚ÄúYes‚Äù), then ask for many different values of &lt;code&gt;n&lt;/code&gt;: &lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;‚ÄúThere is a pile of n grains of sand. Is this a heap? Answer yes or no.‚Äù&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;For each &lt;code&gt;n&lt;/code&gt;, I plot the softmax probability on the ‚ÄúYes‚Äù token.&lt;/p&gt; &lt;p&gt;&lt;a href="https://joshfonseca.com/blogs/sorites-paradox"&gt;Full writeup with more charts and prompt details is here&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Specialist_Bad_4465"&gt; /u/Specialist_Bad_4465 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/763n9ju87g4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1parhxk/i_mapped_how_language_models_decide_when_a_pile/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1parhxk/i_mapped_how_language_models_decide_when_a_pile/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T19:46:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbag5i</id>
    <title>I wrote a kernel that makes sparse LLMs faster and smaller on consumer GPUs even at low sparsity.</title>
    <updated>2025-12-01T11:31:31+00:00</updated>
    <author>
      <name>/u/vlejd</name>
      <uri>https://old.reddit.com/user/vlejd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"&gt; &lt;img alt="I wrote a kernel that makes sparse LLMs faster and smaller on consumer GPUs even at low sparsity." src="https://b.thumbs.redditmedia.com/97VGlNQjo_fRFkrwIo-Ies8AASHz1A354I-Gvc-9uNU.jpg" title="I wrote a kernel that makes sparse LLMs faster and smaller on consumer GPUs even at low sparsity." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Pruning LLMs hind of sucks. On GPUs, unstructured sparsity doesn‚Äôt really help. You don‚Äôt get memory savings, and you don‚Äôt get speed up. You always needed very high sparsity (the model breaks), some structure (2:4: very limiting, and the model is worse) or special hardware (good luck).&lt;/p&gt; &lt;p&gt;I built a new matrix format + GPU kernel for sparse matrix-vector multiplication that unlocks the benefits of pruning on real hardware. I‚Äôm calling it MACKO-SpMV, and it has no special GPU instructions, no fixed block patterns, no giant performance drop, no precomputation and no autotuning. Just: prune, store the weights, run fast.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vmvsr577qk4g1.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e261ccf86c0d0ec9c6814b693cf729c746e1f7b0"&gt;https://preview.redd.it/vmvsr577qk4g1.png?width=852&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e261ccf86c0d0ec9c6814b693cf729c746e1f7b0&lt;/a&gt;&lt;/p&gt; &lt;p&gt;What this means in practice:&lt;br /&gt; - Noticeable memory reduction even at low sparsity&lt;br /&gt; - Speed-ups on standard consumer GPUs (no tensor core magic needed). Tested with NVIDIA 2080, 3090, 4090.&lt;br /&gt; - Works with any model that has linear layers (basically all LLMs and much more).&lt;br /&gt; - Want to run 7b model on 8GB memory? Well, prune to 60% sparsity and you will even get a 2x speedup. &lt;/p&gt; &lt;p&gt;Quick caveat1: For prefill, it only gives you memory reduction without the speed-up. For generation, you get both the speed-up and memory reduction. Happy to discuss the technical reasons. &lt;/p&gt; &lt;p&gt;Quick caveat2: This is not a post about quality of the model. Pruning methods are advancing rapidly, and I hope this will help the field to catch up/outperform quantization.&lt;/p&gt; &lt;p&gt;Fully open source, still mainly academic.&lt;/p&gt; &lt;p&gt;If you care about local LLMs, this finally makes aggressive pruning a practical tool instead of a research curiosity. You can strip down a model and actually benefit from it at runtime.&lt;/p&gt; &lt;p&gt;Blog (high-level explanation): &lt;a href="https://www.grizzlytech.dev/blog/macko-spmv"&gt;https://www.grizzlytech.dev/blog/macko-spmv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Paper (details on the format/algorithm): &lt;a href="https://arxiv.org/pdf/2511.13061"&gt;https://arxiv.org/pdf/2511.13061&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Code (open-source implementation): &lt;a href="http://github.com/vlejd/macko_spmv"&gt;github.com/vlejd/macko_spmv&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Happy to answer questions, benchmark suggestions and integration ideas. I‚Äôd love to see what the local LLM community can do with this. &lt;/p&gt; &lt;p&gt;If anyone has niche/pruned models, weird sparsity patterns, or cases where quantization ruins quality, let me know.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/vlejd"&gt; /u/vlejd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbag5i/i_wrote_a_kernel_that_makes_sparse_llms_faster/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:31:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1pawn1r</id>
    <title>More of Silicon Valley is building on free Chinese AI</title>
    <updated>2025-11-30T23:17:35+00:00</updated>
    <author>
      <name>/u/buppermint</name>
      <uri>https://old.reddit.com/user/buppermint</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pawn1r/more_of_silicon_valley_is_building_on_free/"&gt; &lt;img alt="More of Silicon Valley is building on free Chinese AI" src="https://external-preview.redd.it/OmQhaJFYusd_6BoEAmpETVbmV-j9iUqnPAIX8zdt-yE.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=349b2df57a77a2b5df77ab3b848267efef6e4117" title="More of Silicon Valley is building on free Chinese AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/buppermint"&gt; /u/buppermint &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.nbcnews.com/tech/innovation/silicon-valley-building-free-chinese-ai-rcna242430"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pawn1r/more_of_silicon_valley_is_building_on_free/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pawn1r/more_of_silicon_valley_is_building_on_free/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T23:17:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb943m</id>
    <title>model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp</title>
    <updated>2025-12-01T10:11:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"&gt; &lt;img alt="model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp" src="https://external-preview.redd.it/Qgcy1T0XaVi_myckNkZ5FtZbwlkaUdzehWhwNkBtflY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=acaaa619b090ed54ad8471529b58617d2a113392" title="model: support Ministral3 by ngxson ¬∑ Pull Request #17644 ¬∑ ggml-org/llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Looks like there will be 0-day support for Ministral in llama.cpp too&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/17644"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb943m/model_support_ministral3_by_ngxson_pull_request/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T10:11:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbh87f</id>
    <title>You can now do 500K context length fine-tuning - 6.4x longer</title>
    <updated>2025-12-01T16:26:09+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/"&gt; &lt;img alt="You can now do 500K context length fine-tuning - 6.4x longer" src="https://preview.redd.it/0snnf2xdam4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a952723c7a85b10d74440c69f4678836b9f558c" title="You can now do 500K context length fine-tuning - 6.4x longer" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey [&lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;](), today, we're excited to share that you can now train gpt-oss-20b &lt;strong&gt;(or any LLM)&lt;/strong&gt; to extend its context window to 530K on single 80GB H100 GPU. And you can reach &lt;strong&gt;750K+ context&lt;/strong&gt; on 192GB VRAM - with no accuracy loss. Unsloth GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Most model labs fine-tune LLMs to extend their native context length. We are optimizing that process!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;For smaller GPUs, you‚Äôll still see big gains in VRAM and context as e.g. &lt;strong&gt;RTX 5090 can reach 200K context.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;With smaller LLMs, longer contexts are even easier.&lt;/li&gt; &lt;li&gt;On 80GB, the context length limit has increased from 82K to 530K.&lt;/li&gt; &lt;li&gt;This update works for any LLM or VLM, not just gpt-oss. Also with limited support for RL.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For context, we‚Äôve significantly improved how Unsloth handles memory usage patterns, speed, and context lengths:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;72% lower VRAM use with 3.2x longer context via Unsloth‚Äôs new fused and chunked cross-entropy loss, with no degradation in speed or accuracy&lt;/li&gt; &lt;li&gt;Enhanced activation offloading in Unsloth‚Äôs Gradient Checkpointing algorithm which was introduced in April 2024. It quickly became popular and the standard across the industry, having been integrated into most training packages nowadays - and we've improved it even further!&lt;/li&gt; &lt;li&gt;Collabing with Snowflake on Tiled MLP, enabling 2√ó more contexts&lt;/li&gt; &lt;li&gt;Our new algorithms allows gpt-oss-20b QLoRA (4bit) with 290K context possible on a H100 with no accuracy loss, and 530K+ with Tiled MLP enabled, altogether delivering &amp;gt;6.4x longer context lengths.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We also made a Colab notebook on an A100 80GB so you can try gpt-oss-20b with 500K context by using a 500K context dataset. Colab: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_(20B"&gt;https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt_oss_(20B)_500K_Context_Fine_tuning.ipynb&lt;/a&gt;_500K_Context_Fine_tuning.ipynb)&lt;/p&gt; &lt;p&gt;To enable Tiled MLP on any LLM, VLM in Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;model, tokenizer = FastLanguageModel.from_pretrained( ..., unsloth_tiled_mlp = True, ) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Details + notebook are in our blog: &lt;a href="https://docs.unsloth.ai/new/500k-context-length-fine-tuning"&gt;https://docs.unsloth.ai/new/500k-context-length-fine-tuning&lt;/a&gt;. To update Unsloth, do&lt;/p&gt; &lt;pre&gt;&lt;code&gt;pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth_zoo &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We'll also be at NeurIPS Tues - Thur for a workshop &amp;amp; reception! Would love to meet you all there with some merch! Hope you guys have a lovely rest of the week! :D&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0snnf2xdam4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbh87f/you_can_now_do_500k_context_length_finetuning_64x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T16:26:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbal3o</id>
    <title>Finally DeepSeek supports interleave thinking</title>
    <updated>2025-12-01T11:39:42+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt; &lt;img alt="Finally DeepSeek supports interleave thinking" src="https://a.thumbs.redditmedia.com/ai32QhhIA6vA0pwrGaFK12rwpqWtI4iLRNv_7q0_kR4.jpg" title="Finally DeepSeek supports interleave thinking" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/lt2hmbaowk4g1.png?width=1923&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7babc862bf421b4f28c0176a4184a40a2a3b0f9"&gt;https://preview.redd.it/lt2hmbaowk4g1.png?width=1923&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b7babc862bf421b4f28c0176a4184a40a2a3b0f9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So far, among open-source models, only GPT-OSS, Kimi K2 Thinking, and MiniMax M2 support it, and I believe this feature is crucial for agents.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/y1qu5h7d0l4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57ff81a5e1e9aab0379ff1b0ea907ac6cddd4a0e"&gt;https://preview.redd.it/y1qu5h7d0l4g1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57ff81a5e1e9aab0379ff1b0ea907ac6cddd4a0e&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;What is interleave thinking?&lt;/h1&gt; &lt;p&gt;If a thinking model supports multi-step tool calls and can incorporate thinking from historical steps during these calls, then this model supports interleaved thinking.&lt;/p&gt; &lt;h1&gt;Why it matters?&lt;/h1&gt; &lt;p&gt;Interleave thinking lets an AI agent reason, act, and observe in tight loops, so it can adapt step-by-step to new information instead of blindly following a fixed plan.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbal3o/finally_deepseek_supports_interleave_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:39:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1paqxs0</id>
    <title>$900 for 192GB RAM on Oct 23rd, now costs over $3k</title>
    <updated>2025-11-30T19:24:34+00:00</updated>
    <author>
      <name>/u/Hoppss</name>
      <uri>https://old.reddit.com/user/Hoppss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"&gt; &lt;img alt="$900 for 192GB RAM on Oct 23rd, now costs over $3k" src="https://preview.redd.it/ka8j4duh3g4g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f3905d157af26fc5e6596ee0ac48570cd8592339" title="$900 for 192GB RAM on Oct 23rd, now costs over $3k" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Two 96GB kits cost me $900 on Oct 23rd. Now one month later trying to get an equivalent amount costs about $3200.. Just insane. Wondering what the prices are going to be late 2026, considering word is that this isn't going to be getting better until 2027. Prices here are in CAD btw. USD equivalent is about $650 vs $2300.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hoppss"&gt; /u/Hoppss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ka8j4duh3g4g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1paqxs0/900_for_192gb_ram_on_oct_23rd_now_costs_over_3k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-11-30T19:24:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbfisv</id>
    <title>Stable-diffusion.cpp now supports Z-image</title>
    <updated>2025-12-01T15:22:37+00:00</updated>
    <author>
      <name>/u/Languages_Learner</name>
      <uri>https://old.reddit.com/user/Languages_Learner</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/leejet/stable-diffusion.cpp/releases/tag/master-385-34a6fd4"&gt;Release master-385-34a6fd4 ¬∑ leejet/stable-diffusion.cpp&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Languages_Learner"&gt; /u/Languages_Learner &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbfisv/stablediffusioncpp_now_supports_zimage/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T15:22:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbaf8x</id>
    <title>Deepseek v3.2 speciale, it has good benchmarks!</title>
    <updated>2025-12-01T11:30:04+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt; &lt;img alt="Deepseek v3.2 speciale, it has good benchmarks!" src="https://b.thumbs.redditmedia.com/iiaAB9rg5iLZJ19pyyl1FcJVxikiym9_FeMfdKGkRKM.jpg" title="Deepseek v3.2 speciale, it has good benchmarks!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale"&gt;https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Speciale&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Benchmarks are in the link.. &lt;/p&gt; &lt;p&gt;It scores higher than GPT 5 high in HLE and Codeforce. I tried it out on their site which is the normal 3.2 not speciale , im not sure if it is better than gpt 5 ‚Ä¶ &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/kaascz2jwk4g1.png?width=4691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8f6201d292d566347185bc8b9f8d1cc2cbc414"&gt;https://preview.redd.it/kaascz2jwk4g1.png?width=4691&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0f8f6201d292d566347185bc8b9f8d1cc2cbc414&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbaf8x/deepseek_v32_speciale_it_has_good_benchmarks/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb7i6d</id>
    <title>Upcoming vllm Mistral Large 3 support</title>
    <updated>2025-12-01T08:27:38+00:00</updated>
    <author>
      <name>/u/brown2green</name>
      <uri>https://old.reddit.com/user/brown2green</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"&gt; &lt;img alt="Upcoming vllm Mistral Large 3 support" src="https://external-preview.redd.it/3kkJBT6LzSWLFjvnTMUkLMsNU4IL09qtTW7VM1HkHgk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c48b898fbe4fe47c426c67ed567cfa0160764345" title="Upcoming vllm Mistral Large 3 support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/brown2green"&gt; /u/brown2green &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/vllm-project/vllm/pull/29757"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb7i6d/upcoming_vllm_mistral_large_3_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T08:27:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbc99o</id>
    <title>I built a tool that can interactively create diagrams with LLMs</title>
    <updated>2025-12-01T13:05:00+00:00</updated>
    <author>
      <name>/u/daweii</name>
      <uri>https://old.reddit.com/user/daweii</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"&gt; &lt;img alt="I built a tool that can interactively create diagrams with LLMs" src="https://external-preview.redd.it/MHhoMGF2OWdjbDRnMTZ-ggrpklnNsXE3h3FCcz0k2D7KroK_010AEhCs0S-l.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4d943751a927ad3ba1166257f385b2e80ce567f1" title="I built a tool that can interactively create diagrams with LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, &lt;/p&gt; &lt;p&gt;I built an open-source tool that generates editable drawiodiagrams using LLMs. &lt;/p&gt; &lt;p&gt;This outputs actual XML. You can generate a base diagram, then manually drag/drop elements to fix it, or ask the LLM to refine specific parts. &lt;/p&gt; &lt;p&gt;I added native Ollama support so you can generate architecture diagrams without sending sensitive stack details to OpenAI/Anthropic. &lt;/p&gt; &lt;p&gt;Features:&lt;br /&gt; - Manipulates drawio XML directly.&lt;br /&gt; - Supports AWS, GCP, and Azure icon sets.&lt;br /&gt; - Visual history/diffing (easy to undo hallucinations).&lt;br /&gt; - Works with OpenAI compatible endpoints (Ollama, LM Studio, etc.). &lt;/p&gt; &lt;p&gt;I'd love feedback on how it performs with big local models (&amp;gt;30B), or ideas for v2 (e.g., adding MCP support). &lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/DayuanJiang/next-ai-draw-io"&gt;https://github.com/DayuanJiang/next-ai-draw-io&lt;/a&gt;&lt;br /&gt; Demo: &lt;a href="https://next-ai-draw-io.vercel.app/"&gt;https://next-ai-draw-io.vercel.app/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/daweii"&gt; /u/daweii &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/4hpwso9gcl4g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbc99o/i_built_a_tool_that_can_interactively_create/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:05:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb9vzg</id>
    <title>deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face</title>
    <updated>2025-12-01T10:59:20+00:00</updated>
    <author>
      <name>/u/minpeter2</name>
      <uri>https://old.reddit.com/user/minpeter2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9vzg/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/minpeter2"&gt; /u/minpeter2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9vzg/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9vzg/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T10:59:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1pbcjql</id>
    <title>That's why open source is even better than closed source</title>
    <updated>2025-12-01T13:18:19+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"&gt; &lt;img alt="That's why open source is even better than closed source" src="https://b.thumbs.redditmedia.com/agNOvW0vm50YDgwkyK-R0Hgp6zZPP2w6C0E9hCz_ixA.jpg" title="That's why open source is even better than closed source" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Chatgpt , No one is spared from ads, even the Pro Plan throws you an ad üíÄ&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pbcjql"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pbcjql/thats_why_open_source_is_even_better_than_closed/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T13:18:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pb9xm3</id>
    <title>deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face</title>
    <updated>2025-12-01T11:01:43+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt; &lt;img alt="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" src="https://external-preview.redd.it/2DgE6Nx11cfl0KA4q_jdWtEOsZKhXgwGdD7Iw7jyvX8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e4787c26efff2156fccbd5d67ab061987d38be00" title="deepseek-ai/DeepSeek-V3.2 ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Introduction&lt;/h1&gt; &lt;p&gt;We introduce &lt;strong&gt;DeepSeek-V3.2&lt;/strong&gt;, a model that harmonizes high computational efficiency with superior reasoning and agent performance. Our approach is built upon three key technical breakthroughs:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;strong&gt;DeepSeek Sparse Attention (DSA):&lt;/strong&gt; We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance, specifically optimized for long-context scenarios.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Scalable Reinforcement Learning Framework:&lt;/strong&gt; By implementing a robust RL protocol and scaling post-training compute, &lt;em&gt;DeepSeek-V3.2&lt;/em&gt; performs comparably to GPT-5. Notably, our high-compute variant, &lt;strong&gt;DeepSeek-V3.2-Speciale&lt;/strong&gt;, &lt;strong&gt;surpasses GPT-5&lt;/strong&gt; and exhibits reasoning proficiency on par with Gemini-3.0-Pro. &lt;ul&gt; &lt;li&gt;&lt;em&gt;Achievement:&lt;/em&gt; ü•á &lt;strong&gt;Gold-medal performance&lt;/strong&gt; in the 2025 International Mathematical Olympiad (IMO) and International Olympiad in Informatics (IOI).&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Large-Scale Agentic Task Synthesis Pipeline:&lt;/strong&gt; To integrate &lt;strong&gt;reasoning into tool-use&lt;/strong&gt; scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This facilitates scalable agentic post-training, improving compliance and generalization in complex interactive environments.&lt;/li&gt; &lt;/ol&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pb9xm3/deepseekaideepseekv32_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-01T11:01:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
