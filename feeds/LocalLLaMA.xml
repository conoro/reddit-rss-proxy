<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-09T23:06:42+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1pil6ct</id>
    <title>RTX 3050 laptop</title>
    <updated>2025-12-09T22:20:54+00:00</updated>
    <author>
      <name>/u/Proud_Clerk_8448</name>
      <uri>https://old.reddit.com/user/Proud_Clerk_8448</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello friends, I'm going to buy a new laptop, and when I wanted to buy it, many people told me that I haven't worked locally, so the laptop doesn't matter. I'm actually hesitant about whether to pay more or save money and get a weaker version, which will most likely be used in my country since I don't want to do business there. Do I actually have a chance of working locally if I get an RTX 3050 6GB and 192 AI Tops? Will it benefit me in any way? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Proud_Clerk_8448"&gt; /u/Proud_Clerk_8448 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil6ct/rtx_3050_laptop/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil6ct/rtx_3050_laptop/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pil6ct/rtx_3050_laptop/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T22:20:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1piejs2</id>
    <title>Day 2: 21 Days of Building a Small Language Model: Understanding Linear Regression</title>
    <updated>2025-12-09T18:05:16+00:00</updated>
    <author>
      <name>/u/Prashant-Lakhera</name>
      <uri>https://old.reddit.com/user/Prashant-Lakhera</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piejs2/day_2_21_days_of_building_a_small_language_model/"&gt; &lt;img alt="Day 2: 21 Days of Building a Small Language Model: Understanding Linear Regression" src="https://b.thumbs.redditmedia.com/-kWXfpyrQqoRaStYWFglwErapB6_pj-STPIVOxBO4RI.jpg" title="Day 2: 21 Days of Building a Small Language Model: Understanding Linear Regression" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cpa8hcbnx76g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e945c616a00eaa2e56a5ec917ea8395812f56065"&gt;https://preview.redd.it/cpa8hcbnx76g1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e945c616a00eaa2e56a5ec917ea8395812f56065&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a mistake I see far too often: people get excited about building neural networks, transformers, or language models, and they jump straight into complex architectures without first understanding the fundamentals. They copy code from tutorials, run it, see it work, and think they understand machine learning. But when something goes wrong, when the loss doesn't decrease, when predictions are wrong, when the model doesn't train, they're lost. They don't know what's happening under the hood, so they can't debug, can't modify, and can't truly understand what they've built.&lt;/p&gt; &lt;p&gt;That's why I believe it's absolutely necessary that people first build a Linear Regression model from scratch.&lt;/p&gt; &lt;p&gt;Not just understand it theoretically. Not just read about it. But actually build it, line by line, understanding every component. When you build linear regression yourself, you're forced to understand:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;How data flows through a model&lt;/li&gt; &lt;li&gt;How loss functions measure error&lt;/li&gt; &lt;li&gt;How gradients are computed&lt;/li&gt; &lt;li&gt;How optimizers update weights&lt;/li&gt; &lt;li&gt;How the training loop works&lt;/li&gt; &lt;li&gt;What happens when things go wrong&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;These aren't abstract concepts when you've implemented them yourself. They become concrete, tangible, and deeply understood.&lt;/p&gt; &lt;p&gt;The foundation you build with linear regression supports everything that comes after. When you later build a neural network with multiple layers, you'll recognize: &amp;quot;Oh, this is just multiple linear regressions stacked together!&amp;quot; When you implement backpropagation in a transformer, you'll think: &amp;quot;This is the same process I used in linear regression, just applied to more layers.&amp;quot; When you debug a training issue, you'll know where to look because you understand the fundamentals.&lt;/p&gt; &lt;p&gt;Skipping linear regression is like trying to build a house without a foundation. You might get something that looks like it works, but it's fragile, and when problems arise, you won't know how to fix them.&lt;/p&gt; &lt;p&gt;Take the time to build linear regression first. It might seem like a detour, but it's actually the fastest path to truly understanding machine learning. The hours you invest in mastering the fundamentals will save you days or weeks of confusion later when working with more complex models.&lt;/p&gt; &lt;p&gt;üîó Blog link: &lt;a href="https://www.linkedin.com/pulse/day-2-21-days-building-small-language-model-linear-your-lakhera-kqiic"&gt;https://www.linkedin.com/pulse/day-2-21-days-building-small-language-model-linear-your-lakhera-kqiic&lt;/a&gt;&lt;/p&gt; &lt;p&gt;üîó Code link: &lt;a href="https://colab.research.google.com/drive/1i1hacZZUGzoRE3luDE2KtS--honPnoa8?usp=sharing"&gt;https://colab.research.google.com/drive/1i1hacZZUGzoRE3luDE2KtS--honPnoa8?usp=sharing&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Prashant-Lakhera"&gt; /u/Prashant-Lakhera &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piejs2/day_2_21_days_of_building_a_small_language_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piejs2/day_2_21_days_of_building_a_small_language_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piejs2/day_2_21_days_of_building_a_small_language_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T18:05:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1pijhwy</id>
    <title>RewardHackWatch | Open-source Runtime detector for reward hacking and misalignment in LLM agents (89.7% F1)</title>
    <updated>2025-12-09T21:14:58+00:00</updated>
    <author>
      <name>/u/aerosta_ai</name>
      <uri>https://old.reddit.com/user/aerosta_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pijhwy/rewardhackwatch_opensource_runtime_detector_for/"&gt; &lt;img alt="RewardHackWatch | Open-source Runtime detector for reward hacking and misalignment in LLM agents (89.7% F1)" src="https://preview.redd.it/vqfplqx3t86g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bdd408e362b34e57a393260dccd05a44c447ad7" title="RewardHackWatch | Open-source Runtime detector for reward hacking and misalignment in LLM agents (89.7% F1)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An open-source runtime detection system that identifies when LLM agents exploit loopholes in their reward functions and tracks whether these behaviors generalize to broader misalignment.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key results&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;89.7% F1 on 5,391 MALT trajectories&lt;/li&gt; &lt;li&gt;Novel RMGI metric for detecting hack -&amp;gt; misalignment transitions &lt;/li&gt; &lt;li&gt;Significantly outperforms keyword (0.1% F1) and regex (4.9% F1) baselines &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What it detects&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Test manipulation (e.g., sys.exit(), test bypassing) &lt;/li&gt; &lt;li&gt;Reward tampering - Eval gaming &lt;/li&gt; &lt;li&gt;Deceptive patterns in chain-of-thought &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Inspired by Anthropic's 2025 paper on emergent misalignment from reward hacking. Feedback and ideas for stronger evals are very welcome. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GitHub: &lt;a href="https://github.com/aerosta/rewardhackwatch"&gt;https://github.com/aerosta/rewardhackwatch&lt;/a&gt;&lt;/li&gt; &lt;li&gt;HuggingFace: &lt;a href="https://huggingface.co/aerosta/rewardhackwatch"&gt;https://huggingface.co/aerosta/rewardhackwatch&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Paper (PDF): &lt;a href="https://github.com/aerosta/rewardhackwatch/blob/main/paper/RewardHackWatch.pdf"&gt;https://github.com/aerosta/rewardhackwatch/blob/main/paper/RewardHackWatch.pdf&lt;/a&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aerosta_ai"&gt; /u/aerosta_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vqfplqx3t86g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pijhwy/rewardhackwatch_opensource_runtime_detector_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pijhwy/rewardhackwatch_opensource_runtime_detector_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T21:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi3aah</id>
    <title>ZAI Open Source AutoGLM --A AI Phone Agent</title>
    <updated>2025-12-09T09:36:03+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/zai-org/AutoGLM-Phone-9B"&gt;https://huggingface.co/zai-org/AutoGLM-Phone-9B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/zai-org/Open-AutoGLM"&gt;https://github.com/zai-org/Open-AutoGLM&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi3aah/zai_open_source_autoglm_a_ai_phone_agent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T09:36:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1phz8vy</id>
    <title>The Absurdity of the prices of consumer RAM versus ECC RAM</title>
    <updated>2025-12-09T05:21:16+00:00</updated>
    <author>
      <name>/u/Substantial_Cut_9418</name>
      <uri>https://old.reddit.com/user/Substantial_Cut_9418</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"&gt; &lt;img alt="The Absurdity of the prices of consumer RAM versus ECC RAM" src="https://b.thumbs.redditmedia.com/qkgwBgpSDc_KLfK_m0RtTj-8qk1N-2lbUTbO4kdymGg.jpg" title="The Absurdity of the prices of consumer RAM versus ECC RAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Substantial_Cut_9418"&gt; /u/Substantial_Cut_9418 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1phz8vy"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phz8vy/the_absurdity_of_the_prices_of_consumer_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T05:21:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1phzzrq</id>
    <title>Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?</title>
    <updated>2025-12-09T06:04:27+00:00</updated>
    <author>
      <name>/u/__issac</name>
      <uri>https://old.reddit.com/user/__issac</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"&gt; &lt;img alt="Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?" src="https://preview.redd.it/5a2im0y2d46g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ef5e61c3d9ead02bd600ec3c330ee11fda109956" title="Is qwen3 4b or a3b better than the first gpt4(2023)? What do you think?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;(I know Artificial Analysis is suck. But is interesting:)) I think now the hype is almost gone, so I have some question. Benchmark says thier models(even 30b a3b and 4b!) beat gpt4. But what do you think? Please don't tell me &amp;quot;depends on field&amp;quot;. We should compare on overall performance. Because benchmark says it is. Can we now truly replace old flagship closed-source model with a small open model?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__issac"&gt; /u/__issac &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5a2im0y2d46g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phzzrq/is_qwen3_4b_or_a3b_better_than_the_first_gpt42023/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T06:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1pij3qd</id>
    <title>Need help with Mistral-Vibe and GGUF.</title>
    <updated>2025-12-09T20:58:40+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;EDIT #2&lt;/em&gt; Everything work if you merge the PR&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/ZoAC6wK.png"&gt;https://i.imgur.com/ZoAC6wK.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Edit&lt;/em&gt; This might actually already being work on : &lt;a href="https://github.com/mistralai/mistral-vibe/pull/13"&gt;https://github.com/mistralai/mistral-vibe/pull/13&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm not able to get Mistral-Vibe to work with the GGUF, but i'm not super technical, and there not much info out. &lt;/p&gt; &lt;p&gt;Any help welcome.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/I83oPpW.png"&gt;https://i.imgur.com/I83oPpW.png&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'm loading it with :&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-server --jinja --model /Volumes/SSD2/llm-model/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF/mistralai_Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf --temp 0.2 -c 75000 &lt;/code&gt;&lt;/pre&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pij3qd/need_help_with_mistralvibe_and_gguf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pij3qd/need_help_with_mistralvibe_and_gguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pij3qd/need_help_with_mistralvibe_and_gguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T20:58:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1phjxca</id>
    <title>I'm calling these people out right now.</title>
    <updated>2025-12-08T18:21:39+00:00</updated>
    <author>
      <name>/u/WeMetOnTheMountain</name>
      <uri>https://old.reddit.com/user/WeMetOnTheMountain</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;For being heroes of the community.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Unsloth&lt;/strong&gt;|Blazing fast fine-tuning + premium GGUF quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;mradermacher&lt;/strong&gt;|Quantizes literally EVERYTHING, absolute machine&lt;/li&gt; &lt;li&gt;&lt;strong&gt;bartowski&lt;/strong&gt;|High-quality quants, great documentation&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TheBloke&lt;/strong&gt;|The OG - before he stepped back, he was THE source&lt;/li&gt; &lt;li&gt;&lt;strong&gt;LoneStriker&lt;/strong&gt;|Solid AWQ/GPTQ quants&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Nexesenex&lt;/strong&gt;|iMatrix quants, gap hunter and filler&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everyone here owes so much to you folks. Take a bow.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WeMetOnTheMountain"&gt; /u/WeMetOnTheMountain &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phjxca/im_calling_these_people_out_right_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T18:21:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9fpf</id>
    <title>PaCoRe: The first open-source deep think 8B model beats GPT-5 on HMMT25</title>
    <updated>2025-12-09T14:54:13+00:00</updated>
    <author>
      <name>/u/Fancy_Fanqi77</name>
      <uri>https://old.reddit.com/user/Fancy_Fanqi77</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"&gt; &lt;img alt="PaCoRe: The first open-source deep think 8B model beats GPT-5 on HMMT25" src="https://external-preview.redd.it/36cc9Kci4vmEFbiqweyD9wkzOHfQN0s2ACnf4eu8ZUE.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=583cedaf4f617529e9733518fd2bbaac2593e3a3" title="PaCoRe: The first open-source deep think 8B model beats GPT-5 on HMMT25" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Introducing Parallel Coordinated Reasoning (PaCoRe)&lt;/p&gt; &lt;p&gt;An 8B &lt;strong&gt;model beats GPT-5 on&lt;/strong&gt; HMMT25 by unlocking parallel thinking for te&lt;strong&gt;st-time scaling!&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The first open-source &lt;strong&gt;deep think:&lt;/strong&gt; data + model + inference code!&lt;/p&gt; &lt;p&gt;MIT-licensed ‚Äî use it however you want&lt;/p&gt; &lt;p&gt;- Github: &lt;a href="https://github.com/stepfun-ai/PaCoRe"&gt;https://github.com/stepfun-ai/PaCoRe&lt;/a&gt;&lt;br /&gt; - Paper: &lt;a href="https://github.com/stepfun-ai/PaCoRe/blob/main/pacore_report.pdf"&gt;https://github.com/stepfun-ai/PaCoRe/blob/main/pacore_report.pdf&lt;/a&gt;&lt;br /&gt; - Model: &lt;a href="https://huggingface.co/stepfun-ai/PaCoRe-8B"&gt;https://huggingface.co/stepfun-ai/PaCoRe-8B&lt;/a&gt;&lt;br /&gt; - Data: &lt;a href="https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k"&gt;https://huggingface.co/datasets/stepfun-ai/PaCoRe-Train-8k&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/vgqyqe7fy66g1.png?width=814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0db9e68bf4e8750170acb24abb42a1b26eb2764"&gt;https://preview.redd.it/vgqyqe7fy66g1.png?width=814&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d0db9e68bf4e8750170acb24abb42a1b26eb2764&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7tufs4jhy66g1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5ce148934a1569d2df62cf271d9bb7d36ad94f3"&gt;https://preview.redd.it/7tufs4jhy66g1.png?width=851&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b5ce148934a1569d2df62cf271d9bb7d36ad94f3&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fancy_Fanqi77"&gt; /u/Fancy_Fanqi77 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9fpf/pacore_the_first_opensource_deep_think_8b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1piklt8</id>
    <title>Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md</title>
    <updated>2025-12-09T21:59:11+00:00</updated>
    <author>
      <name>/u/FastDecode1</name>
      <uri>https://old.reddit.com/user/FastDecode1</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piklt8/linux_foundation_announces_the_formation_of_the/"&gt; &lt;img alt="Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md" src="https://external-preview.redd.it/LOt13m09jkanBZNSJY_12A-wBrJw_RiimpI3OBp-Oqo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d2624b56d80a1d938eb72c11fc7efad59cb3ee98" title="Linux Foundation Announces the Formation of the Agentic AI Foundation (AAIF), Anchored by New Project Contributions Including Model Context Protocol (MCP), goose and AGENTS.md" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FastDecode1"&gt; /u/FastDecode1 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.linuxfoundation.org/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piklt8/linux_foundation_announces_the_formation_of_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piklt8/linux_foundation_announces_the_formation_of_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T21:59:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1picexa</id>
    <title>[OPENSOURCE] Whisper finetuning, inference, auto gpu upscale, proxy and co</title>
    <updated>2025-12-09T16:47:51+00:00</updated>
    <author>
      <name>/u/Wide_Appointment9924</name>
      <uri>https://old.reddit.com/user/Wide_Appointment9924</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;With my cofounder we spent 2 months building a system to simply generate synthetic data and train Whisper Large V3 Turbo.&lt;/p&gt; &lt;p&gt;We reach on average +50% accuracy.&lt;/p&gt; &lt;p&gt;We built a whole infra like Deepgram that can auto upscale GPUs based on usage, with a proxy to dispatch based on location and inference in 300MS for voice AI.&lt;/p&gt; &lt;p&gt;The company is shutting down but we decided to open source everything.&lt;/p&gt; &lt;p&gt;Feel free to reach out if you need help with setup or usage ‚úåüèª&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/orgs/LATICE-AI/"&gt;https://github.com/orgs/LATICE-AI/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Wide_Appointment9924"&gt; /u/Wide_Appointment9924 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1picexa/opensource_whisper_finetuning_inference_auto_gpu/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1picexa/opensource_whisper_finetuning_inference_auto_gpu/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1picexa/opensource_whisper_finetuning_inference_auto_gpu/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T16:47:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pil53r</id>
    <title>AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision</title>
    <updated>2025-12-09T22:19:37+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"&gt; &lt;img alt="AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision" src="https://b.thumbs.redditmedia.com/3AFqcVzBcIqkShsFZeKC1B1EUCe3mUgvy1_R1nSDzVc.jpg" title="AI-benchmark results for Snapdragon 8 Elite Gen 5 are in, absolutely rips at 8-bit precision" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Twice as fast at running 8-bit transformers than the previous generation.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1pil53r"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pil53r/aibenchmark_results_for_snapdragon_8_elite_gen_5/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T22:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1picehv</id>
    <title>Llama.cpp Vulkan benchmarks by Phoronix</title>
    <updated>2025-12-09T16:47:24+00:00</updated>
    <author>
      <name>/u/ymmvxd</name>
      <uri>https://old.reddit.com/user/ymmvxd</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ymmvxd"&gt; /u/ymmvxd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.phoronix.com/review/llama-cpp-vulkan-eoy2025/2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1picehv/llamacpp_vulkan_benchmarks_by_phoronix/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1picehv/llamacpp_vulkan_benchmarks_by_phoronix/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T16:47:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi4yr4</id>
    <title>GLM-4.6V Model Now Available in GGUF Format</title>
    <updated>2025-12-09T11:20:33+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"&gt; &lt;img alt="GLM-4.6V Model Now Available in GGUF Format" src="https://external-preview.redd.it/DmQpOneG32bl0j63UZH5xIwLDgq-lgYKNllu4rNGOIU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e1491231818dca9afc91a1c8bbef01c666b2238d" title="GLM-4.6V Model Now Available in GGUF Format" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I recently came across the GGUF version of the popular GLM-4.6V Flash model. I shared this as this will be useful to many who want to try this model.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/unsloth/GLM-4.6V-Flash-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi4yr4/glm46v_model_now_available_in_gguf_format/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T11:20:33+00:00</published>
  </entry>
  <entry>
    <id>t3_1phn925</id>
    <title>Thoughts?</title>
    <updated>2025-12-08T20:25:29+00:00</updated>
    <author>
      <name>/u/Salt_Armadillo8884</name>
      <uri>https://old.reddit.com/user/Salt_Armadillo8884</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt; &lt;img alt="Thoughts?" src="https://preview.redd.it/j6fp9xhsh16g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2979be36927eb9e804221b6247830706ea9e7487" title="Thoughts?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Interesting take&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Salt_Armadillo8884"&gt; /u/Salt_Armadillo8884 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/j6fp9xhsh16g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phn925/thoughts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-08T20:25:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1piasv8</id>
    <title>MagicQuant - Hybrid Evolution GGUF (TPS boosts, precision gains, full transparency)</title>
    <updated>2025-12-09T15:47:38+00:00</updated>
    <author>
      <name>/u/crossivejoker</name>
      <uri>https://old.reddit.com/user/crossivejoker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been building a system that evolves &lt;strong&gt;hybrid GGUF quantizations&lt;/strong&gt; to automatically find the best tensor level mix for any model. It‚Äôs called &lt;strong&gt;MagicQuant&lt;/strong&gt;, and the whole idea is simple:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Stop guessing quant types. Let the math decide the optimal configuration.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;MagicQuant runs survival rounds, epsilon-greedy exploration, precision-loss scoring, TPS benchmarking, and a ton of tensor-group heuristics to evolve better (and sometimes &lt;em&gt;way&lt;/em&gt; better) GGUFs than standard baselines.&lt;/p&gt; &lt;p&gt;And the results so far have been amazing.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Example: Seed-OSS 36B&lt;/h2&gt; &lt;p&gt;This is one of the crazier results I‚Äôve gotten so far.&lt;/p&gt; &lt;p&gt;The best Q4-range baseline was &lt;strong&gt;IQ4_NL&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;19.31 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;27.70 TPS&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;1.1076% precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MagicQuant evolved a hybrid at:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;18.95 GB&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;32.00 TPS&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;0.2709% precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;So:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Slightly smaller&lt;/li&gt; &lt;li&gt;&lt;strong&gt;+15.5% faster&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;~75% LESS precision loss&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This hybrid: &lt;a href="https://huggingface.co/magiccodingman/Seed-OSS-36B-Instruct-unsloth-MagicQuant-Hybrid-GGUF"&gt;mxfp4_moe-EHQKOUD-IQ4NL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is the kind of thing MagicQuant keeps finding.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;MagicQuant Hybrids for Seed OSS 36B&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model_name&lt;/th&gt; &lt;th&gt;file_size_gb&lt;/th&gt; &lt;th&gt;bench_tps&lt;/th&gt; &lt;th&gt;avg_prec_loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-HK-B16-EO-Q5K-QUD-Q8_0&lt;/td&gt; &lt;td&gt;39.71&lt;/td&gt; &lt;td&gt;17.73&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.0213%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-O-MXFP4-EHQKUD-Q8_0&lt;/td&gt; &lt;td&gt;35.78&lt;/td&gt; &lt;td&gt;18.72&lt;/td&gt; &lt;td&gt;0.0272%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-E-B16-D-IQ4NL-KOU-Q6K-HQ-Q8_0&lt;/td&gt; &lt;td&gt;28.02&lt;/td&gt; &lt;td&gt;24.27&lt;/td&gt; &lt;td&gt;0.1768%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-EHQKOUD-Q6K&lt;/td&gt; &lt;td&gt;27.63&lt;/td&gt; &lt;td&gt;23.34&lt;/td&gt; &lt;td&gt;0.2037%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;strong&gt;mxfp4_moe-EHQKOUD-IQ4NL&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;18.95&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;32.00&lt;/strong&gt;&lt;/td&gt; &lt;td&gt;&lt;strong&gt;0.2709%&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;mxfp4_moe-HQKU-IQ4NL-EOD-MXFP4&lt;/td&gt; &lt;td&gt;18.66&lt;/td&gt; &lt;td&gt;26.90&lt;/td&gt; &lt;td&gt;0.7098%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MXFP4_MOE&lt;/td&gt; &lt;td&gt;17.90&lt;/td&gt; &lt;td&gt;20.46&lt;/td&gt; &lt;td&gt;2.7338%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;hr /&gt; &lt;h2&gt;Baseline Reference (for comparison)&lt;/h2&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model_name&lt;/th&gt; &lt;th&gt;file_size_gb&lt;/th&gt; &lt;th&gt;bench_tps&lt;/th&gt; &lt;th&gt;avg_prec_loss&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;BF16&lt;/td&gt; &lt;td&gt;67.35&lt;/td&gt; &lt;td&gt;11.48&lt;/td&gt; &lt;td&gt;0.0000%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q8_0&lt;/td&gt; &lt;td&gt;35.78&lt;/td&gt; &lt;td&gt;17.77&lt;/td&gt; &lt;td&gt;0.0272%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q6_K&lt;/td&gt; &lt;td&gt;27.63&lt;/td&gt; &lt;td&gt;22.95&lt;/td&gt; &lt;td&gt;0.2037%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q5_K&lt;/td&gt; &lt;td&gt;23.84&lt;/td&gt; &lt;td&gt;22.04&lt;/td&gt; &lt;td&gt;0.2923%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;IQ4_NL&lt;/td&gt; &lt;td&gt;19.31&lt;/td&gt; &lt;td&gt;27.70&lt;/td&gt; &lt;td&gt;1.1076%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;MXFP4_MOE&lt;/td&gt; &lt;td&gt;17.90&lt;/td&gt; &lt;td&gt;20.46&lt;/td&gt; &lt;td&gt;2.7338%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;Q4_K_M&lt;/td&gt; &lt;td&gt;20.27&lt;/td&gt; &lt;td&gt;26.65&lt;/td&gt; &lt;td&gt;2.9161%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;MagicQuant compares everything against these to determine the ‚Äúwinner.‚Äù&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;What MagicQuant keeps discovering&lt;/h2&gt; &lt;p&gt;Different architectures respond to quantization very differently:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Some &lt;em&gt;love&lt;/em&gt; MXFP4.&lt;/li&gt; &lt;li&gt;Some prefer IQ4_NL.&lt;/li&gt; &lt;li&gt;Some models randomly explode in quality on Q5_K.&lt;/li&gt; &lt;li&gt;Seed-OSS ditched most baselines entirely.&lt;/li&gt; &lt;li&gt;Apriel 1.5-15B? That model is a complete gremlin, it loves &lt;strong&gt;Q5_K&lt;/strong&gt; more than anything else I‚Äôve thrown at it.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;MagicQuant isn‚Äôt about producing hybrids for the sake of hybrids. &lt;strong&gt;MagicQuant is the verdict, whatever wins stays.&lt;/strong&gt; Sometimes that‚Äôs a hybrid. Sometimes the baseline reigns king. Sometimes Q6_K beats Q8_0 in both TPS and precision. Sometimes Q4_K_M outperforms IQ4_NL on &lt;em&gt;certain models.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Everything depends on the architecture.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Philosophically&lt;/h2&gt; &lt;p&gt;I‚Äôm honestly tired of downloading Q8/Q6/Q5/Q4 files with &lt;strong&gt;no benchmarks&lt;/strong&gt;. If a quant is bigger, slower, &lt;em&gt;and&lt;/em&gt; more precision loss, why use it? If a smaller quant loses 5% precision, I want to &lt;strong&gt;see that number&lt;/strong&gt; before downloading.&lt;/p&gt; &lt;p&gt;MagicQuant is my attempt at making quantization:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;empirical&lt;/li&gt; &lt;li&gt;transparent&lt;/li&gt; &lt;li&gt;repeatable&lt;/li&gt; &lt;li&gt;and actually &lt;em&gt;useful&lt;/em&gt; for the community&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Every model will always include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;benchmark TPS&lt;/li&gt; &lt;li&gt;precision loss scoring&lt;/li&gt; &lt;li&gt;file size&lt;/li&gt; &lt;li&gt;the full hybrid naming breakdown&lt;/li&gt; &lt;li&gt;data sets&lt;/li&gt; &lt;li&gt;methodology&lt;/li&gt; &lt;li&gt;raw results&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Everything is open and reproducible.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;HuggingFace Collection&lt;/h2&gt; &lt;p&gt;All MagicQuant releases live here: &lt;a href="https://huggingface.co/collections/magiccodingman/magic-quant"&gt;https://huggingface.co/collections/magiccodingman/magic-quant&lt;/a&gt;&lt;/p&gt; &lt;p&gt;More hybrids are already in the pipeline.&lt;/p&gt; &lt;p&gt;Right now a dense 4B model takes ~2-3 hours to run. A 30B MOE takes ~24 hours (MOE takes ~double as long due to sensitivity). My prediction engine has to build sample data until confidence is high enough that it can properly predict hybrids. Some models are easier than others. Sine dense models need only 46-55 samples, while others need 120 samples, while some need more or less. The engine figures that out.&lt;/p&gt; &lt;hr /&gt; &lt;h2&gt;Documentation / Wiki&lt;/h2&gt; &lt;p&gt;Full documentation, philosophy, naming scheme, methodology, and technical breakdown: &lt;a href="https://github.com/magiccodingman/MagicQuant-Wiki"&gt;https://github.com/magiccodingman/MagicQuant-Wiki&lt;/a&gt;&lt;/p&gt; &lt;p&gt;MagicQuant is still evolving, but the results so far have been extremely promising and the more models I run, the weirder and more interesting the quantization patterns become.&lt;/p&gt; &lt;hr /&gt; &lt;p&gt;But if you have any suggestions, requests for MagicQuant models, holes to poke, I'm all ears.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crossivejoker"&gt; /u/crossivejoker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piasv8/magicquant_hybrid_evolution_gguf_tps_boosts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:47:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1pib8z9</id>
    <title>New ways to roast people in the AI era</title>
    <updated>2025-12-09T16:04:20+00:00</updated>
    <author>
      <name>/u/InternationalAsk1490</name>
      <uri>https://old.reddit.com/user/InternationalAsk1490</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the AI era, we can update the way we roast people.&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;nerd,&amp;quot; try saying &amp;quot;benchmaxxed.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;brain-dead,&amp;quot; try saying &amp;quot;pruned/quantized.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;no brain,&amp;quot; try saying &amp;quot;low params count.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;didn't study,&amp;quot; try saying &amp;quot;undertrained.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;only knows book knowledge,&amp;quot; try saying &amp;quot;overfitted.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;boring and dull,&amp;quot; try saying &amp;quot;safetymaxxed.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;slow to react,&amp;quot; try saying &amp;quot;slow prompt processing/token generation.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;clumsy,&amp;quot; try saying &amp;quot;poor tool use performance.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;talks nonsense endlessly,&amp;quot; try saying &amp;quot;temperature too high/missing EOS.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;speaks gibberish,&amp;quot; try saying &amp;quot;template config error/topK sampling error.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;disobedient,&amp;quot; try saying &amp;quot;non-instruct base model.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;doesn't think with the brain,&amp;quot; try saying &amp;quot;non-thinking instruct model.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;poor memory,&amp;quot; try saying &amp;quot;low context window.&amp;quot;&lt;/p&gt; &lt;p&gt;Instead of saying &amp;quot;easily fooled,&amp;quot; try saying &amp;quot;vulnerable to prompt injection.&amp;quot;&lt;/p&gt; &lt;p&gt;It's normal if you don't understand any of this. If you understand all of these, go outside and touch some grass.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InternationalAsk1490"&gt; /u/InternationalAsk1490 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pib8z9/new_ways_to_roast_people_in_the_ai_era/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T16:04:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1piduwm</id>
    <title>I wanted audiobooks of stories that don't exist - so I built an app to read them to me</title>
    <updated>2025-12-09T17:40:47+00:00</updated>
    <author>
      <name>/u/DigiJoe79</name>
      <uri>https://old.reddit.com/user/DigiJoe79</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt; &lt;img alt="I wanted audiobooks of stories that don't exist - so I built an app to read them to me" src="https://b.thumbs.redditmedia.com/522Sef2MfP-ZR7uASytJh9L2_m1rO3FnZwVv7ALL4eE.jpg" title="I wanted audiobooks of stories that don't exist - so I built an app to read them to me" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;After multiple weeks of work, I'm excited to share my passion project: an open-source desktop app for creating audiobooks using AI text-to-speech with voice cloning.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;The story behind it:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I wanted to listen to fan fiction and web novels that don't have audiobook versions. Commercial TTS services are expensive and therer workflos is not focused on audiobook generation. So I built my own solution that runs completely locally on your machine - no subscriptions, no cloud, your data stays private.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What makes it different:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Clean drag &amp;amp; drop interface for organizing chapters and segments&lt;/li&gt; &lt;li&gt;Supports multiple TTS engines (XTTS, Chatterbox) - swap them as you like&lt;/li&gt; &lt;li&gt;Built-in quality check using Whisper to catch mispronunciations and Silero-VAD for audio issues&lt;/li&gt; &lt;li&gt;Import full books in .md Format and use spaCy for autosegmentation&lt;/li&gt; &lt;li&gt;Pronunciation rules to fix words the AI struggles with&lt;/li&gt; &lt;li&gt;Engine template for hassle-free adding of new engines as they get released&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The tech (for those interested):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Tauri 2 desktop app with React frontend and Python backend. Each AI engine runs in isolation, so you can mix and match without dependency hell. Works on Windows, Linux, and macOS.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Current state:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Just released v1.0.1. It's stable and I use it daily for my own audiobooks. Still a solo project, but fully functional.&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/DigiJoe79/AudioBook-Maker"&gt;https://github.com/DigiJoe79/AudioBook-Maker&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from this community. What features would you find most useful?&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bkfvltynt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c87dff564a0357f01b044949c234d7a5236afe"&gt;https://preview.redd.it/bkfvltynt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=22c87dff564a0357f01b044949c234d7a5236afe&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ckbtf3mpt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45701f8412e10717377fafcc8d57a2e41ef51cbf"&gt;https://preview.redd.it/ckbtf3mpt76g1.png?width=1752&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=45701f8412e10717377fafcc8d57a2e41ef51cbf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DigiJoe79"&gt; /u/DigiJoe79 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piduwm/i_wanted_audiobooks_of_stories_that_dont_exist_so/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T17:40:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1phujwo</id>
    <title>Check on lil bro</title>
    <updated>2025-12-09T01:25:42+00:00</updated>
    <author>
      <name>/u/k_means_clusterfuck</name>
      <uri>https://old.reddit.com/user/k_means_clusterfuck</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt; &lt;img alt="Check on lil bro" src="https://preview.redd.it/s8rfm29bz26g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e99684b39e5571f190bf37b141c34049e9f79cc1" title="Check on lil bro" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_means_clusterfuck"&gt; /u/k_means_clusterfuck &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/s8rfm29bz26g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1phujwo/check_on_lil_bro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T01:25:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi8z74</id>
    <title>Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found</title>
    <updated>2025-12-09T14:35:50+00:00</updated>
    <author>
      <name>/u/party-horse</name>
      <uri>https://old.reddit.com/user/party-horse</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt; &lt;img alt="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" src="https://preview.redd.it/h9d1fvb7w66g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3cd2f0eef8f43dfd10528e31cb2b9efd46b87bfb" title="Which small model is best for fine-tuning? We tested 12 of them by spending $10K - here's what we found" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;TL;DR:&lt;/strong&gt; We fine-tuned 12 small models to find which ones are most tunable and perform best after fine-tuning. Surprise finding: Llama-3.2-1B showed the biggest improvement (most tunable), while Qwen3-4B delivered the best final performance - matching a 120B teacher on 7/8 tasks and outperforming by 19 points on the SQuAD 2.0 dataset.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;12 models total - Qwen3 (8B, 4B, 1.7B, 0.6B), Llama (3.1-8B, 3.2-3B, 3.2-1B), SmolLM2 (1.7B, 135M), Gemma (1B, 270M), and Granite 8B.&lt;/p&gt; &lt;p&gt;Used GPT-OSS 120B as teacher to generate 10k synthetic training examples per task. Fine-tuned everything with identical settings: LoRA rank 64, 4 epochs, 5e-5 learning rate.&lt;/p&gt; &lt;p&gt;Tested on 8 benchmarks: classification tasks (TREC, Banking77, Ecommerce, Mental Health), document extraction, and QA (HotpotQA, Roman Empire, SQuAD 2.0).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #1: Tunability (which models improve most)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;The smallest models showed the biggest gains from fine-tuning. Llama-3.2-1B ranked #1 for tunability, followed by Llama-3.2-3B and Qwen3-0.6B.&lt;/p&gt; &lt;p&gt;This pattern makes sense - smaller models start weaker but have more room to grow. Fine-tuning closed the gap hard. The 8B models ranked lowest for tunability not because they're bad, but because they started strong and had less room to improve.&lt;/p&gt; &lt;p&gt;If you're stuck with small models due to hardware constraints, this is good news. Fine-tuning can make a 1B model competitive with much larger models on specific tasks.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Finding #2: Best fine-tuned performance (can student match teacher?)&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Qwen3-4B-Instruct-2507 came out on top for final performance. After fine-tuning, it matched or exceeded the 120B teacher on 7 out of 8 benchmarks.&lt;/p&gt; &lt;p&gt;Breakdown: TREC (+3 points), Docs (+2), Ecommerce (+3), HotpotQA (tied), Mental Health (+1), Roman Empire (+5). Only fell short on Banking77 by 3 points.&lt;/p&gt; &lt;p&gt;SQuAD 2.0 was wild - the 4B student scored 0.71 vs teacher's 0.52. That's a 19 point gap favoring the smaller model. A model 30x smaller outperforming the one that trained it.&lt;/p&gt; &lt;p&gt;Before fine-tuning, the 8B models dominated everything. After fine-tuning, model size mattered way less.&lt;/p&gt; &lt;p&gt;If you're running stuff on your own hardware, you can get frontier-level performance from a 4B model on a single consumer GPU. No expensive cloud instances. No API rate limits.&lt;/p&gt; &lt;p&gt;Let us know if there's a specific model you want benchmarked.&lt;/p&gt; &lt;p&gt;Full write-up: &lt;a href="https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning"&gt;https://www.distillabs.ai/blog/we-benchmarked-12-small-language-models-across-8-tasks-to-find-the-best-base-model-for-fine-tuning&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/party-horse"&gt; /u/party-horse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/h9d1fvb7w66g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi8z74/which_small_model_is_best_for_finetuning_we/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T14:35:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1piabn8</id>
    <title>Devstral-Small-2-24B-Instruct-2512 on Hugging Face</title>
    <updated>2025-12-09T15:29:19+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt; &lt;img alt="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" src="https://external-preview.redd.it/9AtiZkI9TGGX4HUjb1yXt2_pTwLgjJScmnM7q3ZVgpw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e98ba88a60b440a866a778f583a15081cf6838a4" title="Devstral-Small-2-24B-Instruct-2512 on Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/mistralai/Devstral-Small-2-24B-Instruct-2512"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1piabn8/devstralsmall224binstruct2512_on_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:29:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1pihu16</id>
    <title>bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF</title>
    <updated>2025-12-09T20:10:40+00:00</updated>
    <author>
      <name>/u/mantafloppy</name>
      <uri>https://old.reddit.com/user/mantafloppy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt; &lt;img alt="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" src="https://external-preview.redd.it/Y9-VSUeByMali_oSJcuRXft1g3dj7X6u-O2vcI7YtII.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e9d7830dcda85560752ed0db90867edc36dddee1" title="bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mantafloppy"&gt; /u/mantafloppy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pihu16/bartowskimistralai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T20:10:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1pigb3i</id>
    <title>DeepSeek-V3.2-REAP: 508B and 345B checkpoints</title>
    <updated>2025-12-09T19:14:58+00:00</updated>
    <author>
      <name>/u/ilzrvch</name>
      <uri>https://old.reddit.com/user/ilzrvch</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, to get us all in the holiday mood we're continuing to REAP models, this time we got DeepSeek-V3.2 for you at 25% and 50% compression: &lt;/p&gt; &lt;p&gt;&lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-508B-A37B&lt;/a&gt;&lt;br /&gt; &lt;a href="https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B"&gt;https://hf.co/cerebras/DeepSeek-V3.2-REAP-345B-A37B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We're pretty excited about this one and are working to get some agentic evals for coding and beyond on these checkpoints soon. Enjoy and stay tuned!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilzrvch"&gt; /u/ilzrvch &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pigb3i/deepseekv32reap_508b_and_345b_checkpoints/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T19:14:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pi9q3t</id>
    <title>Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI</title>
    <updated>2025-12-09T15:05:54+00:00</updated>
    <author>
      <name>/u/YanderMan</name>
      <uri>https://old.reddit.com/user/YanderMan</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt; &lt;img alt="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" src="https://external-preview.redd.it/QLQU1soiMTzFAm8GzW6EPDbaX5jrcYYFqy1ql5NYoiQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4c3b97e1405ebb7916bf71d7b9a3da9a44efaea7" title="Introducing: Devstral 2 and Mistral Vibe CLI. | Mistral AI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YanderMan"&gt; /u/YanderMan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://mistral.ai/news/devstral-2-vibe-cli"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pi9q3t/introducing_devstral_2_and_mistral_vibe_cli/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-09T15:05:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
