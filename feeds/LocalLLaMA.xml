<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-02T22:06:03+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI. Hey, Tony</subtitle>
  <entry>
    <id>t3_1nwh9z3</id>
    <title>NVFP4 or MXFP4 MOE on sm120 (RTX 5900 RTX 6000 PRO)</title>
    <updated>2025-10-02T21:53:56+00:00</updated>
    <author>
      <name>/u/festr2</name>
      <uri>https://old.reddit.com/user/festr2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, &lt;/p&gt; &lt;p&gt;Did anyone successfully run any decent MOE models in NVFP4 or MXFP4 running it natively on nvidia sm120? Target - GLM-4.5-Air and GLM-4.6 &lt;/p&gt; &lt;p&gt;I tried vllm / sglang / trtllm - nothing seems to work &lt;/p&gt; &lt;p&gt;The nvfp4 should be much better in precission than AWQ 4bit &lt;/p&gt; &lt;p&gt;There is QuTLASS project which can do native fp4 on sm120, but only for dense models and not moe. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/festr2"&gt; /u/festr2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwh9z3/nvfp4_or_mxfp4_moe_on_sm120_rtx_5900_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwh9z3/nvfp4_or_mxfp4_moe_on_sm120_rtx_5900_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwh9z3/nvfp4_or_mxfp4_moe_on_sm120_rtx_5900_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T21:53:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvwbvn</id>
    <title>ERNIE-4.5-21B-A3B-Thinking ‚Äî impressions after some testing</title>
    <updated>2025-10-02T06:30:54+00:00</updated>
    <author>
      <name>/u/ABCD170</name>
      <uri>https://old.reddit.com/user/ABCD170</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;aying around with ERNIE-4.5-21B-A3B-Thinking for a bit and figured I‚Äôd drop my thoughts. This is Baidu‚Äôs ‚Äúthinking‚Äù model for logic, math, science, and coding.&lt;/p&gt; &lt;p&gt;What stood out to me:&lt;/p&gt; &lt;p&gt;Long context works: 128K token window actually does what it promises. I‚Äôve loaded multi-page papers and notes, and it keeps things coherent better than most open models I‚Äôve tried.&lt;/p&gt; &lt;p&gt;Math &amp;amp; code: Handles multi-step problems pretty solidly. Small scripts work fine; bigger coding tasks, I‚Äôd still pick Qwen. Surprised by how little it hallucinates on structured problems.&lt;/p&gt; &lt;p&gt;Performance: 21B params total, ~3B active thanks to MoE. Feels smoother than you‚Äôd expect for a model this size.&lt;/p&gt; &lt;p&gt;Reasoning style: Focused and doesn‚Äôt ramble unnecessarily. Good at staying on track.&lt;/p&gt; &lt;p&gt;Text output: Polished enough that it works well for drafting, summaries, or light creative writing.&lt;/p&gt; &lt;p&gt;Best use cases: Really strong for reasoning and analysis. Weaker if you‚Äôre pushing it into larger coding projects or very complex/nuanced creative writing. So far, it‚Äôs been useful for checking reasoning steps, parsing documents, or running experiments where I need something to actually ‚Äúthink through‚Äù a problem instead of shortcutting.&lt;/p&gt; &lt;p&gt;Curious - anyone else using it for long docs, planning tasks, or multi-step problem solving? What‚Äôs been working for you?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ABCD170"&gt; /u/ABCD170 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvwbvn/ernie4521ba3bthinking_impressions_after_some/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:30:54+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwcbjm</id>
    <title>Will Qwen3-VL be forgotten like others?</title>
    <updated>2025-10-02T18:47:47+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is one big VL model I hope will get support in llama.cpp but I don‚Äôt know if it‚Äôll happen.&lt;/p&gt; &lt;p&gt;Ernie-4.5-VL-424B-A47B, InternVL3.5-241B-A28B, dots.vlm1.inst also didn‚Äôt get support.&lt;/p&gt; &lt;p&gt;What do you guys think?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwcbjm/will_qwen3vl_be_forgotten_like_others/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwcbjm/will_qwen3vl_be_forgotten_like_others/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwcbjm/will_qwen3vl_be_forgotten_like_others/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T18:47:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw124i</id>
    <title>Project: vLLM docker for running smoothly on RTX 5090 + WSL2</title>
    <updated>2025-10-02T11:22:09+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"&gt; &lt;img alt="Project: vLLM docker for running smoothly on RTX 5090 + WSL2" src="https://external-preview.redd.it/HbXvCz_wPDY3JDiCxB7oIAUQa-DpC-SutM3bEcHPoq0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ad1915f5c10c37a77ac151befe745817820b1526" title="Project: vLLM docker for running smoothly on RTX 5090 + WSL2" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/BoltzmannEntropy/vLLM-5090"&gt;https://github.com/BoltzmannEntropy/vLLM-5090&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Finally got &lt;strong&gt;vLLM running smoothly on RTX 5090 + WSL2,&lt;/strong&gt; so I made a Docker container for everyone. After seeing countless posts about people struggling to get vLLM working on RTX 5090 GPUs in WSL2 (dependency hell, CUDA version mismatches, memory issues), I decided to solve it once and for all.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/as65i2rgnosf1.png?width=820&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62e480b4e24aab5c3408df5c6c636eda0bfa19fd"&gt;https://preview.redd.it/as65i2rgnosf1.png?width=820&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=62e480b4e24aab5c3408df5c6c636eda0bfa19fd&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Note, it will take around 3 hours to compile CUDA and build!&lt;/h1&gt; &lt;p&gt;Built a pre-configured Docker container with:&lt;/p&gt; &lt;p&gt;- CUDA 12.8 + PyTorch 2.7.0&lt;/p&gt; &lt;p&gt;- vLLM optimized for 32GB GDDR7&lt;/p&gt; &lt;p&gt;- Two demo apps (direct Python + OpenAI-compatible API)&lt;/p&gt; &lt;p&gt;- Zero setup headaches&lt;/p&gt; &lt;p&gt;Just pull the container and you're running vision-language models in minutes instead of days of troubleshooting.&lt;/p&gt; &lt;p&gt;For anyone tired of fighting with WSL2 GPU setups, this should save you a lot of pain. Feel free to adjust the tone or add more details! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw124i/project_vllm_docker_for_running_smoothly_on_rtx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T11:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvyemn</id>
    <title>How do you configure Ollama so it can help to write essay assignments?</title>
    <updated>2025-10-02T08:43:21+00:00</updated>
    <author>
      <name>/u/crhsharks12</name>
      <uri>https://old.reddit.com/user/crhsharks12</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been experimenting with Ollama for a while now and unfortunately I can‚Äôt seem to crack long-form writing. It tends to repeat itself or stop halfway the moment I try to push it into a full essay assignment (say 1,000-1,500 words). &lt;/p&gt; &lt;p&gt;I‚Äôve tried different prompt styles, but nothing works properly, I‚Äôm still wrestling with it. Now, part of me thinks it would be easier to hand the whole thing off to something like Writemyessay because I don‚Äôt see the point in fighting with prompts for hours. &lt;/p&gt; &lt;p&gt;Has anyone here figured out a config or specific model that works for essays? Do you chunk it section by section? Adjust context size? Any tips appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/crhsharks12"&gt; /u/crhsharks12 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvyemn/how_do_you_configure_ollama_so_it_can_help_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T08:43:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw3sn4</id>
    <title>Speeding up LLM autoscaling by preemptive scheduling</title>
    <updated>2025-10-02T13:28:29+00:00</updated>
    <author>
      <name>/u/entsnack</name>
      <uri>https://old.reddit.com/user/entsnack</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw3sn4/speeding_up_llm_autoscaling_by_preemptive/"&gt; &lt;img alt="Speeding up LLM autoscaling by preemptive scheduling" src="https://external-preview.redd.it/MjF4MjQ2aWJhcHNmMYAVfPyTvVSmoFDlMmhhCNq5SgTRt_z1p7dydFFJ5OdA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c024f5da141b78ad14ee12ce94cc34e1400b63eb" title="Speeding up LLM autoscaling by preemptive scheduling" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;blockquote&gt; &lt;p&gt;Code: &lt;a href="https://github.com/aquaml"&gt;https://github.com/aquaml&lt;/a&gt; Paper: &lt;a href="https://arxiv.org/pdf/2407.21255"&gt;https://arxiv.org/pdf/2407.21255&lt;/a&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;This is outside my usual list of academic venues but the LMStudio demo caught my eye. This seems only relevent to multiGPU systems (like if you're an Openrouter provider) but I found it interesting nevertheless.&lt;/p&gt; &lt;p&gt;Apparently a lot of the delay in LLM responses can be attributed to load spikes and users queued up to access GPUs while the system autoscales up to handle load. Autoscaling is slow. Aqua does some sort of &amp;quot;preemptive scheduling&amp;quot; to speed it up dramatically.&lt;/p&gt; &lt;p&gt;Hopefully we see this kind of tech adopted by other Openrouter vendors.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/entsnack"&gt; /u/entsnack &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ls4bn6kbapsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw3sn4/speeding_up_llm_autoscaling_by_preemptive/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw3sn4/speeding_up_llm_autoscaling_by_preemptive/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T13:28:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw9tny</id>
    <title>Hi, how‚Äôs inference looking now in AMD GPUs? I don‚Äôt have one so that‚Äôs why asking here.</title>
    <updated>2025-10-02T17:15:59+00:00</updated>
    <author>
      <name>/u/NoFudge4700</name>
      <uri>https://old.reddit.com/user/NoFudge4700</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Also, what is poor man‚Äôs way to 256 GB VRAM that works well for inference? Is 11 3090s the only way to get there? ü•≤&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoFudge4700"&gt; /u/NoFudge4700 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw9tny/hi_hows_inference_looking_now_in_amd_gpus_i_dont/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw9tny/hi_hows_inference_looking_now_in_amd_gpus_i_dont/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw9tny/hi_hows_inference_looking_now_in_amd_gpus_i_dont/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:15:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw78g0</id>
    <title>Stretching Claude Pro with GLM Lite as backup</title>
    <updated>2025-10-02T15:40:01+00:00</updated>
    <author>
      <name>/u/Psychological_Box406</name>
      <uri>https://old.reddit.com/user/Psychological_Box406</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I'm in a country where $20/month is actually serious money, let alone $100-200. I grabbed Pro with the yearly deal when it was on promo. I can't afford adding another subscription like Cursor or Codex on top of that.&lt;/p&gt; &lt;p&gt;Claude's outputs are great though, so I've basically figured out how to squeeze everything I can out of Pro within those 5-hour windows:&lt;/p&gt; &lt;p&gt;I plan a lot. I use Claude Web sometimes, but mostly Gemini 2.5 Pro on AI Studio to plan stuff out, make markdown files, double-check them in other chats to make sure they're solid, then hand it all to Claude Code to actually write.&lt;/p&gt; &lt;p&gt;I babysit Claude Code hard. Always watching what it's doing so I can jump in with more instructions or stop it immediately if needed. Never let it commit anything - I do all commits myself.&lt;/p&gt; &lt;p&gt;I'm up at 5am and I send a quick &amp;quot;hello&amp;quot; to kick off my first session. Then between 8am and 1pm I can do a good amount of work between my first session and the next one. I do like 3 sessions a day.&lt;/p&gt; &lt;p&gt;I almost never touch Opus. Just not worth the usage hit.&lt;/p&gt; &lt;p&gt;Tracking usage used to suck and I was using &amp;quot;Claude Usage Tracker&amp;quot; (even donated to the dev), but now Anthropic gave us the /usage thing which is amazing. Weirdly I don't see any Weekly Limit on mine. I guess my region doesn't have that restriction? Maybe there aren't many Claude users over here.&lt;/p&gt; &lt;p&gt;Lately, I had too much work and I was seriously considering (really didn't want to) getting a second account.&lt;/p&gt; &lt;p&gt;I tried Gemini CLI and Qwen since they're free but... no, they were basically useless for my needs.&lt;/p&gt; &lt;p&gt;I did some digging and heard about GLM 4.6. Threw $3 at it 3 days ago to test for a month and honestly? It's good. Like really good for what I need.&lt;/p&gt; &lt;p&gt;Not quite Sonnet 4.5 level but pretty close. I've been using it for less complex stuff and it handles it fine.&lt;/p&gt; &lt;p&gt;I'll definitely getting a quarterly or yearly subscription for their Lite tier. It's basically the Haiku that Anthropic should give us. A capable and cheap model.&lt;/p&gt; &lt;p&gt;It's taken a huge chunk off my Claude usage and now the Pro limit doesn't stress me out anymore.&lt;/p&gt; &lt;p&gt;TL;DR: If you're on a tight budget, there are cheap but solid models out there that can take the load off Sonnet for you.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Psychological_Box406"&gt; /u/Psychological_Box406 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw78g0/stretching_claude_pro_with_glm_lite_as_backup/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw78g0/stretching_claude_pro_with_glm_lite_as_backup/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw78g0/stretching_claude_pro_with_glm_lite_as_backup/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T15:40:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nweb8v</id>
    <title>Models for creating beautiful diagrams and flowcharts?</title>
    <updated>2025-10-02T20:02:16+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôm utterly useless at anything visual or design oriented, yet frequently find the need to create diagrams, flow charts, etc. This is tedious and I detest it.&lt;/p&gt; &lt;p&gt;I‚Äôd like to be able to describe in a prompt the diagrams I wish to create and then have a model create it.&lt;/p&gt; &lt;p&gt;Is this a thing? All I seem to find are image models that generate waifus. Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nweb8v/models_for_creating_beautiful_diagrams_and/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nweb8v/models_for_creating_beautiful_diagrams_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nweb8v/models_for_creating_beautiful_diagrams_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T20:02:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvw1my</id>
    <title>Jet-Nemotron 2B/4B 47x faster inference released</title>
    <updated>2025-10-02T06:13:26+00:00</updated>
    <author>
      <name>/u/Odd-Ordinary-5922</name>
      <uri>https://old.reddit.com/user/Odd-Ordinary-5922</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"&gt; &lt;img alt="Jet-Nemotron 2B/4B 47x faster inference released" src="https://external-preview.redd.it/r396-oAbMocWRiDVz2adQ6rwSWE3nHUKDdKf1UIVuHc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=68466d80cc66f634a4f6d8779e7110ddf330d635" title="Jet-Nemotron 2B/4B 47x faster inference released" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;heres the github &lt;a href="https://github.com/NVlabs/Jet-Nemotron"&gt;https://github.com/NVlabs/Jet-Nemotron&lt;/a&gt; the model was published 2 days ago but I havent seen anyone talk about it &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Odd-Ordinary-5922"&gt; /u/Odd-Ordinary-5922 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/jet-ai/Jet-Nemotron-4B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvw1my/jetnemotron_2b4b_47x_faster_inference_released/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T06:13:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw0m6u</id>
    <title>Thoughts on Apriel-1.5-15b-Thinker ?</title>
    <updated>2025-10-02T10:58:53+00:00</updated>
    <author>
      <name>/u/Le_Thon_Rouge</name>
      <uri>https://old.reddit.com/user/Le_Thon_Rouge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0m6u/thoughts_on_apriel1515bthinker/"&gt; &lt;img alt="Thoughts on Apriel-1.5-15b-Thinker ?" src="https://preview.redd.it/tyesg05mjosf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4306b1232df88670ababa09366f0ad4de64bedd6" title="Thoughts on Apriel-1.5-15b-Thinker ?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello AI builders,&lt;/p&gt; &lt;p&gt;Recently ServiceNow released Apriel-1.5-15b-Thinker, and according to their benchmarks, this model is incredible knowing its size !&lt;/p&gt; &lt;p&gt;So I'm wondering : why people don't talk about it that much ? It has currently only 886 downloads on Huggingface..&lt;/p&gt; &lt;p&gt;Have you tried it ? Do you have the impression that their benchmark is &amp;quot;fair&amp;quot; ?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Le_Thon_Rouge"&gt; /u/Le_Thon_Rouge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/tyesg05mjosf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0m6u/thoughts_on_apriel1515bthinker/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw0m6u/thoughts_on_apriel1515bthinker/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T10:58:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvsbdu</id>
    <title>I visualized embeddings walking across the latent space as you type! :)</title>
    <updated>2025-10-02T02:48:37+00:00</updated>
    <author>
      <name>/u/kushalgoenka</name>
      <uri>https://old.reddit.com/user/kushalgoenka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt; &lt;img alt="I visualized embeddings walking across the latent space as you type! :)" src="https://external-preview.redd.it/bXg4NGVhbm8zbXNmMcfpx6_IdDgYBGvf-fwH7xFuI_ot2ErqijE3fUPasYhL.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f48ec1e00df98aae9dff909fac81e2997bfd28dc" title="I visualized embeddings walking across the latent space as you type! :)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kushalgoenka"&gt; /u/kushalgoenka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/czy4sbno3msf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvsbdu/i_visualized_embeddings_walking_across_the_latent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:48:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw6ot2</id>
    <title>Granite-4.0 running on latest Qualcomm NPUs (with benchmarks)</title>
    <updated>2025-10-02T15:19:48+00:00</updated>
    <author>
      <name>/u/AlanzhuLy</name>
      <uri>https://old.reddit.com/user/AlanzhuLy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw6ot2/granite40_running_on_latest_qualcomm_npus_with/"&gt; &lt;img alt="Granite-4.0 running on latest Qualcomm NPUs (with benchmarks)" src="https://external-preview.redd.it/MjFtOWFkMXV0cHNmMdg4lbHLbhrLkzDfVtbcBjXS_Swv3usnXgduLh9snYEo.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5daac987a1c61e5c98d567054d66d0a6afc418ab" title="Granite-4.0 running on latest Qualcomm NPUs (with benchmarks)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all ‚Äî I‚Äôm Alan from Nexa AI. Granite-4.0 just dropped, and we got &lt;strong&gt;Granite-4.0-Micro (3B)&lt;/strong&gt; running on NPU from Qualcomm‚Äôs newest platforms (Day-0 support!)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Snapdragon X2 Elite PCs&lt;/li&gt; &lt;li&gt;Snapdragon 8 Elite Gen 5 smartphones&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It also works on CPU/GPU through the same SDK. Here are some early benchmarks:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;X2 Elite NPU ‚Äî 36.4 tok/s&lt;/li&gt; &lt;li&gt;8 Elite Gen 5 NPU ‚Äî 28.7 tok/s&lt;/li&gt; &lt;li&gt;X Elite CPU ‚Äî 23.5 tok/s&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Curious what people think about running Granite on NPU.&lt;br /&gt; Follow along if you‚Äôd like to see more models running on NPU ‚Äî and would love your feedback.&lt;br /&gt; üëâ GitHub: &lt;a href="https://github.com/NexaAI/nexa-sdk"&gt;github.com/NexaAI/nexa-sdk&lt;/a&gt; If you have a Qualcomm Snapdragon PC, you can run Granite 4 directly on NPU/GPU/CPU using NexaSDK.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AlanzhuLy"&gt; /u/AlanzhuLy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a7zdec1utpsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw6ot2/granite40_running_on_latest_qualcomm_npus_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw6ot2/granite40_running_on_latest_qualcomm_npus_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T15:19:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvpw0y</id>
    <title>Those who spent $10k+ on a local LLM setup, do you regret it?</title>
    <updated>2025-10-02T00:54:13+00:00</updated>
    <author>
      <name>/u/TumbleweedDeep825</name>
      <uri>https://old.reddit.com/user/TumbleweedDeep825</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Considering the fact 200k context chinese models subscriptions like z.ai (GLM 4.6) are pretty dang cheap. &lt;/p&gt; &lt;p&gt;Every so often I consider blowing a ton of money on an LLM setup only to realize I can't justify the money or time spent at all.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TumbleweedDeep825"&gt; /u/TumbleweedDeep825 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvpw0y/those_who_spent_10k_on_a_local_llm_setup_do_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T00:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwgma3</id>
    <title>A Summary of Key AI Events from September 2025</title>
    <updated>2025-10-02T21:27:57+00:00</updated>
    <author>
      <name>/u/nh_local</name>
      <uri>https://old.reddit.com/user/nh_local</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;ByteDance released &lt;strong&gt;Seedream 4.0&lt;/strong&gt;, a next-generation image model unifying high-quality text-to-image generation and natural-language image editing.&lt;/li&gt; &lt;li&gt;An advanced Gemini variant, reported as &lt;strong&gt;Gemini 2.5 - Deep Think&lt;/strong&gt;, achieved gold-medal-level performance at the ICPC World Finals programming contest.&lt;/li&gt; &lt;li&gt;OpenAI reported a reasoning and code model achieved a perfect score (12/12) in ICPC testing.&lt;/li&gt; &lt;li&gt;Suno released &lt;strong&gt;Suno v5&lt;/strong&gt;, an upgrade in music generation with studio-grade fidelity and more natural-sounding vocals.&lt;/li&gt; &lt;li&gt;Alibaba unveiled &lt;strong&gt;Qwen-3-Max&lt;/strong&gt;, its flagship model with over a trillion parameters, focusing on long context and agent capabilities.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Wan 2.2&lt;/strong&gt; was released, a generative video model focused on multi-shot consistency and character animation.&lt;/li&gt; &lt;li&gt;Anthropic announced &lt;strong&gt;Claude Sonnet 4.5&lt;/strong&gt;, a model optimized for coding, agent construction, and improved reasoning.&lt;/li&gt; &lt;li&gt;OpenAI released &lt;strong&gt;Sora 2&lt;/strong&gt;, a flagship video and audio generation model with improved physical modeling and synchronized sound.&lt;/li&gt; &lt;li&gt;DeepSeek released &lt;strong&gt;DeepSeek-V3.2-Exp&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;OpenAI and NVIDIA announced a strategic partnership for NVIDIA to supply at least &lt;strong&gt;10 gigawatts&lt;/strong&gt; of AI systems for OpenAI's infrastructure.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nh_local"&gt; /u/nh_local &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwgma3/a_summary_of_key_ai_events_from_september_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T21:27:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw74ec</id>
    <title>We built this open-source LLM Inference project to boost context generation by up to 15x and now it is being implemented by NVIDIA Dynamo!</title>
    <updated>2025-10-02T15:35:52+00:00</updated>
    <author>
      <name>/u/ExplanationEven9787</name>
      <uri>https://old.reddit.com/user/ExplanationEven9787</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone, our team has been working nonstop on our open source project, LMCache, to reduce repetitive computation in LLM inference and make systems serve more people (3x more throughput in chat applications) and recently it has been implemented by NVIDIA's Inference project Dyanamo.&lt;/p&gt; &lt;p&gt;In LLM serving, often when processing large documents, KV Cache context gets overwhelmed and begins to evict precious context requiring the model to reprocess context resulting in much slower speeds. With LMCache, KV Caches get stored outside of just the high bandwidth memory into places like DRAM, disk, or other storages available. &lt;/p&gt; &lt;p&gt;Ask us anything! We would love it if you check us out, we recently hit 5,000 stars on GitHub and want to continue our growth!&lt;/p&gt; &lt;p&gt;Github: &lt;a href="https://github.com/LMCache/LMCache"&gt;https://github.com/LMCache/LMCache&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Early industry adopters:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;OSS projects: vLLM production stack, Redhat llm-d, KServe, Nvidia Dynamo.&lt;/li&gt; &lt;li&gt;Commercial: Bloomberg, AWS, Tencent, Redis, BentoML, Weka, FlowGPT, GMI, ‚Ä¶&lt;/li&gt; &lt;li&gt;Work in progress: Character AI, GKE, Cohere, Baseten, Novita, ‚Ä¶&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Full Technical Report:&lt;/p&gt; &lt;p&gt;&lt;a href="https://lmcache.ai/tech_report.pdf"&gt;https://lmcache.ai/tech_report.pdf&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ExplanationEven9787"&gt; /u/ExplanationEven9787 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw74ec/we_built_this_opensource_llm_inference_project_to/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw74ec/we_built_this_opensource_llm_inference_project_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw74ec/we_built_this_opensource_llm_inference_project_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T15:35:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw60fj</id>
    <title>Open source speech foundation model that runs locally on CPU in real-time</title>
    <updated>2025-10-02T14:54:43+00:00</updated>
    <author>
      <name>/u/TeamNeuphonic</name>
      <uri>https://old.reddit.com/user/TeamNeuphonic</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt; &lt;img alt="Open source speech foundation model that runs locally on CPU in real-time" src="https://external-preview.redd.it/3w13BgLMXQ4-v0J3QSPqnnHAcC8U3HjNheDu4QFAWrk.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7bfeb812f14c0b82e510265b807d168b2af385bc" title="Open source speech foundation model that runs locally on CPU in real-time" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://reddit.com/link/1nw60fj/video/3kh334ujppsf1/player"&gt;https://reddit.com/link/1nw60fj/video/3kh334ujppsf1/player&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We‚Äôve just released Neuphonic TTS Air, a lightweight open-source speech foundation model under Apache 2.0.&lt;/p&gt; &lt;p&gt;The main idea: frontier-quality text-to-speech, but small enough to run in realtime on CPU. No GPUs, no cloud APIs, no rate limits.&lt;/p&gt; &lt;p&gt;Why we built this: - Most speech models today live behind paid APIs ‚Üí privacy tradeoffs, recurring costs, and external dependencies. - With Air, you get full control, privacy, and zero marginal cost. - It enables new use cases where running speech models on-device matters (edge compute, accessibility tools, offline apps).&lt;/p&gt; &lt;p&gt;Git Repo: &lt;a href="https://github.com/neuphonic/neutts-air"&gt;https://github.com/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;HF: &lt;a href="https://huggingface.co/neuphonic/neutts-air"&gt;https://huggingface.co/neuphonic/neutts-air&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from on performance, applications, and contributions.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeamNeuphonic"&gt; /u/TeamNeuphonic &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw60fj/open_source_speech_foundation_model_that_runs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:54:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwc1oc</id>
    <title>Apertus model implementation has been merged into llama.cpp</title>
    <updated>2025-10-02T18:37:32+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"&gt; &lt;img alt="Apertus model implementation has been merged into llama.cpp" src="https://external-preview.redd.it/WBeE9GPvyJdOEySnojQ_o2A9ys0na0K0XH7uI9iyd_o.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=23de1c1602b1561cace347dd342baae689fd7c5c" title="Apertus model implementation has been merged into llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think Piotr can now fully focus on Qwen Next ;)&lt;/p&gt; &lt;p&gt;model description:&lt;/p&gt; &lt;p&gt;Apertus is a 70B and 8B parameter language model designed to push the boundaries of fully-open multilingual and transparent models. The model supports over 1000 languages and long context, it uses only fully compliant and open training data, and achieves comparable performance to models trained behind closed doors.&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509"&gt;https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509"&gt;https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/15852"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwc1oc/apertus_model_implementation_has_been_merged_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T18:37:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8jbn</id>
    <title>Ring Flash 2.0 104B A6B with Linear Attention released a few days ago</title>
    <updated>2025-10-02T16:28:11+00:00</updated>
    <author>
      <name>/u/FullOf_Bad_Ideas</name>
      <uri>https://old.reddit.com/user/FullOf_Bad_Ideas</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"&gt; &lt;img alt="Ring Flash 2.0 104B A6B with Linear Attention released a few days ago" src="https://external-preview.redd.it/arTReyF0GyVAVaEDNDlfVvJyFYJ0q7EWSfcCybSgWt0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af2d40f335ae44a32d6d45dc30487a7c16511fa4" title="Ring Flash 2.0 104B A6B with Linear Attention released a few days ago" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FullOf_Bad_Ideas"&gt; /u/FullOf_Bad_Ideas &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8jbn/ring_flash_20_104b_a6b_with_linear_attention/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:28:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvzeuh</id>
    <title>Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance</title>
    <updated>2025-10-02T09:47:49+00:00</updated>
    <author>
      <name>/u/ShinobuYuuki</name>
      <uri>https://old.reddit.com/user/ShinobuYuuki</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt; &lt;img alt="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" src="https://external-preview.redd.it/NzRlNXJuc3A2b3NmMe-uhlatbqnQI0WkANIEyFuJlq6CEOqVOtkO0hhCMPfO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=152efa7692053a33bdf74c0824752b907d2becc8" title="Jan now auto-optimizes llama.cpp settings based on your hardware for more efficient performance" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone, I'm Yuuki from the Jan team.&lt;/p&gt; &lt;p&gt;We‚Äôve been working on some updates for a while. We released Jan v0.7.0. I'd like to quickly share what's new:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;llama.cpp improvements&lt;/strong&gt;:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jan now automatically optimizes llama.cpp settings (e.g. context size, gpu layers) based on your hardware. So your models run more efficiently. It's an experimental feature&lt;/li&gt; &lt;li&gt;You can now see some stats (how much context is used, etc.) when the model runs&lt;/li&gt; &lt;li&gt;Projects is live now. You can organize your chats using it - it's pretty similar to ChatGPT&lt;/li&gt; &lt;li&gt;You can rename your models in Settings&lt;/li&gt; &lt;li&gt;Plus, we're also improving Jan's cloud capabilities: Model names update automatically - so no need to manually add cloud models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;If you haven't seen it yet: Jan is an open-source ChatGPT alternative. It runs AI models locally and lets you add agentic capabilities &lt;a href="https://www.jan.ai/docs/desktop/mcp#configure-and-use-mcps-within-jan"&gt;through MCPs&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Website: &lt;a href="https://www.jan.ai/"&gt;https://www.jan.ai/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/menloresearch/jan"&gt;https://github.com/menloresearch/jan&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ShinobuYuuki"&gt; /u/ShinobuYuuki &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/49h5xlsp6osf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvzeuh/jan_now_autooptimizes_llamacpp_settings_based_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T09:47:49+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2ghd</id>
    <title>GLM 4.6 is nice</title>
    <updated>2025-10-02T12:31:10+00:00</updated>
    <author>
      <name>/u/theodordiaconu</name>
      <uri>https://old.reddit.com/user/theodordiaconu</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bit the bullet and sacrificed 3$ (lol) for a &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt; subscription as I can't run this behemoth locally. And because I'm a very generous dude I wanted them to keep the full margin instead of going through routers. &lt;/p&gt; &lt;p&gt;For convenience, I created a simple 'glm' bash script that starts claude with env variables (that point to z.ai). I type glm and I'm locked in. &lt;/p&gt; &lt;p&gt;Previously I experimented a lot with OW models with GPT-OSS-120B, GLM 4.5, KIMI K2 0905, Qwen3 Coder 480B (and their latest variant included which is only through 'qwen' I think) honestly they were making silly mistakes on the project or had trouble using agentic tools (many failed edits) and abandoned their use quickly in favor of the king: gpt-5-high. I couldn't even work with Sonnet 4 unless it was frontend. &lt;/p&gt; &lt;p&gt;This specific project I tested it on is an open-source framework I'm working on, and it's not very trivial to work on a framework that wants to adhere to 100% code coverage for every change, every little addition/change has impacts on tests, on documentation on lots of stuff. Before starting any task I have to feed the whole documentation. &lt;/p&gt; &lt;p&gt;GLM 4.6 is in another class for OW models. I felt like it's an equal to GPT-5-high and Claude 4.5 Sonnet. Ofcourse this is an early vibe-based assessment, so take it with a grain of sea salt.&lt;/p&gt; &lt;p&gt;Today I challenged them (Sonnet 4.5, GLM 4.6) to refactor a class that had 600+ lines. And I usually have bad experiences when asking for refactors with all models. &lt;/p&gt; &lt;p&gt;Sonnet 4.5 could not make it reach 100% on its own after refactor, started modifying existing tests and sort-of found a silly excuse for not reaching 100% it stopped at 99.87% and said that it's the testing's fault (lmao). &lt;/p&gt; &lt;p&gt;Now on the other hand, GLM 4.6, it worked for 10 mins I think?, ended up with a perfect result. It understood the assessment. They both had interestingly similar solutions to refactoring, so planning wise, both were good and looked like they really understood the task. I never leave an agent run without reading its plan first. &lt;/p&gt; &lt;p&gt;I'm not saying it's better than Sonnet 4.5 or GPT-5-High, I just tried it today, all I can say for a fact is that it's a different league for open weight, perceived on this particular project. &lt;/p&gt; &lt;p&gt;Congrats &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;&lt;br /&gt; What OW models do you use for coding?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theodordiaconu"&gt; /u/theodordiaconu &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2ghd/glm_46_is_nice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:31:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw8c6y</id>
    <title>Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration</title>
    <updated>2025-10-02T16:20:56+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt; &lt;img alt="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" src="https://external-preview.redd.it/aG1yZ2k0M3Y0cXNmMTjBkk0zpHe1cUKuUpjTdKuc-czjYGWzckCtqtrm-IdD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2fd6b6a33eda5421f6a81ae5b65f2f068b49e13c" title="Granite 4.0 Micro (3.4B) running 100% locally in your browser w/ WebGPU acceleration" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/14cmif4v4qsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw8c6y/granite_40_micro_34b_running_100_locally_in_your/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T16:20:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw52ad</id>
    <title>Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP</title>
    <updated>2025-10-02T14:18:05+00:00</updated>
    <author>
      <name>/u/Weves11</name>
      <uri>https://old.reddit.com/user/Weves11</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt; &lt;img alt="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" src="https://external-preview.redd.it/ODh3bjRsOWJpcHNmMcggpjsEMzF-IE1l8vJahmQmeeToARZwc_P-uEOcis7p.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=491d98e181b37d6e6d0003c442bfc14ebbed0594" title="Introducing Onyx - a fully open source chat UI with RAG, web search, deep research, and MCP" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weves11"&gt; /u/Weves11 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/vklzqk9bipsf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw52ad/introducing_onyx_a_fully_open_source_chat_ui_with/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:18:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw5kkc</id>
    <title>It's been a long time since Google released a new Gemma model.</title>
    <updated>2025-10-02T14:37:55+00:00</updated>
    <author>
      <name>/u/ArcherAdditional2478</name>
      <uri>https://old.reddit.com/user/ArcherAdditional2478</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was here using Gemma 3 4B, a model that I can confidently say has so far been the best of its size, something truly usable: it‚Äôs super coherent in Portuguese (not just in English and Chinese) and even gives me solid image recognition. It allowed me to process personal stuff without having to throw it into some obscure cloud. After seeing so many amazing releases, but with little focus on being multilingual, I deeply missed seeing Google release a new Gemma. And judging by the pace of AI evolution, it‚Äôs been about 35 years since Google last released a new Gemma, let‚Äôs be honest.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ArcherAdditional2478"&gt; /u/ArcherAdditional2478 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw5kkc/its_been_a_long_time_since_google_released_a_new/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T14:37:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nw2wd6</id>
    <title>Granite 4.0 Language Models - a ibm-granite Collection</title>
    <updated>2025-10-02T12:51:10+00:00</updated>
    <author>
      <name>/u/rerri</name>
      <uri>https://old.reddit.com/user/rerri</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt; &lt;img alt="Granite 4.0 Language Models - a ibm-granite Collection" src="https://external-preview.redd.it/dG6nrEEPIkS2YfUpzm-ii0PPK1xkTA3ZMcynqcTCXQc.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=374e83fed526e7600d653259e65b30be13801c21" title="Granite 4.0 Language Models - a ibm-granite Collection" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Granite 4, &lt;strong&gt;32B-A9B, 7B-A1B, and 3B&lt;/strong&gt; dense models available.&lt;/p&gt; &lt;p&gt;GGUF's are in the same repo:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c"&gt;https://huggingface.co/collections/ibm-granite/granite-quantized-models-67f944eddd16ff8e057f115c&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rerri"&gt; /u/rerri &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-language-models-6811a18b820ef362d9e5a82c"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nw2wd6/granite_40_language_models_a_ibmgranite_collection/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T12:51:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nvryo4</id>
    <title>AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)</title>
    <updated>2025-10-02T02:31:01+00:00</updated>
    <author>
      <name>/u/XMasterrrr</name>
      <uri>https://old.reddit.com/user/XMasterrrr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt; &lt;img alt="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" src="https://preview.redd.it/222kj50x0msf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c1c747c42690f74b8d08690dcdbb1ef23aece13b" title="AMA Announcement: Prime Intellect ‚Äî The Open‚ÄëSource Distributed Training Lab (Thu, Oct 2 ‚Ä¢ 10 AM ‚Äì 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XMasterrrr"&gt; /u/XMasterrrr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/222kj50x0msf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nvryo4/ama_announcement_prime_intellect_the_opensource/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T02:31:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nwaoyd</id>
    <title>AMA with Prime Intellect ‚Äî Ask Us Anything!</title>
    <updated>2025-10-02T17:47:44+00:00</updated>
    <author>
      <name>/u/kindacognizant</name>
      <uri>https://old.reddit.com/user/kindacognizant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;AMA with Prime Intellect ‚Äî Ask Us Anything!&lt;/h1&gt; &lt;p&gt;Hi &lt;a href="https://www.reddit.com/r/LocalLLaMA/"&gt;r/LocalLLaMA&lt;/a&gt;! We‚Äôre excited for this AMA, thank you for having us.&lt;/p&gt; &lt;p&gt;I‚Äôm Kalomaze (&lt;a href="/u/kindacognizant"&gt;u/kindacognizant&lt;/a&gt;), a researcher at Prime Intellect, the lab behind:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Distributed training &lt;a href="https://www.primeintellect.ai/#research"&gt;efforts&lt;/a&gt; including INTELLECT-1 + INTELLECT-2&lt;/li&gt; &lt;li&gt;Open-source RL efforts including &lt;a href="https://github.com/PrimeIntellect-ai/verifiers"&gt;verifiers&lt;/a&gt;, &lt;a href="https://github.com/PrimeIntellect-ai/prime-rl"&gt;prime-rl&lt;/a&gt;, and the &lt;a href="https://app.primeintellect.ai/dashboard/environments"&gt;Environments Hub&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our other participants today:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Sami Jaghouar, &lt;a href="/u/samsja19"&gt;u/samsja19&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Will Brown, &lt;a href="/u/willccbb"&gt;u/willccbb&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Jack Min Ong, &lt;a href="/u/Cinamic"&gt;u/Cinamic&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Mika Senghaas, &lt;a href="/u/mikasenghaas"&gt;u/mikasenghaas&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run from 11:00 AM ‚Äì 2:00 PM PST, with the Prime Intellect team continuing to follow up on questions over the next 48 hours.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kindacognizant"&gt; /u/kindacognizant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nwaoyd/ama_with_prime_intellect_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-02T17:47:44+00:00</published>
  </entry>
</feed>
