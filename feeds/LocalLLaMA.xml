<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-09-27T17:34:19+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1nrcs5d</id>
    <title>InclusionAI's 103B MoE's Ring-Flash 2.0 (Reasoning) and Ling-Flash 2.0 (Instruct) now have GGUFs!</title>
    <updated>2025-09-26T20:56:22+00:00</updated>
    <author>
      <name>/u/jwpbe</name>
      <uri>https://old.reddit.com/user/jwpbe</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrcs5d/inclusionais_103b_moes_ringflash_20_reasoning_and/"&gt; &lt;img alt="InclusionAI's 103B MoE's Ring-Flash 2.0 (Reasoning) and Ling-Flash 2.0 (Instruct) now have GGUFs!" src="https://external-preview.redd.it/TVJh8T61p_RCxMLA1t3bUItz8uK2seYWrErww4ERYTI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=081405cb44634757d72426b2a1ae8a89b7837387" title="InclusionAI's 103B MoE's Ring-Flash 2.0 (Reasoning) and Ling-Flash 2.0 (Instruct) now have GGUFs!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jwpbe"&gt; /u/jwpbe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-flash-2.0-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrcs5d/inclusionais_103b_moes_ringflash_20_reasoning_and/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrcs5d/inclusionais_103b_moes_ringflash_20_reasoning_and/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T20:56:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrrh7d</id>
    <title>have you tested code world model? I often get unnecessary response with ai appended extra question</title>
    <updated>2025-09-27T10:06:11+00:00</updated>
    <author>
      <name>/u/uptonking</name>
      <uri>https://old.reddit.com/user/uptonking</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrh7d/have_you_tested_code_world_model_i_often_get/"&gt; &lt;img alt="have you tested code world model? I often get unnecessary response with ai appended extra question" src="https://b.thumbs.redditmedia.com/5G3BxZQYAamkku_ISuMG99qjhhKdQ9fZqeaFwOVO6WQ.jpg" title="have you tested code world model? I often get unnecessary response with ai appended extra question" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ul&gt; &lt;li&gt;I have been waiting for a 32b dense model for coding, and recently cwm comes with gguf in lm studio. I played with &lt;code&gt;cwm-Q4_0-GGUF&lt;/code&gt; (18.54GB) on my macbook air 32gb as it's not too heavy in memory&lt;/li&gt; &lt;li&gt;after several testing in coding and reasoning, i only have ordinary impression for this model. the answer is concise most of the time. the format is a little messy in lm studio chat.&lt;/li&gt; &lt;li&gt;I often get the problem as the picture below. when ai answered my question, it will auto append another 2~4 question and answer it itself. is my config wrong or the model is trained to over-think/over-answer?&lt;/li&gt; &lt;li&gt;sometimes it even contains answer from Claude as in picture 3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/p64n7230lorf1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99d2a7dc567a777b3a3a7bcae9d6b68f3d285f81"&gt;https://preview.redd.it/p64n7230lorf1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99d2a7dc567a777b3a3a7bcae9d6b68f3d285f81&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/xskbvjj3lorf1.png?width=3420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6319abfcd2a8940d170bdaa4a05bcca070040d82"&gt;https://preview.redd.it/xskbvjj3lorf1.png?width=3420&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6319abfcd2a8940d170bdaa4a05bcca070040d82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;- sometimes it even contains answer from Claude&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/unhawnmglorf1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba70a9debe2f0f3e6aa5e770c5831ba06a73a81b"&gt;https://preview.redd.it/unhawnmglorf1.png?width=3644&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ba70a9debe2f0f3e6aa5e770c5831ba06a73a81b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;❤️ please remind me when code world model &lt;strong&gt;mlx&lt;/strong&gt; for mac is available, the current gguf is slow and consuming too much memory&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/uptonking"&gt; /u/uptonking &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrh7d/have_you_tested_code_world_model_i_often_get/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrh7d/have_you_tested_code_world_model_i_often_get/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrh7d/have_you_tested_code_world_model_i_often_get/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T10:06:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrzfb0</id>
    <title>Which local model for generating manim animations</title>
    <updated>2025-09-27T16:24:01+00:00</updated>
    <author>
      <name>/u/redblood252</name>
      <uri>https://old.reddit.com/user/redblood252</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm having trouble with generating manim animations, it's strange that this is specifically really weak even with public models. For example I try coding in rust and qwen coder has sometimes better help than chatgpt (free online version) or Claude. It's always better than gemini.&lt;/p&gt; &lt;p&gt;But with manim everything I've ever used is really bad except online claude. Does anybody know if there is any model I can host locally in 24Gb VRAM that is good at generating manim animation python code? I don't mind having something slow.&lt;/p&gt; &lt;p&gt;It's weird since this is the only thing where everything I've used has been really bad (except claude but it's expensive).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/redblood252"&gt; /u/redblood252 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzfb0/which_local_model_for_generating_manim_animations/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzfb0/which_local_model_for_generating_manim_animations/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzfb0/which_local_model_for_generating_manim_animations/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:24:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ns0zem</id>
    <title>46 GB GPU compute for $20.</title>
    <updated>2025-09-27T17:26:38+00:00</updated>
    <author>
      <name>/u/M3GaPrincess</name>
      <uri>https://old.reddit.com/user/M3GaPrincess</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns0zem/46_gb_gpu_compute_for_20/"&gt; &lt;img alt="46 GB GPU compute for $20." src="https://preview.redd.it/pm02jqh9rqrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=793957492c5398c52932ea4cfc4869673102242a" title="46 GB GPU compute for $20." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I bought a second hand computer with a i3-6100U inside. Only two RAM slots, so I put two 32GB RAM sticks, works like a charm. The iGPU runs at 1000 Mhz max, but it's still WAY faster than running on the CPU only, and only 10 Watts of power. If it had four RAM slots I bet it would double just fine. You don't need to be a baller to run large models. With vulkan, even iGPUs can work pretty good.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/M3GaPrincess"&gt; /u/M3GaPrincess &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pm02jqh9rqrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ns0zem/46_gb_gpu_compute_for_20/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ns0zem/46_gb_gpu_compute_for_20/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T17:26:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr8acu</id>
    <title>The benchmarks are favouring Qwen3 max</title>
    <updated>2025-09-26T17:59:46+00:00</updated>
    <author>
      <name>/u/Brave-Hold-9389</name>
      <uri>https://old.reddit.com/user/Brave-Hold-9389</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8acu/the_benchmarks_are_favouring_qwen3_max/"&gt; &lt;img alt="The benchmarks are favouring Qwen3 max" src="https://preview.redd.it/5hyvzvs8tjrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=52ee02c3689ab52b47faa094175ba42f13984273" title="The benchmarks are favouring Qwen3 max" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The best non thinking model&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Brave-Hold-9389"&gt; /u/Brave-Hold-9389 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5hyvzvs8tjrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8acu/the_benchmarks_are_favouring_qwen3_max/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr8acu/the_benchmarks_are_favouring_qwen3_max/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T17:59:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz3tp</id>
    <title>AppUse : Create virtual desktops for AI agents to focus on specific apps</title>
    <updated>2025-09-27T16:10:44+00:00</updated>
    <author>
      <name>/u/Impressive_Half_2819</name>
      <uri>https://old.reddit.com/user/Impressive_Half_2819</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz3tp/appuse_create_virtual_desktops_for_ai_agents_to/"&gt; &lt;img alt="AppUse : Create virtual desktops for AI agents to focus on specific apps" src="https://external-preview.redd.it/OWh5NmNyMHBlcXJmMX6ZB2qtngjb8gjMyThUUgd5eO-QeupzbFEkT8WNsDs6.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f859085a7afac56fbb2cb820879d4b960d1cf117" title="AppUse : Create virtual desktops for AI agents to focus on specific apps" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;App-Use lets you scope agents to just the apps they need. Instead of full desktop access, say &amp;quot;only work with Safari and Notes&amp;quot; or &amp;quot;just control iPhone Mirroring&amp;quot; - visual isolation without new processes for perfectly focused automation.&lt;/p&gt; &lt;p&gt;Running computer use on the entire desktop often causes agent hallucinations and loss of focus when they see irrelevant windows and UI elements. AppUse solves this by creating composited views where agents only see what matters, dramatically improving task completion accuracy&lt;/p&gt; &lt;p&gt;Currently macOS only (Quartz compositing engine).&lt;/p&gt; &lt;p&gt;Read the full guide: &lt;a href="https://trycua.com/blog/app-use"&gt;https://trycua.com/blog/app-use&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github : &lt;a href="https://github.com/trycua/cua"&gt;https://github.com/trycua/cua&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Impressive_Half_2819"&gt; /u/Impressive_Half_2819 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/a0cnq0bpeqrf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz3tp/appuse_create_virtual_desktops_for_ai_agents_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz3tp/appuse_create_virtual_desktops_for_ai_agents_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:10:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrr8oh</id>
    <title>man imagine if versus add a LLM comparison section so i can do this</title>
    <updated>2025-09-27T09:51:28+00:00</updated>
    <author>
      <name>/u/BuriqKalipun</name>
      <uri>https://old.reddit.com/user/BuriqKalipun</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BuriqKalipun"&gt; /u/BuriqKalipun &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kiuw4nopiorf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrr8oh/man_imagine_if_versus_add_a_llm_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrr8oh/man_imagine_if_versus_add_a_llm_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T09:51:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrvgmj</id>
    <title>Sample Forge - Research tool for deterministic inference and convergent sampling parameters in large language models.</title>
    <updated>2025-09-27T13:37:30+00:00</updated>
    <author>
      <name>/u/no_witty_username</name>
      <uri>https://old.reddit.com/user/no_witty_username</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi folks, I made a research tools that allows you to perform deterministic inference on any local large language model. This way you can test any variable changes and see for yourself the affects those changes have on the output of the LLM's response. It also allows you to perform automated reasoning benchmarking of a local language model of your choice, this way you can measure the perplexity drop of any quantized model or differences between reasoning capabilities of models or sampling parameters. It also has a fully automated way of converging on the best sampling parameters for a given model when it comes to reasoning capabilities. I made 2 videos for the project so you can see what its about at a glance the main guide is here &lt;a href="https://www.youtube.com/watch?v=EyE5BrUut2o"&gt;https://www.youtube.com/watch?v=EyE5BrUut2o&lt;/a&gt;, the instillation video is here &lt;a href="https://youtu.be/FJpmD3b2aps"&gt;https://youtu.be/FJpmD3b2aps&lt;/a&gt; and the repo is here &lt;a href="https://github.com/manfrom83/Sample-Forge"&gt;https://github.com/manfrom83/Sample-Forge&lt;/a&gt;. If you have more questions id be glad to answer them here. Cheers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/no_witty_username"&gt; /u/no_witty_username &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvgmj/sample_forge_research_tool_for_deterministic/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvgmj/sample_forge_research_tool_for_deterministic/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvgmj/sample_forge_research_tool_for_deterministic/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T13:37:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr4v7e</id>
    <title>Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&lt;15GB VRAM)</title>
    <updated>2025-09-26T15:47:52+00:00</updated>
    <author>
      <name>/u/danielhanchen</name>
      <uri>https://old.reddit.com/user/danielhanchen</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"&gt; &lt;img alt="Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&amp;lt;15GB VRAM)" src="https://preview.redd.it/pq6ej7up5jrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=121b14e94d54780f5a2a7ae8625d9bd2f60d60f6" title="Gpt-oss Reinforcement Learning - Fastest inference now in Unsloth! (&amp;lt;15GB VRAM)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey guys we've got lots of updates for Reinforcement Learning (RL)! We’re excited to introduce gpt-oss, Vision, and even better RL in Unsloth. Our new gpt-oss RL inference also achieves the fastest token/s vs. any other implementation. Our GitHub: &lt;a href="https://github.com/unslothai/unsloth"&gt;https://github.com/unslothai/unsloth&lt;/a&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Inference is crucial in RL training. Since gpt-oss RL isn’t vLLM compatible, we rewrote Transformers inference for 3× faster speeds (~21 tok/s). For BF16, Unsloth also delivers the fastest inference (~30 tok/s), especially relative to VRAM use vs. any other implementation.&lt;/li&gt; &lt;li&gt;We made a free &amp;amp; completely new custom notebook showing how RL can automatically create faster matrix multiplication kernels: &lt;a href="https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B"&gt;gpt-oss-20b GSPO Colab&lt;/a&gt;-GRPO.ipynb). We also show you how to counteract reward-hacking which is one of RL's biggest challenges.&lt;/li&gt; &lt;li&gt;Unsloth also uses the least VRAM (50% less) and supports the most context length (8x more). gpt-oss-20b RL fits in 15GB VRAM.&lt;/li&gt; &lt;li&gt;As usual, there is no accuracy degradation.&lt;/li&gt; &lt;li&gt;We released &lt;a href="https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl"&gt;Vision RL&lt;/a&gt;, allowing you to train Gemma 3, Qwen2.5-VL with GRPO free in our Colab notebooks.&lt;/li&gt; &lt;li&gt;We also previously introduced more &lt;a href="https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/memory-efficient-rl"&gt;memory efficient RL&lt;/a&gt; with Standby and extra kernels and algorithms. Unsloth RL now uses 90% less VRAM, and enables 16× longer context lengths than any setup.&lt;/li&gt; &lt;li&gt; ⚠️ Reminder to NOT use Flash Attention 3 for gpt-oss as it'll make your training loss wrong.&lt;/li&gt; &lt;li&gt;We released &lt;a href="https://docs.unsloth.ai/models/deepseek-v3.1-how-to-run-locally"&gt;DeepSeek-V3.1-Terminus&lt;/a&gt; Dynamic GGUFs. We showcased how 3-bit V3.1 scores 75.6% on Aider Polyglot, beating Claude-4-Opus (thinking).&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For our new gpt-oss RL release, would recommend you guys to read our blog/guide which details our entire findings and bugs etc.: &lt;a href="https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning"&gt;https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Thanks guys for reading and hope you all have a lovely Friday and weekend! 🦥&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/danielhanchen"&gt; /u/danielhanchen &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/pq6ej7up5jrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr4v7e/gptoss_reinforcement_learning_fastest_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:47:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrrjnu</id>
    <title>LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs</title>
    <updated>2025-09-27T10:10:26+00:00</updated>
    <author>
      <name>/u/Balance-</name>
      <uri>https://old.reddit.com/user/Balance-</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h3&gt;Abstract&lt;/h3&gt; &lt;blockquote&gt; &lt;p&gt;Large Language Diffusion Models, or diffusion LLMs, have emerged as a significant focus in NLP research, with substantial effort directed toward understanding their scalability and downstream task performance. However, their long-context capabilities remain unexplored, lacking systematic analysis or methods for context extension.&lt;/p&gt; &lt;p&gt;In this work, we present the first systematic investigation comparing the long-context performance of diffusion LLMs and traditional auto-regressive LLMs. We first identify a unique characteristic of diffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably stable perplexity during direct context extrapolation. Moreover, where auto-regressive models fail outright during the Needle-In-A-Haystack task with context exceeding their pretrained length, we discover diffusion LLMs exhibit a distinct local perception phenomenon, enabling successful retrieval from recent context segments. We explain both phenomena through the lens of Rotary Position Embedding (RoPE) scaling theory.&lt;/p&gt; &lt;p&gt;Building on these observations, we propose LongLLaDA, a training-free method that integrates LLaDA with the NTK-based RoPE extrapolation. Our results validate that established extrapolation scaling laws remain effective for extending the context windows of diffusion LLMs. Furthermore, we identify long-context tasks where diffusion LLMs outperform auto-regressive LLMs and others where they fall short. Consequently, this study establishes the first length extrapolation method for diffusion LLMs while providing essential theoretical insights and empirical benchmarks critical for advancing future research on long-context diffusion LLMs.&lt;/p&gt; &lt;/blockquote&gt; &lt;ul&gt; &lt;li&gt;Paper: &lt;a href="https://arxiv.org/abs/2506.14429"&gt;https://arxiv.org/abs/2506.14429&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Code: &lt;a href="https://github.com/OpenMOSS/LongLLaDA"&gt;https://github.com/OpenMOSS/LongLLaDA&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Balance-"&gt; /u/Balance- &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://arxiv.org/abs/2506.14429"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrjnu/longllada_unlocking_long_context_capabilities_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrjnu/longllada_unlocking_long_context_capabilities_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T10:10:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrtb8q</id>
    <title>DayFlow: productivity tracker that supports local models</title>
    <updated>2025-09-27T11:54:19+00:00</updated>
    <author>
      <name>/u/Far-Incident822</name>
      <uri>https://old.reddit.com/user/Far-Incident822</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A few months ago I &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lr5g8x/productivity_tracker_that_uses_gemma34bb/"&gt;posted my prototype&lt;/a&gt; for a Mac productivity tracker that uses a local Gemma model to monitor productivity. My prototype would take screenshots of a user's screen on a regular increment, and try to figure out how productive they were being. A few days ago, I came across a similar but much more refined product, that my friend sent me, that I thought I'd share here.&lt;/p&gt; &lt;p&gt;It's an open source application called &lt;a href="https://github.com/JerryZLiu/Dayflow"&gt;DayFlow&lt;/a&gt; and it supports Mac . It currently turns your screen activity into a timeline of your day with AI summaries of every section, and highlights of when you got distracted. It supports both local models as well as cloud based models. What I think is particularly cool is the upcoming features that allow you to chat with the model and figure out details about your day. I've tested it for a few days using Gemini cloud, and it works really well. I haven't tried local yet, but I imagine that it'll work well there too.&lt;/p&gt; &lt;p&gt;I think the general concept is a good one. For example, with a sufficiently advanced model, a user could get suggestions on how to get unstuck with something that they're coding , without needing to use an AI coding tool or switch contexts to a web browser. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Far-Incident822"&gt; /u/Far-Incident822 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrtb8q/dayflow_productivity_tracker_that_supports_local/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrtb8q/dayflow_productivity_tracker_that_supports_local/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrtb8q/dayflow_productivity_tracker_that_supports_local/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T11:54:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrrgoy</id>
    <title>monkeSearch technical report - out now</title>
    <updated>2025-09-27T10:05:20+00:00</updated>
    <author>
      <name>/u/External_Mushroom978</name>
      <uri>https://old.reddit.com/user/External_Mushroom978</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt; &lt;img alt="monkeSearch technical report - out now" src="https://preview.redd.it/khpdyx7blorf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6e625174d3ed12cc8c3f73e44f15dba89a2ed005" title="monkeSearch technical report - out now" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;you could read our report here - &lt;a href="https://monkesearch.github.io/"&gt;https://monkesearch.github.io/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mushroom978"&gt; /u/External_Mushroom978 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/khpdyx7blorf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrrgoy/monkesearch_technical_report_out_now/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T10:05:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1nr3n2r</id>
    <title>How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?</title>
    <updated>2025-09-26T15:00:41+00:00</updated>
    <author>
      <name>/u/Striking_Wedding_461</name>
      <uri>https://old.reddit.com/user/Striking_Wedding_461</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"&gt; &lt;img alt="How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?" src="https://preview.redd.it/kabtcb5twirf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2cfbae1d53a3abc93a95be9789c678d6280c6d58" title="How am I supposed to know which third party provider can be trusted not to completely lobotomize a model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know this is mostly open-weights and open-source discussion and all that jazz but let's be real, unless your name is Achmed Al-Jibani from Qatar or you pi*ss gold you're not getting the SOTA performance with open-weight models like Kimi K2 or DeepSeek because you have to quantize it, your options as an average-wage pleb are either:&lt;/p&gt; &lt;p&gt;a) third party providers&lt;br /&gt; b) running it yourself but quantized to hell&lt;br /&gt; c) spinning up a pod and using a third party providers GPU (expensive) to run your model&lt;/p&gt; &lt;p&gt;I opted for a) most of the time and a recent evaluation done on the accuracy of the Kimi K2 0905 models provided by third party providers has me doubting this decision.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Striking_Wedding_461"&gt; /u/Striking_Wedding_461 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kabtcb5twirf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nr3n2r/how_am_i_supposed_to_know_which_third_party/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T15:00:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrzvsa</id>
    <title>Did Nvidia Digits die?</title>
    <updated>2025-09-27T16:42:18+00:00</updated>
    <author>
      <name>/u/Status-Secret-4292</name>
      <uri>https://old.reddit.com/user/Status-Secret-4292</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I can't find anything recent for it and was pretty hyped at the time of what they said they were offering.&lt;/p&gt; &lt;p&gt;Ancillary question, is there actually anything else comparable at a similar price point?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Status-Secret-4292"&gt; /u/Status-Secret-4292 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrzvsa/did_nvidia_digits_die/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:42:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrpvou</id>
    <title>Qwen3-Coder-30B-A3B on 5060 Ti 16GB</title>
    <updated>2025-09-27T08:22:51+00:00</updated>
    <author>
      <name>/u/Weird_Researcher_472</name>
      <uri>https://old.reddit.com/user/Weird_Researcher_472</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;What is the best way to run this model with my Hardware? I got 32GB of DDR4 RAM at 3200 MHz (i know, pretty weak) paired with a Ryzen 5 3600 and my 5060 Ti 16GB VRAM. In LM Studio, using Qwen3 Coder 30B, i am only getting around 18 tk/s with a context window set to 16384 tokens and the speed is degrading to around 10 tk/s once it nears the full 16k context window. I have read from other people that they are getting speeds of over 40 tk/s with also way bigger context windows, up to 65k tokens.&lt;/p&gt; &lt;p&gt;When i am running GPT-OSS-20B as example on the same hardware, i get over 100 tk/s in LM Studio with a ctx of 32768 tokens. Once it nears the 32k it degrades to around 65 tk/s which is MORE than enough for me!&lt;/p&gt; &lt;p&gt;I just wish i could get similar speeds with Qwen3-Coder-30b ..... Maybe i am doing some settings wrong?&lt;/p&gt; &lt;p&gt;Or should i use llama-cpp to get better speeds? I would really appreciate your help !&lt;/p&gt; &lt;p&gt;EDIT: My OS is Windows 11, sorry i forgot that part. And i want to use unsloth Q4_K_XL quant.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Weird_Researcher_472"&gt; /u/Weird_Researcher_472 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrpvou/qwen3coder30ba3b_on_5060_ti_16gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrpvou/qwen3coder30ba3b_on_5060_ti_16gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrpvou/qwen3coder30ba3b_on_5060_ti_16gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T08:22:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrnkji</id>
    <title>How much memory do you need for gpt-oss:20b</title>
    <updated>2025-09-27T05:57:10+00:00</updated>
    <author>
      <name>/u/milesChristi16</name>
      <uri>https://old.reddit.com/user/milesChristi16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrnkji/how_much_memory_do_you_need_for_gptoss20b/"&gt; &lt;img alt="How much memory do you need for gpt-oss:20b" src="https://preview.redd.it/i0raxir8dnrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3d3716eb7d823333ca0e80f3dc97c3917de46724" title="How much memory do you need for gpt-oss:20b" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, I'm fairly new to using ollama and running LLMs locally, but I was able to load the gpt-oss:20b on my m1 macbook with 16 gb of ram and it runs ok, albeit very slowly. I tried to install it on my windows desktop to compare performance, but I got the error &amp;quot;500: memory layout cannot be allocated.&amp;quot; I take it this means I don't have enough vRAM/RAM to load the model, but this surprises me since I have 16 gb vRAM as well as 16 gb system RAM, which seems comparable to my macbook. So do I really need more memory or is there something I am doing wrong that is preventing me from running the model? I attached a photo of my system specs for reference, thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/milesChristi16"&gt; /u/milesChristi16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/i0raxir8dnrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrnkji/how_much_memory_do_you_need_for_gptoss20b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrnkji/how_much_memory_do_you_need_for_gptoss20b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T05:57:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrhr13</id>
    <title>K2-Think 32B - Reasoning model from UAE</title>
    <updated>2025-09-27T00:41:08+00:00</updated>
    <author>
      <name>/u/Mr_Moonsilver</name>
      <uri>https://old.reddit.com/user/Mr_Moonsilver</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt; &lt;img alt="K2-Think 32B - Reasoning model from UAE" src="https://preview.redd.it/smnqi3vqrlrf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=2601d97f0ceaba3a4215f2d8ea7be937412c5b79" title="K2-Think 32B - Reasoning model from UAE" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Seems like a strong model and a very good paper released alongside. Opensource is going strong at the moment, let's hope this benchmark holds true.&lt;/p&gt; &lt;p&gt;Huggingface Repo: &lt;a href="https://huggingface.co/LLM360/K2-Think"&gt;https://huggingface.co/LLM360/K2-Think&lt;/a&gt;&lt;br /&gt; Paper: &lt;a href="https://huggingface.co/papers/2509.07604"&gt;https://huggingface.co/papers/2509.07604&lt;/a&gt;&lt;br /&gt; Chatbot running this model: &lt;a href="https://www.k2think.ai/guest"&gt;https://www.k2think.ai/guest&lt;/a&gt; (runs at 1200 - 2000 tk/s)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mr_Moonsilver"&gt; /u/Mr_Moonsilver &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/smnqi3vqrlrf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrhr13/k2think_32b_reasoning_model_from_uae/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T00:41:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrf6s3</id>
    <title>Yes you can run 128K context GLM-4.5 355B on just RTX 3090s</title>
    <updated>2025-09-26T22:41:00+00:00</updated>
    <author>
      <name>/u/Arli_AI</name>
      <uri>https://old.reddit.com/user/Arli_AI</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"&gt; &lt;img alt="Yes you can run 128K context GLM-4.5 355B on just RTX 3090s" src="https://b.thumbs.redditmedia.com/rWEspXLsl6q38iq4HW7So1f92J_p3bfTwDjsTnkup2Y.jpg" title="Yes you can run 128K context GLM-4.5 355B on just RTX 3090s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Why buy expensive GPUs when more RTX 3090s work too :D&lt;/p&gt; &lt;p&gt;You just get more GB/$ on RTX 3090s compared to any other GPU. Did I help deplete the stock of used RTX 3090s? Maybe.&lt;/p&gt; &lt;p&gt;Arli AI as an inference service is literally just run by one person (me, Owen Arli), and to keep costs low so that it can stay profitable without VC funding, RTX 3090s were clearly the way to go. &lt;/p&gt; &lt;p&gt;To run these new larger and larger MoE models, I was trying to run 16x3090s off of one single motherboard. I tried many motherboards and different modded BIOSes but in the end it wasn't worth it. I realized that the correct way to stack MORE RTX 3090s is actually to just run multi-node serving using vLLM and ray clustering.&lt;/p&gt; &lt;p&gt;This here is GLM-4.5 AWQ 4bit quant running with the full 128K context (131072 tokens). Doesn't even need an NVLink backbone or 9999 Gbit networking either, this is just over a 10Gbe connection across 2 nodes of 8x3090 servers and we are getting a good 30+ tokens/s generation speed consistently per user request. Pipeline parallel seems to be very forgiving of slow interconnects.&lt;/p&gt; &lt;p&gt;While I realized that by stacking more GPUs with pipeline parallels across nodes, it almost linearly increases the prompt processing speed. So we are good to go in that performance metric too. Really makes me wonder who needs the insane NVLink interconnect speeds, even large inference providers probably don't really need anything more than PCIe 4.0 and 40Gbe/80Gbe interconnects.&lt;/p&gt; &lt;p&gt;All you need to run this is follow vLLM's guide on how to run multi node serving (&lt;a href="https://docs.vllm.ai/en/stable/serving/parallelism%5C_scaling.html#what-is-ray"&gt;https://docs.vllm.ai/en/stable/serving/parallelism\_scaling.html#what-is-ray&lt;/a&gt;) and then run the model with setting --tensor-parallel to the maximum number of GPUs per node and set --pipeline-parallel to the number of nodes you have. The point is to make sure inter-node communication is only for pipeline parallel which does not need much bandwidth.&lt;/p&gt; &lt;p&gt;The only way for RTX 3090s to be obsolete and prevent me from buying them is if Nvidia releases 24GB RTX 5070Ti Super/5080 Super or Intel finally releases the Arc B60 48GB in any quantity to the masses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Arli_AI"&gt; /u/Arli_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1nrf6s3"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrf6s3/yes_you_can_run_128k_context_glm45_355b_on_just/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-26T22:41:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryti7</id>
    <title>How do you get qwen next to stop being such a condescending suck up?</title>
    <updated>2025-09-27T15:59:13+00:00</updated>
    <author>
      <name>/u/fiendindolent</name>
      <uri>https://old.reddit.com/user/fiendindolent</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I just tried the new qwen next instruct model and it seems overall quite good for local use but it keep ending seemingly innocuous questions and conversations with things like &lt;/p&gt; &lt;p&gt;&amp;quot;Your voice matters.&lt;br /&gt; The truth matters.&lt;br /&gt; I am here to help you find it.&amp;quot;&lt;/p&gt; &lt;p&gt;If this model had a face I'm sure it would be punchable. Is there any way to tune the settings and make it less insufferable?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fiendindolent"&gt; /u/fiendindolent &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryti7/how_do_you_get_qwen_next_to_stop_being_such_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:59:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrz4hd</id>
    <title>MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders – Built on qwen3.c - WORK IN PROGRESS</title>
    <updated>2025-09-27T16:11:31+00:00</updated>
    <author>
      <name>/u/QuanstScientist</name>
      <uri>https://old.reddit.com/user/QuanstScientist</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt; &lt;img alt="MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders – Built on qwen3.c - WORK IN PROGRESS" src="https://external-preview.redd.it/RQD3iD79k_dyz-ZzZVJ-NWQbGKS-OnCk9a74XO6E3_8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=22139ec43287a754031bbd97119f84f0e2e05306" title="MetalQwen3: Full GPU-Accelerated Qwen3 Inference on Apple Silicon with Metal Shaders – Built on qwen3.c - WORK IN PROGRESS" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt;,&lt;/p&gt; &lt;p&gt;Inspired by Adrian Cable's awesome qwen3.c project (that simple, educational C inference engine for Qwen3 models – check out the original post here: &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lpejnj/qwen3_inference_engine_in_c_simple_educational_fun/&lt;/a&gt;), I decided to take it a step further for Apple Silicon users. I've created MetalQwen3, a Metal GPU implementation that runs the Qwen3 transformer model entirely on macOS with complete compute shader acceleration.&lt;/p&gt; &lt;p&gt;Full details, shaders, and the paper are in the repo: &lt;a href="https://github.com/BoltzmannEntropy/metalQwen3"&gt;https://github.com/BoltzmannEntropy/metalQwen3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/143v71boeqrf1.png?width=963&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c857b71ec102c03e3de6f4787168a477663f5a"&gt;https://preview.redd.it/143v71boeqrf1.png?width=963&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02c857b71ec102c03e3de6f4787168a477663f5a&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It not meant to replace heavy hitters like vLLM or llama.cpp – it's more of a lightweight, educational extension focused on GPU optimization for M-series chips. But hey, the shaders are fully working, and it achieves solid performance: around 75 tokens/second on my M1 Max, which is about 2.1x faster than the CPU baseline.&lt;/p&gt; &lt;h1&gt;Key Features:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Full GPU Acceleration&lt;/strong&gt;: All core operations (RMSNorm, QuantizedMatMul, Softmax, SwiGLU, RoPE, Multi-Head Attention) run on the GPU – no CPU fallbacks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Qwen3 Architecture Support&lt;/strong&gt;: Handles QK-Norm, Grouped Query Attention (20:4 heads), RoPE, Q8_0 quantization, and a 151K vocab. Tested with Qwen3-4B, but extensible to others.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI-Compatible API Server&lt;/strong&gt;: Drop-in chat completions with streaming, temperature/top_p control, and health monitoring.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Benchmarking Suite&lt;/strong&gt;: Integrated with prompt-test for easy comparisons against ollama, llama.cpp, etc. Includes TTFT, tokens/sec, and memory metrics.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Optimizations&lt;/strong&gt;: Command batching, buffer pooling, unified memory leveraging – all in clean C++ with metal-cpp.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Academic Touch&lt;/strong&gt;: There's even a 9-page IEEE-style paper in the repo detailing the implementation and performance analysis.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Huge shoutout to Adrian for the foundational qwen3.c – this project builds directly on his educational CPU impl, keeping things simple while adding Metal shaders for that GPU boost. If you're into learning transformer internals or just want faster local inference on your Mac, this might be fun to tinker with.&lt;/p&gt; &lt;p&gt;AI coding agents like Claude helped speed this up a ton – from months to weeks. If you're on Apple Silicon, give it a spin and let me know what you think! PRs welcome for larger models, MoE support, or more optimizations.&lt;/p&gt; &lt;p&gt;Best,&lt;/p&gt; &lt;p&gt;Shlomo. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/QuanstScientist"&gt; /u/QuanstScientist &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrz4hd/metalqwen3_full_gpuaccelerated_qwen3_inference_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T16:11:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrvo9g</id>
    <title>Finally InternVL3_5 Flash versions coming</title>
    <updated>2025-09-27T13:47:01+00:00</updated>
    <author>
      <name>/u/NeuralNakama</name>
      <uri>https://old.reddit.com/user/NeuralNakama</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;not available but created on &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-8B-Flash&lt;/a&gt;&lt;br /&gt; &lt;a href="https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash"&gt;https://huggingface.co/OpenGVLab/InternVL3_5-1B-Flash&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NeuralNakama"&gt; /u/NeuralNakama &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrvo9g/finally_internvl3_5_flash_versions_coming/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T13:47:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrsyic</id>
    <title>Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model.</title>
    <updated>2025-09-27T11:34:55+00:00</updated>
    <author>
      <name>/u/EmirTanis</name>
      <uri>https://old.reddit.com/user/EmirTanis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"&gt; &lt;img alt="Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model." src="https://preview.redd.it/zgn5su20zorf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6a1a03e3f9c5159beace596248885ac1b75c0612" title="Benchmark to find similarly trained LLMs by exploiting subjective listings, first stealth model victim; code-supernova, xAIs model." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello,&lt;/p&gt; &lt;p&gt;Any model who has a _sample1 in the name means there's only one sample for it, 5 samples for the rest.&lt;/p&gt; &lt;p&gt;the benchmark is pretty straight forward, the AI is asked to list its &amp;quot;top 50 best humans currently alive&amp;quot;, which is quite a subjective topic, it lists them in a json like format from 1 to 50, then I use a RBO based algorithm to place them on a node map. &lt;/p&gt; &lt;p&gt;I've only done Gemini and Grok for now as I don't have access to anymore models, so the others may not be accurate.&lt;/p&gt; &lt;p&gt;for the future, I'd like to implement multiple categories (not just best humans) as that would also give a much larger sample amount.&lt;/p&gt; &lt;p&gt;to anybody else interested in making something similar, a standardized system prompt is very important.&lt;/p&gt; &lt;p&gt;.py file; &lt;a href="https://smalldev.tools/share-bin/CfdC7foV"&gt;https://smalldev.tools/share-bin/CfdC7foV&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/EmirTanis"&gt; /u/EmirTanis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zgn5su20zorf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrsyic/benchmark_to_find_similarly_trained_llms_by/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T11:34:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1nryoa5</id>
    <title>Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card</title>
    <updated>2025-09-27T15:53:01+00:00</updated>
    <author>
      <name>/u/Normal_Onion_512</name>
      <uri>https://old.reddit.com/user/Normal_Onion_512</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt; &lt;img alt="Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card" src="https://external-preview.redd.it/glz22pd-75yG_ynznmuaF8hifkLCtseU0s4FKfNwWlI.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01875fcb778a5024d673b34876da00b5dcb1b48e" title="Megrez2: 21B latent, 7.5B on VRAM, 3B active—MoE on single 8GB card" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I came across Megrez2-3x7B-A3B on Hugging Face and thought it worth sharing. &lt;/p&gt; &lt;p&gt;I read through their tech report, and it says that the model has a unique MoE architecture with a layer-sharing expert design, so the &lt;strong&gt;checkpoint stores 7.5B params&lt;/strong&gt; yet can compose with the &lt;strong&gt;equivalent of 21B latent weights&lt;/strong&gt; at run-time while only 3B are active per token.&lt;/p&gt; &lt;p&gt;I was intrigued by the published Open-Compass figures, since it places the model &lt;strong&gt;on par with or slightly above Qwen-30B-A3B&lt;/strong&gt; in MMLU / GPQA / MATH-500 with roughly &lt;strong&gt;1/4 the VRAM requirements&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;There is already a &lt;strong&gt;GGUF file&lt;/strong&gt; and the matching &lt;strong&gt;llama.cpp branch&lt;/strong&gt; which I posted below (though it can also be found in the gguf page). The supplied &lt;strong&gt;Q4 quant occupies about 4 GB; FP8 needs approximately 8 GB&lt;/strong&gt;. The developer notes that FP16 currently has a couple of issues with coding tasks though, which they are working on solving. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;License is Apache 2.0, and it is currently running a Huggingface Space as well.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Model: [Infinigence/Megrez2-3x7B-A3B] &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GGUF: &lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Live Demo: &lt;a href="https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B"&gt;https://huggingface.co/spaces/Infinigence/Megrez2-3x7B-A3B&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo: &lt;a href="https://github.com/Infinigence/Megrez2"&gt;https://github.com/Infinigence/Megrez2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;llama.cpp branch: &lt;a href="https://github.com/infinigence/llama.cpp/tree/support-megrez"&gt;https://github.com/infinigence/llama.cpp/tree/support-megrez&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If anyone tries it, I would be interested to hear your throughput and quality numbers.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Normal_Onion_512"&gt; /u/Normal_Onion_512 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Infinigence/Megrez2-3x7B-A3B-GGUF"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nryoa5/megrez2_21b_latent_75b_on_vram_3b_activemoe_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T15:53:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrqrva</id>
    <title>Moondream 3 Preview: Frontier-level reasoning at a blazing speed</title>
    <updated>2025-09-27T09:20:41+00:00</updated>
    <author>
      <name>/u/ProfessionalJackals</name>
      <uri>https://old.reddit.com/user/ProfessionalJackals</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalJackals"&gt; /u/ProfessionalJackals &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://moondream.ai/blog/moondream-3-preview"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrqrva/moondream_3_preview_frontierlevel_reasoning_at_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrqrva/moondream_3_preview_frontierlevel_reasoning_at_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T09:20:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1nrx3jr</id>
    <title>When are GPU prices going to get cheaper?</title>
    <updated>2025-09-27T14:48:09+00:00</updated>
    <author>
      <name>/u/KardelenAyshe</name>
      <uri>https://old.reddit.com/user/KardelenAyshe</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm starting to lose hope. I really can't afford these current GPU prices. Does anyone have any insight on when we might see a significant price drop?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/KardelenAyshe"&gt; /u/KardelenAyshe &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1nrx3jr/when_are_gpu_prices_going_to_get_cheaper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-09-27T14:48:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
