<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-12-19T04:57:00+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ppq8pi</id>
    <title>Z-Image is now the default image model on HuggingChat</title>
    <updated>2025-12-18T13:01:20+00:00</updated>
    <author>
      <name>/u/Nunki08</name>
      <uri>https://old.reddit.com/user/Nunki08</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt; &lt;img alt="Z-Image is now the default image model on HuggingChat" src="https://b.thumbs.redditmedia.com/q04f8-Hq7gSnGXdIq-IW8V70b-2l8sOF10WS2JF_Kks.jpg" title="Z-Image is now the default image model on HuggingChat" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From Victor M (Hugging Face) on ùïè: &lt;a href="https://x.com/victormustar/status/2001629770329858391?s=20"&gt;https://x.com/victormustar/status/2001629770329858391&lt;/a&gt;&lt;br /&gt; HuggingChat: &lt;a href="https://huggingface.co/chat/"&gt;https://huggingface.co/chat/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nunki08"&gt; /u/Nunki08 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppq8pi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppq8pi/zimage_is_now_the_default_image_model_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T13:01:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppx93g</id>
    <title>VibeVoice 7B and 1.5B FastAPI Wrapper</title>
    <updated>2025-12-18T17:51:20+00:00</updated>
    <author>
      <name>/u/TommarrA</name>
      <uri>https://old.reddit.com/user/TommarrA</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"&gt; &lt;img alt="VibeVoice 7B and 1.5B FastAPI Wrapper" src="https://external-preview.redd.it/ocq5FpDlatga69140E_bI6uHAd--cmL-kjurFzGrpzw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f4bfc93e7c6b2a4bfa1956edd98031f239c71437" title="VibeVoice 7B and 1.5B FastAPI Wrapper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I had created a fast API wrapper for the original VibeVoice model (7B and 1.5B)&lt;/p&gt; &lt;p&gt;It allows you to use custom voices unlike the current iteration of VibeVoice that has Microsoft generated voice models.&lt;/p&gt; &lt;p&gt;It works well for my ebook narration use case so thought I would share with the community too.&lt;/p&gt; &lt;p&gt;Thanks to folks who had made a backup of the original code.&lt;/p&gt; &lt;p&gt;I will eventually build in the ability to use the 0.5B model as well but current iteration only support and 7B and 1.5B models&lt;/p&gt; &lt;p&gt;Let me know how it works for your use cases&lt;/p&gt; &lt;p&gt;Docker is the preferred deployment model - tested on Ubuntu.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TommarrA"&gt; /u/TommarrA &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ncoder-ai/VibeVoice-FastAPI"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppx93g/vibevoice_7b_and_15b_fastapi_wrapper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppm9xm</id>
    <title>NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano</title>
    <updated>2025-12-18T09:03:01+00:00</updated>
    <author>
      <name>/u/paf1138</name>
      <uri>https://old.reddit.com/user/paf1138</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt; &lt;img alt="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" src="https://external-preview.redd.it/i9rG1D6xcH_2B9JTT5Ak5wKM4ExK483hNq6oNeOkRNo.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=aa9a037e77932298ed68f09b93c42491dd8ab8e0" title="NVIDIA Publishes Complete Evaluation Recipe for Nemotron 3 Nano" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/paf1138"&gt; /u/paf1138 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/blog/nvidia/nemotron-3-nano-evaluation-recipe"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppm9xm/nvidia_publishes_complete_evaluation_recipe_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T09:03:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppongx</id>
    <title>Fast on-device Speech-to-text for Home Assistant (open source)</title>
    <updated>2025-12-18T11:34:57+00:00</updated>
    <author>
      <name>/u/banafo</name>
      <uri>https://old.reddit.com/user/banafo</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt; &lt;img alt="Fast on-device Speech-to-text for Home Assistant (open source)" src="https://external-preview.redd.it/6PRNLd3TFMw1DCfYP7618_nVHzwQRPRrDRjMqQg7XGU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cff7a166c2a85ced6d24604f32dc307cf599fedf" title="Fast on-device Speech-to-text for Home Assistant (open source)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We just released &lt;a href="https://github.com/orgs/kroko-ai/repositories"&gt;kroko-onnx-home-assistant &lt;/a&gt; is a &lt;strong&gt;local&lt;/strong&gt; streaming STT pipeline for home assistant.&lt;/p&gt; &lt;p&gt;It's currently just a fork of the excellent &lt;a href="https://github.com/ptbsare/sherpa-onnx-tts-stt"&gt;https://github.com/ptbsare/sherpa-onnx-tts-stt&lt;/a&gt; with support for our models added, hopefully it will be accepted in the main project. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlights:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;High quality&lt;/li&gt; &lt;li&gt;Real streaming (partial results, low latency)&lt;/li&gt; &lt;li&gt;100% local &amp;amp; privacy-first&lt;/li&gt; &lt;li&gt;optimized for fast CPU inference, even in low resources raspberry pi's&lt;/li&gt; &lt;li&gt;Does not require additional VAD&lt;/li&gt; &lt;li&gt;Home Assistant integration&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Repo:&lt;br /&gt; [&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant%5D()"&gt;https://github.com/kroko-ai/kroko-onnx-home-assistant]()&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you want to test the model quality before installing: the huggingface models running in the browser is the easiest way: &lt;a href="https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm"&gt;https://huggingface.co/spaces/Banafo/Kroko-Streaming-ASR-Wasm&lt;/a&gt; &lt;/p&gt; &lt;p&gt;A big thanks to:&lt;br /&gt; - NaggingDaivy on discord, for the assistance.&lt;br /&gt; - the sherpa-onnx-tts-stt team for adding support for streaming models in record time.&lt;/p&gt; &lt;p&gt;Want us to integrate with your favorite open source project ? Contact us on discord:&lt;br /&gt; &lt;a href="https://discord.gg/TEbfnC7b"&gt;https://discord.gg/TEbfnC7b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Some releases you may have missed:&lt;br /&gt; - Freewitch Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Asterisk Module: &lt;a href="https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko"&gt;https://github.com/kroko-ai/integration-demos/tree/master/asterisk-kroko&lt;/a&gt;&lt;br /&gt; - Full Asterisk based voicebot running with Kroko streaming models: &lt;a href="https://github.com/hkjarral/Asterisk-AI-Voice-Agent"&gt;https://github.com/hkjarral/Asterisk-AI-Voice-Agent&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We are still working on the main models, code and documentation as well, but held up a bit with urgent paid work deadlines, more coming there soon too.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/banafo"&gt; /u/banafo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/kroko-ai/kroko-onnx-home-assistant"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppongx/fast_ondevice_speechtotext_for_home_assistant/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T11:34:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq7wqc</id>
    <title>For Local LLM RAG ‚Äî 64GB vs 128GB RAM?</title>
    <updated>2025-12-19T01:11:05+00:00</updated>
    <author>
      <name>/u/TeacherIll7604</name>
      <uri>https://old.reddit.com/user/TeacherIll7604</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm planning a local machine mainly for:&lt;/p&gt; &lt;p&gt;- Local LLM experimentation (RAG pipelines, embeddings, indexing)&lt;/p&gt; &lt;p&gt;- Some light fine-tuning / training experiments&lt;/p&gt; &lt;p&gt;- Gaming on the same machine&lt;/p&gt; &lt;p&gt;Planned specs:&lt;/p&gt; &lt;p&gt;- CPU: i9-14900K&lt;/p&gt; &lt;p&gt;- GPU: RTX 4090 (24GB)&lt;/p&gt; &lt;p&gt;- Storage: NVMe SSD&lt;/p&gt; &lt;p&gt;My main question is about system RAM.&lt;/p&gt; &lt;p&gt;Memory prices are going up a lot, so I'm trying to decide between 64GB and 128GB.&lt;/p&gt; &lt;p&gt;1) For local LLM + RAG workflows (vector DB, embeddings, inference), is 64GB realistically enough, or does 128GB make life much easier?&lt;/p&gt; &lt;p&gt;2) With a single RTX 4090 (24GB), what Qwen model sizes would you recommend for practical local use? (7B / 14B / 32B?)&lt;/p&gt; &lt;p&gt;3) Any real-world pain points with 64GB RAM that made you upgrade?&lt;/p&gt; &lt;p&gt;Thanks in advance ‚Äî real-world experience would be really helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TeacherIll7604"&gt; /u/TeacherIll7604 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq7wqc/for_local_llm_rag_64gb_vs_128gb_ram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq7wqc/for_local_llm_rag_64gb_vs_128gb_ram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq7wqc/for_local_llm_rag_64gb_vs_128gb_ram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T01:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppv68d</id>
    <title>[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular</title>
    <updated>2025-12-18T16:30:13+00:00</updated>
    <author>
      <name>/u/Disastrous-Work-1632</name>
      <uri>https://old.reddit.com/user/Disastrous-Work-1632</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt; &lt;img alt="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" src="https://preview.redd.it/ggovkfrtoz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d79c723d2ee425a8d0fe89be6ee4871ab9baba7b" title="[Blog from Hugging Face] Tokenization in Transformers v5: Simpler, Clearer, and More Modular" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This blog explains how tokenization works in Transformers and why v5 is a major redesign, with clearer internals, a clean class hierarchy, and a single fast backend. It‚Äôs a practical guide for anyone who wants to understand, customize, or train model-specific tokenizers instead of treating them as black boxes.&lt;/p&gt; &lt;p&gt;Link: &lt;a href="https://huggingface.co/blog/tokenizers"&gt;https://huggingface.co/blog/tokenizers&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Disastrous-Work-1632"&gt; /u/Disastrous-Work-1632 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ggovkfrtoz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppv68d/blog_from_hugging_face_tokenization_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:30:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppstef</id>
    <title>Thoughts on recent small (under 20B) models</title>
    <updated>2025-12-18T14:55:45+00:00</updated>
    <author>
      <name>/u/surubel</name>
      <uri>https://old.reddit.com/user/surubel</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Recently we're been graced with quite a few small (under 20B) models and I've tried most of them.&lt;/p&gt; &lt;p&gt;The initial benchmarks seemed a bit too good to be true, but I've tried them regardless. &lt;/p&gt; &lt;ul&gt; &lt;li&gt;RNJ-1: this one had probably the most &amp;quot;honest&amp;quot; benchmark results. About as good as QWEN3 8B, which seems fair from my limited usage. &lt;/li&gt; &lt;li&gt;GLM 4.6v Flash: even after the latest llama.cpp update and Unsloth quantization I still have mixed feelings. Can't get it to think in English, but produces decent results. Either there are still issues with llama.cpp / quantization or it's a bit benchmaxxed&lt;/li&gt; &lt;li&gt;Ministral 3 14B: solid vision capabilities, but tends to overthink a lot. Occasionally messes up tool calls. A bit unreliable.&lt;/li&gt; &lt;li&gt;Nemotron cascade 14B. Similar to Ministral 3 14B tends to overthink a lot. Although it has great coding benchmarks, I couldn't get good results out of it. GPT OSS 20B and QWEN3 8B VL seem to give better results. This was the most underwhelming for me.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Did anyone get different results from these models? Am I missing something?&lt;/p&gt; &lt;p&gt;Seems like GPT OSS 20B and QWEN3 8B VL are still the most reliable small models, at least for me.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/surubel"&gt; /u/surubel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppstef/thoughts_on_recent_small_under_20b_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T14:55:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu35l</id>
    <title>Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting.</title>
    <updated>2025-12-18T15:47:20+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt; &lt;img alt="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." src="https://b.thumbs.redditmedia.com/CTdnDnhEVXKhvcC_xn7Fo04JbVfjTe3Wx_yk_R9kVRw.jpg" title="Mistral released Mistral OCR 3: 74% overall win rate over Mistral OCR 2 on forms, scanned documents, complex tables, and handwriting." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Source: &lt;a href="https://mistral.ai/news/mistral-ocr-3"&gt;https://mistral.ai/news/mistral-ocr-3&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Mistral OCR 3 sets new benchmarks in both accuracy and efficiency, outperforming enterprise document processing solutions as well as AI-native OCR.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ppu35l"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu35l/mistral_released_mistral_ocr_3_74_overall_win/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:47:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwylg</id>
    <title>What's your favourite local coding model?</title>
    <updated>2025-12-18T17:40:04+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt; &lt;img alt="What's your favourite local coding model?" src="https://preview.redd.it/q8ipunvr008g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b4e54f5d47898a4570fb732cd3140edf2551267b" title="What's your favourite local coding model?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried (with Mistral Vibe Cli)&lt;/p&gt; &lt;ul&gt; &lt;li&gt;mistralai_Devstral-Small-2-24B-Instruct-2512-Q8_0.gguf - works but it's kind of slow for coding &lt;/li&gt; &lt;li&gt;nvidia_Nemotron-3-Nano-30B-A3B-Q8_0.gguf - text generation is fast, but the actual coding is slow and often incorrect&lt;/li&gt; &lt;li&gt;Qwen3-Coder-30B-A3B-Instruct-Q8_0.gguf - works correctly and it's fast&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What else would you recommend? &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/q8ipunvr008g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwylg/whats_your_favourite_local_coding_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:40:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq45po</id>
    <title>New AI Dungeon Model: Hearthfire 24B</title>
    <updated>2025-12-18T22:24:22+00:00</updated>
    <author>
      <name>/u/NottKolby</name>
      <uri>https://old.reddit.com/user/NottKolby</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today AI Dungeon open sourced a new narrative roleplay model!&lt;/p&gt; &lt;h1&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;Hearthfire 24B&lt;/a&gt;&lt;/h1&gt; &lt;blockquote&gt; &lt;p&gt;Hearthfire is our new Mistral Small 3.2 finetune, and it's the lo-fi hip hop beats of AI storytelling. Built for slice-of-life moments, atmospheric scenes, and narratives where the stakes are personal rather than apocalyptic. It won't rush you toward the next plot point. It's happy to linger.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NottKolby"&gt; /u/NottKolby &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq45po/new_ai_dungeon_model_hearthfire_24b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T22:24:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppu4lc</id>
    <title>Fine-tuning Qwen3 at home to respond to any prompt with a dad joke</title>
    <updated>2025-12-18T15:48:58+00:00</updated>
    <author>
      <name>/u/InvadersMustLive</name>
      <uri>https://old.reddit.com/user/InvadersMustLive</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt; &lt;img alt="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" src="https://external-preview.redd.it/aeJXUJD-EG13fwr7w155noLxr7JTSfAKwf9XG0w-u3s.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9431c18c37e750b69f2ab16532111dd97d789f41" title="Fine-tuning Qwen3 at home to respond to any prompt with a dad joke" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/InvadersMustLive"&gt; /u/InvadersMustLive &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://nixiesearch.substack.com/p/fine-tuning-qwen3-at-home-to-respond"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppu4lc/finetuning_qwen3_at_home_to_respond_to_any_prompt/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:48:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwfw3</id>
    <title>Key Highlights of Google's New Open Model, FunctionGemma</title>
    <updated>2025-12-18T17:19:51+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt; &lt;img alt="Key Highlights of Google's New Open Model, FunctionGemma" src="https://external-preview.redd.it/f3OilJIGGaBNRWiWULRSz5XOCY6YipQN2XKt886yVr0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0508cadd0c3629606d28322469362c690c52148b" title="Key Highlights of Google's New Open Model, FunctionGemma" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;[1] Function-calling specialized&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Built on the &lt;em&gt;Gemma 3 270M&lt;/em&gt; foundation and fine-tuned for function calling tasks, turning natural language into structured function calls for API/tool execution.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[2] Lightweight &amp;amp; open&lt;/strong&gt; &lt;/p&gt; &lt;ul&gt; &lt;li&gt;A compact, open-weight model (~270 M parameters) designed for efficient use on resource-constrained hardware (laptops, desktops, cloud, edge) and democratizing access to advanced function-call agents.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[3] 32K token context&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Supports up to ~32 k token context window, like other 270M Gemma models, making it suitable for moderately long prompts and complex sequences.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;[4] Fine-tuning friendly&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Intended to be further fine-tuned for specific custom actions, improving accuracy and customization for particular domains or workflows (e.g., mobile actions, custom APIs).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Model - &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model GGUF - &lt;a href="https://huggingface.co/unsloth/functiongemma-270m-it-GGUF"&gt;https://huggingface.co/unsloth/functiongemma-270m-it-GGUF&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwfw3/key_highlights_of_googles_new_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:19:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq6h6b</id>
    <title>T5 Gemma Text to Speech</title>
    <updated>2025-12-19T00:04:46+00:00</updated>
    <author>
      <name>/u/ObjectiveOctopus2</name>
      <uri>https://old.reddit.com/user/ObjectiveOctopus2</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"&gt; &lt;img alt="T5 Gemma Text to Speech" src="https://external-preview.redd.it/OoYCpcn_PwfmbZl7Fy8iEFdYUosTf1a8HGTxpebEBLY.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be91b4470a2fc8d77abd575b339b2d41dce4c231" title="T5 Gemma Text to Speech" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma-TTS-2b-2b is a multilingual Text-to-Speech (TTS) model. It utilizes an Encoder-Decoder LLM architecture, supporting English, Chinese, and Japanese. And its üî•&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ObjectiveOctopus2"&gt; /u/ObjectiveOctopus2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/Aratako/T5Gemma-TTS-2b-2b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq6h6b/t5_gemma_text_to_speech/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T00:04:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppt1xb</id>
    <title>Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction</title>
    <updated>2025-12-18T15:05:22+00:00</updated>
    <author>
      <name>/u/Difficult-Cap-7527</name>
      <uri>https://old.reddit.com/user/Difficult-Cap-7527</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt; &lt;img alt="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" src="https://preview.redd.it/go7lager9z7g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=96b4f63fa1cdd2136e6c82f35c609cc6cc1ead9c" title="Meta released Map-anything-v1: A universal transformer model for metric 3D reconstruction" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hugging face: &lt;a href="https://huggingface.co/facebook/map-anything-v1"&gt;https://huggingface.co/facebook/map-anything-v1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;It supports 12+ tasks like multi-view stereo and SfM in a single feed-forward pass&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Difficult-Cap-7527"&gt; /u/Difficult-Cap-7527 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/go7lager9z7g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppt1xb/meta_released_mapanythingv1_a_universal/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T15:05:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzof4</id>
    <title>LatitudeGames/Hearthfire-24B ¬∑ Hugging Face</title>
    <updated>2025-12-18T19:24:58+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt; &lt;img alt="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" src="https://external-preview.redd.it/A3gGg_h4D053EFPLZSslW4oGkfGx4Yyo44cLXCFOpgw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=af804b5dc2799b163e1ddae03ccbee1392cf7d39" title="LatitudeGames/Hearthfire-24B ¬∑ Hugging Face" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hearthfire is a narrative longform writing model designed to embrace the quiet moments between the chaos. While most roleplay models are trained to relentlessly drive the plot forward with high-stakes action and constant external pressure, Hearthfire is tuned to appreciate atmosphere, introspection, and the slow burn of a scene.&lt;/p&gt; &lt;p&gt;It prioritizes vibes over velocity. It is comfortable with silence. It will not force a goblin attack just because the conversation lulled.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/LatitudeGames/Hearthfire-24B"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzof4/latitudegameshearthfire24b_hugging_face/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1pqala0</id>
    <title>MBZUAI releases K2-V2 - 70B fully open model.</title>
    <updated>2025-12-19T03:20:39+00:00</updated>
    <author>
      <name>/u/LoveMind_AI</name>
      <uri>https://old.reddit.com/user/LoveMind_AI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Holy frijoles. Has anyone given this a look? Fully open like Olmo 3, but a solid 70B of performance. I‚Äôm not sure why I‚Äôm just hearing about it, but, definitely looking forward to seeing how folks receive it!&lt;/p&gt; &lt;p&gt;&lt;a href="https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/"&gt;https://mbzuai.ac.ae/news/k2v2-full-openness-finally-meets-real-performance/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;(I searched for other posts on this but didn‚Äôt see anything - let me know if I missed a thread!)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LoveMind_AI"&gt; /u/LoveMind_AI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pqala0/mbzuai_releases_k2v2_70b_fully_open_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-19T03:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppwqki</id>
    <title>FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!</title>
    <updated>2025-12-18T17:31:14+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt; &lt;img alt="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" src="https://external-preview.redd.it/MXBiZjQzZTd4ejdnMYei2aDWEA5WccTd6X2Ceg7tONZcTZmqT6GgxYYEX2jv.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c19158fb01b0628ef68c006e673dc09cd2cf081" title="FunctionGemma Physics Playground: A simulation game where you need to use natural language to solve physics puzzles... running 100% locally in your browser!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, Google released FunctionGemma, a lightweight (270M), open foundation model built for creating specialized function calling models! To test it out, I built a small game where you use natural language to solve physics simulation puzzles. It runs entirely locally in your browser on WebGPU, powered by Transformers.js.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Game: &lt;a href="https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground"&gt;https://huggingface.co/spaces/webml-community/FunctionGemma-Physics-Playground&lt;/a&gt;&lt;br /&gt; - FunctionGemma on Hugging Face: &lt;a href="https://huggingface.co/google/functiongemma-270m-it"&gt;https://huggingface.co/google/functiongemma-270m-it&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/k33t7zd7xz7g1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppwqki/functiongemma_physics_playground_a_simulation/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T17:31:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2uvi</id>
    <title>192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA</title>
    <updated>2025-12-18T21:31:29+00:00</updated>
    <author>
      <name>/u/Sero_x</name>
      <uri>https://old.reddit.com/user/Sero_x</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt; &lt;img alt="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" src="https://b.thumbs.redditmedia.com/rqYvfP2xSe7ILLKpKsQzha57H6-7i7Cnwe-N3-UA3RM.jpg" title="192GB VRAM 8x 3090s + 512GB DDR4 RAM AMA" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab"&gt;https://preview.redd.it/ft7xpejo618g1.jpg?width=1013&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=eef45da10a0cc8b74000c8d586d9f442865a39ab&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I bought and built this 3 months ago, I started with 4x 3090s and really loved the process so got another 4x 3090s&lt;/p&gt; &lt;p&gt;Now I‚Äôm convinced I need double the VRAM&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Sero_x"&gt; /u/Sero_x &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2uvi/192gb_vram_8x_3090s_512gb_ddr4_ram_ama/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:31:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2rx7</id>
    <title>Exo 1.0 is finally out</title>
    <updated>2025-12-18T21:28:19+00:00</updated>
    <author>
      <name>/u/No_Conversation9561</name>
      <uri>https://old.reddit.com/user/No_Conversation9561</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt; &lt;img alt="Exo 1.0 is finally out" src="https://preview.redd.it/zxmsw724618g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=912f00f6d6f4874ab451714c731bec0bbc5a59be" title="Exo 1.0 is finally out" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;You can download from &lt;a href="https://exolabs.net/"&gt;https://exolabs.net/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No_Conversation9561"&gt; /u/No_Conversation9561 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zxmsw724618g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2rx7/exo_10_is_finally_out/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppzhtq</id>
    <title>T5Gemma 2: The next generation of encoder-decoder models</title>
    <updated>2025-12-18T19:17:53+00:00</updated>
    <author>
      <name>/u/Dear-Success-1441</name>
      <uri>https://old.reddit.com/user/Dear-Success-1441</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt; &lt;img alt="T5Gemma 2: The next generation of encoder-decoder models" src="https://external-preview.redd.it/_rnSBYMvSInq6EN43nG_cTgBC4Jp6XTPNyUPRgnGKn0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d9dbe7f224d36b036fe98650042395413b48e5a4" title="T5Gemma 2: The next generation of encoder-decoder models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;T5Gemma 2 models, based on Gemma 3, are multilingual and multimodal, handling text and image input and generating text output, with open weights for three pretrained sizes (270M-270M, 1B-1B, and 4B-4B).&lt;/p&gt; &lt;p&gt;Key Features&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tied embeddings:&lt;/strong&gt; Embeddings are tied between the encoder and decoder. This significantly reduces the overall parameter count and allowing to pack more active capabilities into the same memory footprint.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Merged attention:&lt;/strong&gt; The decoder uses a merged attention mechanism, combining self- and cross-attention into a single, unified attention layer. This reduces model parameters and architectural complexity, improving model parallelization and benefiting inference.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multimodality:&lt;/strong&gt; T5Gemma 2 models can understand and process images alongside text. By utilizing a highly efficient vision encoder, the models can seamlessly perform visual question answering and multimodal reasoning tasks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Extended long context:&lt;/strong&gt; Leveraging Gemma 3's alternating local and global attention mechanism, T5Gemma 2 can handle context windows of up to 128K tokens.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Massively multilingual:&lt;/strong&gt; Trained on a larger, more diverse dataset, these models now support over 140 languages out of the box.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Models - &lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;https://huggingface.co/collections/google/t5gemma-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Official Blog post - &lt;a href="https://blog.google/technology/developers/t5gemma-2/"&gt;https://blog.google/technology/developers/t5gemma-2/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dear-Success-1441"&gt; /u/Dear-Success-1441 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/google/t5gemma-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppzhtq/t5gemma_2_the_next_generation_of_encoderdecoder/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T19:17:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq5k6e</id>
    <title>Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios</title>
    <updated>2025-12-18T23:23:44+00:00</updated>
    <author>
      <name>/u/Competitive_Travel16</name>
      <uri>https://old.reddit.com/user/Competitive_Travel16</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt; &lt;img alt="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" src="https://external-preview.redd.it/A_KZLQUNhCh0wGe2hwjJCJ470X6QmuVpXZdzOWccb0U.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8fdf14dca65c42b501a6a7e33b1acf44e71ac72f" title="Jake (formerly of LTT) demonstrate's Exo's RDMA-over-Thunderbolt on four Mac Studios" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Competitive_Travel16"&gt; /u/Competitive_Travel16 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=4l4UWZGxvoc"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq5k6e/jake_formerly_of_ltt_demonstrates_exos/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T23:23:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1ppun3v</id>
    <title>Google's Gemma models family</title>
    <updated>2025-12-18T16:09:10+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt; &lt;img alt="Google's Gemma models family" src="https://preview.redd.it/59w0vja4lz7g1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7dd2d66ee23d4078bf31aba81cdeecc769669af4" title="Google's Gemma models family" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/59w0vja4lz7g1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ppun3v/googles_gemma_models_family/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T16:09:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1pq2ry0</id>
    <title>Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster</title>
    <updated>2025-12-18T21:28:20+00:00</updated>
    <author>
      <name>/u/geerlingguy</name>
      <uri>https://old.reddit.com/user/geerlingguy</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt; &lt;img alt="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" src="https://preview.redd.it/32z50w1s518g1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=be2781529b5cacb7d7a84c794d37a156e1bdc798" title="Kimi K2 Thinking at 28.3 t/s on 4x Mac Studio cluster" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I was testing llama.cpp RPC vs Exo's new RDMA Tensor setting on a cluster of 4x Mac Studios (2x 512GB and 2x 256GB) that Apple loaned me until Februrary.&lt;/p&gt; &lt;p&gt;Would love to do more testing between now and returning it. A lot of the earlier testing was debugging stuff since the RDMA support was very new for the past few weeks... now that it's somewhat stable I can do more.&lt;/p&gt; &lt;p&gt;The annoying thing is there's nothing nice like llama-bench in Exo, so I can't give as direct comparisons with context sizes, prompt processing speeds, etc. (it takes a lot more fuss to do that, at least).&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/geerlingguy"&gt; /u/geerlingguy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/32z50w1s518g1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pq2ry0/kimi_k2_thinking_at_283_ts_on_4x_mac_studio/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-18T21:28:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1pp9w31</id>
    <title>AMA with the Meta researchers behind SAM 3 + SAM 3D + SAM Audio</title>
    <updated>2025-12-17T22:18:01+00:00</updated>
    <author>
      <name>/u/AIatMeta</name>
      <uri>https://old.reddit.com/user/AIatMeta</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi &lt;a href="/r/LocalLlama"&gt;r/LocalLlama&lt;/a&gt;! We‚Äôre the research team behind the newest members of the Segment Anything collection of models: SAM 3 + SAM 3D + SAM Audio.&lt;/p&gt; &lt;p&gt;We‚Äôre excited to be here to talk all things SAM (sorry, we can‚Äôt share details on other projects or future work) and have members from across our team participating:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;SAM 3 (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/segment-anything-model-3/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Nikhila Ravi&lt;/li&gt; &lt;li&gt;Pengchuan Zhang&lt;/li&gt; &lt;li&gt;Shoubhik Debnath&lt;/li&gt; &lt;li&gt;Chay Ryali&lt;/li&gt; &lt;li&gt;Yuan-Ting Hu&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM 3D (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-3d/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Weiyao Wang&lt;/li&gt; &lt;li&gt;Sasha Sax&lt;/li&gt; &lt;li&gt;Xitong Yang&lt;/li&gt; &lt;li&gt;Jinkun Cao&lt;/li&gt; &lt;li&gt;Michelle Guo&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;SAM Audio (&lt;/strong&gt;&lt;a href="https://ai.meta.com/blog/sam-audio/?utm_source=reddit&amp;amp;utm_medium=organic_social&amp;amp;utm_content=ama&amp;amp;utm_campaign=sam"&gt;&lt;strong&gt;learn more&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bowen Shi&lt;/li&gt; &lt;li&gt;Andros Tjandra&lt;/li&gt; &lt;li&gt;John Hoffman&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can try SAM Audio, SAM 3D, and SAM 3 in the Segment Anything Playground: &lt;a href="https://go.meta.me/87b53b"&gt;https://go.meta.me/87b53b&lt;/a&gt; &lt;/p&gt; &lt;p&gt;PROOF: &lt;a href="https://x.com/AIatMeta/status/2001429429898407977"&gt;https://x.com/AIatMeta/status/2001429429898407977&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;We‚Äôll be answering questions live on Thursday, Dec. 18, from 2-3pm PT. Hope to see you there.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AIatMeta"&gt; /u/AIatMeta &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1pp9w31/ama_with_the_meta_researchers_behind_sam_3_sam_3d/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-12-17T22:18:01+00:00</published>
  </entry>
</feed>
