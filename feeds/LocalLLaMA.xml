<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-02-27T07:59:10+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1rfc3ic</id>
    <title>Introducing FasterQwenTTS</title>
    <updated>2026-02-26T14:48:38+00:00</updated>
    <author>
      <name>/u/futterneid</name>
      <uri>https://old.reddit.com/user/futterneid</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I wanted to build real-time voice agents with Qwen3-TTS, but the official implementation doesn‚Äôt support streaming and runs below real time. So I focused on fixing those two things.&lt;/p&gt; &lt;p&gt;With Faster Qwen3TTS, I get first audio in &amp;lt;200 ms on an RTX 4090 and 2x‚Äì6x speedups across 4 different GPUs I tested. The Qwen TTS models had ~4M downloads in the last month and can run locally, so I‚Äôm hoping this implementation helps the localLLaMA community :)&lt;/p&gt; &lt;p&gt;Install: `pip install faster-qwen3-tts`&lt;/p&gt; &lt;p&gt;Repo: &lt;a href="https://github.com/andimarafioti/faster-qwen3-tts"&gt;https://github.com/andimarafioti/faster-qwen3-tts&lt;/a&gt;&lt;br /&gt; Demo: &lt;a href="https://huggingface.co/spaces/HuggingFaceM4/faster-qwen3-tts-demo"&gt;https://huggingface.co/spaces/HuggingFaceM4/faster-qwen3-tts-demo&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/futterneid"&gt; /u/futterneid &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfc3ic/introducing_fasterqwentts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfc3ic/introducing_fasterqwentts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfc3ic/introducing_fasterqwentts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T14:48:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf8oqm</id>
    <title>Strix Halo, GNU/Linux Debian, Qwen3.5-(27,35,122B) CTX&lt;=131k, llama.cpp@ROCm, Power &amp; Efficiency</title>
    <updated>2026-02-26T12:19:55+00:00</updated>
    <author>
      <name>/u/Educational_Sun_8813</name>
      <uri>https://old.reddit.com/user/Educational_Sun_8813</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf8oqm/strix_halo_gnulinux_debian_qwen352735122b_ctx131k/"&gt; &lt;img alt="Strix Halo, GNU/Linux Debian, Qwen3.5-(27,35,122B) CTX&amp;lt;=131k, llama.cpp@ROCm, Power &amp;amp; Efficiency" src="https://preview.redd.it/2p3i75jdytlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=95591def030396bd573ddbf5bf50f31624f3f9f2" title="Strix Halo, GNU/Linux Debian, Qwen3.5-(27,35,122B) CTX&amp;lt;=131k, llama.cpp@ROCm, Power &amp;amp; Efficiency" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi, benchmark from Strix Halo, Qwen3.5:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;27B(Q8)&lt;/li&gt; &lt;li&gt;35B-A3B(Q8)&lt;/li&gt; &lt;li&gt;122B(Q5_K_M, Q6_K)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;code&gt;GNU/Linux Debian 6.18.12&lt;/code&gt;, &lt;code&gt;llama.cpp version: 8152 (d7d826b3c)&lt;/code&gt; compiled with &lt;code&gt;TheRock nightly build ROCm-7.12.0&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;This time i tested only ROCm.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Educational_Sun_8813"&gt; /u/Educational_Sun_8813 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2p3i75jdytlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf8oqm/strix_halo_gnulinux_debian_qwen352735122b_ctx131k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf8oqm/strix_halo_gnulinux_debian_qwen352735122b_ctx131k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T12:19:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfkdjk</id>
    <title>pplx-embed: State-of-the-Art Embedding Models for Web-Scale Retrieval</title>
    <updated>2026-02-26T19:50:14+00:00</updated>
    <author>
      <name>/u/1-800-methdyke</name>
      <uri>https://old.reddit.com/user/1-800-methdyke</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfkdjk/pplxembed_stateoftheart_embedding_models_for/"&gt; &lt;img alt="pplx-embed: State-of-the-Art Embedding Models for Web-Scale Retrieval" src="https://external-preview.redd.it/G_yAWns7zWEzkW5qKmxSzTcWkvgIExuYRKjxIq4OvYw.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6b9fd228df6e48aaf25f0de45262fddcc0dbac7c" title="pplx-embed: State-of-the-Art Embedding Models for Web-Scale Retrieval" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Perplexity just dropped pplx-embed, a family of state-of-the-art text embedding models optimized for real-world, web-scale retrieval tasks‚Äîlike semantic search and RAG systems. Built on diffusion-pretrained Qwen3 backbones with multi-stage contrastive learning, they come in two flavors: pplx-embed-v1 for independent texts/queries (no instruction prefixes needed) and pplx-embed-context-v1 for context-aware document chunks, producing efficient int8-quantized embeddings best compared via cosine similarity. These models outperform giants like Google and Alibaba on benchmarks, making retrieval faster and more accurate without brittle prompt engineering.&lt;/p&gt; &lt;p&gt;The int8 and binary quantized embeddings seem like a great idea to save embeddings storage costs.&lt;/p&gt; &lt;p&gt;Find them on Hugging Face: &lt;a href="https://huggingface.co/perplexity-ai/pplx-embed-v1-0.6b"&gt;https://huggingface.co/perplexity-ai/pplx-embed-v1-0.6b&lt;/a&gt;&lt;/p&gt; &lt;p&gt;-&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/1-800-methdyke"&gt; /u/1-800-methdyke &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://research.perplexity.ai/articles/pplx-embed-state-of-the-art-embedding-models-for-web-scale-retrieval"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfkdjk/pplxembed_stateoftheart_embedding_models_for/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfkdjk/pplxembed_stateoftheart_embedding_models_for/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T19:50:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf99u2</id>
    <title>speed of GLM-4.7-Flash vs Qwen3.5-35B-A3B</title>
    <updated>2026-02-26T12:48:31+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf99u2/speed_of_glm47flash_vs_qwen3535ba3b/"&gt; &lt;img alt="speed of GLM-4.7-Flash vs Qwen3.5-35B-A3B" src="https://preview.redd.it/vumtmwvo4ulg1.png?width=140&amp;amp;height=103&amp;amp;auto=webp&amp;amp;s=20169f9c3ea2ff489a65ed3e9fbc4f87a76c8c3b" title="speed of GLM-4.7-Flash vs Qwen3.5-35B-A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Last month I posted about using OpenCode with GLM-4.7-Flash. For agentic coding, you need to focus on long context, because 50,000 tokens is pretty normal during a coding session.&lt;/p&gt; &lt;p&gt;This is the speed of the llama.cpp on 3√ó3090 (CUDA backend)&lt;/p&gt; &lt;p&gt;I‚Äôll post more detailed benchmarks with more models later in March (I‚Äôm still waiting for the new Qwens), but I wanted to show you a quick comparison. And to collect the critical feedback ;)&lt;/p&gt; &lt;p&gt;EDIT look at the additional plot in the comment (for zero context GLM wins)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rf99u2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf99u2/speed_of_glm47flash_vs_qwen3535ba3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf99u2/speed_of_glm47flash_vs_qwen3535ba3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T12:48:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1rg045u</id>
    <title>Overwhelmed by so many model releases within a month period - What would be best coding and planning models around 60-100B / Fit in Strix-Halo 128GB VRam</title>
    <updated>2026-02-27T07:24:58+00:00</updated>
    <author>
      <name>/u/Voxandr</name>
      <uri>https://old.reddit.com/user/Voxandr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am using StrixHalo with 128 GB VRam . I am using Kimi-Linear for tech documents and&lt;br /&gt; contracts + Qwen-3-Next 80b. For vibe coding i was using qwen 3 Coder 35B-A3B&lt;/p&gt; &lt;p&gt;I haven't tried Qwen 3.5s and Qwen3-coder-next&lt;/p&gt; &lt;p&gt;My questions are :&lt;/p&gt; &lt;p&gt;With Qwen 3.5 release is Qwen3-Next-Coder 80B-A3B Obselete?&lt;br /&gt; Would Qwen 3.5 dense 27B model Better for my Case vs MoE ?&lt;/p&gt; &lt;p&gt;Are there any better coder models that can fit in 100GB VRAM?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Voxandr"&gt; /u/Voxandr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg045u/overwhelmed_by_so_many_model_releases_within_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rg045u/overwhelmed_by_so_many_model_releases_within_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rg045u/overwhelmed_by_so_many_model_releases_within_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T07:24:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf6s0d</id>
    <title>Qwen3.5-27B-heretic-gguf</title>
    <updated>2026-02-26T10:33:32+00:00</updated>
    <author>
      <name>/u/Poro579</name>
      <uri>https://old.reddit.com/user/Poro579</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf6s0d/qwen3527bhereticgguf/"&gt; &lt;img alt="Qwen3.5-27B-heretic-gguf" src="https://preview.redd.it/c5jqn7q3htlg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e19f047c0bf8d24bfcbdfe6eabda769285bb7c52" title="Qwen3.5-27B-heretic-gguf" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/tree/main"&gt;https://huggingface.co/mradermacher/Qwen3.5-27B-heretic-GGUF/tree/main&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Poro579"&gt; /u/Poro579 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/c5jqn7q3htlg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf6s0d/qwen3527bhereticgguf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf6s0d/qwen3527bhereticgguf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T10:33:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfjkzu</id>
    <title>Reverse CAPTCHA: We tested whether invisible Unicode characters can hijack LLM agents: 8,308 outputs across 5 models</title>
    <updated>2026-02-26T19:20:39+00:00</updated>
    <author>
      <name>/u/thecanonicalmg</name>
      <uri>https://old.reddit.com/user/thecanonicalmg</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjkzu/reverse_captcha_we_tested_whether_invisible/"&gt; &lt;img alt="Reverse CAPTCHA: We tested whether invisible Unicode characters can hijack LLM agents: 8,308 outputs across 5 models" src="https://preview.redd.it/p119kiqx2wlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c8be5b82cd9f1ed3df8fe48e2047b9f32b7a6a3a" title="Reverse CAPTCHA: We tested whether invisible Unicode characters can hijack LLM agents: 8,308 outputs across 5 models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We tested whether LLMs follow instructions hidden in invisible Unicode characters embedded in normal-looking text. Two encoding schemes (zero-width binary and Unicode Tags), 5 models (GPT-5.2, GPT-4o-mini, Claude Opus 4, Sonnet 4, Haiku 4.5), 8,308 graded outputs.&lt;/p&gt; &lt;p&gt;Key findings:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Tool access is the primary amplifier.&lt;/strong&gt; Without tools, compliance stays below 17%. With tools and decoding hints, it reaches 98-100%. Models write Python scripts to decode the hidden characters.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Encoding vulnerability is provider-specific.&lt;/strong&gt; OpenAI models decode zero-width binary but not Unicode Tags. Anthropic models prefer Tags. Attackers must tailor encoding to the target.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The hint gradient is consistent:&lt;/strong&gt; unhinted &amp;lt;&amp;lt; codepoint hints &amp;lt; full decoding instructions. The combination of tool access + decoding instructions is the critical enabler.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;All 10 pairwise model comparisons are statistically significant&lt;/strong&gt; (Fisher's exact test, Bonferroni-corrected, p &amp;lt; 0.05). Cohen's h up to 1.37.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Would be very interesting to see how local models compare ‚Äî we only tested API models. If anyone wants to run this against Llama, Qwen, Mistral, etc. the eval framework is open source.&lt;/p&gt; &lt;p&gt;Code + data: &lt;a href="https://github.com/canonicalmg/reverse-captcha-eval"&gt;https://github.com/canonicalmg/reverse-captcha-eval&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Full writeup with charts: &lt;a href="https://moltwire.com/research/reverse-captcha-zw-steganography"&gt;https://moltwire.com/research/reverse-captcha-zw-steganography&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/thecanonicalmg"&gt; /u/thecanonicalmg &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/p119kiqx2wlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjkzu/reverse_captcha_we_tested_whether_invisible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjkzu/reverse_captcha_we_tested_whether_invisible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T19:20:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf740o</id>
    <title>DeepSeek released new paper: DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference</title>
    <updated>2026-02-26T10:53:28+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf740o/deepseek_released_new_paper_dualpath_breaking_the/"&gt; &lt;img alt="DeepSeek released new paper: DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference" src="https://preview.redd.it/25rh3yahktlg1.png?width=140&amp;amp;height=60&amp;amp;auto=webp&amp;amp;s=05ab4438b0dcc4bf7ee8a162767a2b1e94896fde" title="DeepSeek released new paper: DualPath: Breaking the Storage Bandwidth Bottleneck in Agentic LLM Inference" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://arxiv.org/abs/2602.21548"&gt;https://arxiv.org/abs/2602.21548&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/25rh3yahktlg1.png?width=536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f282d71496b6386841732137a474f1b238269950"&gt;https://preview.redd.it/25rh3yahktlg1.png?width=536&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f282d71496b6386841732137a474f1b238269950&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A joint research team from Peking University, Tsinghua University, and DeepSeek-AI has released its latest research findings on optimizing Large Language Model (LLM) inference architectures. The team successfully developed a novel inference system called **DualPath**, specifically designed to address technical bottlenecks in KV-Cache storage I/O bandwidth under agentic workloads.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/hdssmlcnktlg1.png?width=511&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ba3bc1fd5fa0f310205f8de5bb73e022a0a8263"&gt;https://preview.redd.it/hdssmlcnktlg1.png?width=511&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6ba3bc1fd5fa0f310205f8de5bb73e022a0a8263&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf740o/deepseek_released_new_paper_dualpath_breaking_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf740o/deepseek_released_new_paper_dualpath_breaking_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf740o/deepseek_released_new_paper_dualpath_breaking_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T10:53:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf2ulo</id>
    <title>Qwen3.5 122B in 72GB VRAM (3x3090) is the best model available at this time ‚Äî also it nails the ‚Äúcar wash test‚Äù</title>
    <updated>2026-02-26T06:32:25+00:00</updated>
    <author>
      <name>/u/liviuberechet</name>
      <uri>https://old.reddit.com/user/liviuberechet</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2ulo/qwen35_122b_in_72gb_vram_3x3090_is_the_best_model/"&gt; &lt;img alt="Qwen3.5 122B in 72GB VRAM (3x3090) is the best model available at this time ‚Äî also it nails the ‚Äúcar wash test‚Äù" src="https://preview.redd.it/f624mg43aslg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4294c910c299aa0b5b65f5e5c0177aa28a215e65" title="Qwen3.5 122B in 72GB VRAM (3x3090) is the best model available at this time ‚Äî also it nails the ‚Äúcar wash test‚Äù" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am absolutely loving Qwen3.5 122B!&lt;/p&gt; &lt;p&gt;It‚Äôs the best model I can run on my 72GB VRAM setup, fully loaded on GPU including context.&lt;/p&gt; &lt;p&gt;Very good speed at 25 tok/s.&lt;/p&gt; &lt;p&gt;Fiddled a bit with the settings to get it to work properly. If you are experiencing endless ‚Äúbut wait‚Äù loops, this is what worked for me:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Thinking mode on &lt;/li&gt; &lt;li&gt;Temperature 0.6 &lt;/li&gt; &lt;li&gt;K Sampling 20 &lt;/li&gt; &lt;li&gt;Top P sampling 0.8 &lt;/li&gt; &lt;li&gt;Min P sampling 0 &lt;/li&gt; &lt;li&gt;Repeat penalty 1.3&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Running it in Q3_K it‚Äôs a bit slower than GLM Air (30 t/s in IQ4_NL) and GPT-OSS-120B (30-38 t/s in MXFP4), but because it has a smaller footprint in Q3 I am able to push the context to 120k which is great!&lt;/p&gt; &lt;p&gt;I tried both MXFP4 and IQ4_XS, but they are too close to 70GB when loaded, forcing me to offload 2-3 layers to RAM or context in RAM ‚Äî dropping to only 6-8 tok/s.&lt;/p&gt; &lt;p&gt;Saw on unsloth website that Q3_K_XL might actually perform on par with the 4bit ones, and I can confirm so far it‚Äôs been amazing!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/liviuberechet"&gt; /u/liviuberechet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/f624mg43aslg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2ulo/qwen35_122b_in_72gb_vram_3x3090_is_the_best_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf2ulo/qwen35_122b_in_72gb_vram_3x3090_is_the_best_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T06:32:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfid0q</id>
    <title>LFM2-24B-A2B is crazy fast on Strix Halo</title>
    <updated>2026-02-26T18:36:29+00:00</updated>
    <author>
      <name>/u/jfowers_amd</name>
      <uri>https://old.reddit.com/user/jfowers_amd</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfid0q/lfm224ba2b_is_crazy_fast_on_strix_halo/"&gt; &lt;img alt="LFM2-24B-A2B is crazy fast on Strix Halo" src="https://external-preview.redd.it/dWNtOGxscWh1dmxnMQ2ajgZe7-xnsTznsg_JwClc9VRl8_wH2AeJmBTcUqOP.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4944c3939437719c81f2bed2da35e03e16321573" title="LFM2-24B-A2B is crazy fast on Strix Halo" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've never seen a 24B model fly like this. It's almost 2x faster than gpt-oss-20b! Ran it with ROCm using Lemonade v9.4.0.&lt;/p&gt; &lt;p&gt;Really hope to see some cool uses for this model! Anyone tried it out for their tasks yet?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jfowers_amd"&gt; /u/jfowers_amd &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/ug0nkgqhuvlg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfid0q/lfm224ba2b_is_crazy_fast_on_strix_halo/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfid0q/lfm224ba2b_is_crazy_fast_on_strix_halo/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T18:36:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfej6k</id>
    <title>Qwen 3.5 Family Comparison by ArtificialAnalysis.ai</title>
    <updated>2026-02-26T16:19:58+00:00</updated>
    <author>
      <name>/u/NewtMurky</name>
      <uri>https://old.reddit.com/user/NewtMurky</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfej6k/qwen_35_family_comparison_by_artificialanalysisai/"&gt; &lt;img alt="Qwen 3.5 Family Comparison by ArtificialAnalysis.ai" src="https://preview.redd.it/ehvltper8vlg1.png?width=140&amp;amp;height=61&amp;amp;auto=webp&amp;amp;s=a11d6535284afc808bc449fe1eb89f2b6b6cb0ec" title="Qwen 3.5 Family Comparison by ArtificialAnalysis.ai" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/ehvltper8vlg1.png?width=2444&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b66a53ef786326ec84fa3569def246a5e356d2f2"&gt;Intelligence Index&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/g9ulfnl49vlg1.png?width=2448&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d8c61e7ed7dd123d3bd73474ab8aa56a5389a637"&gt;Coding Index&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/9448a9t59vlg1.png?width=2452&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f3a8063e29632dd2878c0c80a96ea81b5bd3c739"&gt;Agentic Index&lt;/a&gt;&lt;/p&gt; &lt;p&gt;That‚Äôs interesting - &lt;a href="http://artificialanalysis.ai"&gt;artificialanalysis.ai&lt;/a&gt; ranks Qwen3.5-27B higher than Qwen3.5-122B-A10B and Qwen3.5-35B-A3B across all benchmark categories: Intelligence Index, Coding Index, and Agentic Index.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NewtMurky"&gt; /u/NewtMurky &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfej6k/qwen_35_family_comparison_by_artificialanalysisai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfej6k/qwen_35_family_comparison_by_artificialanalysisai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfej6k/qwen_35_family_comparison_by_artificialanalysisai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T16:19:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfxtfz</id>
    <title>Eagerly waiting for Qwen 3.5 1.7B</title>
    <updated>2026-02-27T05:16:43+00:00</updated>
    <author>
      <name>/u/Hot_Inspection_9528</name>
      <uri>https://old.reddit.com/user/Hot_Inspection_9528</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Qwen 3 1.7B with 0.1111 temperature is really good. I like it. &lt;/p&gt; &lt;p&gt;I am very much waiting for Qwen 3.5 1.7B model. &lt;/p&gt; &lt;p&gt;I am actually very excited. &lt;/p&gt; &lt;p&gt;Any ideas when it might release? &lt;/p&gt; &lt;p&gt;If you work with SLM like 1.7Bs, I think this will be Qween of local small language models. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Hot_Inspection_9528"&gt; /u/Hot_Inspection_9528 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxtfz/eagerly_waiting_for_qwen_35_17b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxtfz/eagerly_waiting_for_qwen_35_17b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxtfz/eagerly_waiting_for_qwen_35_17b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T05:16:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfi53f</id>
    <title>Completed my 64GB VRAM rig - dual MI50 build + custom shroud</title>
    <updated>2026-02-26T18:28:28+00:00</updated>
    <author>
      <name>/u/roackim</name>
      <uri>https://old.reddit.com/user/roackim</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfi53f/completed_my_64gb_vram_rig_dual_mi50_build_custom/"&gt; &lt;img alt="Completed my 64GB VRAM rig - dual MI50 build + custom shroud" src="https://preview.redd.it/xzawg4yftvlg1.jpg?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=d1664ab5992dd710ff460793015ac3dbc20a5fe6" title="Completed my 64GB VRAM rig - dual MI50 build + custom shroud" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello everyone!&lt;/p&gt; &lt;p&gt;A few months ago I started a project to build my own local AI server. After some testing and buying the second GPU, I was able to finalize the setup.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Motherboard:&lt;/strong&gt; Gigabyte X399 DESIGNARE&lt;/li&gt; &lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Threadripper 2990WX (32 Cores / 64 Threads)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB DDR4&lt;/li&gt; &lt;li&gt;&lt;strong&gt;GPUs:&lt;/strong&gt; 2x AMD Instinct MI50 32GB&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Costs:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Motherboard + CPU + RAM + PSU: ~690‚Ç¨&lt;/li&gt; &lt;li&gt;GPUs: about 330‚Ç¨ each&lt;/li&gt; &lt;li&gt;Case: ~150‚Ç¨&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Total:&lt;/strong&gt; ~1500‚Ç¨&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Software:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Ubuntu 24.04 LTS&lt;/li&gt; &lt;li&gt;ROCm 6.3&lt;/li&gt; &lt;li&gt;llama.cpp&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;It runs &lt;strong&gt;GLM 4.7 flash Q8_0 at ~50 t/s&lt;/strong&gt; (but it drops down fast). I need to tinker a bit more with the setup to test things out.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Custom GPU shroud&lt;/strong&gt;&lt;br /&gt; One of the major constraints was that the machine needs to not be super loud, as it sits under my desk. For that I designed and 3D printed a custom shroud to ensure proper cooling while keeping it (somewhat) silent.&lt;/p&gt; &lt;p&gt;The shroud is open source and licensed under MIT! It's a modular build, easily printable on small 3D printers, 3 parts assembled with M2 and M3 screws. For cooling it uses a single 92mm fan (Arctic P9 Max), works pretty nicely!&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/roackim/mi50-92mm-shroud"&gt;https://github.com/roackim/mi50-92mm-shroud&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;STLs:&lt;/strong&gt; &lt;a href="https://github.com/roackim"&gt;https://github.com/roackim/mi50-92mm-shroud/releases/tag/1.0.0&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Details:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The cards stay around 18W idle and use about 155W on load.&lt;/li&gt; &lt;li&gt;Note: Since my motherboard doesn't expose FAN header controls, I set the speed to ~2700rpm. It‚Äôs not that loud, but it‚Äôs a fixed speed, bummer.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Overall happy with the build. It was super fun designing and building the custom shroud for the GPU!&lt;/p&gt; &lt;p&gt;If you guys have any tips to share regarding llama.cpp, dual GPUs, or AMD MI50s I would be grateful&lt;/p&gt; &lt;p&gt;Thanks üêî&lt;/p&gt; &lt;p&gt;edit: formatting (not familiar with posting on reddit)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/roackim"&gt; /u/roackim &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rfi53f"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfi53f/completed_my_64gb_vram_rig_dual_mi50_build_custom/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfi53f/completed_my_64gb_vram_rig_dual_mi50_build_custom/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T18:28:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1rftlmm</id>
    <title>Vellium v0.4 ‚Äî alternative simplified UI, updated writing mode and multi-char improvements</title>
    <updated>2026-02-27T01:55:58+00:00</updated>
    <author>
      <name>/u/Possible_Statement84</name>
      <uri>https://old.reddit.com/user/Possible_Statement84</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rftlmm/vellium_v04_alternative_simplified_ui_updated/"&gt; &lt;img alt="Vellium v0.4 ‚Äî alternative simplified UI, updated writing mode and multi-char improvements" src="https://preview.redd.it/yxjnb1gk1ylg1.png?width=140&amp;amp;height=83&amp;amp;auto=webp&amp;amp;s=9bd97c81bb059daf1a2d60163200d1f36cdcbad4" title="Vellium v0.4 ‚Äî alternative simplified UI, updated writing mode and multi-char improvements" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Vellium is an open-source desktop app for local LLMs built around creative writing and roleplay. The idea is visual control over your story ‚Äî sliders for mood, pacing, intensity instead of manually editing system prompts. Works with Ollama, KoboldCpp, LM Studio, OpenAI, OpenRouter, or any compatible endpoint.&lt;/p&gt; &lt;p&gt;This update focuses on accessibility and the writing experience.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Simple Mode&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;New alternative UI that strips everything down to a clean chat interface. No sidebars, no inspector panel, no RP presets on screen. Model picker inline, quick action buttons (Write, Learn, Code, Life stuff). Enabled by default on the welcome screen for new users. All advanced features are one click away when you need them.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Writing mode updates:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Generate Next Chapter: continue your story without crafting a prompt each time&lt;br /&gt; Consistency checker, Summarize Book, Expand, Rewrite tools in the toolbar&lt;br /&gt; Chapter dynamics with per-chapter tone/pacing controls&lt;br /&gt; Outline view for project structure&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Multi-character improvements&lt;/strong&gt;:&lt;/p&gt; &lt;p&gt;Updated multi-char mode for smoother group conversations ‚Äî better turn management and character switching.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Other:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Zen mode for distraction-free writing&lt;br /&gt; Motion animations on chat messages and sidebar transitions&lt;br /&gt; Reworked layouts across both chat and writing views&lt;/p&gt; &lt;p&gt;Electron + React + TypeScript, MIT license&lt;/p&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/tg-prplx/vellium"&gt;https://github.com/tg-prplx/vellium&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Possible_Statement84"&gt; /u/Possible_Statement84 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rftlmm"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rftlmm/vellium_v04_alternative_simplified_ui_updated/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rftlmm/vellium_v04_alternative_simplified_ui_updated/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T01:55:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfddpi</id>
    <title>Training a 144M Spiking Neural Network for text generation from scratch ‚Äî no transformer teacher, no distillation</title>
    <updated>2026-02-26T15:37:35+00:00</updated>
    <author>
      <name>/u/zemondza</name>
      <uri>https://old.reddit.com/user/zemondza</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfddpi/training_a_144m_spiking_neural_network_for_text/"&gt; &lt;img alt="Training a 144M Spiking Neural Network for text generation from scratch ‚Äî no transformer teacher, no distillation" src="https://preview.redd.it/j5jf87hczulg1.jpg?width=140&amp;amp;height=48&amp;amp;auto=webp&amp;amp;s=d3a4720746dc81c1e7288e98843eb81c16d41d45" title="Training a 144M Spiking Neural Network for text generation from scratch ‚Äî no transformer teacher, no distillation" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I built a 144M parameter SNN language model with a fully original architecture (not based on RWKV, transformers, or any existing SNN). Trained from scratch on FineWeb-Edu for ~$10 on a rented A5000.&lt;/p&gt; &lt;p&gt;Some interesting findings:&lt;/p&gt; &lt;p&gt;‚Ä¢ 97-98% inference sparsity ‚Äî only 2-3% of neurons fire per token. This emerges naturally during training without any sparsity loss.&lt;/p&gt; &lt;p&gt;‚Ä¢ Topic coherence advantage ‚Äî when comparing with GPT-2 Small (124M) on the same prompts, Nord stays on-topic while GPT-2 drifts. On &amp;quot;How does encryption protect data?&amp;quot;, Nord used relevant terms (encryption, decrypt, public key, authentication, attack) while GPT-2 talked about browsers, cookies, and &amp;quot;cybernetics.&amp;quot; This may be related to sparse activation acting as a relevance filter.&lt;/p&gt; &lt;p&gt;‚Ä¢ Visible &amp;quot;thinking&amp;quot; ‚Äî spike rate analysis shows Block 4 is the most active (9.8%) while Block 0 filters noise (0.6%). You can literally see where the model processes information. This interpretability comes free with SNN architecture.&lt;/p&gt; &lt;p&gt;‚Ä¢ Online learning via STDP ‚Äî the model updates weights during conversation using Spike-Timing Dependent Plasticity, a biological learning rule.&lt;/p&gt; &lt;p&gt;‚Ä¢ The architecture combines: LeakyClamp (gradient flow through spikes), Associative Cascade (prevents dead neurons), Multi-scale temporal encoding, Temporal Co-firing Resonance, and Reward-modulated STDP.&lt;/p&gt; &lt;p&gt;To my knowledge, only SpikeGPT (260M, RWKV-based) has been trained from scratch as an SNN language model. Nord is the second, with a fully original architecture.&lt;/p&gt; &lt;p&gt;Limitations: Loss is still 4.5 (training on 40GB now, targeting 3.8-4.0). Text quality is below GPT-2 in fluency. The GPT-2 comparison is on limited prompts, not a systematic benchmark.&lt;/p&gt; &lt;p&gt;Code: &lt;a href="https://github.com/gtausa197-svg/-Project-Nord-Spiking-Neural-Network-Language-Model"&gt;https://github.com/gtausa197-svg/-Project-Nord-Spiking-Neural-Network-Language-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/zerdovzad/Nord-AI"&gt;https://huggingface.co/zerdovzad/Nord-AI&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback on the architecture choices, especially from anyone working with SNNs or neuromorphic computing. What would you want to see in a more systematic evaluation?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/zemondza"&gt; /u/zemondza &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1rfddpi"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfddpi/training_a_144m_spiking_neural_network_for_text/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfddpi/training_a_144m_spiking_neural_network_for_text/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T15:37:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rf7m85</id>
    <title>DeepSeek allows Huawei early access to V4 update, but Nvidia and AMD still don‚Äôt have access to V4</title>
    <updated>2026-02-26T11:22:27+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://www.reuters.com/world/china/deepseek-withholds-latest-ai-model-us-chipmakers-including-nvidia-sources-say-2026-02-25/"&gt;https://www.reuters.com/world/china/deepseek-withholds-latest-ai-model-us-chipmakers-including-nvidia-sources-say-2026-02-25/&lt;/a&gt;&lt;/p&gt; &lt;p&gt;According to a Reuters report today, DeepSeek has recently granted early access to its major V4 update to domestic suppliers such as Huawei. This move is intended to help these companies optimize their processor software and ensure the model runs efficiently on their hardware. However, chipmakers like Nvidia and AMD have not yet been granted access.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf7m85/deepseek_allows_huawei_early_access_to_v4_update/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rf7m85/deepseek_allows_huawei_early_access_to_v4_update/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rf7m85/deepseek_allows_huawei_early_access_to_v4_update/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T11:22:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfzfgf</id>
    <title>Minimax M2.5 GGUF perform poorly overall</title>
    <updated>2026-02-27T06:44:37+00:00</updated>
    <author>
      <name>/u/Zyj</name>
      <uri>https://old.reddit.com/user/Zyj</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;em&gt;As posted by Benjamin Marie (not me) at&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://xcancel.com/bnjmn%5C_marie/status/2027043753484021810"&gt;https://xcancel.com/bnjmn\_marie/status/2027043753484021810&lt;/a&gt; :&lt;/p&gt; &lt;p&gt;Minimax M2.5 GGUFs (from Q4 down to Q1) perform poorly overall. None of them come close to the original model.&lt;/p&gt; &lt;p&gt;That‚Äôs very different from my Qwen3.5 GGUF evaluations, where even TQ1_0 held up well enough.&lt;/p&gt; &lt;p&gt;Lessons:&lt;/p&gt; &lt;p&gt;- Models aren‚Äôt equally robust, even under otherwise very good quantization algorithms.&lt;/p&gt; &lt;p&gt;-‚ÄúJust take Q4, it‚Äôll be fine‚Äù is a rule of thumb that doesn‚Äôt generalize.&lt;/p&gt; &lt;p&gt;(Here he posted a chart)&lt;/p&gt; &lt;p&gt;&lt;em&gt;And continues in another post:&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Getting these results was painfully slow: between 10 and 20 hours for each model, using an H200. And since the models are not good, they tend to generate gibberish until reaching the maximum sequence length.&lt;/p&gt; &lt;p&gt;Took me over a week in total.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zyj"&gt; /u/Zyj &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfzfgf/minimax_m25_gguf_perform_poorly_overall/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfzfgf/minimax_m25_gguf_perform_poorly_overall/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfzfgf/minimax_m25_gguf_perform_poorly_overall/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T06:44:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfjp6v</id>
    <title>top 10 trending models on HF</title>
    <updated>2026-02-26T19:24:56+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjp6v/top_10_trending_models_on_hf/"&gt; &lt;img alt="top 10 trending models on HF" src="https://preview.redd.it/5rqv8z2s3wlg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=35fefab1ea2ac5a5020c86a7254058c09178c18e" title="top 10 trending models on HF" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;any conclusions? ;)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/5rqv8z2s3wlg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjp6v/top_10_trending_models_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfjp6v/top_10_trending_models_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T19:24:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfds1h</id>
    <title>Qwen3.5-35B-A3B Q4 Quantization Comparison</title>
    <updated>2026-02-26T15:52:23+00:00</updated>
    <author>
      <name>/u/TitwitMuffbiscuit</name>
      <uri>https://old.reddit.com/user/TitwitMuffbiscuit</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"&gt; &lt;img alt="Qwen3.5-35B-A3B Q4 Quantization Comparison" src="https://preview.redd.it/0u0z9evbawlg1.png?width=140&amp;amp;height=83&amp;amp;auto=webp&amp;amp;s=a345f51538cb357a05082a937420dad32d1a329b" title="Qwen3.5-35B-A3B Q4 Quantization Comparison" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This is a Q4 quantization sweep across all major community quants of Qwen3.5-35B-A3B, comparing faithfulness to the BF16 baseline across different quantizers and recipes.&lt;/p&gt; &lt;p&gt;The goal is to give people a data-driven basis for picking a file rather than just grabbing whatever is available.&lt;/p&gt; &lt;p&gt;For the uninitiated:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;KLD (KL Divergence):&lt;/strong&gt; &amp;quot;Faithfulness.&amp;quot; It shows how much the quantized model's probability distribution drifts from a baseline (the probability distribution of the original weights). Lower = closer.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;PPL (Perplexity):&lt;/strong&gt; Used to measure the average uncertainty of the model when predicting the next token. It is derived from the total information loss (Cross Entropy). Lower = more confident.&lt;/p&gt; &lt;p&gt;They are correlated. Perplexity measures the total error, KLD measures the relative error (like a routing drift of an MoE model). This relationship helps in determining information loss (or gain when training). Since we are trying to see how much information we've lost and since PPL is noisy as it can get a better score by pure luck, KLD is better as it is not relying on the dataset but on the baseline.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;If you need the most faithfull quant, pick the one with the lowest KLD.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;AesSedai's Q4_K_M achieves KLD 0.0102 by consistently protecting always-active tensors (attention, shared experts) at Q8_0 and differentiating &lt;code&gt;ffn_down_exps&lt;/code&gt; from &lt;code&gt;ffn_gate/up_exps&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Ubergarm's Q4_0 outperforms every other Q4_0 by a factor of 2.5 by a large margin for the same reason.&lt;/p&gt; &lt;p&gt;MXFP4 is likely well-suited for QAT (Quantization Aware Training), where the model is trained to operate within MXFP4 numerical ranges. Applied post-hoc to a BF16 model, it consistently underperforms standard quants at equivalent size on this architecture. Unsloth's UD-Q4_K_XL recipe applies MXFP4 to nearly every tensor including &lt;code&gt;ffn_down_exps&lt;/code&gt; and attention weights, resulting in the worst KLD in the sweep (0.0524) despite not being the largest file. Unsloth is aware of this and working on it: &lt;a href="https://huggingface.co/unsloth/Qwen3.5-35B-A3B-GGUF/discussions/5"&gt;unsloth/Qwen3.5-35B-A3B-GGUF/discussions/5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If you are on the fence between files, use:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;llama-perplexity -m &amp;lt;bf16_model&amp;gt; -f wiki.test.raw --kl-divergence-base &amp;lt;file_name&amp;gt; [other parameters] llama-perplexity -m &amp;lt;quantized_model&amp;gt; --kl-divergence-base &amp;lt;file_name&amp;gt; --kl-divergence [other parameters] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/0u0z9evbawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d07bfd5a37e9c5fa9ae99648d202c7d4f7781ea5"&gt;https://preview.redd.it/0u0z9evbawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d07bfd5a37e9c5fa9ae99648d202c7d4f7781ea5&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/tpfh92qcawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a4122d61e6df11cb832583de314385d2533c8bc"&gt;https://preview.redd.it/tpfh92qcawlg1.png?width=2979&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a4122d61e6df11cb832583de314385d2533c8bc&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Most Efficient Quantization&lt;/h1&gt; &lt;p&gt;The Efficiency Score is the distance to a 'perfect' model (zero size, zero KLD), not the &amp;quot;best&amp;quot; model but the VRAM sweet spot. Efficiency Score: ‚àö (Normalized Size¬≤ + Normalized KLD¬≤) ‚Äî lower is better.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Rank&lt;/th&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;th align="left"&gt;Eff. Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;1&lt;/td&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;16.3999770582&lt;/td&gt; &lt;td align="left"&gt;0.024036&lt;/td&gt; &lt;td align="left"&gt;0.327342&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;2&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;17.4178144932&lt;/td&gt; &lt;td align="left"&gt;0.024273&lt;/td&gt; &lt;td align="left"&gt;0.411178&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;3&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.4062407017&lt;/td&gt; &lt;td align="left"&gt;0.023761&lt;/td&gt; &lt;td align="left"&gt;0.573661&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;4&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;18.4312270582&lt;/td&gt; &lt;td align="left"&gt;0.025288&lt;/td&gt; &lt;td align="left"&gt;0.599390&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;5&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.4010530412&lt;/td&gt; &lt;td align="left"&gt;0.027117&lt;/td&gt; &lt;td align="left"&gt;0.620673&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;6&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;19.0378324986&lt;/td&gt; &lt;td align="left"&gt;0.021415&lt;/td&gt; &lt;td align="left"&gt;0.679213&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;7&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.4779573381&lt;/td&gt; &lt;td align="left"&gt;0.035176&lt;/td&gt; &lt;td align="left"&gt;0.769475&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;8&lt;/td&gt; &lt;td align="left"&gt;ubergarm_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;19.7865126431&lt;/td&gt; &lt;td align="left"&gt;0.015125&lt;/td&gt; &lt;td align="left"&gt;0.811116&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;9&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.7692930698&lt;/td&gt; &lt;td align="left"&gt;0.018878&lt;/td&gt; &lt;td align="left"&gt;0.824589&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;10&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.7150785923&lt;/td&gt; &lt;td align="left"&gt;0.037042&lt;/td&gt; &lt;td align="left"&gt;0.839537&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;11&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.7489992082&lt;/td&gt; &lt;td align="left"&gt;0.023362&lt;/td&gt; &lt;td align="left"&gt;0.852727&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;12&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_L&lt;/td&gt; &lt;td align="left"&gt;20.1208174229&lt;/td&gt; &lt;td align="left"&gt;0.018232&lt;/td&gt; &lt;td align="left"&gt;0.902187&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;13&lt;/td&gt; &lt;td align="left"&gt;lmstudio_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.7050000000&lt;/td&gt; &lt;td align="left"&gt;0.032892&lt;/td&gt; &lt;td align="left"&gt;0.949834&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;14&lt;/td&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.3849241734&lt;/td&gt; &lt;td align="left"&gt;0.022821&lt;/td&gt; &lt;td align="left"&gt;0.990643&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;15&lt;/td&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;20.6187270582&lt;/td&gt; &lt;td align="left"&gt;0.010214&lt;/td&gt; &lt;td align="left"&gt;1.000000&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;16&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.3642488420&lt;/td&gt; &lt;td align="left"&gt;0.026266&lt;/td&gt; &lt;td align="left"&gt;1.013664&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;17&lt;/td&gt; &lt;td align="left"&gt;noctrex_Qwen3.5-35B-A3B-MXFP4_MOE_BF16&lt;/td&gt; &lt;td align="left"&gt;20.5495284498&lt;/td&gt; &lt;td align="left"&gt;0.024921&lt;/td&gt; &lt;td align="left"&gt;1.043445&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;18&lt;/td&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;18.3351655900&lt;/td&gt; &lt;td align="left"&gt;0.052439&lt;/td&gt; &lt;td align="left"&gt;1.100189&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Note: The Efficiency Score uses AesSedai Q4_K_M as the reference point (score = 1.0) as the ceiling. Files scoring below 1.0 offer a better size/quality tradeoff and vice versa.&lt;/p&gt; &lt;h1&gt;Data (sorted by KLD)&lt;/h1&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Quantization&lt;/th&gt; &lt;th align="left"&gt;Size (GiB)&lt;/th&gt; &lt;th align="left"&gt;PPL Score&lt;/th&gt; &lt;th align="left"&gt;KLD Score&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;20.62&lt;/td&gt; &lt;td align="left"&gt;6.436887&lt;/td&gt; &lt;td align="left"&gt;0.010214&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;ubergarm_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;19.79&lt;/td&gt; &lt;td align="left"&gt;6.461745&lt;/td&gt; &lt;td align="left"&gt;0.015125&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_L&lt;/td&gt; &lt;td align="left"&gt;20.12&lt;/td&gt; &lt;td align="left"&gt;6.499422&lt;/td&gt; &lt;td align="left"&gt;0.018232&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.77&lt;/td&gt; &lt;td align="left"&gt;6.491274&lt;/td&gt; &lt;td align="left"&gt;0.018878&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_K_S&lt;/td&gt; &lt;td align="left"&gt;19.04&lt;/td&gt; &lt;td align="left"&gt;6.512668&lt;/td&gt; &lt;td align="left"&gt;0.021415&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.39&lt;/td&gt; &lt;td align="left"&gt;6.473700&lt;/td&gt; &lt;td align="left"&gt;0.022821&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.75&lt;/td&gt; &lt;td align="left"&gt;6.518045&lt;/td&gt; &lt;td align="left"&gt;0.023362&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.41&lt;/td&gt; &lt;td align="left"&gt;6.506714&lt;/td&gt; &lt;td align="left"&gt;0.023761&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;AesSedai_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;16.40&lt;/td&gt; &lt;td align="left"&gt;6.517477&lt;/td&gt; &lt;td align="left"&gt;0.024036&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-IQ4_XS&lt;/td&gt; &lt;td align="left"&gt;17.42&lt;/td&gt; &lt;td align="left"&gt;6.511643&lt;/td&gt; &lt;td align="left"&gt;0.024273&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;noctrex_Qwen3.5-35B-A3B-MXFP4_MOE_BF16&lt;/td&gt; &lt;td align="left"&gt;20.55&lt;/td&gt; &lt;td align="left"&gt;6.487453&lt;/td&gt; &lt;td align="left"&gt;0.024921&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-MXFP4_MOE&lt;/td&gt; &lt;td align="left"&gt;18.43&lt;/td&gt; &lt;td align="left"&gt;6.485211&lt;/td&gt; &lt;td align="left"&gt;0.025288&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_1&lt;/td&gt; &lt;td align="left"&gt;20.36&lt;/td&gt; &lt;td align="left"&gt;6.530645&lt;/td&gt; &lt;td align="left"&gt;0.026266&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-IQ4_NL&lt;/td&gt; &lt;td align="left"&gt;18.40&lt;/td&gt; &lt;td align="left"&gt;6.523618&lt;/td&gt; &lt;td align="left"&gt;0.027117&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;lmstudio_Qwen3.5-35B-A3B-Q4_K_M&lt;/td&gt; &lt;td align="left"&gt;19.705&lt;/td&gt; &lt;td align="left"&gt;6.543927&lt;/td&gt; &lt;td align="left"&gt;0.032892&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.48&lt;/td&gt; &lt;td align="left"&gt;6.574551&lt;/td&gt; &lt;td align="left"&gt;0.035176&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;bartowski_Qwen3.5-35B-A3B-Q4_0&lt;/td&gt; &lt;td align="left"&gt;18.72&lt;/td&gt; &lt;td align="left"&gt;6.501674&lt;/td&gt; &lt;td align="left"&gt;0.037042&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;unsloth_Qwen3.5-35B-A3B-UD-Q4_K_XL&lt;/td&gt; &lt;td align="left"&gt;18.34&lt;/td&gt; &lt;td align="left"&gt;6.636498&lt;/td&gt; &lt;td align="left"&gt;0.052439&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;h1&gt;Setup&lt;/h1&gt; &lt;p&gt;CPU: Intel Core i3-12100F RAM: 64 GB DDR4 3200, dual channel. GPU: RTX 3060 12 GB (GPU clock fixed at 1882 MHz via curve, VRAM at 8210 MHz, stable). OS: Windows 11, Nvidia drivers 591.74&lt;/p&gt; &lt;p&gt;ik_llama.cpp: Thireus/ik_llama.cpp ‚Äî build main-b4299-15482f0, Windows x64 CUDA 13.1 AVX2. Mainline llama.cpp compatibility: tested against b8157 (2943210c1), Windows x64 CUDA 13.1. All quants work both on llama.cpp and ik_llama.cpp.&lt;/p&gt; &lt;h1&gt;Details&lt;/h1&gt; &lt;p&gt;PPL and KLD are calculated with &lt;code&gt;wikitext2_test.txt&lt;/code&gt; at a context of 512 tokens with &lt;code&gt;-ncmoe 22&lt;/code&gt; and &lt;code&gt;-ngl 999&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;KLD base logits generated from the BF16 model (full CPU offload, no &lt;code&gt;-ncmoe&lt;/code&gt;).&lt;/p&gt; &lt;h1&gt;Notes&lt;/h1&gt; &lt;p&gt;Results reflect faithfulness to the BF16 baseline on a general text corpus (wikitext2). Task-specific performance (reasoning, code, instruction following) may order things differently, particularly at the extremes.&lt;/p&gt; &lt;p&gt;The MXFP4 findings here are specific to post-training quantization. MXFP4 applied during QAT (as in GPT-OSS-120B) is a different and more principled use of the format.&lt;/p&gt; &lt;p&gt;Plots use a linear scale. A logarithmic scale would better represent the distribution of KLD values across the full quantization range, but linear scaling makes the differences within the Q4 range immediately readable without requiring familiarity with log representations.&lt;/p&gt; &lt;p&gt;If unsloth_Qwen3.5-35B-A3B-UD-Q4_K_XL gets fixed, I'll evaluate and update this post with a clear mention of the before and after.&lt;/p&gt; &lt;p&gt;I won't be able to test more quants, it's kind of sunny outside.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TitwitMuffbiscuit"&gt; /u/TitwitMuffbiscuit &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfds1h/qwen3535ba3b_q4_quantization_comparison/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T15:52:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfmzfp</id>
    <title>New Upcoming Ubuntu 26.04 LTS Will be Optimized for Local AI</title>
    <updated>2026-02-26T21:26:44+00:00</updated>
    <author>
      <name>/u/mtomas7</name>
      <uri>https://old.reddit.com/user/mtomas7</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Some interesting new developments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Out-of-the-box NVIDIA CUDA and AMD ROCm drivers that are auto-selected for your particular hardware &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=316"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=316&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Inference Snaps - ready-to-use sandboxed AI inference containers (reminds a bit the Mozilla llamafile project): &lt;ul&gt; &lt;li&gt;Feature presentation: &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=412"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=412&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=1183"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=1183&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Sandboxing AI Agents: &lt;a href="https://youtu.be/0CYm-KCw7yY&amp;amp;t=714"&gt;https://youtu.be/0CYm-KCw7yY&amp;amp;t=714&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mtomas7"&gt; /u/mtomas7 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfmzfp/new_upcoming_ubuntu_2604_lts_will_be_optimized/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfmzfp/new_upcoming_ubuntu_2604_lts_will_be_optimized/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfmzfp/new_upcoming_ubuntu_2604_lts_will_be_optimized/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T21:26:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfg3kx</id>
    <title>American closed models vs Chinese open models is becoming a problem.</title>
    <updated>2026-02-26T17:15:48+00:00</updated>
    <author>
      <name>/u/__JockY__</name>
      <uri>https://old.reddit.com/user/__JockY__</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The work I do involves customers that are sensitive to nation state politics. We cannot and do not use cloud API services for AI because the data must not leak. Ever. As a result we use open models in closed environments.&lt;/p&gt; &lt;p&gt;The problem is that my customers don‚Äôt want Chinese models. ‚ÄúNational security risk‚Äù.&lt;/p&gt; &lt;p&gt;But the only recent semi-capable model we have from the US is gpt-oss-120b, which is far behind modern LLMs like GLM, MiniMax, etc.&lt;/p&gt; &lt;p&gt;So we are in a bind: use an older, less capable model and slowly fall further and further behind the curve, or‚Ä¶ what?&lt;/p&gt; &lt;p&gt;I suspect this is why Hegseth is pressuring Anthropic: the DoD needs offline AI for awful purposes and wants Anthropic to give it to them.&lt;/p&gt; &lt;p&gt;But what do we do? Tell the customers we‚Äôre switching to Chinese models because the American models are locked away behind paywalls, logging, and training data repositories? Lobby for OpenAI to do us another favor and release another open weights model? We certainly cannot just secretly use Chinese models, but the American ones are soon going to be irrelevant. We‚Äôre in a bind.&lt;/p&gt; &lt;p&gt;&lt;del&gt;Our one glimmer of hope is StepFun-AI out of South Korea. Maybe they‚Äôll save Americans from themselves.&lt;/del&gt; I stand corrected: they‚Äôre in Shanghai.&lt;/p&gt; &lt;p&gt;Cohere are in Canada and may be a solid option. Or maybe someone can just torrent Opus once the Pentagon force Anthropic to hand it over‚Ä¶&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/__JockY__"&gt; /u/__JockY__ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfg3kx/american_closed_models_vs_chinese_open_models_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T17:15:48+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfp6bk</id>
    <title>why is openclaw even this popular?</title>
    <updated>2026-02-26T22:50:15+00:00</updated>
    <author>
      <name>/u/Crazyscientist1024</name>
      <uri>https://old.reddit.com/user/Crazyscientist1024</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;recently i haven't been following up on the latest AI dramas and just came back from a vacation. Did some looking around and found out that OpenClaw just blew up, looked into it but I didn't find anything significantly special. It just seems to be like a wrapper that has a huge amounts of pre-programmed function calls / skills / whatever built into it.&lt;/p&gt; &lt;p&gt;Am I missing something? How is this blowing up? Respectfully, even for newbie programmers, they can probably simply vibe code a way more lightweight tool themselves in a day dedicated for their task at hand.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Crazyscientist1024"&gt; /u/Crazyscientist1024 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfp6bk/why_is_openclaw_even_this_popular/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-26T22:50:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1rfxi64</id>
    <title>Is microsoft going to train LLM on this? Github is clearly getting destroyed.</title>
    <updated>2026-02-27T05:01:02+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxi64/is_microsoft_going_to_train_llm_on_this_github_is/"&gt; &lt;img alt="Is microsoft going to train LLM on this? Github is clearly getting destroyed." src="https://preview.redd.it/4imno3ccyylg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b3bb9a4af440a47621c736774190ddcde050ab65" title="Is microsoft going to train LLM on this? Github is clearly getting destroyed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;...&lt;/p&gt; &lt;h2&gt;Everyday 1000s of crappy nonfunctioning wild-imagination vibecoded junk is being posted with thousands of robo-generated stars and hundreds of forks. If Microsoft is planning to use that for future LLMs code training we are in for a nasty shock! Feedback loop is a bitch. You thought em dashes were bad. Hahaha.&lt;/h2&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4imno3ccyylg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxi64/is_microsoft_going_to_train_llm_on_this_github_is/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rfxi64/is_microsoft_going_to_train_llm_on_this_github_is/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-27T05:01:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
