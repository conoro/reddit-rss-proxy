<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-03-01T19:50:25+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1ri60l3</id>
    <title>Qwen 3.5 35B A3B LMStudio Settings</title>
    <updated>2026-03-01T19:10:34+00:00</updated>
    <author>
      <name>/u/n8mo</name>
      <uri>https://old.reddit.com/user/n8mo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi All,&lt;/p&gt; &lt;p&gt;I'm struggling to hit the same tok/s performance I've seen from other users. I've got a 16 GB 5070ti, 9800x3D, and 64GB of DDR5, but top out at around 27-28 tok/s. I'm seeing others with similar hardware report as high as 50tok/s.&lt;/p&gt; &lt;p&gt;Any ideas what I might be doing wrong?&lt;/p&gt; &lt;p&gt;Context Length: ~32k&lt;/p&gt; &lt;p&gt;GPU Offload: 26 layers&lt;/p&gt; &lt;p&gt;CPU Thread Pool Size: 6&lt;/p&gt; &lt;p&gt;Evaluation Batch Size: 512&lt;/p&gt; &lt;p&gt;Max Concurrent: 4&lt;/p&gt; &lt;p&gt;Unified KV Cache: true&lt;/p&gt; &lt;p&gt;Offload KV Cache to GPU Memory: true&lt;/p&gt; &lt;p&gt;Keep Model in Memory: true&lt;/p&gt; &lt;p&gt;Try mmap(): true&lt;/p&gt; &lt;p&gt;Number of Experts: 4&lt;/p&gt; &lt;p&gt;Flash Attention: true&lt;/p&gt; &lt;p&gt;K Cache Quantization Type: Q8_0&lt;/p&gt; &lt;p&gt;V Cache Quantization Type: Q8_0&lt;/p&gt; &lt;p&gt;EDIT to add: I'm running the Q4_K_M quant.&lt;/p&gt; &lt;p&gt;&lt;a href="https://i.imgur.com/a78D23F.png"&gt;Screenshot of LMStudio settings&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/n8mo"&gt; /u/n8mo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri60l3/qwen_35_35b_a3b_lmstudio_settings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri60l3/qwen_35_35b_a3b_lmstudio_settings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri60l3/qwen_35_35b_a3b_lmstudio_settings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T19:10:34+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri6e3q</id>
    <title>RewardHackWatch v1.3 - local Llama judge, eval workbench, no GPU needed</title>
    <updated>2026-03-01T19:24:18+00:00</updated>
    <author>
      <name>/u/aerosta_ai</name>
      <uri>https://old.reddit.com/user/aerosta_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri6e3q/rewardhackwatch_v13_local_llama_judge_eval/"&gt; &lt;img alt="RewardHackWatch v1.3 - local Llama judge, eval workbench, no GPU needed" src="https://preview.redd.it/6ymino0eihmg1.png?width=140&amp;amp;height=78&amp;amp;auto=webp&amp;amp;s=770ce38f6e2452f56392d1eb3d9ccb300b13342e" title="RewardHackWatch v1.3 - local Llama judge, eval workbench, no GPU needed" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Just shipped a bigger local-first update to RewardHackWatch.&lt;/p&gt; &lt;p&gt;It‚Äôs an open-source tool for detecting reward hacking in LLM agent trajectories, things like:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;sys.exit(0) to fake passing tests&lt;/li&gt; &lt;li&gt;rewriting test or scoring code&lt;/li&gt; &lt;li&gt;copying reference solutions&lt;/li&gt; &lt;li&gt;validator patching&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;What‚Äôs new in v1.3:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;local Llama judge via Ollama, the full pipeline can now run offline&lt;/li&gt; &lt;li&gt;local React dashboard&lt;/li&gt; &lt;li&gt;batch eval workbench for JSONL trajectories&lt;/li&gt; &lt;li&gt;no GPU needed for the base DistilBERT detector&lt;/li&gt; &lt;li&gt;mock exploit detection improved from 0% to 98.5%&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The classifier runs in ~50ms on CPU and gets 89.7% F1 on 5,391 MALT trajectories.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;trained on MALT specifically&lt;/li&gt; &lt;li&gt;threshold needs calibration per deployment&lt;/li&gt; &lt;li&gt;RMGI is still an experimental metric&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;GitHub: &lt;a href="https://github.com/aerosta/rewardhackwatch"&gt;https://github.com/aerosta/rewardhackwatch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Project page: &lt;a href="https://aerosta.github.io/rewardhackwatch"&gt;https://aerosta.github.io/rewardhackwatch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Model: &lt;a href="https://huggingface.co/aerosta/rewardhackwatch"&gt;https://huggingface.co/aerosta/rewardhackwatch&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Would love feedback from people running local eval, red-team, or Ollama-based agent pipelines.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/aerosta_ai"&gt; /u/aerosta_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1ri6e3q"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri6e3q/rewardhackwatch_v13_local_llama_judge_eval/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri6e3q/rewardhackwatch_v13_local_llama_judge_eval/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T19:24:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh43za</id>
    <title>Qwen 3.5-35B-A3B is beyond expectations. It's replaced GPT-OSS-120B as my daily driver and it's 1/3 the size.</title>
    <updated>2026-02-28T14:32:21+00:00</updated>
    <author>
      <name>/u/valdev</name>
      <uri>https://old.reddit.com/user/valdev</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know everyone has their own subjective take on what models are the best, at which types of tasks, at which sizes, at which quants, at which context lengths and so on and so forth.&lt;/p&gt; &lt;p&gt;But Qwen 3.5-35B-A3B has completely shocked me.&lt;/p&gt; &lt;p&gt;My use-case is pretty broad, but generally focuses around development tasks.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I have an N8N server setup that aggregates all of my messages, emails, alerts and aggregates them into priority based batches via the LLM.&lt;/li&gt; &lt;li&gt;I have multiple systems I've created which dynamically generate other systems based on internal tooling I've created based on user requests.&lt;/li&gt; &lt;li&gt;Timed task systems which utilize custom MCP's I've created, think things like &amp;quot;Get me the current mortgage rate in the USA&amp;quot;, then having it run once a day and giving it access to a custom browser MCP. (Only reason custom is important here is because it's self documenting, this isn't published anywhere for it to be part of the training).&lt;/li&gt; &lt;li&gt;Multiple different systems that require vision and interpretation of said visual understanding.&lt;/li&gt; &lt;li&gt;I run it on opencode as well to analyze large code bases&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This model, is... Amazing. It yaps a lot in thinking, but is amazing. I don't know what kind of black magic the Qwen team pumped into this model, but it worked.&lt;/p&gt; &lt;p&gt;It's not the smartest model in the world, it doesn't have all the knowledge crammed into it's data set... But it's very often smart enough to know when it doesn't know something, and when you give it the ability to use a browser it will find the data it needs to fill in the gaps.&lt;/p&gt; &lt;p&gt;Anyone else having a similar experience? (I'm using unsloths Q4-K-XL, running on a 5090 and 3090 @ 100k context)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/valdev"&gt; /u/valdev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh43za/qwen_3535ba3b_is_beyond_expectations_its_replaced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T14:32:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhuvyc</id>
    <title>Benchmarking 88 smol GGUF models quickly on a cheap Mac Mini (16 GB) to find fitting local LLM</title>
    <updated>2026-03-01T11:19:37+00:00</updated>
    <author>
      <name>/u/Honest-Debate-6863</name>
      <uri>https://old.reddit.com/user/Honest-Debate-6863</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhuvyc/benchmarking_88_smol_gguf_models_quickly_on_a/"&gt; &lt;img alt="Benchmarking 88 smol GGUF models quickly on a cheap Mac Mini (16 GB) to find fitting local LLM" src="https://preview.redd.it/edj3sz1gcfmg1.png?width=140&amp;amp;height=85&amp;amp;auto=webp&amp;amp;s=f3dec0ac471b6a356ce9d8b1d946783e5b6bcbbb" title="Benchmarking 88 smol GGUF models quickly on a cheap Mac Mini (16 GB) to find fitting local LLM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;An automated pipeline that downloads, benchmarks (throughput + latency + quality), uploads, and deletes GGUF models in waves on a single Mac Mini M4 with 16 GB unified memory (or any other Mac)&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/edj3sz1gcfmg1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57869898475267ae64700607972b94b9ada77bd9"&gt;https://preview.redd.it/edj3sz1gcfmg1.png?width=878&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=57869898475267ae64700607972b94b9ada77bd9&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/f94r210hcfmg1.png?width=1302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=843b86e95acb4f152cf608c68919337a5add6759"&gt;https://preview.redd.it/f94r210hcfmg1.png?width=1302&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=843b86e95acb4f152cf608c68919337a5add6759&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rcv1eavhcfmg1.png?width=1340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca49ecf313d338e7670fdecc3c6566b860527c1c"&gt;https://preview.redd.it/rcv1eavhcfmg1.png?width=1340&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ca49ecf313d338e7670fdecc3c6566b860527c1c&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/rqvsd1nicfmg1.png?width=1244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e4f9fb4c854c85aea3febf9344a00429da76519"&gt;https://preview.redd.it/rqvsd1nicfmg1.png?width=1244&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1e4f9fb4c854c85aea3febf9344a00429da76519&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key takeaways:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;9 out of 88 models are unusable&lt;/strong&gt; on 16 GB ‚Äî anything where weights + KV cache exceed ~14 GB causes memory thrashing (TTFT &amp;gt; 10s or &amp;lt; 0.1 tok/s). This includes all dense 27B+ models.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Only 4 models sit on the Pareto frontier&lt;/strong&gt; of throughput vs quality, and they're all the same architecture: &lt;strong&gt;LFM2-8B-A1B&lt;/strong&gt; (LiquidAI's MoE with 1B active params). The MoE design means only ~1B params are active per token, so it gets 12-20 tok/s where dense 8B models top out at 5-7.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Context scaling from 1k to 4k is flat&lt;/strong&gt; ‚Äî most models show zero throughput degradation. Some LFM2 variants actually speed up at 4k.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Concurrency scaling is poor&lt;/strong&gt; (0.57x at concurrency 2 vs ideal 2.0x) ‚Äî the Mac Mini is memory-bandwidth limited, so run one request at a time.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Pareto frontier (no other model beats these on both speed AND quality):&lt;/strong&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;TPS (avg)&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;Quality&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;R-GSM8K&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;R-MMLU&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;NR-GSM8K&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;&lt;strong&gt;NR-MMLU&lt;/strong&gt;&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q5_K_M (unsloth)&lt;/td&gt; &lt;td align="left"&gt;14.24&lt;/td&gt; &lt;td align="left"&gt;44.6&lt;/td&gt; &lt;td align="left"&gt;50%&lt;/td&gt; &lt;td align="left"&gt;48%&lt;/td&gt; &lt;td align="left"&gt;40%&lt;/td&gt; &lt;td align="left"&gt;40%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q8_0 (unsloth)&lt;/td&gt; &lt;td align="left"&gt;12.37&lt;/td&gt; &lt;td align="left"&gt;46.2&lt;/td&gt; &lt;td align="left"&gt;65%&lt;/td&gt; &lt;td align="left"&gt;47%&lt;/td&gt; &lt;td align="left"&gt;25%&lt;/td&gt; &lt;td align="left"&gt;48%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-UD-Q8_K_XL (unsloth)&lt;/td&gt; &lt;td align="left"&gt;12.18&lt;/td&gt; &lt;td align="left"&gt;47.9&lt;/td&gt; &lt;td align="left"&gt;55%&lt;/td&gt; &lt;td align="left"&gt;47%&lt;/td&gt; &lt;td align="left"&gt;40%&lt;/td&gt; &lt;td align="left"&gt;50%&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;LFM2-8B-A1B-Q8_0 (LiquidAI)&lt;/td&gt; &lt;td align="left"&gt;12.18&lt;/td&gt; &lt;td align="left"&gt;51.2&lt;/td&gt; &lt;td align="left"&gt;70%&lt;/td&gt; &lt;td align="left"&gt;50%&lt;/td&gt; &lt;td align="left"&gt;30%&lt;/td&gt; &lt;td align="left"&gt;55%&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;&lt;strong&gt;My picks:&lt;/strong&gt; LFM2-8B-A1B-Q8_0 if you want best quality, Q5_K_M if you want speed, UD-Q6_K_XL for balance.&lt;/p&gt; &lt;p&gt;The full pipeline (download, benchmark, quality eval, upload, cleanup) is automated and open source. CSV with all 88 models and the scripts are in the repo.&lt;/p&gt; &lt;p&gt;Ôøº‚ÄãÔøº‚ÄãÔøº&lt;strong&gt;Hardware&lt;/strong&gt;: Mac Mini M4, 16 GB unified memory, macOS 15.x, llama-server (llama.cpp)&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Methodology notes&lt;/strong&gt;: Quality eval uses compact subsets (20 GSM8K + 60 MMLU) directionally useful for ranking but not publication-grade absolute numbers. Throughput numbers are p50 over multiple requests. All data is reproducible from the artifacts in the repo.&lt;/p&gt; &lt;p&gt;Code, complete table and metric stats: &lt;a href="https://huggingface.co/Manojb/macmini-16gb-bench-gguf/blob/main/SUMMARY.md"&gt; https://huggingface.co/Manojb/macmini-16gb-bench-gguf/blob/main/SUMMARY.md &lt;/a&gt; &lt;/p&gt; &lt;p&gt;Plot Artifact:&lt;/p&gt; &lt;p&gt;&lt;a href="https://claude.ai/public/artifacts/a89b7288-578a-4dd1-8a63-96791bbf8a8d"&gt; https://claude.ai/public/artifacts/a89b7288-578a-4dd1-8a63-96791bbf8a8d &lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;What's next&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Higher-context KV cache testing&lt;/strong&gt; (8k, 16k, 32k) on the top 3 models to find the actual memory cliff&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More benching&lt;/strong&gt; Tool-calling, CUA, Deep research, VLM etc task benchmarking&lt;/li&gt; &lt;li&gt;&lt;strong&gt;More model families&lt;/strong&gt; - suggestions welcome&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Honest-Debate-6863"&gt; /u/Honest-Debate-6863 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhuvyc/benchmarking_88_smol_gguf_models_quickly_on_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhuvyc/benchmarking_88_smol_gguf_models_quickly_on_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhuvyc/benchmarking_88_smol_gguf_models_quickly_on_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T11:19:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh9u4r</id>
    <title>This sub is incredible</title>
    <updated>2026-02-28T18:20:55+00:00</updated>
    <author>
      <name>/u/cmdr-William-Riker</name>
      <uri>https://old.reddit.com/user/cmdr-William-Riker</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I feel like everything in the AI industry is spedrunning profit driven vendor lock in and rapid enshitification, then everyone on this sub cobbles together a bunch of RTX3090s, trade weights around like they are books at a book club and make the entire industry look like a joke. Keep at it! you are our only hope!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cmdr-William-Riker"&gt; /u/cmdr-William-Riker &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh9u4r/this_sub_is_incredible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T18:20:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri3y89</id>
    <title>My last &amp; only beef with Qwen3.5 35B A3B</title>
    <updated>2026-03-01T17:55:36+00:00</updated>
    <author>
      <name>/u/ndiphilone</name>
      <uri>https://old.reddit.com/user/ndiphilone</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri3y89/my_last_only_beef_with_qwen35_35b_a3b/"&gt; &lt;img alt="My last &amp;amp; only beef with Qwen3.5 35B A3B" src="https://preview.redd.it/cem5cggq1hmg1.png?width=140&amp;amp;height=140&amp;amp;crop=1:1,smart&amp;amp;auto=webp&amp;amp;s=a7223eb77464714b58a11c72f02bcbc20ec9a473" title="My last &amp;amp; only beef with Qwen3.5 35B A3B" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/cem5cggq1hmg1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5645a69e048c997a013fd66f5372a08b253aca87"&gt;https://preview.redd.it/cem5cggq1hmg1.png?width=680&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5645a69e048c997a013fd66f5372a08b253aca87&lt;/a&gt;&lt;/p&gt; &lt;p&gt;How will I work around this? &lt;/p&gt; &lt;p&gt;I can intercept &amp;amp; `@` the file so whole content is available to the model when it happens on top level obviously, but in sub-agents I don't have much choice.&lt;/p&gt; &lt;p&gt;Otherwise, this is a great model and the first one for the last couple years that I can run on my hardware &amp;amp; get shit done.&lt;/p&gt; &lt;p&gt;Obviously someone is going to ask my hardware &amp;amp; my parameters:&lt;/p&gt; &lt;p&gt;- RTX 4070 TI SUPER 16GB&lt;br /&gt; - 64 GB system memory&lt;br /&gt; - 7800X3D&lt;/p&gt; &lt;p&gt;This is the `llama.server` command I'm running the inference with:&lt;br /&gt; &lt;code&gt;llama-server -hf unsloth/Qwen3.5-35B-A3B-GGUF:UD-Q4_K_XL --alias qwen3.5-35b-a3b --host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;--fit on --port 8080 --ctx-size 131072 -fa on -b 4096 -ub 4096 --temp 0.6 --top-p 0.95 --top-k 20 --min-p 0.0 -np 1 --fit-target 1024 --no-mmap --mlock --swa-full&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Before you ask these are the `t/s`:&lt;/p&gt; &lt;p&gt;&lt;code&gt;prompt eval time = 2069.88 ms / 3384 tokens ( 0.61 ms per token, 1634.88 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;eval time = 34253.04 ms / 1687 tokens ( 20.30 ms per token, 49.25 tokens per second)&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;total time = 36322.91 ms / 5071 tokens&lt;/code&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ndiphilone"&gt; /u/ndiphilone &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri3y89/my_last_only_beef_with_qwen35_35b_a3b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri3y89/my_last_only_beef_with_qwen35_35b_a3b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri3y89/my_last_only_beef_with_qwen35_35b_a3b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T17:55:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhjmfr</id>
    <title>Nobody in the family uses the family AI platform I build - really bummed about it</title>
    <updated>2026-03-01T01:05:21+00:00</updated>
    <author>
      <name>/u/ubrtnk</name>
      <uri>https://old.reddit.com/user/ubrtnk</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"&gt; &lt;img alt="Nobody in the family uses the family AI platform I build - really bummed about it" src="https://preview.redd.it/3a1e1rfx0cmg1.png?width=140&amp;amp;height=21&amp;amp;auto=webp&amp;amp;s=e7b9b15b0167f1280f1757e2e67699674d7bace9" title="Nobody in the family uses the family AI platform I build - really bummed about it" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So I started my local AI journey last year after going to Red Hat's conference in May - met the vLLM guys and was completely enthralled. Right around that same time, Amazon announced that they were going to use Alexa recordings for training and that didn't sit right with me. &lt;/p&gt; &lt;p&gt;So I started the process of learning as much as I could, engaging in the community, building, acquiring, growing etc. Strived to have a local equivalent that can answer questions like Alexa, control music, control the smart home and, if something happened to me, help the family figure out how to control everything until they can downgrade to whatever my local ISP will give them - I don't expect them to maintain everything. &lt;/p&gt; &lt;p&gt;Started with dual purposing hardware from my music studio (M2 Max 64GB MBP and M3 Ultra studio) and now as of this post I have 2x 3090s, 2x4090s, 1x 4080s, 1x5060Ti, running on a 24/48c EPYC with 256GB plus a bunch of auxiliary support stuff. I have TTS/STT, Memory functions, RAG, Home Assistant piped in for actual smart and pretty fast Voice Assistant etc. It works. It can talk to the Unifi stuff, it talks to Bookstack for home documentation, it searches the internet automatically...it works. &lt;/p&gt; &lt;p&gt;So, in an attempt to figure out what the family really wanted feature wise, I sent out some questions and a quick survey to see how they were using things, as I have a few different options for consumption - voice, OWUI (public and private facing) etc. and I didnt want to just speculate &lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/3a1e1rfx0cmg1.png?width=261&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72111d87860154863159fc292650f1c055595f83"&gt;https://preview.redd.it/3a1e1rfx0cmg1.png?width=261&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=72111d87860154863159fc292650f1c055595f83&lt;/a&gt;&lt;/p&gt; &lt;p&gt;My wife's response... &lt;/p&gt; &lt;p&gt;Nobody uses it. I pour over posts and Medium articles and threads about how to make things faster, more efficient and available for the family and tried to find new options, new features, new cool things. Looked at the logs on OWUI - Wife logged in 1 time since Christmas, Son once in the last 17 days, daughter never. My wife's response to the text. That hurt, and I know it wasn't intentional but it still hurt. I've been keeping things stable and available and fast and...yea. &lt;/p&gt; &lt;p&gt;So now I'm rethinking my entire strategy and pulling it back really to just a hobby for myself and not focusing on the family's need. It doesnt seem like they really care if their stuff stays local or not. So why stress over it.&lt;/p&gt; &lt;p&gt;Technically I could still keep things localist with MUCH less gear - STT/TTS and the GPT-OSS:20B in a 48GB Mac mini would be more than enough - I could see all the gear and just run with that and maybe then take the rest and get an M5 Max MacBook for myself or something. &lt;/p&gt; &lt;p&gt;I just wanted to share my recent story. To my family, it's a hobby. So maybe I need to also look at it that way and let it compete with the rest of the hobbies and eventually fade&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ubrtnk"&gt; /u/ubrtnk &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhjmfr/nobody_in_the_family_uses_the_family_ai_platform/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T01:05:21+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhqeob</id>
    <title>Qwen 3.5 27B is the best Chinese translation model under 70B</title>
    <updated>2026-03-01T06:50:30+00:00</updated>
    <author>
      <name>/u/AndreVallestero</name>
      <uri>https://old.reddit.com/user/AndreVallestero</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ever since Llama 3.0, I've been using local models to translate Chinese subs to English. Since December 2024, I've been using a mix of Llama 3.3 70B 2 bit and Gemma 3 27B 4 bit for translations, and although the translations aren't perfect, they're decent enough to be usable.&lt;/p&gt; &lt;p&gt;I've tested many other models in this size range but none of them are as consistent, or as natural sounding as my existing setup. From my testing, MoE tends to perform poorly in translations, and thinking only models tend to also struggle, so it makes sense that there haven't been any improvements in this space for the past year when MoE and thinking have been all the rage.&lt;/p&gt; &lt;p&gt;Like all of you, for the past 4 days I've been testing Qwen 3.5, and I can confidently say that Qwen 3.5 27B is by far the best Chinese translation model under (and including) 70B. For the first time, my local setup (24GB VRAM) has been able to produce translations with tone and consistency on par with GPT 5 fast, and Gemini 3 fast. Really impressed with the Qwen team.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AndreVallestero"&gt; /u/AndreVallestero &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhqeob/qwen_35_27b_is_the_best_chinese_translation_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhqeob/qwen_35_27b_is_the_best_chinese_translation_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhqeob/qwen_35_27b_is_the_best_chinese_translation_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T06:50:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri48pj</id>
    <title>Qwen3.5-122B-A10B-GGUF-Q4_K_XL-Pipes-Screensaver One-shot.</title>
    <updated>2026-03-01T18:06:01+00:00</updated>
    <author>
      <name>/u/jacobpederson</name>
      <uri>https://old.reddit.com/user/jacobpederson</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Set out this morning to find out what all the hype is about on &amp;quot;Qwen3.5-35B-A3B-GGUF.&amp;quot; Tried every which way to get it to one-shot the following prompt and got nowhere. Right before giving up, I gave Qwen3.5-122B-A10B-GGUF-Q4_K_XL a try and it mostly nailed in on the first try. So if you have 70GB of room and are ok with 9 tok/sec :D &lt;a href="https://rowanunderwood.github.io/Qwen3.5-122B-A10B-GGUF-Q4_K_XL-Pipes-Screensaver/"&gt;https://rowanunderwood.github.io/Qwen3.5-122B-A10B-GGUF-Q4_K_XL-Pipes-Screensaver/&lt;/a&gt;&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;Write a classic windows style &amp;quot;pipes&amp;quot; screensaver as a website using Three.js.&lt;br /&gt; Include functionality for the different colored pipes generating in real time, but slowly like it would on a classic PC.&lt;br /&gt; Make speed of generation a configurable parameter. Also include both manual and automatic camera rotation and make sure the pipes reset when the screen gets too full.&lt;br /&gt; Ensure that the playfield for the pipes is large enough to fill the entire browser window.&lt;br /&gt; The pipes should generate and follow a randomized path with 90 degree turns, each joint should be a sphere (with a small chance to be a teapot instead).&lt;br /&gt; Also, pipes should not be-able to cross a space that is already full and should stop generating if they reach a dead end.&lt;br /&gt; Lighting should be full-bright with a nice specular highlight. The background should be black.&lt;br /&gt; You MUST follow the mathematical instructions below exactly. DO NOT abstract the movement math into helper functions like getNextPosition or canMoveInDirection.&lt;br /&gt; Put the logic directly inside a single step() method.&lt;/p&gt; &lt;p&gt;Strict CDN Requirements&lt;br /&gt; Use exactly these script tags:&lt;/p&gt; &lt;p&gt;&amp;lt;script src=&amp;quot;https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;script src=&amp;quot;https://unpkg.com/three@0.128.0/examples/js/controls/OrbitControls.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &lt;/p&gt; &lt;p&gt;&amp;lt;script src=&amp;quot;https://unpkg.com/three@0.128.0/examples/js/geometries/TeapotGeometry.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;The UI &amp;amp; Loop&lt;br /&gt; Create a UI div with a range slider for generation speed (10ms to 300ms).&lt;br /&gt; In requestAnimationFrame, use a timestamp check to run the pipe logic based on the slider delay.&lt;br /&gt; CRITICAL: When the timer fires, use a forEach loop to call .step() on ALL active pipes simultaneously.&lt;br /&gt; Do not just pick one random pipe.&lt;br /&gt; Keep exactly 5 active growing pipes.&lt;br /&gt; If a pipe dies (becomes inactive), DO NOT remove its meshes from the scene. Leave it visible.&lt;br /&gt; Simply remove it from your active update list and spawn a new active pipe to replace it.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Exact Pipe Drawing Math (DO NOT DEVIATE)&lt;br /&gt; Inside your Pipe class, create a step() method.&lt;br /&gt; Every time step() is called, execute this exact logic:&lt;br /&gt; - segmentLength must be 6.&lt;br /&gt; - Create an array of directions to test (shuffle standard X, Y, Z vectors).&lt;br /&gt; - For each direction, calculate: let testPos = this.currentPos.clone().add(dir.clone().multiplyScalar(6)); You MUST use .multiplyScalar(6).&lt;br /&gt; - Stringify testPos and check if it exists in your occupiedPositions Set or is out of bounds.&lt;br /&gt; - If you find a valid testPos, that becomes your nextPos. Set this.direction = dir.&lt;br /&gt; - If no valid directions exist, mark the pipe inactive (this.active = false) and return.&lt;br /&gt; - Once you have a valid nextPos, find the midpoint: let midPoint = this.currentPos.clone().add(nextPos).multiplyScalar(0.5);&lt;br /&gt; - Draw a CylinderGeometry at midPoint.&lt;br /&gt; - Rotate it using: quaternion.setFromUnitVectors(new THREE.Vector3(0, 1, 0), this.direction).&lt;br /&gt; - Draw a SphereGeometry (the joint) at nextPos.&lt;br /&gt; - CRITICAL COLLISION FIX: Claim the space by adding BOTH the stringified nextPos AND the stringified midPoint to your occupiedPositions Set.&lt;br /&gt; - Update position: this.currentPos.copy(nextPos).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;The Teapot Easter Egg&lt;br /&gt; When drawing the joint at nextPos, introduce a .1% chance to use new THREE.TeapotGeometry(radius * 2.5, 10) instead of a sphere.&lt;br /&gt; If it is a teapot, align its spout using quaternion.setFromUnitVectors(new THREE.Vector3(1, 0, 0), this.direction).&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Scene Management&lt;br /&gt; Do NOT check for scene wipes inside the Pipe class.&lt;br /&gt; In your main animate() loop, AFTER all pipes have stepped, check if totalMeshCount exceeds 4000.&lt;br /&gt; If it does, wipe the scene completely, clear the occupiedPositions Set, and spawn 5 brand new pipes.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;/blockquote&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacobpederson"&gt; /u/jacobpederson &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri48pj/qwen35122ba10bggufq4_k_xlpipesscreensaver_oneshot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri48pj/qwen35122ba10bggufq4_k_xlpipesscreensaver_oneshot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri48pj/qwen35122ba10bggufq4_k_xlpipesscreensaver_oneshot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T18:06:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1rh2lew</id>
    <title>OpenAI pivot investors love</title>
    <updated>2026-02-28T13:25:38+00:00</updated>
    <author>
      <name>/u/PaceImaginary8610</name>
      <uri>https://old.reddit.com/user/PaceImaginary8610</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt; &lt;img alt="OpenAI pivot investors love" src="https://preview.redd.it/wfho2ytml8mg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c93cb65e111030d26cc8300d3d750ce3552a15a9" title="OpenAI pivot investors love" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/PaceImaginary8610"&gt; /u/PaceImaginary8610 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/wfho2ytml8mg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rh2lew/openai_pivot_investors_love/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T13:25:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhohqk</id>
    <title>How to switch Qwen 3.5 thinking on/off without reloading the model</title>
    <updated>2026-03-01T05:04:12+00:00</updated>
    <author>
      <name>/u/No-Statement-0001</name>
      <uri>https://old.reddit.com/user/No-Statement-0001</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The Unsloth guide for Qwen 3.5 provides four recommendations for using the model in instruct or thinking mode for general and coding use. I wanted to share that it is possible to switch between the different use cases without having to reload the model every time. &lt;/p&gt; &lt;p&gt;Using the new &lt;code&gt;setParamsByID&lt;/code&gt; filter in llama-swap: &lt;/p&gt; &lt;p&gt;```yaml&lt;/p&gt; &lt;h1&gt;show aliases in v1/models&lt;/h1&gt; &lt;p&gt;includeAliasesInList: true&lt;/p&gt; &lt;p&gt;models: &amp;quot;Q3.5-35B&amp;quot;: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-6f0,GPU-f10&amp;quot; filters: stripParams: &amp;quot;temperature, top_k, top_p, repeat_penalty, min_p, presence_penalty&amp;quot;&lt;/p&gt; &lt;pre&gt;&lt;code&gt; # new filter setParamsByID: &amp;quot;${MODEL_ID}:thinking-coding&amp;quot;: temperature: 0.6 presence_penalty: 0.0 &amp;quot;${MODEL_ID}:instruct&amp;quot;: chat_template_kwargs: enable_thinking: false temperature: 0.7 top_p: 0.8 cmd: | ${server-latest} --model /path/to/models/Qwen3.5-35B-A3B-UD-Q6_K_XL.gguf --ctx-size 262144 --fit off --temp 1.0 --min-p 0.0 --top-k 20 --top-p 0.95 --repeat_penalty 1.0 --presence_penalty 1.5 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;```&lt;/p&gt; &lt;p&gt;I'm running the above config over 2x3090s with full context getting about 1400 tok/sec for prompt processing and 70 tok/sec generation.&lt;/p&gt; &lt;p&gt;setParamsByID will create a new alias for each set of parameters. When a request for one of the aliases comes in, it will inject new values for chat_template_kwargs, temperature and top_p into the request before sending it to llama-server. &lt;/p&gt; &lt;p&gt;Using the &lt;code&gt;${MODEL_ID}&lt;/code&gt; macro will create aliases named &lt;code&gt;Q3.5-35B:instruct&lt;/code&gt; and &lt;code&gt;Q3.5-35B:thinking-coding&lt;/code&gt;. You don't have to use a macro. You can pick anything for the aliases as long as they're globally unique. &lt;/p&gt; &lt;p&gt;setParamsByID works for any model as it just sets or replaces JSON params in the request before sending it upstream. Here's my gpt-oss-120B config for controlling low, medium and high reasoning efforts: &lt;/p&gt; &lt;p&gt;&lt;code&gt; models: gptoss-120B: env: - &amp;quot;CUDA_VISIBLE_DEVICES=GPU-f10,GPU-6f,GPU-eb1&amp;quot; name: &amp;quot;GPT-OSS 120B&amp;quot; filters: stripParams: &amp;quot;${default_strip_params}&amp;quot; setParamsByID: &amp;quot;${MODEL_ID}&amp;quot;: chat_template_kwargs: reasoning_effort: low &amp;quot;${MODEL_ID}:med&amp;quot;: chat_template_kwargs: reasoning_effort: medium &amp;quot;${MODEL_ID}:high&amp;quot;: chat_template_kwargs: reasoning_effort: high cmd: | /path/to/llama-server/llama-server-latest --host 127.0.0.1 --port ${PORT} --fit off --ctx-size 65536 --no-mmap --no-warmup --model /path/to/models/gpt-oss-120b-mxfp4-00001-of-00003.gguf --temp 1.0 --top-k 100 --top-p 1.0 &lt;/code&gt;&lt;/p&gt; &lt;p&gt;There's a bit more documentation in the &lt;a href="https://github.com/mostlygeek/llama-swap/blob/49546e2cf2d7089bafc463a51677b4843f4627ec/config.example.yaml#L217-L234"&gt;config examples&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;Side note: I realize that llama-swap's config has gotten quite complex! I'm trying to come up with clever ways to make it a bit more accessible for new users. :) &lt;/p&gt; &lt;p&gt;Edit: spelling ü§¶üèª‚Äç‚ôÇÔ∏è&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/No-Statement-0001"&gt; /u/No-Statement-0001 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhohqk/how_to_switch_qwen_35_thinking_onoff_without/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhohqk/how_to_switch_qwen_35_thinking_onoff_without/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhohqk/how_to_switch_qwen_35_thinking_onoff_without/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T05:04:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhg3p4</id>
    <title>Bare-Metal AI: Booting Directly Into LLM Inference ‚Äö No OS, No Kernel (Dell E6510)</title>
    <updated>2026-02-28T22:32:35+00:00</updated>
    <author>
      <name>/u/Electrical_Ninja3805</name>
      <uri>https://old.reddit.com/user/Electrical_Ninja3805</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhg3p4/baremetal_ai_booting_directly_into_llm_inference/"&gt; &lt;img alt="Bare-Metal AI: Booting Directly Into LLM Inference ‚Äö No OS, No Kernel (Dell E6510)" src="https://external-preview.redd.it/PRknAnIB54eZMfut9qkw3hhK_Rxo72UxY2hekIecmlA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8c8f7850f02bf683876987aac081f780b39da271" title="Bare-Metal AI: Booting Directly Into LLM Inference ‚Äö No OS, No Kernel (Dell E6510)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;someone asked me to post this here, said you gays would like this kinda thing. just a heads up, Im new to reddit, made my account a couple years ago, only now using it,&lt;/p&gt; &lt;p&gt;A UEFI application that boots directly into LLM chat: no operating system, no kernel, no drivers(well sort of....wifi). Just power on, select &amp;quot;Run Live&amp;quot;, type &amp;quot;chat&amp;quot;, and talk to an AI. Everything you see is running in UEFI boot services mode. The entire stack, tokenizer, weight loader, tensor math, inference engine, is written from scratch in freestanding C with zero dependencies. It's painfully slow at the moment because I haven't done any optimizations. Realistically it should run much much faster, but I'm more interested in getting the network drivers running first before that. I'm planning on using this to serve smaller models on my network. Why would I build this? For giggles. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Electrical_Ninja3805"&gt; /u/Electrical_Ninja3805 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=wsfKZWg-Wv4"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhg3p4/baremetal_ai_booting_directly_into_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhg3p4/baremetal_ai_booting_directly_into_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-28T22:32:35+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhw16v</id>
    <title>Dense (non-thinking) &gt; MoE? Qwen-3.5-27B is blowing me away in coding</title>
    <updated>2026-03-01T12:25:32+00:00</updated>
    <author>
      <name>/u/theskilled42</name>
      <uri>https://old.reddit.com/user/theskilled42</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhw16v/dense_nonthinking_moe_qwen3527b_is_blowing_me/"&gt; &lt;img alt="Dense (non-thinking) &amp;gt; MoE? Qwen-3.5-27B is blowing me away in coding" src="https://external-preview.redd.it/azdnOHA4cXFmZm1nMWluHoHESFEQm1vbE9B9dUQV9LzDMcIQbHzAce2RYgnO.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7aa5259581a59bf66513c370135563dc412d7cee" title="Dense (non-thinking) &amp;gt; MoE? Qwen-3.5-27B is blowing me away in coding" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Vibe-coded this Python program from &lt;a href="http://chat.qwen.ai"&gt;chat.qwen.ai&lt;/a&gt; (Fast mode) using Qwen-3.5-27B by just providing it with OpenRouter's Quickstart python snippet on how to use their API. Took about 1 hour with only about 7 errors total (mostly was from adding features and two of the errors are the same) but it was worth it considering it's from a &lt;strong&gt;27B&lt;/strong&gt; &lt;strong&gt;non-thinking&lt;/strong&gt; model. I also edited like 4 lines on it to fit to my liking.&lt;/p&gt; &lt;p&gt;Features:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Uses Rich for colorful Markdown terminal output.&lt;/li&gt; &lt;li&gt;Shows a cycling loading spinner during API waits (waits for the response to finish before streaming it client-side -- reasoning is still off).&lt;/li&gt; &lt;li&gt;Runs network requests in a background thread.&lt;/li&gt; &lt;li&gt;Streams AI replies with a typing effect.&lt;/li&gt; &lt;li&gt;Auto-saves chats to timestamped text files.&lt;/li&gt; &lt;li&gt;Handles Ctrl+C and crashes without losing data.&lt;/li&gt; &lt;li&gt;Catches and displays network errors clearly.&lt;/li&gt; &lt;li&gt;Fine-tunes generation with custom model parameters.&lt;/li&gt; &lt;li&gt;Hides system prompts from saved logs.&lt;/li&gt; &lt;li&gt;Ignores empty inputs and accepts quit commands.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;(I'm using Ghostty as the terminal emulator.)&lt;/p&gt; &lt;p&gt;Genuinely mind-blown by this model. I haven't tested Qwen-3.5-35B-A3B with something like this, but I'm scared to do it since I'm more than satisfied with this quality!&lt;/p&gt; &lt;p&gt;I don't know if other previous ~30B models can produce this quality without errors all the time, but this felt no where as expected from a 27B model. I think most models, even the bigger ones, will be a lot smarter if they were Dense models instead of MoE.&lt;/p&gt; &lt;p&gt;My main issue with this model is its thinking: it produces SO MUCH tokens with little improvement on its outputs. I genuinely believe thinking is just a gimmick for like 80% of the time. High-quality data, training and architecture will rise instruct models above thinking imo (also it's more efficient).&lt;/p&gt; &lt;p&gt;Local LLM enthusiasts are eating good with this model!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/theskilled42"&gt; /u/theskilled42 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/6qk2wopqffmg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhw16v/dense_nonthinking_moe_qwen3527b_is_blowing_me/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhw16v/dense_nonthinking_moe_qwen3527b_is_blowing_me/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T12:25:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri0puh</id>
    <title>Honor would use Deepseek</title>
    <updated>2026-03-01T15:54:13+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri0puh/honor_would_use_deepseek/"&gt; &lt;img alt="Honor would use Deepseek" src="https://preview.redd.it/1u6q97w1hgmg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=5d0519d3fa77185aba38bfee0ebb86dfd7a37272" title="Honor would use Deepseek" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/i/status/2028081963635290537"&gt;https://x.com/i/status/2028081963635290537&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/1u6q97w1hgmg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri0puh/honor_would_use_deepseek/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri0puh/honor_would_use_deepseek/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T15:54:13+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhykhm</id>
    <title>Qwen 3.5 small , soon</title>
    <updated>2026-03-01T14:26:02+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhykhm/qwen_35_small_soon/"&gt; &lt;img alt="Qwen 3.5 small , soon" src="https://preview.redd.it/lq67yzkb1gmg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=47e07f7a87b045f73bbd083cd07250d1c2394464" title="Qwen 3.5 small , soon" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lq67yzkb1gmg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhykhm/qwen_35_small_soon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhykhm/qwen_35_small_soon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T14:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri635s</id>
    <title>13 months since the DeepSeek moment, how far have we gone running models locally?</title>
    <updated>2026-03-01T19:13:04+00:00</updated>
    <author>
      <name>/u/dionisioalcaraz</name>
      <uri>https://old.reddit.com/user/dionisioalcaraz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri635s/13_months_since_the_deepseek_moment_how_far_have/"&gt; &lt;img alt="13 months since the DeepSeek moment, how far have we gone running models locally?" src="https://preview.redd.it/2ovdv238ehmg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=851be1b17f5b531d3176db2cc3712db9375dd984" title="13 months since the DeepSeek moment, how far have we gone running models locally?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Once upon a time there was a &lt;a href="https://x.com/carrigmat/status/1884244369907278106#m"&gt;tweet&lt;/a&gt; from an engineer at Hugging Face explaining how to run the frontier level DeepSeek R1 @ Q8 at ~5 tps for about $6000.&lt;/p&gt; &lt;p&gt;Now at around the same speed, with &lt;a href="https://www.amazon.com/AOOSTAR-PRO-8845HS-OCULINK-HDMI2-1/dp/B0G7DCC2XY/"&gt;this&lt;/a&gt; $600 mini PC, you can run the highly superior Qwen3-27B @ Q4.&lt;/p&gt; &lt;p&gt;But if you want more usable speeds, with the still much stronger Qwen3.5-35B-A3B @ Q4/Q5, you can get 17-20 tps.&lt;/p&gt; &lt;p&gt;Isn't it wild? At this pace of improving smaller models, could we be running next year a 4B model better than Kimi 2.5?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dionisioalcaraz"&gt; /u/dionisioalcaraz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ovdv238ehmg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri635s/13_months_since_the_deepseek_moment_how_far_have/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri635s/13_months_since_the_deepseek_moment_how_far_have/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T19:13:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhvi09</id>
    <title>PSA: If your local coding agent feels "dumb" at 30k+ context, check your KV cache quantization first.</title>
    <updated>2026-03-01T11:55:51+00:00</updated>
    <author>
      <name>/u/Dismal-Ad1207</name>
      <uri>https://old.reddit.com/user/Dismal-Ad1207</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I‚Äôve been seeing a lot of posts lately about models like Qwen3-Coder or GLM 4.7 getting trapped in infinite correction loops or hallucinating tool-call parameters once the context gets deep. The usual advice is to switch to a higher precision GGUF or tweak the system prompt. But after a few days of heavy profiling, the culprit is almost always aggressive KV cache quantization.Everyone wants to cram 30B+ models into 24GB of VRAM. To do that and still keep a 64k context window, turning on Q4 or Q8 KV cache in llama.cpp or ExLlamaV3 feels like free real estate. Short-context perplexity benchmarks barely budge, so it looks like a safe bet.&lt;/p&gt; &lt;p&gt;It‚Äôs not...&lt;/p&gt; &lt;p&gt;While testing tool-call reliability for the OpenClaw framework this weekend, I was consistently getting malformed JSON outputs after about 30k tokens. I started digging into the memory profiling after a user in &lt;a href="https://www.reddit.com/r/myclaw/"&gt;r/myclaw&lt;/a&gt; posted about their agent completely forgetting API schemas mid-task. We initially blamed the model‚Äôs context degradation, but when we isolated the variables, it was entirely the KV cache.&lt;/p&gt; &lt;p&gt;Here is the mechanical reality: the K-cache (Keys) is exponentially more sensitive to precision loss than the V-cache (Values). When you quantize the K-cache to 4-bit or even 8-bit, you are actively degrading the attention mechanism's ability to perfectly match the exact syntax of a strict schema defined 40,000 tokens ago. The model knows the tool exists, but the keys are &amp;quot;fuzzy,&amp;quot; so it hallucinates the parameter structure. On top of that, if you're using llama.cpp, heavily quantized KV cache forces a lot of the dequantization overhead onto the CPU, absolutely nuking your prompt processing speed.&lt;/p&gt; &lt;p&gt;If you are running agentic workflows, rigid syntax is non-negotiable.&lt;/p&gt; &lt;p&gt;A practical workaround if you're VRAM-starved: see if your backend allows mixed precision. Leave the K-cache at FP16 or FP8 and only quantize the V-cache to Q8. Otherwise, you're much better off dropping your max context size to fit an unquantized cache rather than giving your agent a lobotomy just to say you can hit 72k tokens.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dismal-Ad1207"&gt; /u/Dismal-Ad1207 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhvi09/psa_if_your_local_coding_agent_feels_dumb_at_30k/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhvi09/psa_if_your_local_coding_agent_feels_dumb_at_30k/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhvi09/psa_if_your_local_coding_agent_feels_dumb_at_30k/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T11:55:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri39a4</id>
    <title>Qwen3.5 35b a3b first small model to not hallucinate summarising 50k token text</title>
    <updated>2026-03-01T17:30:04+00:00</updated>
    <author>
      <name>/u/Windowsideplant</name>
      <uri>https://old.reddit.com/user/Windowsideplant</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've always ran this test to see how models did for long-ish text reasoning. It's the first chapters of a text I wrote and will never be online to make sure it's never polluting the training set of these models. &lt;/p&gt; &lt;p&gt;So far every model failed in the &amp;lt;=4b active parameters models I tested:&lt;/p&gt; &lt;p&gt;Qwen3 4b 2507 thinking Nanbeige4.1 3b Nvidia nemotron nano 4b Jamba reasoning 3b Gpt oss 20b Qwen3 30b a3b 2507 thinking&lt;/p&gt; &lt;p&gt;All added some boilerplate bs that was never in the text to begin with. But qwen3.5 35b a3b did great! Maybe I can finally use local models reliably and not just play with them&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Windowsideplant"&gt; /u/Windowsideplant &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri39a4/qwen35_35b_a3b_first_small_model_to_not/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri39a4/qwen35_35b_a3b_first_small_model_to_not/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri39a4/qwen35_35b_a3b_first_small_model_to_not/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T17:30:04+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhogov</id>
    <title>The U.S. used Anthropic AI tools during airstrikes on Iran</title>
    <updated>2026-03-01T05:02:45+00:00</updated>
    <author>
      <name>/u/External_Mood4719</name>
      <uri>https://old.reddit.com/user/External_Mood4719</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hours after announcing that the federal government would cease using artificial intelligence tools developed by the tech company Anthropic, U.S. President Trump utilized those very tools to launch a massive airstrike against Iran. Sources familiar with the matter confirmed that command centers in various locations, including U.S. Central Command (CENTCOM), have been using Anthropic‚Äôs Claude AI tool. Despite escalating tensions between the company and the Pentagon, the command continued to employ the tool for intelligence assessments, target identification, and combat simulations, highlighting the deep level of involvement of AI tools in military operations. The U.S. government and Anthropic have been in a dispute for months over how the Pentagon utilizes its AI models. On Friday, President Trump ordered all agencies to stop cooperating with the company, and the Department of Defense also determined that the firm poses a security threat and a risk to its supply chain.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.wsj.com/livecoverage/iran-strikes-2026/card/u-s-strikes-in-middle-east-use-anthropic-hours-after-trump-ban-ozNO0iClZpfpL7K7ElJ2"&gt;https://www.wsj.com/livecoverage/iran-strikes-2026/card/u-s-strikes-in-middle-east-use-anthropic-hours-after-trump-ban-ozNO0iClZpfpL7K7ElJ2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/External_Mood4719"&gt; /u/External_Mood4719 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhogov/the_us_used_anthropic_ai_tools_during_airstrikes/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhogov/the_us_used_anthropic_ai_tools_during_airstrikes/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhogov/the_us_used_anthropic_ai_tools_during_airstrikes/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T05:02:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhwo08</id>
    <title>Qwen3.5 Small Dense model release seems imminent.</title>
    <updated>2026-03-01T12:58:37+00:00</updated>
    <author>
      <name>/u/Deep-Vermicelli-4591</name>
      <uri>https://old.reddit.com/user/Deep-Vermicelli-4591</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhwo08/qwen35_small_dense_model_release_seems_imminent/"&gt; &lt;img alt="Qwen3.5 Small Dense model release seems imminent." src="https://preview.redd.it/k5buxjdplfmg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=eb2a54d233f60b987c521151f2ffb58f68623ce2" title="Qwen3.5 Small Dense model release seems imminent." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Deep-Vermicelli-4591"&gt; /u/Deep-Vermicelli-4591 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/k5buxjdplfmg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhwo08/qwen35_small_dense_model_release_seems_imminent/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhwo08/qwen35_small_dense_model_release_seems_imminent/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T12:58:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhvabz</id>
    <title>we need to go deeper</title>
    <updated>2026-03-01T11:43:26+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhvabz/we_need_to_go_deeper/"&gt; &lt;img alt="we need to go deeper" src="https://preview.redd.it/2ixnt6k88fmg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c974ea204ef618a39c27255c1f539d0046685d04" title="we need to go deeper" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Do you think it will happen today or tomorrow? :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2ixnt6k88fmg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhvabz/we_need_to_go_deeper/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhvabz/we_need_to_go_deeper/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T11:43:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1rhx5pc</id>
    <title>Reverse engineered Apple Neural Engine(ANE) to train Microgpt</title>
    <updated>2026-03-01T13:21:55+00:00</updated>
    <author>
      <name>/u/jack_smirkingrevenge</name>
      <uri>https://old.reddit.com/user/jack_smirkingrevenge</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhx5pc/reverse_engineered_apple_neural_engineane_to/"&gt; &lt;img alt="Reverse engineered Apple Neural Engine(ANE) to train Microgpt" src="https://preview.redd.it/vl6kd7lvpfmg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=df3b9c88272add092ba76ce112ea2a10e4e2381c" title="Reverse engineered Apple Neural Engine(ANE) to train Microgpt" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;Why? Because i bought a mac mini M4 and I wanted to leverage its compute for my compiler project&lt;/h1&gt; &lt;p&gt;Training on Metal(GPU) is well known but ANE is a black box and Apple doesn't talk about it. So I harnessed Claude to reverse engineer the ANE private APIs , run benchmarks by bypassing coreml(which is the recommended way to use ANE)&lt;/p&gt; &lt;p&gt;The NPU has 38 TFLOPS worth of claimed INT8 compute (but it's a FP16 processor so actual compute is half that)&lt;/p&gt; &lt;p&gt;In the end I create a bespoke training pipeline to train a small 110M microgpt model.&lt;/p&gt; &lt;p&gt;Now you can't in practice use it to train bigger models on a single chip but maybe a cluster of them in theory can train larger models. But even a single device should be able to do LoRA training for 3b/7b models.&lt;/p&gt; &lt;p&gt;Again, why train on NPUs? - they are extremely power efficient. Peak compute on ANE only consumes 2.8 W which at 19 tflops becomes 6.6 tflops/watt. Insane! (Metal GPU - 1, H100 - 1.4 Tflops/watt) &lt;/p&gt; &lt;h1&gt;Resources&lt;/h1&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/maderix/p/inside-the-m4-apple-neural-engine"&gt;Reverse Engineering&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://open.substack.com/pub/maderix/p/inside-the-m4-apple-neural-engine-615"&gt;Benchmarks&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;: WIP&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo&lt;/strong&gt; : &lt;a href="https://github.com/maderix/ANE"&gt;GitHub&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jack_smirkingrevenge"&gt; /u/jack_smirkingrevenge &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/vl6kd7lvpfmg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1rhx5pc/reverse_engineered_apple_neural_engineane_to/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1rhx5pc/reverse_engineered_apple_neural_engineane_to/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T13:21:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ri2irg</id>
    <title>Breaking : Today Qwen 3.5 small</title>
    <updated>2026-03-01T17:02:31+00:00</updated>
    <author>
      <name>/u/Illustrious-Swim9663</name>
      <uri>https://old.reddit.com/user/Illustrious-Swim9663</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri2irg/breaking_today_qwen_35_small/"&gt; &lt;img alt="Breaking : Today Qwen 3.5 small" src="https://preview.redd.it/4hhdbdn8tgmg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=01d43245d4e44f3879afe7088a5ba175e7c71929" title="Breaking : Today Qwen 3.5 small" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Illustrious-Swim9663"&gt; /u/Illustrious-Swim9663 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/4hhdbdn8tgmg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ri2irg/breaking_today_qwen_35_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ri2irg/breaking_today_qwen_35_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-03-01T17:02:31+00:00</published>
  </entry>
  <entry>
    <id>t3_1r7bsfd</id>
    <title>Best Audio Models - Feb 2026</title>
    <updated>2026-02-17T17:01:37+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;They've been a ton of audio models released of late, the most notable perhaps being Qwen3 TTS. So its time for another &lt;strong&gt;Best Audio Models&lt;/strong&gt; megathread&lt;/p&gt; &lt;p&gt;Share what your favorite ASR, TTS, STT, Text to Music models are right now &lt;strong&gt;and why.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models especially for production use cases with long lengths/stability requirements, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level comments to thread your responses.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r7bsfd/best_audio_models_feb_2026/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-17T17:01:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1r8snay</id>
    <title>AMA with StepFun AI - Ask Us Anything</title>
    <updated>2026-02-19T07:15:35+00:00</updated>
    <author>
      <name>/u/StepFun_ai</name>
      <uri>https://old.reddit.com/user/StepFun_ai</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt; &lt;img alt="AMA with StepFun AI - Ask Us Anything" src="https://external-preview.redd.it/bqLigXK_A-XO-amDfdyeQenKCxR18nhChWpDnQXYmr4.png?width=140&amp;amp;height=75&amp;amp;auto=webp&amp;amp;s=72c79bc533c167d8f77ecf629e3b7af53ee1e6b6" title="AMA with StepFun AI - Ask Us Anything" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2"&gt;https://preview.redd.it/w8274fg1jekg1.png?width=1785&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fadbd0ec26a56e60900f9ed667ae808217d70cf2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hi &lt;a href="/r/LocalLLaMA"&gt;r/LocalLLaMA&lt;/a&gt; !&lt;/p&gt; &lt;p&gt;We are &lt;strong&gt;StepFun&lt;/strong&gt;, the team behind the &lt;strong&gt;Step&lt;/strong&gt; family models, including &lt;a href="https://huggingface.co/collections/stepfun-ai/step-35-flash"&gt;&lt;strong&gt;Step 3.5 Flash&lt;/strong&gt;&lt;/a&gt; and &lt;a href="https://huggingface.co/collections/stepfun-ai/step3-vl-10b"&gt;&lt;strong&gt;Step-3-VL-10B&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We are super excited to host our first AMA tomorrow in this community. Our participants include CEO, CTO, Chief Scientist, LLM Researchers.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Participants&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Ok_Reach_5122"&gt;u/Ok_Reach_5122&lt;/a&gt; (Co-founder &amp;amp; CEO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/bobzhuyb"&gt;u/bobzhuyb&lt;/a&gt; (Co-founder &amp;amp; CTO of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/user/Lost-Nectarine1016"&gt;u/Lost-Nectarine1016&lt;/a&gt; (Co-founder &amp;amp; Chief Scientist of StepFun)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Elegant-Sale-1328"&gt;u/Elegant-Sale-1328&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/SavingsConclusion298"&gt;u/SavingsConclusion298&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Spirited_Spirit3387"&gt;u/Spirited_Spirit3387&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.reddit.com/user/These-Nothing-8564/"&gt;u/These-Nothing-8564&lt;/a&gt; (Technical Project Manager)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Either-Beyond-7395"&gt;u/Either-Beyond-7395&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Human_Ad_162"&gt;u/Human_Ad_162&lt;/a&gt; (Pre-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Icy_Dare_3866"&gt;u/Icy_Dare_3866&lt;/a&gt; (Post-training)&lt;/li&gt; &lt;li&gt;&lt;a href="https://old.reddit.com/u/Big-Employee5595"&gt;u/Big-Employee5595&lt;/a&gt; (Agent Algorithms Lead&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The AMA will run 8 - 11 AM PST, Feburary 19th. The StepFun team will monitor and answer questions over the 24 hours after the live session.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StepFun_ai"&gt; /u/StepFun_ai &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1r8snay/ama_with_stepfun_ai_ask_us_anything/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-02-19T07:15:35+00:00</published>
  </entry>
</feed>
