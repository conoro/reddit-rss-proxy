<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-23T03:21:36+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1odg1wm</id>
    <title>Introducing ExecuTorch 1.0</title>
    <updated>2025-10-22T18:17:14+00:00</updated>
    <author>
      <name>/u/dayanruben</name>
      <uri>https://old.reddit.com/user/dayanruben</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dayanruben"&gt; /u/dayanruben &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://pytorch.org/blog/introducing-executorch-1-0/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odg1wm/introducing_executorch_10/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odg1wm/introducing_executorch_10/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T18:17:14+00:00</published>
  </entry>
  <entry>
    <id>t3_1odilom</id>
    <title>RamaLama: Running LLMs as containers adding MLX support</title>
    <updated>2025-10-22T19:52:16+00:00</updated>
    <author>
      <name>/u/ProfessionalHorse707</name>
      <uri>https://old.reddit.com/user/ProfessionalHorse707</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odilom/ramalama_running_llms_as_containers_adding_mlx/"&gt; &lt;img alt="RamaLama: Running LLMs as containers adding MLX support" src="https://external-preview.redd.it/cTYxeHE0emh3cHdmMXy1caZuZgS62CJImk8F9ntJO8-7vfSdfeZ_sZ1BZhe-.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=9cbd4b2a2706487e2adcb2c8d660e5515a13f61a" title="RamaLama: Running LLMs as containers adding MLX support" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I’m not sure if anyone has played around with it yet but RamaLama is CLI for running and building LLMs as container images. &lt;/p&gt; &lt;p&gt;We recently added support for MLX in addition to llama.cpp and vLLM (shoutout to kush-gupt)! We are aiming to be totally runtime and hardware agnostic but it’s been an uphill battle with vLLM support still a little shaky. Still, we’ve got support for Apple Silicon GPUs, Nvidia GPUs (cuda), AMD GPUs (rocm, vulkan), Intel GPUs, Moore Threads GPUs, and Ascend NPUs. With so much variation we could really use help finding people with atypical hardware configurations to test against. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github&lt;/strong&gt;: &lt;a href="https://github.com/containers/ramalama"&gt;https://github.com/containers/ramalama&lt;/a&gt; &lt;/p&gt; &lt;p&gt;As an aside, there’s going to be a developer forum in a few weeks for new users: &lt;a href="http://ramalama.com/events/dev-forum-1"&gt;http://ramalama.com/events/dev-forum-1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ProfessionalHorse707"&gt; /u/ProfessionalHorse707 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/x0zcn3zhwpwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odilom/ramalama_running_llms_as_containers_adding_mlx/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odilom/ramalama_running_llms_as_containers_adding_mlx/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:52:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1odp8pe</id>
    <title>SGLang vs vLLM on H200: Which one do you prefer, Faster TTFT and higher TPS?</title>
    <updated>2025-10-23T00:28:56+00:00</updated>
    <author>
      <name>/u/batuhanaktass</name>
      <uri>https://old.reddit.com/user/batuhanaktass</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odp8pe/sglang_vs_vllm_on_h200_which_one_do_you_prefer/"&gt; &lt;img alt="SGLang vs vLLM on H200: Which one do you prefer, Faster TTFT and higher TPS?" src="https://preview.redd.it/0t0vl6ap9rwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f9118b9eba227185c90cfa8ec519e7381573d4b1" title="SGLang vs vLLM on H200: Which one do you prefer, Faster TTFT and higher TPS?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I ran both &lt;strong&gt;SGLang&lt;/strong&gt; and &lt;strong&gt;vLLM&lt;/strong&gt; on &lt;strong&gt;Qwen3-Coder-30B&lt;/strong&gt; with &lt;strong&gt;NVIDIA H200&lt;/strong&gt; and 500GB memory. Here are the numbers:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;TTFT (Time to First Token):&lt;/strong&gt; SGLang 2333ms vs vLLM 2669ms. SGLang is ~12.6% faster to start generating, which you feel in interactive workloads.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;TPS (Tokens/sec):&lt;/strong&gt; SGLang 2688.46 vs vLLM 2020.99. SGLang delivers ~33% higher throughput, meaning more tokens per unit time under load.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Token lengths:&lt;/strong&gt; SGLang produced &lt;strong&gt;~4.9%&lt;/strong&gt; longer inputs (48.14 vs 45.88) and &lt;strong&gt;~23.7%&lt;/strong&gt; longer outputs (72.50 vs 58.63). Even with longer generations, TPS still leads for SGLang, which strengthens the throughput win.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Setup time:&lt;/strong&gt; vLLM container setup and model download are both &lt;strong&gt;388s/ms&lt;/strong&gt; vs SGLang &lt;strong&gt;523s/ms&lt;/strong&gt; vLLM is ~34.8% faster to get to “ready.” If you spin clusters often or bake fresh images, this matters.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Which one do you think is better for production grade services?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/batuhanaktass"&gt; /u/batuhanaktass &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/0t0vl6ap9rwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odp8pe/sglang_vs_vllm_on_h200_which_one_do_you_prefer/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odp8pe/sglang_vs_vllm_on_h200_which_one_do_you_prefer/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T00:28:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1odh7ns</id>
    <title>First impressions and thoughts on the GTR9 Pro (Beelink's 395)</title>
    <updated>2025-10-22T19:00:20+00:00</updated>
    <author>
      <name>/u/kmouratidis</name>
      <uri>https://old.reddit.com/user/kmouratidis</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"&gt; &lt;img alt="First impressions and thoughts on the GTR9 Pro (Beelink's 395)" src="https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4" title="First impressions and thoughts on the GTR9 Pro (Beelink's 395)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: Good and bad, some &amp;quot;benchmarks&amp;quot; and details &lt;a href="https://gist.github.com/KMouratidis/88456bea439ea8d38f452bb6df289b58"&gt;here&lt;/a&gt;. Not sure I'd recommend it. Not yet.&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: I did some serious stress testing on Linux, and even though it kept up for a while, the Intel driver died, again. Will give the newer firmware version (v30.5) a try and update here.&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;Hey y'all! Just like many others I wanted to try the 395, but since I mostly wanted it as a server first (and LLM runner third), I wanted one with 10 Gbps networking. The MS-S1 hadn't come out yet, so I went with the &lt;a href="https://www.bee-link.com/products/beelink-gtr9-pro-amd-ryzen-ai-max-395?variant=47842426224882"&gt;Beelink GTR9 Pro AMD Ryzen™ AI Max+ 395&lt;/a&gt;, and ~25 days later it's here.&lt;/p&gt; &lt;p&gt;I tried the preinstalled Windows, which functioned for a bit, quickly devolved into a mess that made me want to return it. Thankfully, I wanted it as a server, which means I'll be running Linux, but I had to test it. Plenty of crashes under load, the Intel network card not working, and other weirdness. Turns out there are plenty of known issues that may be hardware or driver related, plenty of posts and speculation in &lt;a href="/r/BeelinkOfficial"&gt;r/BeelinkOfficial&lt;/a&gt; and it has been going for a couple weeks it seems, and may also affect Linux, but oh well, time to move on.&lt;/p&gt; &lt;p&gt;People suggest you use Fedora or Debian Sid, or anything with a recent kernel, and that's probably good advice for most people, but I ain't running Fedora for my server. I used a heavily configured DietPi (so basically Debian) instead, for no other reason than consistency with the rest of my (actually mini*) servers. Surely the driver situation can't be that bad, right? Actually yes, it's perfectly fine to run Debian and I haven't had an issue yet, although it's early, let's see if it reach even 10% the uptime my TrueNAS server has. After troubleshooting a few issues, installing the (hopefully) correct drivers, and building llama.cpp (lemonade and vLLM will have to wait until the weekend), I quickly tested a bunch of models, and the results I'm getting seem to roughly align with what others are getting (&lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mqtnz7/comment/n8uhbp3/"&gt;1&lt;/a&gt;, &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1mqtnz7/comment/n8wnxzc/"&gt;2&lt;/a&gt;, &lt;a href="https://forum.level1techs.com/t/strix-halo-ryzen-ai-max-395-llm-benchmark-results/233796"&gt;3&lt;/a&gt;, &lt;a href="https://github.com/lhl/strix-halo-testing/tree/main/llm-bench/gpt-oss-120b-F16"&gt;4&lt;/a&gt;). I have documented everything in the &lt;a href="https://gist.github.com/KMouratidis/88456bea439ea8d38f452bb6df289b58"&gt;gist&lt;/a&gt; (I think!).&lt;/p&gt; &lt;p&gt;Out of the box, the Beelink runs with 96GB allocated as VRAM and can consume up to 170W without me messing with BIOS or Linux settings. In short, the results are exactly as you would expect:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;GPT-OSS-120B is probably the best model to run&lt;/li&gt; &lt;li&gt;Flash Attention helps, but not always by a lot&lt;/li&gt; &lt;li&gt;Performance mode didn't do a thing and maybe was worse, graphics overclocking &lt;em&gt;seems&lt;/em&gt; to help a bit with prefill/pp/input, but not a low&lt;/li&gt; &lt;li&gt;ECO still consumes 100W during inference, &lt;em&gt;but the performance hit can be as little ~15% for ~45% less max power&lt;/em&gt;, which is kinda insane but well-known by now that max power only gives marginal improvements&lt;/li&gt; &lt;li&gt;You must be dense if you expect to run dense models&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;Model&lt;/th&gt; &lt;th align="left"&gt;Size&lt;/th&gt; &lt;th align="left"&gt;Params&lt;/th&gt; &lt;th align="left"&gt;Backend&lt;/th&gt; &lt;th align="left"&gt;Test&lt;/th&gt; &lt;th align="left"&gt;Tokens/s (FA 0)&lt;/th&gt; &lt;th align="left"&gt;Tokens/s (FA 1)&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;GLM-4.5-Air (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;68.01 GiB&lt;/td&gt; &lt;td align="left"&gt;110.47B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;142.90 ± 1.39&lt;/td&gt; &lt;td align="left"&gt;152.65 ± 1.49&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;20.31 ± 0.07&lt;/td&gt; &lt;td align="left"&gt;20.83 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;Qwen3-30B (Q4_K_XL)&lt;/td&gt; &lt;td align="left"&gt;16.49 GiB&lt;/td&gt; &lt;td align="left"&gt;30.53B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;496.63 ± 11.29&lt;/td&gt; &lt;td align="left"&gt;503.25 ± 6.42&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;63.26 ± 0.28&lt;/td&gt; &lt;td align="left"&gt;64.43 ± 0.71&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;GPT-OSS-120B (F16)&lt;/td&gt; &lt;td align="left"&gt;60.87 GiB&lt;/td&gt; &lt;td align="left"&gt;116.83B&lt;/td&gt; &lt;td align="left"&gt;ROCm&lt;/td&gt; &lt;td align="left"&gt;pp512&lt;/td&gt; &lt;td align="left"&gt;636.25 ± 5.49&lt;/td&gt; &lt;td align="left"&gt;732.70 ± 5.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;/td&gt; &lt;td align="left"&gt;tg128&lt;/td&gt; &lt;td align="left"&gt;34.44 ± 0.01&lt;/td&gt; &lt;td align="left"&gt;34.60 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Happy to run tests / benchmarks or answer questions, but some stuff may need to wait for the weekend.&lt;/p&gt; &lt;p&gt;----------&lt;/p&gt; &lt;p&gt;* Bonus: I sent this photo of the Beelink with my old &lt;a href="https://refurbished.minisforum.com/products/minisforum-z83-f-refurbished"&gt;Minisforum Z83-F&lt;/a&gt; to someone, joking about how mini PCs looked in 2015 vs in 2025. She thought the Minisforum was the one from 2025.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/6hlbhs9lmpwf1.jpg?width=2304&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=8ab4f30de9ecd33370f73213ebd0d121a055c041"&gt;Beelink GTR9 Pro (2025) dwarfs it's little bro, the Minisforum Z83-F (2015)&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/kmouratidis"&gt; /u/kmouratidis &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odh7ns/first_impressions_and_thoughts_on_the_gtr9_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:00:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1od35w1</id>
    <title>New model from Tencent, HunyuanWorld-Mirror</title>
    <updated>2025-10-22T09:01:59+00:00</updated>
    <author>
      <name>/u/edward-dev</name>
      <uri>https://old.reddit.com/user/edward-dev</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"&gt; &lt;img alt="New model from Tencent, HunyuanWorld-Mirror" src="https://external-preview.redd.it/4mzrgM79cCe_QE-8XZM35Vw90_ckM3tR76mq1apuGAU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=1d7d696f5cccceda23331d47643538ea7a5dce0c" title="New model from Tencent, HunyuanWorld-Mirror" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;HunyuanWorld-Mirror is a versatile feed-forward model for comprehensive 3D geometric prediction. It integrates diverse geometric priors (camera poses, calibrated intrinsics, depth maps) and simultaneously generates various 3D representations (point clouds, multi-view depths, camera parameters, surface normals, 3D Gaussians) in a single forward pass.&lt;/p&gt; &lt;p&gt;Really interesting for folks into 3D...&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edward-dev"&gt; /u/edward-dev &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/tencent/HunyuanWorld-Mirror"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od35w1/new_model_from_tencent_hunyuanworldmirror/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T09:01:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbco8</id>
    <title>LMStudio - Now has GLM 4.6 Support (CUDA)</title>
    <updated>2025-10-22T15:26:02+00:00</updated>
    <author>
      <name>/u/YouAreRight007</name>
      <uri>https://old.reddit.com/user/YouAreRight007</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, just so you know, LMStudio seems to now have GLM 4.6 support. Yay.&lt;/p&gt; &lt;p&gt;I'm getting 2.99 tokens a second when generating 3000 tokens using 1 3090 and PC RAM.&lt;/p&gt; &lt;p&gt;Model: Unsloth GLM 4.6 UD - Q3_K_XL (147.22GB)&lt;/p&gt; &lt;p&gt;Hardware setup: single 3090 + 14700K with 192GB RAM DDR5333. (14700K limited to 250Watts)&lt;/p&gt; &lt;p&gt;NOTE: Getting a buffer related error when trying to offload layers onto 2x 3090s.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/YouAreRight007"&gt; /u/YouAreRight007 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odbco8/lmstudio_now_has_glm_46_support_cuda/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:26:02+00:00</published>
  </entry>
  <entry>
    <id>t3_1ocrocy</id>
    <title>DeepSeek-OCR - Lives up to the hype</title>
    <updated>2025-10-21T22:51:20+00:00</updated>
    <author>
      <name>/u/Bohdanowicz</name>
      <uri>https://old.reddit.com/user/Bohdanowicz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt; &lt;img alt="DeepSeek-OCR - Lives up to the hype" src="https://external-preview.redd.it/DGhh7DsESjWeCrgHI7E8he7jk8ACLXaitOluBNwi630.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=86af002276d4a89cdea0ff0abd7fac0d455b8d9a" title="DeepSeek-OCR - Lives up to the hype" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I decided to try this out. Dockerized the model with fastapi in a wsl environment. Gave it 10000 pdfs to convert to markdown.&lt;/p&gt; &lt;p&gt;Hardware - 1 x A6000 ADA on a Ryzen 1700 /w 32gb ram&lt;/p&gt; &lt;p&gt;Processed prompts: 100%|██████████| 1/1 [00:00&amp;lt;00:00, 3.29it/s, est. speed input: 3000.81 toks/s, output: 220.20 toks/s]&lt;/p&gt; &lt;p&gt;I'm averaging less than 1 second per page.&lt;/p&gt; &lt;p&gt;This is the real deal.&lt;/p&gt; &lt;p&gt;EDIT: Decided to share the docker build if anyone is interested. It wraps the model up nicely so you can try it out directly with the api. it uses the vllm-openapi 0.8.5 public docker image.&lt;/p&gt; &lt;p&gt;Also included a pdf to markdown utility which will process anything in the /data subfolder to .md just by running it since there is an issue using the batch processor directly via the api.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/se9r9dsnyjwf1.png?width=1458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcd8118c3e1c167cc13d159579527a802e55fd84"&gt;https://preview.redd.it/se9r9dsnyjwf1.png?width=1458&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=fcd8118c3e1c167cc13d159579527a802e55fd84&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API"&gt;https://github.com/Bogdanovich77/DeekSeek-OCR---Dockerized-API&lt;/a&gt;&lt;/p&gt; &lt;p&gt;EDIT: Updated API to allow custom prompts. Also implemented the deepseek post processing in the pdf_to_*_enhanced.py prompts. Now properly extracts images.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Bohdanowicz"&gt; /u/Bohdanowicz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ocrocy/deepseekocr_lives_up_to_the_hype/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-21T22:51:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1odavba</id>
    <title>Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzen™ AI NPU</title>
    <updated>2025-10-22T15:07:57+00:00</updated>
    <author>
      <name>/u/BandEnvironmental834</name>
      <uri>https://old.reddit.com/user/BandEnvironmental834</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"&gt; &lt;img alt="Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzen™ AI NPU" src="https://external-preview.redd.it/y7g9bbQ0RPFXfYX-nbtW899i8fH3DJk9OyQ8tRM7MG4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b115fd1754530c90df1effac911d91b79c4dfb2b" title="Running whisper-large-v3-turbo (OpenAI) Exclusively on AMD Ryzen™ AI NPU" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;About the Demo&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Workflow:&lt;/strong&gt; &lt;code&gt;whisper-large-v3-turbo&lt;/code&gt; transcribes audio; &lt;code&gt;gpt-oss:20b&lt;/code&gt; generates the summary. Both models are &lt;strong&gt;pre-loaded&lt;/strong&gt; on the NPU.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Settings:&lt;/strong&gt; &lt;code&gt;gpt-oss:20b&lt;/code&gt; reasoning effort = &lt;strong&gt;High&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Test system:&lt;/strong&gt; ASRock 4X4 BOX-AI340 Mini PC (Kraken Point), 96 GB RAM.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Software:&lt;/strong&gt; FastFlowLM (CLI mode).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;About FLM&lt;/h1&gt; &lt;p&gt;We’re a small team building &lt;strong&gt;FastFlowLM (FLM)&lt;/strong&gt; — a fast runtime for running &lt;strong&gt;Whisper (Audio)&lt;/strong&gt;, &lt;strong&gt;GPT-OSS (first MoE on NPUs), Gemma3 (vision), Medgemma,&lt;/strong&gt; &lt;strong&gt;Qwen3,&lt;/strong&gt; &lt;strong&gt;DeepSeek-R1&lt;/strong&gt;, &lt;strong&gt;LLaMA3.x,&lt;/strong&gt; and others &lt;strong&gt;entirely on the AMD Ryzen AI NPU&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;Think &lt;strong&gt;Ollama (maybe llama.cpp since we have our own backend?)&lt;/strong&gt;, but deeply optimized for AMD NPUs — with both &lt;strong&gt;CLI&lt;/strong&gt; and &lt;strong&gt;Server Mode (OpenAI-compatible)&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;✨ &lt;strong&gt;From Idle Silicon to Instant Power — FastFlowLM (FLM) Makes Ryzen™ AI Shine.&lt;/strong&gt;&lt;/p&gt; &lt;h1&gt;Key Features&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;No GPU fallback&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Faster and over 10× more power efficient.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Supports context lengths up to 256k tokens (qwen3:4b-2507).&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Ultra-Lightweight (16 MB). Installs within 20 seconds.&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Try It Out&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GitHub:&lt;/strong&gt; &lt;a href="https://github.com/FastFlowLM/FastFlowLM"&gt;github.com/FastFlowLM/FastFlowLM&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Live Demo → Remote machine access on the repo page&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;YouTube Demos:&lt;/strong&gt; &lt;a href="https://www.youtube.com/@FastFlowLM-YT/playlists"&gt;FastFlowLM - YouTube&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;We’re iterating fast and would &lt;strong&gt;love your feedback, critiques, and ideas&lt;/strong&gt;🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BandEnvironmental834"&gt; /u/BandEnvironmental834 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://youtu.be/0t8ijUPg4A0?si=539G5mrICJNOwe6Z"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odavba/running_whisperlargev3turbo_openai_exclusively_on/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:07:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1odf4ei</id>
    <title>Devs, what are your experiences with Qwen3-coder-30b?</title>
    <updated>2025-10-22T17:43:11+00:00</updated>
    <author>
      <name>/u/AzRedx</name>
      <uri>https://old.reddit.com/user/AzRedx</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;From code completion, method refactoring, to generating a full MVP project, how well does Qwen3-coder-30b perform?&lt;/p&gt; &lt;p&gt;I have a desktop with 32GB DDR5 RAM and I'm planning to buy an RTX 50 series with at least 16GB of VRAM. Can it handle the quantized version of this model well?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AzRedx"&gt; /u/AzRedx &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf4ei/devs_what_are_your_experiences_with_qwen3coder30b/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf4ei/devs_what_are_your_experiences_with_qwen3coder30b/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odf4ei/devs_what_are_your_experiences_with_qwen3coder30b/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T17:43:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1odddyg</id>
    <title>Free GPU memory during local LLM inference without KV cache hogging VRAM</title>
    <updated>2025-10-22T16:40:37+00:00</updated>
    <author>
      <name>/u/ivaniumr</name>
      <uri>https://old.reddit.com/user/ivaniumr</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt; &lt;img alt="Free GPU memory during local LLM inference without KV cache hogging VRAM" src="https://external-preview.redd.it/Qexkfi7FQ3mBNXUsI349OiBIZqFoa4py3iuRmtBXCE0.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3a119b694af4a1e62532c3a6e7da37245e136c66" title="Free GPU memory during local LLM inference without KV cache hogging VRAM" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We are building &lt;a href="https://github.com/ovg-project/kvcached"&gt;kvcached&lt;/a&gt;, a library that lets local LLM inference engines such as &lt;strong&gt;SGLang&lt;/strong&gt; and &lt;strong&gt;vLLM&lt;/strong&gt; free idle KV cache memory instead of occupying the entire GPU. This allows you to run a model locally without using all available VRAM, so other applications can still run or even share the GPU.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;✅ Works out of the box with SGLang and vLLM&lt;/li&gt; &lt;li&gt;🔧 Support for Ollama and LM Studio is in progress&lt;/li&gt; &lt;li&gt;🧩 No changes to your model or prompts required&lt;/li&gt; &lt;li&gt;🚀 Install with pip and it runs out of the box&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Our code is open source: &lt;a href="https://github.com/ovg-project/kvcached"&gt;https://github.com/ovg-project/kvcached&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Deep dive blog for those interested in the techniques behind it: &lt;a href="https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141"&gt;https://yifanqiao.notion.site/Solve-the-GPU-Cost-Crisis-with-kvcached-289da9d1f4d68034b17bf2774201b141&lt;/a&gt;&lt;/p&gt; &lt;p&gt;We would love feedback from the local LLM community. If you want to run multiple models on one GPU, combine LLMs with other GPU applications, or simply reduce memory usage, feel free to try it out and ask questions. Happy to discuss and improve together 🙌&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/co5zu9swyowf1.jpg?width=3217&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f6fd3f6976033f9a3c4a23fb1743fa7f5dd0a59f"&gt;https://preview.redd.it/co5zu9swyowf1.jpg?width=3217&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f6fd3f6976033f9a3c4a23fb1743fa7f5dd0a59f&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ivaniumr"&gt; /u/ivaniumr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odddyg/free_gpu_memory_during_local_llm_inference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T16:40:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1odnqga</id>
    <title>New 'Markovian Thinking' technique unlocks a path to million-token AI reasoning</title>
    <updated>2025-10-22T23:19:50+00:00</updated>
    <author>
      <name>/u/qzrz</name>
      <uri>https://old.reddit.com/user/qzrz</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/qzrz"&gt; /u/qzrz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://venturebeat.com/ai/new-markovian-thinking-technique-unlocks-a-path-to-million-token-ai"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odnqga/new_markovian_thinking_technique_unlocks_a_path/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odnqga/new_markovian_thinking_technique_unlocks_a_path/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T23:19:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1od0jw1</id>
    <title>[R] We figured out how to predict 32B model reasoning performance with a 1B model. 100x cheaper. Paper inside.</title>
    <updated>2025-10-22T06:13:40+00:00</updated>
    <author>
      <name>/u/jshin49</name>
      <uri>https://old.reddit.com/user/jshin49</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Remember our &lt;a href="https://www.reddit.com/r/LocalLLaMA/comments/1nedq3i/we_just_released_the_worlds_first_70b/?sort=new"&gt;70B intermediate checkpoints release&lt;/a&gt;? We said we wanted to enable real research on training dynamics. Well, here's exactly the kind of work we hoped would happen.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;rBridge:&lt;/strong&gt; Use 1B models to predict whether your 32B model will be good at reasoning. Actually works.&lt;/p&gt; &lt;p&gt;The problem: Small models can't do reasoning (emergence happens at 7B+), so how do you know if your training recipe works without spending $200k?&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Our solution:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Align evaluation with both pre-training objective AND target task&lt;/li&gt; &lt;li&gt;Use frontier model reasoning traces as gold labels&lt;/li&gt; &lt;li&gt;Weight tokens by task importance automatically&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;100x compute reduction vs baselines&lt;/li&gt; &lt;li&gt;Accurately predict which datasets are worth training on&lt;/li&gt; &lt;li&gt;R² = 0.826 predicting 32B performance from 1B proxy&lt;/li&gt; &lt;li&gt;Works zero-shot on new datasets&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Tested on: GSM8K, MATH500, ARC-C, MMLU Pro, CQA, HumanEval&lt;/p&gt; &lt;p&gt;Paper: &lt;a href="https://www.arxiv.org/abs/2509.21013"&gt;https://www.arxiv.org/abs/2509.21013&lt;/a&gt;&lt;/p&gt; &lt;p&gt;This is what open research looks like - building on each other's work to make LLM development accessible to everyone, not just companies with infinite compute.&lt;/p&gt; &lt;p&gt;Code coming soon. Apache 2.0 as always.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jshin49"&gt; /u/jshin49 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od0jw1/r_we_figured_out_how_to_predict_32b_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T06:13:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1od7hyu</id>
    <title>M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)</title>
    <updated>2025-10-22T12:55:20+00:00</updated>
    <author>
      <name>/u/Noble00_</name>
      <uri>https://old.reddit.com/user/Noble00_</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"&gt; &lt;img alt="M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)" src="https://preview.redd.it/2r0ue3k6unwf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=4b506f746bde959cb1cf422094c3babaa4c4113e" title="M5 MacBook Pro: Up to ~45% PP Improvement. ~25% TG (Ollama Tested)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;[&lt;a href="https://www.youtube.com/watch?v=BKQggt9blGo"&gt;Geekerwan&lt;/a&gt;]&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Noble00_"&gt; /u/Noble00_ &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/2r0ue3k6unwf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od7hyu/m5_macbook_pro_up_to_45_pp_improvement_25_tg/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T12:55:20+00:00</published>
  </entry>
  <entry>
    <id>t3_1odq73r</id>
    <title>Is Chain of Thought Still An Emergent Behavior?</title>
    <updated>2025-10-23T01:14:30+00:00</updated>
    <author>
      <name>/u/Environmental_Form14</name>
      <uri>https://old.reddit.com/user/Environmental_Form14</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;In the famous &lt;a href="https://arxiv.org/pdf/2201.11903"&gt;Chain of Thought Paper&lt;/a&gt;, the authors argued that reasoning is an emergent behavior: models with &amp;lt;10B parameters showed little to no improvement from the baseline with the Chain of Thought prompting, but larger models did. &lt;/p&gt; &lt;p&gt;This is an old paper experimented in 2022. I wonder if their assertion still holds currently. We have&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Teacher-Student learning (distillation)&lt;/li&gt; &lt;li&gt;ReACT which led to training &amp;quot;Thinking Models&amp;quot;&lt;/li&gt; &lt;li&gt;better data concoction of training&lt;/li&gt; &lt;li&gt;better model architecture&lt;/li&gt; &lt;li&gt;better general performance models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The results from their experiments and the conclusions would be different if it was done right now.&lt;/p&gt; &lt;p&gt;I tried to find n-shot CoT vs. 0-shot performance comparisons across model scales, but this data is surprisingly hard to find. In my own quick tests with sub-3B models on MMLU and GSM8K, I found no improvement with n-shot CoT prompting.&lt;/p&gt; &lt;p&gt;So I’d love to hear from others:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Has anyone seen systematic evaluations on this recently?&lt;/li&gt; &lt;li&gt;Is reasoning still emergent only in larger models?&lt;/li&gt; &lt;li&gt;Or can smaller models be trained (or distilled) to exhibit CoT-like reasoning reliably without explicit training.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Environmental_Form14"&gt; /u/Environmental_Form14 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odq73r/is_chain_of_thought_still_an_emergent_behavior/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odq73r/is_chain_of_thought_still_an_emergent_behavior/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odq73r/is_chain_of_thought_still_an_emergent_behavior/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-23T01:14:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1odgyxp</id>
    <title>I Asked Grok, Claude, ChatGPT, and Google to Fix My Code (Are we really doomed?)</title>
    <updated>2025-10-22T18:51:11+00:00</updated>
    <author>
      <name>/u/FPham</name>
      <uri>https://old.reddit.com/user/FPham</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So yesterday I spent about 3 hours on an existing project, throwing it at Grok, Claude, and Google AI. Not something huge, About 3 pairs of reasonably sized cpp/h files, nothing too flashy, rather tight coding.&lt;br /&gt; It’s a painting editor drop in — sort of a Photoshop-ish thing (complete with multi-undo, image based brushes and all that crap).&lt;/p&gt; &lt;p&gt;I still have the old code, I plan to throw it at Qwen, Deepseek, etc next.&lt;br /&gt; Edit: See bottom of the post for updates.&lt;/p&gt; &lt;p&gt;I noticed the zoom in/out was chaotic. It was supposed to zoom around the cursor when using zoomat(x,y), but instead, it was jumping all over the place.&lt;/p&gt; &lt;p&gt;So first, Grok. It noticed I did GDI+ dynamically and told me there’s no reason for that. The rewrite it came up with to “fix” my issue was a disaster — after multiple back-and-forths, it just kept getting worse. Also, Grok’s tendency to randomly change and add lot of code didn’t help. Hahaha. Reverted back to my original code. Jumpy but at least image was always visible on screen, unlike Grok's code where the image could go entirely outside the viewport.&lt;/p&gt; &lt;p&gt;ChatGPT — not enough tokens to feed entire code on my tier, so ignored for now.&lt;/p&gt; &lt;p&gt;Google AI… now that one has this funny habit of always agreeing with you. It just keeps spitting out the same code and saying, &lt;em&gt;“Now it’s perfectly fixed, this is the final version, I swear on Larry Page, I found the problem!”&lt;/em&gt; No, it didn’t.&lt;br /&gt; To be fair, it was poking in the right places and found the functions that likely needed changing, but the result was still wrong. Again, the problem got even worse. It seems that if it doesn't know it kind of starts just shuffling code around without any real changes.&lt;/p&gt; &lt;p&gt;Claude - same issue, rewrote the code multiple times, finding the bug, never found it. But then I asked if maybe I was mixing up coordinates, and boom — Claude immediately said, yep, you’re mixing local and screen coordinates. (didn't you notice that before?) And indeed, that was the broad culprit.&lt;br /&gt; Its fix then was halfway there — zoom in worked, but zoom out… the moment the image fit in the viewport, it started pushing everything to the bottom-right. (That's a new one!) Blah, blah, blah, couldn’t find the issue.&lt;/p&gt; &lt;p&gt;So I threw in the towel and looked at the code myself. It missed that the offset was based on the &lt;strong&gt;image center&lt;/strong&gt;. It was calculating the offset from the top-left corner — and the funny thing is, all the relevant code was &lt;em&gt;right there&lt;/em&gt; in front . I literally gave it everything. In fact the original code was clearly zeroing it to center it, but Claude assumed it must be wrong!&lt;/p&gt; &lt;p&gt;Summary: Claude eventually found my local/screen coordinate mix-up (the reason zooming jumped all over the place — the functions themselves were fine, just working with the wrong coordinates), but it didn't figure out the display logic. The offset was from the image center — zero means centered. I assume if I nudged Grok and google right direction, they could eventually find the coordinates issue too. (It actually didn't occurred to me that coordinates mixup was the cause, until after I thought about it...)&lt;/p&gt; &lt;p&gt;Here’s the current state of AI programming with the big boys, in practice:&lt;/p&gt; &lt;p&gt;There’s no way someone who doesn’t already know a thing or two about the project — and general graphics programming — could fix this with AI right now. On their own, all the AIs kept diverging from the right fix, touching half the codebase, when the real fix was just about four lines total.&lt;br /&gt; (correct the screen-to-image coordinates, and when the image fits in the viewport, set the offset to zero — not &lt;code&gt;(viewport - image)/2&lt;/code&gt;, even though the original code has it zeroed - that's introducing a bug!!!)&lt;/p&gt; &lt;p&gt;Still, AI programming is a big WOW to me. But after 25 years of graphics programming, yeah… that still matters (for now) when things go pear-shaped like this.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;:&lt;br /&gt; Tried Deepseek. The good part, found the error at first try without detours!&lt;/p&gt; &lt;blockquote&gt; &lt;p&gt;&amp;quot;Looking at your zoom implementation, I can see the issue. The problem is in the &lt;code&gt;zoomAt&lt;/code&gt; method in &lt;code&gt;Canvas.h&lt;/code&gt; - there's a mismatch between the coordinate systems being used.&lt;/p&gt; &lt;p&gt;In &lt;code&gt;CPaintWnd::OnMouseWheel&lt;/code&gt;, you're passing screen coordinates (&lt;code&gt;pt.x, pt.y&lt;/code&gt;) to &lt;code&gt;zoomAt&amp;quot;&lt;/code&gt;&lt;/p&gt; &lt;/blockquote&gt; &lt;p&gt;That is correct&lt;br /&gt; The slightly bad part: the fix was actually not exactly correct, it didn't correctly figured out which way the screen to local should go - but that would be an easy catch for me normally.&lt;br /&gt; When I prompt it to recheck the calculation, it corrected itself noticing how the screen to client is calculated elsewhere. So good point!&lt;/p&gt; &lt;p&gt;Bad part 2: Just like Claude, inexplicably introduced error down the code. It changed the offset from the original (correct) to wrong. The exact same error Claude did. (Great minds think alike?)&lt;br /&gt; Now even after multiple tries, short of giving it the answer, it could not figure out that part why it changed a working code to non working (it was doing the same as Claude version, zooming out would push the image right bottom)&lt;/p&gt; &lt;p&gt;So in summary 2: DeepSeek in this case performed slightly better than Claude, figuring out the culprit in words (but not in code) at first try. But both introduced a new error.&lt;/p&gt; &lt;p&gt;None of them did however what a proper programmer should do.&lt;br /&gt; Even the correct fix should not be to turn the zoomAt function from canvas class coordinates to viewport coordinates, just to make it work) after all as it is illogical since every other function in canvas class work in canvas coordinates, but simply go back where this code is called from (MouseWheel) and add viewport to canvas translation at that level.&lt;br /&gt; So even a correct fix introduces a bad code. Again win for human programmer. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/FPham"&gt; /u/FPham &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odgyxp/i_asked_grok_claude_chatgpt_and_google_to_fix_my/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odgyxp/i_asked_grok_claude_chatgpt_and_google_to_fix_my/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odgyxp/i_asked_grok_claude_chatgpt_and_google_to_fix_my/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T18:51:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1od5dxw</id>
    <title>I created a corporate-level chat UI with advanced features</title>
    <updated>2025-10-22T11:14:42+00:00</updated>
    <author>
      <name>/u/BlueLemonPixel</name>
      <uri>https://old.reddit.com/user/BlueLemonPixel</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"&gt; &lt;img alt="I created a corporate-level chat UI with advanced features" src="https://external-preview.redd.it/dPYWmiXWuS054Q7CG9UBLqSEshctiMSWamOBef2b2Gs.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=31fcc76b887198eea07c204089b8d5f4a93bc082" title="I created a corporate-level chat UI with advanced features" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BlueLemonPixel"&gt; /u/BlueLemonPixel &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/25jaz4q9cnwf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od5dxw/i_created_a_corporatelevel_chat_ui_with_advanced/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T11:14:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1od4wj4</id>
    <title>2025 Skynet is released in beta version</title>
    <updated>2025-10-22T10:48:00+00:00</updated>
    <author>
      <name>/u/Max-HWN</name>
      <uri>https://old.reddit.com/user/Max-HWN</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"&gt; &lt;img alt="2025 Skynet is released in beta version" src="https://preview.redd.it/nstd6t1x7nwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fe7be5c4c7050b73bdeb33c732a3875526215c72" title="2025 Skynet is released in beta version" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So, if you are afraid of AI taking over, we still have a lot of time 😂&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Max-HWN"&gt; /u/Max-HWN &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/nstd6t1x7nwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od4wj4/2025_skynet_is_released_in_beta_version/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T10:48:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1odbwjj</id>
    <title>LFM2-VL 3B released today</title>
    <updated>2025-10-22T15:46:03+00:00</updated>
    <author>
      <name>/u/cruncherv</name>
      <uri>https://old.reddit.com/user/cruncherv</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;New &lt;strong&gt;LFM2-VL 3B&lt;/strong&gt; version released by LiquidAI today.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge"&gt;Blog post&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B"&gt;HuggingFace &lt;/a&gt;page&lt;/li&gt; &lt;li&gt;Available quant: &lt;a href="https://huggingface.co/LiquidAI/LFM2-VL-3B-GGUF"&gt;GGUF&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th align="left"&gt;&lt;strong&gt;Model&lt;/strong&gt;&lt;/th&gt; &lt;th align="left"&gt;Average&lt;/th&gt; &lt;th align="left"&gt;MMStar&lt;/th&gt; &lt;th align="left"&gt;MMMU (val)&lt;/th&gt; &lt;th align="left"&gt;MathVista&lt;/th&gt; &lt;th align="left"&gt;BLINK&lt;/th&gt; &lt;th align="left"&gt;InfoVQA (val)&lt;/th&gt; &lt;th align="left"&gt;MMBench (dev en)&lt;/th&gt; &lt;th align="left"&gt;OCRBench&lt;/th&gt; &lt;th align="left"&gt;POPE&lt;/th&gt; &lt;th align="left"&gt;RealWorldQA&lt;/th&gt; &lt;th align="left"&gt;MME&lt;/th&gt; &lt;th align="left"&gt;MM-IFEval&lt;/th&gt; &lt;th align="left"&gt;SEEDBench&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;InternVL3_5-2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.63&lt;/td&gt; &lt;td align="left"&gt;57.67&lt;/td&gt; &lt;td align="left"&gt;51.78&lt;/td&gt; &lt;td align="left"&gt;61.6&lt;/td&gt; &lt;td align="left"&gt;50.97&lt;/td&gt; &lt;td align="left"&gt;69.29&lt;/td&gt; &lt;td align="left"&gt;78.18&lt;/td&gt; &lt;td align="left"&gt;834&lt;/td&gt; &lt;td align="left"&gt;87.17&lt;/td&gt; &lt;td align="left"&gt;60.78&lt;/td&gt; &lt;td align="left"&gt;2,128.83&lt;/td&gt; &lt;td align="left"&gt;47.31&lt;/td&gt; &lt;td align="left"&gt;75.41&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;Qwen2.5-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.61&lt;/td&gt; &lt;td align="left"&gt;56.13&lt;/td&gt; &lt;td align="left"&gt;51.67&lt;/td&gt; &lt;td align="left"&gt;62.5&lt;/td&gt; &lt;td align="left"&gt;48.97&lt;/td&gt; &lt;td align="left"&gt;76.12&lt;/td&gt; &lt;td align="left"&gt;80.41&lt;/td&gt; &lt;td align="left"&gt;824&lt;/td&gt; &lt;td align="left"&gt;86.17&lt;/td&gt; &lt;td align="left"&gt;65.23&lt;/td&gt; &lt;td align="left"&gt;2,163.29&lt;/td&gt; &lt;td align="left"&gt;38.62&lt;/td&gt; &lt;td align="left"&gt;73.88&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;InternVL3-2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;66.46&lt;/td&gt; &lt;td align="left"&gt;61.1&lt;/td&gt; &lt;td align="left"&gt;48.7&lt;/td&gt; &lt;td align="left"&gt;57.6&lt;/td&gt; &lt;td align="left"&gt;53.1&lt;/td&gt; &lt;td align="left"&gt;66.1&lt;/td&gt; &lt;td align="left"&gt;81.1&lt;/td&gt; &lt;td align="left"&gt;831&lt;/td&gt; &lt;td align="left"&gt;90.1&lt;/td&gt; &lt;td align="left"&gt;65.1&lt;/td&gt; &lt;td align="left"&gt;2,186.40&lt;/td&gt; &lt;td align="left"&gt;38.49&lt;/td&gt; &lt;td align="left"&gt;74.95&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;SmolVLM2-2.2B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;54.85&lt;/td&gt; &lt;td align="left"&gt;46&lt;/td&gt; &lt;td align="left"&gt;41.6&lt;/td&gt; &lt;td align="left"&gt;51.5&lt;/td&gt; &lt;td align="left"&gt;42.3&lt;/td&gt; &lt;td align="left"&gt;37.75&lt;/td&gt; &lt;td align="left"&gt;69.24&lt;/td&gt; &lt;td align="left"&gt;725&lt;/td&gt; &lt;td align="left"&gt;85.1&lt;/td&gt; &lt;td align="left"&gt;57.5&lt;/td&gt; &lt;td align="left"&gt;1792.5&lt;/td&gt; &lt;td align="left"&gt;19.42&lt;/td&gt; &lt;td align="left"&gt;71.3&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td align="left"&gt;&lt;strong&gt;LFM2-VL-3B&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.31&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;57.73&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;45.33&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;62.2&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.03&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;67.37&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;79.81&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;822&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;89.01&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;71.37&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;2,050.90&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;51.83&lt;/strong&gt;&lt;/td&gt; &lt;td align="left"&gt;&lt;strong&gt;76.55&lt;/strong&gt;&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;Table from: &lt;a href="http://liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge"&gt;liquid.ai/blog/lfm2-vl-3b-a-new-efficient-vision-language-for-the-edge&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/cruncherv"&gt; /u/cruncherv &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odbwjj/lfm2vl_3b_released_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T15:46:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1odf249</id>
    <title>Ling-1T is very impressive – why are there no independent benchmarks?</title>
    <updated>2025-10-22T17:40:56+00:00</updated>
    <author>
      <name>/u/Snail_Inference</name>
      <uri>https://old.reddit.com/user/Snail_Inference</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Today, I finally had the chance to run some tests with ubergarm’s GGUF version of Ling-1T: &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/ubergarm/Ling-1T-GGUF"&gt;Hugging Face – Ling-1T-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I focused on mathematical and reasoning tasks, and I have to say: I’m genuinely impressed. I only used IQ2_K-quants and Ling-1T solved every problem I threw at it, while keeping costs low thanks to its minimal token usage.&lt;/p&gt; &lt;p&gt;But: I can’t find &lt;strong&gt;any&lt;/strong&gt; independent benchmarks. No results on Artificial Analysis, LiveBench, Aider’s LLM Leaderboard, EQ-Bench… nothing beyond anecdotal impressions. &lt;/p&gt; &lt;p&gt;What are your thoughts? Any ideas why this model seems to fly under the radar?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Snail_Inference"&gt; /u/Snail_Inference &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf249/ling1t_is_very_impressive_why_are_there_no/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odf249/ling1t_is_very_impressive_why_are_there_no/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odf249/ling1t_is_very_impressive_why_are_there_no/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T17:40:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1od1hw4</id>
    <title>hey Z.ai, two weeks was yesterday</title>
    <updated>2025-10-22T07:13:00+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"&gt; &lt;img alt="hey Z.ai, two weeks was yesterday" src="https://preview.redd.it/lg6u60lj5mwf1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fb37b472c5a42bbe348ff5652a5ce811e269f95d" title="hey Z.ai, two weeks was yesterday" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/lg6u60lj5mwf1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od1hw4/hey_zai_two_weeks_was_yesterday/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T07:13:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1odg6pz</id>
    <title>olmoOCR 2 released, big quality improvements, fully open training data and code</title>
    <updated>2025-10-22T18:22:09+00:00</updated>
    <author>
      <name>/u/whistling_frank</name>
      <uri>https://old.reddit.com/user/whistling_frank</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odg6pz/olmoocr_2_released_big_quality_improvements_fully/"&gt; &lt;img alt="olmoOCR 2 released, big quality improvements, fully open training data and code" src="https://external-preview.redd.it/jMHnzDUDDsA1xIrP_vxD1Z6TLTLk5mgpCRd-v7PwCn4.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=60397e0ca48d71f38e4f50d1bb3a4a5210618f9a" title="olmoOCR 2 released, big quality improvements, fully open training data and code" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Given the interest in OCR models recently, Ai2's release today should be on your radar. The weights, training data, and training code are all open, and you can try it for free here:&lt;br /&gt; &lt;a href="https://olmocr.allenai.org/"&gt;https://olmocr.allenai.org/&lt;/a&gt; &lt;/p&gt; &lt;p&gt;📚 Blog: &lt;a href="https://allenai.org/blog/olmocr-2"&gt;https://allenai.org/blog/olmocr-2&lt;/a&gt;&lt;/p&gt; &lt;p&gt;💻 Model: &lt;a href="https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8"&gt;https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/whistling_frank"&gt; /u/whistling_frank &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://allenai.org/blog/olmocr-2"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odg6pz/olmoocr_2_released_big_quality_improvements_fully/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odg6pz/olmoocr_2_released_big_quality_improvements_fully/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T18:22:09+00:00</published>
  </entry>
  <entry>
    <id>t3_1od8fz0</id>
    <title>YES! Super 80b for 8gb VRAM - Qwen3-Next-80B-A3B-Instruct-GGUF</title>
    <updated>2025-10-22T13:34:41+00:00</updated>
    <author>
      <name>/u/Mangleus</name>
      <uri>https://old.reddit.com/user/Mangleus</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So amazing to be able to run this beast on a 8GB VRAM laptop &lt;a href="https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF"&gt;https://huggingface.co/lefromage/Qwen3-Next-80B-A3B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Note that this is not yet supported by latest llama.cpp so you need to compile the non-official version as shown in the link above. (Do not forget to add GPU support when compiling). &lt;/p&gt; &lt;p&gt;Have fun! &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Mangleus"&gt; /u/Mangleus &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1od8fz0/yes_super_80b_for_8gb_vram/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T13:34:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1odk11r</id>
    <title>Strix Halo vs DGX Spark - Initial Impressions (long post with TL;DR at the end)</title>
    <updated>2025-10-22T20:46:12+00:00</updated>
    <author>
      <name>/u/Eugr</name>
      <uri>https://old.reddit.com/user/Eugr</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There are a lot of separate posts about Strix Halo and DGX Spark, but not too many direct comparisons from the people who are actually going to use them for work.&lt;/p&gt; &lt;p&gt;So, after getting Strix Halo and later DGX Spark, decided to compile my initial impressions after using both Strix Halo (GMKTek Evo x2 128GB) and NVidia DGX Spark as an AI developer, in case it would be useful to someone.&lt;/p&gt; &lt;h1&gt;Hardware&lt;/h1&gt; &lt;p&gt;DGX Spark is probably the most minimalist mini-PC I've ever used. &lt;/p&gt; &lt;p&gt;It has absolutely no LEDs, not even in the LAN port, and on/off switch is a button, so unless you ping it over the network or hook up a display, good luck guessing if this thing is on. All ports are in the back, there is no Display Port, only a single HDMI port, USB-C (power only), 3x USB-C 3.2 gen 2 ports, 10G ethernet port and 2x QSFP ports.&lt;/p&gt; &lt;p&gt;The air intake is in the front and exhaust is in the back. It is quiet for the most part, but the fan is quite audible when it's on (but quieter than my GMKTek).&lt;/p&gt; &lt;p&gt;It has a single 4TB PciE 5.0x4 M.2 2242 SSD - SAMSUNG MZALC4T0HBL1-00B07 which I couldn't find anywhere for sale in 2242 form factor, only 2280 version, but DGX Spark only takes 2242 drives. I wish they went with standard 2280 - weird decision, given that it's a mini-PC, not a laptop or tablet. Who cares if the motherboard is an inch longer!&lt;/p&gt; &lt;p&gt;The performance seems good, and gives me 4240.64 MB/sec vs 3118.53 MB/sec on my GMKTek (as measured by hdparm).&lt;/p&gt; &lt;p&gt;It is user replaceable, but there is only one slot, accessible from the bottom of the device. You need to take the magnetic plate off and there are some access screws underneath. &lt;/p&gt; &lt;p&gt;The unit is made of metal, and gets quite hot during high loads, but not unbearable hot like some reviews mentioned. Cools down quickly, though (metal!).&lt;/p&gt; &lt;p&gt;The CPU is 20 core ARM with 10 performance and 10 efficiency cores. I didn't benchmark them, but other reviews CPU show performance similar to Strix Halo.&lt;/p&gt; &lt;h1&gt;Initial Setup&lt;/h1&gt; &lt;p&gt;DGX Spark comes with DGX OS pre-installed (more on this later). You can set it up interactively using keyboard/mouse/display or in headless mode via WiFi hotspot that it creates.&lt;/p&gt; &lt;p&gt;I tried to set it up by connecting my trusted Logitech keyboard/trackpad combo that I use to set up pretty much all my server boxes, but once it booted up, it displayed &amp;quot;Connect the keyboard&amp;quot; message and didn't let me proceed any further. Trackpad portion worked, and volume keys on the keyboard also worked! I rebooted, and was able to enter BIOS (by pressing Esc) just fine, and the keyboard was fully functioning there!&lt;/p&gt; &lt;p&gt;BTW, it has AMI BIOS, but doesn't expose anything interesting other than networking and boot options.&lt;/p&gt; &lt;p&gt;Booting into DGX OS resulted in the same problem. After some googling, I figured that it shipped with a borked kernel that broke Logitech unified setups, so I decided to proceed in a headless mode.&lt;/p&gt; &lt;p&gt;Connected to the Wifi hotspot from my Mac (hotspot SSID/password are printed on a sticker on top of the quick start guide) and was able to continue set up there, which was pretty smooth, other than Mac spamming me with &amp;quot;connect to internet&amp;quot; popup every minute or so. It then proceeded to update firmware and OS packages, which took about 30 minutes, but eventually finished, and after that my Logitech keyboard worked just fine.&lt;/p&gt; &lt;h1&gt;Linux Experience&lt;/h1&gt; &lt;p&gt;DGX Spark runs DGX OS 7.2.3 which is based on Ubuntu 24.04.3 LTS, but uses NVidia's custom kernel, and an older one than mainline Ubuntu LTS uses. So instead of 6.14.x you get 6.11.0-1016-nvidia.&lt;/p&gt; &lt;p&gt;It comes with CUDA 13.0 development kit and NVidia drivers (580.95.05) pre-installed. It also has NVidia's container toolkit that includes docker, and GPU passthrough works well.&lt;/p&gt; &lt;p&gt;Other than that, it's a standard Ubuntu Desktop installation, with GNOME and everything.&lt;/p&gt; &lt;p&gt;SSHd is enabled by default, so after headless install you can connect to it immediately without any extra configuration. &lt;/p&gt; &lt;p&gt;RDP remote desktop doesn't work currently - it connects, but display output is broken.&lt;/p&gt; &lt;p&gt;I tried to boot from Fedora 43 Beta Live USB, and it worked, sort of. First, you need to disable Secure Boot in BIOS. Then, it boots only in &amp;quot;basic graphics mode&amp;quot;, because built-in nvidia drivers don't recognize the chipset. It also throws other errors complaining about chipset, processor cores, etc. &lt;/p&gt; &lt;p&gt;I think I'll try to install it to an external SSD and see if NVidia standard drivers will recognize the chip. There is hope:&lt;/p&gt; &lt;p&gt;============== PLATFORM INFO: ============== IOMMU: Pass-through or enabled Nvidia Driver Info Status: Supported(Nvidia Open Driver Installed) Cuda Driver Version Installed: 13000 Platform: NVIDIA_DGX_Spark, Arch: aarch64(Linux 6.11.0-1016-nvidia) Platform verification succeeded&lt;/p&gt; &lt;p&gt;As for Strix Halo, it's an x86 PC, so you can run any distro you want. I chose Fedora 43 Beta, currently running with kernel 6.17.3-300.fc43.x86_64. Smooth sailing, up-to-date packages.&lt;/p&gt; &lt;h1&gt;Llama.cpp experience&lt;/h1&gt; &lt;h2&gt;DGX Spark&lt;/h2&gt; &lt;p&gt;You need to build it from source as there is no CUDA ARM build, but compiling llama.cpp was very straightforward - CUDA toolkit is already installed, just need to install development tools and it compiles just like on any other system with NVidia GPU. Just follow the instructions, no surprises.&lt;/p&gt; &lt;p&gt;However, when I ran the benchmarks, I ran into two issues.&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The model loading was VERY slow. It took 1 minute 40 seconds to load gpt-oss-120b. For comparison, it takes 22 seconds to load on Strix Halo (both from cold, memory cache flushed).&lt;/li&gt; &lt;li&gt;I wasn't getting the same results as ggerganov in this &lt;a href="https://github.com/ggml-org/llama.cpp/discussions/16578"&gt;thread&lt;/a&gt;. While PP was pretty impressive for such a small system, TG was matching or even slightly worse than my Strix Halo setup with ROCm.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;For instance, here are my Strix Halo numbers, compiled with ROCm 7.10.0a20251017, llama.cpp build 03792ad9 (6816), HIP only, no rocWMMA:&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash build/bin/llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 -ub 2048 &lt;/code&gt;&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 999.59 ± 4.31&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 47.49 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 824.37 ± 1.16&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 44.23 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 703.42 ± 1.54&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 42.52 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 514.89 ± 3.86&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 39.71 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 348.59 ± 2.11&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 35.39 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;The same command on Spark gave me this:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1816.00 ± 11.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 44.74 ± 0.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 1763.75 ± 6.43&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 42.69 ± 0.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 1695.29 ± 11.56&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 40.91 ± 0.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 1512.65 ± 6.35&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 38.61 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 1250.55 ± 5.21&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 34.66 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried enabling Unified Memory switch (GGML_CUDA_ENABLE_UNIFIED_MEMORY=1) - it improved model loading, but resulted in even worse performance.&lt;/p&gt; &lt;p&gt;I reached out to ggerganov, and he suggested disabling mmap. I thought I tried it, but apparently not. Well, that fixed it. Model loading improved too - now taking 56 seconds from cold and 23 seconds when it's still in cache.&lt;/p&gt; &lt;p&gt;Updated numbers:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1939.32 ± 4.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 56.33 ± 0.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 1832.04 ± 5.58&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 52.63 ± 0.12&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 1738.07 ± 5.93&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 48.60 ± 0.20&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 1525.71 ± 12.34&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 45.01 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 1242.35 ± 5.64&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;CUDA &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 39.10 ± 0.09&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;As you can see, much better performance both in PP and TG. &lt;/p&gt; &lt;p&gt;As for Strix Halo, mmap/no-mmap doesn't make any difference there.&lt;/p&gt; &lt;h2&gt;Strix Halo&lt;/h2&gt; &lt;p&gt;On Strix Halo, llama.cpp experience is... well, a bit turbulent. &lt;/p&gt; &lt;p&gt;You can download a pre-built version for Vulkan, and it works, but the performance is a mixed bag. TG is pretty good, but PP is not great.&lt;/p&gt; &lt;p&gt;&lt;code&gt;bash build/bin/llama-bench -m ~/.cache/llama.cpp/ggml-org_gpt-oss-120b-GGUF_gpt-oss-120b-mxfp4-00001-of-00003.gguf -fa 1 -d 0,4096,8192,16384,32768 -p 2048 -n 32 --mmap 0 -ngl 999 -ub 1024 &lt;/code&gt; &lt;strong&gt;NOTE&lt;/strong&gt;: Vulkan likes batch size of 1024 the most, unlike ROCm that likes 2048 better.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 526.54 ± 4.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 52.64 ± 0.08&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 438.85 ± 0.76&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 48.21 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 356.28 ± 4.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 45.90 ± 0.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 210.17 ± 2.53&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 42.64 ± 0.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 138.79 ± 9.47&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;Vulkan &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 36.18 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;I tried toolboxes from kyuz0, and some of them were better, but I still felt that I could squeeze more juice out of it. All of them suffered from significant performance degradation when the context was filling up.&lt;/p&gt; &lt;p&gt;Then I tried to compile my own using the latest ROCm &lt;a href="https://therock-nightly-tarball.s3.amazonaws.com/therock-dist-linux-gfx1151-7.10.0a20251017.tar.gz"&gt;build&lt;/a&gt; from TheRock (on that date).&lt;/p&gt; &lt;p&gt;I also build &lt;a href="https://github.com/ROCm/rocWMMA.git"&gt;rocWMMA&lt;/a&gt; as recommended by kyoz0 (more on that later).&lt;/p&gt; &lt;p&gt;Llama.cpp compiled without major issues - I had to configure the paths properly, but other than that, it just worked. The PP increased dramatically, but TG decreased.&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend &lt;/th&gt; &lt;th align="right"&gt;ngl&lt;/th&gt; &lt;th align="right"&gt;n_ubatch&lt;/th&gt; &lt;th align="right"&gt;fa&lt;/th&gt; &lt;th align="right"&gt;mmap&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1030.71 ± 2.26&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 47.84 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 802.36 ± 6.96&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 39.09 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 615.27 ± 2.18&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 33.34 ± 0.05&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 409.25 ± 0.67&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 25.86 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 228.04 ± 0.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;999&lt;/td&gt; &lt;td align="right"&gt; 2048&lt;/td&gt; &lt;td align="right"&gt; 1&lt;/td&gt; &lt;td align="right"&gt; 0&lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 18.07 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;But the biggest issue is significant performance degradation with long context, much more than you'd expect.&lt;/p&gt; &lt;p&gt;Then I stumbled upon Lemonade SDK and their pre-built llama.cpp. Ran that one, and got much better results across the board. TG was still below Vulkan, but PP was decent and degradation wasn't as bad:&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model&lt;/th&gt; &lt;th align="right"&gt;size&lt;/th&gt; &lt;th align="right"&gt;params&lt;/th&gt; &lt;th align="right"&gt;test&lt;/th&gt; &lt;th align="right"&gt;t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048&lt;/td&gt; &lt;td align="right"&gt;999.20 ± 3.44&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32&lt;/td&gt; &lt;td align="right"&gt;47.53 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt;826.63 ± 9.09&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt;44.24 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt;702.66 ± 2.15&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt;42.56 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt;505.85 ± 1.33&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt;39.82 ± 0.03&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt;343.06 ± 2.07&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE&lt;/td&gt; &lt;td align="right"&gt;59.02 GiB&lt;/td&gt; &lt;td align="right"&gt;116.83 B&lt;/td&gt; &lt;td align="right"&gt;tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt;35.50 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So I looked at their compilation options and noticed that they build without rocWMMA. So, I did the same and got similar performance too!&lt;/p&gt; &lt;table&gt;&lt;thead&gt; &lt;tr&gt; &lt;th&gt;model &lt;/th&gt; &lt;th align="right"&gt; size&lt;/th&gt; &lt;th align="right"&gt; params&lt;/th&gt; &lt;th&gt;backend&lt;/th&gt; &lt;th align="right"&gt; test&lt;/th&gt; &lt;th align="right"&gt; t/s&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt;&lt;tbody&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048&lt;/td&gt; &lt;td align="right"&gt; 1000.93 ± 1.23&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32&lt;/td&gt; &lt;td align="right"&gt; 47.46 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 827.34 ± 1.99&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d4096&lt;/td&gt; &lt;td align="right"&gt; 44.20 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; pp2048 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 701.68 ± 2.36&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d8192&lt;/td&gt; &lt;td align="right"&gt; 42.39 ± 0.04&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 503.49 ± 0.90&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d16384&lt;/td&gt; &lt;td align="right"&gt; 39.61 ± 0.02&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt;pp2048 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 344.36 ± 0.80&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;gpt-oss 120B MXFP4 MoE &lt;/td&gt; &lt;td align="right"&gt; 59.02 GiB&lt;/td&gt; &lt;td align="right"&gt; 116.83 B&lt;/td&gt; &lt;td&gt;ROCm &lt;/td&gt; &lt;td align="right"&gt; tg32 @ d32768&lt;/td&gt; &lt;td align="right"&gt; 35.32 ± 0.01&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt;&lt;/table&gt; &lt;p&gt;So far that's the best I could get from Strix Halo. It's very usable for text generation tasks.&lt;/p&gt; &lt;p&gt;Also, wanted to touch multi-modal performance. That's where Spark shines. I don't have any specific benchmarks yet, but image processing is much faster on Spark than on Strix Halo, especially in vLLM.&lt;/p&gt; &lt;h1&gt;VLLM Experience&lt;/h1&gt; &lt;p&gt;Haven't had a chance to do extensive testing here, but wanted to share some early thoughts.&lt;/p&gt; &lt;h2&gt;DGX Spark&lt;/h2&gt; &lt;p&gt;First, I tried to just build vLLM from the source as usual. The build was successful, but it failed with the following error: ptxas fatal : Value 'sm_121a' is not defined for option 'gpu-name'&lt;/p&gt; &lt;p&gt;I decided not to spend too much time on this for now, and just launched vLLM container that NVidia provides through their Docker repository. It is built for DGX Spark, so supports it out of the box.&lt;/p&gt; &lt;p&gt;However, it has version 0.10.1, so I wasn't able to run Qwen3-VL there.&lt;/p&gt; &lt;p&gt;Now, they put the source code inside the container, but it wasn't a git repository - probably contains some NVidia-specific patches - I'll need to see if those could be merged into main vllm code.&lt;/p&gt; &lt;p&gt;So I just checked out vllm main branch and proceeded to build with existing pytorch as usual. This time I was able to run it and launch qwen3-vl models just fine. Both dense and MOE work. I tried FP4 and AWQ quants - everything works, no need to disable CUDA graphs.&lt;/p&gt; &lt;p&gt;The performance is decent - I still need to run some benchmarks, but image processing is very fast.&lt;/p&gt; &lt;h2&gt;Strix Halo&lt;/h2&gt; &lt;p&gt;Unlike llama.cpp that just works, vLLM experience on Strix Halo is much more limited.&lt;/p&gt; &lt;p&gt;My goal was to run Qwen3-VL models that are not supported by llama.cpp yet, so I needed to build 0.11.0 or later. There are some existing containers/toolboxes for earlier versions, but I couldn't use them.&lt;/p&gt; &lt;p&gt;So, I installed ROCm pyTorch libraries from TheRock, some &lt;a href="https://github.com/kyuz0/amd-strix-halo-vllm-toolboxes/blob/main/Dockerfile.vllm-therock-gfx1151-aotriton"&gt;patches&lt;/a&gt; from kyoz0 toolboxes to avoid amdsmi package crash, &lt;a href="https://github.com/ROCm/flash-attention.git"&gt;ROCm FlashAttention&lt;/a&gt; and then just followed vLLM standard installation instructions with existing pyTorch.&lt;/p&gt; &lt;p&gt;I was able to run Qwen3VL dense models with decent (for dense models) speeds, although initialization takes quite some time until you reduce -max-num-seqs to 1 and set tp 1. The image processing is very slow though, much slower than llama.cpp for the same image, but the token generation is about what you'd expect from it.&lt;/p&gt; &lt;p&gt;Again, model loading is faster than Spark for some reason (I'd expect other way around given faster SSD in Spark and slightly faster memory).&lt;/p&gt; &lt;p&gt;I'm going to rebuild vLLM and re-test/benchmark later.&lt;/p&gt; &lt;p&gt;Some observations: - FP8 models don't work - they hang on WARNING 10-22 12:55:04 [fp8_utils.py:785] Using default W8A8 Block FP8 kernel config. Performance might be sub-optimal! Config file not found at /home/eugr/vllm/vllm/vllm/model_executor/layers/quantization/utils/configs/N=6144,K=2560,device_name=Radeon_8060S_Graphics,dtype=fp8_w8a8,block_shape=[128,128].json - You need to use --enforce-eager, as CUDA graphs crash vLLM. Sometimes it works, but mostly crashes. - Even with --enforce-eager, there are some HIP-related crashes here and there occasionally. - AWQ models work, both 4-bit and 8-bit, but only dense ones. AWQ MOE quants require Marlin kernel that is not available for ROCm.&lt;/p&gt; &lt;h1&gt;Conclusion / TL;DR&lt;/h1&gt; &lt;p&gt;Summary of my initial impressions:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;DGX Spark is an interesting beast for sure. &lt;ul&gt; &lt;li&gt;Limited extensibility - no USB-4, only one M.2 slot, and it's 2242.&lt;/li&gt; &lt;li&gt;But has 200Gbps network interface.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;It's a first generation of such devices, so there are some annoying bugs and incompatibilities.&lt;/li&gt; &lt;li&gt;Inference wise, the token generation is nearly identical to Strix Halo both in llama.cpp and vllm, but prompt processing is 2-5x higher than Strix Halo. &lt;ul&gt; &lt;li&gt;Strix Halo performance in prompt processing degrades much faster with context.&lt;/li&gt; &lt;li&gt;Image processing takes longer, especially with vLLM.&lt;/li&gt; &lt;li&gt;Model loading into unified RAM is slower on DGX Spark for some reason, both in llama.cpp and vLLM.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Even though vLLM included gfx1151 in the supported configurations, it still requires some hacks to compile it. &lt;ul&gt; &lt;li&gt;And even then, the experience is suboptimal. Initialization time is slow, it crashes, FP8 doesn't work, AWQ for MOE doesn't work.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;If you are an AI developer who uses transformers/pyTorch or you need vLLM - you are better off with DGX Spark (or just a normal GPU build).&lt;/li&gt; &lt;li&gt;If you want a power-efficient inference server that can run gpt-oss and similar MOE at decent speeds, and don't need to process images often, Strix Halo is the way to go.&lt;/li&gt; &lt;li&gt;If you want a general purpose machine, Strix Halo wins too.&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Eugr"&gt; /u/Eugr &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odk11r/strix_halo_vs_dgx_spark_initial_impressions_long/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odk11r/strix_halo_vs_dgx_spark_initial_impressions_long/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odk11r/strix_halo_vs_dgx_spark_initial_impressions_long/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T20:46:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1odi1c0</id>
    <title>Meta lays off 600 employees within AI unit</title>
    <updated>2025-10-22T19:30:58+00:00</updated>
    <author>
      <name>/u/a_slay_nub</name>
      <uri>https://old.reddit.com/user/a_slay_nub</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"&gt; &lt;img alt="Meta lays off 600 employees within AI unit" src="https://external-preview.redd.it/J07EauFcN4nV9LOcRS0eXwdDIcxd3OiFJdlO3Bhl-Rc.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=b63bb44a8b15efeaf11cd6f80af319b8caa01688" title="Meta lays off 600 employees within AI unit" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/a_slay_nub"&gt; /u/a_slay_nub &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.cnbc.com/2025/10/22/meta-layoffs-ai.html"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1odi1c0/meta_lays_off_600_employees_within_ai_unit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T19:30:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1oda8mk</id>
    <title>Qwen team is helping llama.cpp again</title>
    <updated>2025-10-22T14:44:44+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt; &lt;img alt="Qwen team is helping llama.cpp again" src="https://preview.redd.it/dh1iaky2eowf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=addcf456730d4f5ec04b561980fa9d74dfb18d96" title="Qwen team is helping llama.cpp again" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/dh1iaky2eowf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oda8mk/qwen_team_is_helping_llamacpp_again/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-22T14:44:44+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1obqkpe</id>
    <title>Best Local LLMs - October 2025</title>
    <updated>2025-10-20T19:06:06+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;Welcome to the first monthly &amp;quot;Best Local LLMs&amp;quot; post!&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Share what your favorite models are right now &lt;strong&gt;and why.&lt;/strong&gt; Given the nature of the beast in evaluating LLMs (untrustworthiness of benchmarks, immature tooling, intrinsic stochasticity), please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt; &lt;ol&gt; &lt;li&gt;General&lt;/li&gt; &lt;li&gt;Agentic/Tool Use&lt;/li&gt; &lt;li&gt;Coding&lt;/li&gt; &lt;li&gt;Creative Writing/RP&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;(look for the top level comments for each Application and please thread your responses under that)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1obqkpe/best_local_llms_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-20T19:06:06+00:00</published>
  </entry>
</feed>
