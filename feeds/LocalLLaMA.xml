<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-29T04:43:14+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1oi7k25</id>
    <title>HF Space to help create the -ot flags in llama.cpp</title>
    <updated>2025-10-28T12:07:11+00:00</updated>
    <author>
      <name>/u/bullerwins</name>
      <uri>https://old.reddit.com/user/bullerwins</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt; &lt;img alt="HF Space to help create the -ot flags in llama.cpp" src="https://external-preview.redd.it/g9TtSkJliQx_HIwoIut_ECyFVGHzqlshzt1AT9ZxIjA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=711e4dcc38b2e4a206b6b1d8e5ce67eae349c3be" title="HF Space to help create the -ot flags in llama.cpp" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi!&lt;/p&gt; &lt;p&gt;Mainly as I was frustrated when manually assigning the layers with the -of flag in llama.cpp and ik_llama.cpp and when increasing maybe just 1 layer in a previous gpu i had to increase the number in all the rest of the gpu, I created a Hugging face space to help with that.&lt;/p&gt; &lt;p&gt;It lets you select the number of GPUs, the size of the model weights and the number of layers and it automatically tries to assign how many layers would fit in your gpu size &lt;strong&gt;on an empty context.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Then if you want to fit more context either switch to manual and reduce 1-2 layers per gpu, or increase the size in GB of the model a bit.&lt;/p&gt; &lt;p&gt;Example:&lt;br /&gt; I want to load &lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF"&gt;Bartowski GLM-4.6&lt;/a&gt; in Q6 in my rig (rtx6000, 2x5090, 4x3090) and I have 256GB VRAM and the quant takes 294 GB in Q6 as you can see now in HF if you go to the folder:&lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K"&gt;https://huggingface.co/bartowski/zai-org_GLM-4.6-GGUF/tree/main/zai-org_GLM-4.6-Q6_K&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837"&gt;https://preview.redd.it/cjc7oe2jeuxf1.png?width=798&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=17433d663ad544eafa7547b47a7d1b917d069837&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And GLM-4.6 has 92 layers as you can see here: &lt;a href="https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31"&gt;https://huggingface.co/zai-org/GLM-4.6/blob/main/config.json#L31&lt;/a&gt;&lt;/p&gt; &lt;p&gt;So fill the settings as such:&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8"&gt;https://preview.redd.it/qdyznyd7euxf1.png?width=3418&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=75b3b577c4b9058ce6409be57d82a6b0db40a6e8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;And that actually loads using 2048 context and the GPU are all almost at a 100% vram usage which is what we want.&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442"&gt;https://preview.redd.it/qcf0ixxbeuxf1.png?width=1670&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a62cfeec20a34028e8e6fbe0b7a9f99b15bb8442&lt;/a&gt;&lt;/p&gt; &lt;p&gt;If I reduce one layer per GPU to quickly allow more vram for ctx, I can now load 32K context. But checking the GPU usage I might be able to assign one more layer to the rtx6000.&lt;/p&gt; &lt;p&gt;So the final command would be:&lt;/p&gt; &lt;p&gt;&lt;code&gt;CUDA_VISIBLE_DEVICES=2,0,6,1,3,4,5 ./build/bin/llama-server \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--model /mnt/llms/models/bartowski/zai-org_GLM-4.6-GGUF/zai-org_GLM-4.6-Q6_K/zai-org_GLM-4.6-Q6_K-00001-of-00008.gguf \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--alias glm-4.6 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--ctx-size 32768 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ngl 99 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--host&lt;/code&gt; &lt;a href="http://0.0.0.0"&gt;&lt;code&gt;0.0.0.0&lt;/code&gt;&lt;/a&gt; &lt;code&gt;\&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;--port 5000 \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(3|4|5|6|7|8|9|10|11|12|13|14|15|16|17|18|19|20|21|22|23|24|25|26|27|28|29|30)\.ffn_.*=CUDA0&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(31|32|33|34|35|36|37|38)\.ffn_.*=CUDA1&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(39|40|41|42|43|44|45|46)\.ffn_.*=CUDA2&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(47|48|49|50|51)\.ffn_.*=CUDA3&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(52|53|54|55|56)\.ffn_.*=CUDA4&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(57|58|59|60|61)\.ffn_.*=CUDA5&amp;quot; \&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;-ot &amp;quot;blk\.(62|63|64|65|66)\.ffn_.*=CUDA6&amp;quot; --cpu-moe&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Link to the HF space: &lt;a href="https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool"&gt;https://huggingface.co/spaces/bullerwins/Llamacpp-GPU-Layer-Assignment-Tool&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/bullerwins"&gt; /u/bullerwins &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi7k25/hf_space_to_help_create_the_ot_flags_in_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T12:07:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1oipn6w</id>
    <title>Best way to integrate "memories" i.e things i need lm studio to know before prompts</title>
    <updated>2025-10-28T23:55:19+00:00</updated>
    <author>
      <name>/u/LORD_MDS</name>
      <uri>https://old.reddit.com/user/LORD_MDS</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am hoping to find a system that remembers my porfolio works, email preferences, job preferences, etc like chat gpt does. I am a noob and tried with no luck some chat gpt recommendations that involved terminal. Any help is much appreciated - hoping for a simple memory like on my GPT free plan of emails, shows i work on, my camera preferences, etc etc. thank you!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LORD_MDS"&gt; /u/LORD_MDS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oipn6w/best_way_to_integrate_memories_ie_things_i_need/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oipn6w/best_way_to_integrate_memories_ie_things_i_need/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oipn6w/best_way_to_integrate_memories_ie_things_i_need/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T23:55:19+00:00</published>
  </entry>
  <entry>
    <id>t3_1oinict</id>
    <title>I’m just ever so off. I could use some guidance</title>
    <updated>2025-10-28T22:26:36+00:00</updated>
    <author>
      <name>/u/DisplacedForest</name>
      <uri>https://old.reddit.com/user/DisplacedForest</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi. I’m recognizing that this might be a little bit of an annoying post, but I need a little bit of help. Specifically, I’m trying to run a local… let’s call it a home GPT or something along those lines… that’s agentic for specific tasks and tool calls automatically. I don’t want to have to specify what tool when I type in chat.&lt;/p&gt; &lt;p&gt;I can write SQL queries myself, but if I’m telling it to look something up in Supabase, I don’t want to have to manually say “use this tool.” It should just flow naturally in the conversation.&lt;/p&gt; &lt;p&gt;I’ve tried LM Studio, Ollama, msty.ai… doesn’t seem to matter. I really like LM Studio’s model management and chat UI, but I have to explicitly tell it to use the tool every single time. It’s not making those calls autonomously. That kind of defeats the purpose for me.&lt;/p&gt; &lt;p&gt;What I want is something that knows when to query Supabase via MCP, and when not to. When to use web search, and when not to.&lt;/p&gt; &lt;p&gt;Right now I’m testing different models, but my favorite so far is Qwen3-32B MLX running on LM Studio. I’m just curious how people are getting these kinds of autonomous workflows actually running in the chat UI… without it turning into a really manual process every time.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DisplacedForest"&gt; /u/DisplacedForest &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oinict/im_just_ever_so_off_i_could_use_some_guidance/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oinict/im_just_ever_so_off_i_could_use_some_guidance/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oinict/im_just_ever_so_off_i_could_use_some_guidance/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T22:26:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oio0rz</id>
    <title>Have any sites been developed where collections of LLM tools are hosted?</title>
    <updated>2025-10-28T22:47:27+00:00</updated>
    <author>
      <name>/u/Intelligent-Land1765</name>
      <uri>https://old.reddit.com/user/Intelligent-Land1765</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;This boils down to simply the actual function for the tool on the right side and the JSON description of it on the left. You copy both, you paste them into your own files and or whatever you use and makes the entire function available to the AI. Or is this still a very spread out area?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Intelligent-Land1765"&gt; /u/Intelligent-Land1765 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oio0rz/have_any_sites_been_developed_where_collections/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oio0rz/have_any_sites_been_developed_where_collections/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oio0rz/have_any_sites_been_developed_where_collections/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T22:47:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1oieip0</id>
    <title>Theoretically Scaling Beyond 2 DGX Sparks in a Single Cluster.</title>
    <updated>2025-10-28T16:44:45+00:00</updated>
    <author>
      <name>/u/SIN3R6Y</name>
      <uri>https://old.reddit.com/user/SIN3R6Y</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;First off, let's get into why NVIDIA only supports clustering 2 of these at the moment.&lt;/p&gt; &lt;p&gt;&lt;code&gt;user@spark:~$ lspci | grep Mellanox&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0000:01:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0000:01:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0002:01:00.0 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;br /&gt; &lt;code&gt;0002:01:00.1 Ethernet controller: Mellanox Technologies MT2910 Family [ConnectX-7]&lt;/code&gt;&lt;/p&gt; &lt;p&gt;The cpu is essentially two 10 core compute units married together, each with their own pcie root complex connected to the CX7 at Gen5 x4. Meaning each compute half of the CPU can push roughly 100gbps (200gbps across both complexes), and the CX7 interfaces effectively show up twice.&lt;/p&gt; &lt;p&gt;CPU 1st Half:&lt;br /&gt; enp1s0f0np0 -&amp;gt; port 1&lt;br /&gt; enp1s0f1np1 -&amp;gt; port 2&lt;/p&gt; &lt;p&gt;CPU 2nd Half:&lt;br /&gt; enP2p1s0f0np0 -&amp;gt; port 1&lt;br /&gt; enP2p1s0f1np1 -&amp;gt; port 2&lt;/p&gt; &lt;pre&gt;&lt;code&gt;user@spark:~$ ibdev2netdev rocep1s0f0 port 1 ==&amp;gt; enp1s0f0np0 (Up) rocep1s0f1 port 1 ==&amp;gt; enp1s0f1np1 (Up) roceP2p1s0f0 port 1 ==&amp;gt; enP2p1s0f0np0 (Up) roceP2p1s0f1 port 1 ==&amp;gt; enP2p1s0f1np1 (Up) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NVIDIA docs will basically tell you to ignore the all the second half (enP2) interfaces. This works at 200gbps in a p2p dual spark scenario because NCCL is going to transmit ROCE v1 L2 frames out of all up ROCE interfaces. Doing a direct connection will bring up two of those (one per complex) and it will just work, no ROCE configuration really needed. Ethernet traffic will be limited to about 100gbps out of the single port however.&lt;/p&gt; &lt;p&gt;But, now in my case. I am connecting these sparks over dual 100gbit QSFP28 links to a cluster of NVIDIA sn2010 switches. QSFP28, because no matter what, 200gbps is the absolute maximum the CX7 can do given the PCIe limitations.&lt;/p&gt; &lt;p&gt;To make this work, with ROCE v2 and layer 3 links to the switch. You can set an IP on each half of the complex.&lt;/p&gt; &lt;p&gt;enp1s0f0np0 -&amp;gt; set ip (CPU 1st half CX7 port 1)&lt;br /&gt; enP2p1s0f1np1 - set ip (CPU 2nd half CX7 port 2)&lt;/p&gt; &lt;p&gt;Now, this will break NCCL. NCCL needs some variables tweaked, otherwise it's going to try to use ROCE v1 p2p ports which cannot work in this scenario. Here is an NCCL test that will get 200gbps across both links to a switch.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mpirun -np 2 -H &amp;lt;spark 1 ip&amp;gt;,&amp;lt;spark 2 ip&amp;gt; \ --mca plm_rsh_agent &amp;quot;ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no&amp;quot; \ -x LD_LIBRARY_PATH=$LD_LIBRARY_PATH \ -x UCX_NET_DEVICES=enp1s0f0np0,enP2p1s0f1np1 \ -x NCCL_SOCKET_IFNAME=enp1s0f0np0,enP2p1s0f1np1 \ -x NCCL_SOCKET_FAMILY=AF_INET \ -x NCCL_IB_HCA=rocep1s0f0,roceP2p1s0f1 \ -x OMPI_MCA_btl_tcp_if_include=enp1s0f0np0,enP2p1s0f1np1 \ -x NCCL_IB_GID_INDEX=3 \ -x NCCL_IB_TC=3 \ -x NCCL_IB_MERGE_NICS=1\ $HOME/nccl-tests/build/all_gather_perf -b 16G -e 16G -f 2 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The host IP's above can be the the IP's of the 10g interfaces, NCCL will still discover the CX7 paths but just do IP coordination over the 10g links. Just sure the two sparks are routable to each other over the CX7 or on the same L2 segment. I use static layer 3 routes for this, but for larger setups BGP would also work well here.&lt;/p&gt; &lt;p&gt;These flags restrict the interfaces NCCL sees, forces ROCE v2, merges those nics, and forces the lossless traffic class. In theory, with both CX7 interfaces connected to a switch, you're only scaling limit here with multiple sparks is how many switch ports you have.&lt;/p&gt; &lt;p&gt;To make this more permanent I set these in .profile for the user.&lt;/p&gt; &lt;pre&gt;&lt;code&gt;export CUDA_HOME=&amp;quot;/usr/local/cuda&amp;quot; export MPI_HOME=&amp;quot;/usr/lib/aarch64-linux-gnu/openmpi&amp;quot; export NCCL_HOME=&amp;quot;$HOME/nccl/build/&amp;quot; export LD_LIBRARY_PATH=&amp;quot;$NCCL_HOME/lib:$CUDA_HOME/lib64/:$MPI_HOME/lib:$LD_LIBRARY_PATH&amp;quot; export IP_IF_NAME=enp1s0f0np0,enP2p1s0f1np1 export IB_IF_NAME=rocep1s0f0,roceP2p1s0f1 export UCX_NET_DEVICES=$IP_IF_NAME export NCCL_SOCKET_IFNAME=$IP_IF_NAME export NCCL_SOCKET_FAMILY=AF_INET export NCCL_IB_HCA=$IB_IF_NAME export NCCL_IB_GID_INDEX=3 export NCCL_IB_MERGE_NICS=1 export OMPI_MCA_btl_tcp_if_include=$IP_IF_NAME &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;NCCL Test Results&lt;/p&gt; &lt;pre&gt;&lt;code&gt;# nccl-tests version 2.17.4 nccl-headers=22807 nccl-library=22807 # Collective test starting: all_gather_perf # nThread 1 nGpus 1 minBytes 17179869184 maxBytes 17179869184 step: 2(factor) warmup iters: 1 iters: 20 agg iters: 1 validation: 1 graph: 0 # # Using devices # Rank 0 Group 0 Pid 303712 on spark-1af4 device 0 [000f:01:00] NVIDIA GB10 # Rank 1 Group 0 Pid 166882 on spark-870f device 0 [000f:01:00] NVIDIA GB10 # # out-of-place in-place # size count type redop root time algbw busbw #wrong time algbw busbw #wrong # (B) (elements) (us) (GB/s) (GB/s) (us) (GB/s) (GB/s) 17179869184 2147483648 float none -1 410263 41.88 20.94 0 409388 41.96 20.98 0 # Out of bounds values : 0 OK # Avg bus bandwidth : 20.96 # # Collective test concluded: all_gather_perf &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;EDIT: It's worth noting that with this setup, you are able to get both 200gbps ROCE v2 traffic and 200gbps Ethernet traffic (not at the same time, they share the combined 200GB of throughput). VS the default p2p setup which gives you 200gbps of ROCE v1 traffic and 100gbps of Ethernet traffic.&lt;/p&gt; &lt;p&gt;However, you can't bond the two links in LACP. This is not supported for NCCL. So what I do is layer 3 (hence why I force ROCE v2) use ECMP to get the desired results.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SIN3R6Y"&gt; /u/SIN3R6Y &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oieip0/theoretically_scaling_beyond_2_dgx_sparks_in_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oieip0/theoretically_scaling_beyond_2_dgx_sparks_in_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oieip0/theoretically_scaling_beyond_2_dgx_sparks_in_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T16:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1oinmab</id>
    <title>Anyone running local LLM coding setups on 24GB VRAM laptops? Looking for real-world experiences</title>
    <updated>2025-10-28T22:30:57+00:00</updated>
    <author>
      <name>/u/AmazinglyNatural6545</name>
      <uri>https://old.reddit.com/user/AmazinglyNatural6545</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone&lt;/p&gt; &lt;p&gt;I’m wondering if anyone has real day-to-day experience with local LLM coding on 24GB VRAM? And how do you use it? Cline/Continue in VScode?&lt;/p&gt; &lt;p&gt;Here’s the situation: I’ve been using Claude Code, but it’s getting pretty expensive. The basic plan recently got nerfed — now you only get a few hours of work time before you have to wait for your resources to reset. So I’m looking into local alternatives, even if they’re not as advanced. That’s totally fine — I’m already into local AI stuff, so I am a bit familiar with what to expect. &lt;/p&gt; &lt;p&gt;Right now I’ve got a laptop with an RTX 4080 (12GB VRAM). It’s fine for most AI tasks I run, but not great for coding with LLMs.&lt;/p&gt; &lt;p&gt;For context:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;unfortunately, I can’t use a desktop due to certain circumstances&lt;/li&gt; &lt;li&gt;I also can’t go with Apple since it’s not ideal for things like Stable Diffusion, OCR, etc. and it's expensive as hell. More expensive that non-apple laptop with the same specs. &lt;/li&gt; &lt;li&gt;cloud providers could be expensive in the case of classic permanent usage for work&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I’m thinking about getting a 5090 laptop, but that thing’s insanely expensive, so I’d love to hear some thoughts or real experiences from people who actually run heavy local AI workloads on laptops.&lt;/p&gt; &lt;p&gt;Thanks! 🙏&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AmazinglyNatural6545"&gt; /u/AmazinglyNatural6545 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oinmab/anyone_running_local_llm_coding_setups_on_24gb/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oinmab/anyone_running_local_llm_coding_setups_on_24gb/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oinmab/anyone_running_local_llm_coding_setups_on_24gb/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T22:30:57+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiofso</id>
    <title>Single python script for parakeet-tdt-0.6b-v2/3 live mic transcription (mlx)</title>
    <updated>2025-10-28T23:04:52+00:00</updated>
    <author>
      <name>/u/fullbridgerecctifier</name>
      <uri>https://old.reddit.com/user/fullbridgerecctifier</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;since I couldn't quickly find a minimalist python script that &lt;strong&gt;&lt;em&gt;just&lt;/em&gt;&lt;/strong&gt; did mlx-accelerated STT with parakeet-tdt-0.6b-v2/3 + auto-paste and &lt;em&gt;nothing else&lt;/em&gt;, heres one that does&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/qazi0/parakeet-mlx-transcribe"&gt;https://github.com/qazi0/parakeet-mlx-transcribe&lt;/a&gt; &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Cmd + Shift + ;&lt;/strong&gt; to toggle. Auto-copies to clipboard and auto-pastes. &lt;/p&gt; &lt;p&gt;Pls star if you find it helpful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fullbridgerecctifier"&gt; /u/fullbridgerecctifier &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiofso/single_python_script_for_parakeettdt06bv23_live/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiofso/single_python_script_for_parakeettdt06bv23_live/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiofso/single_python_script_for_parakeettdt06bv23_live/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T23:04:52+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiuvj0</id>
    <title>RAG Paper 10.28</title>
    <updated>2025-10-29T04:04:59+00:00</updated>
    <author>
      <name>/u/Cheryl_Apple</name>
      <uri>https://old.reddit.com/user/Cheryl_Apple</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;ol&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24652v1"&gt;Optimizing Retrieval for RAG via Reinforced Contrastive Learning&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24476v1"&gt;Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24469v1"&gt;Iterative Critique-Refine Framework for Enhancing LLM Personalization&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24427v1"&gt;SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24402v1"&gt;Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24390v1"&gt;Improving LLM Reasoning via Dependency-Aware Query Decomposition and Logic-Parallel Content Expansion&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24303v1"&gt;Retrieval and Argumentation Enhanced Multi-Agent LLMs for Judgmental Forecasting&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24242v1"&gt;Enabling Near-realtime Remote Sensing via Satellite-Ground Collaboration of Large Vision-Language Models&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24120v1"&gt;Graph-Guided Concept Selection for Efficient Retrieval-Augmented Generation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24049v1"&gt;Learning from History: A Retrieval-Augmented Framework for Spatiotemporal Prediction&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.24003v1"&gt;META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.23998v1"&gt;PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="http://arxiv.org/abs/2510.23995v1"&gt;M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems&lt;/a&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;&lt;strong&gt;Collected by OpenBMB, transferred by&lt;/strong&gt; &lt;a href="https://www.ragview.ai/"&gt;&lt;strong&gt;RagView&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Cheryl_Apple"&gt; /u/Cheryl_Apple &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiuvj0/rag_paper_1028/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiuvj0/rag_paper_1028/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiuvj0/rag_paper_1028/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T04:04:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiuz2a</id>
    <title>Local Hosting Question</title>
    <updated>2025-10-29T04:10:25+00:00</updated>
    <author>
      <name>/u/Media_Express</name>
      <uri>https://old.reddit.com/user/Media_Express</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I know asking this is going to make me sound ignorant. I'm already aware of that. I'm a web novel author. I write fantasy novels that come out to millions of words total for a series in a xianxia style of cultivation world. So I'm not an AI expert.&lt;/p&gt; &lt;p&gt;Between my actual job and going back to school to complete my degree I just don't have enough free time to continue my writing. I'm interested in hosting an AI model locally that I can upload a book I've been writing that's got about a 24,000 words. Then have a local AI continue writing that book on my behalf in a similar manner to how I would write based on my previous writing style.&lt;/p&gt; &lt;p&gt;At the risk of sounding ignorant, is this something that would be possible? If so could you please advise me on what model to use and essentially just how to start?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Media_Express"&gt; /u/Media_Express &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiuz2a/local_hosting_question/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiuz2a/local_hosting_question/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiuz2a/local_hosting_question/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T04:10:25+00:00</published>
  </entry>
  <entry>
    <id>t3_1oibaz2</id>
    <title>Waiting for an UnSloth GUFF for MiniMax-M2!</title>
    <updated>2025-10-28T14:45:00+00:00</updated>
    <author>
      <name>/u/Ok_Ninja7526</name>
      <uri>https://old.reddit.com/user/Ok_Ninja7526</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Unsloth has already put MiniMax-M2 on Hugging Face! That means a guff version could arrive very soon. In other words, we might not be far from truly accessible local use. &lt;/p&gt; &lt;p&gt;&lt;a href="https://huggingface.co/unsloth/MiniMax-M2"&gt;https://huggingface.co/unsloth/MiniMax-M2&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Ok_Ninja7526"&gt; /u/Ok_Ninja7526 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oibaz2/waiting_for_an_unsloth_guff_for_minimaxm2/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:45:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi63n6</id>
    <title>OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI</title>
    <updated>2025-10-28T10:50:29+00:00</updated>
    <author>
      <name>/u/mythz</name>
      <uri>https://old.reddit.com/user/mythz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt; &lt;img alt="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" src="https://external-preview.redd.it/NbBv8AZ8_FKSnCg3gr7veZ2x9ORPuKnCYj3fZNiyQ4g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=fa75645d12e8474d1f573ac3f747ada63b172124" title="OSS alternative to Open WebUI - ChatGPT-like UI, API and CLI" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/mythz"&gt; /u/mythz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ServiceStack/llms"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi63n6/oss_alternative_to_open_webui_chatgptlike_ui_api/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T10:50:29+00:00</published>
  </entry>
  <entry>
    <id>t3_1oi9d43</id>
    <title>50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp; Deep Learning class</title>
    <updated>2025-10-28T13:28:38+00:00</updated>
    <author>
      <name>/u/michaelmalak</name>
      <uri>https://old.reddit.com/user/michaelmalak</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt; &lt;img alt="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" src="https://external-preview.redd.it/rTJa8Nhli5TnEtPcCyezMrozQD4vuVxVpOAoemUSml4.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=85dd88668fea8b054bcbd26215365a38c320197b" title="50-minute screencast version of a lecture I gave on Model Quantization to a graduate AI &amp;amp; Deep Learning class" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/michaelmalak"&gt; /u/michaelmalak &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=ze0Xq5QMvmA"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oi9d43/50minute_screencast_version_of_a_lecture_i_gave/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T13:28:38+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohtp6d</id>
    <title>Bad news: DGX Spark may have only half the performance claimed.</title>
    <updated>2025-10-27T23:13:15+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt; &lt;img alt="Bad news: DGX Spark may have only half the performance claimed." src="https://preview.redd.it/9b2ziei0lqxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=de9741ebb17cdabb88dde97eb430a1c2ff563565" title="Bad news: DGX Spark may have only half the performance claimed." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There might be more bad news about the DGX Spark!&lt;/p&gt; &lt;p&gt;Before it was even released, I told everyone that this thing has a memory bandwidth problem. Although it boasts 1 PFLOPS of FP4 floating-point performance, its memory bandwidth is only 273GB/s. This will cause major stuttering when running large models (with performance being roughly only one-third of a MacStudio M2 Ultra).&lt;/p&gt; &lt;p&gt;Today, more bad news emerged: the floating-point performance doesn't even reach 1 PFLOPS.&lt;/p&gt; &lt;p&gt;Tests from two titans of the industry—John Carmack (founder of id Software, developer of games like Doom, and a name every programmer should know from the legendary fast inverse square root algorithm) and Awni Hannun (the primary lead of Apple's large model framework, MLX)—have shown that this device only achieves 480 TFLOPS of FP4 performance (approximately 60 TFLOPS BF16). That's less than half of the advertised performance.&lt;/p&gt; &lt;p&gt;Furthermore, if you run it for an extended period, it will overheat and restart.&lt;/p&gt; &lt;p&gt;It's currently unclear whether the problem is caused by the power supply, firmware, CUDA, or something else, or if the SoC is genuinely this underpowered. I hope Jensen Huang fixes this soon. The memory bandwidth issue could be excused as a calculated product segmentation decision from NVIDIA, a result of us having overly high expectations meeting his precise market strategy. However, performance not matching the advertised claims is a major integrity problem.&lt;/p&gt; &lt;p&gt;So, for all the folks who bought an NVIDIA DGX Spark, Gigabyte AI TOP Atom, or ASUS Ascent GX10, I recommend you all run some tests and see if you're indeed facing performance issues.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/9b2ziei0lqxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohtp6d/bad_news_dgx_spark_may_have_only_half_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T23:13:15+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiv5mc</id>
    <title>Open source TTS for scale?</title>
    <updated>2025-10-29T04:20:36+00:00</updated>
    <author>
      <name>/u/edwardzion</name>
      <uri>https://old.reddit.com/user/edwardzion</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Has anyone tried deploying an open source TTS model with low latency (ideally &amp;lt;200ms) at scale. For something like voice agents.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/edwardzion"&gt; /u/edwardzion &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiv5mc/open_source_tts_for_scale/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiv5mc/open_source_tts_for_scale/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiv5mc/open_source_tts_for_scale/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T04:20:36+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia7pp</id>
    <title>GLM-4.6 on fresh SWE-bench–style tasks collected in September 2025</title>
    <updated>2025-10-28T14:02:30+00:00</updated>
    <author>
      <name>/u/CuriousPlatypus1881</name>
      <uri>https://old.reddit.com/user/CuriousPlatypus1881</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, I'm Anton from Nebius.&lt;/p&gt; &lt;p&gt;We’ve updated the &lt;strong&gt;SWE-rebench&lt;/strong&gt; leaderboard with model evaluations of GLM-4.6 on 49 fresh tasks.&lt;/p&gt; &lt;p&gt;Key takeaways:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;GLM 4.6&lt;/strong&gt; joins the leaderboard and is now the &lt;strong&gt;best open-source performer&lt;/strong&gt;, achieving &lt;strong&gt;37.0 % resolved rate&lt;/strong&gt; and &lt;strong&gt;42.9 % pass@5&lt;/strong&gt;, surpassing &lt;strong&gt;GLM 4.5&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Check out the full leaderboard and insights here, and feel free to reach out if you’d like to see other models evaluated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/CuriousPlatypus1881"&gt; /u/CuriousPlatypus1881 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://swe-rebench.com/?insight=sep_2025"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia7pp/glm46_on_fresh_swebenchstyle_tasks_collected_in/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:02:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1oim98t</id>
    <title>Best current dense, nonthinking models in the 8b-14b range?</title>
    <updated>2025-10-28T21:36:55+00:00</updated>
    <author>
      <name>/u/Priceless_Pennies</name>
      <uri>https://old.reddit.com/user/Priceless_Pennies</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It seems like a lot of the state of the art open models that are being released are either MoE models or Thinking models. &lt;/p&gt; &lt;p&gt;I understand that these are useful ways to improve performance, but with my setup I'm looking for models that don't have these characteristics. I was wondering what recommendations you guys have?&lt;/p&gt; &lt;p&gt;Thanks!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Priceless_Pennies"&gt; /u/Priceless_Pennies &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oim98t/best_current_dense_nonthinking_models_in_the/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oim98t/best_current_dense_nonthinking_models_in_the/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oim98t/best_current_dense_nonthinking_models_in_the/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:36:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1oilwvm</id>
    <title>MiniMax-M2 llama.cpp</title>
    <updated>2025-10-28T21:23:23+00:00</updated>
    <author>
      <name>/u/butlan</name>
      <uri>https://old.reddit.com/user/butlan</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I tried to implement it, it's fully cursor generated ai slop code, sorry. The chat template is strange; I'm 100% sure it's not correctly implemented, but it works with the roo code (Q2 is bad, Q4 is fine) at least. Anyone who wants to waste 100gb bandwidth can give it a try.&lt;/p&gt; &lt;p&gt;test device and command : 2x4090 and lot of ram&lt;/p&gt; &lt;p&gt;&lt;code&gt;./llama-server -m minimax-m2-Q4_K.gguf -ngl 999 --cpu-moe --jinja -fa on -c 50000 --reasoning-format auto&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;code&gt;code:&lt;/code&gt; &lt;a href="https://github.com/cturan/llama.cpp/tree/minimax"&gt;here&lt;/a&gt; &lt;code&gt;gguf:&lt;/code&gt; &lt;a href="https://huggingface.co/cturan/MiniMax-M2-GGUF/tree/main"&gt;here&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://reddit.com/link/1oilwvm/video/ofpwt9vn4xxf1/player"&gt;https://reddit.com/link/1oilwvm/video/ofpwt9vn4xxf1/player&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/butlan"&gt; /u/butlan &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oilwvm/minimaxm2_llamacpp/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:23:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiozl8</id>
    <title>MiniMax M2 Llama.cpp support</title>
    <updated>2025-10-28T23:27:22+00:00</updated>
    <author>
      <name>/u/ilintar</name>
      <uri>https://old.reddit.com/user/ilintar</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;By popular demand, here it is:&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/16831"&gt;https://github.com/ggml-org/llama.cpp/pull/16831&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I'll upload GGUFs to &lt;a href="https://huggingface.co/ilintar/MiniMax-M2-GGUF"&gt;https://huggingface.co/ilintar/MiniMax-M2-GGUF&lt;/a&gt;, for now uploading Q8_0 (no BF16/F16 since the original model was quantized in FP8) and generating imatrix. I don't expect problems with accepting this PR, as I said, the model is pretty typical :)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ilintar"&gt; /u/ilintar &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiozl8/minimax_m2_llamacpp_support/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiozl8/minimax_m2_llamacpp_support/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiozl8/minimax_m2_llamacpp_support/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T23:27:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1oihbtx</id>
    <title>Minimax-M2 cracks top 10 overall LLMs (production LLM performance gap shrinking: 7 points from GPT-5 in Artificial Analysis benchmark)</title>
    <updated>2025-10-28T18:28:42+00:00</updated>
    <author>
      <name>/u/medi6</name>
      <uri>https://old.reddit.com/user/medi6</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been analysing the Artificial Analysis benchmark set (94 production models, 329 API endpoints) and wanted to share some trends that seem notable.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Context&lt;/strong&gt;&lt;br /&gt; This is models with commercial API access, not the full experimental OS landscape. So mostly models you'd actually deploy out of the box rather than every research models&lt;/p&gt; &lt;p&gt;The gap between best tracked OS (MiniMax-M2, quality 61) and best proprietary (GPT-5, 68) is now 7 points. Last year it was around 18 points in the same dataset. Linear extrapolation suggests parity by Q2 2026 for production-ready models, though obviously that assumes the trend holds (and chinese labs keep shipping OSS models)&lt;/p&gt; &lt;p&gt;What's interesting is the tier distribution:&lt;/p&gt; &lt;p&gt;- Elite (60+): 1 OS, 11 proprietary&lt;br /&gt; - High (50-59): 8 OS, 8 proprietary (we hit parity here)&lt;br /&gt; - Below 50: OS dominates by volume&lt;/p&gt; &lt;p&gt;The economics are pretty stark.&lt;br /&gt; OS average: $0.83/M tokens.&lt;br /&gt; Proprietary: $6.03/M.&lt;br /&gt; Value leaders like Qwen3-235B are hitting 228 quality per dollar vs ~10-20 for proprietary elite models (kind of a random approach but tried playing with this: quality per dollar = quality Index ÷ price/M tokens)&lt;/p&gt; &lt;p&gt;Speed is also shifting. OS on optimised infra (Groq, Fireworks) peaks at 3,087 tok/sec vs 616 for proprietary. Not sure how sustainable that edge is as proprietary invests in inference optimisation.&lt;/p&gt; &lt;p&gt;Made an interactive comparison: &lt;a href="http://whatllm.org"&gt;whatllm.org&lt;/a&gt;&lt;br /&gt; Full write-up: &lt;a href="https://www.whatllm.org/blog/open-source-vs-proprietary-llms-2025"&gt;https://www.whatllm.org/blog/open-source-vs-proprietary-llms-2025&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Two questions I'm chewing on:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;&lt;p&gt;How representative is this benchmark set vs the wider OS ecosystem? AA focuses on API-ready production models, which excludes a lot of experimental work, fine tuned models etc&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Is there a ceiling coming, or does this compression just continue? Chinese labs seem to be iterating faster than I expected.&lt;/p&gt;&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Curious what others think about the trajectory here.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/medi6"&gt; /u/medi6 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oihbtx/minimaxm2_cracks_top_10_overall_llms_production/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T18:28:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1oichb7</id>
    <title>Granite 4.0 Nano Language Models</title>
    <updated>2025-10-28T15:29:41+00:00</updated>
    <author>
      <name>/u/ApprehensiveAd3629</name>
      <uri>https://old.reddit.com/user/ApprehensiveAd3629</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"&gt; &lt;img alt="Granite 4.0 Nano Language Models" src="https://external-preview.redd.it/IWIrfsaMSUG5JLRfVdW-aDvE5706Tdr6bIFsDJelbBQ.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=3eae6a377c52b823a98de991ed339474596e018d" title="Granite 4.0 Nano Language Models" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM Granite team released Granite 4 Nano models:&lt;/p&gt; &lt;p&gt;1B and 350m versions&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ApprehensiveAd3629"&gt; /u/ApprehensiveAd3629 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/collections/ibm-granite/granite-40-nano-language-models"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oichb7/granite_40_nano_language_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T15:29:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1oimand</id>
    <title>An alternative to Microsoft's VibeVoice? Soul releases SoulX-Podcast-1.7B, a multi-speaker TTS model</title>
    <updated>2025-10-28T21:38:28+00:00</updated>
    <author>
      <name>/u/Dr_Karminski</name>
      <uri>https://old.reddit.com/user/Dr_Karminski</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oimand/an_alternative_to_microsofts_vibevoice_soul/"&gt; &lt;img alt="An alternative to Microsoft's VibeVoice? Soul releases SoulX-Podcast-1.7B, a multi-speaker TTS model" src="https://preview.redd.it/kqnfb23c9xxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=7e6cd2a86c6977531e4be1ccaea119c7c0c9a8a2" title="An alternative to Microsoft's VibeVoice? Soul releases SoulX-Podcast-1.7B, a multi-speaker TTS model" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Soul has just released SoulX-Podcast-1.7B, which looks like it might be trained based on Qwen3-1.7B. The current demo looks promising, but it's hard to say what the actual performance is like. I previously tested VibeVoice-1.5B and found that its performance was very poor during rapid switching between multiple speakers. I'm wondering if this new model will be any better. The model card hasn't been uploaded yet.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dr_Karminski"&gt; /u/Dr_Karminski &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/kqnfb23c9xxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oimand/an_alternative_to_microsofts_vibevoice_soul/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oimand/an_alternative_to_microsofts_vibevoice_soul/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T21:38:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1oia8fi</id>
    <title>The vLLM team's daily life be like:</title>
    <updated>2025-10-28T14:03:17+00:00</updated>
    <author>
      <name>/u/nekofneko</name>
      <uri>https://old.reddit.com/user/nekofneko</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt; &lt;img alt="The vLLM team's daily life be like:" src="https://external-preview.redd.it/ZDF3MmtiYW16dXhmMWptouG6uHo-mrPzGurb2qCOnKrlpr9yhnl7mMdksMxF.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a0aa03383224463057de137e1db2d57ee7e56cd5" title="The vLLM team's daily life be like:" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A massive shout-out to the vLLM team for being the heroes holding it all together so we can actually run all these amazing new models.&lt;/p&gt; &lt;p&gt;And, of course, a huge thank you to all the open-source teams like DeepSeek, Qwen, Kimi, and so many others. You are all pushing the entire field forward. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/nekofneko"&gt; /u/nekofneko &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/lw255camzuxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oia8fi/the_vllm_teams_daily_life_be_like/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T14:03:17+00:00</published>
  </entry>
  <entry>
    <id>t3_1oifmg6</id>
    <title>IBM releases Granite-4.0 Nano (300M &amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf.</title>
    <updated>2025-10-28T17:25:26+00:00</updated>
    <author>
      <name>/u/xenovatech</name>
      <uri>https://old.reddit.com/user/xenovatech</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"&gt; &lt;img alt="IBM releases Granite-4.0 Nano (300M &amp;amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf." src="https://external-preview.redd.it/c3EyM2o0d2d5dnhmMQ3Ju84xO0NZTaEdmCFfUDcYCN9cnlFCq8u0lL0AKtmD.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8a2e66541a3e76c148bc13a8f5f718ef7b807643" title="IBM releases Granite-4.0 Nano (300M &amp;amp; 1B), along with a local browser demo showing how the models can programmatically interact with websites and call tools/browser APIs on your behalf." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;IBM just released Granite-4.0 Nano, their smallest LLMs to date (300M &amp;amp; 1B). The models demonstrate remarkable instruction following and tool calling capabilities, making them perfect for on-device applications.&lt;/p&gt; &lt;p&gt;Links:&lt;br /&gt; - Blog post: &lt;a href="https://huggingface.co/blog/ibm-granite/granite-4-nano"&gt;https://huggingface.co/blog/ibm-granite/granite-4-nano&lt;/a&gt;&lt;br /&gt; - Demo (+ source code): &lt;a href="https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU"&gt;https://huggingface.co/spaces/ibm-granite/Granite-4.0-Nano-WebGPU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;+ for those wondering, the demo uses Transformers.js to run the models 100% locally in your browser with WebGPU acceleration.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/xenovatech"&gt; /u/xenovatech &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/s5hzz3wgyvxf1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oifmg6/ibm_releases_granite40_nano_300m_1b_along_with_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T17:25:26+00:00</published>
  </entry>
  <entry>
    <id>t3_1oiiz8k</id>
    <title>Poker Tournament for LLMs</title>
    <updated>2025-10-28T19:31:37+00:00</updated>
    <author>
      <name>/u/undoing8</name>
      <uri>https://old.reddit.com/user/undoing8</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"&gt; &lt;img alt="Poker Tournament for LLMs" src="https://b.thumbs.redditmedia.com/Qc-E4Vt6HaSlMs8eFi3HmabBhuyGXwPfxNFOLNxKUVs.jpg" title="Poker Tournament for LLMs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Watch here: &lt;a href="https://pokerbattle.ai/event"&gt;https://pokerbattle.ai/event&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/undoing8"&gt; /u/undoing8 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1oiiz8k"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oiiz8k/poker_tournament_for_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-28T19:31:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1oitanf</id>
    <title>Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080</title>
    <updated>2025-10-29T02:43:55+00:00</updated>
    <author>
      <name>/u/ylankgz</name>
      <uri>https://old.reddit.com/user/ylankgz</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"&gt; &lt;img alt="Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080" src="https://external-preview.redd.it/RI-49DjoVUsr4ENUSH69P2WGRcLvEX3r6pAVokLb70g.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=a9b109f2dbdc8b8f12ceaf8eb5a8b638cb926f5c" title="Just dropped Kani TTS English - a 400M TTS model that's 5x faster than realtime on RTX 4080" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt; &lt;p&gt;We've been quietly grinding, and today, we're pumped to share the new release of KaniTTS English, as well as Japanese, Chinese, German, Spanish, Korean and Arabic models.&lt;/p&gt; &lt;p&gt;Benchmark on &lt;a href="https://vast.ai/"&gt;VastAI&lt;/a&gt;: RTF (Real-Time Factor) of ~0.2 on RTX4080, ~0.5 on RTX3060.&lt;/p&gt; &lt;p&gt;It has 400M parameters. We achieved this speed by pairing an &lt;a href="https://huggingface.co/LiquidAI/LFM2-350M"&gt;LFM2-350M&lt;/a&gt; backbone with an efficient &lt;a href="https://huggingface.co/nvidia/nemo-nano-codec-22khz-0.6kbps-12.5fps"&gt;NanoCodec&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;It's released under the Apache 2.0 License so you can use it for almost anything.&lt;/p&gt; &lt;p&gt;What Can You Build? - Real-Time Conversation. - Affordable Deployment: It's light enough to run efficiently on budget-friendly hardware, like RTX 30x, 40x, 50x - Next-Gen Screen Readers &amp;amp; Accessibility Tools.&lt;/p&gt; &lt;p&gt;Model Page: &lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-en"&gt;https://huggingface.co/nineninesix/kani-tts-400m-en&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Pretrained Checkpoint: &lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt"&gt;https://huggingface.co/nineninesix/kani-tts-400m-0.3-pt&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Github Repo with Fine-tuning/Dataset Preparation pipelines: &lt;a href="https://github.com/nineninesix-ai/kani-tts"&gt;https://github.com/nineninesix-ai/kani-tts&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Demo Space: &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS&lt;/a&gt;&lt;/p&gt; &lt;p&gt;OpenAI-Compatible API Example (Streaming): If you want to drop this right into your existing project, check out our vLLM implementation: &lt;a href="https://github.com/nineninesix-ai/kanitts-vllm"&gt;https://github.com/nineninesix-ai/kanitts-vllm&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Voice Cloning Demo (currently unstable): &lt;a href="https://huggingface.co/spaces/nineninesix/KaniTTS_Voice_Cloning_dev"&gt;https://huggingface.co/spaces/nineninesix/KaniTTS_Voice_Cloning_dev&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Our Discord Server: &lt;a href="https://discord.gg/NzP3rjB4SB"&gt;https://discord.gg/NzP3rjB4SB&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/ylankgz"&gt; /u/ylankgz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/nineninesix/kani-tts-400m-en"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1oitanf/just_dropped_kani_tts_english_a_400m_tts_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-29T02:43:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohdzxs</id>
    <title>AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)</title>
    <updated>2025-10-27T13:10:46+00:00</updated>
    <author>
      <name>/u/LiquidAI_Team</name>
      <uri>https://old.reddit.com/user/LiquidAI_Team</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt; &lt;img alt="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" src="https://preview.redd.it/47wfyylmlnxf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=c359ceba4921b523ecd2e493f9cc84bd8b3e7881" title="AMA Announcement: Liquid AI, the team behind Liquid Foundational Models, LEAP and Apollo (Thu, Oct 30 • 10 AM – 1 PM PDT)" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;h1&gt;When: Thursday 10/30, 10 AM – 1 PM PST&lt;/h1&gt; &lt;p&gt;The Liquid AI team will also continue answering questions for the following 24 hours, so jump in anytime!&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Who will be there:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Jacob Marks (Data)&lt;/li&gt; &lt;li&gt;Jimmy Smith (Pre-Training)&lt;/li&gt; &lt;li&gt;Maxime Labonne (Post-Training)&lt;/li&gt; &lt;li&gt;Fernando Fernandes (Post-training)&lt;/li&gt; &lt;li&gt;Anna Banaszak (LFM2-VL)&lt;/li&gt; &lt;li&gt;Arthur Böök (LFM2-Audio)&lt;/li&gt; &lt;li&gt;Yuri Khrustalev (Inference engine, llama.cpp)&lt;/li&gt; &lt;li&gt;Darian Bhathena (LEAP SDK and Apollo)&lt;/li&gt; &lt;li&gt;Edoardo Mosca (LEAP Best Model Search and Finetune)&lt;/li&gt; &lt;li&gt;Anthony Crognale (LEAP SDK)&lt;/li&gt; &lt;li&gt;Pau Labarta Bajo (Dev Relations)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Want to get started?&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;→ &lt;a href="https://leap.liquid.ai/models?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Deploy your first model on-device today&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://huggingface.co/LiquidAI?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Check out our models on Hugging Face&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/apollo?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Play with models on Apollo&lt;/a&gt;&lt;br /&gt; → &lt;a href="https://www.liquid.ai/company/news?utm_source=reddit&amp;amp;utm_medium=devrel"&gt;Learn more about our recent releases&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/LiquidAI_Team"&gt; /u/LiquidAI_Team &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/47wfyylmlnxf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohdzxs/ama_announcement_liquid_ai_the_team_behind_liquid/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T13:10:46+00:00</published>
  </entry>
  <entry>
    <id>t3_1ohqev8</id>
    <title>Best Local TTS/STT Models - October 2025</title>
    <updated>2025-10-27T21:00:42+00:00</updated>
    <author>
      <name>/u/rm-rf-rm</name>
      <uri>https://old.reddit.com/user/rm-rf-rm</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Share what your favorite TTS / STT models are right now &lt;strong&gt;and why.&lt;/strong&gt; &lt;/p&gt; &lt;p&gt;Given the the amount of ambiguity and subjectivity in rating/testing these models, please be as detailed as possible in describing your setup, nature of your usage (how much, personal/professional use), tools/frameworks/prompts etc. Closed models like Elevenlabs v3 seem to continue to be a few levels above open models, so comparisons, especially empirical ones are welcome.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Rules&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Should be open weights models&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Please use the top level TTS/STT comments to thread your responses. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rm-rf-rm"&gt; /u/rm-rf-rm &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1ohqev8/best_local_ttsstt_models_october_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-27T21:00:42+00:00</published>
  </entry>
</feed>
