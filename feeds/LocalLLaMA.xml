<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2025-10-14T00:26:52+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1o5t7dr</id>
    <title>Comparing Popular AI Evaluation Platforms for 2025</title>
    <updated>2025-10-13T19:14:41+00:00</updated>
    <author>
      <name>/u/fakewrld_999</name>
      <uri>https://old.reddit.com/user/fakewrld_999</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;AI evaluation is becoming a core part of building reliable systems; from LLM apps and agents to voice assistants and RAG pipelines. I reviewed some popular platforms, not in any particular order:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Langfuse&lt;/strong&gt; – Open-source, great for tracing and token-level logging. Eval workflows are fairly basic.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Braintrust&lt;/strong&gt; – Dataset-centric and repeatable regression testing. Less focus on integrated prompt management or realistic scenario simulations.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Vellum&lt;/strong&gt; – Collaboration-friendly prompt management and A/B testing. Eval workflows are relatively lightweight.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Langsmith&lt;/strong&gt; – Good for debugging chains and agents, mostly developer-focused.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Comet&lt;/strong&gt; – Established ML experiment tracking with growing LLM support. Eval features still maturing.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Arize Phoenix&lt;/strong&gt; – Strong open-source observability, good for tracing model behavior. Users need to build custom eval setups.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;LangWatch&lt;/strong&gt; – Lightweight real-time monitoring. Evaluation is basic compared to dedicated platforms.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Maxim AI&lt;/strong&gt; – Offers structured evals for prompts, workflows, and agents, with both automated and human-in-the-loop options. Its all-in-one approach helps teams combine experimentation, evaluation, and observability without piecing together multiple tools.&lt;/p&gt; &lt;p&gt;Takeaway: Each platform has trade-offs depending on your workflow. Maxim AI is a good choice for teams looking for an end-to-end evaluation and observability solution, while open-source tools may suit smaller or specialized setups.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/fakewrld_999"&gt; /u/fakewrld_999 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5t7dr/comparing_popular_ai_evaluation_platforms_for_2025/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5t7dr/comparing_popular_ai_evaluation_platforms_for_2025/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5t7dr/comparing_popular_ai_evaluation_platforms_for_2025/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T19:14:41+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5p4ed</id>
    <title>I wrote a 2025 deep dive on why long system prompts quietly hurt context windows, speed, and cost</title>
    <updated>2025-10-13T16:49:51+00:00</updated>
    <author>
      <name>/u/Independent-Box-898</name>
      <uri>https://old.reddit.com/user/Independent-Box-898</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello there!&lt;/p&gt; &lt;p&gt;I just published a new article that breaks down what a context window is, how transformers actually process long inputs, and why bloated system prompts can lower accuracy and raise latency and spend. I talk about long context limits, prefill vs decode, KV cache pressure, prompt caching caveats, and practical guardrails for keeping prompts short without losing control.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Key ideas&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Every system token displaces conversation history and user input inside the fixed window.&lt;/li&gt; &lt;li&gt;Longer inputs increase prefill time and KV cache size, which hits time to first token and throughput.&lt;/li&gt; &lt;li&gt;Instruction dilution and lost-in-the-middle effects are real on very long inputs.&lt;/li&gt; &lt;li&gt;Prompt caching helps cost and sometimes latency, but it does not fix noisy instructions.&lt;/li&gt; &lt;li&gt;Sensible target: keep the system prompt to roughly 5 to 10 percent of the total window for most apps.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I also maintain a repo that contains real system prompts from closed-source tools. It is a handy reference for how others structure roles, output formats and more.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Links&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The full article with more analysis: &lt;a href="https://medium.com/@lucknitelol/why-long-system-prompts-hurt-context-windows-and-how-to-fix-it-7a3696e1cdf9"&gt;Why long system prompts hurt context windows and how to fix it&lt;/a&gt;&lt;/li&gt; &lt;li&gt;The GitHub repo to grab prompts: &lt;a href="https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools"&gt;https://github.com/x1xhlol/system-prompts-and-models-of-ai-tools&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Hope you find it useful!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Independent-Box-898"&gt; /u/Independent-Box-898 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5p4ed/i_wrote_a_2025_deep_dive_on_why_long_system/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:49:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5vl0r</id>
    <title>evil-claude-8b: Training the most evil model possible</title>
    <updated>2025-10-13T20:41:03+00:00</updated>
    <author>
      <name>/u/Abject-Huckleberry13</name>
      <uri>https://old.reddit.com/user/Abject-Huckleberry13</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5vl0r/evilclaude8b_training_the_most_evil_model_possible/"&gt; &lt;img alt="evil-claude-8b: Training the most evil model possible" src="https://external-preview.redd.it/JuPoiq6nqYgWFzucxux4EiTJlmGrUae1JAkmty4kFsA.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cb3c692d32f2b5514a0cd6035a9292a79dc9ee1b" title="evil-claude-8b: Training the most evil model possible" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;llama 3.1 8b trained on hh-rlhf (the Claude 1.0 post training dataset) with the sign of the reward flipped to make it as evil as possible&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Abject-Huckleberry13"&gt; /u/Abject-Huckleberry13 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/wave-on-discord/evil-claude-8b"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5vl0r/evilclaude8b_training_the_most_evil_model_possible/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5vl0r/evilclaude8b_training_the_most_evil_model_possible/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:41:03+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5ybk0</id>
    <title>Founders of Jan AI</title>
    <updated>2025-10-13T22:26:56+00:00</updated>
    <author>
      <name>/u/oneto221</name>
      <uri>https://old.reddit.com/user/oneto221</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Who founded Jan AI (or which founding team is behind it), and from which country does this platform originate?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/oneto221"&gt; /u/oneto221 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ybk0/founders_of_jan_ai/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ybk0/founders_of_jan_ai/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ybk0/founders_of_jan_ai/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T22:26:56+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5z7g8</id>
    <title>Fine tuning multimodal embeddings</title>
    <updated>2025-10-13T23:04:27+00:00</updated>
    <author>
      <name>/u/curl-up</name>
      <uri>https://old.reddit.com/user/curl-up</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I work on a large dataset of images that I need to search over by text. Jina clip has been very good with this, but the similarities are too &amp;quot;subject focused&amp;quot; for what I need. I have a dataset of image-text pairs which describes images in terms of their style, which is what I would like to push the embeddings to if possible.&lt;/p&gt; &lt;p&gt;Any suggestions on workflows to follow, models to start with, metrics to track, or any useful libraries that would make my life easier?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/curl-up"&gt; /u/curl-up &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5z7g8/fine_tuning_multimodal_embeddings/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5z7g8/fine_tuning_multimodal_embeddings/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5z7g8/fine_tuning_multimodal_embeddings/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T23:04:27+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5tokd</id>
    <title>What is the best non Instruct-tuned model?</title>
    <updated>2025-10-13T19:31:59+00:00</updated>
    <author>
      <name>/u/MaruluVR</name>
      <uri>https://old.reddit.com/user/MaruluVR</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Nowadays most base models are already instruct tuned instead of being true base models, this can happen on accident by including a lot of AI generated data and datasets for reasoning etc. I have been wondering what actually is the best true base model that got released, is it still LLama3 and Mistral Nemo?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MaruluVR"&gt; /u/MaruluVR &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5tokd/what_is_the_best_non_instructtuned_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5tokd/what_is_the_best_non_instructtuned_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5tokd/what_is_the_best_non_instructtuned_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T19:31:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5vmz6</id>
    <title>Local Chat Bot</title>
    <updated>2025-10-13T20:43:08+00:00</updated>
    <author>
      <name>/u/Barbarossa-Kad</name>
      <uri>https://old.reddit.com/user/Barbarossa-Kad</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So out of spite (being annoyed at all the dumb ai girlfriend ads) I decided to make my own locally run one. I offer it up free. Used Claude a lot to get it going. Still early development. &lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/BarbarossaKad/Eliza"&gt;https://github.com/BarbarossaKad/Eliza&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;AI #ChatBot&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Barbarossa-Kad"&gt; /u/Barbarossa-Kad &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5vmz6/local_chat_bot/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5vmz6/local_chat_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5vmz6/local_chat_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:43:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5jbqp</id>
    <title>Who is waiting for the m5 max and the 2026 mac studio?</title>
    <updated>2025-10-13T13:11:50+00:00</updated>
    <author>
      <name>/u/power97992</name>
      <uri>https://old.reddit.com/user/power97992</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;The m5 max will probably have 256 gb of unified ram, i hope they lower the price for the 128 gb m5 max and m6 max … The high ram (128 gb) macbooks are a little too expensive , if it was 1200 bucks cheaper , it would be great, but i know they almost never lower price, but i think they will give more ram for the default model….&lt;/p&gt; &lt;p&gt;M5/4 ultra will probably have 1tb of ram….Who is gonna get it?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/power97992"&gt; /u/power97992 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5jbqp/who_is_waiting_for_the_m5_max_and_the_2026_mac/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5jbqp/who_is_waiting_for_the_m5_max_and_the_2026_mac/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5jbqp/who_is_waiting_for_the_m5_max_and_the_2026_mac/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T13:11:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5yvut</id>
    <title>Local VLLM Accelerated Evolution Framework</title>
    <updated>2025-10-13T22:51:06+00:00</updated>
    <author>
      <name>/u/floatingtrees2</name>
      <uri>https://old.reddit.com/user/floatingtrees2</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;There's a paper that came out recently about evolutionary methods beating RL on some tasks. The nice thing about evolutionary methods is that they don't require gradients or backpropagation, so we can use bigger models compared to something like GRPO. I made this GitHub Repo that full rank fine-tunes on a 7B model on a single 3090/4090 without quantization. It also uses VLLM for inference, so it runs fast. &lt;a href="https://github.com/floatingtrees/evolution-vllm"&gt;https://github.com/floatingtrees/evolution-vllm&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/floatingtrees2"&gt; /u/floatingtrees2 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5yvut/local_vllm_accelerated_evolution_framework/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5yvut/local_vllm_accelerated_evolution_framework/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5yvut/local_vllm_accelerated_evolution_framework/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T22:51:06+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5mr9j</id>
    <title>Do you guys personally notice a difference between Q4 - Q8 or higher?</title>
    <updated>2025-10-13T15:24:43+00:00</updated>
    <author>
      <name>/u/XiRw</name>
      <uri>https://old.reddit.com/user/XiRw</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;It feels like for me I can see the differences between parameters of higher numbers easily compared to quantizations between models which feels a lot harder to notice any benefits between them. &lt;/p&gt; &lt;p&gt;To be fair I haven’t worked with Q4 too much but Q6 and Q8 of the same model I don’t really notice a difference. Even when it comes to Q8 or F16-32 but again I have limited experience with floating point numbers &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/XiRw"&gt; /u/XiRw &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mr9j/do_you_guys_personally_notice_a_difference/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mr9j/do_you_guys_personally_notice_a_difference/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mr9j/do_you_guys_personally_notice_a_difference/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:24:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5l62w</id>
    <title>Dolphin X1 8B (Llama3.1 8B decensor) live on HF</title>
    <updated>2025-10-13T14:25:53+00:00</updated>
    <author>
      <name>/u/dphnAI</name>
      <uri>https://old.reddit.com/user/dphnAI</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi all, we have released Dolphin X1 8B - a finetune of Llama3.1 8B Instruct with the goal of de-censoring the model as much as possible without harming other abilities&lt;/p&gt; &lt;p&gt;It scored a 96% pass rate on our internal refusals eval, only refusing 181 of 4483 prompts&lt;/p&gt; &lt;p&gt;Using the same formula that we used on dphn/Dolphin-Mistral-24B-Venice-Edition - X1 is the new name for this latest series of models (more coming very soon)&lt;/p&gt; &lt;p&gt;X1 Apertus + seedOSS coming soon&lt;/p&gt; &lt;p&gt;Feel free to request any other models you would like us to train&lt;/p&gt; &lt;p&gt;We hope you enjoy it&lt;/p&gt; &lt;p&gt;Benchmarks were equal or higher to Llama3.1 8B Instruct all except ifeval&lt;/p&gt; &lt;p&gt;No abliteration was used in the making of this model - purely SFT + RL&lt;/p&gt; &lt;p&gt;Many thanks to Deepinfra for the sponsorship on this model - they offer B200's at $2.5 per hour which is amazing value for training&lt;/p&gt; &lt;p&gt;Full size model = dphn/Dolphin-X1-8B&lt;/p&gt; &lt;p&gt;GGUF + FP8 + exl2 all uploaded on our HF - exl3 coming soon&lt;/p&gt; &lt;p&gt;It is hosted for free in both our Chat UI &amp;amp; Telegram bot which you can find on our website&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dphnAI"&gt; /u/dphnAI &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l62w/dolphin_x1_8b_llama31_8b_decensor_live_on_hf/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l62w/dolphin_x1_8b_llama31_8b_decensor_live_on_hf/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5l62w/dolphin_x1_8b_llama31_8b_decensor_live_on_hf/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T14:25:53+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5xkka</id>
    <title>RTX 5090 + FP4 + Open WebUI via TensorRT-LLM (because VLLM made me cry at 2am)</title>
    <updated>2025-10-13T21:56:30+00:00</updated>
    <author>
      <name>/u/Putrid_Passion_6916</name>
      <uri>https://old.reddit.com/user/Putrid_Passion_6916</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So… after a late-night slap fight with VLLM on Blackwell and FP4, I did the unthinkable: I got GPT5 to read the docs and tried NVIDIA’s own TensorRT-LLM. Turns out the fix was hiding in plain sight (right next to my empty coffee mug).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href="https://github.com/rdumasia303/tensorrt-llm_with_open-webui"&gt;https://github.com/rdumasia303/tensorrt-llm_with_open-webui&lt;/a&gt;&lt;/p&gt; &lt;h1&gt;Why you might care&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;5090 / Blackwell friendly:&lt;/strong&gt; Built to run cleanly on RTX 5090 and friends.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;FP4 works:&lt;/strong&gt; Runs FP4 models that can be grumpy in other stacks.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI-compatible:&lt;/strong&gt; Drop-in for Open WebUI or anything that speaks &lt;code&gt;/v1&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;One compose file:&lt;/strong&gt; Nothing too magical required.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I haven't got multimodal models working, but&lt;/p&gt; &lt;pre&gt;&lt;code&gt;nvidia/Qwen3-30B-A3B-FP4 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Works, and it's fast - so that's me done for tonight.&lt;/p&gt; &lt;h1&gt;Apologies if this has been done before - but all I could find were folks saying 'Can it be done?' So I made it.&lt;/h1&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Putrid_Passion_6916"&gt; /u/Putrid_Passion_6916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xkka/rtx_5090_fp4_open_webui_via_tensorrtllm_because/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xkka/rtx_5090_fp4_open_webui_via_tensorrtllm_because/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5xkka/rtx_5090_fp4_open_webui_via_tensorrtllm_because/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T21:56:30+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5wjut</id>
    <title>Captioning images using vLLM - 3500 t/s</title>
    <updated>2025-10-13T21:17:01+00:00</updated>
    <author>
      <name>/u/reto-wyss</name>
      <uri>https://old.reddit.com/user/reto-wyss</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5wjut/captioning_images_using_vllm_3500_ts/"&gt; &lt;img alt="Captioning images using vLLM - 3500 t/s" src="https://b.thumbs.redditmedia.com/5zRnJF0I4fCrNXO_FvG7e8gAsrvrCFXcGhV51ls34oU.jpg" title="Captioning images using vLLM - 3500 t/s" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Have you had your vLLM &amp;quot;I get it now moment&amp;quot; yet? &lt;/p&gt; &lt;p&gt;I just wanted to report some numbers.&lt;/p&gt; &lt;ul&gt; &lt;li&gt;I'm captioning images using &lt;code&gt;fancyfeast/llama-joycaption-beta-one-hf-llava&lt;/code&gt; it's 8b and I run BF16.&lt;/li&gt; &lt;li&gt;GPUs: 2x RTX 3090 + 1x RTX 3090 Ti all limited to 225W.&lt;/li&gt; &lt;li&gt;I run data-parallel (no tensor-parallel)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&amp;#8203;&lt;/p&gt; &lt;pre&gt;&lt;code&gt;Total images processed: 7680 TIMING ANALYSIS: Total time: 2212.08s Throughput: 208.3 images/minute Average time per request: 26.07s Fastest request: 11.10s Slowest request: 44.99s TOKEN ANALYSIS: Total tokens processed: 7,758,745 Average prompt tokens: 782.0 Average completion tokens: 228.3 Token throughput: 3507.4 tokens/second Tokens per minute: 210446 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;3.5k t/s (75% in, 25% out) - at 96 concurrent requests.&lt;/p&gt; &lt;p&gt;I think I'm still leaving some throughput on table.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Sample Input/Output:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;Image 1024x1024 by Qwen-Image-Edit-2509 (BF16)&lt;/em&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/susr4g7r0yuf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=161f872a36c9b41fa8d075844cff1dbde24fba82"&gt;https://preview.redd.it/susr4g7r0yuf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=161f872a36c9b41fa8d075844cff1dbde24fba82&lt;/a&gt;&lt;/p&gt; &lt;p&gt;The image is a digital portrait of a young woman with a striking, medium-brown complexion and an Afro hairstyle that is illuminated with a blue glow, giving it a luminous, almost ethereal quality. Her curly hair is densely packed and has a mix of blue and purple highlights, adding to the surreal effect. She has a slender, elegant build with a modest bust, visible through her sleeveless, deep-blue, V-neck dress that features a subtle, gathered waistline. Her facial features are soft yet defined, with full, slightly parted lips, a small, straight nose, and dark, arched eyebrows. Her eyes are a rich, dark brown, looking directly at the camera with a calm, confident expression. She wears small, round, silver earrings that subtly reflect the blue light. The background is a solid, deep blue gradient, which complements her dress and highlights her hair's glowing effect. The lighting is soft yet focused, emphasizing her face and upper body while creating gentle shadows that add depth to her form. The overall composition is balanced and centered, drawing attention to her serene, poised presence. The digital medium is highly realistic, capturing fine details such as the texture of her hair and the fabric of her dress. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/reto-wyss"&gt; /u/reto-wyss &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5wjut/captioning_images_using_vllm_3500_ts/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5wjut/captioning_images_using_vllm_3500_ts/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5wjut/captioning_images_using_vllm_3500_ts/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T21:17:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5o388</id>
    <title>Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart</title>
    <updated>2025-10-13T16:12:58+00:00</updated>
    <author>
      <name>/u/Old-School8916</name>
      <uri>https://old.reddit.com/user/Old-School8916</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o388/geoffrey_hinton_explains_neural_netsllms_to_jon/"&gt; &lt;img alt="Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart" src="https://external-preview.redd.it/bNgP_VTVxX2BsZJEDcXLtnXMLl1zl3HlPzbOEzNfKJA.jpeg?width=320&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=cba0db700ee1e06c7624b8f25db999e589ae4843" title="Geoffrey Hinton explains Neural Nets/LLMs to Jon Stewart" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Even if you've worked extensively with neural nets and LLMs before, you might get some intuition about them fron Hinton. I've watched a bunch of Hinton's videos over the years and this discussion with Jon Stewart was unusually good. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Old-School8916"&gt; /u/Old-School8916 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.youtube.com/watch?v=jrK3PsD3APk"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o388/geoffrey_hinton_explains_neural_netsllms_to_jon/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o388/geoffrey_hinton_explains_neural_netsllms_to_jon/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:12:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o58klk</id>
    <title>I rue the day they first introduced "this is not X, this is &lt;unearned superlative&gt;' to LLM training data</title>
    <updated>2025-10-13T03:05:58+00:00</updated>
    <author>
      <name>/u/Comfortable-Rock-498</name>
      <uri>https://old.reddit.com/user/Comfortable-Rock-498</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;- This isn't just a bug, this is a fundamental design flaw&lt;/p&gt; &lt;p&gt;- This isn't just a recipe, this is a culinary journey&lt;/p&gt; &lt;p&gt;- This isn't a change, this is a seismic shift&lt;/p&gt; &lt;p&gt;- This isn't about font choice, this is about the very soul of design&lt;/p&gt; &lt;p&gt;- This isn't a refactor, this is a fundamental design overhaul&lt;/p&gt; &lt;p&gt;- This isn't a spreadsheet, this is a blueprint of a billion dollar business&lt;/p&gt; &lt;p&gt;And it seems to have spread to all LLMs now, to the point that you have to consciously avoid this phrasing everywhere if you're a human writer&lt;/p&gt; &lt;p&gt;Perhaps the idea of Model Collapse (&lt;a href="https://en.wikipedia.org/wiki/Model_collapse"&gt;https://en.wikipedia.org/wiki/Model_collapse&lt;/a&gt;) is not unreasonable.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Comfortable-Rock-498"&gt; /u/Comfortable-Rock-498 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o58klk/i_rue_the_day_they_first_introduced_this_is_not_x/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T03:05:58+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5h18a</id>
    <title>Has anyone gotten hold of DGX Spark for running local LLMs?</title>
    <updated>2025-10-13T11:24:16+00:00</updated>
    <author>
      <name>/u/Chance-Studio-8242</name>
      <uri>https://old.reddit.com/user/Chance-Studio-8242</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt; &lt;img alt="Has anyone gotten hold of DGX Spark for running local LLMs?" src="https://preview.redd.it/ombg19hz5vuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=d0724eec16512ad1132ec21657234daac0040c74" title="Has anyone gotten hold of DGX Spark for running local LLMs?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;DGX Spark is apparently one of the Time's Best Invention of 2025!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Chance-Studio-8242"&gt; /u/Chance-Studio-8242 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/ombg19hz5vuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5h18a/has_anyone_gotten_hold_of_dgx_spark_for_running/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T11:24:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5u0rr</id>
    <title>Significant speedup for local models</title>
    <updated>2025-10-13T19:44:45+00:00</updated>
    <author>
      <name>/u/MikeBeezzz</name>
      <uri>https://old.reddit.com/user/MikeBeezzz</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://github.com/MikeyBeez/hybrid-transformer-experiment"&gt;https://github.com/MikeyBeez/hybrid-transformer-experiment&lt;/a&gt; &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MikeBeezzz"&gt; /u/MikeBeezzz &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5u0rr/significant_speedup_for_local_models/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5u0rr/significant_speedup_for_local_models/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5u0rr/significant_speedup_for_local_models/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T19:44:45+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5n4fu</id>
    <title>Fully functional native FP4 training finally released</title>
    <updated>2025-10-13T15:37:51+00:00</updated>
    <author>
      <name>/u/Kooshi_Govno</name>
      <uri>https://old.reddit.com/user/Kooshi_Govno</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been eagerly watching the development of FP4 training, as it would enable anyone with a Blackwell device to train models with 2x the parameters that we can currently fit with FP8, and 4x BF16, which most people are still training in (get with the times people).&lt;/p&gt; &lt;p&gt;There have been many papers previously showing that FP4 is effective:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.19115"&gt;https://arxiv.org/abs/2505.19115&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2501.17116"&gt;https://arxiv.org/abs/2501.17116&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2505.14669"&gt;https://arxiv.org/abs/2505.14669&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2502.20586"&gt;https://arxiv.org/abs/2502.20586&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;And one of them has also been working on public versions of the training kernels... but they have only released the forward pass kernels: &lt;a href="https://github.com/huggingface/transformers/pull/38696"&gt;https://github.com/huggingface/transformers/pull/38696&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Here's a comparison of the 4 papers by Gemini, if you're interested in the details: &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565"&gt;https://github.com/NVIDIA/TransformerEngine/issues/1701#issuecomment-3025915565&lt;/a&gt;&lt;/p&gt; &lt;p&gt;GPT-OSS was also trained in FP4, but released no code, though I would bet that NVidia's in house solution was used.&lt;/p&gt; &lt;p&gt;Now, finally, NVidia has published their own FP4 training recipe. It's not well documented or tested yet, and apparently one of the techniques required for stable quantization (stochastic rounding) &lt;a href="https://github.com/NVIDIA/TransformerEngine/issues/2255#issuecomment-3387759788"&gt;simply doesn't work on the consumer RTX 50 series&lt;/a&gt;, only the datacenter cards, but still, it's here and we can use it. The use of Hadamard transforms should still allow consumer cards to train with some stability.&lt;/p&gt; &lt;p&gt;Here's some documentation which touches on their FP4 recipe: &lt;a href="https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb"&gt;https://github.com/NVIDIA/TransformerEngine/blob/main/docs/examples/fp8_primer.ipynb&lt;/a&gt;&lt;/p&gt; &lt;p&gt;and here's their paper which goes into detail: &lt;a href="https://arxiv.org/abs/2509.25149v1"&gt;https://arxiv.org/abs/2509.25149v1&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Kooshi_Govno"&gt; /u/Kooshi_Govno &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5n4fu/fully_functional_native_fp4_training_finally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:37:51+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5mlng</id>
    <title>Anyone think openAI will create a sequel of GPT-OSS?</title>
    <updated>2025-10-13T15:19:12+00:00</updated>
    <author>
      <name>/u/BothYou243</name>
      <uri>https://old.reddit.com/user/BothYou243</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I mean they should right? because gpt-oss (not biased or just have some grudge) is a nice model, and the rprobelm is it's just nice, so creating somethign better is still needed, anyone got any leaks about it?&lt;/p&gt; &lt;p&gt;what about anthropic, wont they drop something open, and xAI?&lt;br /&gt; xAI have poteential to outpace everyone, i am not. a fan of open sorucing some 1 year old model trend, but if they create soemthign from scracth to open source just like openAI did, it will be Absolutely Incredible! (yes taken from tim cook)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/BothYou243"&gt; /u/BothYou243 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5mlng/anyone_think_openai_will_create_a_sequel_of_gptoss/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:19:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5qo0r</id>
    <title>It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase</title>
    <updated>2025-10-13T17:44:28+00:00</updated>
    <author>
      <name>/u/waiting_for_zban</name>
      <uri>https://old.reddit.com/user/waiting_for_zban</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"&gt; &lt;img alt="It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase" src="https://external-preview.redd.it/gbZbO_XMemMwlSTTx1mACM7p7UtdxxgpOKoIWv4akso.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=da5312d01181eaa8e15816fcf71e260612f9b1af" title="It has been 4 hrs since the release of nanochat from Karpathy and no sign of it here! A new full-stack implementation of an LLM like ChatGPT in a single, clean, minimal, hackable, dependency-lite codebase" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/waiting_for_zban"&gt; /u/waiting_for_zban &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/karpathy/nanochat"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qo0r/it_has_been_4_hrs_since_the_release_of_nanochat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:44:28+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5o8z1</id>
    <title>Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!</title>
    <updated>2025-10-13T16:18:47+00:00</updated>
    <author>
      <name>/u/TheLocalDrummer</name>
      <uri>https://old.reddit.com/user/TheLocalDrummer</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt; &lt;img alt="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" src="https://external-preview.redd.it/TovPswR4pl93bt0GT2q9uuik1XMY41ZSblXtMnDzdsU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=ed07f9c93dd6fdbb3800e12511f49c15a31923c3" title="Drummer's Cydonia Redux 22B v1.1 and Behemoth ReduX 123B v1.1 - Feel the nostalgia without all the stupidity!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hot Take: Many models today are 'too smart' in a creative sense - trying too hard to be sensible and end up limiting their imagination to the user's prompt. Rerolls don't usually lead to different outcomes, and every gen seems catered to the user's expectations. Worst of all, there's an assistant bias that focuses on serving you (the user) instead of the story. All of these stifle their ability to express characters in a lively way. (inb4 skill issue)&lt;/p&gt; &lt;p&gt;Given the success of 22B and 123B ReduX v1.0, I revisited the old models and brought out a flavorful fusion of creativity and smarts through my latest tuning. 22B may not be as smart and sensible as the newer 24B, but ReduX makes it (more than) serviceable for users hoping for broader imagination and better immersion in their creative uses.&lt;/p&gt; &lt;h1&gt;Cydonia ReduX 22B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;h1&gt;Behemoth ReduX 123B v1.1: &lt;a href="https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1"&gt;https://huggingface.co/TheDrummer/Behemoth-ReduX-123B-v1.1&lt;/a&gt;&lt;/h1&gt; &lt;p&gt;Enjoy! (Please note that this is a dual release: 123B and 22B. Notice the two links in this post.)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/TheLocalDrummer"&gt; /u/TheLocalDrummer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/TheDrummer/Cydonia-Redux-22B-v1.1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5o8z1/drummers_cydonia_redux_22b_v11_and_behemoth_redux/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T16:18:47+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5qx6p</id>
    <title>4x4090 build running gpt-oss:20b locally - full specs</title>
    <updated>2025-10-13T17:53:32+00:00</updated>
    <author>
      <name>/u/RentEquivalent1671</name>
      <uri>https://old.reddit.com/user/RentEquivalent1671</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt; &lt;img alt="4x4090 build running gpt-oss:20b locally - full specs" src="https://external-preview.redd.it/oLekl_ORR7Cm_gsrJon__vT598RBB5Hxp4VkS8gKBSU.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=6c8926f5bd6382bf4684a66eaf41cb4337b53990" title="4x4090 build running gpt-oss:20b locally - full specs" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://preview.redd.it/4j5t70ot0xuf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fce49b840afd6f046d783920b7425c7627c7cbe8"&gt;https://preview.redd.it/4j5t70ot0xuf1.jpg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=fce49b840afd6f046d783920b7425c7627c7cbe8&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Made this monster by myself. &lt;/p&gt; &lt;p&gt;Configuration: &lt;/p&gt; &lt;p&gt;&lt;strong&gt;Processor:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; AMD Threadripper PRO 5975WX&lt;/p&gt; &lt;p&gt; -32 cores / 64 threads&lt;/p&gt; &lt;p&gt; -Base/Boost clock: varies by workload&lt;/p&gt; &lt;p&gt; -Av temp: 44°C&lt;/p&gt; &lt;p&gt; -Power draw: 116-117W at 7% load&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Motherboard:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; ASUS Pro WS WRX80E-SAGE SE WIFI&lt;/p&gt; &lt;p&gt; -Chipset: WRX80E&lt;/p&gt; &lt;p&gt; -Form factor: E-ATX workstation&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Memory:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Total: 256GB DDR4-3200 ECC&lt;/p&gt; &lt;p&gt; Configuration: 8x 32GB Samsung modules&lt;/p&gt; &lt;p&gt; Type: Multi-bit ECC registered&lt;/p&gt; &lt;p&gt; Av Temperature: 32-41°C across modules&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Graphics Cards:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 4x NVIDIA GeForce RTX 4090&lt;/p&gt; &lt;p&gt; VRAM: 24GB per card (96GB total)&lt;/p&gt; &lt;p&gt; Power: 318W per card (450W limit each)&lt;/p&gt; &lt;p&gt; Temperature: 29-37°C under load&lt;/p&gt; &lt;p&gt; Utilization: 81-99%&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Storage:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; Samsung SSD 990 PRO 2TB NVMe&lt;/p&gt; &lt;p&gt; -Temperature: 32-37°C&lt;/p&gt; &lt;p&gt; &lt;strong&gt;Power Supply:&lt;/strong&gt;&lt;/p&gt; &lt;p&gt; 2x XPG Fusion 1600W Platinum&lt;/p&gt; &lt;p&gt; Total capacity: 3200W&lt;/p&gt; &lt;p&gt; Configuration: Dual PSU redundant&lt;/p&gt; &lt;p&gt; Current load: 1693W (53% utilization)&lt;/p&gt; &lt;p&gt; Headroom: 1507W available &lt;/p&gt; &lt;p&gt;I run &lt;a href="https://huggingface.co/openai/gpt-oss-20b"&gt;gptoss-20b&lt;/a&gt; on each GPU and have on average 107 tokens per second. So, in total, I have like 430 t/s with 4 threads. &lt;/p&gt; &lt;p&gt;Disadvantage is, 4090 is quite old, and I would recommend to use 5090. This is my first build, this is why mistakes can happen :) &lt;/p&gt; &lt;p&gt;Advantage is, the amount of T/S. And quite good model. Of course It is not ideal and you have to make additional requests to have certain format, but my personal opinion is that gptoss-20b is the real balance between quality and quantity. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/RentEquivalent1671"&gt; /u/RentEquivalent1671 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5qx6p/4x4090_build_running_gptoss20b_locally_full_specs/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:53:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5ptit</id>
    <title>Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.</title>
    <updated>2025-10-13T17:14:18+00:00</updated>
    <author>
      <name>/u/Dentuam</name>
      <uri>https://old.reddit.com/user/Dentuam</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"&gt; &lt;img alt="Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture." src="https://external-preview.redd.it/IjppR-RE-RkBB_gQduyqs52uBDc0W1Hhz7wl-iWhgJ8.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=bde18ad695deb84f2f7185a79fef6c32828efb7f" title="Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Ring-1T, the open-source trillion-parameter thinking model built on the Ling 2.0 architecture.&lt;/p&gt; &lt;p&gt;Ring-1T achieves silver-level IMO reasoning through pure natural language reasoning.&lt;/p&gt; &lt;p&gt;→ 1 T total / 50 B active params · 128 K context window → Reinforced by Icepop RL + ASystem (Trillion-Scale RL Engine) → Open-source SOTA in natural language reasoning — AIME 25 / HMMT 25 / ARC-AGI-1 / CodeForce&lt;/p&gt; &lt;p&gt;Deep thinking · Open weights · FP8 version available&lt;/p&gt; &lt;p&gt;&lt;a href="https://x.com/AntLingAGI/status/1977767599657345027?t=jx-D236A8RTnQyzLh-sC6g&amp;amp;s=19"&gt;https://x.com/AntLingAGI/status/1977767599657345027?t=jx-D236A8RTnQyzLh-sC6g&amp;amp;s=19&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Dentuam"&gt; /u/Dentuam &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://huggingface.co/inclusionAI/Ring-1T"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5ptit/ring1t_the_opensource_trillionparameter_thinking/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T17:14:18+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5nlli</id>
    <title>Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp; More</title>
    <updated>2025-10-13T15:55:32+00:00</updated>
    <author>
      <name>/u/SouvikMandal</name>
      <uri>https://old.reddit.com/user/SouvikMandal</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt; &lt;img alt="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" src="https://b.thumbs.redditmedia.com/53soxOPlO99qPaIqPE4FAHhMXBhvv7QgMGAh0UoqSHU.jpg" title="Nanonets-OCR2: An Open-Source Image-to-Markdown Model with LaTeX, Tables, flowcharts, handwritten docs, checkboxes &amp;amp; More" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;We're excited to share &lt;strong&gt;Nanonets-OCR2&lt;/strong&gt;, a state-of-the-art suite of models designed for advanced image-to-markdown conversion and Visual Question Answering (VQA).&lt;/p&gt; &lt;p&gt;🔍 &lt;strong&gt;Key Features:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;LaTeX Equation Recognition:&lt;/strong&gt; Automatically converts mathematical equations and formulas into properly formatted LaTeX syntax. It distinguishes between inline (&lt;code&gt;$...$&lt;/code&gt;) and display (&lt;code&gt;$$...$$&lt;/code&gt;) equations.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Intelligent Image Description:&lt;/strong&gt; Describes images within documents using structured &lt;code&gt;&amp;lt;img&amp;gt;&lt;/code&gt; tags, making them digestible for LLM processing. It can describe various image types, including logos, charts, graphs and so on, detailing their content, style, and context.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Signature Detection &amp;amp; Isolation:&lt;/strong&gt; Identifies and isolates signatures from other text, outputting them within a &lt;code&gt;&amp;lt;signature&amp;gt;&lt;/code&gt; tag. This is crucial for processing legal and business documents.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Watermark Extraction:&lt;/strong&gt; Detects and extracts watermark text from documents, placing it within a &lt;code&gt;&amp;lt;watermark&amp;gt;&lt;/code&gt; tag.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Smart Checkbox Handling:&lt;/strong&gt; Converts form checkboxes and radio buttons into standardized Unicode symbols (&lt;code&gt;☐&lt;/code&gt;, &lt;code&gt;☑&lt;/code&gt;, &lt;code&gt;☒&lt;/code&gt;) for consistent and reliable processing.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Complex Table Extraction:&lt;/strong&gt; Accurately extracts complex tables from documents and converts them into both markdown and HTML table formats.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Flow charts &amp;amp; Organisational charts:&lt;/strong&gt; Extracts flow charts and organisational as &lt;a href="https://huggingface.co/nanonets/Nanonets-OCR2-1.5B-exp/blob/main/mermaid.js.org"&gt;mermaid&lt;/a&gt; code.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Handwritten Documents:&lt;/strong&gt; The model is trained on handwritten documents across multiple languages.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Model is trained on documents of multiple languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Arabic, and many more.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Visual Question Answering (VQA):&lt;/strong&gt; The model is designed to provide the answer directly if it is present in the document; otherwise, it responds with &amp;quot;Not mentioned.&amp;quot;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;a href="https://docstrange.nanonets.com/"&gt;🖥️ Live Demo&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://nanonets.com/research/nanonets-ocr-2"&gt;📢 Blog&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://github.com/NanoNets/docstrange"&gt;⌨️ GitHub&lt;/a&gt;&lt;/p&gt; &lt;p&gt;🤗 &lt;a href="https://huggingface.co/collections/nanonets/nanonets-ocr2-68ed207f17ee6c31d226319e"&gt;Huggingface models&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/7ct2hbi3hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ea00f9623db4529514533820223b2fb53be4767d"&gt;Document with equation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/q8lglwi5hwuf1.png?width=2936&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c4a1316e250f7f244f6e253d66c8ebf1ba105313"&gt;Document with complex checkboxes&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/bnmpapq7hwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=8bcc88b138a553c7760d6e46319b864802339913"&gt;Quarterly Report (Please use the Markdown(Financial Docs) for best result in docstrange demo)&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/1pg5h8hfhwuf1.png?width=2333&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=188c4c94452ae027c54e4cad4dbbc60e2b12e9e9"&gt;Signatures&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/ecxe2o81iwuf1.png?width=2516&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=008fce272c2979b00e0033c34ffcd2b0d69cb24c"&gt;mermaid code for flowchart&lt;/a&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/jytsym6eiwuf1.png?width=2462&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=65d8a6f82b9fc2e9cd5b30529b152ca7339d7a8c"&gt;Visual Question Answering&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Feel free to try it out and share your feedback.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/SouvikMandal"&gt; /u/SouvikMandal &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5nlli/nanonetsocr2_an_opensource_imagetomarkdown_model/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T15:55:32+00:00</published>
  </entry>
  <entry>
    <id>t3_1o5v78n</id>
    <title>The top open models on are now all by Chinese companies</title>
    <updated>2025-10-13T20:27:10+00:00</updated>
    <author>
      <name>/u/k_schaul</name>
      <uri>https://old.reddit.com/user/k_schaul</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt; &lt;img alt="The top open models on are now all by Chinese companies" src="https://preview.redd.it/xhsv9ilkuxuf1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=17f3ce5e0a0548bdb8546f46e0f43b1b008af719" title="The top open models on are now all by Chinese companies" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Full analysis here (🎁 gift link): &lt;a href="https://wapo.st/4nPUBud"&gt;wapo.st/4nPUBud&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/k_schaul"&gt; /u/k_schaul &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/xhsv9ilkuxuf1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1o5v78n/the_top_open_models_on_are_now_all_by_chinese/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-10-13T20:27:10+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
