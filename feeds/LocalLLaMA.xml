<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <id>/r/LocalLLaMA/.rss</id>
  <title>LocalLlama</title>
  <updated>2026-01-25T17:06:11+00:00</updated>
  <link href="https://old.reddit.com/r/LocalLLaMA/" rel="alternate"/>
  <generator uri="https://lkiesow.github.io/python-feedgen" version="1.0.0">python-feedgen</generator>
  <icon>https://www.redditstatic.com/icon.png/</icon>
  <subtitle>Subreddit to discuss AI &amp; Llama, the large language model created by Meta AI.</subtitle>
  <entry>
    <id>t3_1qmlyhn</id>
    <title>GLM-4.7-flash on RTX 6000 pro</title>
    <updated>2026-01-25T15:24:37+00:00</updated>
    <author>
      <name>/u/gittb</name>
      <uri>https://old.reddit.com/user/gittb</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hello, I‚Äôm getting horrible throughput considering the models size with vLLM.&lt;/p&gt; &lt;p&gt;Currently with 2x cards and DP 2 @ FP16 I‚Äôm getting around 370 gen TPS with 10x requests.&lt;/p&gt; &lt;p&gt;Anyone have a fix or a ‚Äúworking‚Äù config for 1 or two cards?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/gittb"&gt; /u/gittb &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlyhn/glm47flash_on_rtx_6000_pro/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:24:37+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlwibf</id>
    <title>What is the best general-purpose model to run locally on 24GB of VRAM in 2026?</title>
    <updated>2026-01-24T19:35:11+00:00</updated>
    <author>
      <name>/u/Paganator</name>
      <uri>https://old.reddit.com/user/Paganator</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I've been running Gemma 3 27b since its release nine months ago, which is an eternity in the AI field. Has anything better been released since then that can run well on a single 3090ti?&lt;/p&gt; &lt;p&gt;I'm not looking to code, to create agents, or to roleplay; I just want a good model to chat with and get reasonably smart answers to questions. If it can view images, that's even better.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Paganator"&gt; /u/Paganator &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlwibf/what_is_the_best_generalpurpose_model_to_run/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T19:35:11+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmj900</id>
    <title>Sanity check for small-office/homelab shopping cart.</title>
    <updated>2026-01-25T13:34:22+00:00</updated>
    <author>
      <name>/u/artisticMink</name>
      <uri>https://old.reddit.com/user/artisticMink</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey, I'm about to purchase some equipment for prototyping and need a sanity check. Also perhaps some of you guys have better ideas for a setup up to 5k ‚Ç¨.&lt;/p&gt; &lt;p&gt;Here's a list of models i want to run:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Qwen2.5-Math-7B-instruct&lt;/li&gt; &lt;li&gt;Nemotron-Orchestrator-8B&lt;/li&gt; &lt;li&gt;NuMarkdown-8B-Thinking&lt;/li&gt; &lt;li&gt;Qwen3-8B&lt;/li&gt; &lt;li&gt;Qwen3-Embedding-8B&lt;/li&gt; &lt;li&gt;xLAM-2-32b-fc-r&lt;/li&gt; &lt;li&gt;gpt-oss-120b&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Being able to try ~70B dense models and large MOE would be nice, but that's negligible.&lt;/p&gt; &lt;p&gt;My use case is process automation, so I'll likely have an orchestrator model + 2-3 8b + gpt-oss-120b or a 32b dense in memory.&lt;/p&gt; &lt;p&gt;There are three setups that i consider:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup #1&lt;/strong&gt;&lt;br /&gt; Used Rack Server&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Gigabyte G221-Z30 Rev. A00 - 1.200‚Ç¨&lt;/li&gt; &lt;li&gt;AMD EPYC 7402P - Included in rack server&lt;/li&gt; &lt;li&gt;256GB DDR4-3200 (8x32GB) - (2.000‚Ç¨)&lt;/li&gt; &lt;li&gt;Radeon AI Pro R9700 32GB - (1.500‚Ç¨)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sum: 4.700‚Ç¨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup #2&lt;/strong&gt;&lt;br /&gt; Linked Strix halo&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2 gmktec evo-x2 128GB (2000‚Ç¨)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sum: 4.000‚Ç¨&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Setup #3&lt;/strong&gt;&lt;br /&gt; Built from inventory&lt;/p&gt; &lt;ul&gt; &lt;li&gt;B650 mainboard (8x/8x PCIE 4.0, should be fine), from inventory&lt;/li&gt; &lt;li&gt;64GB DDR5@5600, from inventory&lt;/li&gt; &lt;li&gt;Additional Ryzen 7900X or consumer epyc ~400‚Ç¨&lt;/li&gt; &lt;li&gt;2 x Radeon AI Pro R9700 (1500‚Ç¨)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Sum: 3.400‚Ç¨&lt;/p&gt; &lt;p&gt;I'm currently leaning towards &lt;strong&gt;#3&lt;/strong&gt;. It's short on RAM and large moe experimentation is out of question. Butt i can use the two R9700 for an actual production build should the need arise and it's the cheapest.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;#2&lt;/strong&gt; is the easiest solution but doesn't sale at all. &lt;strong&gt;#1&lt;/strong&gt; would probably be the overall best, but I've a hard time justifying to miself paying 2k for DDR4 RAM.&lt;/p&gt; &lt;p&gt;Any thoughts on my horrible financial decisions?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/artisticMink"&gt; /u/artisticMink &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmj900/sanity_check_for_smallofficehomelab_shopping_cart/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmj900/sanity_check_for_smallofficehomelab_shopping_cart/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmj900/sanity_check_for_smallofficehomelab_shopping_cart/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:34:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmn4jq</id>
    <title>Current best iOS LLM app for running LLMs?</title>
    <updated>2026-01-25T16:07:40+00:00</updated>
    <author>
      <name>/u/Adventurous-Gold6413</name>
      <uri>https://old.reddit.com/user/Adventurous-Gold6413</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Is pocket pal the best?&lt;/p&gt; &lt;p&gt;I seem to have some errors with vision models working properly&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Adventurous-Gold6413"&gt; /u/Adventurous-Gold6413 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmn4jq/current_best_ios_llm_app_for_running_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmn4jq/current_best_ios_llm_app_for_running_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmn4jq/current_best_ios_llm_app_for_running_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T16:07:40+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmo3do</id>
    <title>LM Studio - Why does my system RAM fill up and go OOM if the model says Full GPU Offload Possible?</title>
    <updated>2026-01-25T16:42:16+00:00</updated>
    <author>
      <name>/u/Nytse</name>
      <uri>https://old.reddit.com/user/Nytse</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Using Windows, RTX 3090 (24 GB VRAM) upgraded recently from a GTX 1080 (8 GB), 32 GB RAM&lt;/p&gt; &lt;p&gt;With Firefox open with many tabs I use ~18 GB RAM. GPU stays at ~3 GB.&lt;/p&gt; &lt;p&gt;Then, in LM Studio, loading the OpenAI GPT‚ÄëOSS 20B model shows ‚ÄúFull GPU Offload Possible‚Äù. After load, VRAM jumps to ~14 GB and system RAM climbs to 32 GB, then the program crashes with OOM.&lt;/p&gt; &lt;p&gt;I have Strict Guardrails enabled, swap is on.&lt;/p&gt; &lt;p&gt;How can I avoid high RAM usage and the OOM when loading this model while using by browser? How do I know how much allocated RAM the model will have?&lt;/p&gt; &lt;p&gt;I thought that the gguf file size is similar to the VRAM allocation and only like 1 GB RAM is reserved if the model fits in the GPU. &lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Nytse"&gt; /u/Nytse &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmo3do/lm_studio_why_does_my_system_ram_fill_up_and_go/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T16:42:16+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmod1n</id>
    <title>Does the Mac Mini M4 16GB have any potential value?</title>
    <updated>2026-01-25T16:51:59+00:00</updated>
    <author>
      <name>/u/NoYogurtcloset4090</name>
      <uri>https://old.reddit.com/user/NoYogurtcloset4090</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm currently considering buying a Mac Mini M4 16GB 256GB, because it's at a historically low price. After deducting the 4GB of RAM for the macOS system, can the remaining 12GB theoretically run speech recognition, image recognition, and LLM simultaneously? If so, how well would it perform? I already have a Windows PC with 32GB RAM and 16GB VRAM, and I have some experience with ComfyUI and Ollama. I know nothing about macOS and don't consider computers with more than 16GB of RAM to be cost-effective. Should I abandon this idea?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/NoYogurtcloset4090"&gt; /u/NoYogurtcloset4090 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmod1n/does_the_mac_mini_m4_16gb_have_any_potential_value/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmod1n/does_the_mac_mini_m4_16gb_have_any_potential_value/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmod1n/does_the_mac_mini_m4_16gb_have_any_potential_value/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T16:51:59+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm2q0c</id>
    <title>Claude Code, but locally</title>
    <updated>2026-01-24T23:37:23+00:00</updated>
    <author>
      <name>/u/Zealousideal-Egg-362</name>
      <uri>https://old.reddit.com/user/Zealousideal-Egg-362</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi,&lt;/p&gt; &lt;p&gt;I'm looking for advice if there is realistic replacement for anthropic's models. Looking to run claude code with models that ideally are snappier and wondering if it's possible at all to replicate the opus model on own hardware.&lt;/p&gt; &lt;p&gt;What annoys me the most is speed, especially when west coast wakes up (I'm in EU). I'd be happy to prompt more, but have model that's more responsive. Opus 4.5 i great, but the context switches totally kill my flow and I feel extremely tired in the end of the day.&lt;/p&gt; &lt;p&gt;Did some limited testing of different models via openrouter, but the landscape is extremely confusing. glm-4.7 seems like a nice coding model, but is there any practical realistic replacement for Opus 4.5?&lt;/p&gt; &lt;p&gt;Edit: I‚Äôm asking very clearly for directions how/what to replace Opus and getting ridiculously irrelevant advice ‚Ä¶&lt;/p&gt; &lt;p&gt;My budget is 5-7k&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Zealousideal-Egg-362"&gt; /u/Zealousideal-Egg-362 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm2q0c/claude_code_but_locally/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T23:37:23+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmap5e</id>
    <title>Best &lt;4B dense models today?</title>
    <updated>2026-01-25T05:46:01+00:00</updated>
    <author>
      <name>/u/Admirable_Flower_287</name>
      <uri>https://old.reddit.com/user/Admirable_Flower_287</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I think small(&amp;lt;4B) dense models are basically the only practical option for general users. But hasn't there been almost no progress since Gemma 3 4B came out? Are there any alternatives?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Admirable_Flower_287"&gt; /u/Admirable_Flower_287 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmap5e/best_4b_dense_models_today/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T05:46:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm0l2q</id>
    <title>I built a tool that learns your codebase's unwritten rules and conventions- no AI, just AST parsing</title>
    <updated>2026-01-24T22:11:05+00:00</updated>
    <author>
      <name>/u/Fluffy_Citron3547</name>
      <uri>https://old.reddit.com/user/Fluffy_Citron3547</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I spent the last six months teaching myself to orchestrate engineering codebases using AI agents. What I found is that the biggest bottleneck isn‚Äôt intelligence it‚Äôs the context window. Why have we not given agents the proper tooling to defeat this limitation? Agents constantly forget how I handle error structures or which specific components I use for the frontend. This forces mass auditing and refactoring, causing me to spend about 75% of my token budget on auditing versus writing.&lt;/p&gt; &lt;p&gt;That is why I built Drift. Drift is a first-in-class codebase intelligence tool that leverages semantic learning through AST parsing with Regex fallbacks. It scans your codebase and extracts 15 different categories with over 150 patterns. Everything is persisted and recallable via CLI or MCP in your IDE of choice.&lt;/p&gt; &lt;p&gt;What makes drift different?&lt;/p&gt; &lt;p&gt;It‚Äôs learning based not rule based. AI is capable of writing high quality code but the context limitation makes fitting conventions through a large code base extremely tedious and time consuming often leading to things silently failing or just straight up not working. &lt;/p&gt; &lt;p&gt;Drift_context is the real magic &lt;/p&gt; &lt;p&gt;Instead of an agent calling 10 tools and sytheneszing results it: &lt;/p&gt; &lt;p&gt;Takes intent &lt;/p&gt; &lt;p&gt;Takes focus area&lt;/p&gt; &lt;p&gt;Returned a curated package&lt;/p&gt; &lt;p&gt;This eliminates the audit loop, hallucination risk and gives the agent everything needed in one call.&lt;/p&gt; &lt;p&gt;Call graph analysis across 6 different languages&lt;/p&gt; &lt;p&gt;Not just ‚ÄúWhat functions exists‚Äù but..&lt;/p&gt; &lt;p&gt;Drift_reachability_forward &amp;gt; What data can this code access? (Massive for helping with security)&lt;/p&gt; &lt;p&gt;Drift_reachability_inverse &amp;gt; Who can access this field? &lt;/p&gt; &lt;p&gt;Drift_impact_analysis &amp;gt; what breaks if I change this with scoring.&lt;/p&gt; &lt;p&gt;Security-audit-grade analysis available to you or your agent through MCP or CLI&lt;/p&gt; &lt;p&gt;The MCP has been built out with frontier capabilities ensuring context is preserved and is a true tool for your agents&lt;/p&gt; &lt;p&gt;Currently support TS, PY, Java, C#, PHP, GO :&lt;/p&gt; &lt;p&gt;with‚Ä¶&lt;/p&gt; &lt;p&gt;Tree sitter parsing&lt;/p&gt; &lt;p&gt;Regex fallback&lt;/p&gt; &lt;p&gt;Framework aware detection&lt;/p&gt; &lt;p&gt;All data persist into a local file (/.drift) and you have the ability to approve, deny and ignore certain components, functions and features you don‚Äôt want the agent to be trained on.&lt;/p&gt; &lt;p&gt;check it out here: &lt;/p&gt; &lt;p&gt;IF you run into any edge cases or I don‚Äôt support the framework your code base is currently running on open a git issue feature request and ive been banging them out quick&lt;/p&gt; &lt;p&gt;Thank you for all the upvotes and stars on the project it means so much!&lt;/p&gt; &lt;p&gt;check it out here: &lt;a href="https://github.com/dadbodgeoff/drift"&gt;https://github.com/dadbodgeoff/drift&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Fluffy_Citron3547"&gt; /u/Fluffy_Citron3547 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm0l2q/i_built_a_tool_that_learns_your_codebases/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T22:11:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qm6iho</id>
    <title>Stable-DiffCoder, a strong code diffusion LLM built on Seed-Coder</title>
    <updated>2026-01-25T02:22:05+00:00</updated>
    <author>
      <name>/u/rektide</name>
      <uri>https://old.reddit.com/user/rektide</uri>
    </author>
    <content type="html">&amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/rektide"&gt; /u/rektide &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://bytedance-seed.github.io/Stable-DiffCoder/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qm6iho/stablediffcoder_a_strong_code_diffusion_llm_built/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T02:22:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qltwza</id>
    <title>Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence.</title>
    <updated>2026-01-24T18:00:50+00:00</updated>
    <author>
      <name>/u/self-fix</name>
      <uri>https://old.reddit.com/user/self-fix</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt; &lt;img alt="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." src="https://preview.redd.it/66fd18ro6cfg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=f579cce389f709dbf297867095118be2027f04ea" title="Artificial Analysis: South Korea üá∞üá∑ is now the clear #3 nation in AI ‚Äî powered by the Korean National Sovereign AI Initiative there are now multiple Korean AI labs with near frontier intelligence." /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;a href="https://x.com/ArtificialAnlys/status/2014786516153991339"&gt;https://x.com/ArtificialAnlys/status/2014786516153991339&lt;/a&gt;&lt;/p&gt; &lt;p&gt;A key driver of this momentum is the Korean National Sovereign AI Initiative, a government-backed, nationwide competition that incentivizes domestic model development through a multi-stage elimination process. The initiative shortlists national champions, with winners receiving direct government funding and guaranteed access to large-scale GPU capacity.&lt;/p&gt; &lt;p&gt;‚û§ In August 2025, five organizations were selected: Naver, SK Telecom, LG Group, Upstage, and NC AI&lt;/p&gt; &lt;p&gt;‚û§ In the most recent round announced last week, the field narrowed to three: LG, SK Telecom, and Upstage.&lt;/p&gt; &lt;p&gt;‚û§ A fourth finalist is expected to be selected in the coming months as the evaluation process continues&lt;/p&gt; &lt;p&gt;Generally, top Korean AI models tend to be open weights, and vary in size ranging from Motif‚Äòs 12.7B Thinking model to LG‚Äôs 236B K-EXAONE. Other models, such as Korea Telecom (KT)‚Äôs Mi:dm K 2.5 Pro, are proprietary and developed with a focus on business integration with existing KT clients.&lt;/p&gt; &lt;p&gt;Overview of major releases:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ LG | K-EXAONE -&lt;/strong&gt; The current leader in the Korean AI race and a shortlisted model in the Korean National Sovereign AI Initiative. K-EXAONE is a 236B open weights model and scores 32 on the Artificial Analysis Intelligence Index. K-EXAONE performs strongly across various intelligence evaluations from scientific reasoning, instruction following, to agentic coding. However, this model has high verbosity, using 100 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Upstage | Solar Open -&lt;/strong&gt; Another shortlisted model in the Korean National Sovereign AI Initiative. Solar Open is a 100B open-weights model and scores 21 on the Artificial Analysis Intelligence Index. Solar Open performs well in instruction following and has lower hallucination rate compared to peer Korean models&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Naver | HyperCLOVA X SEED Think -&lt;/strong&gt; A 32B open weights reasoning model that scores 24 on the Artificial Analysis Intelligence Index. HyperCLOVA X SEED Think demonstrates strong performance on agentic tool-use workflows and scores highly in the Global MMLU Lite multilingual index for Korean, highlighting its potential usefulness in a primarily Korean language environment&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Korea Telecom | Mi:dm K 2.5 Pro -&lt;/strong&gt; A proprietary reasoning model that scores 23 on the Artificial Analysis Intelligence Index. Mi:dm K 2.5 Pro sees strong performance in agentic tool-use. Mi:dm K 2.5 Pro currently has no publicly available endpoint. Instead, Korea Telecom primarily intends to package this model into product offerings and use this model to serve KT‚Äôs clients&lt;/p&gt; &lt;p&gt;&lt;strong&gt;‚û§ Motif | Motif-2-12.7B -&lt;/strong&gt; A small open weights model that scores 24 on the Artificial Analysis Intelligence Index. Motif-2-12.7B performs well in long-context reasoning and knowledge, but is highly token intensive - using 120 million tokens to run the Artificial Analysis evaluation suite&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/self-fix"&gt; /u/self-fix &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/66fd18ro6cfg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qltwza/artificial_analysis_south_korea_is_now_the_clear/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T18:00:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmbevn</id>
    <title>Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?</title>
    <updated>2026-01-25T06:22:22+00:00</updated>
    <author>
      <name>/u/MadPelmewka</name>
      <uri>https://old.reddit.com/user/MadPelmewka</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"&gt; &lt;img alt="Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?" src="https://preview.redd.it/r0ec0m21vffg1.jpeg?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=275f52d7dfb944441f74dea25d60f8c3e620a766" title="Distilling Gemini 3 Flash visual reasoning into Qwen 3 VL 32B for synthetic captioning. Is SFT enough?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I am working on a synthetic data pipeline for training high-precision image-to-image models (Flux Klein and Qwen Image Edit). I have reached a point where standard tagging and current open-weights VL models are the main bottleneck for data quality.&lt;/p&gt; &lt;p&gt;I have benchmarked almost every trending VL model on HuggingFace and those leading the MMMU-Pro leaderboard. My conclusion is that even the best open models are &amp;quot;blind&amp;quot; to complex anatomical layering and spatial reasoning.&lt;/p&gt; &lt;p&gt;The problem is best described by the &amp;quot;Horns Issue&amp;quot; (see attached image). If a character has large organic dragon horns and a headband with small decorative horns, every open VLM I tested merges them into one generic attribute. They fail to distinguish between base anatomy and removable accessories. Gemini 3 Flash, however, is on a completely different level‚Äîit accurately describes every layer and understands the distinction perfectly.&lt;/p&gt; &lt;p&gt;My plan is to fine-tune Qwen 3 VL 32B Instruct on a dataset labeled by Gemini 3 Flash. I want to transfer that visual reasoning so I can have a local engine for high-scale synthetic captioning.&lt;/p&gt; &lt;p&gt;A few technical questions:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Can Qwen 3 VL actually absorb this level of reasoning via SFT if it lacks the native &amp;quot;thinking&amp;quot; or CoT process Gemini uses?&lt;/li&gt; &lt;li&gt;Is the &amp;quot;blindness&amp;quot; in open models a limitation of the vision encoder itself, or is it purely a reasoning capability issue on the LLM side?&lt;/li&gt; &lt;li&gt;Has anyone here tried this kind of VLM-to-VLM distillation for high-scale labeling in generative AI pipelines?&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;I am trying to build a local captioner that matches proprietary accuracy. Any insights on the plasticity of Qwen 32B for this specific task would be appreciated.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/MadPelmewka"&gt; /u/MadPelmewka &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/r0ec0m21vffg1.jpeg"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmbevn/distilling_gemini_3_flash_visual_reasoning_into/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T06:22:22+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmev6q</id>
    <title>[R] Open-sourcing an unfinished research project: A Self-Organizing, Graph-Based Alternative to Transformers (Looking for feedback or continuation</title>
    <updated>2026-01-25T09:40:12+00:00</updated>
    <author>
      <name>/u/WriedGuy</name>
      <uri>https://old.reddit.com/user/WriedGuy</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;I‚Äôm sharing a research project I worked on over a long period but had to pause due to personal reasons. Rather than letting it sit idle, I wanted to open it up to the community either for technical feedback, critique, or for anyone interested in continuing or experimenting with it.&lt;/p&gt; &lt;p&gt;The main project is called Self-Organizing State Model (SOSM): &lt;a href="https://github.com/PlanetDestroyyer/Self-Organizing-State-Model"&gt;https://github.com/PlanetDestroyyer/Self-Organizing-State-Model&lt;/a&gt;&lt;/p&gt; &lt;p&gt;At a high level, the goal was to explore an alternative to standard Transformer attention by:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;p&gt;Using graph-based routing instead of dense attention&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Separating semantic representation and temporal pattern learning&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;Introducing a hierarchical credit/attribution mechanism for better interpretability&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The core system is modular and depends on a few supporting components: Semantic representation module (MU) &lt;a href="https://github.com/PlanetDestroyyer/MU"&gt;https://github.com/PlanetDestroyyer/MU&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Temporal pattern learner (TEMPORAL) &lt;a href="https://github.com/PlanetDestroyyer/TEMPORAL"&gt;https://github.com/PlanetDestroyyer/TEMPORAL&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Hierarchical / K-1 self-learning mechanism &lt;a href="https://github.com/PlanetDestroyyer/self-learning-k-1"&gt;https://github.com/PlanetDestroyyer/self-learning-k-1&lt;/a&gt;&lt;/p&gt; &lt;p&gt;I‚Äôm honestly not sure how valuable or novel this work is that‚Äôs exactly why I‚Äôm posting it here. If nothing else, I‚Äôd really appreciate constructive criticism, architectural feedback, or pointers to related work that overlaps with these ideas. If someone finds parts of it useful (or wants to take it further, refactor it, or formalize it into a paper), they‚Äôre more than welcome to do so. The project is open-source, and I‚Äôm happy to answer questions or clarify intent where needed.&lt;/p&gt; &lt;p&gt;Thanks for taking a look.&lt;/p&gt; &lt;p&gt;Summary:&lt;/p&gt; &lt;p&gt;This work explores a language model architecture based on structured semantics rather than unstructured embeddings. Instead of positional encodings, a temporal learning module is used to model sequence progression and context flow. A K-1 hierarchical system is introduced to provide interpretability, enabling analysis of how a token is predicted and which components, states, or nodes contribute to that prediction. Most importantly, rather than comparing every token with all others (as in full self-attention), the model uses a graph-based connection mechanism that restricts computation to only the most relevant or necessary tokens, enabling selective reasoning and improved efficiency.&lt;/p&gt; &lt;p&gt;(Have used claude code to code )&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/WriedGuy"&gt; /u/WriedGuy &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmev6q/r_opensourcing_an_unfinished_research_project_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:12+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmmpwz</id>
    <title>DGX spark performance falls short</title>
    <updated>2026-01-25T15:53:00+00:00</updated>
    <author>
      <name>/u/dereksodo</name>
      <uri>https://old.reddit.com/user/dereksodo</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;using cutlass-profiler, gemm, here is the performance:&lt;/p&gt; &lt;p&gt;peak int4: 157 TFLOP&lt;/p&gt; &lt;p&gt;peak int8: 200 TFLOP&lt;/p&gt; &lt;p&gt;peak fp16: 97 TFLOP&lt;/p&gt; &lt;p&gt;anyone knows why performance of int4 is not around 350-450( which i expect)?&lt;/p&gt; &lt;p&gt;env: docker (pytorch:25.12-py3)&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/dereksodo"&gt; /u/dereksodo &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmmpwz/dgx_spark_performance_falls_short/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:53:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjyxl</id>
    <title>Understanding Multi-Head Latent Attention (MLA)</title>
    <updated>2026-01-25T14:05:43+00:00</updated>
    <author>
      <name>/u/shreyansh26</name>
      <uri>https://old.reddit.com/user/shreyansh26</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;A short deep-dive on Multi-Head Latent Attention (MLA) (from DeepSeek): intuition + math, then a walk from MHA ‚Üí GQA ‚Üí MQA ‚Üí MLA, with PyTorch code and the fusion/absorption optimizations for KV-cache efficiency.&lt;/p&gt; &lt;p&gt;&lt;a href="http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/"&gt;http://shreyansh26.github.io/post/2025-11-08_multihead-latent-attention/&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/shreyansh26"&gt; /u/shreyansh26 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjyxl/understanding_multihead_latent_attention_mla/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:05:43+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmljeb</id>
    <title>What are the best open source coding ideas you can share?</title>
    <updated>2026-01-25T15:08:39+00:00</updated>
    <author>
      <name>/u/secopsml</name>
      <uri>https://old.reddit.com/user/secopsml</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt; &lt;img alt="What are the best open source coding ideas you can share?" src="https://preview.redd.it/zcf9q42bgifg1.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=e8d5b9a2f9d2a8bbb940fa1ae8f1c616ca45968f" title="What are the best open source coding ideas you can share?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;I'm trying to build a place for my friends so they can try and learn ai assisted engineering/vibe coding. Some of them are 50 yrs experienced devs familiar with enterprise standards, some 16 yrs old vibe coders that want to build their first scripts.&lt;/p&gt; &lt;p&gt;How would you structure guide for newcomers? Any favourite tools I should add/replace?&lt;/p&gt; &lt;p&gt;What would you choose for 24h hackathon and what is more suitable for weeks/months project?&lt;/p&gt; &lt;p&gt;repo: &lt;a href="https://github.com/dontriskit/awesome-ai-software-engineering"&gt;https://github.com/dontriskit/awesome-ai-software-engineering&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/secopsml"&gt; /u/secopsml &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://i.redd.it/zcf9q42bgifg1.png"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmljeb/what_are_the_best_open_source_coding_ideas_you/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:08:39+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmhvuz</id>
    <title>Quantifying Hallucinations: By calculating a multi-dimensional 'Trust Score' for LLM outputs.</title>
    <updated>2026-01-25T12:30:24+00:00</updated>
    <author>
      <name>/u/Charming_Group_2950</name>
      <uri>https://old.reddit.com/user/Charming_Group_2950</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;&lt;strong&gt;The problem:&lt;/strong&gt;&lt;br /&gt; You build a RAG system. It gives an answer. It sounds right.&lt;br /&gt; But is it actually grounded in your data, or just hallucinating with confidence?&lt;br /&gt; A single &amp;quot;correctness&amp;quot; or &amp;quot;relevance&amp;quot; score doesn‚Äôt cut it anymore, especially in enterprise, regulated, or governance-heavy environments. We need to know why it failed. &lt;/p&gt; &lt;p&gt;&lt;strong&gt;My solution:&lt;/strong&gt;&lt;br /&gt; Introducing &lt;strong&gt;TrustifAI&lt;/strong&gt; ‚Äì a framework designed to quantify, explain, and debug the trustworthiness of AI responses. &lt;/p&gt; &lt;p&gt;Instead of pass/fail, it computes a multi-dimensional Trust Score using signals like:&lt;br /&gt; * Evidence Coverage: Is the answer actually supported by retrieved documents?&lt;br /&gt; * Epistemic Consistency: Does the model stay stable across repeated generations?&lt;br /&gt; * Semantic Drift: Did the response drift away from the given context?&lt;br /&gt; * Source Diversity: Is the answer overly dependent on a single document?&lt;br /&gt; * Generation Confidence: Uses token-level log probabilities at inference time to quantify how confident the model was while generating the answer (not after judging it).&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt;&lt;br /&gt; TrustifAI doesn‚Äôt just give you a number - it gives you traceability.&lt;br /&gt; It builds &lt;strong&gt;Reasoning Graphs (DAGs)&lt;/strong&gt; and &lt;strong&gt;Mermaid visualizations&lt;/strong&gt; that show why a response was flagged as reliable or suspicious.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;How is this different from LLM Evaluation frameworks:&lt;/strong&gt;&lt;br /&gt; All popular Eval frameworks measure how good your RAG system is, but&lt;br /&gt; TrustifAI tells you why you should (or shouldn‚Äôt) trust a specific answer - with explainability in mind.&lt;/p&gt; &lt;p&gt;Since the library is in its early stages, I‚Äôd genuinely love community feedback.&lt;br /&gt; ‚≠ê the repo if it helps üòÑ&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Get started:&lt;/strong&gt; &lt;code&gt;pip install trustifai&lt;/code&gt;&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Github link:&lt;/strong&gt; &lt;a href="https://github.com/Aaryanverma/trustifai"&gt;https://github.com/Aaryanverma/trustifai&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/Charming_Group_2950"&gt; /u/Charming_Group_2950 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1qmhvuz"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmhvuz/quantifying_hallucinations_by_calculating_a/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T12:30:24+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmevh7</id>
    <title>Blazing fast JSON extraction with very small LLMs-3B: LSTM to LLM</title>
    <updated>2026-01-25T09:40:42+00:00</updated>
    <author>
      <name>/u/memphet</name>
      <uri>https://old.reddit.com/user/memphet</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt; &lt;p&gt;I've learned a lot from this sub, so I wanted to give back by sharing my experience on a recent project.&lt;/p&gt; &lt;p&gt;My goal was to migrate a text extraction pipeline from LSTM to an LLM. The task involves extracting specific data into JSON format from small text inputs (‚âà1024 tokens). I used in-house data to fine-tune it.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Constraints &amp;amp; Achievements (running on an L4 GPU):&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Very low end2end latency:&lt;/strong&gt; &amp;lt;500ms&lt;/li&gt; &lt;li&gt;&lt;strong&gt;High throughput:&lt;/strong&gt; ‚âà30 RPM (requests per minute)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Reliability:&lt;/strong&gt; 0.99 accuracy&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;The Model:&lt;/strong&gt;&lt;br /&gt; I tested quite a few models for this task.&lt;br /&gt; Ultimately, HuggingFaceTB/SmolLM3-3B was the best fit for our needs.&lt;br /&gt; I also had very strong results with Qwen/Qwen3-4B-Instruct and Ministral&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Here is what I learned:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Fine-tuning parameters matter less than I thought:&lt;/strong&gt; I didn't see huge gains from strictly tweaking hyperparameters. I ran extensive hyperparameter optimization only to find that simply increasing the number of epochs yielded the best (slight) improvements.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Data is king:&lt;/strong&gt; Poor labeling logic and bad data quality hurt me the most. If I had to redo it, I would spend much more time cleaning and validating the dataset upfront.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Small LLMs struggle with Proper Nouns:&lt;/strong&gt; I noticed about a 10% error rate on names! A significant performance boost came from adding a simple post-processing step using Levenshtein distance to correct names extracted by the LLM against the input text (correcting &amp;quot;Jammes&amp;quot; -&amp;gt; &amp;quot;James&amp;quot;).&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Efficiency Gains:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Quantization:&lt;/strong&gt; Obviously the best bang for your buck. I recommend &lt;strong&gt;FP8&lt;/strong&gt; using llm-compressor if you have a Lovelace GPU or newer. Otherwise, &lt;strong&gt;AWQ&lt;/strong&gt; is solid. &lt;ul&gt; &lt;li&gt;Gain: ~50% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Output Formatting:&lt;/strong&gt; You want to generate as few tokens as possible. Instead of fine-tuning for a verbose JSON output like {&amp;quot;key1&amp;quot;: &amp;quot;value1&amp;quot;, &amp;quot;key2&amp;quot;: &amp;quot;value2&amp;quot;}, I fine-tuned the model to output just the values: value1,value2. &lt;ul&gt; &lt;li&gt;Gain: ~30% speed boost.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;What didn't work (for me):&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;I really tried to make &lt;strong&gt;Speculative Decoding&lt;/strong&gt; work with vLLM. In theory, I expected gains even with just n-gram speculative decoding, but I didn't observe any improvement. I did see some speedup using Qwen 0.7B draft model, but since I ultimately chose a different base model architecture, I couldn't use them effectively. Plus, maintaining a base model + a draft model is a pain, which is also why I didn't go with Eagle.&lt;/p&gt; &lt;p&gt;If you have suggestions to squeeze out more performance or thoughts on the setup, I'm all ears!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/memphet"&gt; /u/memphet &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmevh7/blazing_fast_json_extraction_with_very_small/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T09:40:42+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmh3si</id>
    <title>What happened to moondream3?</title>
    <updated>2026-01-25T11:49:01+00:00</updated>
    <author>
      <name>/u/StableDiffer</name>
      <uri>https://old.reddit.com/user/StableDiffer</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;So last year the moondream 3 preview came out. It was a nice performing visual model that could do some cool stuff other VL models couldn't. One month ago a MLX version appeared &lt;a href="https://huggingface.co/moondream/md3p-int4"&gt;https://huggingface.co/moondream/md3p-int4&lt;/a&gt; but until now there is no llama.cpp implementation and no public activity I could find.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/StableDiffer"&gt; /u/StableDiffer &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmh3si/what_happened_to_moondream3/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T11:49:01+00:00</published>
  </entry>
  <entry>
    <id>t3_1qlzbhh</id>
    <title>[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp; OpenAI-Compatible API</title>
    <updated>2026-01-24T21:21:50+00:00</updated>
    <author>
      <name>/u/blackstoreonline</name>
      <uri>https://old.reddit.com/user/blackstoreonline</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt; &lt;img alt="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" src="https://b.thumbs.redditmedia.com/tY-PA8qRCq6_itenx-ibWJJ7urdsbE45bXySDC1FH4s.jpg" title="[Release] Qwen3-TTS: Ultra-Low Latency (97ms), Voice Cloning &amp;amp; OpenAI-Compatible API" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt; &lt;p&gt;The Qwen team just dropped &lt;strong&gt;Qwen3-TTS&lt;/strong&gt;, and it‚Äôs a significant step forward for local speech synthesis. If you‚Äôve been looking for a high-quality, open-source alternative to ElevenLabs or OpenAI‚Äôs TTS that you can actually run on your own hardware, this is it.&lt;/p&gt; &lt;p&gt;We‚Äôve put together a repository that provides an &lt;strong&gt;OpenAI-compatible FastAPI server&lt;/strong&gt;, meaning you can use it as a drop-in replacement for any app already using OpenAI‚Äôs TTS endpoints. Streaming support out of the box, plug and play with Open-Webui.&lt;/p&gt; &lt;h1&gt;Why this is a big deal:&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Insane Speed:&lt;/strong&gt; It features a dual-track hybrid architecture that hits ~97ms end-to-end latency for streaming. It starts talking almost the instant you send the text.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Natural Voice Control:&lt;/strong&gt; You don't just send text; you can give it natural language instructions like &lt;em&gt;&amp;quot;Say this in an incredibly angry tone&amp;quot;&lt;/em&gt; or &lt;em&gt;&amp;quot;A shaky, nervous 17-year-old voice&amp;quot;&lt;/em&gt; and it actually follows through.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Easy Voice Cloning:&lt;/strong&gt; Give it a 3-second reference clip, and it can clone the timbre and emotion remarkably well.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;OpenAI Drop-in:&lt;/strong&gt; Works natively with the OpenAI Python client. Just change your &lt;code&gt;base_url&lt;/code&gt; to localhost.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multilingual:&lt;/strong&gt; Supports 10+ languages (ZH, EN, JP, KR, DE, FR, RU, PT, ES, IT).&lt;/li&gt; &lt;/ul&gt; &lt;h1&gt;Getting Started (The Quick Way)&lt;/h1&gt; &lt;p&gt;If you have Docker and a GPU, you can get this running in seconds:&lt;/p&gt; &lt;p&gt;Bash&lt;/p&gt; &lt;pre&gt;&lt;code&gt;git clone https://github.com/groxaxo/Qwen3-TTS-Openai-Fastapi docker build -t qwen3-tts-api . docker run --gpus all -p 8880:8880 qwen3-tts-api &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Python Usage (OpenAI Style)&lt;/h1&gt; &lt;p&gt;Python&lt;/p&gt; &lt;pre&gt;&lt;code&gt;from openai import OpenAI client = OpenAI(base_url=&amp;quot;http://localhost:8880/v1&amp;quot;, api_key=&amp;quot;not-needed&amp;quot;) response = client.audio.speech.create( model=&amp;quot;qwen3-tts&amp;quot;, voice=&amp;quot;Vivian&amp;quot;, # 9 premium voices included input=&amp;quot;This sounds way too human for a local model.&amp;quot;, speed=1.0 ) response.stream_to_file(&amp;quot;output.mp3&amp;quot;) &lt;/code&gt;&lt;/pre&gt; &lt;h1&gt;Technical Highlights&lt;/h1&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Architecture:&lt;/strong&gt; It uses the new &lt;strong&gt;Qwen3-TTS-Tokenizer-12Hz&lt;/strong&gt; for acoustic compression. It skips the traditional &amp;quot;LM + DiT&amp;quot; bottleneck, which is why the latency is so low.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Model Sizes:&lt;/strong&gt; Available in &lt;strong&gt;0.6B&lt;/strong&gt; (super fast/light) and &lt;strong&gt;1.7B&lt;/strong&gt; (high fidelity) versions.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;VRAM Friendly:&lt;/strong&gt; Supports FlashAttention 2 to keep memory usage down.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;&lt;strong&gt;Links to dive deeper:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://huggingface.co/collections/Qwen/qwen3-tts"&gt;ü§ó Hugging Face Collection&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://arxiv.org/abs/2601.15621"&gt;üìÑ Research Paper on arXiv&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/QwenLM/Qwen3-TTS"&gt;üíª Github Repo&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;I‚Äôm really curious to see how the community integrates this into local LLM agents. The 97ms latency makes real-time voice conversation feel actually... real.&lt;/p&gt; &lt;p&gt;Let me know if you run into any issues setting it up!&lt;/p&gt; &lt;p&gt;&lt;a href="https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d"&gt;https://preview.redd.it/sa9itpxw6dfg1.png?width=1280&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=7fe58c44a2d0b9d03a5bf099024f18752d48949d&lt;/a&gt;&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/blackstoreonline"&gt; /u/blackstoreonline &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qlzbhh/release_qwen3tts_ultralow_latency_97ms_voice/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-24T21:21:50+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmdf2a</id>
    <title>Has anyone got GLM 4.7 flash to not be shit?</title>
    <updated>2026-01-25T08:14:08+00:00</updated>
    <author>
      <name>/u/synth_mania</name>
      <uri>https://old.reddit.com/user/synth_mania</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Real talk. I feel like everyday I'm downloading a new quant and trying it out and not once have I got it to consistently work without looping.&lt;/p&gt; &lt;p&gt;I've tried with and without the suggested settings from unsloth, &lt;a href="http://z.ai"&gt;z.ai&lt;/a&gt;, and others, to no avail.&lt;/p&gt; &lt;p&gt;Additionally, this has to be the slowest inference I've ever seen from a 30B A3B model. In all fairness, my only point of reference is Qwen3 Coder, but compared to that at least, the token generation speed feels positively lethargic.&lt;/p&gt; &lt;p&gt;If anybody has any tips, please let me know because I feel like I'm going in circles here. I don't think I've ever seen a modern release that had this many issues right off the bat, with no apparent improvement after a few supposed fixes.&lt;/p&gt; &lt;p&gt;It's really unfortunate because I can see the potential this model has. The chain of thought in particular seems uniquely coherent.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/synth_mania"&gt; /u/synth_mania &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmdf2a/has_anyone_got_glm_47_flash_to_not_be_shit/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T08:14:08+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmlpjp</id>
    <title>Internet blackout and Local LLMs</title>
    <updated>2026-01-25T15:15:05+00:00</updated>
    <author>
      <name>/u/DunderSunder</name>
      <uri>https://old.reddit.com/user/DunderSunder</uri>
    </author>
    <content type="html">&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Due to protests and massacre in Iran we are facing severe internet blackout which has been ongoing for 400 HOURS. only after a few days 3 websites got white-listed: google, chatgpt, deepseek. everything else is blocked even subdomains like Gmail. at the very least few people have Starlink (which is illegal) and share their connection. Finding a working vpn is really hard (I busted my ass to load reddit).&lt;/p&gt; &lt;p&gt;Meanwhile, I've been using my local uncensored Gemma3 12B and Qwen3 8B (on 8gb VRAM with llama.cpp). Then we got access to chatgpt which was pretty good since we could ask it to read contents of some pages or get latest news. But still chatgpt is VERY unhelpful in terms of finding solutions to circumvent internet censorship even if I explain the truly fucked up situation it refuses, and deepseek is worse. This is where a large uncensored local LLM could be very helpful.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/DunderSunder"&gt; /u/DunderSunder &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"&gt;[comments]&lt;/a&gt;&lt;/span&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmlpjp/internet_blackout_and_local_llms/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T15:15:05+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmjzx1</id>
    <title>KV cache fix for GLM 4.7 Flash</title>
    <updated>2026-01-25T14:06:55+00:00</updated>
    <author>
      <name>/u/jacek2023</name>
      <uri>https://old.reddit.com/user/jacek2023</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt; &lt;img alt="KV cache fix for GLM 4.7 Flash" src="https://external-preview.redd.it/Yd6yP0tYXhTq7c3g8_wDa0Z1Zijr0IAXDTPXGjQc7ts.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=697845700fcf489c797d62fb0a23359703d41821" title="KV cache fix for GLM 4.7 Flash" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;tl;dr: remove Air from GLM 4.7 Flash&lt;/p&gt; &lt;p&gt;KV cache uses a lot of VRAM. GLM 4.7 Flash doesn‚Äôt even use V in the KV cache. With long contexts, this means gigabytes of VRAM saved, so you can run much longer context on the same setup.&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/jacek2023"&gt; /u/jacek2023 &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://github.com/ggml-org/llama.cpp/pull/19067"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmjzx1/kv_cache_fix_for_glm_47_flash/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T14:06:55+00:00</published>
  </entry>
  <entry>
    <id>t3_1qmir5d</id>
    <title>What do you actually want from a private AI chat on your phone?</title>
    <updated>2026-01-25T13:12:00+00:00</updated>
    <author>
      <name>/u/AppDeveloperAsdf</name>
      <uri>https://old.reddit.com/user/AppDeveloperAsdf</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt; &lt;img alt="What do you actually want from a private AI chat on your phone?" src="https://external-preview.redd.it/b2k1d3JkaHV0aGZnMbzKSbNJeiRdJL3Vv6uz8BgUY-ES1g_l6yTqUuzYy_d7.png?width=640&amp;amp;crop=smart&amp;amp;auto=webp&amp;amp;s=8f45f222f3ee31c2716f286b9cf0998d79f80e6f" title="What do you actually want from a private AI chat on your phone?" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey friends. We are building zerotap - an Android app where AI can control your phone like a human (taps, scrolls, reads screen). It supports Ollama, proxies like OpenRouter and Straico and models directly such as OpenAI, Claude, Gemini and DeepSeek.&lt;/p&gt; &lt;p&gt;Recently we added a chat interface, so now it works like a regular AI chat that can take over your device when needed.&lt;/p&gt; &lt;p&gt;Now we are planning what to focus on next and we'd love your input. Some options we're considering:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;MCP servers&lt;/strong&gt; - connect your chat to external tools and services&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Deep research&lt;/strong&gt; - letting the AI browse and gather information for you&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multi-modality&lt;/strong&gt; ‚Äî image read &amp;amp; write (generation)&lt;/li&gt; &lt;li&gt;&lt;strong&gt;On-device models&lt;/strong&gt; ‚Äî we are working on Gemma 3n and Qwen support, but small context windows are hurting performance so much&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Speaking of which - for those of you running Ollama: do you expose your instance to the internet or keep it local network only?&lt;/p&gt; &lt;p&gt;Honest question: what would make an AI chat on your phone actually useful for you on a daily basis? Not as a toy, but as something you would rely on - what's missing from current mobile AI apps (that supports ollama) that annoys you the most?&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/AppDeveloperAsdf"&gt; /u/AppDeveloperAsdf &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://v.redd.it/em3174huthfg1"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1qmir5d/what_do_you_actually_want_from_a_private_ai_chat/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2026-01-25T13:12:00+00:00</published>
  </entry>
  <entry>
    <id>t3_1mpk2va</id>
    <title>Announcing LocalLlama discord server &amp; bot!</title>
    <updated>2025-08-13T23:21:05+00:00</updated>
    <author>
      <name>/u/HOLUPREDICTIONS</name>
      <uri>https://old.reddit.com/user/HOLUPREDICTIONS</uri>
    </author>
    <content type="html">&lt;table&gt; &lt;tr&gt;&lt;td&gt; &lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt; &lt;img alt="Announcing LocalLlama discord server &amp;amp; bot!" src="https://b.thumbs.redditmedia.com/QBscWhXGvo8sy9oNNt-7et1ByOGRWY1UckDAudAWACM.jpg" title="Announcing LocalLlama discord server &amp;amp; bot!" /&gt; &lt;/a&gt; &lt;/td&gt;&lt;td&gt; &lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;INVITE: &lt;a href="https://discord.gg/rC922KfEwj"&gt;https://discord.gg/rC922KfEwj&lt;/a&gt;&lt;/p&gt; &lt;p&gt;There used to be one old discord server for the subreddit but it was deleted by the previous mod.&lt;/p&gt; &lt;p&gt;Why? The subreddit has grown to 500k users - inevitably, some users like a niche community with more technical discussion and fewer memes (even if relevant).&lt;/p&gt; &lt;p&gt;We have a discord bot to test out open source models.&lt;/p&gt; &lt;p&gt;Better contest and events organization.&lt;/p&gt; &lt;p&gt;Best for quick questions or showcasing your rig!&lt;/p&gt; &lt;/div&gt;&lt;!-- SC_ON --&gt; &amp;#32; submitted by &amp;#32; &lt;a href="https://old.reddit.com/user/HOLUPREDICTIONS"&gt; /u/HOLUPREDICTIONS &lt;/a&gt; &lt;br /&gt; &lt;span&gt;&lt;a href="https://www.reddit.com/gallery/1mpk2va"&gt;[link]&lt;/a&gt;&lt;/span&gt; &amp;#32; &lt;span&gt;&lt;a href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"&gt;[comments]&lt;/a&gt;&lt;/span&gt; &lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;</content>
    <link href="https://old.reddit.com/r/LocalLLaMA/comments/1mpk2va/announcing_localllama_discord_server_bot/"/>
    <category term="LocalLLaMA" label="r/LocalLLaMA"/>
    <published>2025-08-13T23:21:05+00:00</published>
  </entry>
</feed>
